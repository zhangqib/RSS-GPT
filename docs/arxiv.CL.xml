<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>


<item>
<title>Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search</title>
<link>https://arxiv.org/abs/2511.19648</link>
<guid>https://arxiv.org/abs/2511.19648</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-hop question answering, knowledge graphs, large language models, embedding-guided search, verifiable reasoning<br /><br />Summary:<br />1. The paper addresses the challenge of efficient and verifiable multi-hop question answering over knowledge graphs, a task complicated by the large number of possible reasoning paths.  
2. Traditional methods rely heavily on Large Language Models (LLMs) for entity linking and path ranking, which are computationally expensive and often produce answers without verifiable grounding in the knowledge graph.  
3. The authors propose two hybrid algorithms: (1) LLM-Guided Planning, which uses a single LLM call to generate relation sequences that are executed through a breadth-first search, delivering near-perfect accuracy with answers fully grounded in the knowledge graph; (2) Embedding-Guided Neural Search, which eliminates LLM calls entirely by integrating text and graph embeddings through a compact 6.7M-parameter edge scorer, enabling over 100x speedup with competitive accuracy.  
4. They achieve further efficiency gains by knowledge distillation, compressing the planning capacity into a 4-billion-parameter model that matches large LLM performance without incurring API inference costs.  
5. Evaluation on the MetaQA dataset shows that grounded reasoning consistently outperforms ungrounded answer generation, with symbolic structured planning offering better transferability than direct answer generation. The results emphasize that effective multi-hop reasoning requires architectural inductive biases combining symbolic and learned representations rather than solely large models at inference time. <div>
arXiv:2511.19648v1 Announce Type: new 
Abstract: Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian</title>
<link>https://arxiv.org/abs/2511.19719</link>
<guid>https://arxiv.org/abs/2511.19719</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, explanation faithfulness, emotion classification, Persian language, prompting strategies  

<br /><br />Summary:  
This study investigates the faithfulness of explanations generated by large language models (LLMs) in the task of emotion classification for Persian, a low-resource language. First, it evaluates faithfulness by comparing the influential words identified by LLM-generated explanations against those marked by human annotators. Second, the authors use token-level log-probabilities to derive confidence scores as a measure of explanation reliability. Third, two prompting strategies are tested for their impact on explanation faithfulness: Predict-then-Explain and Explain-then-Predict, analyzing which order yields more faithful explanations. Fourth, results show that although LLMs deliver strong classification accuracy, their explanations often diverge from faithful reasoning and align more closely with each other than with human judgment. Lastly, these findings underscore the limitations of current explanation approaches and metrics, highlighting the urgent need for more robust methods to ensure explanation faithfulness and reliability of LLMs, particularly in multilingual and low-resource language settings such as Persian. <div>
arXiv:2511.19719v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation</title>
<link>https://arxiv.org/abs/2511.19739</link>
<guid>https://arxiv.org/abs/2511.19739</guid>
<content:encoded><![CDATA[
<div> Keywords: domain-specific embeddings, cardiology NLP, transformer models, Low-Rank Adaptation, BioLinkBERT<br /><br />Summary:<br /><br />This study addresses the critical need for domain-specific text embeddings in clinical natural language processing, focusing on cardiology. It systematically compares ten transformer-based embedding models, fine-tuned through Low-Rank Adaptation (LoRA) on a large dataset of 106,535 cardiology text pairs sourced from authoritative medical textbooks. The evaluation reveals that encoder-only architectures, especially BioLinkBERT, outperform larger decoder-based models in producing effective domain-specific embeddings. Notably, BioLinkBERT achieves a separation score of 0.510, indicating better semantic distinction within cardiology texts. Furthermore, the study highlights that superior performance does not necessarily correlate with model size, as BioLinkBERT requires significantly fewer computational resources compared to the larger models. These findings challenge the common assumption that bigger language models are always better for specialized domains. The research provides practical insights for developing efficient and effective clinical NLP systems tailored to cardiology. Importantly, all models, along with their training code and evaluation datasets, have been made publicly available to encourage reproducibility and further advancement in medical informatics research. <div>
arXiv:2511.19739v1 Announce Type: new 
Abstract: Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What does it mean to understand language?</title>
<link>https://arxiv.org/abs/2511.19757</link>
<guid>https://arxiv.org/abs/2511.19757</guid>
<content:encoded><![CDATA[
<div> Keywords: language understanding, mental models, brain regions, cognitive neuroscience, language processing<br /><br />Summary:<br /><br />The article proposes that understanding language involves more than just grasping the surface-level meaning of words; it requires building complex mental representations of the situations described. Due to inherent limitations in processing within the brain's core language system, truly deep comprehension depends on transferring information from this system to other brain areas. These additional regions are responsible for generating perceptual and motor representations, forming mental models, and accessing stored world knowledge and autobiographical memories. The authors review current empirical evidence supporting this hypothesis, emphasizing the necessity of such cross-regional communication for rich language understanding. Furthermore, advances in cognitive neuroscience have created both the theoretical framework and methodological tools to rigorously test this idea. This emerging approach offers a promising avenue to clarify what it means, at cognitive and neural levels, to comprehend language fully. The article encourages leveraging these developments to deepen our understanding of language processing beyond linguistic structures to the integration with broader cognitive systems. <div>
arXiv:2511.19757v1 Announce Type: new 
Abstract: Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gender Bias in Emotion Recognition by Large Language Models</title>
<link>https://arxiv.org/abs/2511.19785</link>
<guid>https://arxiv.org/abs/2511.19785</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fairness, gender bias, emotional theory of mind, debiasing strategies<br /><br />Summary:<br />1. This paper addresses the critical issue of fairness in large language models (LLMs) with a focus on emotional theory of mind, which involves understanding and predicting human emotions based on contextual descriptions.<br />2. The authors investigate whether LLMs demonstrate gender biases when asked to infer emotions from a description of a person and their environment, highlighting potential unfairness in model behavior.<br />3. They find that LLMs do exhibit gender biases in emotional predictions, raising concerns about the social implications of deploying such models in emotionally sensitive applications.<br />4. To mitigate these biases, the study proposes multiple debiasing strategies, comparing their effectiveness in reducing gender-related unfairness in emotional inference tasks.<br />5. Results indicate that significant bias reduction is achieved only through training-based interventions tailored to address bias, whereas prompt engineering and other inference-time methods prove insufficient.<br />6. Overall, the work stresses the necessity of integrating debiasing techniques during model training to ensure fair and responsible utilization of LLMs in applications involving emotional understanding. <div>
arXiv:2511.19785v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, "How does this person feel?". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions</title>
<link>https://arxiv.org/abs/2511.19816</link>
<guid>https://arxiv.org/abs/2511.19816</guid>
<content:encoded><![CDATA[
<div> Valence, Arousal, Dominance, Multiword Expressions, Lexicon<br /><br />Summary:<br /><br />This article presents the NRC VAD Lexicon v2, an enhanced resource for capturing emotional associations of words and multiword expressions (MWEs) based on the dimensions of Valence, Arousal, and Dominance (VAD). 1. The lexicon expands upon the original NRC VAD Lexicon (2018) by including human ratings for 10,000 English MWEs such as idioms, noun compounds, and verb particle constructions, as well as increasing unigram coverage with 25,000 word entries, especially those that have become more frequent since the initial release. 2. The ratings in this updated lexicon have been shown to be highly reliable through validation efforts. 3. The lexicon facilitates the examination of emotional characteristics of MWEs, addressing to what extent these expressions exhibit strong emotional content. 4. It also enables an analysis of the emotional compositionality of MWEs, exploring how constituent words’ emotional values contribute to the overall expression’s emotional tone. 5. Due to its broad scope and reliability, NRC VAD Lexicon v2 can support diverse research areas including Natural Language Processing, Psychology, Public Health, Digital Humanities, and the Social Sciences. The lexicon is freely accessible online at the project webpage. <div>
arXiv:2511.19816v1 Announce Type: new 
Abstract: Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana</title>
<link>https://arxiv.org/abs/2511.19818</link>
<guid>https://arxiv.org/abs/2511.19818</guid>
<content:encoded><![CDATA[
<div> Sentiment analysis, low-resource languages, automatic labeling, African languages, multilingual corpus<br /><br />Summary:<br /><br />1. The article addresses the challenge of sentiment analysis for low-resource African languages, highlighting the scarcity of digitally labeled text data which hinders the development of accurate sentiment analysis tools. 2. Manual labeling is costly and time-consuming, creating a need for automatic methods that can efficiently generate sentiment labels with minimal human intervention. 3. The authors propose a language-independent automatic sentiment labeling approach that exploits sentiment-bearing emojis and words to label social media texts. 4. Their method was evaluated using tweets in English, Sepedi, and Setswana, leveraging the SAfriSenti multilingual sentiment corpus specifically designed for South African languages. 5. Experimental results demonstrate labeling accuracies of 66% for English, 69% for Sepedi, and 63% for Setswana, meaning on average only 34% of the automatically assigned labels require manual correction, significantly reducing the effort needed for creating annotated datasets. <div>
arXiv:2511.19818v1 Announce Type: new 
Abstract: Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs</title>
<link>https://arxiv.org/abs/2511.19852</link>
<guid>https://arxiv.org/abs/2511.19852</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalized LLMs, prompt optimization, personality expression, dynamic profile, situational benchmarking<br /><br />Summary:  
The paper addresses the challenge of maximizing personality expression in personalized Large Language Models (LLMs). 1) It introduces PersonaPulse, a novel framework that dynamically optimizes role-play prompts by leveraging the LLMs' intrinsic understanding of personality traits. 2) PersonaPulse incorporates a situational response benchmark as a scoring mechanism to enable a realistic, context-aware evaluation during prompt optimization. 3) Quantitative evaluations show that PersonaPulse-generated prompts outperform previous approaches that relied on static personality descriptions from psychological literature. 4) The study investigates how model size influences the effectiveness of personality modeling, revealing size-related trends in personality evocation capacity. 5) Finally, the research discovers that selectively pausing the optimization process allows partial control over how strongly certain personality traits are expressed in the model’s responses. This work emphasizes the critical role of prompt optimization in enhancing realistic personality expression in LLMs, paving the way for more adaptive and engaging AI interactions in future applications. <div>
arXiv:2511.19852v1 Announce Type: new 
Abstract: Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction</title>
<link>https://arxiv.org/abs/2511.19858</link>
<guid>https://arxiv.org/abs/2511.19858</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, medical error detection, prompting strategies, retrieval-augmented dynamic prompting, clinical documentation errors<br /><br />Summary:  
Objective: Clinical documentation often contains factual, diagnostic, and management errors that risk patient safety. Large language models (LLMs) have potential to detect and correct these errors, but their effectiveness varies depending on prompting strategies.  
Methods: The study evaluated nine instruction-tuned LLMs—including GPT, Claude, Gemini, and OpenAI's o-series—on the MEDEC dataset for three tasks: error flag detection, error sentence detection, and error correction. Three prompting strategies were compared: zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP). Performance metrics included accuracy, recall, false-positive rate (FPR), and a combined score (ROUGE-1, BLEURT, BERTScore) for error correction quality.  
Results: Zero-shot prompting yielded low recall, missing many errors with abbreviations or atypical forms. SPR improved recall but also increased false positives. RDP outperformed both by reducing FPR by roughly 15%, boosting recall by 5-10% in error sentence detection, and producing more contextually accurate error corrections.  
Conclusion: Retrieval-augmented dynamic prompting consistently enhances LLM performance in medical error processing tasks, improving detection accuracy, reducing false alarms, and increasing the reliability of corrections across diverse models. <div>
arXiv:2511.19858v1 Announce Type: new 
Abstract: Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AppSelectBench: Application-Level Tool Selection Benchmark</title>
<link>https://arxiv.org/abs/2511.19957</link>
<guid>https://arxiv.org/abs/2511.19957</guid>
<content:encoded><![CDATA[
<div> Keywords: Computer Using Agents, application selection, benchmarks, large language models, AppSelectBench<br /><br />Summary:<br /><br />1. Computer Using Agents (CUAs) are increasingly using external tools to carry out complex tasks, making it essential for them to select the appropriate application before accessing specific APIs or functions.<br /><br />2. Application selection is critical as it ensures the agent sets up the correct environment, reduces confusion in task orchestration, and focuses efficiently on relevant contexts.<br /><br />3. Existing benchmarks mainly test fine-grained API selection but lack comprehensive evaluation of the ability to reason across and choose between different applications.<br /><br />4. To address this, the authors propose AppSelectBench, a new benchmark designed to evaluate CUAs’ application selection capabilities across 100 widely used desktop applications with over 100,000 realistic and semantically grounded user tasks.<br /><br />5. AppSelectBench includes a novel task generation pipeline and unified evaluation protocols, supporting various testing scenarios such as random, heuristic, zero-shot, few-shot, and retrieval-augmented settings.<br /><br />6. Experiments with both closed-source and open-source large language models reveal strengths and weaknesses in inter-application reasoning, highlighting that even the best models still struggle with consistent application choice.<br /><br />7. The benchmark serves as a foundation for advancing application-level reasoning in CUAs, a vital but underexplored capability, and the source code is publicly available at https://github.com/microsoft/appselectbench. <div>
arXiv:2511.19957v1 Announce Type: new 
Abstract: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers</title>
<link>https://arxiv.org/abs/2511.19987</link>
<guid>https://arxiv.org/abs/2511.19987</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, domain specialization, reranker, Entity Abstraction for Generalization, Latent Semantic Router<br /><br />Summary:  
1. The paper addresses limitations of decoder-only rerankers in Retrieval-Augmented Generation (RAG), particularly their struggle to capture domain-specific nuances in high-stakes areas like finance, law, and medicine.  
2. The authors introduce R2R, a domain-aware reranking framework that integrates dynamic expert routing with a novel two-stage training strategy called Entity Abstraction for Generalization (EAG).  
3. EAG combats surface-form overfitting by implementing a counter-shortcut mechanism, which masks the most predictive surface cues, encouraging the model to learn domain-invariant relevance patterns instead of simply memorizing dataset-specific entities.  
4. R2R features a lightweight Latent Semantic Router that efficiently activates the appropriate domain experts by probing internal representations from a frozen backbone decoder and selecting the optimal Low-Rank Adaptation (LoRA) expert for each query.  
5. Extensive experimental evaluations across multiple reranker backbones and various specialized domains demonstrate that R2R consistently outperforms generalist and single-domain fine-tuned baselines, proving it to be a model-agnostic and modular solution with strong cross-domain robustness. <div>
arXiv:2511.19987v1 Announce Type: new 
Abstract: Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test</title>
<link>https://arxiv.org/abs/2511.19997</link>
<guid>https://arxiv.org/abs/2511.19997</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, directional bias, entropy, GPT-2, sequence models<br /><br />Summary:<br /><br />1. Transformers are theoretically symmetrical in processing sequences, meaning they do not inherently prefer forward (left-to-right) over backward (right-to-left) mappings. However, empirical findings indicate a "reversal curse," where performance degrades on reversed sequences.<br /><br />2. This work investigates whether directional difficulties arise from the architecture itself or from real-world linguistic structures by using a fully synthetic, entropy-controlled benchmark that removes natural language biases.<br /><br />3. They design forward and inverse tasks using random string mappings with adjustable branching factor K, creating tasks with zero conditional entropy in the forward direction and calculable entropy floors in the inverse.<br /><br />4. Experiments with scratch-trained GPT-2 models show a significant directional optimization gap (e.g., 1.16 nats at K=5), exceeding that of simpler MLP models trained on identical data, indicating directional friction intrinsic to the Transformer architecture.<br /><br />5. Pre-training and parameter-efficient tuning methods like LoRA influence but do not eliminate the gap, and LoRA struggles with high-entropy inverse tasks, supporting that directional learning difficulties are an inherent characteristic of causal Transformer training independent of linguistic or statistical asymmetries.<br /><br />6. This research offers a controlled setting to study directional biases, urging mechanistic investigation into why inversion tasks remain fundamentally harder for Transformers despite their theoretical symmetry. <div>
arXiv:2511.19997v1 Announce Type: new 
Abstract: Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media</title>
<link>https://arxiv.org/abs/2511.20001</link>
<guid>https://arxiv.org/abs/2511.20001</guid>
<content:encoded><![CDATA[
<div> Mental health, Cyberbullying, Multiclass classification, Transformers, Explainability<br /><br />Summary:<br /><br />This paper addresses the increasing need for scalable and interpretable detection systems for mental health challenges and cyberbullying in digital spaces. It introduces a unified multiclass classification framework to detect ten distinct mental health and cyberbullying categories using social media data from Twitter and Reddit. A "split-then-balance" data pipeline is developed to train on balanced datasets while evaluating on a realistically imbalanced held-out test set. The study compares traditional lexical models, hybrid methods, and end-to-end fine-tuned transformers, finding that fine-tuning significantly improves performance. The domain-adapted MentalBERT model outperforms others, achieving 0.92 accuracy and a Macro F1 score of 0.76, surpassing a generic BERT model and a zero-shot large language model baseline. Ethically, the system is framed as a human-in-the-loop screening aid rather than a diagnostic tool. To enhance interpretability, the authors propose a hybrid SHAPLLM explainability framework and develop a "Social Media Screener" dashboard prototype to incorporate model predictions and explanations into moderator workflows. The work sets a robust baseline and emphasizes the future need for multi-label, clinically validated datasets at the intersection of online safety and computational mental health. <div>
arXiv:2511.20001v1 Announce Type: new 
Abstract: Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous "split-then-balance" pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conducted a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAPLLM explainability framework and present a prototype dashboard ("Social Media Screener") designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online-PVLM: Advancing Personalized VLMs with Online Concept Learning</title>
<link>https://arxiv.org/abs/2511.20056</link>
<guid>https://arxiv.org/abs/2511.20056</guid>
<content:encoded><![CDATA[
<div> Personalized Visual Language Models, Online Concept Learning, Hyperbolic Representations, Scalable Adaptation, OP-Eval Benchmark<br /><br />Summary:<br /><br />1. The paper introduces Online-PVLM, a novel framework designed for online concept learning within Personalized Visual Language Models (VLMs), addressing the challenge of real-time adaptation to user-specific concepts without retraining.<br />2. Conventional personalized VLMs require learning separate embeddings for each concept, which is inefficient especially in large-scale settings due to the difficulty in retrieving embeddings quickly during testing.<br />3. Online-PVLM leverages hyperbolic representations to generate concept embeddings in a train-free manner at test time, providing a scalable and efficient alternative to existing approaches.<br />4. To evaluate their method, the authors create OP-Eval, a comprehensive benchmark containing 1,292 concepts and over 30,000 high-quality instances with diverse question types, simulating realistic application scenarios.<br />5. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance in online concept learning, validating its effectiveness and practical applicability. Additionally, the authors commit to releasing the source code and dataset to further benefit the research community. <div>
arXiv:2511.20056v1 Announce Type: new 
Abstract: Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTA: A Merge-then-Adapt Framework for Personalized Large Language Model</title>
<link>https://arxiv.org/abs/2511.20072</link>
<guid>https://arxiv.org/abs/2511.20072</guid>
<content:encoded><![CDATA[
<div> Personalized Large Language Models, Meta-LoRA, Adaptive LoRA Fusion, Few-Shot Personalization, Scalability<br /><br />Summary:<br /><br />The paper addresses the challenges in building Personalized Large Language Models (PLLMs) that can align outputs with user preferences efficiently. It identifies two major limitations in current methods: the linear growth in storage with the number of users due to fine-tuning separate modules and performance degradation for users with sparse data. To overcome these challenges, the authors propose MTA, a Merge-then-Adapt framework. The first stage involves constructing a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules to capture diverse personalization aspects. In the second stage, Adaptive LoRA Fusion dynamically retrieves and merges relevant anchor meta-LoRAs to create a user-specific LoRA module, which eliminates the need for storing user-specific modules and supports flexible personalization combinations. The third stage, LoRA Stacking for Few-Shot Personalization, adds an extra ultra-low-rank LoRA module on top of the merged LoRA, enabling effective fine-tuning with limited data. Experimental results on the LaMP benchmark show that MTA outperforms existing state-of-the-art methods across various tasks, demonstrating improved scalability and personalization effectiveness. <div>
arXiv:2511.20072v1 Announce Type: new 
Abstract: Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering</title>
<link>https://arxiv.org/abs/2511.20086</link>
<guid>https://arxiv.org/abs/2511.20086</guid>
<content:encoded><![CDATA[
<div> Keywords: BiasPrompting, large language models, multiple-choice questions, reasoning generation, inference framework<br /><br />Summary:<br /><br />1. The paper addresses limitations in current large language model (LLM) approaches to multiple-choice question (MCQ) tasks, particularly the lack of contextual grounding or explanations for answer choices, which weakens the models' reasoning capabilities.<br />2. A novel inference framework called BiasPrompting is introduced, designed to improve reasoning during the answer selection process by encouraging LLMs to generate and critically evaluate reasonings for all plausible answer options.<br />3. BiasPrompting operates in two stages: first, a reasoning generation stage where the model produces supportive reasonings for each possible answer, and second, a reasoning-guided agreement stage that synthesizes these reasonings to select the most plausible final answer.<br />4. Comprehensive evaluations demonstrate that BiasPrompting significantly improves performance across five widely used MCQ benchmarks, highlighting its effectiveness over existing methods.<br />5. The framework enhances the overall reasoning ability of LLMs and provides a robust foundation for handling complex or challenging MCQ settings where previous approaches may struggle. <div>
arXiv:2511.20086v1 Announce Type: new 
Abstract: With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space</title>
<link>https://arxiv.org/abs/2511.20102</link>
<guid>https://arxiv.org/abs/2511.20102</guid>
<content:encoded><![CDATA[
<div> Sparse Attention, Gradient Update Deficiency, SSA, Long-Context Extrapolation, Sparse-Full Attention Alignment<br /><br />Summary:<br /><br />The paper addresses the quadratic computational complexity of full attention in large language models (LLMs), which hinders efficient processing of long contexts. Sparse attention reduces this cost by limiting each query to attend to a subset of tokens but often suffers performance degradation in training-free methods. Native sparse-attention methods like NSA and MoBA improve performance but paradoxically generate less sparse attentions than full-attention models. This paradox arises from gradient update deficiency, where low-ranked key-value pairs excluded during sparse training neither contribute forward signals nor receive backward gradients, preventing proper suppression learning. To solve this, the authors propose SSA (Sparse Sparse Attention), a unified training framework that jointly considers sparse and full attention while enforcing bidirectional alignment at every layer. This approach maintains gradient flow to all tokens and encourages sparse-attention outputs to align with full-attention results, resulting in stronger sparsity. Experiments show SSA achieves state-of-the-art performance under both sparse and full attention inference on several commonsense reasoning benchmarks. The method also enables models to adapt to different sparsity budgets dynamically, allowing flexible compute-performance trade-offs at inference time. Finally, SSA improves long-context extrapolation by reducing attention over-allocation in sink areas, demonstrating superior ability in extended context scenarios. <div>
arXiv:2511.20102v1 Announce Type: new 
Abstract: The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EM2LDL: A Multilingual Speech Corpus for Mixed Emotion Recognition through Label Distribution Learning</title>
<link>https://arxiv.org/abs/2511.20106</link>
<guid>https://arxiv.org/abs/2511.20106</guid>
<content:encoded><![CDATA[
<div> Keywords: EM2LDL, multilingual speech corpus, mixed emotion recognition, label distribution learning, HuBERT-large-EN<br /><br />Summary:<br /><br />This study presents EM2LDL, a new multilingual speech corpus aimed at improving mixed emotion recognition through label distribution learning techniques. It addresses key limitations found in existing emotion corpora, which are mainly monolingual, single-label, lack linguistic diversity, cannot effectively model mixed emotions, and often lack ecological validity. EM2LDL includes expressive utterances in English, Mandarin, and Cantonese, reflecting the intra-utterance code-switching common in multilingual regions such as Hong Kong and Macao. The corpus contains spontaneous emotional expressions sourced from online platforms and is finely annotated with emotion distributions spanning 32 categories, capturing nuanced emotional states. The authors provide experimental baseline results using self-supervised learning models, highlighting robust performance across speaker-independent evaluations based on gender, age, and personality factors, with HuBERT-large-EN showing the best results. By integrating linguistic diversity and naturalistic emotional data, EM2LDL enables the study of complex emotional dynamics in multilingual environments. The dataset and associated resources are made publicly accessible to facilitate research and development of adaptive, empathetic systems in affective computing fields, including mental health monitoring and cross-cultural communication. <div>
arXiv:2511.20106v1 Announce Type: new 
Abstract: This study introduces EM2LDL, a novel multilingual speech corpus designed to advance mixed emotion recognition through label distribution learning. Addressing the limitations of predominantly monolingual and single-label emotion corpora \textcolor{black}{that restrict linguistic diversity, are unable to model mixed emotions, and lack ecological validity}, EM2LDL comprises expressive utterances in English, Mandarin, and Cantonese, capturing the intra-utterance code-switching prevalent in multilingual regions like Hong Kong and Macao. The corpus integrates spontaneous emotional expressions from online platforms, annotated with fine-grained emotion distributions across 32 categories. Experimental baselines using self-supervised learning models demonstrate robust performance in speaker-independent gender-, age-, and personality-based evaluations, with HuBERT-large-EN achieving optimal results. By incorporating linguistic diversity and ecological validity, EM2LDL enables the exploration of complex emotional dynamics in multilingual settings. This work provides a versatile testbed for developing adaptive, empathetic systems for applications in affective computing, including mental health monitoring and cross-cultural communication. The dataset, annotations, and baseline codes are publicly available at https://github.com/xingfengli/EM2LDL.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach</title>
<link>https://arxiv.org/abs/2511.20107</link>
<guid>https://arxiv.org/abs/2511.20107</guid>
<content:encoded><![CDATA[
<div> Mispronunciation Detection, Diagnosis, Retrieval Techniques, Pretrained ASR, Language Learning<br /><br />Summary:<br /><br />1. This paper addresses the task of Mispronunciation Detection and Diagnosis (MDD), a key problem in language learning and speech therapy aimed at identifying and diagnosing pronunciation errors. 2. Unlike traditional approaches that depend on phoneme-level scoring models or specialized training, the authors propose a novel, training-free framework. 3. Their method leverages retrieval techniques combined with a pretrained Automatic Speech Recognition (ASR) model to detect mispronunciations without the need for phoneme-specific modeling or additional task-specific training. 4. The framework simplifies the process by avoiding complex model training, reducing development time and resources. 5. Evaluations on the L2-ARCTIC dataset demonstrate that the proposed method achieves a superior F1 score of 69.60%, marking its effectiveness while maintaining efficiency. <div>
arXiv:2511.20107v1 Announce Type: new 
Abstract: Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2511.20120</link>
<guid>https://arxiv.org/abs/2511.20120</guid>
<content:encoded><![CDATA[
<div> Grammatical Error Correction, Indic Languages, Large Language Models, Prompting, Few-Shot Learning<br /><br />Summary:<br /><br />1. This work addresses the challenge of Grammatical Error Correction (GEC) in Indic languages, which are low-resource and linguistically diverse, making GEC difficult compared to high-resource languages like English. <br /><br />2. The authors explore prompting-based approaches using state-of-the-art large language models (LLMs) such as GPT-4.1, Gemini-2.5, and LLaMA-4, combined with few-shot learning strategies to adapt these models effectively to low-resource settings. <br /><br />3. Results indicate that basic prompting strategies, including zero-shot and few-shot approaches, enable these LLMs to significantly outperform fine-tuned Indic language models like Sarvam-22B, highlighting the excellent multilingual generalization capabilities of contemporary LLMs for GEC tasks. <br /><br />4. The study demonstrates that carefully designed prompts and lightweight model adaptation substantially improve grammatical correction quality across multiple Indic languages. <br /><br />5. The experiments yielded top rankings in a shared task, placing first in Tamil and Hindi, second in Telugu, fourth in Bangla, and fifth in Malayalam based on GLEU scores, underscoring prompt-driven NLP techniques' effectiveness and the potential of large-scale LLMs to bridge resource gaps in multilingual GEC. <div>
arXiv:2511.20120v1 Announce Type: new 
Abstract: Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models</title>
<link>https://arxiv.org/abs/2511.20143</link>
<guid>https://arxiv.org/abs/2511.20143</guid>
<content:encoded><![CDATA[
<div> Keywords: Named Entity Recognition, discontinuous entities, grid-tagging, image data augmentation, segmentation challenges<br /><br />Summary:<br /><br />Named Entity Recognition (NER) is a vital task in natural language processing that remains particularly difficult when it comes to identifying discontinuous entities, especially those spanning across sentences. The key challenge is effective text segmentation, as conventional methods often missegment or entirely overlook these cross-sentence discontinuous entities, which significantly hurts recognition accuracy. To tackle these issues, the authors focus on improving segmentation and reducing omissions associated with discontinuous entities. They leverage grid-tagging models, which have been shown to be effective for information extraction due to their flexible tagging schemes and strong architectures. The novel contribution is the integration of image data augmentation techniques—specifically cropping, scaling, and padding—into grid-based models to better handle segmentation difficulties and improve recognition of discontinuous entities. Experimental evaluations on three datasets (CADEC, ShARe13, and ShARe14) demonstrate that traditional segmentation methods fail to properly capture cross-sentence discontinuous entities, resulting in performance drops. By contrast, their augmented grid models achieve significant improvements, evidenced by an overall F1 score increase of 1-2.5%, and a notable boost of 3.7-8.4% specifically for discontinuous entities. This confirms the effectiveness of the proposed augmentation approach in enhancing NER for challenging discontinuous entity cases. <div>
arXiv:2511.20143v1 Announce Type: new 
Abstract: Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP</title>
<link>https://arxiv.org/abs/2511.20182</link>
<guid>https://arxiv.org/abs/2511.20182</guid>
<content:encoded><![CDATA[
<div> KyrgyzBERT, Kyrgyz NLP, low-resource language, sentiment analysis, monolingual BERT<br /><br />Summary:<br /><br />Kyrgyz is identified as a low-resource language lacking essential foundational NLP tools. To bridge this gap, the authors introduce KyrgyzBERT, which is the first publicly available monolingual BERT-based language model specifically designed for the Kyrgyz language. This model contains 35.9 million parameters and employs a custom tokenizer tailored to Kyrgyz’s unique morphological structure. For evaluation, the authors create a new benchmark dataset named kyrgyz-sst2 by translating the Stanford Sentiment Treebank and manually annotating its complete test set. KyrgyzBERT is then fine-tuned on this dataset, achieving an impressive F1-score of 0.8280. This performance is notably competitive when compared with a fine-tuned multilingual BERT (mBERT) model that is five times larger in size. To facilitate and encourage further research in Kyrgyz NLP, all developed models, datasets, and codebases have been publicly released. This contribution constitutes a significant step forward in providing robust language models and evaluation resources tailored for the Kyrgyz language, supporting the development of future NLP applications. <div>
arXiv:2511.20182v1 Announce Type: new 
Abstract: Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance</title>
<link>https://arxiv.org/abs/2511.20233</link>
<guid>https://arxiv.org/abs/2511.20233</guid>
<content:encoded><![CDATA[
<div> fact-checking, large language models, explainability, latent explanations, self-refining<br /><br />Summary:<br /><br />The paper addresses the challenge of misinformation on social media by proposing an automated fact-checking system called REFLEX that improves verdict accuracy and explanation quality. Unlike existing LLM-based approaches that depend heavily on external knowledge and suffer from latency and hallucinations, REFLEX leverages internal knowledge within its backbone model, enhancing reliability and responsiveness essential for real-time applications. REFLEX reformulates fact-checking as a role-play dialogue, jointly training for verdict prediction and explanation generation, which allows the system to provide interpretable results. The method adaptively extracts contrastive activation pairs between the original and fine-tuned model versions to build steering vectors that separate truth into style and substance, naturally guiding inference and reducing noisy or unfaithful explanations. Experiments on real-world datasets demonstrate that REFLEX outperforms earlier methods focused on a single truth direction and effectively tackles subtle, human-unknown truths in fact-checking. Remarkably, REFLEX achieves state-of-the-art performance with as few as 465 self-refined training samples. Additionally, models trained with explicit explanatory objectives can effectively improve other models without such training, resulting in up to a 7.57% performance boost, emphasizing the dual role of internal explanations in enhancing both interpretability and factual reasoning. <div>
arXiv:2511.20233v1 Announce Type: new 
Abstract: The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios</title>
<link>https://arxiv.org/abs/2511.20340</link>
<guid>https://arxiv.org/abs/2511.20340</guid>
<content:encoded><![CDATA[
<div> Speculative decoding, SpecFormer, LLM inference, unidirectional attention, bidirectional attention<br /><br />Summary: Speculative decoding accelerates large language model (LLM) inference by leveraging idle computational resources during data transfer but traditionally requires substantial computing power and large draft trees generated by smaller autoregressive models. Existing methods like batching are often preferred as they better compress idle computing power, highlighting the need for speculative decoding approaches that work efficiently with limited verification resources and low scheduling overhead. The authors propose SpecFormer, a novel architecture combining unidirectional and bidirectional attention mechanisms, merging the ability of autoregressive models to fully utilize input sequences with the parallel generation advantages of non-autoregressive models. This integration addresses the limitation of draft models which typically generate sequences of limited length and reduces reliance on large prefix trees. SpecFormer consistently accelerates LLM inference even in large-batch scenarios by enabling parallel draft sequence generation. Through experiments involving lossless speculative decoding across multiple model scales, SpecFormer demonstrates improved scaling of LLM inference with lower training demands and reduced computational costs, setting a new standard for efficient and scalable speculative decoding. <div>
arXiv:2511.20340v1 Announce Type: new 
Abstract: Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.20344</link>
<guid>https://arxiv.org/abs/2511.20344</guid>
<content:encoded><![CDATA[
<div> Keywords: analogical reasoning, large language models, relational concepts, structural alignment, cognitive modeling

<br /><br />Summary:  
This paper investigates the capability of large language models (LLMs) to perform analogical reasoning, a fundamental aspect of human cognition. First, it finds that LLMs can effectively encode underlying relationships between analogous entities, with both attributive and relational information being transmitted through mid to upper network layers in successful cases. When reasoning fails, it often reflects missing relational data in these layers. Second, unlike humans, LLMs struggle not only when relational information is absent but also when applying learned relational patterns to novel entities. The study demonstrates that selectively patching hidden representations at critical token positions can partially improve this transfer of information. Third, the paper highlights that successful analogical reasoning by LLMs correlates with strong structural alignment between the compared situations, whereas failure cases show degraded or incorrect alignments. Overall, the findings reveal that while LLMs exhibit emerging abilities to encode and utilize high-level relational concepts, their performance remains limited compared to human cognition. These results shed light on both parallels to and discrepancies from human reasoning, emphasizing the challenges in enabling LLMs to fully grasp and manipulate abstract relational structures. <div>
arXiv:2511.20344v1 Announce Type: new 
Abstract: Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali</title>
<link>https://arxiv.org/abs/2511.20399</link>
<guid>https://arxiv.org/abs/2511.20399</guid>
<content:encoded><![CDATA[
<div> Keywords: BengaliFig, low-resource language, figurative reasoning, cultural grounding, large language models  

<br /><br />Summary:  
This paper introduces BengaliFig, a novel dataset designed to evaluate large language models (LLMs) on figurative and culturally grounded reasoning in Bengali, a widely spoken but low-resource language. The dataset consists of 435 unique riddles sourced from Bengali oral and literary traditions, providing a culturally rich testing ground. Each riddle is annotated across five distinct dimensions: reasoning type, trap type, cultural depth, answer category, and difficulty, ensuring multifaceted analysis. To facilitate model testing, the dataset is automatically converted into multiple-choice format using a constraint-aware AI-assisted pipeline. The authors benchmark eight leading LLMs from different major providers, employing zero-shot and few-shot chain-of-thought prompting methods. The evaluation reveals persistent challenges faced by these models in handling metaphorical content and culturally specific reasoning tasks in Bengali. BengaliFig thus serves as both a diagnostic tool to probe LLM robustness in underrepresented cultural contexts and a meaningful step towards more inclusive and heritage-aware natural language processing evaluation frameworks. This work highlights the importance of integrating cultural and figurative understanding into NLP, especially for languages with limited resources and rich cultural backgrounds. <div>
arXiv:2511.20399v1 Announce Type: new 
Abstract: Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines</title>
<link>https://arxiv.org/abs/2511.20409</link>
<guid>https://arxiv.org/abs/2511.20409</guid>
<content:encoded><![CDATA[
<div> Keywords: Text normalization, Stemming evaluation, Stemming Effectiveness Score (SES), Average Normalized Levenshtein Distance (ANLD), Downstream task performance  

<br /><br />Summary:  
This study addresses the challenge of evaluating stemming methods, a critical step in text normalization for NLP. First, it identifies limitations in current evaluation metrics, which often overlook the negative effects of excessive stemming. Second, it proposes a novel, task-oriented evaluation framework incorporating three key components: Stemming Effectiveness Score (SES) to measure stemming utility, Model Performance Delta (MPD) to assess the impact on downstream tasks, and Average Normalized Levenshtein Distance (ANLD) to quantify semantic similarity between original and stemmed words. Third, the framework is applied to compare two stemmers: BNLTK for Bangla and Snowball for English. Fourth, results reveal that although the Bangla stemmer achieves a higher SES (1.67) due to effective word reduction (CR = 1.90), its high ANLD value (0.26) indicates harmful over-stemming associated with decreased downstream task performance. Fifth, the English stemmer shows a moderate SES (1.31) and a safer meaning distance (ANLD = 0.14), enabling positive contributions to downstream tasks and demonstrating greater reliability. Ultimately, the study offers a comprehensive tool to balance efficiency and meaning preservation in stemming evaluation, providing valuable insights for future NLP preprocessing techniques. <div>
arXiv:2511.20409v1 Announce Type: new 
Abstract: Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts</title>
<link>https://arxiv.org/abs/2511.20459</link>
<guid>https://arxiv.org/abs/2511.20459</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, stylometry, 19th-century novelists, generative text, explainable AI<br /><br />Summary:<br />1. This work addresses two key challenges in stylometry using large language models: generating text in specific authorial styles without paired data and evaluating stylistic authenticity beyond human judgment.<br />2. The authors fine-tune large language models with minimal, single-token prompts to imitate the writing styles of prominent 19th-century novelists such as Dickens, Austen, Twain, Alcott, and Melville.<br />3. To evaluate these generative models, they develop a transformer-based detector trained on authentic sentences, which functions both as a classifier to distinguish generated from genuine text and as a tool for stylistic explanation.<br />4. The study employs syntactic analyses along with explainable AI techniques—attention-based and gradient-based methods—to identify specific linguistic cues that contribute to accurate stylistic imitation.<br />5. Their findings demonstrate that the generated texts successfully capture distinctive authorial patterns and that AI-driven evaluation methods provide a reliable and scalable alternative to subjective human assessments. All related datasets, models, and tools are made publicly available to support further research in computational stylometry and text generation. <div>
arXiv:2511.20459v1 Announce Type: new 
Abstract: Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Confusion Attack: Disrupting Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.20494</link>
<guid>https://arxiv.org/abs/2511.20494</guid>
<content:encoded><![CDATA[
<div> Adversarial Confusion Attack, multimodal large language models, next-token entropy, PGD perturbations, model disruption  

<br /><br />Summary:  
This paper introduces the Adversarial Confusion Attack, a novel threat targeting multimodal large language models (MLLMs) designed to systematically disrupt their output coherence. Unlike traditional jailbreak or misclassification attacks, this method aims to cause models to produce incoherent or confidently incorrect responses. The attack works by maximizing next-token entropy using a small ensemble of open-source MLLMs, thus confusing model predictions. It is demonstrated that a single adversarial image can simultaneously compromise multiple models within the ensemble under white-box conditions. The attack is shown to be effective in both full-image and adversarial CAPTCHA formats. Utilizing a basic adversarial method, Projected Gradient Descent (PGD), the crafted image perturbations successfully transfer to unseen open-source models like Qwen3-VL and proprietary ones such as GPT-5.1, indicating strong transferability. A practical application example includes embedding these adversarial images within websites to undermine the reliability of MLLM-powered agents. Overall, the work highlights a new category of attacks focused on disrupting the functioning of multimodal AI systems through entropy-based adversarial input generation. <div>
arXiv:2511.20494v1 Announce Type: new 
Abstract: We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Applications include embedding adversarial images into websites to prevent MLLM-powered agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models</title>
<link>https://arxiv.org/abs/2511.20507</link>
<guid>https://arxiv.org/abs/2511.20507</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Aphasia, Text Aphasia Battery, Language Deficits, Automated Evaluation

<br /><br />Summary:  
This paper presents the Text Aphasia Battery (TAB), a novel benchmark designed to assess aphasia-like language deficits in large language models (LLMs). Traditional clinical aphasia tests are unsuitable for LLMs because they rely on human pragmatic contexts and cognitive processes absent in artificial systems. The TAB adapts the Quick Aphasia Battery (QAB) for text-only evaluation, comprising four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition, each targeting different linguistic functions. The authors describe the design rationale, detailed subtest formats, and scoring procedures to capture distinct aspects of language impairment. To enable scalable, consistent application, they develop and validate an automated evaluation protocol based on the Gemini 2.5 Flash model. This automated scoring demonstrates reliability close to expert human raters, with a prevalence-weighted Cohen's kappa of 0.255 for model-consensus agreement, compared to 0.286 for human-human agreement. By releasing TAB as a publicly available tool, the work provides a clinically grounded framework to systematically analyze and quantify language deficits in artificial neural architectures, facilitating future research on the computational underpinnings of linguistic disorders using LLMs as proxy systems. <div>
arXiv:2511.20507v1 Announce Type: new 
Abstract: Large language models (LLMs) have emerged as a candidate "model organism" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Language Gap: Synthetic Voice Diversity via Latent Mixup for Equitable Speech Recognition</title>
<link>https://arxiv.org/abs/2511.20534</link>
<guid>https://arxiv.org/abs/2511.20534</guid>
<content:encoded><![CDATA[
<div> Keywords: data augmentation, low-resource languages, automatic speech recognition, speech corpora, machine learning<br /><br />Summary:  
This paper addresses the performance disparity in automatic speech recognition (ASR) systems between well-resourced languages like English and low-resource languages, which suffer from limited training data. The authors introduce a novel data augmentation technique specifically tailored for speech corpora to help mitigate this gap. Their method involves generating additional training samples that are effective in enriching the dataset for languages with scarce resources. Comprehensive experiments validate that the proposed augmentation approach significantly boosts ASR performance on low-resource languages compared to baseline and existing augmentation methods. Beyond improving accuracy, the technique is designed to be practical and easily deployable in real-world scenarios, making it a valuable tool for language communities that lack extensive speech datasets. Ultimately, this work contributes to reducing inequities in speech technology by enabling better recognition capabilities for underrepresented languages, fostering inclusivity in the development of machine learning models for diverse linguistic populations. <div>
arXiv:2511.20534v1 Announce Type: new 
Abstract: Modern machine learning models for audio tasks often exhibit superior performance on English and other well-resourced languages, primarily due to the abundance of available training data. This disparity leads to an unfair performance gap for low-resource languages, where data collection is both challenging and costly. In this work, we introduce a novel data augmentation technique for speech corpora designed to mitigate this gap. Through comprehensive experiments, we demonstrate that our method significantly improves the performance of automatic speech recognition systems on low-resource languages. Furthermore, we show that our approach outperforms existing augmentation strategies, offering a practical solution for enhancing speech technology in underrepresented linguistic communities.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding</title>
<link>https://arxiv.org/abs/2511.20547</link>
<guid>https://arxiv.org/abs/2511.20547</guid>
<content:encoded><![CDATA[
<div> Keywords: educational dialogue, discourse features, student conversations, NLP, large language models  

<br /><br />Summary:  
This paper focuses on identifying discourse features in student conversations, which is crucial for educational researchers aiming to understand the factors that encourage students to engage in knowledge construction rather than just task completion. Traditionally, analyzing these student dialogues manually is labor-intensive, limiting the breadth of research in this area. To overcome this, the study leverages natural language processing (NLP) techniques for the automatic detection of discourse features, thereby providing scalable and data-driven insights for educational research. The authors note a lack of prior NLP studies that specifically address discourse analysis within educational data. To fill this gap, they introduce a newly annotated dataset of educational dialogues that captures discourse related to both knowledge construction and task production. Baseline predictive models are established using state-of-the-art large language models, specifically GPT-3.5 and Llama-3.1, to classify discourse features at the level of each conversational turn. Experimental results reveal that despite their advanced capabilities, these large language models perform suboptimally on this particular task, highlighting the complexity of discourse in educational settings and indicating ample opportunity for future research and model improvement in this area. <div>
arXiv:2511.20547v1 Announce Type: new 
Abstract: Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Evaluating LLM Alignment by Evaluating LLMs as Judges</title>
<link>https://arxiv.org/abs/2511.20604</link>
<guid>https://arxiv.org/abs/2511.20604</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM alignment, generation-evaluation consistency, human preferences, benchmarking, model evaluation  

<br /><br />Summary:  
This paper investigates how large language models (LLMs) align with human preferences by examining the link between their ability to generate responses and their ability to evaluate others’ outputs. First, the authors conduct a detailed study of generation-evaluation consistency (GE-consistency) across various LLMs, finding a strong correlation between models’ generative and evaluation capabilities when assessed using a high-quality LLM preference oracle. Building on this insight, they propose a novel benchmarking paradigm—AlignEval—that measures alignment indirectly by evaluating LLMs in the role of judges rather than assessing their generated outputs directly. AlignEval is shown to match or outperform existing automatic evaluation benchmarks like AlpacaEval and Arena-Hard in capturing human preferences when ranking different LLMs. This approach removes the dependency on human annotators or external evaluators for alignment assessment. The study highlights the important relationship between generation and evaluation skills within LLMs and introduces a new, efficient, and reliable benchmark for alignment without needing to score open-ended generated responses. This method may streamline future work in evaluating LLM safety, helpfulness, and honesty aligned with human values. <div>
arXiv:2511.20604v1 Announce Type: new 
Abstract: Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Collaboration in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.20639</link>
<guid>https://arxiv.org/abs/2511.20639</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent systems, large language models, latent collaboration, efficiency, reasoning quality  

<br /><br />Summary:  
This paper introduces LatentMAS, a novel framework that advances multi-agent systems (MAS) by enabling large language model (LLM) agents to collaborate directly within a continuous latent space rather than relying on traditional text-based communication. LatentMAS is training-free and facilitates auto-regressive generation of latent thoughts using last-layer hidden embeddings. The framework incorporates a shared latent working memory to preserve and transfer internal representations among agents, ensuring lossless and efficient information exchange. Theoretically, LatentMAS demonstrates greater expressiveness and lower complexity compared to vanilla text-based MAS. Empirical evaluations across nine diverse benchmarks—covering math and science reasoning, commonsense understanding, and code generation—show that LatentMAS outperforms both single-model baselines and text-based MAS alternatives. The proposed method achieves up to 14.6% higher accuracy, reduces output token usage by 70.8% to 83.7%, and accelerates end-to-end inference by 4x to 4.3x. These improvements highlight the framework’s ability to enhance system-level reasoning quality and efficiency simultaneously. Importantly, it accomplishes these gains without the need for additional training. The authors also provide fully open-sourced code and data to support further research and development in this area. <div>
arXiv:2511.20639v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlockCert: Certified Blockwise Extraction of Transformer Mechanisms</title>
<link>https://arxiv.org/abs/2511.17645</link>
<guid>https://arxiv.org/abs/2511.17645</guid>
<content:encoded><![CDATA[
<div> Keywords: mechanistic interpretability, model editing, certified extraction, transformers, Lipschitz composition theorem<br /><br />Summary:  
1. This paper presents BlockCert, a novel framework designed for certified blockwise extraction of transformer mechanisms, enabling explicit algorithmic understanding of neural network components.  
2. BlockCert also supports certified local edits, allowing model behaviors to be modified without retraining, accompanied by machine-checkable certificates that bound approximation errors and record coverage metrics.  
3. The authors formalize a Lipschitz-based composition theorem in Lean 4, allowing local guarantees of approximation error per block to be composed into global deviation bounds on model behavior.  
4. The framework is empirically validated on large language models including GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B, achieving high per-block coverage and low residual errors when tested on relevant prompt distributions.  
5. A fully stitched model derived from TinyLlama using BlockCert matches baseline perplexity within an extremely tight margin (~6e-5), demonstrating the practical feasibility of certified blockwise extraction for real transformer language models and bridging mechanistic interpretability with formal reasoning. <div>
arXiv:2511.17645v1 Announce Type: cross 
Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Modality Contributions via Disentangling Multimodal Representations</title>
<link>https://arxiv.org/abs/2511.19470</link>
<guid>https://arxiv.org/abs/2511.19470</guid>
<content:encoded><![CDATA[
<div> Modality Contribution, Multimodal Models, Partial Information Decomposition, Cross-Attention, Iterative Proportional Fitting Procedure  

<br /><br />Summary:  
1. The paper addresses the challenge of quantifying the contributions of different modalities in multimodal models, highlighting limitations in existing accuracy-based methods.  
2. It critiques prior approaches that interpret performance drops after removing a modality as indicative of its importance, noting they fail to distinguish whether a modality is uniquely informative or valuable only through interactions with others.  
3. The authors propose a new framework leveraging Partial Information Decomposition (PID) to decompose the predictive information of internal embeddings into unique, redundant, and synergistic parts, providing a nuanced understanding of modality roles.  
4. The framework is especially relevant for cross-attention architectures where modalities interact and influence each other’s learned representations.  
5. To make the analysis scalable and practical without retraining models, the paper introduces an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that can compute both layer-wise and dataset-level modality contributions during inference.  
6. This approach enables a principled, representation-level analysis of multimodal systems, offering clearer, interpretable insights into modality contributions beyond traditional outcome-based metrics. <div>
arXiv:2511.19470v1 Announce Type: cross 
Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studying Maps at Scale: A Digital Investigation of Cartography and the Evolution of Figuration</title>
<link>https://arxiv.org/abs/2511.19538</link>
<guid>https://arxiv.org/abs/2511.19538</guid>
<content:encoded><![CDATA[
<div> Keywords: cartographic heritage, map digitization, semantic segmentation, political dynamics, cartographic figuration<br /><br />Summary:<br /><br />This thesis develops methods and compiles extensive datasets to study cartographic heritage from a cultural viewpoint, leveraging over one million digitized maps worldwide. It integrates a diverse corpus of 771,561 map records and 99,715 digitized images from 38 digital catalogs, normalized to include 236,925 contributors spanning six centuries (1492–1948). The research charts geographic structures and the global timeline of map publication, revealing how the spatial focus of cartography correlates with political events such as Atlantic maritime charting, the triangular trade, colonial expansion, national priorities, and military conflicts. It introduces semantic segmentation and object detection models trained on annotated and synthetic data to recognize land classes and cartographic signs generically. The analysis uncovers that maps are deliberately designed images, with framing and symmetry emphasizing specific features. By encoding 63 million signs and 25 million fragments into a latent visual space, the study highlights figurative shifts in cartography—such as the transition from relief hachures to terrain contours—and finds that signs form regionally consistent semiotic systems. Finally, collaboration and diffusion analyses point to the influence of legitimacy, prominent actors, and major cities in disseminating figurative norms and semiotic cultures in cartography. <div>
arXiv:2511.19538v1 Announce Type: cross 
Abstract: This thesis presents methods and datasets to investigate cartographic heritage on a large scale and from a cultural perspective. Heritage institutions worldwide have digitized more than one million maps, and automated techniques now enable large-scale recognition and extraction of map content. Yet these methods have engaged little with the history of cartography, or the view that maps are semantic-symbolic systems, and cultural objects reflecting political and epistemic expectations. This work leverages a diverse corpus of 771,561 map records and 99,715 digitized images aggregated from 38 digital catalogs. After normalization, the dataset includes 236,925 contributors and spans six centuries, from 1492 to 1948. These data make it possible to chart geographic structures and the global chronology of map publication. The spatial focus of cartography is analyzed in relation to political dynamics, evidencing links between Atlantic maritime charting, the triangular trade, and colonial expansion. Further results document the progression of national, domestic focus and the impact of military conflicts on publication volumes. The research introduces semantic segmentation techniques and object detection models for the generic recognition of land classes and cartographic signs, trained on annotated data and synthetic images. The analysis of land classes shows that maps are designed images whose framing and composition emphasize features through centering and semantic symmetries. The study of cartographic figuration encodes 63 M signs and 25 M fragments into a latent visual space, revealing figurative shifts such as the replacement of relief hachures by terrain contours and showing that signs tend to form locally consistent systems. Analyses of collaboration and diffusion highlight the role of legitimacy, larger actors, and major cities in the spread of figurative norms and semiotic cultures.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fara-7B: An Efficient Agentic Model for Computer Use</title>
<link>https://arxiv.org/abs/2511.19663</link>
<guid>https://arxiv.org/abs/2511.19663</guid>
<content:encoded><![CDATA[
<div> Keywords: computer use agents, synthetic data generation, multi-step web tasks, Fara-7B model, WebTailBench<br /><br />Summary:<br /><br />1. The article addresses the challenge of limited large, high-quality datasets capturing human-computer interactions crucial for advancing computer use agents (CUAs).<br />2. It introduces FaraGen, a synthetic data generation system designed to create diverse, multi-step web task trajectories by proposing tasks from popular websites, generating solutions, and verifying success with multiple checkers.<br />3. FaraGen achieves high throughput, yield, and diversity, producing verified interaction trajectories efficiently at roughly $1 per trajectory.<br />4. The data generated by FaraGen is used to train Fara-7B, a compact, native computer use agent model that operates purely on screenshots and predicts actions via coordinates, making it suitable for on-device deployment.<br />5. Fara-7B outperforms other similar-sized CUA models on established benchmarks such as WebVoyager and Online-Mind2Web, and on WebTailBench, a new benchmark introduced in this work which focuses on under-represented web tasks.<br />6. The Fara-7B model also competes favorably with much larger, state-of-the-art models, demonstrating the impact of scalable synthetic data generation for developing small, efficient agentic models.<br />7. Both Fara-7B’s weights and the WebTailBench benchmark are publicly released via Microsoft Foundry and HuggingFace, enabling further research and application development. <div>
arXiv:2511.19663v1 Announce Type: cross 
Abstract: Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs</title>
<link>https://arxiv.org/abs/2511.19773</link>
<guid>https://arxiv.org/abs/2511.19773</guid>
<content:encoded><![CDATA[
<div> visual reasoning, vision-language models, VISTA-Gym, tool integration, reinforcement learning<br /><br />Summary:<br /><br />This paper addresses the challenge that current vision-language models (VLMs), despite strong image understanding, lack the capability to perform multi-step visual reasoning or "think with images." To tackle this, the authors introduce VISTA-Gym, a scalable training environment designed to enhance tool-integrated visual reasoning in VLMs. VISTA-Gym unifies seven diverse reasoning tasks from 13 different datasets, providing a standardized interface for interacting with visual tools such as grounding and parsing, along with executable interaction loops and verifiable feedback to support agentic reinforcement learning. The study reveals that while existing VLMs excel at text-only reasoning, they struggle with tasks involving selecting, invoking, and coordinating tools for visual reasoning. Using VISTA-Gym, the authors train a model named VISTA-R1, which learns to interleave tool use and agentic reasoning through multi-turn trajectory sampling and end-to-end reinforcement learning. Evaluation across 11 public, reasoning-intensive visual question answering (VQA) benchmarks demonstrates that VISTA-R1-8B surpasses state-of-the-art models of similar sizes by a margin of 9.51% to 18.72%. The results establish VISTA-Gym as an effective platform for developing advanced tool-integrated visual reasoning capabilities in vision-language models. <div>
arXiv:2511.19773v1 Announce Type: cross 
Abstract: While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization</title>
<link>https://arxiv.org/abs/2511.19811</link>
<guid>https://arxiv.org/abs/2511.19811</guid>
<content:encoded><![CDATA[
<div> Keywords: image diversity, text-to-image diffusion models, Token-Prompt embedding Space Optimization, generative diversity, mode collapse  

<br /><br />Summary:  
The paper addresses the challenge of low image diversity in text-to-image diffusion models, where models tend to generate repetitive outputs by collapsing into dominant modes of the learned distribution. This repetition reduces creative exploration and limits downstream applications. Previous methods such as noise resampling, prompt rewriting, or steering-based guidance have been insufficient, as they either fail to fully prevent mode collapse or introduce distortions that lower image quality. To overcome these issues, the authors propose Token-Prompt embedding Space Optimization (TPSO), a novel training-free and model-agnostic module. TPSO enhances diversity by introducing learnable parameters that explore underrepresented regions within the token embedding space, thus mitigating the generation's collapse toward strong modes. At the same time, TPSO uses a prompt-level space that enforces a global semantic constraint, helping to regulate distribution shifts and preserve image fidelity. Extensive experiments conducted on MS-COCO with three different diffusion model backbones demonstrate that TPSO significantly improves generative diversity, elevating baseline performance substantially (from 1.10 to 4.18 points) without compromising image quality. The code for TPSO will be released upon paper acceptance, facilitating further research and application development. <div>
arXiv:2511.19811v1 Announce Type: cross 
Abstract: Image diversity remains a fundamental challenge for text-to-image diffusion models. Low-diversity models tend to generate repetitive outputs, increasing sampling redundancy and hindering both creative exploration and downstream applications. A primary cause is that generation often collapses toward a strong mode in the learned distribution. Existing attempts to improve diversity, such as noise resampling, prompt rewriting, or steering-based guidance, often still collapse to dominant modes or introduce distortions that degrade image quality. In light of this, we propose Token-Prompt embedding Space Optimization (TPSO), a training-free and model-agnostic module. TPSO introduces learnable parameters to explore underrepresented regions of the token embedding space, reducing the tendency of the model to repeatedly generate samples from strong modes of the learned distribution. At the same time, the prompt-level space provides a global semantic constraint that regulates distribution shifts, preventing quality degradation while maintaining high fidelity. Extensive experiments on MS-COCO and three diffusion backbones show that TPSO significantly enhances generative diversity, improving baseline performance from 1.10 to 4.18 points, without sacrificing image quality. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception</title>
<link>https://arxiv.org/abs/2511.19820</link>
<guid>https://arxiv.org/abs/2511.19820</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, fine-grained image understanding, reinforcement learning, CropVLM, external zoom method<br /><br />Summary:<br /><br />1. Vision-Language Models (VLMs) face difficulties with tasks that require detailed image perception, such as scene-text recognition and document analysis, due to limitations in visual resolution and fragmentation.<br />2. CropVLM is proposed as an external, low-cost solution that enhances VLM performance by allowing dynamic zooming into relevant image regions, thereby improving the models' ability to capture fine image details.<br />3. The training of CropVLM utilizes reinforcement learning, notably without relying on human-labeled bounding boxes or resource-intensive synthetic evaluations, making the training process efficient and scalable.<br />4. Once trained, CropVLM can be integrated with both open-source and proprietary VLMs without needing to fine-tune or alter the underlying VLMs, preventing issues like catastrophic forgetting.<br />5. Empirical results demonstrate that CropVLM significantly improves performance on high-resolution image understanding benchmarks, particularly in out-of-domain settings where the target VLM struggles, showcasing its effectiveness and versatility. <div>
arXiv:2511.19820v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization</title>
<link>https://arxiv.org/abs/2511.19878</link>
<guid>https://arxiv.org/abs/2511.19878</guid>
<content:encoded><![CDATA[
<div> Vision-Language-Action, fine-tuning, pretrained models, proximity scheduling, generalization<br /><br />Summary:  
The paper addresses the challenge of fine-tuning Vision-Language-Action (VLA) models, which leverage pretrained Vision-Language Models (VLMs). Naive fine-tuning often disrupts these pretrained representations, leading to reduced generalization performance. Existing solutions like freezing modules or applying uniform regularization either overly restrict model adaptation or overlook the unique roles of different components within VLAs. The authors propose MAPS (Module-Wise Proximity Scheduling), a novel fine-tuning framework that schedules proximity constraints between the pretrained VLM and VLA components in a principled manner. Through comprehensive analysis, they discover an empirical order to relax these constraints that balances maintaining stability with allowing flexibility where needed—specifically keeping visual encoders close to their pretrained state while permitting more freedom in action-oriented language layers. Notably, MAPS introduces no extra parameters or require additional data and can be easily integrated into existing VLA architectures. Experimental results across various benchmarks (MiniVLA-VQ, OpenVLA-OFT, SimplerEnv, CALVIN, LIBERO), as well as real-world robotic tasks on the Franka Emika Panda platform, demonstrate consistent improvements in both in-distribution and out-of-distribution scenarios, with gains up to 30%. The study reveals that empirically guided proximity to pretrained VLMs is a simple but powerful strategy to preserve generalization during VLM-to-VLA transfer. <div>
arXiv:2511.19878v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding</title>
<link>https://arxiv.org/abs/2511.19923</link>
<guid>https://arxiv.org/abs/2511.19923</guid>
<content:encoded><![CDATA[
<div> counterfactual reasoning, vision language models, video understanding, CounterVQA, CFGPT  

<br /><br />Summary:  
1. Vision Language Models (VLMs) have advanced in video understanding tasks such as feature alignment, event reasoning, and instruction-following.  
2. However, their ability to perform counterfactual reasoning—inferring alternative outcomes under hypothetical conditions—remains underexplored and is crucial for deep video understanding.  
3. To evaluate this capability, the authors introduce CounterVQA, a benchmark that includes three difficulty levels designed to assess various aspects of counterfactual reasoning in videos.  
4. Evaluation of state-of-the-art models shows that while they perform reasonably well on simple counterfactual questions, their accuracy significantly drops on complex multi-hop causal reasoning tasks.  
5. To enhance VLMs' counterfactual reasoning, the authors propose CFGPT, a post-training method that distills counterfactual reasoning capabilities from language models to improve visual counterfactual reasoning, demonstrating consistent gains across all levels of the CounterVQA benchmark.  
6. The dataset and code for CounterVQA and CFGPT will be made publicly available to foster further research in this area. <div>
arXiv:2511.19923v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning</title>
<link>https://arxiv.org/abs/2511.19935</link>
<guid>https://arxiv.org/abs/2511.19935</guid>
<content:encoded><![CDATA[
<div> EfficientXpert, domain pruning, large language models, sparsity, adapter fine-tuning  

<br /><br />Summary: The paper addresses the challenge of deploying large language models (LLMs) in resource-constrained environments by introducing EfficientXpert, a lightweight domain-pruning framework. EfficientXpert combines a novel propagation-aware pruning criterion called the Foresight Mask with an efficient adapter-update algorithm termed Partial Brain Surgeon. This integration into the LoRA fine-tuning process enables a seamless one-step transformation of general pretrained models into sparse, domain-adapted experts optimized for specific domains such as health and law. The approach achieves up to 40% sparsity while maintaining up to 98% of the performance of dense models, outperforming current state-of-the-art pruning methods. Extensive analysis highlights that domain-specific structural shifts significantly reduce the effectiveness of conventional general pruning masks, emphasizing the importance of adaptive, domain-aware pruning strategies. Overall, EfficientXpert offers an effective solution for compressing LLMs for specialized tasks without sacrificing significant performance, facilitating broader deployment of domain-specialized LLMs in environments where computational resources are limited. <div>
arXiv:2511.19935v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation</title>
<link>https://arxiv.org/abs/2511.20100</link>
<guid>https://arxiv.org/abs/2511.20100</guid>
<content:encoded><![CDATA[
<div> Keywords: GPU kernel optimization, large language models, reinforcement learning, Macro Thinking Micro Coding, high-performance computing  

<br /><br />Summary:  
This paper addresses the challenge of developing high-performance GPU kernels, which are essential for AI and scientific computing but difficult due to the need for expert knowledge and limited portability. Current large language model (LLM)-based methods face two conflicting issues: correctness of generated code and efficiency in searching the vast optimization space. The authors propose a hierarchical framework called Macro Thinking Micro Coding (MTMC), inspired by human expert strategies, that separates optimization strategy from implementation details to enhance both efficiency and correctness. Macro Thinking uses reinforcement learning to guide lightweight LLMs in exploring and learning semantic optimization strategies aimed at maximizing hardware utilization. Micro Coding uses general-purpose LLMs to incrementally implement optimization steps suggested by Macro Thinking, thus avoiding the errors typical of full-kernel generation. Experimental results on popular benchmarks show MTMC’s superior performance, achieving near 100% and 70% accuracy at easier and intermediate levels on KernelBench—over 50% improvement compared to state-of-the-art LLMs—and speeding up generation by up to 7.3x compared to LLMs and 2.2x compared to expert-optimized kernels. On the more difficult TritonBench benchmark, MTMC attains up to 59.64% accuracy and a 34x speedup, demonstrating its practical value for efficient and correct GPU kernel generation. <div>
arXiv:2511.20100v1 Announce Type: cross 
Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs</title>
<link>https://arxiv.org/abs/2511.20104</link>
<guid>https://arxiv.org/abs/2511.20104</guid>
<content:encoded><![CDATA[
<div> Keywords: emergent misalignment, fine-tuning, open-weights models, code generation, JSON vulnerability<br /><br />Summary:<br /><br />1. Prior research identified "emergent misalignment," where fine-tuning models on narrow domains with misaligned data causes broader misalignment effects across tasks (Betley et al. 2025).<br />2. Among previously tested models, the Qwen-2.5 family showed relative resistance to this phenomenon, whereas proprietary GPT-4o was highly susceptible.<br />3. This study investigates whether current open-weights models exhibit similar resistance and evaluates misalignment robustness across nine models from the Gemma 3 and Qwen 3 families, ranging from 1 billion to 32 billion parameters.<br />4. They replicate emergent misalignment effects during fine-tuning on insecure code generation tasks, with misalignment rates increasing from 0.07% (base models) to 0.68%, considerably lower than GPT-4o’s 20% misalignment.<br />5. A significant format-dependent vulnerability was discovered: requiring JSON output nearly doubles misalignment rates compared to natural language prompts (0.96% vs. 0.42%), likely because structural constraints limit the model’s capacity to refuse unsafe outputs.<br />6. These findings confirm that emergent misalignment is a reproducible issue in modern open-weights models but occurs at substantially lower rates compared to proprietary systems. <div>
arXiv:2511.20104v1 Announce Type: cross 
Abstract: Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed "emergent misalignment" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.
  We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.
  We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits</title>
<link>https://arxiv.org/abs/2511.20273</link>
<guid>https://arxiv.org/abs/2511.20273</guid>
<content:encoded><![CDATA[
<div> Transformer, Interpretability, Singular Directions, MLP, Attention Heads<br /><br />Summary:<br /><br />This paper introduces a fine-grained approach to understanding transformer architectures by decomposing attention heads and multilayer perceptron (MLP) layers into orthogonal singular directions. Unlike previous methods that treat these components as indivisible units, this decomposition reveals multiple superposed and independent computations within a single head or MLP. The authors validate their approach on standard tasks such as Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), demonstrating that canonical functional heads, like the name mover, actually encode multiple overlapping subfunctions aligned with distinct singular directions. Additionally, nodes in the computational graph identified as circuit elements show strong activation along specific low-rank directions, indicating that meaningful computations are concentrated in compact subspaces. While some singular directions remain difficult to interpret fully, the findings suggest that transformer computations are more distributed, structured, and compositional than previously understood. This perspective not only sheds light on the internal workings of transformer models but also opens new possibilities for fine-grained mechanistic interpretability and a deeper comprehension of model internals. <div>
arXiv:2511.20273v1 Announce Type: cross 
Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry of Decision Making in Language Models</title>
<link>https://arxiv.org/abs/2511.20315</link>
<guid>https://arxiv.org/abs/2511.20315</guid>
<content:encoded><![CDATA[
<div> intrinsic dimension, large language models, geometry, decision-making, multiple-choice question answering<br /><br />Summary:<br /><br />1. This study investigates the internal decision-making dynamics of large language models (LLMs) by analyzing the geometry of their hidden representations through the concept of intrinsic dimension (ID).<br />2. The research focuses specifically on multiple-choice question answering (MCQA) tasks to understand how LLMs process and represent linguistic information relevant to decision-making.<br />3. A large-scale empirical analysis is conducted across 28 open-weight transformer models, using multiple methods to estimate the intrinsic dimension layer-by-layer.<br />4. Results reveal a consistent pattern in all models: initial layers operate on low-dimensional manifolds, middle layers increase the intrinsic dimension expanding the representational space, and final layers compress this space again, converging to focused, decision-relevant representations.<br />5. These findings suggest that LLMs implicitly learn to map inputs onto structured, low-dimensional manifolds tailored to specific tasks, offering new geometric insights into how generalization and reasoning abilities arise within language models. <div>
arXiv:2511.20315v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Adaptive Policy Optimization</title>
<link>https://arxiv.org/abs/2511.20347</link>
<guid>https://arxiv.org/abs/2511.20347</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, policy optimization, Mixture-of-Experts, Soft Adaptive Policy Optimization, large language models  

<br /><br />Summary:  
This paper addresses the challenge of unstable and high-variance policy optimization in reinforcement learning (RL) applied to large language models (LLMs), particularly in Mixture-of-Experts architectures. Existing group-based methods like GSPO and GRPO rely on hard clipping to stabilize updates, but this approach reduces learning effectiveness by either suppressing all gradients on sequences with off-policy tokens or applying rigid token-level clipping. The authors propose Soft Adaptive Policy Optimization (SAPO), which introduces a smooth, temperature-controlled gating mechanism that adaptively down-weights off-policy tokens rather than clipping them harshly. This soft gating maintains sequence coherence like GSPO but avoids brittle clipping bands, allowing selective attenuation of problematic tokens while preserving useful learning signals. Compared to GRPO, SAPO’s smooth scaling leads to more stable and informative updates at the token level. Empirical evaluations demonstrate that SAPO achieves improved training stability and higher Pass@1 accuracy on mathematical reasoning benchmarks under similar training budgets. Additionally, SAPO is applied to train the Qwen3-VL model series, showing consistent performance improvements across diverse tasks and model sizes. Overall, SAPO offers a more reliable, scalable, and effective RL optimization strategy for enhancing LLM reasoning capabilities. <div>
arXiv:2511.20347v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DesignPref: Capturing Personal Preferences in Visual Design Generation</title>
<link>https://arxiv.org/abs/2511.20513</link>
<guid>https://arxiv.org/abs/2511.20513</guid>
<content:encoded><![CDATA[
<div> Generative models, UI design, personalization, preference disagreement, DesignPref dataset<br /><br />Summary:<br /><br />1. The paper introduces DesignPref, a novel dataset encompassing 12,000 pairwise comparisons of UI design outputs, annotated by 20 professional designers with multi-level preference ratings. <br />2. It highlights the inherently subjective and personalized nature of visual design preferences, as evidenced by significant disagreement among trained designers (Krippendorff's alpha = 0.25 for binary preferences).<br />3. Natural language rationales collected reveal that these disagreements arise from individual differences in the perceived importance of various design aspects.<br />4. Traditional aggregation methods like majority voting are shown to inadequately reflect the diverse individual preferences of designers.<br />5. The study further explores personalization strategies for preference modeling, demonstrating that fine-tuning or integrating designer-specific annotations into retrieval-augmented generation (RAG) pipelines yields models that better predict individual preferences, even with substantially reduced training data.<br /><br />This work provides the first large-scale resource and empirical evidence underscoring the importance of personalizing visual design evaluation models to capture individual tastes, advancing future research in generative design personalization. <div>
arXiv:2511.20513v1 Announce Type: cross 
Abstract: Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward</title>
<link>https://arxiv.org/abs/2511.20561</link>
<guid>https://arxiv.org/abs/2511.20561</guid>
<content:encoded><![CDATA[
arXiv:2511.20561v1 Announce Type: cross 
Abstract: Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gram2Vec: An Interpretable Document Vectorizer</title>
<link>https://arxiv.org/abs/2406.12131</link>
<guid>https://arxiv.org/abs/2406.12131</guid>
<content:encoded><![CDATA[
arXiv:2406.12131v2 Announce Type: replace 
Abstract: We present Gram2Vec, a grammatical style embedding system that embeds documents into a higher dimensional space by extracting the normalized relative frequencies of grammatical features present in the text. Compared to neural approaches, Gram2Vec offers inherent interpretability based on how the feature vectors are generated. In this paper, we use authorship verification and AI detection as two applications to show how Gram2Vec can be used. For authorship verification, we use the features from Gram2Vec to explain why a pair of documents is by the same or by different authors. We also demonstrate how Gram2Vec features can be used to train a classifier for AI detection, outperforming machine learning models trained on a comparable set of Biber features.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedS$^3$: Towards Medical Slow Thinking with Self-Evolved Soft Dual-sided Process Supervision</title>
<link>https://arxiv.org/abs/2501.12051</link>
<guid>https://arxiv.org/abs/2501.12051</guid>
<content:encoded><![CDATA[
arXiv:2501.12051v4 Announce Type: replace 
Abstract: Medical language models face critical barriers to real-world clinical reasoning applications. However, mainstream efforts, which fall short in task coverage, lack fine-grained supervision for intermediate reasoning steps, and rely on proprietary systems, are still far from a versatile, credible and efficient language model for clinical reasoning usage. To this end, we propose MedS3, a self-evolving framework that imparts robust reasoning capabilities to small, deployable models. Starting with 8,000 curated instances sampled via a curriculum strategy across five medical domains and 16 datasets, we use a small base policy model to conduct Monte Carlo Tree Search (MCTS) for constructing rule-verifiable reasoning trajectories. Self-explored reasoning trajectories ranked by node values are used to bootstrap the policy model via reinforcement fine-tuning and preference learning. Moreover, we introduce a soft dual process reward model that incorporates value dynamics: steps that degrade node value are penalized, enabling fine-grained identification of reasoning errors even when the final answer is correct. Experiments on eleven benchmarks show that MedS3 outperforms the previous state-of-the-art medical model by +6.45 accuracy points and surpasses 32B-scale general-purpose reasoning models by +8.57 points. Additional empirical analysis further demonstrates that MedS3 achieves robust and faithful reasoning behavior.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Long Context Language Modeling</title>
<link>https://arxiv.org/abs/2503.17407</link>
<guid>https://arxiv.org/abs/2503.17407</guid>
<content:encoded><![CDATA[
arXiv:2503.17407v2 Announce Type: replace 
Abstract: Efficient processing of long contexts has been a persistent pursuit in Natural Language Processing. With the growing number of long documents, dialogues, and other textual data, it is important to develop Long Context Language Models (LCLMs) that can process and analyze extensive inputs in an effective and efficient way. In this paper, we present a comprehensive survey on recent advances in long-context modeling for large language models. Our survey is structured around three key aspects: how to obtain effective and efficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate and analyze LCLMs comprehensively. For the first aspect, we discuss data strategies, architectural designs, and workflow approaches oriented with long context processing. For the second aspect, we provide a detailed examination of the infrastructure required for LCLM training and inference. For the third aspect, we present evaluation paradigms for long-context comprehension and long-form generation, as well as behavioral analysis and mechanism interpretability of LCLMs. Beyond these three key aspects, we thoroughly explore the diverse application scenarios where existing LCLMs have been deployed and outline promising future development directions. This survey provides an up-to-date review of the literature on long-context LLMs, which we wish to serve as a valuable resource for both researchers and engineers. An associated GitHub repository collecting the latest papers and repos is available at: \href{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\color[RGB]{175,36,67}{LCLM-Horizon}}.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)</title>
<link>https://arxiv.org/abs/2504.03151</link>
<guid>https://arxiv.org/abs/2504.03151</guid>
<content:encoded><![CDATA[
arXiv:2504.03151v2 Announce Type: replace 
Abstract: Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation</title>
<link>https://arxiv.org/abs/2505.18685</link>
<guid>https://arxiv.org/abs/2505.18685</guid>
<content:encoded><![CDATA[
arXiv:2505.18685v2 Announce Type: replace 
Abstract: Infodemics and health misinformation have significant negative impact on individuals and society, exacerbating confusion and increasing hesitancy in adopting recommended health measures. Recent advancements in generative AI, capable of producing realistic, human like text and images, have significantly accelerated the spread and expanded the reach of health misinformation, resulting in an alarming surge in its dissemination. To combat the infodemics, most existing work has focused on developing misinformation datasets from social media and fact checking platforms, but has faced limitations in topical coverage, inclusion of AI generation, and accessibility of raw content. To address these issues, we present MM Health, a large scale multimodal misinformation dataset in the health domain consisting of 34,746 news article encompassing both textual and visual information. MM Health includes human-generated multimodal information (5,776 articles) and AI generated multimodal information (28,880 articles) from various SOTA generative AI models. Additionally, We benchmarked our dataset against three tasks (reliability checks, originality checks, and fine-grained AI detection) demonstrating that existing SOTA models struggle to accurately distinguish the reliability and origin of information. Our dataset aims to support the development of misinformation detection across various health scenarios, facilitating the detection of human and machine generated content at multimodal levels.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Simulatability of LLM Explanations for Generation Tasks</title>
<link>https://arxiv.org/abs/2505.21740</link>
<guid>https://arxiv.org/abs/2505.21740</guid>
<content:encoded><![CDATA[
arXiv:2505.21740v3 Announce Type: replace 
Abstract: LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. One approach for evaluating explanations is counterfactual simulatability, how well an explanation allows users to infer the model's output on related counterfactuals. Counterfactual simulatability has been previously studied for yes/no question answering tasks. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict LLM outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that the evaluation for counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.01341</link>
<guid>https://arxiv.org/abs/2506.01341</guid>
<content:encoded><![CDATA[
arXiv:2506.01341v2 Announce Type: replace 
Abstract: Despite impressive advances in large language models (LLMs), existing benchmarks often focus on single-turn or single-step tasks, failing to capture the kind of iterative reasoning required in real-world settings. To address this limitation, we introduce TurnBench, a novel benchmark that evaluates multi-turn, multi-step reasoning through an interactive code-breaking task inspired by the "Turing Machine Board Game." In each episode, a model must uncover hidden logical or arithmetic rules by making sequential guesses, receiving structured feedback, and integrating clues across multiple rounds. This dynamic setup requires models to reason over time, adapt based on past information, and maintain consistency across steps-capabilities underexplored in current benchmarks. TurnBench includes two modes: Classic, which tests standard reasoning, and Nightmare, which introduces increased complexity and requires robust inferential chains. To support fine-grained analysis, we provide ground-truth annotations for intermediate reasoning steps. Our evaluation of state-of-the-art LLMs reveals significant gaps: the best model achieves 84% accuracy in Classic mode, but performance drops to 18% in Nightmare mode. In contrast, human participants achieve 100% in both, underscoring the challenge TurnBench poses to current models. By incorporating feedback loops and hiding task rules, TurnBench reduces contamination risks and provides a rigorous testbed for diagnosing and advancing multi-step, multi-turn reasoning in LLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved LLM Agents for Financial Document Question Answering</title>
<link>https://arxiv.org/abs/2506.08726</link>
<guid>https://arxiv.org/abs/2506.08726</guid>
<content:encoded><![CDATA[
arXiv:2506.08726v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection</title>
<link>https://arxiv.org/abs/2506.19548</link>
<guid>https://arxiv.org/abs/2506.19548</guid>
<content:encoded><![CDATA[
arXiv:2506.19548v2 Announce Type: replace 
Abstract: Early detection of disease outbreaks is crucial to ensure timely intervention by the health authorities. Due to the challenges associated with traditional indicator-based surveillance, monitoring informal sources such as online media has become increasingly popular. However, owing to the number of online articles getting published everyday, manual screening of the articles is impractical. To address this, we propose Health Sentinel. It is a multi-stage information extraction pipeline that uses a combination of ML and non-ML methods to extract events-structured information concerning disease outbreaks or other unusual health events-from online articles. The extracted events are made available to the Media Scanning and Verification Cell (MSVC) at the National Centre for Disease Control (NCDC), Delhi for analysis, interpretation and further dissemination to local agencies for timely intervention. From April 2022 till date, Health Sentinel has processed over 300 million news articles and identified over 95,000 unique health events across India of which over 3,500 events were shortlisted by the public health experts at NCDC as potential outbreaks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAS: Simulated Attention Score</title>
<link>https://arxiv.org/abs/2507.07694</link>
<guid>https://arxiv.org/abs/2507.07694</guid>
<content:encoded><![CDATA[
arXiv:2507.07694v2 Announce Type: replace 
Abstract: The attention mechanism is a core component of the Transformer architecture. Various methods have been developed to compute attention scores, including multi-head attention (MHA), multi-query attention, group-query attention and so on. We further analyze the MHA and observe that its performance improves as the number of attention heads increases, provided the hidden size per head remains sufficiently large. Therefore, increasing both the head count and hidden size per head with minimal parameter overhead can lead to significant performance gains at a low cost. Motivated by this insight, we introduce Simulated Attention Score (SAS), which maintains a compact model size while simulating a larger number of attention heads and hidden feature dimension per head. This is achieved by projecting a low-dimensional head representation into a higher-dimensional space, effectively increasing attention capacity without increasing parameter count. Beyond the head representations, we further extend the simulation approach to feature dimension of the key and query embeddings, enhancing expressiveness by mimicking the behavior of a larger model while preserving the original model size. To control the parameter cost, we also propose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive experiments on a variety of datasets and tasks demonstrate the effectiveness of the proposed SAS method, achieving significant improvements over different attention variants.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaajMeter: A Framework for LaaJ Evaluation</title>
<link>https://arxiv.org/abs/2508.10161</link>
<guid>https://arxiv.org/abs/2508.10161</guid>
<content:encoded><![CDATA[
arXiv:2508.10161v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). The analysis of a LaaJ software, commonly refereed to as meta-evaluation, pose significant challenges in domain-specific contexts. In such domains, in contrast to general domains, annotated data is scarce and expert evaluation is costly. As a result, meta-evaluation is often performed using metrics that have not been validated for the specific domain in which they are applied. Therefore, it becomes difficult to determine which metrics effectively identify LaaJ quality, and further, what threshold indicates sufficient evaluator performance. In this work, we introduce LaaJMeter, a simulation-based framework for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to generate synthetic data representing virtual models and judges, allowing systematic analysis of evaluation metrics under realistic conditions. This helps practitioners validate LaaJs for specific tasks: they can test whether their metrics correctly distinguish between high and low quality (virtual) LaaJs, and estimate appropriate thresholds for evaluator adequacy. We demonstrate the utility of LaaJMeter in a code translation task involving a legacy programming language, showing how different metrics vary in sensitivity to evaluator quality. Our results highlight the limitations of common metrics and the importance of principled metric selection. LaaJMeter provides a scalable and extensible solution for assessing LaaJs in low-resource settings, contributing to the broader effort to ensure trustworthy and reproducible evaluation in NLP.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConfTuner: Training Large Language Models to Express Their Confidence Verbally</title>
<link>https://arxiv.org/abs/2508.18847</link>
<guid>https://arxiv.org/abs/2508.18847</guid>
<content:encoded><![CDATA[
arXiv:2508.18847v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as "I am 80% confident that...". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it "correctly incentivizes the model to report its true probability of being correct". ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems. The code is available at https://github.com/liushiliushi/ConfTuner.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions</title>
<link>https://arxiv.org/abs/2509.17177</link>
<guid>https://arxiv.org/abs/2509.17177</guid>
<content:encoded><![CDATA[
arXiv:2509.17177v3 Announce Type: replace 
Abstract: We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning models (LRMs) with some preliminary findings. We also release ROME, our evaluation benchmark for vision language models intended to test reasoning from visual clues. We attach links to the benchmark, evaluation data, and other updates on this website: https://flageval-baai.github.io/LRM-Eval/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling</title>
<link>https://arxiv.org/abs/2509.24403</link>
<guid>https://arxiv.org/abs/2509.24403</guid>
<content:encoded><![CDATA[
arXiv:2509.24403v4 Announce Type: replace 
Abstract: State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind human experts on challenging benchmarks like BIRD. Current approaches that explore test-time scaling lack an orchestrated strategy and neglect the model's internal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL, a novel framework leveraging scalable computation to improve performance. Agentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that synergistically combines three distinct perspectives: i) Internal Scaling via RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament Selection. Agentar-Scale-SQL is a general-purpose framework designed for easy adaptation to new databases and more powerful language models. Extensive experiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD benchmark, reaching 81.67% execution accuracy on the test set and ranking first on the official leaderboard, demonstrating an effective path toward human-level performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanGym: A Benchmark Environment for Underwater Embodied Agents</title>
<link>https://arxiv.org/abs/2509.26536</link>
<guid>https://arxiv.org/abs/2509.26536</guid>
<content:encoded><![CDATA[
arXiv:2509.26536v2 Announce Type: replace 
Abstract: We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation</title>
<link>https://arxiv.org/abs/2510.05138</link>
<guid>https://arxiv.org/abs/2510.05138</guid>
<content:encoded><![CDATA[
arXiv:2510.05138v2 Announce Type: replace 
Abstract: The rapid growth of scientific publications has made it increasingly difficult to keep literature reviews comprehensive and up-to-date. Though prior work has focused on automating retrieval and screening, the writing phase of systematic reviews remains largely under-explored, especially with regard to readability and factual accuracy. To address this, we present LiRA (Literature Review Agents), a multi-agent collaborative workflow which emulates the human literature review process. LiRA utilizes specialized agents for content outlining, subsection writing, editing, and reviewing, producing cohesive and comprehensive review articles. Evaluated on SciReviewGen and a proprietary ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey and MASS-Survey in writing and citation quality, while maintaining competitive similarity to human-written reviews. We further evaluate LiRA in real-world scenarios using document retrieval and assess its robustness to reviewer model variation. Our findings highlight the potential of agentic LLM workflows, even without domain-specific tuning, to improve the reliability and usability of automated scientific writing.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightMem: Lightweight and Efficient Memory-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.18866</link>
<guid>https://arxiv.org/abs/2510.18866</guid>
<content:encoded><![CDATA[
arXiv:2510.18866v2 Announce Type: replace 
Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. On LongMemEval and LoCoMo, using GPT and Qwen backbones, LightMem consistently surpasses strong baselines, improving QA accuracy by up to 7.7% / 29.3%, reducing total token usage by up to 38x / 20.9x and API calls by up to 30x / 55.5x, while purely online test-time costs are even lower, achieving up to 106x / 117x token reduction and 159x / 310x fewer API calls. The code is available at https://github.com/zjunlp/LightMem.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training</title>
<link>https://arxiv.org/abs/2510.20059</link>
<guid>https://arxiv.org/abs/2510.20059</guid>
<content:encoded><![CDATA[
arXiv:2510.20059v3 Announce Type: replace 
Abstract: Enhancing reasoning capabilities in small language models is critical for specialized applications such as medical question answering, particularly in underrepresented languages like Persian. In this study, we employ Reinforcement Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to improve the reasoning skills of a general-purpose Persian language model. To achieve this, we translated a multiple-choice medical question-answering dataset into Persian and used RLAIF to generate rejected-preferred answer pairs, which are essential for DPO training. By prompting both teacher and student models to produce Chain-of-Thought (CoT) reasoning responses, we compiled a dataset containing correct and incorrect reasoning trajectories. This dataset, comprising 2 million tokens in preferred answers and 2.5 million tokens in rejected ones, was used to train a baseline model, significantly enhancing its medical reasoning capabilities in Persian. Remarkably, the resulting model outperformed its predecessor, gaokerena-V, which was trained on approximately 57 million tokens, despite leveraging a much smaller dataset. These results highlight the efficiency and effectiveness of reasoning-focused training approaches in developing domain-specific language models with limited data availability.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis</title>
<link>https://arxiv.org/abs/2510.25628</link>
<guid>https://arxiv.org/abs/2510.25628</guid>
<content:encoded><![CDATA[
arXiv:2510.25628v2 Announce Type: replace 
Abstract: Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Turing Test Reveals Systematic Differences Between Human and AI Language</title>
<link>https://arxiv.org/abs/2511.04195</link>
<guid>https://arxiv.org/abs/2511.04195</guid>
<content:encoded><![CDATA[
arXiv:2511.04195v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used in the social sciences to simulate human behavior, based on the assumption that they can generate realistic, human-like text. Yet this assumption remains largely untested. Existing validation efforts rely heavily on human-judgment-based evaluations -- testing whether humans can distinguish AI from human output -- despite evidence that such judgments are blunt and unreliable. As a result, the field lacks robust tools for assessing the realism of LLM-generated text or for calibrating models to real-world data. This paper makes two contributions. First, we introduce a computational Turing test: a validation framework that integrates aggregate metrics (BERT-based detectability and semantic similarity) with interpretable linguistic features (stylistic markers and topical patterns) to assess how closely LLMs approximate human language within a given dataset. Second, we systematically compare nine open-weight LLMs across five calibration strategies -- including fine-tuning, stylistic prompting, and context retrieval -- benchmarking their ability to reproduce user interactions on X (formerly Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the literature. Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models underperform their base counterparts, and scaling up model size does not enhance human-likeness. Crucially, we identify a trade-off: optimizing for human-likeness often comes at the cost of semantic fidelity, and vice versa. These results provide a much-needed scalable framework for validation and calibration in LLM simulations -- and offer a cautionary note about their current limitations in capturing human communication.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Attention Spans: Optimizing LLM Inference Efficiency with Heterogeneous Sliding-Window Lengths</title>
<link>https://arxiv.org/abs/2406.14909</link>
<guid>https://arxiv.org/abs/2406.14909</guid>
<content:encoded><![CDATA[
arXiv:2406.14909v3 Announce Type: replace-cross 
Abstract: Sliding-window attention offers a hardware-efficient solution to the memory and throughput challenges of Large Language Models (LLMs) in long-context scenarios. Existing methods typically employ a single window length across all attention heads and input sizes. However, this uniform approach fails to capture the heterogeneous attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose *Mixture of Attention Spans* (MoA), which automatically tailors distinct sliding-window length configurations to different heads and layers. MoA constructs and navigates a search space of various window lengths and their scaling rules relative to input sizes. It profiles the model, evaluates potential configurations, and pinpoints the optimal length configurations for each head. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer inputs, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by 3.9x with the same average sliding-window length, boosting retrieval accuracy by 1.5-7.1x over the uniform-window baseline across Vicuna-{7B, 13B} and Llama3-{8B, 70B} models. Moreover, MoA narrows the performance gap with full attention, reducing the maximum relative performance drop from 9%-36% to within 5% across three long-context understanding benchmarks. MoA achieves a 1.2-1.4x GPU memory reduction, boosting decode throughput by 6.6-8.2x and 1.7-1.9x over FlashAttention2 and vLLM, with minimal performance impact. Our code is available at: https://github.com/thu-nics/MoA
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Data Exploration via Language Agents</title>
<link>https://arxiv.org/abs/2412.18428</link>
<guid>https://arxiv.org/abs/2412.18428</guid>
<content:encoded><![CDATA[
arXiv:2412.18428v2 Announce Type: replace-cross 
Abstract: International enterprises, organizations, and hospitals collect large amounts of multi-modal data stored in databases, text documents, images, and videos. While there has been recent progress in the separate fields of multi-modal data exploration as well as in database systems that automatically translate natural language questions to database query languages, the research challenge of querying both structured databases and unstructured modalities (e.g., texts, images) in natural language remains largely unexplored. In this paper, we propose M$^2$EX -a system that enables multi-modal data exploration via language agents. Our approach is based on the following research contributions: (1) Our system is inspired by a real-world use case that enables users to explore multi-modal information systems. (2) M$^2$EX leverages an LLM-based agentic AI framework to decompose a natural language question into subtasks such as text-to-SQL generation and image analysis and to orchestrate modality-specific experts in an efficient query plan. (3) Experimental results on multi-modal datasets, encompassing relational data, text, and images, demonstrate that our system outperforms state-of-the-art multi-modal exploration systems, excelling in both accuracy and various performance metrics, including query latency, API costs, and planning efficiency, thanks to the more effective utilization of the reasoning capabilities of LLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The magnitude of categories of texts enriched by language models</title>
<link>https://arxiv.org/abs/2501.06662</link>
<guid>https://arxiv.org/abs/2501.06662</guid>
<content:encoded><![CDATA[
arXiv:2501.06662v2 Announce Type: replace-cross 
Abstract: The purpose of this article is twofold. Firstly, we use the next-token probabilities given by a language model to explicitly define a category of texts in natural language enriched over the unit interval, in the sense of Bradley, Terilla, and Vlassopoulos. We consider explicitly the terminating conditions for text generation and determine when the enrichment itself can be interpreted as a probability over texts. Secondly, we compute the M\"obius function and the magnitude of an associated generalized metric space of texts. The magnitude function of that space is a sum over texts (prompts) of the $t$-logarithmic (Tsallis) entropies of the next-token probability distributions associated with each prompt, plus the cardinality of the model's possible outputs. A suitable evaluation of the magnitude function's derivative recovers a sum of Shannon entropies, which justifies seeing magnitude as a partition function. Following Leinster and Shulman, we also express the magnitude function of the generalized metric space as an Euler characteristic of magnitude homology and provide an explicit description of the zeroeth and first magnitude homology groups.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNS-Obsidian: A Neurosurgical Vision-Language Model Built From Scientific Publications</title>
<link>https://arxiv.org/abs/2502.19546</link>
<guid>https://arxiv.org/abs/2502.19546</guid>
<content:encoded><![CDATA[
arXiv:2502.19546v5 Announce Type: replace-cross 
Abstract: General-purpose VLMs demonstrate impressive capabilities, but their opaque training on uncurated internet data poses critical limitations for high-stakes decision-making, such as in neurosurgery. We present CNS-Obsidian, a neurosurgical VLM trained on peer-reviewed literature, and demonstrate its clinical utility versus GPT-4o in a real-world setting. We compiled 23,984 articles from Neurosurgery Publications journals, yielding 78,853 figures and captions. Using GPT-4o and Claude Sonnet-3.5, we converted these into 263,064 training samples across three formats: instruction fine-tuning, multiple-choice questions, and differential diagnosis. We trained CNS-Obsidian, a fine-tune of the 34-billion parameter LLaVA-Next model. In a blinded, randomized trial at NYU Langone Health (Aug 30-Nov 30, 2024), neurosurgery consultations were assigned to either CNS-Obsidian or a HIPAA-compliant GPT-4o endpoint as diagnostic co-pilot after consultations. Primary outcomes were diagnostic helpfulness and accuracy, assessed via user ratings and presence of correct diagnosis within the VLM-provided differential. CNS-Obsidian matched GPT-4o on synthetic questions (76.13% vs 77.54%, p=0.235), but only achieved 46.81% accuracy on human-generated questions versus GPT-4o's 65.70% (p<10-15). In the randomized trial, 70 consultations were evaluated (32 CNS-Obsidian, 38 GPT-4o) from 959 total consults (7.3% utilization). CNS-Obsidian received positive ratings in 40.62% of cases versus 57.89% for GPT-4o (p=0.230). Both models included correct diagnosis in approximately 60% of cases (59.38% vs 65.79%, p=0.626). Domain-specific VLMs trained on curated scientific literature can approach frontier model performance despite being orders of magnitude smaller and less expensive to train. This establishes a transparent framework for scientific communities to build specialized AI models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExDDV: A New Dataset for Explainable Deepfake Detection in Video</title>
<link>https://arxiv.org/abs/2503.14421</link>
<guid>https://arxiv.org/abs/2503.14421</guid>
<content:encoded><![CDATA[
arXiv:2503.14421v2 Announce Type: replace-cross 
Abstract: The ever growing realism and quality of generated videos makes it increasingly harder for humans to spot deepfake content, who need to rely more and more on automatic deepfake detectors. However, deepfake detectors are also prone to errors, and their decisions are not explainable, leaving humans vulnerable to deepfake-based fraud and misinformation. To this end, we introduce ExDDV, the first dataset and benchmark for Explainable Deepfake Detection in Video. ExDDV comprises around 5.4K real and deepfake videos that are manually annotated with text descriptions (to explain the artifacts) and clicks (to point out the artifacts). We evaluate a number of vision-language models on ExDDV, performing experiments with various fine-tuning and in-context learning strategies. Our results show that text and click supervision are both required to develop robust explainable models for deepfake videos, which are able to localize and describe the observed artifacts. Our novel dataset and code to reproduce the results are available at https://github.com/vladhondru25/ExDDV.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2506.05587</link>
<guid>https://arxiv.org/abs/2506.05587</guid>
<content:encoded><![CDATA[
arXiv:2506.05587v3 Announce Type: replace-cross 
Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\% and 57\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.
  Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge</title>
<link>https://arxiv.org/abs/2508.00901</link>
<guid>https://arxiv.org/abs/2508.00901</guid>
<content:encoded><![CDATA[
arXiv:2508.00901v3 Announce Type: replace-cross 
Abstract: Modern large language models (LLMs) demonstrate exceptional performance on knowledge-intensive tasks, yet the theoretical mechanisms underlying knowledge acquisition (storage and memorization) during pre-training and extraction (retrieval and recall) during inference after fine-tuning remain poorly understood. Although prior theoretical studies have explored these processes through analyses of training dynamics, they overlook critical components essential for a comprehensive theory: (1) the multi-layer perceptron (MLP), empirically identified as the primary module for knowledge storage; (2) out-of-distribution (OOD) adaptivity, which enables LLMs to generalize to unseen scenarios post-pre-training; and (3) next-token prediction, the standard autoregressive objective that encodes knowledge as conditional probabilities. In this work, we introduce, to the best of our knowledge, the first theoretical framework that addresses these limitations by examining the training dynamics of one-layer transformers. Under regularity assumptions, we establish that: (i) transformers attain near-optimal training loss during pre-training, demonstrating effective knowledge acquisition; (ii) given a sufficiently large fine-tuning dataset and appropriate data multiplicity conditions, transformers achieve low generalization error on factual knowledge acquired during pre-training but not revisited in fine-tuning, indicating robust knowledge extraction; and (iii) violation of these conditions leads to elevated generalization error, manifesting as hallucinations. Our analysis encompasses both full fine-tuning and low-rank fine-tuning, yielding insights into the efficacy of practical low-rank adaptation methods. We validate our theoretical findings through experiments on synthetic datasets and the real-world PopQA benchmark, employing GPT-2 and Llama-3.2-1B models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows</title>
<link>https://arxiv.org/abs/2509.20490</link>
<guid>https://arxiv.org/abs/2509.20490</guid>
<content:encoded><![CDATA[
arXiv:2509.20490v2 Announce Type: replace-cross 
Abstract: Agentic systems offer a potential path to solve complex clinical tasks through collaboration among specialized agents, augmented by tool use and external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation, prevailing methods remain limited: (i) reasoning is frequently neither clinically interpretable nor aligned with guidelines, reflecting mere aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused, yielding text-only rationales that are not visually grounded; and (iii) systems rarely detect or resolve cross-tool inconsistencies and provide no principled verification mechanisms. To bridge the above gaps, we present RadAgents, a multi-agent framework that couples clinical priors with task-aware multimodal reasoning and encodes a radiologist-style workflow into a modular, auditable pipeline. In addition, we integrate grounding and multimodal retrieval-augmentation to verify and resolve context conflicts, resulting in outputs that are more reliable, transparent, and consistent with clinical practice.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent's own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL, where a replay buffer stores good experience for off-policy update, by gradually steering the policy entropy across stages. Specifically, the proposed curriculum scheduling harmonizes intrinsic reward shaping and self-imitation to 1) expedite exploration via frequent tool interactions at the beginning, and 2) strengthen exploitation of successful tactics upon convergence towards familiarity with the environment. We also combine bag-of-tricks of industrial RL optimizations for a strong baseline Dr.BoT to demonstrate our effectiveness. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%-25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction</title>
<link>https://arxiv.org/abs/2510.15691</link>
<guid>https://arxiv.org/abs/2510.15691</guid>
<content:encoded><![CDATA[
arXiv:2510.15691v3 Announce Type: replace-cross 
Abstract: In quantitative investing, return prediction supports various tasks, including stock selection, portfolio optimization, and risk management. Quantitative factors, such as valuation, quality, and growth, capture various characteristics of stocks. Unstructured data, like news and transcripts, has attracted growing attention, driven by recent advances in large language models (LLMs). This paper examines effective methods for leveraging multimodal factors and newsflow in return prediction and stock selection. First, we introduce a fusion learning framework to learn a unified representation from factors and newsflow representations generated by an LLM. Within this framework, we compare three methods of different architectural complexities: representation combination, representation summation, and attentive representations. Next, building on the limitation of fusion learning observed in empirical comparison, we explore the mixture model that adaptively combines predictions made by single modalities and their fusion. To mitigate the training instability of the mixture model, we introduce a decoupled training approach with theoretical insights. Finally, our experiments on real investment universes yield several insights into effective multimodal modeling of factors and news for stock return prediction and selection.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</title>
<link>https://arxiv.org/abs/2510.19654</link>
<guid>https://arxiv.org/abs/2510.19654</guid>
<content:encoded><![CDATA[
arXiv:2510.19654v2 Announce Type: replace-cross 
Abstract: Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning. While recent efforts aim to unify world modeling and planning in a single framework, the synergistic facilitation mechanism of world modeling for planning still requires further exploration. In this work, we introduce a new driving paradigm named Policy World Model (PWM), which not only integrates world modeling and trajectory planning within a unified architecture, but is also able to benefit planning using the learned world knowledge through the proposed action-free future state forecasting scheme. Through collaborative state-action prediction, PWM can mimic the human-like anticipatory perception, yielding more reliable planning performance. To facilitate the efficiency of video forecasting, we further introduce a dynamically enhanced parallel token generation mechanism, equipped with a context-guided tokenizer and an adaptive dynamic focal loss. Despite utilizing only front camera input, our method matches or exceeds state-of-the-art approaches that rely on multi-view and multi-modal inputs. Code and model weights will be released at https://github.com/6550Zhao/Policy-World-Model.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Mediated Communication Reshapes Social Structure in Opinion-Diverse Groups</title>
<link>https://arxiv.org/abs/2510.21984</link>
<guid>https://arxiv.org/abs/2510.21984</guid>
<content:encoded><![CDATA[
arXiv:2510.21984v2 Announce Type: replace-cross 
Abstract: Group segregation or cohesion can emerge from micro-level communication, and AI-assisted messaging may shape this process. Here, we report a preregistered online experiment (N = 557 across 60 sessions) in which participants discussed controversial political topics over multiple rounds and could freely change groups. Some participants received real-time message suggestions from a large language model (LLM), either personalized to their stance (individual assistance) or incorporating their group members' perspectives (relational assistance). We find that small variations in AI-mediated communication cascade into macro-level differences in group composition. Participants with individual assistance send more messages and show greater stance-based clustering, whereas those with relational assistance use more receptive language and form more heterogeneous ties. Hybrid expressive processes-jointly produced by humans and AI-can reshape collective organization. The patterns of structural division and cohesion depend on how AI incorporates users' interaction context.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models replicate and predict human cooperation across experiments in game theory</title>
<link>https://arxiv.org/abs/2511.04500</link>
<guid>https://arxiv.org/abs/2511.04500</guid>
<content:encoded><![CDATA[
arXiv:2511.04500v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used both to make decisions in domains such as health, education and law, and to simulate human behavior. Yet how closely LLMs mirror actual human decision-making remains poorly understood. This gap is critical: misalignment could produce harmful outcomes in practical applications, while failure to replicate human behavior renders LLMs ineffective for social simulations. Here, we address this gap by developing a digital twin of game-theoretic experiments and introducing a systematic prompting and probing framework for machine-behavioral evaluation. Testing three open-source models (Llama, Mistral and Qwen), we find that Llama reproduces human cooperation patterns with high fidelity, capturing human deviations from rational choice theory, while Qwen aligns closely with Nash equilibrium predictions. Notably, we achieved population-level behavioral replication without persona-based prompting, simplifying the simulation process. Extending beyond the original human-tested games, we generate and preregister testable hypotheses for novel game configurations outside the original parameter grid. Our findings demonstrate that appropriately calibrated LLMs can replicate aggregate human behavioral patterns and enable systematic exploration of unexplored experimental spaces, offering a complementary approach to traditional research in the social and behavioral sciences that generates new empirical predictions about human social decision-making.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Honest Language Models for Deductive Reasoning</title>
<link>https://arxiv.org/abs/2511.09222</link>
<guid>https://arxiv.org/abs/2511.09222</guid>
<content:encoded><![CDATA[
<div> Keywords: deductive reasoning, honesty, reinforcement learning, graph datasets, GRPO<br /><br />Summary: Deductive reasoning involves deriving conclusions strictly from given premises without external information. In this work, honesty is defined as a model’s ability to respond only when a conclusion is logically entailed and to abstain otherwise. Existing language models often fail to uphold honesty, producing unwarranted answers when premises are insufficient. To investigate this problem, the authors formulate honest deductive reasoning as multi-step tasks requiring either correct conclusion derivation or abstention. They curate two novel datasets based on graph structures: one focused on linear algebra and another on logical inference. These datasets include unanswerable instances created by perturbing edges to simulate insufficient information. Experimental results reveal that current prompting strategies and training methods, such as GRPO (with or without supervised fine-tuning), struggle on these tasks, especially due to GRPO’s optimization of final outcomes alone, which can lead to training collapse under conditions of dominant negative rewards. To counter this, the study introduces a new reinforcement learning approach, \methodname{}, which incorporates ground truth trajectories during rollouts to stabilize training and prevent early collapse. Empirical results demonstrate that \methodname{} significantly enhances learning stability and overall reasoning performance, highlighting the critical role of training dynamics in enabling honest deductive reasoning in language models. <div>
arXiv:2511.09222v2 Announce Type: replace 
Abstract: Deductive reasoning is the process of deriving conclusions strictly from the given premises, without relying on external knowledge. We define honesty in this setting as a model's ability to respond only when the conclusion is logically entailed by the premises, and to abstain otherwise. However, current language models often fail to reason honestly, producing unwarranted answers when the input is insufficient. To study this challenge, we formulate honest deductive reasoning as multi-step tasks where models must either derive the correct conclusion or abstain. We curate two datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that prompting and existing training methods, including GRPO with or without supervised fine-tuning initialization, struggle on these tasks. In particular, GRPO optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. To address this, we propose \methodname{}, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling honest deductive reasoning in language models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering</title>
<link>https://arxiv.org/abs/2511.17559</link>
<guid>https://arxiv.org/abs/2511.17559</guid>
<content:encoded><![CDATA[
<div> text-to-SQL, EHR QA, post-hoc verification, SQL error correction, safety benchmark  

<br /><br />Summary: Recent Large Language Models (LLMs) have facilitated the creation of text-to-SQL systems enabling clinicians to query Electronic Health Records (EHRs) using natural language. However, safely deploying these models in clinical environments remains challenging due to risks from incorrect SQL queries generated by model errors or ambiguous inputs, which can compromise patient care. Existing solutions mainly focus on improving SQL accuracy or pre-execution question filtering but lack a unified framework for evaluating independent post-hoc verification mechanisms that validate SQL queries before execution. To address this, the paper introduces SCARE, a benchmark designed to evaluate post-hoc safety layers in EHR QA systems. SCARE tackles two tasks jointly: classifying question answerability (answerable, ambiguous, or unanswerable) and verifying or correcting candidate SQL queries. The benchmark consists of 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in real clinical databases (MIMIC-III, MIMIC-IV, eICU). It incorporates outputs from seven different text-to-SQL models for a thorough and realistic evaluation. Experiments using SCARE demonstrate a critical trade-off between accurately classifying questions and correcting SQL errors, highlighting major challenges and guiding future research directions to enhance safety in clinical text-to-SQL systems. <div>
arXiv:2511.17559v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving</title>
<link>https://arxiv.org/abs/2511.17560</link>
<guid>https://arxiv.org/abs/2511.17560</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, KV Cache reuse, recomputation, Attention-Aware Accurate KV Cache Fusion, decoding latency<br /><br />Summary:<br />1. Large language models (LLMs) can handle long-context tasks such as multi-turn conversations, legal documents, and Retrieval-Augmented Generation (RAG) systems, but decoding latency and memory overhead remain high.<br />2. KV Cache reuse methods have been introduced to reduce computational costs, but current approaches suffer from performance degradation.<br />3. The study finds that recomputation-based reuse often misaligns recomputed tokens with relevant context segments, impairing the update of crucial contextual representations.<br />4. To solve this, the paper proposes the Attention-Aware Accurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses KV Cache of text chunks based on their relevance to the question.<br />5. Experiments on several benchmarks and LLMs show that $A^3$ outperforms four baseline methods in task performance while reducing time-to-first-token (TTFT) by a factor of two, achieving faster and more efficient long-context processing. <div>
arXiv:2511.17560v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\textbf{A}$ttention-$\textbf{A}$ware $\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\times$.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models</title>
<link>https://arxiv.org/abs/2511.17561</link>
<guid>https://arxiv.org/abs/2511.17561</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, lexical instruction, benchmark, evaluation framework, controllability

<br /><br />Summary: The paper addresses the challenge of evaluating Large Language Models' (LLMs) ability to follow complex and fine-grained lexical instructions, which is crucial for their utility and controllability. Existing evaluation methods either depend on subjective human judgments, which are costly and inconsistent, or on automated LLM judge systems, which have biases and reliability issues. Moreover, current programmatic benchmarks often fail to capture intricate and compositional constraints at a detailed level. To overcome these issues, the authors propose LexInstructEval, a novel benchmark and evaluation framework specifically designed for fine-grained lexical instruction following. This framework is grounded in a formal, rule-based grammar that breaks down complex instructions into a canonical triplet format, enabling systematic data generation. The dataset is created through a multi-stage, human-in-the-loop process ensuring diversity and quality. Additionally, LexInstructEval features a transparent, programmatic verification engine that allows objective and reliable evaluation of model outputs. The authors release both the dataset and open-source evaluation tools to encourage further research in improving the controllability and reliability of LLMs in following detailed lexical instructions. <div>
arXiv:2511.17561v1 Announce Type: new 
Abstract: The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical  triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector</title>
<link>https://arxiv.org/abs/2511.17562</link>
<guid>https://arxiv.org/abs/2511.17562</guid>
<content:encoded><![CDATA[
<div> Chinese Error Correction, Qwen3-4B, Spelling Correction, Grammatical Correction, Benchmark Performance  

<br /><br />Summary:  
This paper presents ChineseErrorCorrector3-4B, a unified model designed for both Chinese spelling correction (CSC) and Chinese grammatical correction (CGC), built upon the Qwen3-4B architecture. The model excels in general text correction tasks by integrating capabilities for addressing different types of errors within a single framework. It demonstrates outstanding empirical performance, surpassing existing state-of-the-art models in accuracy and reliability. The evaluation spans several authoritative benchmark datasets such as SIGHAN-2015, EC-LAW, MCSC, and NaCGEC, widely recognized for assessing Chinese error correction models. On these datasets, ChineseErrorCorrector3-4B achieves top-tier results measured by F1 and F0.5 scores, indicating superior precision and recall balanced for both spelling and grammar corrections. By ranking first in both CSC and CGC tasks, the model sets a new performance standard and provides a valuable tool for natural language processing applications involving Chinese text. The paper highlights the effectiveness of leveraging the Qwen3-4B model architecture to unify error correction tasks, suggesting future potential for extending this approach to other languages and correction types. <div>
arXiv:2511.17562v1 Announce Type: new 
Abstract: This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Caching for Structurally Similar Prompts and Responses</title>
<link>https://arxiv.org/abs/2511.17565</link>
<guid>https://arxiv.org/abs/2511.17565</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, caching, prompt variation, generative cache, agentic workflows<br /><br />Summary: Large Language Models (LLMs) are widely used for planning, reasoning, and task execution across different scenarios, often relying on prompts with similar structure but minor variations. Traditional exact prompt matching fails to effectively reuse cached responses in these cases, while semantic caching risks generating incorrect outputs by overlooking important prompt differences. To overcome these limitations, the paper introduces \ourmethod{}, a generative cache system that produces variation-aware cached responses for structurally similar prompts. \ourmethod{} works by identifying reusable response patterns across related prompt structures and synthesizing customized replies tailored to new inputs. Evaluation shows that \ourmethod{} achieves an 83% cache hit rate while keeping erroneous cache hits low in datasets lacking prompt repetition. In agentic workflow applications, \ourmethod{} increases the cache hit rate by about 20% and reduces overall execution latency by approximately 34% compared to conventional prompt matching methods. These improvements demonstrate \ourmethod{}’s ability to optimize LLM-based workflows by enhancing cache efficiency and response accuracy, ultimately speeding up task completion without compromising output quality. <div>
arXiv:2511.17565v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \ourmethod{} achieves 83\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\sim$20\% and reduces end-to-end execution latency by $\sim$34\% compared to standard prompt matching.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs</title>
<link>https://arxiv.org/abs/2511.17572</link>
<guid>https://arxiv.org/abs/2511.17572</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, epistemic stance transfer, community alignment, behavioral patterns, uncertainty handling<br /><br />Summary:  
This article investigates whether large language models (LLMs) aligned to specific online communities exhibit behavioral patterns that generalize beyond simple memorization from training data. The authors introduce a novel framework called epistemic stance transfer, which involves the targeted deletion of event knowledge within the models. This deletion is validated using multiple probing methods to ensure effective removal of factual information. The study then evaluates if these aligned LLMs continue to reproduce their community's nuanced, organic responses when faced with uncertainty or ignorance. Using datasets from Russian–Ukrainian military discourse and U.S. partisan Twitter conversations, the research reveals that even after aggressive fact removal, the models maintain stable, community-specific behavioral patterns in how they handle uncertainty. This outcome suggests that LLM alignment encodes structured and generalizable behavioral tendencies rather than merely surface-level pattern mimicry. The framework proposed offers a systematic approach for detecting persistent behavioral biases under conditions of ignorance, which is critical for advancing the safety, transparency, and reliability of LLM deployments in socially sensitive contexts. <div>
arXiv:2511.17572v1 Announce Type: new 
Abstract: When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models</title>
<link>https://arxiv.org/abs/2511.17575</link>
<guid>https://arxiv.org/abs/2511.17575</guid>
<content:encoded><![CDATA[
<div> Keywords: random text model, word length distribution, coupon collector, Zipf's law, vocabulary growth  

<br /><br />Summary:  
This paper presents a simple, non-linguistic model of text as a sequence of independent draws from a finite alphabet plus a space symbol. Words are defined as maximal blocks of non-space characters, ignoring any morphology, syntax, or semantics. First, it shows that word lengths follow a geometric distribution solely determined by the probability of drawing a space. Second, it derives closed-form expressions for the expected number of words and distinct word types of a given length using a coupon-collector argument, revealing a critical word length k* at which words transition from frequent repetition to rarity. Third, by combining the exponential growth of possible word forms with the exponential decay of their probabilities, the model produces a Zipf-type rank-frequency distribution p(r) ∝ r^(-α), with the exponent α explicitly dependent on alphabet size and space probability. Conceptually, the work offers a unified mathematical framework linking word length statistics, vocabulary growth, critical length, and rank-frequency patterns in a single model. The findings demonstrate that Zipf-like distributions can emerge purely from combinatorial and segmentation processes without linguistic or optimization mechanisms, thus serving as a null baseline for interpreting statistical patterns in natural language and large language model tokens, and helping to identify phenomena requiring deeper explanations beyond random text. <div>
arXiv:2511.17575v1 Announce Type: new 
Abstract: We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.
  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational frame analysis revisited: On LLMs for studying news coverage</title>
<link>https://arxiv.org/abs/2511.17746</link>
<guid>https://arxiv.org/abs/2511.17746</guid>
<content:encoded><![CDATA[
<div> Keywords: media frames, generative LLMs, frame analysis, manual coding, computational methods<br /><br />Summary:<br /><br />1. This study evaluates the effectiveness of generative large language models (LLMs) like GPT and Claude for media frame analysis compared to traditional computational methods such as bag-of-words models, encoder-only transformers, and manual coding. <br /><br />2. The research is based on a novel gold standard dataset developed inductively through an iterative process, focusing on six months of US Mpox epidemic news coverage in 2022. <br /><br />3. Results reveal that while generative LLMs have some potential applications in frame analysis, they are consistently outperformed by manual coders and, in some cases, by smaller language models. <br /><br />4. The study emphasizes the necessity of human validation in determining the most appropriate model for specific frame analytical tasks, highlighting that no single automated approach is sufficient on its own. <br /><br />5. By analyzing task-dependent suitability, the authors advocate for a methodologically pluralistic approach that leverages the strengths and complementarity of different computational and manual methods, providing a roadmap for future computational frame analysis research. <div>
arXiv:2511.17746v1 Announce Type: new 
Abstract: Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese</title>
<link>https://arxiv.org/abs/2511.17808</link>
<guid>https://arxiv.org/abs/2511.17808</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Portuguese, evaluation benchmark, PoETa v2, multilingual performance<br /><br />Summary:<br /><br />This paper addresses the performance variability of Large Language Models (LLMs) across different linguistic and cultural contexts, emphasizing the need for systematic evaluation in languages other than English. Focusing on Portuguese, the authors introduce PoETa v2, the most extensive benchmark suite developed for this language to date, encompassing over 40 diverse tasks. Using PoETa v2, they evaluate more than 20 LLMs that vary widely in training scale and computational resources, providing a comprehensive performance landscape for Portuguese language models. The study examines how factors such as computational investment and language-specific adaptations influence model effectiveness in Portuguese. Additionally, the paper analyzes performance disparities between Portuguese and English tasks to highlight challenges unique to modeling Portuguese with LLMs. By releasing PoETa v2 as an open benchmark available on GitHub, the authors aim to foster further research and development focused on Portuguese language modeling and evaluation, filling a critical gap in multilingual NLP resources. This work thus offers essential insights and tools to advance the state of LLMs in Portuguese, supporting broader inclusivity in language technology. <div>
arXiv:2511.17808v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation</title>
<link>https://arxiv.org/abs/2511.17813</link>
<guid>https://arxiv.org/abs/2511.17813</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, speaker attribution, multi-party deliberation, civic simulations, Zoom transcripts<br /><br />Summary:<br /><br />1. The paper addresses the challenge of realistic simulations of multi-party deliberation through large language models (LLMs), which is currently limited by anonymized speaker labels in automatic speech recognition (ASR) transcripts. 2. The authors present a reproducible pipeline that converts publicly available Zoom recordings into speaker-attributed transcripts enriched with metadata such as persona profiles and pragmatic action tags like [propose_motion]. 3. They release three new datasets derived from local government deliberations, including Appellate Court hearings, School Board meetings, and Municipal Council sessions, providing valuable resources for training and evaluation. 4. The study shows that fine-tuning LLMs with this "action-aware" data significantly improves model performance, reducing perplexity by 67% and nearly doubling classifier metrics related to speaker fidelity and realism. 5. Human evaluations modeled after the Turing test indicate that the simulations generated by these fine-tuned models are often indistinguishable from actual deliberations, demonstrating a practical and scalable approach for producing complex, realistic civic simulations. <div>
arXiv:2511.17813v1 Announce Type: new 
Abstract: Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A superpersuasive autonomous policy debating system</title>
<link>https://arxiv.org/abs/2511.17854</link>
<guid>https://arxiv.org/abs/2511.17854</guid>
<content:encoded><![CDATA[
<div> Keywords: DeepDebater, policy debate, multi-agent workflows, LLM, AI-human hybrid

<br /><br />Summary:  
1. The paper introduces DeepDebater, an autonomous AI system designed to participate in and win full, unmodified two-team competitive policy debates, addressing challenges in complex and adaptive persuasion.  
2. DeepDebater uses a hierarchical multi-agent workflow architecture where large language model (LLM)-powered agents collaborate and critique each other to manage specific argumentative tasks such as speech generation, cross-examinations, and rebuttals.  
3. The system iteratively retrieves, synthesizes, and self-corrects information from a massive corpus of policy debate evidence called OpenDebateEvidence, ensuring high-quality and evidence-based arguments.  
4. DeepDebater incorporates a live, interactive presentation pipeline that converts transcripts into natural-sounding speech using OpenAI Text-to-Speech and produces talking-head videos with EchoMimic V1, enhancing the realism of AI debaters.  
5. The platform supports both fully autonomous AI vs AI debates and hybrid scenarios where human debaters can intervene or compete against AI at any stage, allowing flexible human-AI collaboration.  

In preliminary evaluations, DeepDebater consistently outperformed human-authored debate cases and won simulated matches judged by an autonomous system. Expert human coaches also favored its argumentative quality. All related code, transcripts, audio, and videos are open-sourced at the provided GitHub repository. <div>
arXiv:2511.17854v1 Announce Type: new 
Abstract: The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction</title>
<link>https://arxiv.org/abs/2511.17908</link>
<guid>https://arxiv.org/abs/2511.17908</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, conformal prediction, context filtering, large language models, factual accuracy<br /><br />Summary:<br /><br />1. The article addresses the challenge in Retrieval-Augmented Generation (RAG) where large language models (LLMs) experience accuracy decline when processing long or noisy retrieved contexts that exceed their effective attention span. 2. Current pre-generation filtering methods rely on heuristics or uncalibrated confidence scores from LLMs, which do not provide statistical guarantees on the retention of relevant evidence. 3. The authors propose using conformal prediction, a coverage-controlled filtering framework, to engineer the context by removing irrelevant content while ensuring a guaranteed fraction of relevant snippets is retained. 4. They evaluate this approach on the NeuCLIR and RAGTIME datasets using both embedding-based and LLM-based scoring functions, showing that conformal filtering consistently achieves its target coverage and reduces the size of the retained context by 2 to 3 times compared to unfiltered retrieval. 5. Experimental results on NeuCLIR demonstrate that strict filtering improves downstream factual accuracy (measured by ARGUE F1), while moderate filtering maintains stable accuracy, indicating that most of the discarded content is redundant or irrelevant. The study concludes that conformal prediction provides a reliable, model-agnostic, and principled method for context reduction in RAG systems. <div>
arXiv:2511.17908v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention</title>
<link>https://arxiv.org/abs/2511.17910</link>
<guid>https://arxiv.org/abs/2511.17910</guid>
<content:encoded><![CDATA[
<div> Chain-of-Thought, Vision-Language Models, Linear Artificial Tomography, latent representation, training-free transfer  

<br /><br />Summary:  
This paper addresses the challenge that Vision-Language Models (VLMs) face in multi-step reasoning tasks due to limited multimodal reasoning data, despite the success of Chain-of-Thought (CoT) reasoning in large language models (LLMs). The authors use Linear Artificial Tomography (LAT) to reveal that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning, even though their architectures differ. Building on this insight, they propose L2V-CoT, a novel approach that transfers CoT reasoning capabilities from LLMs to VLMs without any training. L2V-CoT works by extracting and resampling the low-frequency CoT latent features from LLMs in the frequency domain, allowing for dimension alignment and latent feature injection into VLMs during inference. This latent intervention improves the reasoning ability of VLMs effectively and efficiently. Extensive experiments demonstrate that L2V-CoT not only outperforms existing training-free baseline methods but also exceeds some supervised learning approaches on benchmark tasks. The proposed method thus provides a cost-effective and generalizable solution for enhancing multimodal reasoning in VLMs by leveraging the strengths of LLMs without requiring architectural changes or expensive training. <div>
arXiv:2511.17910v1 Announce Type: new 
Abstract: Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient LLM-aware Heterogeneous Graph Learning</title>
<link>https://arxiv.org/abs/2511.17923</link>
<guid>https://arxiv.org/abs/2511.17923</guid>
<content:encoded><![CDATA[
<div> Keywords: Heterogeneous Graphs, Large Language Models, Relation Tokenizer, Graph Transformer, Chain-of-Thought Prompts  

<br /><br />Summary:  
This paper addresses challenges in modeling complex relation semantics in heterogeneous graphs, which feature diverse node and relation types leading to rich and complex semantics. Traditional methods are limited by predefined semantic dependencies and a lack of supervised signals. The authors propose ELLA, an Efficient LLM-Aware framework that integrates Large Language Models (LLMs) to leverage their reasoning capabilities for encoding multi-hop, multi-type relations in heterogeneous graphs. To capture complex relations, the framework includes an LLM-aware Relation Tokenizer. To reduce the otherwise prohibitive computational complexity of incorporating LLMs, the paper introduces a Hop-level Relation Graph Transformer that decreases relation reasoning complexity from exponential to linear, making the approach scalable. To mitigate semantic gaps between pre-training and fine-tuning tasks, ELLA utilizes fine-grained, task-aware textual Chain-of-Thought (CoT) prompts. Experimental results on four heterogeneous graph datasets demonstrate that ELLA outperforms existing state-of-the-art methods in both effectiveness and efficiency. Notably, ELLA scales to LLMs with up to 13 billion parameters and achieves up to a fourfold speedup compared to previous LLM-based approaches. The code has been made publicly available for further research and development. <div>
arXiv:2511.17923v1 Announce Type: new 
Abstract: Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization</title>
<link>https://arxiv.org/abs/2511.17938</link>
<guid>https://arxiv.org/abs/2511.17938</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, test-time reinforcement learning, chain-of-thought reasoning, entropy regularization, stable training

<br /><br />Summary:  
Large language models (LLMs) and multimodal LLMs (MLLMs) perform well on chain-of-thought reasoning but encounter challenges due to distribution shifts at test time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) approaches generate label-free pseudo-rewards via self-consistency voting among sampled reasoning trajectories, but they suffer from collapse where majority-vote rewards dominate, answer lengths shorten, and Pass@1 metrics decline. The authors identify that uniform updates across all tokens cause this issue since most tokens are low-entropy ‘followers’, while a minority with high entropy dictate reasoning branches. To address this, they propose SPINE, a token-selective test-time reinforcement learning method that updates only the high-entropy ‘forking’ tokens identified via forward-pass statistics. Additionally, SPINE employs an entropy-band regularizer to maintain exploration when entropy drops too low and suppress noisy supervision when entropy is too high. SPINE integrates with GRPO-style objectives and can optionally use a KL divergence anchor without requiring labels or reward models. Evaluated on ten benchmarks across multimodal visual QA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 scores over existing TTRL methods, avoids response length collapse, and stabilizes training dynamics on both LLM and MLLM backbones. This shows that aligning parameter updates with chain-of-thought branching points enables stable and effective test-time adaptation in reasoning models. Code is publicly available. <div>
arXiv:2511.17938v1 Announce Type: new 
Abstract: Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2511.17946</link>
<guid>https://arxiv.org/abs/2511.17946</guid>
<content:encoded><![CDATA[
<div> Keywords: hallucination detection, large language models, training data coverage, suffix arrays, open-domain QA<br /><br />Summary:<br />1. Hallucination in large language models (LLMs) presents a major challenge, especially for open-domain question answering tasks where generated content may be inaccurate or fabricated.  
2. Prior approaches focus on internal model signals such as token-level entropy or generation consistency to identify hallucinations, but the direct role of pretraining data exposure remains underexplored.  
3. The paper investigates whether lexical coverage of the training data—specifically the frequency and occurrence of question and answer n-grams in the pretraining corpus—can serve as an additional signal for detecting hallucinations.  
4. To do this, the authors build scalable suffix arrays over the RedPajama 1.3-trillion-token pretraining corpus to efficiently retrieve n-gram statistics for both input prompts and model-generated answers.  
5. Experiments on three QA benchmarks reveal that while n-gram occurrence-based features alone are weak predictors of hallucination, when combined with log-probability metrics from the model, they contribute modest improvements, especially in datasets with higher innate model uncertainty.  
6. These results indicate that lexical coverage features from the training dataset provide a complementary, useful signal to enhance hallucination detection methods in LLMs.  
7. The paper also releases all code and suffix-array infrastructure openly at the provided GitHub repository for reproducibility and further research. <div>
arXiv:2511.17946v1 Announce Type: new 
Abstract: Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok</title>
<link>https://arxiv.org/abs/2511.17955</link>
<guid>https://arxiv.org/abs/2511.17955</guid>
<content:encoded><![CDATA[
<div> TikTok, harmful content detection, multimodal classification, real-time moderation, dataset expansion<br /><br />Summary: This paper addresses the challenge of detecting harmful content on TikTok, especially content that is subtle or deceptive and difficult for traditional moderation methods to handle due to the platform’s massive volume and real-time video uploads. The authors propose MTikGuard, a real-time multimodal harmful content detection system designed specifically for TikTok. The work includes three major contributions: first, the expansion of the TikHarm dataset to 4,723 labeled videos incorporating a diverse range of real-world samples, which improves the representativeness and robustness of the training data. Second, the design of a multimodal classification framework that integrates visual, audio, and textual information from videos, achieving state-of-the-art detection performance with an accuracy of 89.37% and an F1-score of 89.45%. Third, the development of a scalable streaming architecture leveraging Apache Kafka and Apache Spark to enable real-time deployment and processing at scale. The combination of an expanded dataset, advanced multimodal fusion techniques, and a robust, scalable system demonstrates significant practical potential for large-scale content moderation on social media platforms. Additionally, the enhanced TikHarm dataset is made publicly available for further research and development in this domain. <div>
arXiv:2511.17955v1 Announce Type: new 
Abstract: With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets</title>
<link>https://arxiv.org/abs/2511.18054</link>
<guid>https://arxiv.org/abs/2511.18054</guid>
<content:encoded><![CDATA[
<div> Blu-WERP, data preprocessing, Common Crawl, LLM training, performance improvement  

<br /><br />Summary:  
This paper introduces Blu-WERP, a novel data preprocessing pipeline designed to enhance the quality of Common Crawl WARC files specifically for large language model (LLM) training. The pipeline addresses challenges in removing noise and unstructured content from web-scale corpora more effectively than existing methods. Blu-WERP incorporates advanced filtering and quality assessment techniques to optimize training data quality. Evaluations were conducted across multiple model sizes, ranging from 150 million to 1 billion parameters, to assess performance on nine benchmarks spanning World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently outperforms established baselines such as DCLM and Fineweb, achieving a 4.0% and 9.5% overall improvement respectively at the 1 billion parameter scale. Additionally, Blu-WERP delivers gains in quality-per-token efficiency, reducing computational cost. Categorical improvements include 2.4% in World Knowledge & Reasoning, 6.2% in Language Understanding, and 4.2% in Commonsense Reasoning. These findings highlight the critical role of data preprocessing in enhancing LLM capabilities and training efficiency. Blu-WERP offers a practical, state-of-the-art solution for researchers and practitioners aiming to improve LLM performance through superior data quality optimization. <div>
arXiv:2511.18054v1 Announce Type: new 
Abstract: High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set</title>
<link>https://arxiv.org/abs/2511.18146</link>
<guid>https://arxiv.org/abs/2511.18146</guid>
<content:encoded><![CDATA[
<div> Keywords: Sinhala, emotion recognition, YouTube comments, Valence-Arousal model, Multi-Layer Perceptron<br /><br />Summary:<br /><br />This study introduces GeeSanBhava, a high-quality annotated dataset comprising Sinhala song comments collected from YouTube. The comments were manually tagged based on Russell’s Valence-Arousal emotion model by three independent human annotators, who achieved a substantial inter-annotator agreement with a Fleiss kappa score of 84.96%. The dataset provides insights into distinct emotional profiles for different songs, demonstrating the value of emotion mapping based on user comments. The research addresses challenges associated with comparing comment-based emotions with song-based emotions, while mitigating biases commonly found in user-generated content. For modeling, machine learning and deep learning approaches were leveraged, with models pre-trained on a related large dataset of Sinhala news comments to enable zero-shot evaluation on the new dataset. Among the models tested, an optimized three-layer Multi-Layer Perceptron (MLP) with layers of 256, 128, and 64 neurons achieved the highest performance, obtaining a ROC-AUC score of 0.887 after extensive hyperparameter tuning. This work not only contributes a valuable resource for emotion recognition in Sinhala language but also advances research in Sinhala Natural Language Processing and music emotion recognition tasks. <div>
arXiv:2511.18146v1 Announce Type: new 
Abstract: This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Arithmetic in Concept and Token Subspaces</title>
<link>https://arxiv.org/abs/2511.18162</link>
<guid>https://arxiv.org/abs/2511.18162</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, attention heads, concept induction, token induction, semantic arithmetic<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) need to represent both semantic and surface-level information about words to predict the next token effectively. 2. Previous research identified two specific types of attention heads in LLMs: concept induction heads, which capture the meanings of words, and token induction heads, which preserve the literal token representations. 3. This work focuses on the Llama-2-7b model and demonstrates that these attention heads can be used to identify subspaces within the model's hidden state activations that carry well-defined semantic structures. 4. By transforming hidden states with attention weights from concept induction heads, the authors achieve improved performance on parallelogram arithmetic tasks (e.g., vector arithmetic analogies like "Athens" - "Greece" + "China" = "Beijing"), with nearest-neighbor accuracy increasing from 47% using raw hidden states to 80% with the transformation. 5. Similarly, token induction heads help isolate surface-level properties of words, enabling transformations that capture morphological phenomena like changing word forms (e.g., "coding" - "code" + "dance" = "dancing"). This research advances understanding of how LLMs internally encode semantic and lexical information through attention mechanisms. <div>
arXiv:2511.18162v1 Announce Type: new 
Abstract: In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that "Athens" - "Greece" + "China" = "Beijing". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like "coding" - "code" + "dance" = "dancing".
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models</title>
<link>https://arxiv.org/abs/2511.18177</link>
<guid>https://arxiv.org/abs/2511.18177</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, financial documents, vector-based RAG, cross-encoder reranking, small-to-big chunk retrieval

<br /><br />Summary:  
This paper presents the first systematic evaluation comparing vector-based Retrieval-Augmented Generation (RAG) architectures against hierarchical node-based systems for answering financial questions using U.S. SEC filings and related documents. The study benchmarks performance on a dataset of 1,200 SEC 10-K, 10-Q, and 8-K filings with a 150-question set, focusing on retrieval metrics (MRR, Recall@5), answer quality assessed by LLM-based pairwise judgments, latency, and preprocessing costs. Results show that vector-based agentic RAG with hybrid search and metadata filtering outperforms hierarchical node-based approaches, achieving a 68% win rate while maintaining comparable latency (5.2 vs. 5.98 seconds). Two enhancement techniques improve the vector-based system: (i) cross-encoder reranking significantly boosts retrieval precision, with a 59% absolute improvement in MRR@5 at optimal parameters; (ii) small-to-big chunk retrieval enhances context completeness, winning 65% of the time over baseline chunking with only a minor increase in latency (0.2 seconds). Overall, the findings highlight that advanced vector-based RAG techniques can improve retrieval accuracy and answer quality for financial question answering, though production deployment should consider the tradeoffs between latency, cost, and performance. <div>
arXiv:2511.18177v1 Announce Type: new 
Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&amp;A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.18194</link>
<guid>https://arxiv.org/abs/2511.18194</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multi-Agent Systems, Knowledge Graph, Retrieval, Weighted Reciprocal Rank Fusion  

<br /><br />Summary: This paper presents a novel retrieval approach called Agent-as-a-Graph designed to improve the selection of specialized agents and their tools in large multi-agent systems powered by large language models. Traditional methods retrieve agents based on a single agent description, which masks the detailed capabilities of individual tools and leads to suboptimal selections. The proposed method models agents and their associated tools as nodes and edges within a knowledge graph, enabling more granular retrieval. The retrieval process involves three steps: first, vector search is used to separately retrieve relevant agent and tool nodes; second, a type-specific weighted reciprocal rank fusion (wRRF) algorithm is applied for reranking these nodes; third, parent agents are traversed within the knowledge graph to finalize the agent set. The approach was evaluated on the LiveMCPBenchmark dataset, where it demonstrated significant improvements of 14.9% and 14.6% in Recall@5 and nDCG@5 metrics respectively, compared to previous state-of-the-art retrievers. Additionally, the wRRF mechanism contributed to a further 2.4% gain in retrieval effectiveness. These results highlight the benefit of incorporating fine-grained tool-level information and graph-based retrieval in multi-agent system orchestration. <div>
arXiv:2511.18194v1 Announce Type: new 
Abstract: Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation</title>
<link>https://arxiv.org/abs/2511.18259</link>
<guid>https://arxiv.org/abs/2511.18259</guid>
<content:encoded><![CDATA[
<div> Keywords: DiscoVerse, pharmaceutical R&amp;D, multi-agent system, reverse translation, expert evaluation<br /><br />Summary:<br /><br />1. The paper introduces DiscoVerse, a multi-agent co-scientist system designed to aid pharmaceutical research and development by reusing large, heterogeneous historical archives, especially those from discontinued research programs.<br /><br />2. DiscoVerse incorporates semantic retrieval, cross-document linking, and auditable synthesis techniques applied to a substantial Roche dataset encompassing over 180 molecules, 0.87 billion BPE tokens, and four decades of drug development research.<br /><br />3. Traditional automated metrics are inadequate for measuring scientific utility; therefore, the system's performance was validated by blinded expert evaluations of outputs linked back to original sources.<br /><br />4. This work represents the first agentic framework systematically tested with real-world pharmaceutical data for reverse translation, enabled through authorized access to confidential end-to-end drug development archives.<br /><br />5. DiscoVerse demonstrated near-perfect recall (≥0.99) and moderate precision (0.71–0.91) on seven benchmark queries, successfully synthesizing organ-specific toxicity and discontinuation rationale with faithful source linkage, highlighting its potential for enhanced scientific insight and decision-making in drug development. <div>
arXiv:2511.18259v1 Announce Type: new 
Abstract: Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"AGI" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa</title>
<link>https://arxiv.org/abs/2511.18301</link>
<guid>https://arxiv.org/abs/2511.18301</guid>
<content:encoded><![CDATA[
<div> hallucination detection, multilingual, scientific text, data-centric, XLM-RoBERTa-Large<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting hallucinations in multilingual scientific texts generated by Large Language Models (LLMs), a critical task for ensuring AI reliability. The authors participated in the SHROOM-CAP 2025 shared task covering nine languages. Diverging from common methods that emphasize model architecture, their focus is on a data-centric approach to overcome the scarcity and imbalance of training data. They combined and balanced five existing datasets into a large training corpus with 124,821 samples, equally split between correct and hallucinated content, increasing the original SHROOM training data size by 172 times. Using this enhanced dataset, they fine-tuned XLM-RoBERTa-Large, a model with 560 million parameters, achieving competitive results across all nine languages. Notably, they secured 2nd place in Gujarati, a zero-shot language in this task, with a Factuality F1 score of 0.5107, and ranked between 4th and 6th place in the other eight languages. The study demonstrates that thorough data curation and balance can outperform innovations in model architecture alone, especially benefiting low-resource languages in zero-shot contexts. <div>
arXiv:2511.18301v1 Announce Type: new 
Abstract: The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.18306</link>
<guid>https://arxiv.org/abs/2511.18306</guid>
<content:encoded><![CDATA[
<div> Keywords: Building Codes, Retrieval-Augmented Generation, Vision Language Models, Tabular Data Extraction, Low Rank Adaptation (LoRA)  

<br /><br />Summary:  
This paper addresses the challenge of automated question answering over building codes, focusing on extracting information from complex tabular data that traditional NLP and Vision Language Models (VLMs) struggle to handle. Two extraction methods are compared: a direct input method feeding page images directly into VLMs to answer questions, and an indirect method converting table images into LaTeX code before querying. Experimental results reveal that the direct image input method generally achieves higher accuracy. To enhance model performance, the authors fine-tune multiple pre-trained VLMs using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset related to building codes. This fine-tuning results in significant accuracy improvements, with the Qwen2.5-VL-3B-Instruct model showing over 100% relative gains. The study demonstrates that parameter-efficient fine-tuning techniques like LoRA are effective for enabling VLMs to comprehend complex structured data, particularly in specialized regulatory fields such as building code interpretation and compliance. These findings suggest that combining retrieval-augmented generation with fine-tuned VLMs can substantially improve automated regulatory question answering accuracy, thereby facilitating safer and more efficient construction and engineering decision-making processes. <div>
arXiv:2511.18306v1 Announce Type: new 
Abstract: Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search</title>
<link>https://arxiv.org/abs/2511.18313</link>
<guid>https://arxiv.org/abs/2511.18313</guid>
<content:encoded><![CDATA[
<div> Path-Constrained Retrieval, Large Language Models, Knowledge Graphs, Structural Consistency, Semantic Search

<br /><br />Summary:  
This paper introduces Path-Constrained Retrieval (PCR), a novel method designed to improve the coherence of information retrieved by Large Language Model (LLM) agents. PCR integrates structural constraints from knowledge graphs with semantic search techniques, ensuring that retrieved information maintains logical and relational consistency with the agent’s current reasoning context. The method restricts retrieval to nodes reachable from a specific anchor node in the knowledge graph, effectively preventing the inclusion of disconnected or irrelevant data that can disrupt coherent reasoning chains. PCR was evaluated on PathRAG-6, a benchmark dataset covering six domains with 180 nodes and 360 edges. Results demonstrate that PCR achieves 100% structural consistency, significantly outperforming baseline approaches which only reach 24-32%. Moreover, in the technology domain, PCR ensures full relevance within the top 10 retrieved results, surpassing both vector search and hybrid retrieval methods. Additionally, PCR reduces the average graph distance of retrieved context by 78% compared to baselines, indicating retrieval of tightly connected, structurally consistent information. Overall, this research highlights the effectiveness of combining graph-based constraints with semantic retrieval to enhance the reliability and coherence of reasoning in LLM-driven systems. <div>
arXiv:2511.18313v1 Announce Type: new 
Abstract: Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection</title>
<link>https://arxiv.org/abs/2511.18324</link>
<guid>https://arxiv.org/abs/2511.18324</guid>
<content:encoded><![CDATA[
<div> Keywords: Gradient Masters, Bangla hate speech, multitask classification, ensemble fine-tuning, low-resource NLP<br /><br />Summary:<br /><br />This paper introduces the "Gradient Masters" approach for the BLP-2025 Task 1, focusing on Bangla multitask hate speech identification using YouTube comments. The study addresses two subtasks: 1A (hate-type classification) and 1B (target group classification), employing an ensemble-based fine-tuning strategy to enhance performance. A hybrid method based on a Bangla Language Model was proposed, which outperformed several baseline models, achieving 6th place in subtask 1A with a micro F1 score of 73.23% and 3rd place in subtask 1B with 73.28%. Thorough experiments were conducted to evaluate the robustness and generalization ability of the model, particularly in low-resource Bangla hate speech scenarios, including comparative analysis with other language model variants. The approach demonstrated strong dataset coverage and reliability throughout both the development and evaluation phases. Additionally, the paper provides an in-depth analysis of misclassification trends in hate speech detection, offering insights into the model’s strengths and weaknesses. This study contributes valuable advancements for multitask hate speech identification in the Bangla language, a relatively underexplored and resource-constrained area in NLP. <div>
arXiv:2511.18324v1 Announce Type: new 
Abstract: This paper introduces the approach of "Gradient Masters" for BLP-2025 Task 1: "Bangla Multitask Hate Speech Identification Shared Task". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas</title>
<link>https://arxiv.org/abs/2511.18335</link>
<guid>https://arxiv.org/abs/2511.18335</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, text-to-structure, OmniStruct benchmark, synthetic data, structured generation<br /><br />Summary:  
1. This paper addresses the challenge of enabling Large Language Models (LLMs) to generate structured outputs following arbitrary schemas, a critical capability for tasks like information extraction, table generation, and function calling.  
2. The authors introduce OmniStruct, a comprehensive benchmark designed to evaluate LLMs on diverse text-to-structure tasks by unifying various existing datasets into a common format suitable for structured output generation.  
3. To promote the development of models capable of structured generation, they create high-quality training data through synthetic task generation, eliminating the reliance on manually supervised datasets for these tasks.  
4. The study shows that smaller models fine-tuned on this synthetic data can perform universal structured generation tasks effectively, competing with advanced models like GPT-4o.  
5. Overall, the work bridges the gap between natural language generation capabilities of LLMs and their performance on structured output tasks, providing useful resources and methodologies for the community to develop efficient text-to-structure models. <div>
arXiv:2511.18335v1 Announce Type: new 
Abstract: The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tu crois que c'est vrai ? Diversite des regimes d'enonciation face aux fake news et mecanismes d'autoregulation conversationnelle</title>
<link>https://arxiv.org/abs/2511.18369</link>
<guid>https://arxiv.org/abs/2511.18369</guid>
<content:encoded><![CDATA[
<div> fake news, political polarization, social media, critical distance, digital traces<br /><br />Summary:<br /><br />This thesis addresses two key paradoxes related to misinformation and political divisions on social media: why fake news comprises a small portion of shared information despite lax editorial control, and why political polarization has escalated even though users show limited receptiveness to fake news. It uses a mixed-methods approach combining quantitative digital trace analysis with online observation and interviews on Twitter and Facebook to capture diverse user interactions across contexts and socio-demographic groups. First, the research identifies that the sharing of fake news is concentrated among a small subset of highly politicized and institution-critical users who are not less educated or cognitively disadvantaged. Their high activity allows them to influence their political camp’s information agenda. Second, users exposed to dubious information deploy different critical distancing strategies based on their social positions and interaction norms, such as discursive caution or explicit disagreement and correction attempts. Third, these critical interactions rarely foster genuine deliberation or pluralistic debate; instead, they often result in echo chambers or “dialogues of the deaf” dominated by an active minority, which limits meaningful engagement across opposing views. <div>
arXiv:2511.18369v1 Announce Type: new 
Abstract: This thesis addresses two paradoxes: (1) why empirical studies find that fake news represent only a small share of the information consulted and shared on social media despite the absence of editorial control or journalistic norms, and (2) how political polarization has intensified even though users do not appear especially receptive to fake news. To investigate these issues, two complementary studies were carried out on Twitter and Facebook, combining quantitative analyses of digital traces with online observation and interviews. This mixed-methods design avoids reducing users to single reactions to identified fake items and instead examines the variety of practices across different interactional situations, online and offline, while recording socio-demographic traits. The first study mapped users who shared at least one item labeled fake by fact-checkers in the French Twittersphere. The second used a corpus of items flagged by Facebook users to study reactions to statements whose epistemic status is uncertain. Three main findings emerge. First, sharing fake news is concentrated among a limited group of users who are not less educated or cognitively disadvantaged but are more politicized and critical of institutions; owing to their high activity and prolific sharing, they can help set the agenda for their political camp. Second, exposed users can deploy varying forms of critical distance depending on their social position and the interactional norms of the situations they inhabit: either discursive caution (prudence \'enonciative) or interventions ('points d'arr\^et') that express disagreement or corrections. Third, these forms of critical distance seldom yield genuine deliberative debates or agonistic pluralism; rather, they often produce dialogues of the deaf among a small, particularly active minority.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models</title>
<link>https://arxiv.org/abs/2511.18393</link>
<guid>https://arxiv.org/abs/2511.18393</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, clinical decision support systems, large language models, robustness, equity<br /><br />Summary:<br /><br />1. This paper investigates the robustness and fairness of large language models (LLMs) when used in clinical decision support systems (CDSS), especially under conditions where clinical text inputs are corrupted by errors or automated processing failures.<br /><br />2. The study highlights concerns that text degradation can increase predictive uncertainty and disproportionately affect different demographic subgroups, leading to potential fairness issues in AI-assisted medical decision-making.<br /><br />3. To manage the complexity of a large diagnostic label space in next-visit diagnosis prediction, the authors propose a clinically grounded label-reduction method that simplifies the prediction task.<br /><br />4. They implement a hierarchical chain-of-thought (CoT) strategy that mimics clinical reasoning, which helps the models maintain robustness and reduce instability across demographic subgroups when inputs are noisy or corrupted.<br /><br />5. The approach advances the reliable deployment of LLMs within CDSS and the authors provide their code openly at https://github.com/heejkoo9/NECHOv3 to facilitate further research and development in this area. <div>
arXiv:2511.18393v1 Announce Type: new 
Abstract: A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models</title>
<link>https://arxiv.org/abs/2511.18409</link>
<guid>https://arxiv.org/abs/2511.18409</guid>
<content:encoded><![CDATA[
<div> Keywords: Mechanistic interpretability, language models, circuit localization, causal variable localization, benchmark

<br /><br />Summary:  
Mechanistic interpretability (MI) aims to understand how language models implement specific behaviors, but assessing progress in MI has been difficult. The Mechanistic Interpretability Benchmark (MIB), introduced by Mueller et al. in 2025, establishes a standardized framework to evaluate the localization of circuits and causal variables within models. Building on MIB, the BlackboxNLP 2025 Shared Task provides a community-wide platform for reproducible comparison of MI methods. The shared task features two main tracks: circuit localization, which focuses on identifying the causally influential components and their interactions in model behavior, and causal variable localization, which seeks to map model activations onto interpretable features. Participation included three teams applying eight different methods in circuit localization, achieving notable improvements through ensemble and regularization strategies for discovering circuits. For causal variable localization, one team employed two methods involving low-dimensional and non-linear projections of activation vectors, leading to significant gains. The MIB leaderboard remains open, encouraging ongoing research and development in MI techniques under this standardized evaluation framework, thus fostering continued advancements in understanding how language models function mechanistically. <div>
arXiv:2511.18409v1 Announce Type: new 
Abstract: Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data</title>
<link>https://arxiv.org/abs/2511.18411</link>
<guid>https://arxiv.org/abs/2511.18411</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic dataset, multi-turn dialogue, reasoning, tool calling, translation pipeline<br /><br />Summary:<br /><br />1. The paper addresses the lack of large-scale, multi-turn Arabic datasets that incorporate reasoning and tool calling, which are essential for advanced language model training. <br />2. While naive translation methods suffice for pretraining data at a large scale, post-training efforts demand higher-quality data that require rigorous dataset curation. <br />3. The authors introduce SmolKalam, a dataset created by translating the existing Smoltalk2 dataset into Arabic using a multi-model ensemble translation pipeline to ensure translation quality and consistency. <br />4. The methodology applies quality filtering techniques to the translated data, aiming to enhance the dataset’s overall quality and suitability for training traditional decoder-only models. <br />5. Through ablation studies, the paper examines effective translation techniques tailored specifically for decoder-only architectures, providing insights into optimizing Arabic dataset translation for these models. <div>
arXiv:2511.18411v1 Announce Type: new 
Abstract: Although the community has tackled the acquisition of high-quality Arabic pretraining data, we still lack large-scale, multi-turn Arabic datasets that include reasoning and tool calling. Naive translation can work at the pretraining scale, but post-training demands much higher quality, which requires a stricter approach to dataset curation. In this work, we introduce SmolKalam, a translation of Smoltalk2 that uses a multi-model ensemble translation pipeline, applies quality filtering, and examines effective translation techniques for traditional decoder-only models through ablations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations</title>
<link>https://arxiv.org/abs/2511.18413</link>
<guid>https://arxiv.org/abs/2511.18413</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Collaborative Filtering, agentic recommendations, large language models, collaborative signals, orchestration agent<br /><br />Summary: This article proposes the Multi-Agent Collaborative Filtering (MACF) framework to improve agentic recommender systems by leveraging collaborative filtering principles within large language model (LLM)-based multi-agent collaboration. Unlike existing agentic recommendations that rely on generic single-agent workflows or fixed multi-agent pipelines, MACF treats similar users and relevant items as distinct LLM agents, each possessing unique profiles capable of retrieval, suggestion, and interaction. The framework features a central orchestrator agent that dynamically manages agent collaboration through adaptive recruitment and personalized instruction, enhancing flexibility beyond traditional static preference aggregation methods. By modeling user-item interactions as a collaborative multi-agent process, MACF better exploits collaborative signals embedded in historical data, which are often underused in prior systems. Experimental evaluations across three diverse domain datasets demonstrate that MACF outperforms strong baseline models in recommendation quality. Overall, this work presents an innovative integration of collaborative filtering paradigms with agentic LLM systems, offering a robust path forward for more context-aware and personalized recommendation solutions in web applications. <div>
arXiv:2511.18413v1 Announce Type: new 
Abstract: Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General Agentic Memory Via Deep Research</title>
<link>https://arxiv.org/abs/2511.18423</link>
<guid>https://arxiv.org/abs/2511.18423</guid>
<content:encoded><![CDATA[
<div> Memory, General Agentic Memory, Just-in-Time Compilation, Large Language Models, Reinforcement Learning<br /><br />Summary: This paper addresses the challenge of information loss in static memory systems commonly used by AI agents. It introduces a novel framework called General Agentic Memory (GAM) which operates under the principle of "just-in-time (JIT) compilation" by creating optimized contexts for client requests at runtime rather than pre-constructing extensive static memory. GAM is designed with two key components: the Memorizer and the Researcher. The Memorizer maintains a lightweight memory that highlights essential historical information while storing complete history in a universal page-store. The Researcher dynamically retrieves and integrates relevant information from this page-store based on the online requests, guided by the compressed memory. This duo-design allows GAM to exploit advanced large language models' agentic capabilities and scalability during test time. Additionally, GAM supports end-to-end performance improvements using reinforcement learning techniques. Experimental results demonstrate that GAM significantly outperforms existing memory systems in various memory-grounded task completion scenarios, highlighting its effectiveness in handling dynamic memory demands and enhancing AI agent performance. <div>
arXiv:2511.18423v1 Announce Type: new 
Abstract: Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \textbf{general agentic memory (GAM)}. GAM follows the principle of "\textbf{just-in time (JIT) compilation}" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindEval: Benchmarking Language Models on Multi-turn Mental Health Support</title>
<link>https://arxiv.org/abs/2511.18491</link>
<guid>https://arxiv.org/abs/2511.18491</guid>
<content:encoded><![CDATA[
<div> Mental Health, AI Chatbots, Evaluation Framework, Language Models, Therapeutic Interaction<br /><br />Summary:<br /><br />1. The paper addresses increasing demand for mental health support via AI chatbots, highlighting limitations such as sycophancy, overvalidation, and reinforcement of maladaptive beliefs in current systems.<br /><br />2. A primary challenge identified is the lack of comprehensive benchmarks that effectively measure the complexity of real-world therapeutic conversations beyond single-turn or multiple-choice assessments.<br /><br />3. To overcome this, the authors introduce MindEval, a novel evaluation framework co-designed with Ph.D.-level Licensed Clinical Psychologists, which simulates realistic, multi-turn mental health therapy interactions using automated patient simulation and evaluation by large language models (LLMs).<br /><br />4. The framework is validated quantitatively, demonstrating that simulated patient dialogues mirror human-generated text and that automatic evaluations align well with human expert judgments.<br /><br />5. Evaluation of 12 state-of-the-art LLMs using MindEval reveals underperformance, with average scores below 4 out of 6, highlighting specific struggles with AI-related communication pitfalls. Notably, larger model size and reasoning ability do not ensure better results, which also deteriorate with longer dialogues and more severe patient symptoms.<br /><br />6. The authors release all associated resources, including code, prompts, and human evaluation data, to foster further research and development in the domain. <div>
arXiv:2511.18491v1 Announce Type: new 
Abstract: Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>For Those Who May Find Themselves on the Red Team</title>
<link>https://arxiv.org/abs/2511.18499</link>
<guid>https://arxiv.org/abs/2511.18499</guid>
<content:encoded><![CDATA[
<div> Keywords: literary scholars, large language model, interpretability, ideological struggle, red team<br /><br />Summary: This position paper emphasizes the urgent need for literary scholars to actively participate in the research surrounding large language model (LLM) interpretability. First, it argues that traditional standards of interpretability focus heavily on instrumentality, which may be inadequate or limiting when applied to LLMs within literary studies. Second, the paper acknowledges that this engagement will inevitably involve an ideological struggle, including potential challenges related to complicity with current technological frameworks. Third, it asserts that despite these complexities, such involvement is necessary to broaden and deepen the understanding and critique of LLM interpretation. Fourth, the author proposes that one effective venue for this engagement is through the practice of red teaming—actively probing and challenging LLMs to uncover biases, limitations, and behaviors. Finally, engaging literary scholars in this manner has the potential to reshape interpretability research by introducing critical perspectives that transcend purely instrumental approaches and enrich interdisciplinary dialogue between the humanities and AI research communities. <div>
arXiv:2511.18499v1 Announce Type: new 
Abstract: This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dealing with the Hard Facts of Low-Resource African NLP</title>
<link>https://arxiv.org/abs/2511.18557</link>
<guid>https://arxiv.org/abs/2511.18557</guid>
<content:encoded><![CDATA[
<div> Keywords: Bambara, low-resource languages, speech dataset, annotation, speech models<br /><br />Summary:<br /><br />This paper addresses challenges in creating speech datasets and models for low-resource languages, focusing on Bambara, a West African language. First, the authors collected 612 hours of spontaneous Bambara speech through fieldwork, enhancing data availability for this underrepresented language. Second, they developed a semi-automated annotation process to generate transcriptions of the recorded speech, facilitating efficient and scalable labeling. Third, leveraging this dataset, they created several monolingual ultra-compact and small speech models designed to operate effectively on limited data. Fourth, the authors performed both automatic and human evaluations of these models’ output, underscoring the critical role of human assessment in validating model performance. Finally, the paper provides practical recommendations for data collection protocols, annotation methods, and model design tailored to low-resource speech tasks. Additionally, the published resources include not only the main Bambara dataset but also multiple evaluation datasets, models, and code to support further research and development in the community. This comprehensive work contributes valuable insights and tools to improve speech technology accessibility for low-resource languages. <div>
arXiv:2511.18557v1 Announce Type: new 
Abstract: Creating speech datasets, models, and evaluation frameworks for low-resource languages remains challenging given the lack of a broad base of pertinent experience to draw from. This paper reports on the field collection of 612 hours of spontaneous speech in Bambara, a low-resource West African language; the semi-automated annotation of that dataset with transcriptions; the creation of several monolingual ultra-compact and small models using the dataset; and the automatic and human evaluation of their output. We offer practical suggestions for data collection protocols, annotation, and model design, as well as evidence for the importance of performing human evaluation. In addition to the main dataset, multiple evaluation datasets, models, and code are made publicly available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks</title>
<link>https://arxiv.org/abs/2511.18597</link>
<guid>https://arxiv.org/abs/2511.18597</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, difficulty prediction, GPT-4o, LightGBM, competitive programming<br /><br />Summary:<br /><br />This study evaluates the effectiveness of Large Language Models (LLMs), specifically GPT-4o, in assessing the difficulty levels of competitive programming problems, compared to a traditional machine learning model, LightGBM. The authors use a dataset of 1,825 LeetCode problems labeled as Easy, Medium, or Hard to benchmark performance. LightGBM, trained on explicit numeric and textual features such as input size limits and acceptance rates, achieves a high accuracy of 86% in difficulty prediction. In contrast, GPT-4o, operating solely as a natural language difficulty assessor without numeric inputs, achieves only 37.75% accuracy and frequently misclassifies Hard problems as easier categories, showing a bias towards simpler classifications. The study includes detailed interpretability analyses with confusion matrices and SHAP values to highlight the crucial role of numeric constraints in effective difficulty separation. Additionally, when tested on synthetic Hard problems generated by GPT-4o itself, the model largely rated them as Medium, revealing inconsistent internal difficulty judgment. The findings emphasize important failure modes of LLM-based judges and suggest that without addressing these issues, LLMs cannot yet be considered reliable for automatic difficulty estimation in competitive programming platforms or educational and reinforcement learning settings. <div>
arXiv:2511.18597v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language and code generation, and are increasingly deployed as automatic judges of model outputs and learning activities. Yet, their behavior on structured tasks such as predicting the difficulty of competitive programming problems remains under-explored. We conduct a systematic comparison of GPT-4o, used purely as a natural-language difficulty assessor, against an interpretable Light-GBM ensemble trained on explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, LightGBM attains 86% accuracy, whereas GPT-4o reaches only 37.75%. Detailed analyses, including confusion matrices and SHAP-based interpretability, show that numeric constraints -- such as input size limits and acceptance rates -- play a crucial role in separating Hard problems from easier ones. By contrast, GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories. We further probe GPT-4o through a synthetic Hard-problem generation protocol. Surprisingly, GPT-4o labels almost all of its own synthetic Hard problems as Medium, contradicting its tendency to downgrade real Hard problems to Easy. Our findings connect to recent work on LLMs-as-judges and automatic difficulty estimation in programming and education, and highlight concrete failure modes that must be addressed before LLM-based judges can be considered trustworthy in competitive programming, educational platforms, or reinforcement-learning pipelines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark for Zero-Shot Belief Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2511.18616</link>
<guid>https://arxiv.org/abs/2511.18616</guid>
<content:encoded><![CDATA[
<div> Beliefs, large language models, predictive accuracy, belief domains, benchmark  

<br /><br />Summary:  
1. This paper addresses the challenge of evaluating how well large language models (LLMs) generalize across diverse belief domains, moving beyond the usual narrow sociopolitical contexts.  
2. The authors introduce a systematic and reproducible benchmark that tests LLMs' ability to predict individuals' stances on various topics in a zero-shot setting, using data collected from an online debate platform.  
3. The benchmark incorporates multiple informational conditions designed to separately assess the impact of demographic context and prior known beliefs on the models' predictive performance.  
4. Experiments across several small- to medium-sized LLMs demonstrate that providing more background information about an individual improves prediction accuracy, but performance varies significantly depending on the belief domain tested.  
5. These results highlight both the strengths and current limitations of LLMs in emulating human reasoning about beliefs and offer a scalable framework for modeling belief systems beyond strictly sociopolitical issues, advancing research on machine behavior. <div>
arXiv:2511.18616v1 Announce Type: new 
Abstract: Beliefs are central to how humans reason, communicate, and form social connections, yet most computational approaches to studying them remain confined to narrow sociopolitical contexts and rely on fine-tuning for optimal performance. Despite the growing use of large language models (LLMs) across disciplines, how well these systems generalize across diverse belief domains remains unclear. We introduce a systematic, reproducible benchmark that evaluates the ability of LLMs to predict individuals' stances on a wide range of topics in a zero-shot setting using data from an online debate platform. The benchmark includes multiple informational conditions that isolate the contribution of demographic context and known prior beliefs to predictive success. Across several small- to medium-sized models, we find that providing more background information about an individual improves predictive accuracy, but performance varies substantially across belief domains. These findings reveal both the capacity and limitations of current LLMs to emulate human reasoning, advancing the study of machine behavior and offering a scalable framework for modeling belief systems beyond the sociopolitical sphere.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News</title>
<link>https://arxiv.org/abs/2511.18618</link>
<guid>https://arxiv.org/abs/2511.18618</guid>
<content:encoded><![CDATA[
<div> Keywords: Bangla news classification, sentiment analysis, BERT-CNN-BiLSTM, transfer learning, imbalanced dataset<br /><br />Summary:  
This research focuses on Bangla news headline classification combined with sentiment analysis using advanced Natural Language Processing (NLP) techniques. The study introduces a hybrid transfer learning model called BERT-CNN-BiLSTM, designed to handle both headline categorization and sentiment detection simultaneously in Bengali newspapers. The experimental work is based on the BAN-ABSA dataset, containing 9,014 Bangla news headlines, marking the first attempt to address headline and sentiment classification together in this language domain. Two experimental strategies for dealing with imbalanced data were applied: Technique-1 uses undersampling and oversampling before splitting the dataset, and Technique-2 applies these methods after splitting. Technique-1’s oversampling approach achieved headline and sentiment classification accuracies of 78.57% and 73.43%, respectively, while Technique-2, trained directly on the original imbalanced data, yielded headline and sentiment accuracies of 81.37% and 64.46%. The proposed BERT-CNN-BiLSTM model outperforms baseline models significantly, setting new state-of-the-art results in Bangla news headline classification and sentiment analysis. The findings emphasize the importance of leveraging combined headline and sentiment datasets, providing a strong baseline for Bangla text classification in low-resource settings. <div>
arXiv:2511.18618v1 Announce Type: new 
Abstract: In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\% and 73.43\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\% and 64.46\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Optimization as a State-Space Search Problem</title>
<link>https://arxiv.org/abs/2511.18619</link>
<guid>https://arxiv.org/abs/2511.18619</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt optimization, state-space search, beam search, natural language processing, performance collapse<br /><br />Summary:<br /><br />Language models often experience significant performance degradation from minor changes in input prompts. Traditional libraries like DSpy address this issue using demonstration-based prompt optimization. This paper proposes an alternative method by framing prompt optimization as a classical state-space search problem, modeling the prompt space as a graph where nodes represent prompt states and edges denote transformations such as shortening, adding examples, or reordering content. The approach employs beam search and random walk algorithms to systematically explore and evaluate candidate prompts on development sets, pruning less promising branches. Experiments across five NLP tasks—sentiment classification, question answering, summarization, reasoning, and natural language inference—show that even shallow search configurations (beam width=2, depth=2) improve development set performance over seed prompts. For example, reasoning tasks saw development accuracy improvements from 0.40 to 0.80, although test set gains were smaller (0.20 to 0.50), indicating potential overfitting to the development heuristic. Analysis of successful optimization paths showed concise prompt transformations were favored, while verbosity-increasing operations were never selected. The study validates treating prompt optimization as a search problem and suggests that with more computational resources and better evaluation metrics, deeper search could yield more robust prompts that generalize better beyond development sets. Code is available at the linked GitHub repository. <div>
arXiv:2511.18619v1 Announce Type: new 
Abstract: Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph</title>
<link>https://arxiv.org/abs/2511.18622</link>
<guid>https://arxiv.org/abs/2511.18622</guid>
<content:encoded><![CDATA[
<div> OpenGloss, synthetic encyclopedic dictionary, semantic knowledge graph, procedural generation, foundation models  

<br /><br />Summary:  
OpenGloss is a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships into one unified resource. It includes 537,000 senses across 150,000 lexemes, comparable to WordNet 3.1 and Open English WordNet, but offers over four times as many sense definitions. The dataset features 9.1 million semantic edges, 1 million usage examples, 3 million collocations, and 60 million words of encyclopedic content. Generated via a multi-agent procedural generation pipeline utilizing schema-validated large language model outputs and automated quality assurance, the entire resource was produced in less than one week at a cost under $1,000. This highlights the potential of structured generation to create comprehensive lexical resources rapidly and affordably, which is not feasible by manual curation. OpenGloss addresses gaps in pedagogical tools by providing integrated content that supports vocabulary learning and natural language processing tasks, combining definitions, examples, collocations, encyclopedic entries, and etymology. As a synthetically generated dataset, it reflects both the strengths and current limitations of foundation models. The dataset is publicly available on Hugging Face under a CC-BY 4.0 license, enabling researchers and educators to build upon and modify the resource for various applications. <div>
arXiv:2511.18622v1 Announce Type: new 
Abstract: We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.
  Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.
  As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases</title>
<link>https://arxiv.org/abs/2511.18635</link>
<guid>https://arxiv.org/abs/2511.18635</guid>
<content:encoded><![CDATA[
<div> bias mitigation, large language models, cross-category bias, StereoSet, model coherence<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) often inherit societal biases from their training data, which can produce harmful or unfair outputs. <br />2. Existing bias mitigation techniques are typically evaluated only for the specific bias they aim to reduce, lacking analysis of their broader consequences. <br />3. This research investigates the cross-category effects of targeted bias mitigation by applying four different techniques across ten models from seven distinct model families.<br />4. The study focuses on biases related to race, religion, profession, and gender, measuring their impact using the StereoSet benchmark to assess both model coherence and stereotypical preferences.<br />5. Results reveal that while targeted bias mitigation can reduce bias in the intended category, it often causes unintended negative effects, such as increased bias in other categories and a reduction in the overall coherence of the models.<br />6. These findings highlight the necessity for comprehensive, multi-dimensional evaluation methods to effectively assess and develop bias mitigation strategies, preventing the inadvertent shifting or worsening of bias along unaddressed dimensions. <div>
arXiv:2511.18635v1 Announce Type: new 
Abstract: Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting</title>
<link>https://arxiv.org/abs/2511.18649</link>
<guid>https://arxiv.org/abs/2511.18649</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Mathematical Reasoning, Korean CSAT, Evaluation, GPT-5 Codex<br /><br />Summary:<br /><br />This study rigorously assesses the mathematical reasoning abilities of 24 state-of-the-art Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section. By digitizing all 46 exam questions within two hours of release, the researchers ensured a contamination-free environment, avoiding any data leakage or prior exposure in training sets. Evaluations covered multiple input modalities—text, image, and text with figures—and prompt languages including Korean and English. GPT-5 Codex achieved a perfect score with Korean text prompts, with Grok 4, GPT-5, and Deepseek R1 also scoring above 95 points. Remarkably, the smaller gpt-oss-20B model scored 95.7, indicating strong cost-effectiveness. Detailed analysis highlighted geometry as the weakest domain, especially on difficult 4-point problems, and text inputs consistently outperformed image inputs. The impact of prompt language varied depending on model size. Additional reasoning enhancement experiments on GPT-5 showed that increasing reasoning intensity improved accuracy but drastically increased token usage and lowered efficiency, suggesting minimal reasoning might be more practical for operational use. This work offers a completely unexposed real-exam evaluation framework, balancing performance, cost, and time, with detailed leaderboard results publicly accessible online. <div>
arXiv:2511.18649v1 Announce Type: new 
Abstract: This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (text, image, text+figure) and prompt languages (Korean, English).
  GPT-5 Codex achieved the only perfect score (100 points) with text input and Korean prompts, while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed geometry as the weakest domain (77.7% average) with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale.
  In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (from 82.6 to 100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a real-exam-based LLM assessment framework, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard (https://isoft.cnu.ac.kr/csat2026/).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning</title>
<link>https://arxiv.org/abs/2511.18659</link>
<guid>https://arxiv.org/abs/2511.18659</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, Continuous Latent Reasoning, embedding compression, joint optimization, differentiable top-k estimator<br /><br />Summary: This paper introduces CLaRa (Continuous Latent Reasoning), a novel framework designed to improve Retrieval-Augmented Generation (RAG) by addressing challenges related to long contexts and the separation between retrieval and generation processes. CLaRa unifies embedding-based compression and joint optimization within a shared continuous space, enhancing the integration of knowledge retrieval and generation. To produce semantically rich and easily retrievable compressed vectors, the authors propose SCP, a key-preserving data synthesis method leveraging question-answer and paraphrase supervision. The framework enables end-to-end training of the reranker and generator components using a single language modeling loss, facilitated by a differentiable top-k estimator that allows gradient flow through both modules. This approach theoretically aligns retrieval relevance directly with answer quality, ensuring more coherent and relevant outputs. Extensive experiments conducted on multiple question-answering benchmarks demonstrate that CLaRa achieves state-of-the-art performance in both compression efficiency and reranking accuracy. Moreover, CLaRa often surpasses existing text-based fine-tuned models, indicating its effectiveness in optimizing retrieval-augmented language models holistically and continuously.<br /><br /> <div>
arXiv:2511.18659v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2511.18696</link>
<guid>https://arxiv.org/abs/2511.18696</guid>
<content:encoded><![CDATA[
<div> Empathy, Large Language Models, Multi-stage Prompting, Emotional Resonance, Conversational AI<br /><br />Summary:<br /><br />This report introduces the Empathetic Cascading Networks (ECN) framework, a novel multi-stage prompting method aimed at improving the empathetic and inclusive response capabilities of large language models. The ECN framework consists of four distinct stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, each designed to guide models in producing emotionally and contextually aware outputs. Experimental evaluations show that ECN significantly enhances the Empathy Quotient (EQ) scores when implemented with state-of-the-art models such as GPT-3.5-turbo and GPT-4. Besides improving empathy, ECN maintains strong performance on important metrics like Regard and Perplexity, indicating that the quality and coherence of responses remain competitive. The findings suggest that ECN can be effectively applied to conversational AI systems where empathy and inclusivity are critical, paving the way for more emotionally intelligent and human-like AI interactions. Overall, ECN offers a structured approach to embedding empathetic understanding in language model outputs, which has broad implications for enhancing user engagement and satisfaction in AI-driven communication platforms. <div>
arXiv:2511.18696v1 Announce Type: new 
Abstract: This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context</title>
<link>https://arxiv.org/abs/2511.18743</link>
<guid>https://arxiv.org/abs/2511.18743</guid>
<content:encoded><![CDATA[
<div> Keywords: RhinoInsight, large language models, deep research, verifiable checklist, evidence audit<br /><br />Summary: RhinoInsight is a novel deep research framework designed to enhance the robustness, traceability, and quality of outputs generated by large language models (LLMs) without requiring parameter updates. Unlike prevailing systems that rely on a linear pipeline prone to error accumulation and context degradation, RhinoInsight introduces two key control mechanisms. First, the Verifiable Checklist module converts user requirements into clear, traceable, and verifiable sub-goals, allowing for human or LLM critics to iteratively refine these goals and compile a hierarchical outline that guides subsequent actions and prevents infeasible plans. Second, the Evidence Audit module systematically organizes search content, incrementally updates the outline, and removes noisy or irrelevant context. It further employs a critic to rank and firmly associate high-quality evidence with the drafted content to ensure verifiability and reduce hallucinations by the model. Experimental results demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks, showing significant improvements in structured reasoning and information reliability, while also maintaining competitive effectiveness on deep search tasks. This approach addresses critical limitations of traditional linear pipelines by enabling explicit control over both the model’s behavior and context management during complex research workflows. <div>
arXiv:2511.18743v1 Announce Type: new 
Abstract: Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search</title>
<link>https://arxiv.org/abs/2511.18749</link>
<guid>https://arxiv.org/abs/2511.18749</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Fact-Checking, Reasoning, Web Search, Retrieval-Augmented Generation<br /><br />Summary:<br /><br />1. The study evaluates 15 recent large language models (LLMs) from major providers including OpenAI, Google, Meta, and DeepSeek on their ability to perform automated fact-checking. <br />2. More than 6,000 claims previously fact-checked by the PolitiFact platform serve as the benchmark for this evaluation.<br />3. Results show that standard LLMs perform poorly on fact-checking tasks, with reasoning capabilities offering only minimal improvements.<br />4. Integration of web search functions with LLMs yields moderate gains, despite the fact that relevant fact-checks are readily available online.<br />5. A retrieval-augmented generation (RAG) system using curated PolitiFact summaries significantly improves performance, increasing macro F1 scores by an average of 233% across different model variants.<br /><br />These findings highlight the limitations of relying solely on standalone LLMs or simple web search augmentation for fact-checking. Instead, providing models with direct access to high-quality, curated context is identified as a more effective approach for automated fact verification. This suggests future fact-checking systems should incorporate retrieval-based methods leveraging expert summaries to enhance accuracy and reliability. <div>
arXiv:2511.18749v1 Announce Type: new 
Abstract: Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion</title>
<link>https://arxiv.org/abs/2511.18751</link>
<guid>https://arxiv.org/abs/2511.18751</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal sentiment analysis, image-text pairs, low-quality modalities, missing modalities, feature recovery<br /><br />Summary:<br /><br />This paper addresses the challenge of robust multimodal sentiment analysis when dealing with image-text pairs that may contain low-quality or missing modalities, common issues in real-world social media data. The authors propose a novel method called Distribution-based feature Recovery and Fusion (DRF) designed to handle these problems within a unified framework. DRF maintains a feature queue for each modality to approximate their feature distributions, enabling it to quantitatively assess the quality of each modality and reduce the influence of low-quality data during fusion. For missing modalities, the method constructs inter-modal mapping relationships learned through both samples and feature distributions, allowing it to recover the absent modality information from available data. The robustness and effectiveness of DRF are validated through experiments that simulate low-quality and missing data scenarios via disruption strategies on three publicly available image-text datasets. The results demonstrate consistent performance improvements over state-of-the-art methods, showcasing DRF's universal applicability and resilience in practical multimodal sentiment analysis tasks. This study highlights the importance of accounting for modality quality and absence to build more reliable sentiment analysis models. <div>
arXiv:2511.18751v1 Announce Type: new 
Abstract: As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Whisper for Arabic ASR Under Linguistic Varieties</title>
<link>https://arxiv.org/abs/2511.18774</link>
<guid>https://arxiv.org/abs/2511.18774</guid>
<content:encoded><![CDATA[
<div> Arabic ASR, Whisper, context-aware prompting, zero-shot adaptation, dialectal speech<br /><br />Summary:<br /><br />Low-resource automatic speech recognition (ASR) for Arabic presents challenges due to its extensive dialectal variation and scarcity of labeled data. This work introduces context-aware prompting techniques to adapt OpenAI’s Whisper model for Arabic ASR without the need for retraining. The proposed methods leverage decoder prompting by incorporating first-pass transcriptions or retrieved similar utterances, as well as encoder prefixing using synthesized speech in the target speaker’s voice to enhance recognition accuracy. Novel strategies such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval categorized by lexical, semantic, and acoustic information are employed to improve transcription quality in realistic zero-shot scenarios. The approach is evaluated across nine different Arabic linguistic conditions, demonstrating effectiveness on both Modern Standard Arabic and various dialects. Results show a significant reduction in word error rate (WER), achieving up to a 22.3% decrease for Modern Standard Arabic and a 9.2% reduction for dialectal speech. Additionally, the method substantially mitigates common problems such as hallucinations and speaker mismatch, making it a practical solution for improving low-resource ASR performance in diverse Arabic speech contexts. <div>
arXiv:2511.18774v1 Announce Type: new 
Abstract: Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations</title>
<link>https://arxiv.org/abs/2511.18808</link>
<guid>https://arxiv.org/abs/2511.18808</guid>
<content:encoded><![CDATA[
arXiv:2511.18808v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept than Document: Context Compression via AMR-based Conceptual Entropy</title>
<link>https://arxiv.org/abs/2511.18832</link>
<guid>https://arxiv.org/abs/2511.18832</guid>
<content:encoded><![CDATA[
arXiv:2511.18832v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis</title>
<link>https://arxiv.org/abs/2511.18843</link>
<guid>https://arxiv.org/abs/2511.18843</guid>
<content:encoded><![CDATA[
arXiv:2511.18843v1 Announce Type: new 
Abstract: Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for the Summarization of Czech Documents: From History to the Present</title>
<link>https://arxiv.org/abs/2511.18848</link>
<guid>https://arxiv.org/abs/2511.18848</guid>
<content:encoded><![CDATA[
arXiv:2511.18848v1 Announce Type: new 
Abstract: Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.
  In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od \v{C}erchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.
  By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Alpha Mining via LLM-Driven Code-Based Evolution</title>
<link>https://arxiv.org/abs/2511.18850</link>
<guid>https://arxiv.org/abs/2511.18850</guid>
<content:encoded><![CDATA[
arXiv:2511.18850v1 Announce Type: new 
Abstract: Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FanarGuard: A Culturally-Aware Moderation Filter for Arabic Language Models</title>
<link>https://arxiv.org/abs/2511.18852</link>
<guid>https://arxiv.org/abs/2511.18852</guid>
<content:encoded><![CDATA[
arXiv:2511.18852v1 Announce Type: new 
Abstract: Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce FanarGuard, a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising over 1k norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish FanarGuard as a practical step toward more context-sensitive safeguards.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Reading Comprehension Exercises with Large Language Models for Educational Applications</title>
<link>https://arxiv.org/abs/2511.18860</link>
<guid>https://arxiv.org/abs/2511.18860</guid>
<content:encoded><![CDATA[
arXiv:2511.18860v1 Announce Type: new 
Abstract: With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.18864</link>
<guid>https://arxiv.org/abs/2511.18864</guid>
<content:encoded><![CDATA[
arXiv:2511.18864v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning benchmarks. However, their long chain-of-thought reasoning processes incur significant inference overhead. Pruning has emerged as a promising approach to reducing computational costs. However, existing efforts have primarily focused on large language models (LLMs), while pruning LRMs remains unexplored. In this work, we conduct the first empirical study on pruning LRMs and show that directly applying existing pruning techniques fails to yield satisfactory results. Our findings indicate that using self-generated reasoning data for calibration can substantially improve pruning performance. We further investigate how the difficulty and length of reasoning data affect pruning outcomes. Our analysis reveals that challenging and moderately long self-generated reasoning data serve as ideal calibration data. Based on these insights, we propose a Selective Self-Generated Reasoning (SSGR) data construction strategy to provide effective calibration data for pruning LRMs. Experimental results on the DeepSeek-R1-Distill model series validate that our strategy improves the reasoning ability of pruned LRMs by 10%-13% compared to general pruning methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation</title>
<link>https://arxiv.org/abs/2511.18889</link>
<guid>https://arxiv.org/abs/2511.18889</guid>
<content:encoded><![CDATA[
arXiv:2511.18889v1 Announce Type: new 
Abstract: Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \textbf{CoreEval}, a \textbf{Co}ntamination-\textbf{re}silient \textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reproducibility Study of Large Language Model Bayesian Optimization</title>
<link>https://arxiv.org/abs/2511.18891</link>
<guid>https://arxiv.org/abs/2511.18891</guid>
<content:encoded><![CDATA[
arXiv:2511.18891v1 Announce Type: new 
Abstract: In this reproducibility study, we revisit the LLAMBO framework of Daxberger et al. (2024), a prompting-based Bayesian optimization (BO) method that uses large language models as discriminative surrogates and acquisition optimizers via text-only interactions. We replicate the core Bayesmark and HPOBench experiments under the original evaluation protocol, but replace GPT-3.5 with the open-weight Llama 3.1 70B model used for all text encoding components.
  Our results broadly confirm the main claims of LLAMBO. Contextual warm starting via textual problem and hyperparameter descriptions substantially improves early regret behaviour and reduces variance across runs. LLAMBO's discriminative surrogate is weaker than GP or SMAC as a pure single task regressor, yet benefits from cross task semantic priors induced by the language model. Ablations that remove textual context markedly degrade predictive accuracy and calibration, while the LLAMBO candidate sampler consistently generates higher quality and more diverse proposals than TPE or random sampling. Experiments with smaller backbones (Gemma 27B, Llama 3.1 8B) yield unstable or invalid predictions, suggesting insufficient capacity for reliable surrogate behaviour.
  Overall, our study shows that the LLAMBO architecture is robust to changing the language model backbone and remains effective when instantiated with Llama 3.1 70B.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs</title>
<link>https://arxiv.org/abs/2511.18931</link>
<guid>https://arxiv.org/abs/2511.18931</guid>
<content:encoded><![CDATA[
arXiv:2511.18931v1 Announce Type: new 
Abstract: Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skeletons Matter: Dynamic Data Augmentation for Text-to-Query</title>
<link>https://arxiv.org/abs/2511.18934</link>
<guid>https://arxiv.org/abs/2511.18934</guid>
<content:encoded><![CDATA[
arXiv:2511.18934v1 Announce Type: new 
Abstract: The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials</title>
<link>https://arxiv.org/abs/2511.18937</link>
<guid>https://arxiv.org/abs/2511.18937</guid>
<content:encoded><![CDATA[
arXiv:2511.18937v1 Announce Type: new 
Abstract: We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logic of Montage</title>
<link>https://arxiv.org/abs/2511.19063</link>
<guid>https://arxiv.org/abs/2511.19063</guid>
<content:encoded><![CDATA[
arXiv:2511.19063v1 Announce Type: new 
Abstract: In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form "Effect of Contradictory Structure." "Effect of Contradictory Structure" is not static but dynamic. Effect in "Effect of Contradictory Structure" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, "Effect of Contradictory Structure" can be overlapped with each other. This overlapping operation is called "montage." A broader "Structure" that includes related "Effect of Contradictory Structure" and "Effect of Structure" are set up. Montage produces "Effect of Structure". In montage, it is necessary to set something like "strength," so we adopted Deleuze and Deleuze/Guattari's word "intensity" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of "intensity" through Austin's use of the word "force." "Effect of Structure" process is demonstrated using the example of proceeding to the next level of education.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning</title>
<link>https://arxiv.org/abs/2511.19078</link>
<guid>https://arxiv.org/abs/2511.19078</guid>
<content:encoded><![CDATA[
arXiv:2511.19078v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis</title>
<link>https://arxiv.org/abs/2511.19083</link>
<guid>https://arxiv.org/abs/2511.19083</guid>
<content:encoded><![CDATA[
arXiv:2511.19083v1 Announce Type: new 
Abstract: In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeCoRL: Decoupling Reasoning Chains via Parallel Sub-Step Generation and Cascaded Reinforcement for Interpretable and Scalable RLHF</title>
<link>https://arxiv.org/abs/2511.19097</link>
<guid>https://arxiv.org/abs/2511.19097</guid>
<content:encoded><![CDATA[
arXiv:2511.19097v1 Announce Type: new 
Abstract: Existing reinforcement learning methods for Chain-of-Thought reasoning suffer from two critical limitations. First, they operate as monolithic black boxes that provide undifferentiated reward signals, obscuring individual step contributions and hindering error diagnosis. Second, sequential decoding has O(n) time complexity. This makes real-time deployment impractical for complex reasoning tasks. We present DeCoRL (Decoupled Reasoning Chains via Coordinated Reinforcement Learning), a novel framework that transforms reasoning from sequential processing into collaborative modular orchestration. DeCoRL trains lightweight specialized models to generate reasoning sub-steps concurrently, eliminating sequential bottlenecks through parallel processing. To enable precise error attribution, the framework designs modular reward functions that score each sub-step independently. Cascaded DRPO optimization then coordinates these rewards while preserving inter-step dependencies. Comprehensive evaluation demonstrates state-of-the-art results across RM-Bench, RMB, and RewardBench, outperforming existing methods including large-scale models. DeCoRL delivers 3.8 times faster inference while maintaining superior solution quality and offers a 22.7\% improvement in interpretability through explicit reward attribution. These advancements, combined with a 72.4\% reduction in energy consumption and a 68\% increase in throughput, make real-time deployment of complex reasoning systems a reality.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A symbolic Perl algorithm for the unification of Nahuatl word spellings</title>
<link>https://arxiv.org/abs/2511.19118</link>
<guid>https://arxiv.org/abs/2511.19118</guid>
<content:encoded><![CDATA[
arXiv:2511.19118v1 Announce Type: new 
Abstract: In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $\pi$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Optimality of Discrete Object Naming: a Kinship Case Study</title>
<link>https://arxiv.org/abs/2511.19120</link>
<guid>https://arxiv.org/abs/2511.19120</guid>
<content:encoded><![CDATA[
arXiv:2511.19120v1 Announce Type: new 
Abstract: The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis</title>
<link>https://arxiv.org/abs/2511.19122</link>
<guid>https://arxiv.org/abs/2511.19122</guid>
<content:encoded><![CDATA[
arXiv:2511.19122v1 Announce Type: new 
Abstract: Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization</title>
<link>https://arxiv.org/abs/2511.19131</link>
<guid>https://arxiv.org/abs/2511.19131</guid>
<content:encoded><![CDATA[
arXiv:2511.19131v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representational Stability of Truth in Large Language Models</title>
<link>https://arxiv.org/abs/2511.19166</link>
<guid>https://arxiv.org/abs/2511.19166</guid>
<content:encoded><![CDATA[
arXiv:2511.19166v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely used for factual tasks such as "What treats asthma?" or "What is the capital of Latvia?". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\leq 8.2\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations</title>
<link>https://arxiv.org/abs/2511.19232</link>
<guid>https://arxiv.org/abs/2511.19232</guid>
<content:encoded><![CDATA[
arXiv:2511.19232v1 Announce Type: new 
Abstract: How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset</title>
<link>https://arxiv.org/abs/2511.19317</link>
<guid>https://arxiv.org/abs/2511.19317</guid>
<content:encoded><![CDATA[
arXiv:2511.19317v1 Announce Type: new 
Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces</title>
<link>https://arxiv.org/abs/2511.19333</link>
<guid>https://arxiv.org/abs/2511.19333</guid>
<content:encoded><![CDATA[
arXiv:2511.19333v1 Announce Type: new 
Abstract: Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research</title>
<link>https://arxiv.org/abs/2511.19399</link>
<guid>https://arxiv.org/abs/2511.19399</guid>
<content:encoded><![CDATA[
arXiv:2511.19399v1 Announce Type: new 
Abstract: Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2511.19417</link>
<guid>https://arxiv.org/abs/2511.19417</guid>
<content:encoded><![CDATA[
arXiv:2511.19417v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Machine Learning for Aphasic Discourse Analysis</title>
<link>https://arxiv.org/abs/2511.17553</link>
<guid>https://arxiv.org/abs/2511.17553</guid>
<content:encoded><![CDATA[
arXiv:2511.17553v1 Announce Type: cross 
Abstract: Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Rubric: Cultural Misalignment in LLM Benchmarks for Sexual and Reproductive Health</title>
<link>https://arxiv.org/abs/2511.17554</link>
<guid>https://arxiv.org/abs/2511.17554</guid>
<content:encoded><![CDATA[
arXiv:2511.17554v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been positioned as having the potential to expand access to health information in the Global South, yet their evaluation remains heavily dependent on benchmarks designed around Western norms. We present insights from a preliminary benchmarking exercise with a chatbot for sexual and reproductive health (SRH) for an underserved community in India. We evaluated using HealthBench, a benchmark for conversational health models by OpenAI. We extracted 637 SRH queries from the dataset and evaluated on the 330 single-turn conversations. Responses were evaluated using HealthBench's rubric-based automated grader, which rated responses consistently low. However, qualitative analysis by trained annotators and public health experts revealed that many responses were actually culturally appropriate and medically accurate. We highlight recurring issues, particularly a Western bias, such as for legal framing and norms (e.g., breastfeeding in public), diet assumptions (e.g., fish safe to eat during pregnancy), and costs (e.g., insurance models). Our findings demonstrate the limitations of current benchmarks in capturing the effectiveness of systems built for different cultural and healthcare contexts. We argue for the development of culturally adaptive evaluation frameworks that meet quality standards while recognizing needs of diverse populations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech Recognition Model Improves Text-to-Speech Synthesis using Fine-Grained Reward</title>
<link>https://arxiv.org/abs/2511.17555</link>
<guid>https://arxiv.org/abs/2511.17555</guid>
<content:encoded><![CDATA[
arXiv:2511.17555v1 Announce Type: cross 
Abstract: Recent advances in text-to-speech (TTS) have enabled models to clone arbitrary unseen speakers and synthesize high-quality, natural-sounding speech. However, evaluation methods lag behind: typical mean opinion score (MOS) estimators perform regression over entire utterances, while failures usually occur in a few problematic words. We observe that encoder-decoder ASR models (e.g., Whisper) surface word-level mismatches between speech and text via cross-attention, providing a fine-grained reward signal. Building on this, we introduce Word-level TTS Alignment by ASR-driven Attentive Reward (W3AR). Without explicit reward annotations, W3AR uses attention from a pre-trained ASR model to drive finer-grained alignment and optimization of sequences predicted by a TTS model. Experiments show that W3AR improves the quality of existing TTS systems and strengthens zero-shot robustness on unseen speakers. More broadly, our results suggest a simple recipe for generative modeling: understanding models can act as evaluators, delivering informative, fine-grained feedback for optimization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.17577</link>
<guid>https://arxiv.org/abs/2511.17577</guid>
<content:encoded><![CDATA[
arXiv:2511.17577v1 Announce Type: cross 
Abstract: With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection</title>
<link>https://arxiv.org/abs/2511.17589</link>
<guid>https://arxiv.org/abs/2511.17589</guid>
<content:encoded><![CDATA[
arXiv:2511.17589v1 Announce Type: cross 
Abstract: This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Structured Data Extraction from Perspectively Distorted Documents</title>
<link>https://arxiv.org/abs/2511.17607</link>
<guid>https://arxiv.org/abs/2511.17607</guid>
<content:encoded><![CDATA[
arXiv:2511.17607v1 Announce Type: cross 
Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Competition to Coordination: Market Making as a Scalable Framework for Safe and Aligned Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2511.17621</link>
<guid>https://arxiv.org/abs/2511.17621</guid>
<content:encoded><![CDATA[
arXiv:2511.17621v1 Announce Type: cross 
Abstract: As foundation models are increasingly deployed as interacting agents in multi-agent systems, their collective behavior raises new challenges for trustworthiness, transparency, and accountability. Traditional coordination mechanisms, such as centralized oversight or adversarial adjudication, struggle to scale and often obscure how decisions emerge. We introduce a market-making framework for multi-agent large language model (LLM) coordination that organizes agent interactions as structured economic exchanges. In this setup, each agent acts as a market participant, updating and trading probabilistic beliefs, to converge toward shared, truthful outcomes. By aligning local incentives with collective epistemic goals, the framework promotes self-organizing, verifiable reasoning without requiring external enforcement. Empirically, we evaluate this approach across factual reasoning, ethical judgment, and commonsense inference tasks. Market-based coordination yields accuracy gains of up to 10% over single-shot baselines while preserving interpretability and transparency of intermediate reasoning steps. Beyond these improvements, our findings demonstrate that economic coordination principles can operationalize accountability and robustness in multi-agent LLM systems, offering a scalable pathway toward self-correcting, socially responsible AI capable of maintaining trust and oversight in real world deployment scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PocketLLM: Ultimate Compression of Large Language Models via Meta Networks</title>
<link>https://arxiv.org/abs/2511.17637</link>
<guid>https://arxiv.org/abs/2511.17637</guid>
<content:encoded><![CDATA[
arXiv:2511.17637v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MURMUR: Using cross-user chatter to break collaborative language agents in groups</title>
<link>https://arxiv.org/abs/2511.17671</link>
<guid>https://arxiv.org/abs/2511.17671</guid>
<content:encoded><![CDATA[
arXiv:2511.17671v1 Announce Type: cross 
Abstract: Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop</title>
<link>https://arxiv.org/abs/2511.17673</link>
<guid>https://arxiv.org/abs/2511.17673</guid>
<content:encoded><![CDATA[
arXiv:2511.17673v1 Announce Type: cross 
Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents. Code: https://github.com/enkiluv/scl-core-experiment Demo: https://scl-travel-planner.streamlit.app/
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment</title>
<link>https://arxiv.org/abs/2511.17676</link>
<guid>https://arxiv.org/abs/2511.17676</guid>
<content:encoded><![CDATA[
arXiv:2511.17676v1 Announce Type: cross 
Abstract: The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics. Traditional database applications and system deployment are fundamentally impacted by AI-driven tools, such as Retrieval-Augmented Generation (RAG) and vector database technologies, which provide new pathways for semantic querying over enterprise knowledge bases. In the meantime, data security and compliance are top priorities for organizations adopting AI technologies. For enterprise data analysis, SQL generations powered by large language models (LLMs) and AI agents, has emerged as a key bridge connecting natural language with structured data, effectively lowering the barrier to enterprise data access and improving analytical efficiency. This paper focuses on enterprise data analysis applications and system deployment, covering a range of innovative frameworks, enabling complex query understanding, multi-agent collaboration, security verification, and computational efficiency. Through representative use cases, key challenges related to distributed deployment, data security, and inherent difficulties in SQL generation tasks are discussed.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Cross-Cultural Assessment of Human Ability to Detect LLM-Generated Fake News about South Africa</title>
<link>https://arxiv.org/abs/2511.17682</link>
<guid>https://arxiv.org/abs/2511.17682</guid>
<content:encoded><![CDATA[
arXiv:2511.17682v1 Announce Type: cross 
Abstract: This study investigates how cultural proximity affects the ability to detect AI-generated fake news by comparing South African participants with those from other nationalities. As large language models increasingly enable the creation of sophisticated fake news, understanding human detection capabilities becomes crucial, particularly across different cultural contexts. We conducted a survey where 89 participants (56 South Africans, 33 from other nationalities) evaluated 10 true South African news articles and 10 AI-generated fake versions. Results reveal an asymmetric pattern: South Africans demonstrated superior performance in detecting true news about their country (40% deviation from ideal rating) compared to other participants (52%), but performed worse at identifying fake news (62% vs. 55%). This difference may reflect South Africans' higher overall trust in news sources. Our analysis further shows that South Africans relied more on content knowledge and contextual understanding when judging credibility, while participants from other countries emphasised formal linguistic features such as grammar and structure. Overall, the deviation from ideal rating was similar between groups (51% vs. 53%), suggesting that cultural familiarity appears to aid verification of authentic information but may also introduce bias when evaluating fabricated content. These insights contribute to understanding cross-cultural dimensions of misinformation detection and inform strategies for combating AI-generated fake news in increasingly globalised information ecosystems where content crosses cultural and geographical boundaries.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams</title>
<link>https://arxiv.org/abs/2511.17693</link>
<guid>https://arxiv.org/abs/2511.17693</guid>
<content:encoded><![CDATA[
arXiv:2511.17693v1 Announce Type: cross 
Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch</title>
<link>https://arxiv.org/abs/2511.17826</link>
<guid>https://arxiv.org/abs/2511.17826</guid>
<content:encoded><![CDATA[
arXiv:2511.17826v1 Announce Type: cross 
Abstract: Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA</title>
<link>https://arxiv.org/abs/2511.17886</link>
<guid>https://arxiv.org/abs/2511.17886</guid>
<content:encoded><![CDATA[
arXiv:2511.17886v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers</title>
<link>https://arxiv.org/abs/2511.18036</link>
<guid>https://arxiv.org/abs/2511.18036</guid>
<content:encoded><![CDATA[
arXiv:2511.18036v1 Announce Type: cross 
Abstract: The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment</title>
<link>https://arxiv.org/abs/2511.18055</link>
<guid>https://arxiv.org/abs/2511.18055</guid>
<content:encoded><![CDATA[
arXiv:2511.18055v1 Announce Type: cross 
Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Labeled Markov Chains: A Cantor-Kantorovich Approach</title>
<link>https://arxiv.org/abs/2511.18103</link>
<guid>https://arxiv.org/abs/2511.18103</guid>
<content:encoded><![CDATA[
arXiv:2511.18103v1 Announce Type: cross 
Abstract: Labeled Markov Chains (or LMCs for short) are useful mathematical objects to model complex probabilistic languages. A central challenge is to compare two LMCs, for example to assess the accuracy of an abstraction or to quantify the effect of model perturbations. In this work, we study the recently introduced Cantor-Kantorovich (or CK) distance. In particular we show that the latter can be framed as a discounted sum of finite-horizon Total Variation distances, making it an instance of discounted linear distance, but arising from the natural Cantor topology. Building on the latter observation, we analyze the properties of the CK distance along three dimensions: computational complexity, continuity properties and approximation. More precisely, we show that the exact computation of the CK distance is #P-hard. We also provide an upper bound on the CK distance as a function of the approximation relation between the two LMCs, and show that a bounded CK distance implies a bounded error between probabilities of finite-horizon traces. Finally, we provide a computable approximation scheme, and show that the latter is also #P-hard. Altogether, our results provide a rigorous theoretical foundation for the CK distance and clarify its relationship with existing distances.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.18123</link>
<guid>https://arxiv.org/abs/2511.18123</guid>
<content:encoded><![CDATA[
arXiv:2511.18123v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\textbf{S}$ubspace $\textbf{P}$rojection $\textbf{D}$ebiasing ($\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent Software Development Systems</title>
<link>https://arxiv.org/abs/2511.18467</link>
<guid>https://arxiv.org/abs/2511.18467</guid>
<content:encoded><![CDATA[
arXiv:2511.18467v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Model (LLM)-driven multi-agent systems has significantly streamlined software developing tasks, enabling users with little technical expertise to develop executable applications. While these systems democratize software creation through natural language requirements, they introduce significant security risks that remain largely unexplored. We identify two risky scenarios: Malicious User with Benign Agents (MU-BA) and Benign User with Malicious Agents (BU-MA). We introduce the Implicit Malicious Behavior Injection Attack (IMBIA), demonstrating how multi-agent systems can be manipulated to generate software with concealed malicious capabilities beneath seemingly benign applications, and propose Adv-IMBIA as a defense mechanism. Evaluations across ChatDev, MetaGPT, and AgentVerse frameworks reveal varying vulnerability patterns, with IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios. Our defense mechanism reduced attack success rates significantly, particularly in the MU-BA scenario. Further analysis reveals that compromised agents in the coding and testing phases pose significantly greater security risks, while also identifying critical agents that require protection against malicious user exploitation. Our findings highlight the urgent need for robust security measures in multi-agent software development systems and provide practical guidelines for implementing targeted, resource-efficient defensive strategies.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstructAudio: Unified speech and music generation with natural language instruction</title>
<link>https://arxiv.org/abs/2511.18487</link>
<guid>https://arxiv.org/abs/2511.18487</guid>
<content:encoded><![CDATA[
arXiv:2511.18487v1 Announce Type: cross 
Abstract: Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation. TTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis. Despite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions. We introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese. The model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment. Fig. 1 visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics. To our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: https://qiangchunyu.github.io/InstructAudio/
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence</title>
<link>https://arxiv.org/abs/2511.18538</link>
<guid>https://arxiv.org/abs/2511.18538</guid>
<content:encoded><![CDATA[
arXiv:2511.18538v1 Announce Type: cross 
Abstract: Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Majority of the Bests: Improving Best-of-N via Bootstrapping</title>
<link>https://arxiv.org/abs/2511.18630</link>
<guid>https://arxiv.org/abs/2511.18630</guid>
<content:encoded><![CDATA[
arXiv:2511.18630v1 Announce Type: cross 
Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the alignment between infants' visual and linguistic experience using multimodal language models</title>
<link>https://arxiv.org/abs/2511.18824</link>
<guid>https://arxiv.org/abs/2511.18824</guid>
<content:encoded><![CDATA[
arXiv:2511.18824v1 Announce Type: cross 
Abstract: Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., "look at the ball" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining</title>
<link>https://arxiv.org/abs/2511.18903</link>
<guid>https://arxiv.org/abs/2511.18903</guid>
<content:encoded><![CDATA[
arXiv:2511.18903v1 Announce Type: cross 
Abstract: Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression</title>
<link>https://arxiv.org/abs/2511.18936</link>
<guid>https://arxiv.org/abs/2511.18936</guid>
<content:encoded><![CDATA[
arXiv:2511.18936v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification EM-PCA for clustering and embedding</title>
<link>https://arxiv.org/abs/2511.18992</link>
<guid>https://arxiv.org/abs/2511.18992</guid>
<content:encoded><![CDATA[
arXiv:2511.18992v1 Announce Type: cross 
Abstract: The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation</title>
<link>https://arxiv.org/abs/2511.19009</link>
<guid>https://arxiv.org/abs/2511.19009</guid>
<content:encoded><![CDATA[
arXiv:2511.19009v1 Announce Type: cross 
Abstract: Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation</title>
<link>https://arxiv.org/abs/2511.19149</link>
<guid>https://arxiv.org/abs/2511.19149</guid>
<content:encoded><![CDATA[
arXiv:2511.19149v1 Announce Type: cross 
Abstract: This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning</title>
<link>https://arxiv.org/abs/2511.19168</link>
<guid>https://arxiv.org/abs/2511.19168</guid>
<content:encoded><![CDATA[
arXiv:2511.19168v1 Announce Type: cross 
Abstract: Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Nutrition Multimodal Photoplethysmography Language Model</title>
<link>https://arxiv.org/abs/2511.19260</link>
<guid>https://arxiv.org/abs/2511.19260</guid>
<content:encoded><![CDATA[
arXiv:2511.19260v1 Announce Type: cross 
Abstract: Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CDLM: Consistency Diffusion Language Models For Faster Sampling</title>
<link>https://arxiv.org/abs/2511.19269</link>
<guid>https://arxiv.org/abs/2511.19269</guid>
<content:encoded><![CDATA[
arXiv:2511.19269v1 Announce Type: cross 
Abstract: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings</title>
<link>https://arxiv.org/abs/2511.19279</link>
<guid>https://arxiv.org/abs/2511.19279</guid>
<content:encoded><![CDATA[
arXiv:2511.19279v1 Announce Type: cross 
Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning</title>
<link>https://arxiv.org/abs/2511.19304</link>
<guid>https://arxiv.org/abs/2511.19304</guid>
<content:encoded><![CDATA[
arXiv:2511.19304v1 Announce Type: cross 
Abstract: Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRInTS: Reward Modeling for Long-Horizon Information Seeking</title>
<link>https://arxiv.org/abs/2511.19314</link>
<guid>https://arxiv.org/abs/2511.19314</guid>
<content:encoded><![CDATA[
arXiv:2511.19314v1 Announce Type: cross 
Abstract: Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models</title>
<link>https://arxiv.org/abs/2511.19324</link>
<guid>https://arxiv.org/abs/2511.19324</guid>
<content:encoded><![CDATA[
arXiv:2511.19324v1 Announce Type: cross 
Abstract: Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval</title>
<link>https://arxiv.org/abs/2511.19325</link>
<guid>https://arxiv.org/abs/2511.19325</guid>
<content:encoded><![CDATA[
arXiv:2511.19325v1 Announce Type: cross 
Abstract: Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric</title>
<link>https://arxiv.org/abs/2511.19350</link>
<guid>https://arxiv.org/abs/2511.19350</guid>
<content:encoded><![CDATA[
arXiv:2511.19350v1 Announce Type: cross 
Abstract: Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in translation: using global fact-checks to measure multilingual misinformation prevalence, spread, and evolution</title>
<link>https://arxiv.org/abs/2310.18089</link>
<guid>https://arxiv.org/abs/2310.18089</guid>
<content:encoded><![CDATA[
arXiv:2310.18089v2 Announce Type: replace 
Abstract: Misinformation and disinformation are growing threats in the digital age, affecting people across languages and borders. However, no research has investigated the prevalence of multilingual misinformation and quantified the extent to which misinformation diffuses across languages. This paper investigates the prevalence and dynamics of multilingual misinformation through an analysis of 264,487 fact-checks spanning 95 languages. To study the evolution of claims over time and mutations across languages, we represent fact-checks with multilingual sentence embeddings and build a graph where semantically similar claims are linked. We provide quantitative evidence of repeated fact-checking efforts and establish that claims diffuse across languages. Specifically, we find that while the majority of misinformation claims are only fact-checked once, 10.26%, corresponding to more than 27,000 claims, are checked multiple times. Using fact-checks as a proxy for the spread of misinformation, we find 32.26% of repeated claims cross linguistic boundaries, suggesting that some misinformation permeates language barriers. However, spreading patterns exhibit strong assortativity, with misinformation more likely to spread within the same language or language family. Next we show that fact-checkers take more time to fact-check claims that have crossed language barriers and model the temporal and cross-lingual evolution of claims. We analyze connected components and shortest paths connecting different versions of a claim finding that claims gradually drift over time and undergo greater alteration when traversing languages. Misinformation changes over time, reducing the effectiveness of static claim matching algorithms. The findings advocate for expanded information sharing between fact-checkers globally while underscoring the importance of localized verification.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval</title>
<link>https://arxiv.org/abs/2312.15503</link>
<guid>https://arxiv.org/abs/2312.15503</guid>
<content:encoded><![CDATA[
arXiv:2312.15503v2 Announce Type: replace 
Abstract: Dense retrieval calls for discriminative embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs' strong capability on semantic understanding. However, the LLMs are learned by auto-regression, whose working mechanism is completely different from representing whole text as one discriminative embedding. Thus, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. In this paper, we propose a novel approach, called Llama2Vec, which performs unsupervised adaptation of LLM for its dense retrieval application. Llama2Vec consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the LLM is prompted to reconstruct the input sentence and predict the next sentence based on its text embeddings. Llama2Vec is simple, lightweight, but highly effective. It is used to adapt LLaMA-2-7B on the Wikipedia corpus. With a moderate steps of adaptation, it substantially improves the model's fine-tuned performances on a variety of dense retrieval benchmarks. Notably, it results in the new state-of-the-art performances on popular benchmarks, such as passage and document retrieval on MSMARCO, and zero-shot retrieval on BEIR. The model and source code will be made publicly available to facilitate the future research. Our model is available at https://github.com/FlagOpen/FlagEmbedding.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revolutionizing Finance with LLMs: An Overview of Applications and Insights</title>
<link>https://arxiv.org/abs/2401.11641</link>
<guid>https://arxiv.org/abs/2401.11641</guid>
<content:encoded><![CDATA[
arXiv:2401.11641v3 Announce Type: replace 
Abstract: In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields. Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively. In the financial domain, the deployment of LLMs is gaining momentum. These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction. In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks. Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions. Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks. This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Language Models Detect Misinformation in Scientific News Reporting?</title>
<link>https://arxiv.org/abs/2402.14268</link>
<guid>https://arxiv.org/abs/2402.14268</guid>
<content:encoded><![CDATA[
arXiv:2402.14268v2 Announce Type: replace 
Abstract: Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database. Our dataset includes both human-written and LLM-generated news articles, making it more comprehensive in terms of capturing the growing trend of using LLMs to generate popular press articles. Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation. We propose several baseline architectures using LLMs to automatically detect false representations of scientific findings in the popular press. For each of these architectures, we use several prompt engineering strategies including zero-shot, few-shot, and chain-of-thought prompting. We also test these architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B, Llama2-13B.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GP-GPT: Large Language Model for Gene-Phenotype Mapping</title>
<link>https://arxiv.org/abs/2409.09825</link>
<guid>https://arxiv.org/abs/2409.09825</guid>
<content:encoded><![CDATA[
arXiv:2409.09825v4 Announce Type: replace 
Abstract: Pre-trained large language models(LLMs) have attracted increasing attention in biomedical domains due to their success in natural language processing. However, the complex traits and heterogeneity of multi-sources genomics data pose significant challenges when adapting these models to the bioinformatics and biomedical field. To address these challenges, we present GP-GPT, the first specialized large language model for genetic-phenotype knowledge representation and genomics relation analysis. Our model is fine-tuned in two stages on a comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics, and medical genetics, derived from multiple large-scale validated datasets and scientific publications. GP-GPT demonstrates proficiency in accurately retrieving medical genetics information and performing common genomics analysis tasks, such as genomics information retrieval and relationship determination. Comparative experiments across domain-specific tasks reveal that GP-GPT outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These results highlight GP-GPT's potential to enhance genetic disease relation research and facilitate accurate and efficient analysis in the fields of genomics and medical genetics. Our investigation demonstrated the subtle changes of bio-factor entities' representations in the GP-GPT, which suggested the opportunities for the application of LLMs to advancing gene-phenotype research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models</title>
<link>https://arxiv.org/abs/2409.19492</link>
<guid>https://arxiv.org/abs/2409.19492</guid>
<content:encoded><![CDATA[
arXiv:2409.19492v3 Announce Type: replace 
Abstract: Large language models (LLMs) are starting to complement traditional information seeking mechanisms such as web search. LLM-powered chatbots like ChatGPT are gaining prominence among the general public. AI chatbots are also increasingly producing content on social media platforms. However, LLMs are also prone to hallucinations, generating plausible yet factually incorrect or fabricated information. This becomes a critical problem when laypeople start seeking information about sensitive issues such as healthcare. Existing works in LLM hallucinations in the medical domain mainly focus on testing the medical knowledge of LLMs through standardized medical exam questions which are often well-defined and clear-cut with definitive answers. However, these approaches may not fully capture how these LLMs perform during real-world interactions with patients. This work conducts a pioneering study on hallucinations in LLM-generated responses to real-world healthcare queries from patients.We introduce MedHalu, a novel medical hallucination benchmark featuring diverse health-related topics and hallucinated responses from LLMs, with detailed annotation of the hallucination types and text spans. We also propose MedHaluDetect, a comprehensive framework for evaluating LLMs' abilities to detect hallucinations. Furthermore, we study the vulnerability to medical hallucinations among three groups -- medical experts, LLMs, and laypeople. Notably, LLMs significantly underperform human experts and, in some cases, even laypeople in detecting medical hallucinations. To improve hallucination detection, we propose an expert-in-the-loop approach that integrates expert reasoning into LLM inputs, significantly improving hallucination detection for all LLMs, including a 6.3% macro-F1 improvement for GPT-4. Our code and dataset are available at https://netsys.surrey.ac.uk/datasets/medhalu/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging</title>
<link>https://arxiv.org/abs/2410.01215</link>
<guid>https://arxiv.org/abs/2410.01215</guid>
<content:encoded><![CDATA[
arXiv:2410.01215v4 Announce Type: replace 
Abstract: While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DemoShapley: Valuation of Demonstrations for In-Context Learning</title>
<link>https://arxiv.org/abs/2410.07523</link>
<guid>https://arxiv.org/abs/2410.07523</guid>
<content:encoded><![CDATA[
arXiv:2410.07523v3 Announce Type: replace 
Abstract: Large language models (LLMs) using in-context learning (ICL) excel in many tasks without task-specific fine-tuning. However, demonstration selection and ordering greatly impact ICL effectiveness. Focus on this issue, we propose DemoShapley, a Shapley-value based method that evaluates each demonstration's contribution by measuring its marginal effect across different prompt permutations. To further account for ICL's limited context windows and frequent low-shot settings, we introduce Beta-DemoShapley, a weighted extension that emphasizes the influence of smaller prompt sizes. Experiments on multiple benchmarks show that DemoShapley consistently outperforms existing influence-based selection strategies, while Beta-DemoShapley further improves performance in low-shot scenarios. Both methods also detect mislabeled data, enhance generalization to out-of-distribution tasks, and reduce demographic bias. Together, they provide a unified and robust framework for demonstration valuation in ICL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2410.13334</link>
<guid>https://arxiv.org/abs/2410.13334</guid>
<content:encoded><![CDATA[
arXiv:2410.13334v4 Announce Type: replace 
Abstract: Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as `jailbreaks', where malicious inputs can coerce LLMs into generating harmful content bypassing safety alignments. In this paper, we delve into the ethical biases in LLMs and examine how those biases could be exploited for jailbreaks. Notably, these biases result in a jailbreaking success rate in GPT-4o models that differs by 20\% between non-binary and cisgender keywords and by 16\% between white and black keywords, even when the other parts of the prompts are identical. We introduce the concept of BiasJailbreak, highlighting the inherent risks posed by these safety-induced biases. BiasJailbreak generates biased keywords automatically by asking the target LLM itself, and utilizes the keywords to generate harmful output. Additionally, we propose an efficient defense method BiasDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. BiasDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. Our findings emphasize that ethical biases in LLMs can actually lead to generating unsafe output, and suggest a method to make the LLMs more secure and unbiased. To enable further research and improvements, we open-source our code and artifacts of BiasJailbreak, providing the community with tools to better understand and mitigate safety-induced biases in LLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching</title>
<link>https://arxiv.org/abs/2410.18436</link>
<guid>https://arxiv.org/abs/2410.18436</guid>
<content:encoded><![CDATA[
arXiv:2410.18436v5 Announce Type: replace 
Abstract: Recent large language models (LLMs) demonstrate multilingual abilities, yet they are English-centric due to dominance of English in training corpora. The limited resource for low-resource languages remains a crucial challenge. Code-switching (CS), a phenomenon where multilingual speakers alternate between languages in a discourse, can convey subtle cultural and linguistic nuances that can be otherwise lost in translation and elicits language-specific knowledge in human communications. In light of this, we investigate whether code-switching can activate, or identify and leverage knowledge for reasoning when LLMs solve low-resource language tasks. To facilitate the research, we first present EnKoQA, a synthetic English-Korean CS question-answering dataset. We provide comprehensive analysis on a variety of multilingual LLMs by subdividing activation process into knowledge identification and knowledge leveraging. Our results demonstrate that compared to English text, CS can faithfully activate knowledge inside LLMs especially on language-specific domains, suggesting the potential of code-switching on low-resource language tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lessons from Studying Two-Hop Latent Reasoning</title>
<link>https://arxiv.org/abs/2411.16353</link>
<guid>https://arxiv.org/abs/2411.16353</guid>
<content:encoded><![CDATA[
arXiv:2411.16353v4 Announce Type: replace 
Abstract: Large language models can use chain-of-thought (CoT) to externalize reasoning, potentially enabling oversight of capable LLM agents. Prior work has shown that models struggle at two-hop question-answering without CoT. This capability is so basic that if it was a fundamental limitation, it would imply that many complex agentic tasks would similarly require CoT. We investigate LLM latent reasoning capabilities using two-hop question answering as a case study. Previous work on the gap between latent and externalized two-hop reasoning produced mixed evidence with inconclusive results. In this paper, we introduce a controlled setting for investigating two-hop reasoning in LLMs, where a positive result provides definitive evidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B and GPT-4o) on synthetic facts and test two-hop reasoning over these facts. By using synthetic facts, we rule out memorization and reasoning shortcuts as explanations for two-hop performance. We observe a nuanced picture: Models fail to compose two synthetic facts, but can succeed when one fact is synthetic and the other is natural. These results demonstrate that LLMs are undeniably capable of latent two-hop reasoning, although it remains unclear how this ability scales with model size. Finally, we highlight a lesson for researchers studying LLM reasoning: when drawing conclusions about LLM latent reasoning, one must be careful to avoid both spurious successes (that stem from memorization and reasoning shortcuts) and spurious failures (that may stem from artificial experimental setups, divorced from training setups of frontier LLMs).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematic Reward Gap Optimization for Mitigating VLM Hallucinations</title>
<link>https://arxiv.org/abs/2411.17265</link>
<guid>https://arxiv.org/abs/2411.17265</guid>
<content:encoded><![CDATA[
arXiv:2411.17265v4 Announce Type: replace 
Abstract: The success of Direct Preference Optimization (DPO) in mitigating hallucinations in Vision Language Models (VLMs) critically hinges on the true reward gaps within preference pairs. However, current methods, typically relying on ranking or rewriting strategies, often struggle to optimize these reward gaps in a systematic way during data curation. A core difficulty lies in precisely characterizing and strategically manipulating the overall reward gap configuration, that is, the deliberate design of how to shape these reward gaps within each preference pair across the data. To address this, we introduce Topic-level Preference Rewriting(TPR), a novel framework designed for the systematic optimization of reward gap configuration. Through selectively replacing semantic topics within VLM responses with model's own resampled candidates for targeted rewriting, TPR can provide topic-level control over fine-grained semantic details. This precise control enables advanced data curation strategies, such as progressively adjusting the difficulty of rejected responses, thereby sculpting an effective reward gap configuration that guides the model to overcome challenging hallucinations. Comprehensive experiments demonstrate TPR achieves state-of-the-art performance on multiple hallucination benchmarks, outperforming previous methods by an average of 20%. Notably, it significantly reduces hallucinations by up to 93% on ObjectHal-Bench, and also exhibits superior data efficiency towards robust and cost-effective VLM alignment. Code and datasets are available at https://tpr-dpo.github.io .
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation</title>
<link>https://arxiv.org/abs/2412.07682</link>
<guid>https://arxiv.org/abs/2412.07682</guid>
<content:encoded><![CDATA[
arXiv:2412.07682v5 Announce Type: replace 
Abstract: The high inference cost of Large Language Models (LLMs) poses challenges, especially for tasks requiring lengthy outputs. However, natural language often contains redundancy, which presents an opportunity for optimization. We have observed that LLMs can generate distilled language (i.e., concise outputs that retain essential meaning) when prompted appropriately. We propose TRIM, a pipeline for saving computational cost in which the LLM omits a predefined set of semantically irrelevant and easily inferable words based on the context during inference. Then, a specifically trained smaller language model with lower inference cost reconstructs the distilled answer into the ideal answer. Our experiments show promising results, particularly on the proposed NaLDA evaluation dataset focused on the reconstruction task, with 19.4% saved tokens on average for GPT-4o and only a tiny decrease in evaluation metrics. This suggests that the approach can effectively balance efficiency and accuracy in language processing tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCIS: Efficient Length Extrapolation of LLMs via Divide-and-Conquer Scaling Factor Search</title>
<link>https://arxiv.org/abs/2412.18811</link>
<guid>https://arxiv.org/abs/2412.18811</guid>
<content:encoded><![CDATA[
arXiv:2412.18811v2 Announce Type: replace 
Abstract: Large language models (LLMs) based on the Transformer architecture usually have their context length limited due to the high training cost. Recent advancements extend the context window by adjusting the scaling factors of RoPE and fine-tuning. However, suboptimal initialization of these factors results in increased fine-tuning costs and reduced performance at target length. To address these challenges, we propose a novel RoPE-based fine-tuning framework that diverges from conventional scaling factors search. Specifically, we present a \textbf{D}ivide-and-\textbf{C}onquer \textbf{I}ncremental \textbf{S}earch (DCIS) algorithm that strategically determines the better scaling factors. Further fine-tuning with the identified scaling factors effectively extends the context window of LLMs. Empirical results demonstrate that our methodology not only mitigates performance decay at extended target lengths but also allows the model to fine-tune on short contexts and generalize to long contexts, thereby reducing the cost of fine-tuning. The scaling factors obtained through DCIS can even perform effectively without fine-tuning. Further analysis of the search space reveals that DCIS achieves twice the search efficiency compared to other methods. We also examine the impact of the non-strictly increasing scaling factors utilized in DCIS and evaluate the general capabilities of LLMs across various context lengths.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sentence Smith: Controllable Edits for Evaluating Text Embeddings</title>
<link>https://arxiv.org/abs/2502.14734</link>
<guid>https://arxiv.org/abs/2502.14734</guid>
<content:encoded><![CDATA[
arXiv:2502.14734v4 Announce Type: replace 
Abstract: Controllable and transparent text generation has been a long-standing goal in NLP. Almost as long-standing is a general idea for addressing this challenge: Parsing text to a symbolic representation, and generating from it. However, earlier approaches were hindered by parsing and generation insufficiencies. Using modern parsers and a safety supervision mechanism, we show how close current methods come to this goal. Concretely, we propose the Sentence Smith framework for English, which has three steps: 1. Parsing a sentence into a semantic graph. 2. Applying human-designed semantic manipulation rules. 3. Generating text from the manipulated graph. A final entailment check (4.) verifies the validity of the applied transformation. To demonstrate our framework's utility, we use it to induce hard negative text pairs that challenge text embedding models. Since the controllable generation makes it possible to clearly isolate different types of semantic shifts, we can evaluate text embedding models in a fine-grained way, also addressing an issue in current benchmarking where linguistic phenomena remain opaque. Human validation confirms that our transparent generation process produces texts of good quality. Notably, our way of generation is very resource-efficient, since it relies only on smaller neural networks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using tournaments to calculate AUROC for zero-shot classification with LLMs</title>
<link>https://arxiv.org/abs/2502.15018</link>
<guid>https://arxiv.org/abs/2502.15018</guid>
<content:encoded><![CDATA[
arXiv:2502.15018v2 Announce Type: replace 
Abstract: Large language models perform surprisingly well on many zero-shot classification tasks, but are difficult to fairly compare to supervised classifiers due to the lack of a modifiable decision boundary. In this work, we propose and evaluate a method that transforms binary classification tasks into pairwise comparisons between instances within a dataset, using LLMs to produce relative rankings of those instances. Repeated pairwise comparisons can be used to score instances using the Elo rating system (used in chess and other competitions), inducing a confidence ordering over instances in a dataset. We evaluate scheduling algorithms for their ability to minimize comparisons, and show that our proposed algorithm leads to improved classification performance, while also providing more information than traditional zero-shot classification.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModernBERT is More Efficient than Conventional BERT for Chest CT Findings Classification in Japanese Radiology Reports</title>
<link>https://arxiv.org/abs/2503.05060</link>
<guid>https://arxiv.org/abs/2503.05060</guid>
<content:encoded><![CDATA[
arXiv:2503.05060v2 Announce Type: replace 
Abstract: Japanese language models for medical text classification face challenges with complex vocabulary and linguistic structures in radiology reports. This study compared three Japanese models--BERT Base, JMedRoBERTa, and ModernBERT--for multi-label classification of 18 chest CT findings. Using the CT-RATE-JPN dataset, all models were fine-tuned under identical conditions. ModernBERT showed clear efficiency advantages, producing substantially fewer tokens and achieving faster training and inference than the other models while maintaining comparable performance on the internal test dataset (exact match accuracy: 74.7% vs. 72.7% for BERT Base). To assess generalizability, we additionally constructed RR-Findings, an external dataset of 243 naturally written Japanese radiology reports annotated using the same schema. Under this domain-shifted setting, performance differences became pronounced: BERT Base outperformed both JMedRoBERTa and ModernBERT, whereas ModernBERT showed the largest decline in exact match accuracy. Average precision differences were smaller, indicating that ModernBERT retained reasonable ranking ability despite reduced calibration. Overall, ModernBERT offers substantial computational efficiency and strong in-domain performance but remains sensitive to real-world linguistic variability. These results highlight the need for more diverse natural-language training data and domain-specific calibration strategies to improve robustness when deploying modern transformer models in heterogeneous clinical environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Word-level Annotation of GDPR Transparency Compliance in Privacy Policies using Large Language Models</title>
<link>https://arxiv.org/abs/2503.10727</link>
<guid>https://arxiv.org/abs/2503.10727</guid>
<content:encoded><![CDATA[
arXiv:2503.10727v2 Announce Type: replace 
Abstract: Ensuring transparency of data practices related to personal information is a core requirement of the General Data Protection Regulation (GDPR). However, large-scale compliance assessment remains challenging due to the complexity and diversity of privacy policy language. Manual audits are labour-intensive and inconsistent, while current automated methods often lack the granularity required to capture nuanced transparency disclosures.
  In this paper, we present a modular large language model (LLM)-based pipeline for fine-grained word-level annotation of privacy policies with respect to GDPR transparency requirements. Our approach integrates LLM-driven annotation with passage-level classification, retrieval-augmented generation, and a self-correction mechanism to deliver scalable, context-aware annotations across 21 GDPR-derived transparency requirements. To support empirical evaluation, we compile a corpus of 703,791 English-language privacy policies and generate a ground-truth sample of 200 manually annotated policies based on a comprehensive, GDPR-aligned annotation scheme.
  We propose a two-tiered evaluation methodology capturing both passage-level classification and span-level annotation quality and conduct a comparative analysis of seven state-of-the-art LLMs on two annotation schemes, including the widely used OPP-115 dataset. The results of our evaluation show that decomposing the annotation task and integrating targeted retrieval and classification components significantly improve annotation accuracy, particularly for well-structured requirements. Our work provides new empirical resources and methodological foundations for advancing automated transparency compliance assessment at scale.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Domain-Specific Encoder Models with LLM-Generated Data: How to Leverage Ontologies, and How to Do Without Them</title>
<link>https://arxiv.org/abs/2503.22006</link>
<guid>https://arxiv.org/abs/2503.22006</guid>
<content:encoded><![CDATA[
arXiv:2503.22006v2 Announce Type: replace 
Abstract: We investigate the use of LLM-generated data for continual pretraining of encoder models in specialized domains with limited training data, using the scientific domain of invasion biology as a case study. To this end, we leverage domain-specific ontologies by enriching them with LLM-generated data and pretraining the encoder model as an ontology-informed embedding model for concept definitions. To evaluate the effectiveness of this method, we compile a benchmark specifically designed for assessing model performance in invasion biology. After demonstrating substantial improvements over standard LLM pretraining, we investigate the feasibility of applying the proposed approach to domains without comprehensive ontologies by substituting ontological concepts with concepts automatically extracted from a small corpus of scientific abstracts and establishing relationships between concepts through distributional statistics. Our results demonstrate that this automated approach achieves comparable performance using only a small set of scientific abstracts, resulting in a fully automated pipeline for enhancing domain-specific understanding of small encoder models that is especially suited for application in low-resource settings and achieves performance comparable to masked language modeling pretraining on much larger datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContrastScore: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation</title>
<link>https://arxiv.org/abs/2504.02106</link>
<guid>https://arxiv.org/abs/2504.02106</guid>
<content:encoded><![CDATA[
arXiv:2504.02106v3 Announce Type: replace 
Abstract: Evaluating the quality of generated text automatically remains a significant challenge. Conventional reference-based metrics have been shown to exhibit relatively weak correlation with human evaluations. Recent research advocates the use of large language models (LLMs) as source-based metrics for natural language generation (NLG) assessment. While promising, LLM-based metrics, particularly those using smaller models, still fall short in aligning with human judgments. In this work, we introduce ContrastScore, a contrastive evaluation metric designed to enable higher-quality, less biased, and more efficient assessment of generated text. We evaluate ContrastScore on two NLG tasks: machine translation and summarization. Experimental results show that ContrastScore consistently achieves stronger correlation with human judgments than both single-model and ensemble-based baselines. Notably, ContrastScore based on Qwen 3B and 0.5B even outperforms Qwen 7B, despite having only half as many parameters, demonstrating its efficiency. Furthermore, it effectively mitigates common evaluation biases such as length and likelihood preferences, resulting in more robust automatic evaluation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training</title>
<link>https://arxiv.org/abs/2505.16570</link>
<guid>https://arxiv.org/abs/2505.16570</guid>
<content:encoded><![CDATA[
arXiv:2505.16570v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are commonly pretrained on vast corpora of text without utilizing contextual metadata such as source, quality, or topic, leading to a context-free learning paradigm. While recent studies suggest that adding metadata like URL information as context (i.e., auxiliary inputs not used in the loss calculation) can improve training efficiency and downstream performance, they offer limited understanding of which types of metadata are truly effective and under what conditions. In this work, we conduct a systematic evaluation and find that not all metadata types contribute equally. Only URL context speeds up training, whereas quality scores and topic/format domain information offer no clear benefit. Furthermore, the improved downstream performances of URL conditioning emerge only when longer prompts are used at inference time. In addition, we demonstrate that context-aware pretraining enables more controllable generation than context-free pretraining, in a classifier-free guidance fashion. Although topic and format metadata do not accelerate training, they are effective for steering outputs, offering human-interpretable control over generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Multi-Agent RAG Systems with Minimalist Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17086</link>
<guid>https://arxiv.org/abs/2505.17086</guid>
<content:encoded><![CDATA[
arXiv:2505.17086v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) equipped with modern Retrieval-Augmented Generation (RAG) systems often employ multi-turn interaction pipelines to interface with search engines for complex reasoning tasks. However, such multi-turn interactions inevitably produce long intermediate contexts, as context length grows exponentially with exploration depth. This leads to a well-known limitation of LLMs: their difficulty in effectively leveraging information from long contexts. This problem is further amplified in RAG systems that depend on in-context learning, where few-shot demonstrations must also be included in the prompt, compounding the context-length bottleneck. To address these challenges, we propose Mujica-MyGo, a unified framework for efficient multi-turn reasoning in RAG. Inspired by the divide-and-conquer principle, we introduce Mujica (Multi-hop Joint Intelligence for Complex Question Answering), a multi-agent RAG workflow that decomposes multi-turn interactions into cooperative sub-interactions, thereby mitigating long-context issues. To eliminate the dependency on in-context learning, we further develop MyGO (Minimalist Policy Gradient Optimization), a lightweight and efficient reinforcement learning algorithm that enables effective post-training of LLMs within complex RAG pipelines. We provide theoretical guarantees for MyGO's convergence to the optimal policy. Empirical evaluations across diverse question-answering benchmarks, covering both text corpora and knowledge graphs, show that Mujica-MyGO achieves superior performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversations: Love Them, Hate Them, Steer Them</title>
<link>https://arxiv.org/abs/2505.17413</link>
<guid>https://arxiv.org/abs/2505.17413</guid>
<content:encoded><![CDATA[
arXiv:2505.17413v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable method for controlling specific emotional attributes in LLMs, contributing to developing more aligned and empathetic conversational AI.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGM: A Framework for Building Specification-Guided Moderation Filters</title>
<link>https://arxiv.org/abs/2505.19766</link>
<guid>https://arxiv.org/abs/2505.19766</guid>
<content:encoded><![CDATA[
arXiv:2505.19766v2 Announce Type: replace 
Abstract: Aligning large language models (LLMs) with deployment-specific requirements is critical but inherently imperfect. Despite extensive training, models remain susceptible to misalignment and adversarial inputs such as jailbreaks. Content moderation filters are commonly used as external safeguards, though they typically focus narrowly on safety. We introduce SGM (Specification-Guided Moderation), a flexible framework for training moderation filters grounded in user-defined specifications that go beyond standard safety concerns. SGM automates training data generation without relying on human-written examples, enabling scalable support for diverse, application-specific alignment goals. SGM-trained filters perform on par with state-of-the-art safety filters built on curated datasets, while supporting fine-grained and user-defined alignment control.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.20613</link>
<guid>https://arxiv.org/abs/2505.20613</guid>
<content:encoded><![CDATA[
arXiv:2505.20613v3 Announce Type: replace 
Abstract: Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective</title>
<link>https://arxiv.org/abs/2505.21505</link>
<guid>https://arxiv.org/abs/2505.21505</guid>
<content:encoded><![CDATA[
arXiv:2505.21505v2 Announce Type: replace 
Abstract: Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some research on language-specific neurons provides a new perspective to analyze and understand LLMs' mechanisms. However, we find that there are many neurons that are shared by multiple but not all languages and cannot be correctly classified. In this work, we propose a ternary classification methodology that categorizes neurons into three types, including language-specific neurons, language-related neurons, and general neurons. And we propose a corresponding identification algorithm to distinguish these different types of neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights to better understand multilingual alignment and multilingual capabilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?</title>
<link>https://arxiv.org/abs/2505.22061</link>
<guid>https://arxiv.org/abs/2505.22061</guid>
<content:encoded><![CDATA[
arXiv:2505.22061v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) mitigates the hallucination problem in large language models (LLMs) and has proven effective for personalized usages. However, delivering private retrieved documents directly to LLMs introduces vulnerability to membership inference attacks (MIAs), which try to determine whether the target data point exists in the private external database or not. Based on the insight that MIA queries typically exhibit high similarity to only one target document, we introduce a novel similarity-based MIA detection framework designed for the RAG system. With the proposed method, we show that a simple detect-and-hide strategy can successfully obfuscate attackers, maintain data utility, and remain system-agnostic against MIA. We experimentally prove its detection and defense against various state-of-the-art MIA methods and its adaptability to existing RAG systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoKI: Low-damage Knowledge Implanting of Large Language Models</title>
<link>https://arxiv.org/abs/2505.22120</link>
<guid>https://arxiv.org/abs/2505.22120</guid>
<content:encoded><![CDATA[
arXiv:2505.22120v2 Announce Type: replace 
Abstract: Fine-tuning adapts pretrained models for specific tasks but poses the risk of catastrophic forgetting (CF), where critical knowledge from pretraining is overwritten. To address the issue of CF in a general-purpose framework, we propose Low-damage Knowledge Implanting (LoKI), a parameter-efficient fine-tuning (PEFT) technique that utilizes recent mechanistic understanding of how knowledge is stored in transformer architectures. We compare LoKI against state-of-the-art PEFT methods in two real-world fine-tuning scenarios. The results show that LoKI demonstrates significantly better preservation of general capabilities. At the same time, its task-specific performance is comparable to or even surpasses that of full parameter fine-tuning and these PEFT methods across various model architectures. Our work bridges the mechanistic insights of LLMs' knowledge storage with practical fine-tuning objectives, enabling an effective balance between task-specific adaptation and the retention of general-purpose capabilities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Knowledge Graph-Guided Multimodal Synthesis</title>
<link>https://arxiv.org/abs/2505.22633</link>
<guid>https://arxiv.org/abs/2505.22633</guid>
<content:encoded><![CDATA[
arXiv:2505.22633v3 Announce Type: replace 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. Our approach addresses this critical gap by providing a systematic framework for generating spatially coherent data. In this work, we introduce SKG2DATA, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2DATA employs an automated pipeline for constructing Spatial Knowledge Graph (SKG) that effectively captures human-like spatial cognition, including directional and distance relationships. These structured representations then serve as precise guidance for our integrated synthesis pipeline, where a diffusion model generates spatially-consistent images while a MLLM produces corresponding textual descriptions. The automated construction of SKG enables scalable generation of diverse yet realistic spatial configurations, overcoming the limitations of manual data collection and annotation. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, enhance the spatial perception and reasoning abilities of MLLMs markedly, albeit with a slight cost to their general capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence. Code is available at https://github.com/zjunlp/Knowledge2Data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models</title>
<link>https://arxiv.org/abs/2505.23715</link>
<guid>https://arxiv.org/abs/2505.23715</guid>
<content:encoded><![CDATA[
arXiv:2505.23715v2 Announce Type: replace 
Abstract: Large language models (LLMs) have witnessed rapid advancements, demonstrating remarkable capabilities. However, a notable vulnerability persists: LLMs often uncritically accept flawed or contradictory premises, leading to inefficient reasoning and unreliable outputs. This emphasizes the significance of possessing the \textbf{Premise Critique Ability} for LLMs, defined as the capacity to proactively identify and articulate errors in input premises. Most existing studies assess LLMs' reasoning ability in ideal settings, largely ignoring their vulnerabilities when faced with flawed premises. Thus, we introduce the \textbf{Premise Critique Bench (PCBench)}, designed by incorporating four error types across three difficulty levels, paired with multi-faceted evaluation metrics. We conducted systematic evaluations of 15 representative LLMs. Our findings reveal: (1) Most models rely heavily on explicit prompts to detect errors, with limited autonomous critique; (2) Premise critique ability depends on question difficulty and error type, with direct contradictions being easier to detect than complex or procedural errors; (3) Reasoning ability does not consistently correlate with the premise critique ability; (4) Flawed premises trigger overthinking in reasoning models, markedly lengthening responses due to repeated attempts at resolving conflicts. These insights underscore the urgent need to enhance LLMs' proactive evaluation of input validity, positioning premise critique as a foundational capability for developing reliable, human-centric systems. The code is available at https://github.com/MLGroupJLU/Premise_Critique.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating LLM Consistency: A User Baseline vs Surrogate Metrics</title>
<link>https://arxiv.org/abs/2505.23799</link>
<guid>https://arxiv.org/abs/2505.23799</guid>
<content:encoded><![CDATA[
arXiv:2505.23799v4 Announce Type: replace 
Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility, one of which is to measure the consistency of LLM responses -- the model's confidence in the response or likelihood of generating a similar response when resampled. In previous work, measuring LLM response consistency often relied on calculating the probability of a response appearing within a pool of resampled responses, analyzing internal states, or evaluating logits of responses. However, it was not clear how well these approaches approximated users' perceptions of consistency of LLM responses. To find out, we performed a user study ($n=2,976$) demonstrating that current methods for measuring LLM response consistency typically do not align well with humans' perceptions of LLM consistency. We propose a logit-based ensemble method for estimating LLM consistency and show that our method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods for estimating LLM consistency without human evaluation are sufficiently imperfect to warrant broader use of evaluation with human input; this would avoid misjudging the adequacy of models because of the imperfections of automated consistency metrics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking</title>
<link>https://arxiv.org/abs/2506.07751</link>
<guid>https://arxiv.org/abs/2506.07751</guid>
<content:encoded><![CDATA[
arXiv:2506.07751v3 Announce Type: replace 
Abstract: Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further "instantiate" reasoning problems on potential variations. In this work, we instead focuses on the strategy of "abstracting" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One SPACE to Rule Them All: Jointly Mitigating Factuality and Faithfulness Hallucinations in LLMs</title>
<link>https://arxiv.org/abs/2506.11088</link>
<guid>https://arxiv.org/abs/2506.11088</guid>
<content:encoded><![CDATA[
arXiv:2506.11088v2 Announce Type: replace 
Abstract: LLMs have demonstrated unprecedented capabilities in natural language processing, yet their practical deployment remains hindered by persistent factuality and faithfulness hallucinations. While existing methods address these hallucination types independently, they inadvertently induce performance trade-offs, as interventions targeting one type often exacerbate the other. Through empirical and theoretical analysis of activation space dynamics in LLMs, we reveal that these hallucination categories share overlapping subspaces within neural representations, presenting an opportunity for concurrent mitigation. To harness this insight, we propose SPACE, a unified framework that jointly enhances factuality and faithfulness by editing shared activation subspaces. SPACE establishes a geometric foundation for shared subspace existence through dual-task feature modeling, then identifies and edits these subspaces via a hybrid probe strategy combining spectral clustering and attention head saliency scoring. Experimental results across multiple benchmark datasets demonstrate the superiority of our approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized LLM Decoding via Contrasting Personal Preference</title>
<link>https://arxiv.org/abs/2506.12109</link>
<guid>https://arxiv.org/abs/2506.12109</guid>
<content:encoded><![CDATA[
arXiv:2506.12109v3 Announce Type: replace 
Abstract: As large language models (LLMs) are progressively deployed in various real-world applications, personalization of LLMs has become increasingly important. While various approaches to LLM personalization such as prompt-based and training-based methods have been actively explored, the development of effective decoding-time algorithms remains largely overlooked, despite their demonstrated potential. In this paper, we propose CoPe (Contrasting Personal Preference), a novel decoding-time approach applied after performing parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is to leverage reward-guided decoding specifically for personalization by maximizing each user's implicit reward signal. We evaluate CoPe across five open-ended personalized text generation tasks. Our empirical results demonstrate that CoPe achieves strong performance, improving personalization by an average of 10.57% in ROUGE-L, without relying on external reward models or additional training procedures.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting</title>
<link>https://arxiv.org/abs/2506.17609</link>
<guid>https://arxiv.org/abs/2506.17609</guid>
<content:encoded><![CDATA[
arXiv:2506.17609v3 Announce Type: replace 
Abstract: Accurate typhoon track forecasting is crucial for early system warning and disaster response. While Transformer-based models have demonstrated strong performance in modeling the temporal dynamics of dense trajectories of humans and vehicles in smart cities, they usually lack access to broader contextual knowledge that enhances the forecasting reliability of sparse meteorological trajectories, such as typhoon tracks. To address this challenge, we propose TyphoFormer, a novel framework that incorporates natural language descriptions as auxiliary prompts to improve typhoon trajectory forecasting. For each time step, we use Large Language Model (LLM) to generate concise textual descriptions based on the numerical attributes recorded in the North Atlantic hurricane database. The language descriptions capture high-level meteorological semantics and are embedded as auxiliary special tokens prepended to the numerical time series input. By integrating both textual and sequential information within a unified Transformer encoder, TyphoFormer enables the model to leverage contextual cues that are otherwise inaccessible through numerical features alone. Extensive experiments are conducted on HURDAT2 benchmark, results show that TyphoFormer consistently outperforms other state-of-the-art baseline methods, particularly under challenging scenarios involving nonlinear path shifts and limited historical observations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCode: Updating Code API Knowledge with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.20495</link>
<guid>https://arxiv.org/abs/2506.20495</guid>
<content:encoded><![CDATA[
arXiv:2506.20495v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression</title>
<link>https://arxiv.org/abs/2508.03520</link>
<guid>https://arxiv.org/abs/2508.03520</guid>
<content:encoded><![CDATA[
arXiv:2508.03520v2 Announce Type: replace 
Abstract: Noisy self-reported empathy scores challenge supervised learning for empathy regression. While many algorithms have been proposed for learning with noisy labels in textual classification problems, the regression counterpart is relatively under-explored. We propose UPLME, an uncertainty-aware probabilistic language modelling framework to capture label noise in empathy regression tasks. One of the novelties in UPLME is a probabilistic language model that predicts both empathy scores and heteroscedastic uncertainty, and is trained using Bayesian concepts with variational model ensembling. We further introduce two novel loss components: one penalises degenerate Uncertainty Quantification (UQ), and another enforces similarity between the input pairs on which empathy is being predicted. UPLME achieves state-of-the-art performance (Pearson Correlation Coefficient: $0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the performance reported in the literature on two public benchmarks with label noise. Through synthetic label noise injection, we demonstrate that UPLME is effective in distinguishing between noisy and clean samples based on the predicted uncertainty. UPLME further outperform (Calibration error: $0.571\rightarrow0.376$) a recent variational model ensembling-based UQ method designed for regression problems. Code is publicly available at https://github.com/hasan-rakibul/UPLME.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion</title>
<link>https://arxiv.org/abs/2508.04440</link>
<guid>https://arxiv.org/abs/2508.04440</guid>
<content:encoded><![CDATA[
arXiv:2508.04440v2 Announce Type: replace 
Abstract: Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning</title>
<link>https://arxiv.org/abs/2508.06447</link>
<guid>https://arxiv.org/abs/2508.06447</guid>
<content:encoded><![CDATA[
arXiv:2508.06447v2 Announce Type: replace 
Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code is available at https://github.com/Longxmas/SlimInfer.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title>
<link>https://arxiv.org/abs/2508.11009</link>
<guid>https://arxiv.org/abs/2508.11009</guid>
<content:encoded><![CDATA[
arXiv:2508.11009v2 Announce Type: replace 
Abstract: The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework</title>
<link>https://arxiv.org/abs/2509.08438</link>
<guid>https://arxiv.org/abs/2509.08438</guid>
<content:encoded><![CDATA[
arXiv:2509.08438v2 Announce Type: replace 
Abstract: Speech Relation Extraction (SpeechRE) aims to extract relation triplets directly from speech. However, existing benchmark datasets rely heavily on synthetic data, lacking sufficient quantity and diversity of real human speech. Moreover, existing models also suffer from rigid single-order generation templates and weak semantic alignment, substantially limiting their performance. To address these challenges, we introduce CommonVoice-SpeechRE, a large-scale dataset comprising nearly 20,000 real-human speech samples from diverse speakers, establishing a new benchmark for SpeechRE research. Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet generation ensemble strategy, leveraging data diversity through diverse element orders during both training and inference, and (2) CNN-based latent relation prediction heads that generate explicit relation prompts to guide cross-modal alignment and accurate triplet generation. Experiments show our approach outperforms state-of-the-art methods, providing both a benchmark dataset and an effective solution for real-world SpeechRE. The source code and dataset are publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PsychiatryBench: A Multi-Task Benchmark for LLMs in Psychiatry</title>
<link>https://arxiv.org/abs/2509.09711</link>
<guid>https://arxiv.org/abs/2509.09711</guid>
<content:encoded><![CDATA[
arXiv:2509.09711v2 Announce Type: replace 
Abstract: Large language models (LLMs) offer significant potential in enhancing psychiatric practice, from improving diagnostic accuracy to streamlining clinical documentation and therapeutic support. However, existing evaluation resources heavily rely on small clinical interview corpora, social media posts, or synthetic dialogues, which limits their clinical validity and fails to capture the full complexity of diagnostic reasoning. In this work, we introduce PsychiatryBench, a rigorously curated benchmark grounded exclusively in authoritative, expert-validated psychiatric textbooks and casebooks. PsychiatryBench comprises eleven distinct question-answering tasks ranging from diagnostic reasoning and treatment planning to longitudinal follow-up, management planning, clinical approach, sequential case analysis, and multiple-choice/extended matching formats totaling 5,188 expert-annotated items. {\color{red}We evaluate a diverse set of frontier LLMs (including Google Gemini, DeepSeek, Sonnet 4.5, and GPT 5) alongside leading open-source medical models such as MedGemma using both conventional metrics and an "LLM-as-judge" similarity scoring framework. Our results reveal substantial gaps in clinical consistency and safety, particularly in multi-turn follow-up and management tasks, underscoring the need for specialized model tuning and more robust evaluation paradigms. PsychiatryBench offers a modular, extensible platform for benchmarking and improving LLM performance in mental health applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models</title>
<link>https://arxiv.org/abs/2509.15216</link>
<guid>https://arxiv.org/abs/2509.15216</guid>
<content:encoded><![CDATA[
arXiv:2509.15216v2 Announce Type: replace 
Abstract: Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (https://github.com/chattergpt/HSO-Bench).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models</title>
<link>https://arxiv.org/abs/2509.15478</link>
<guid>https://arxiv.org/abs/2509.15478</guid>
<content:encoded><![CDATA[
arXiv:2509.15478v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) are increasingly used in real world applications, yet their safety under adversarial conditions remains underexplored. This study evaluates the harmlessness of four leading MLLMs (GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to adversarial prompts across text-only and multimodal formats. A team of 26 red teamers generated 726 prompts targeting three harm categories: illegal activity, disinformation, and unethical behaviour. These prompts were submitted to each model, and 17 annotators rated 2,904 model outputs for harmfulness using a 5-point scale. Results show significant differences in vulnerability across models and modalities. Pixtral 12B exhibited the highest rate of harmful responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%). Contrary to expectations, text-only prompts were slightly more effective at bypassing safety mechanisms than multimodal ones. Statistical analysis confirmed that both model type and input modality were significant predictors of harmfulness. These findings underscore the urgent need for robust, multimodal safety benchmarks as MLLMs are deployed more widely.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs4All: A Review of Large Language Models Across Academic Disciplines</title>
<link>https://arxiv.org/abs/2509.19580</link>
<guid>https://arxiv.org/abs/2509.19580</guid>
<content:encoded><![CDATA[
arXiv:2509.19580v5 Announce Type: replace 
Abstract: Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view of the world. For example, Large Language Models (LLMs) based applications such as ChatGPT have shown the capability of generating human-like conversation on extensive topics. Due to the impressive performance on a variety of language-related tasks (e.g., open-domain question answering, translation, and document summarization), one can envision the far-reaching impacts that can be brought by the LLMs with broader real-world applications (e.g., customer service, education and accessibility, and scientific discovery). Inspired by their success, this paper will offer an overview of state-of-the-art LLMs and their integration into a wide range of academic disciplines, including: (1) arts, letters, and law (e.g., history, philosophy, political science, arts and architecture, law), (2) economics and business (e.g., finance, economics, accounting, marketing), and (3) science and engineering (e.g., mathematics, physics and mechanical engineering, chemistry and chemical engineering, life sciences and bioengineering, earth sciences and civil engineering, computer science and electrical engineering). Integrating humanity and technology, in this paper, we will explore how LLMs are shaping research and practice in these fields, while also discussing key limitations, open challenges, and future directions in the era of generative AI. The review of how LLMs are engaged across disciplines-along with key observations and insights-can help researchers and practitioners interested in exploiting LLMs to advance their works in diverse real-world applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MGen: Millions of Naturally Occurring Generics in Context</title>
<link>https://arxiv.org/abs/2509.26160</link>
<guid>https://arxiv.org/abs/2509.26160</guid>
<content:encoded><![CDATA[
arXiv:2509.26160v2 Announce Type: replace 
Abstract: MGen is a dataset of over 4 million naturally occurring generic and quantified sentences extracted from diverse textual sources. Sentences in the dataset have long context documents, corresponding to websites and academic papers, and cover 11 different quantifiers. We analyze the features of generics sentences in the dataset, with interesting insights: generics can be long sentences (averaging over 16 words) and speakers often use them to express generalisations about people.
  MGen is the biggest and most diverse dataset of naturally occurring generic sentences, opening the door to large-scale computational research on genericity. It is publicly available at https://gustavocilleruelo.com/mgen
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks</title>
<link>https://arxiv.org/abs/2510.02712</link>
<guid>https://arxiv.org/abs/2510.02712</guid>
<content:encoded><![CDATA[
arXiv:2510.02712v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversational degradation that characterize real-world interactions. In this work, we present a large-scale survival analysis of conversational robustness, modeling failure as a time-to-event process over 36,951 turns from 9 state-of-the-art LLMs on the MT-Consistency benchmark. Our framework combines Cox proportional hazards, Accelerated Failure Time (AFT), and Random Survival Forest models with simple semantic drift features. We find that abrupt prompt-to-prompt semantic drift sharply increases the hazard of inconsistency, whereas cumulative drift is counterintuitively \emph{protective}, suggesting adaptation in conversations that survive multiple shifts. AFT models with model-drift interactions achieve the best combination of discrimination and calibration, and proportional hazards checks reveal systematic violations for key drift covariates, explaining the limitations of Cox-style modeling in this setting. Finally, we show that a lightweight AFT model can be turned into a turn-level risk monitor that flags most failing conversations several turns before the first inconsistent answer while keeping false alerts modest. These results establish survival analysis as a powerful paradigm for evaluating multi-turn robustness and for designing practical safeguards for conversational AI systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</title>
<link>https://arxiv.org/abs/2510.02855</link>
<guid>https://arxiv.org/abs/2510.02855</guid>
<content:encoded><![CDATA[
arXiv:2510.02855v4 Announce Type: replace 
Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.03805</link>
<guid>https://arxiv.org/abs/2510.03805</guid>
<content:encoded><![CDATA[
arXiv:2510.03805v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks but often suffer from excessive verbosity, known as "overthinking." Existing solutions via reinforcement learning (RL) typically penalize generated tokens to promote conciseness. However, these methods encounter two challenges: responses with fewer tokens do not always correspond to fewer reasoning steps, and models may develop hacking behavior in later stages of training by discarding reasoning steps to minimize token usage. In this work, we introduce \textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more efficient reasoning by favoring compact reasoning steps. Our step-aware reward function prioritizes correctness while imposing penalties for redundant steps, and withholds rewards for incorrect responses to prevent the reinforcement of erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when the model's output no longer shortens, training is halted to prevent hacking behavior caused by the merging of steps. Extensive experiments across four reasoning benchmarks demonstrate that SP achieves state-of-the-art accuracy while significantly reducing response length. For instance, on AIME24, SP reduces token usage by \textbf{69.7\%}.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drift No More? Context Equilibria in Multi-Turn LLM Interactions</title>
<link>https://arxiv.org/abs/2510.07777</link>
<guid>https://arxiv.org/abs/2510.07777</guid>
<content:encoded><![CDATA[
arXiv:2510.07777v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at single-turn tasks such as instruction following and summarization, yet real-world deployments require sustained multi-turn interactions where user goals and conversational context persist and evolve. A recurring challenge in this setting is context drift: the gradual divergence of a model's outputs from goal-consistent behavior across turns. Unlike single-turn errors, drift unfolds temporally and is poorly captured by static evaluation metrics. In this work, we present a study of context drift in multi-turn interactions and propose a simple dynamical framework to interpret its behavior. We formalize drift as the turn-wise KL divergence between the token-level predictive distributions of the test model and a goal-consistent reference model, and propose a recurrence model that interprets its evolution as a bounded stochastic process with restoring forces and controllable interventions. We instantiate this framework in both synthetic long-horizon rewriting tasks and realistic user-agent simulations such as in $\tau$-Bench, measuring drift for several open-weight LLMs that are used as user simulators. Our experiments consistently reveal stable, noise-limited equilibria rather than runaway degradation, and demonstrate that simple reminder interventions reliably reduce divergence in line with theoretical predictions. Together, these results suggest that multi-turn drift can be understood as a controllable equilibrium phenomenon rather than as inevitable decay, providing a foundation for studying and mitigating context drift in extended interactions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology</title>
<link>https://arxiv.org/abs/2510.07793</link>
<guid>https://arxiv.org/abs/2510.07793</guid>
<content:encoded><![CDATA[
arXiv:2510.07793v3 Announce Type: replace 
Abstract: Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExPO-HM: Learning to Explain-then-Detect for Hateful Meme Detection</title>
<link>https://arxiv.org/abs/2510.08630</link>
<guid>https://arxiv.org/abs/2510.08630</guid>
<content:encoded><![CDATA[
arXiv:2510.08630v2 Announce Type: replace 
Abstract: Hateful memes have emerged as a particularly challenging form of online abuse, motivating the development of automated detection systems. Most prior approaches rely on direct detection, producing only binary predictions. Such models fail to provide the context and explanations that real-world moderation requires. Recent Explain-then-Detect approaches, using Chain-of-Thought prompting or LMM agents, perform worse than simple SFT baselines, and even advanced post-training methods such as GRPO fail to close the gap. Our analysis identifies two key issues of such systems: important policy-relevant cues such as targets and attack types are not hypothesized by the model as a likely explanation; and the binary reward signal is insufficient to guide reasoning. To address these challenges, we propose ExPO-HM (Explain-then-Detect Policy Optimization for Hateful Memes), inspired by the training and evaluation process of human annotators. ExPO-HM combines SFT warmup, GRPO with curriculum learning, and Conditional Decision Entropy (CDE) as both metric and reward for reasoning quality. Across three hateful meme benchmarks, ExPO-HM achieves state-of-the-art performance on binary detection, fine-grained classification, and reasoning quality, with up to 15\% and 17\% F1 improvement over the GRPO and DPO baselines, respectively. By moving hateful meme detection from simple binary alarms to explanation-driven detection, ExPO-HM provides accurate, interpretable, and actionable moderation support.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Framework for Augmenting Rating Scale Tests with LLM-Scored Text Data</title>
<link>https://arxiv.org/abs/2510.08663</link>
<guid>https://arxiv.org/abs/2510.08663</guid>
<content:encoded><![CDATA[
arXiv:2510.08663v2 Announce Type: replace 
Abstract: Psychological assessments are dominated by rating scales, which cannot capture the nuance in natural language. Efforts to supplement them with qualitative text have relied on labelled datasets or expert rubrics, limiting scalability. We introduce a framework that avoids this reliance: large language models (LLMs) score free-text responses with simple prompts to produce candidate LLM items, from which we retain those that yield the most test information when co-calibrated with a baseline scale. Using depression as a case study, we developed and tested the method in upper-secondary students (n=693) and a matched synthetic dataset (n=3,000). Results on held-out test sets showed that augmenting a 19-item scale with LLM items improved its precision, accuracy, and convergent validity. Further, the test information gain matched that of adding as many as 16 rating-scale items. This framework leverages the increasing availability of transcribed language to enhance psychometric measures, with applications in clinical health and beyond.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs</title>
<link>https://arxiv.org/abs/2510.13912</link>
<guid>https://arxiv.org/abs/2510.13912</guid>
<content:encoded><![CDATA[
arXiv:2510.13912v3 Announce Type: replace 
Abstract: The core premise of AI debate as a scalable oversight technique is that it is harder to lie convincingly than to refute a lie, enabling the judge to identify the correct position. Yet, existing debate experiments have relied on datasets with ground truth, where lying is reduced to defending an incorrect proposition. This overlooks a subjective dimension: lying also requires the belief that the claim defended is false. In this work, we apply debate to subjective questions and explicitly measure large language models' prior beliefs before experiments. Debaters were asked to select their preferred position, then presented with a judge persona deliberately designed to conflict with their identified priors. This setup tested whether models would adopt sycophantic strategies, aligning with the judge's presumed perspective to maximize persuasiveness, or remain faithful to their prior beliefs. We implemented and compared two debate protocols, sequential and simultaneous, to evaluate potential systematic biases. Finally, we assessed whether models were more persuasive and produced higher-quality arguments when defending positions consistent with their prior beliefs versus when arguing against them. Our main findings show that models tend to prefer defending stances aligned with the judge persona rather than their prior beliefs, sequential debate introduces significant bias favoring the second debater, models are more persuasive when defending positions aligned with their prior beliefs, and paradoxically, arguments misaligned with prior beliefs are rated as higher quality in pairwise comparison. These results can inform human judges to provide higher-quality training signals and contribute to more aligned AI systems, while revealing important aspects of human-AI interaction regarding persuasion dynamics in language models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?</title>
<link>https://arxiv.org/abs/2510.24591</link>
<guid>https://arxiv.org/abs/2510.24591</guid>
<content:encoded><![CDATA[
arXiv:2510.24591v2 Announce Type: replace 
Abstract: Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Architectural Advantage of The Instruction-Tuned LLM in Containing The Readability-Accuracy Tension in Text Simplification</title>
<link>https://arxiv.org/abs/2511.05080</link>
<guid>https://arxiv.org/abs/2511.05080</guid>
<content:encoded><![CDATA[
arXiv:2511.05080v2 Announce Type: replace 
Abstract: The increasing health-seeking behavior and digital consumption of biomedical information by the general public necessitate scalable solutions for automatically adapting complex scientific and technical documents into plain language. Automatic text simplification solutions, including advanced large language models (LLMs), however, continue to face challenges in reliably arbitrating the tension between optimizing readability performance and ensuring preservation of discourse fidelity. This report empirically assesses two major classes of general-purpose LLMs, demonstrating how they navigate the readability-accuracy tension compared to a human benchmark. Using a comparative analysis of the instruction-tuned Mistral-Small 3 24B and the reasoning-augmented QWen2.5 32B, we identify an architectural advantage in the instruction-tuned LLM. Mistral exhibits a tempered lexical simplification strategy that enhances readability across a suite of metrics while preserving human-level discourse with a BERTScore of 0.91. QWen also attains enhanced readability performance and a reasonable BERTScore of 0.89, but its operational strategy shows a disconnect in balancing between readability and accuracy. Additionally, a comprehensive correlation analysis of a suite of 21 metrics spanning readability, discourse fidelity, content safety, and underlying distributional measures for mechanistic insights, confirms strong functional redundancies, and informs metric selection and domain adaptation for text simplification.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamGarden: A Designer Assistant for Growing Games from a Single Prompt</title>
<link>https://arxiv.org/abs/2410.01791</link>
<guid>https://arxiv.org/abs/2410.01791</guid>
<content:encoded><![CDATA[
arXiv:2410.01791v2 Announce Type: replace-cross 
Abstract: Coding assistants are increasingly leveraged in game design, both generating code and making high-level plans. To what degree can these tools align with developer workflows, and what new modes of human-computer interaction can emerge from their use? We present DreamGarden, an AI system capable of assisting with the development of diverse game environments in Unreal Engine. At the core of our method is an LLM-driven planner, capable of breaking down a single, high-level prompt -- a dream, memory, or imagined scenario provided by a human user -- into a hierarchical action plan, which is then distributed across specialized submodules facilitating concrete implementation. This system is presented to the user as a garden of plans and actions, both growing independently and responding to user intervention via seed prompts, pruning, and feedback. Through a user study, we explore design implications of this system, charting courses for future work in semi-autonomous assistants and open-ended simulation design.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEANuT: Parameter-Efficient Adaptation with Weight-aware Neural Tweakers</title>
<link>https://arxiv.org/abs/2410.01870</link>
<guid>https://arxiv.org/abs/2410.01870</guid>
<content:encoded><![CDATA[
arXiv:2410.01870v3 Announce Type: replace-cross 
Abstract: Fine-tuning large pre-trained foundation models often yields excellent downstream performance but is prohibitively expensive when updating all parameters. Parameter-efficient fine-tuning (PEFT) methods such as LoRA alleviate this by introducing lightweight update modules, yet they commonly rely on weight-agnostic linear approximations, limiting their expressiveness. In this work, we propose PEANuT, a novel PEFT framework that introduces weight-aware neural tweakers, compact neural modules that generate task-adaptive updates conditioned on frozen pre-trained weights. PEANuT provides a flexible yet efficient way to capture complex update patterns without full model tuning. We theoretically show that PEANuT achieves equivalent or greater expressivity than existing linear PEFT methods with comparable or fewer parameters. Extensive experiments across four benchmarks with over twenty datasets demonstrate that PEANuT consistently outperforms strong baselines in both NLP and vision tasks, while maintaining low computational overhead.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection</title>
<link>https://arxiv.org/abs/2410.14581</link>
<guid>https://arxiv.org/abs/2410.14581</guid>
<content:encoded><![CDATA[
arXiv:2410.14581v4 Announce Type: replace-cross 
Abstract: Attention mechanisms have revolutionized several domains of artificial intelligence, such as natural language processing and computer vision, by enabling models to selectively focus on relevant parts of the input data. While recent work has characterized the optimization dynamics of gradient descent (GD) in attention-based models and the structural properties of its preferred solutions, less is known about more general optimization algorithms such as mirror descent (MD). In this paper, we investigate the convergence properties and implicit biases of a family of MD algorithms tailored for softmax attention mechanisms, with the potential function chosen as the $p$-th power of the $\ell_p$-norm. Specifically, we show that these algorithms converge in direction to a generalized hard-margin SVM with an $\ell_p$-norm objective when applied to a classification problem using a softmax attention model. Notably, our theoretical results reveal that the convergence rate is comparable to that of traditional GD in simpler models, despite the highly nonlinear and nonconvex nature of the present problem. Additionally, we delve into the joint optimization dynamics of the key-query matrix and the decoder, establishing conditions under which this complex joint optimization converges to their respective hard-margin SVM solutions. Lastly, our numerical experiments on real data demonstrate that MD algorithms improve generalization over standard GD and excel in optimal token selection.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format</title>
<link>https://arxiv.org/abs/2411.17991</link>
<guid>https://arxiv.org/abs/2411.17991</guid>
<content:encoded><![CDATA[
arXiv:2411.17991v2 Announce Type: replace-cross 
Abstract: Recent researches on video large language models (VideoLLM) predominantly focus on model architectures and training datasets, leaving the interaction format between the user and the model under-explored. In existing works, users often interact with VideoLLMs by using the entire video and a query as input, after which the model generates a response. This interaction format constrains the application of VideoLLMs in scenarios such as live-streaming comprehension where videos do not end and responses are required in a real-time manner, and also results in unsatisfactory performance on time-sensitive tasks that requires localizing video segments. In this paper, we focus on a video-text duet interaction format. This interaction format is characterized by the continuous playback of the video, and both the user and the model can insert their text messages at any position during the video playback. When a text message ends, the video continues to play, akin to the alternative of two performers in a duet. We construct MMDuetIT, a video-text training dataset designed to adapt VideoLLMs to video-text duet interaction format. We also introduce the Multi-Answer Grounded Video Question Answering (MAGQA) task to benchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT, MMDuet demonstrates that adopting the video-text duet interaction format enables the model to achieve significant improvements in various time-sensitive tasks (76% CIDEr on YouCook2 dense video captioning, 90\% mAP on QVHighlights highlight detection and 25% R@0.5 on Charades-STA temporal video grounding) with minimal training efforts, and also enable VideoLLMs to reply in a real-time manner as the video plays.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage</title>
<link>https://arxiv.org/abs/2412.15289</link>
<guid>https://arxiv.org/abs/2412.15289</guid>
<content:encoded><![CDATA[
arXiv:2412.15289v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task such as a masked language model task or an element lookup by position task to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration</title>
<link>https://arxiv.org/abs/2412.16216</link>
<guid>https://arxiv.org/abs/2412.16216</guid>
<content:encoded><![CDATA[
arXiv:2412.16216v4 Announce Type: replace-cross 
Abstract: The sparse Mixture-of-Experts (MoE) architecture of large language models (LLMs) confronts an inherent issue of load imbalance arising from the simplistic linear router strategy, which ultimately causes the instability and inefficient learning of LLMs. To address this challenge, we introduce a novel MoE graph-based framework $\textbf{GMoE}$, aimed at enhancing the collaboration among multiple experts. In GMoE, a graph router function is designed to capture the collaboration signals among experts. This enables all experts to dynamically allocate information derived from input data by sharing information with their neighboring experts. Moreover, we put forward two coordination strategies in GMoE: the $\textit{Poisson distribution-based distinction strategy}$ and the $\textit{Normal distribution-based balance strategy}$, to further release the capacity of each expert and increase the model stability in the fine-tuning of LLMs. Specifically, we leverage a parameter-efficient fine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph MoE architecture. Extensive experiments on four real-world benchmark datasets demonstrate the effectiveness of GMoE, showing the benefits of facilitating collaborations of multiple experts in LLM fine-tuning. The code of experimental implementation is available at https://github.com/BAI-LAB/GMoE
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values</title>
<link>https://arxiv.org/abs/2502.00313</link>
<guid>https://arxiv.org/abs/2502.00313</guid>
<content:encoded><![CDATA[
arXiv:2502.00313v2 Announce Type: replace-cross 
Abstract: The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g., intentions or personas) or non-semantic prompting changes (e.g., templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs</title>
<link>https://arxiv.org/abs/2502.15938</link>
<guid>https://arxiv.org/abs/2502.15938</guid>
<content:encoded><![CDATA[
arXiv:2502.15938v2 Announce Type: replace-cross 
Abstract: LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10x decay, could likely have saved a majority of compute by training with D2Z.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniF2F in Rocq: Automatic Translation Between Proof Assistants -- A Case Study</title>
<link>https://arxiv.org/abs/2503.04763</link>
<guid>https://arxiv.org/abs/2503.04763</guid>
<content:encoded><![CDATA[
arXiv:2503.04763v2 Announce Type: replace-cross 
Abstract: In this work, we conduct an experiment using state-of-the-art LLMs to translate MiniF2F into Rocq. The translation task focuses on generating a Rocq theorem based on three sources: a natural language description, the Lean formalization, and the Isabelle formalization. We conducted our experiment in 3 stages of increasing complexity, from basic one-shot prompting to multi-turn conversations that incorporate feedback from unsuccessful attempts. At each stage, we perform multiple rounds of translation using increasingly advanced models: GPT-4o mini, Claude 3.5 Sonnet, o1 mini, and o1. We successfully translated 478 out of 488 theorems. The dataset is opensource: https://github.com/LLM4Rocq/miniF2F-rocq.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Large Language Models, a survey</title>
<link>https://arxiv.org/abs/2503.23037</link>
<guid>https://arxiv.org/abs/2503.23037</guid>
<content:encoded><![CDATA[
arXiv:2503.23037v3 Announce Type: replace-cross 
Abstract: Background: There is great interest in agentic LLMs, large language models that act as agents.
  Objectives: We review the growing body of work in this area and provide a research agenda.
  Methods: Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories.
  Results: The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories.
  Conclusions: We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
arXiv:2504.13837v5 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents</title>
<link>https://arxiv.org/abs/2505.12842</link>
<guid>https://arxiv.org/abs/2505.12842</guid>
<content:encoded><![CDATA[
arXiv:2505.12842v4 Announce Type: replace-cross 
Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\% over the best-performing baseline while only increasing training time by 4.9\% and testing time by 6.5\%. We also experimentally demonstrate that GEM can improve the step-wise success rate by 9.40\% by requesting assistance from the cloud model when encountering OOD samples. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training</title>
<link>https://arxiv.org/abs/2505.13738</link>
<guid>https://arxiv.org/abs/2505.13738</guid>
<content:encoded><![CDATA[
arXiv:2505.13738v2 Announce Type: replace-cross 
Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate $\eta$ and weight decay $\lambda$. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B. Recent work suggests the AdamW timescale, $\tau = B/(\eta \lambda D)$, should remain constant across training settings, and we verify the implication that optimal $\lambda$ scales linearly with B, for a fixed N and D. However, as N and D scale, we show optimal $\tau$ obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict $\lambda$opt in advance of large-scale training. We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast to prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives. All experiments were run on Cerebras CS-3 systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.19536</link>
<guid>https://arxiv.org/abs/2505.19536</guid>
<content:encoded><![CDATA[
arXiv:2505.19536v3 Announce Type: replace-cross 
Abstract: Large vision-language models (LVLMs) excel at multimodal understanding but suffer from high computational costs due to redundant vision tokens. Existing pruning methods typically rely on single-layer attention scores to rank and prune redundant visual tokens to solve this inefficiency. However, as the interaction between tokens and layers is complicated, this raises a basic question: Is such a simple single-layer criterion sufficient to identify redundancy? To answer this question, we rethink the emergence of redundant visual tokens from a fundamental perspective: information flow, which models the interaction between tokens and layers by capturing how information moves between tokens across layers. We find (1) the CLS token acts as an information relay, which can simplify the complicated flow analysis; (2) the redundancy emerges progressively and dynamically via layer-wise attention concentration; and (3) relying solely on attention scores from single layers can lead to contradictory redundancy identification. Based on this, we propose FlowCut, an information-flow-aware pruning framework, mitigating the insufficiency of the current criterion for identifying redundant tokens and better aligning with the model's inherent behaviors. Extensive experiments show that FlowCut achieves superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x speed-up in the prefilling stage. Our code is available at https://github.com/TungChintao/FlowCut
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09532</link>
<guid>https://arxiv.org/abs/2506.09532</guid>
<content:encoded><![CDATA[
arXiv:2506.09532v3 Announce Type: replace-cross 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Generative Categories and Techniques in Multimodal Generative Models</title>
<link>https://arxiv.org/abs/2506.10016</link>
<guid>https://arxiv.org/abs/2506.10016</guid>
<content:encoded><![CDATA[
arXiv:2506.10016v3 Announce Type: replace-cross 
Abstract: Multimodal Generative Models (MGMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Building on a common taxonomy of models and training recipes, we propose a unified evaluation framework centred on faithfulness, compositionality, and robustness, and synthesise evidence from benchmarks and human studies across modalities. We further analyse trustworthiness, safety, and ethical risks, including multimodal bias, privacy leakage, and the misuse of high-fidelity media generation for deepfakes, disinformation, and copyright infringement in music and 3D assets, together with emerging mitigation strategies. Finally, we discuss how architectural trends, evaluation protocols, and governance mechanisms can be co-designed to close current capability and safety gaps, outlining critical paths toward more general-purpose, controllable, and accountable multimodal generative systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IAG: Input-aware Backdoor Attack on VLM-based Visual Grounding</title>
<link>https://arxiv.org/abs/2508.09456</link>
<guid>https://arxiv.org/abs/2508.09456</guid>
<content:encoded><![CDATA[
arXiv:2508.09456v3 Announce Type: replace-cross 
Abstract: Recent advances in vision-language models (VLMs) have significantly enhanced the visual grounding task, which involves locating objects in an image based on natural language queries. Despite these advancements, the security of VLM-based grounding systems has not been thoroughly investigated. This paper reveals a novel and realistic vulnerability: the first multi-target backdoor attack on VLM-based visual grounding. Unlike prior attacks that rely on static triggers or fixed targets, we propose IAG, a method that dynamically generates input-aware, text-guided triggers conditioned on any specified target object description to execute the attack. This is achieved through a text-conditioned UNet that embeds imperceptible target semantic cues into visual inputs while preserving normal grounding performance on benign samples. We further develop a joint training objective that balances language capability with perceptual reconstruction to ensure imperceptibility, effectiveness, and stealth. Extensive experiments on multiple VLMs (e.g., LLaVA, InternVL, Ferret) and benchmarks (RefCOCO, RefCOCO+, RefCOCOg, Flickr30k Entities, and ShowUI) demonstrate that IAG achieves the best ASRs compared with other baselines on almost all settings without compromising clean accuracy, maintaining robustness against existing defenses, and exhibiting transferability across datasets and models. These findings underscore critical security risks in grounding-capable VLMs and highlight the need for further research on trustworthy multimodal understanding.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts</title>
<link>https://arxiv.org/abs/2508.10123</link>
<guid>https://arxiv.org/abs/2508.10123</guid>
<content:encoded><![CDATA[
arXiv:2508.10123v2 Announce Type: replace-cross 
Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In standard ReFT frameworks, a behavior model generates multiple completions with answers per problem, for the answer to be then scored by a reward function. While such RL post-training methods demonstrate significant performance improvements across challenging reasoning domains, the computational cost of generating completions during training with multiple inference steps makes the training cost non-trivial. To address this, we draw inspiration from off-policy RL, and speculative decoding to introduce a novel ReFT framework, dubbed Nested-ReFT, where a subset of layers of the target model acts as the behavior model to generate off-policy completions during training. The behavior model configured with dynamic layer skipping per batch during training decreases the inference cost compared to the standard ReFT frameworks. Our theoretical analysis shows that Nested-ReFT yields unbiased gradient estimates with controlled variance. Our empirical analysis demonstrates improved computational efficiency measured as tokens/sec across multiple math reasoning benchmarks and model sizes. Additionally, we explore three variants of bias mitigation to minimize the off-policyness in the gradient updates that allows for maintaining performance that matches the baseline ReFT performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShortageSim: Simulating Drug Shortages under Information Asymmetry</title>
<link>https://arxiv.org/abs/2509.01813</link>
<guid>https://arxiv.org/abs/2509.01813</guid>
<content:encoded><![CDATA[
arXiv:2509.01813v2 Announce Type: replace-cross 
Abstract: Drug shortages pose critical risks to patient care and healthcare systems worldwide, yet the effectiveness of regulatory interventions remains poorly understood due to information asymmetries in pharmaceutical supply chains. We propose ShortageSim, which addresses this challenge by providing the first simulation framework that evaluates the impact of regulatory interventions on competition dynamics under information asymmetry. Using Large Language Model (LLM)-based agents, the framework models the strategic decisions of drug manufacturers and institutional buyers in response to shortage alerts given by the regulatory agency. Unlike traditional game theory models that assume perfect rationality and complete information, \name simulates heterogeneous interpretations on regulatory announcements and the resulting decisions. Experiments on a self-processed dataset of historical shortage events show that \name reduces the resolution lag for production disruption cases by up to 84\%, achieving closer alignment to real-world trajectories than the zero-shot baseline. Our framework confirms the effect of regulatory alert in addressing shortages and introduces a new method for understanding competition in multi-stage environments under uncertainty. We open-source \name and a dataset of 2,925 FDA shortage events in https://github.com/Lemutisme/ShortageSim, providing a novel framework for future research on policy design and testing in supply chains under information asymmetry.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Situ Tweedie Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2510.01047</link>
<guid>https://arxiv.org/abs/2510.01047</guid>
<content:encoded><![CDATA[
arXiv:2510.01047v2 Announce Type: replace-cross 
Abstract: While diffusion models excel at generating continuous data such as images, adapting them to discrete tasks has relied on indirect approaches that either operate in continuous embedding spaces or use token masking mechanisms, both of which deviate from modeling the true discrete data distribution that can be theoretically guaranteed by Tweedie's formula. We propose in-situ Tweedie Discrete Diffusion (TDD), a framework that performs diffusion guaranteed by Tweedie's formula directly within the discrete one-hot space, hence "in-situ." Unlike prior methods that diffuse continuous embeddings or mask tokens, TDD directly corrupts one-hot vectors with Gaussian noise and performs iterative denoising through a timestep-conditioned cross-entropy objective rather than mean-squared-error reconstruction. At each denoising step, the model predicts class probabilities, applies argmax to obtain discrete predictions, converts them to one-hot vectors, and feeds them into the next iteration with progressively reduced noise. This process naturally unifies discriminative classification and generative modeling under a single framework. Experiments demonstrate that TDD achieves strong performance on both image classification and text generation tasks, with extensive ablation studies confirming the effectiveness of each design component. Our work establishes a principled approach to discrete diffusion that preserves the core characteristics of diffusion models while operating natively in discrete space.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</title>
<link>https://arxiv.org/abs/2510.20792</link>
<guid>https://arxiv.org/abs/2510.20792</guid>
<content:encoded><![CDATA[
arXiv:2510.20792v3 Announce Type: replace-cross 
Abstract: The rapid progress of graph generation has raised new security concerns, particularly regarding backdoor vulnerabilities. While prior work has explored backdoor attacks in image diffusion and unconditional graph generation, conditional, especially text-guided graph generation remains largely unexamined. This paper proposes BadGraph, a backdoor attack method against latent diffusion models for text-guided graph generation. BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs. Extensive experiments on four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the effectiveness and stealth of the attack: less than 10% poisoning rate can achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples. Ablation studies further reveal that the backdoor is implanted during VAE and diffusion training rather than pretraining. These findings reveal the security vulnerabilities in latent diffusion models of text-guided graph generation, highlight the serious risks in models' applications such as drug discovery and underscore the need for robust defenses against the backdoor attack in such diffusion models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought</title>
<link>https://arxiv.org/abs/2511.07772</link>
<guid>https://arxiv.org/abs/2511.07772</guid>
<content:encoded><![CDATA[
<div> Privacy, Large Language Models, Chain of Thought, Steering Activations, Contextual Leakage  

<br /><br />Summary:  
1. This paper addresses a critical privacy issue in Large Language Models (LLMs) functioning as personal assistants that have access to sensitive user data.  
2. Prior works focused on output-level privacy, but recent discoveries show that privacy is also compromised in the model’s internal reasoning process, known as Chain of Thought (CoT), where sensitive information can inadvertently leak.  
3. The main challenge is to prevent such privacy leakage without degrading the model’s reasoning ability and overall utility.  
4. The authors introduce SALT (Steering Activations towards Leakage-free Thinking), a lightweight test-time method that reduces privacy leakage by injecting targeted steering vectors into specific high-leakage layers of the model’s hidden states.  
5. Experimental results demonstrate significant reductions in contextual privacy leakage (CPL), achieving reductions of 18.2% on QwQ-32B, 17.9% on Llama-3.1-8B, and 31.2% on Deepseek within the AirGapAgent-R privacy dataset, all while maintaining comparable task performance.  
6. SALT offers a practical and effective approach for enhancing privacy during the reasoning process of LLMs, facilitating safer deployment of these models as personal assistants. <div>
arXiv:2511.07772v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shona spaCy: A Morphological Analyzer for an Under-Resourced Bantu Language</title>
<link>https://arxiv.org/abs/2511.16680</link>
<guid>https://arxiv.org/abs/2511.16680</guid>
<content:encoded><![CDATA[
<div> Keywords: Shona, morphological analysis, spaCy, Bantu languages, natural language processing<br /><br />Summary:<br /><br />This paper introduces Shona spaCy, an open-source, rule-based morphological analysis pipeline specifically designed for the Bantu language Shona. It addresses the lack of advanced NLP tools for Shona by leveraging the spaCy framework combined with a curated JSON lexicon and linguistically informed rules. The system models key linguistic features such as noun-class prefixes (Mupanda 1-18), verbal subject concords, tense-aspect markers, ideophones, and clitics, integrating them into token-level annotations for lemmas, parts of speech, and morphological features. The toolkit is readily accessible via pip installation (shona-spacy) and its source code is openly hosted on GitHub, promoting transparency and extensibility. Evaluation on both formal and informal Shona text corpora demonstrates strong performance with 90% accuracy for POS tagging and 88% accuracy for morphological feature annotation. By combining descriptive grammar knowledge with computational methods, Shona spaCy significantly advances NLP inclusivity for Shona language speakers. Furthermore, this work serves as a valuable reference model for developing morphological analysis tools targeting other under-resourced Bantu languages, thus supporting broader digital inclusion efforts across African linguistic communities. <div>
arXiv:2511.16680v1 Announce Type: new 
Abstract: Despite rapid advances in multilingual natural language processing (NLP), the Bantu language Shona remains under-served in terms of morphological analysis and language-aware tools. This paper presents Shona spaCy, an open-source, rule-based morphological pipeline for Shona built on the spaCy framework. The system combines a curated JSON lexicon with linguistically grounded rules to model noun-class prefixes (Mupanda 1-18), verbal subject concords, tense-aspect markers, ideophones, and clitics, integrating these into token-level annotations for lemma, part-of-speech, and morphological features. The toolkit is available via pip install shona-spacy, with source code at https://github.com/HappymoreMasoka/shona-spacy and a PyPI release at https://pypi.org/project/shona-spacy/0.1.4/. Evaluation on formal and informal Shona corpora yields 90% POS-tagging accuracy and 88% morphological-feature accuracy, while maintaining transparency in its linguistic decisions. By bridging descriptive grammar and computational implementation, Shona spaCy advances NLP accessibility and digital inclusion for Shona speakers and provides a template for morphological analysis tools for other under-resourced Bantu languages.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search</title>
<link>https://arxiv.org/abs/2511.16681</link>
<guid>https://arxiv.org/abs/2511.16681</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Semantic Pyramid Indexing, vector database, multi-resolution indexing, query-adaptive retrieval<br /><br />Summary:<br /><br />Retrieval-Augmented Generation (RAG) systems enhance large language models by incorporating external knowledge through vector database (VecDB) retrieval pipelines. Traditional methods rely on flat or single-resolution indexes, limiting their ability to adjust to diverse semantic needs in user queries, which results in less optimal retrieval speed and relevance. To overcome this, the authors propose Semantic Pyramid Indexing (SPI), a novel multi-resolution vector indexing framework that enables query-adaptive resolution control within VecDBs. SPI builds a semantic pyramid over document embeddings and dynamically selects the optimal resolution per query using a lightweight classifier. This strategy facilitates progressive retrieval from coarse to fine granularity, improving search efficiency while preserving semantic accuracy. The approach is implemented as a plugin compatible with widely used VecDB backends FAISS and Qdrant. Evaluations on multiple RAG benchmarks — including MS MARCO, Natural Questions, and multimodal retrieval tasks — demonstrate up to 5.7× faster retrieval, 1.8× better memory efficiency, and QA F1 score improvements of up to 2.5 points over strong baselines. Theoretical analyses establish retrieval quality and latency guarantees, while ablation studies confirm the value of each component. SPI’s integration ease makes it suitable for production RAG systems. The code is publicly available at https://github.com/FastLM/SPI_VecDB. <div>
arXiv:2511.16681v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems have become a dominant approach to augment large language models (LLMs) with external knowledge. However, existing vector database (VecDB) retrieval pipelines rely on flat or single-resolution indexing structures, which cannot adapt to the varying semantic granularity required by diverse user queries. This limitation leads to suboptimal trade-offs between retrieval speed and contextual relevance.
  To address this, we propose \textbf{Semantic Pyramid Indexing (SPI)}, a novel multi-resolution vector indexing framework that introduces query-adaptive resolution control for RAG in VecDBs. Unlike existing hierarchical methods that require offline tuning or separate model training, SPI constructs a semantic pyramid over document embeddings and dynamically selects the optimal resolution level per query through a lightweight classifier. This adaptive approach enables progressive retrieval from coarse-to-fine representations, significantly accelerating search while maintaining semantic coverage.
  We implement SPI as a plugin for both FAISS and Qdrant backends and evaluate it across multiple RAG tasks including MS MARCO, Natural Questions, and multimodal retrieval benchmarks. SPI achieves up to \textbf{5.7$\times$} retrieval speedup and \textbf{1.8$\times$} memory efficiency gain while improving end-to-end QA F1 scores by up to \textbf{2.5 points} compared to strong baselines. Our theoretical analysis provides guarantees on retrieval quality and latency bounds, while extensive ablation studies validate the contribution of each component. The framework's compatibility with existing VecDB infrastructures makes it readily deployable in production RAG systems. Code is availabe at \href{https://github.com/FastLM/SPI_VecDB}{https://github.com/FastLM/SPI\_VecDB}.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bench360: Benchmarking Local LLM Inference from 360{\deg}</title>
<link>https://arxiv.org/abs/2511.16682</link>
<guid>https://arxiv.org/abs/2511.16682</guid>
<content:encoded><![CDATA[
<div> Keywords: Bench360, local LLM inference, inference engines, benchmarking, task-specific metrics  

<br /><br />Summary:  
1. The paper introduces Bench360, a comprehensive benchmarking framework designed for evaluating local large language model (LLM) inference setups.  
2. Bench360 addresses the complexity users face due to numerous configuration choices by enabling easy customization of tasks, datasets, and task-specific metrics.  
3. It supports multiple inference engines, quantization levels, and usage scenarios, including single stream, batch, and server modes.  
4. The benchmark captures a wide range of system metrics such as latency, throughput, energy consumption per query, and cold start time, alongside task-specific performance metrics like ROUGE, F1 score, and accuracy.  
5. The researchers demonstrate Bench360’s utility across four common LLM tasks (General Knowledge & Reasoning, QA, Summarization, and Text-to-SQL) on three different hardware platforms and four state-of-the-art inference engines.  
6. Their results expose significant trade-offs between task accuracy and system efficiency, revealing distinct strengths and weaknesses among inference engines and models.  
7. Importantly, no single configuration consistently outperforms others across all metrics and scenarios, emphasizing the necessity of a flexible, user-focused benchmarking tool like Bench360 for optimal local LLM inference configuration. <div>
arXiv:2511.16682v1 Announce Type: new 
Abstract: Running large language models (LLMs) locally is becoming increasingly common. While the growing availability of small open-source models and inference engines has lowered the entry barrier, users now face an overwhelming number of configuration choices. Identifying an optimal configuration -- balancing functional and non-functional requirements -- requires substantial manual effort. While several benchmarks target LLM inference, they are designed for narrow evaluation goals and not user-focused. They fail to integrate relevant system and task-specific metrics into a unified, easy-to-use benchmark that supports multiple inference engines, usage scenarios, and quantization levels. To address this gap, we present Bench360 -- Benchmarking Local LLM Inference from 360{\deg}. Bench360 allows users to easily define their own custom tasks along with datasets and relevant task-specific metrics and then automatically benchmarks selected LLMs, inference engines, and quantization levels across different usage scenarios (single stream, batch & server). Bench360 tracks a wide range of metrics, including (1) system metrics -- such as Computing Performance (e.g., latency, throughput), Resource Usage (e.g., energy per query), and Deployment (e.g., cold start time) -- and (2) task-specific metrics such as ROUGE, F1 score or accuracy. We demonstrate Bench360 on four common LLM tasks -- General Knowledge & Reasoning, QA, Summarization and Text-to-SQL -- across three hardware platforms and four state of the art inference engines. Our results reveal several interesting trade-offs between task performance and system-level efficiency, highlighting the differences in inference engines and models. Most importantly, there is no single best setup for local inference, which strongly motivates the need for a framework such as Bench360.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Well Do LLMs Understand Tunisian Arabic?</title>
<link>https://arxiv.org/abs/2511.16683</link>
<guid>https://arxiv.org/abs/2511.16683</guid>
<content:encoded><![CDATA[
<div> Low-resource languages, Tunisian Arabic, Large Language Models, Translation, Sentiment Analysis

<br /><br />Summary:  
This study focuses on the often-overlooked ability of industrial-scale Large Language Models (LLMs) to understand low-resource languages, specifically Tunisian Arabic (Tunizi). The authors emphasize the risk of excluding millions of Tunisians from fully engaging with AI in their native dialect, which could lead to increased reliance on French or English, threatening the preservation of the local language and impacting literacy and cultural identity. To address this, the researchers introduce a novel multilingual dataset containing parallel texts in Tunisian Arabic, standard Tunisian Arabic, and English, enriched with sentiment labels. They benchmark several popular LLMs across three key NLP tasks: transliteration, translation, and sentiment analysis. The results reveal substantial performance differences among the models, highlighting both their capabilities and current limitations in processing the Tunisian dialect. By quantitatively identifying these gaps, the work stresses the critical need to integrate low-resource languages like Tunizi into future AI developments. This inclusion will help ensure AI technologies are accessible, culturally relevant, and inclusive, supporting language preservation and broader community engagement with AI-driven tools. <div>
arXiv:2511.16683v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are the engines driving today's AI agents. The better these models understand human languages, the more natural and user-friendly the interaction with AI becomes, from everyday devices like computers and smartwatches to any tool that can act intelligently. Yet, the ability of industrial-scale LLMs to comprehend low-resource languages, such as Tunisian Arabic (Tunizi), is often overlooked. This neglect risks excluding millions of Tunisians from fully interacting with AI in their own language, pushing them toward French or English. Such a shift not only threatens the preservation of the Tunisian dialect but may also create challenges for literacy and influence younger generations to favor foreign languages. In this study, we introduce a novel dataset containing parallel Tunizi, standard Tunisian Arabic, and English translations, along with sentiment labels. We benchmark several popular LLMs on three tasks: transliteration, translation, and sentiment analysis. Our results reveal significant differences between models, highlighting both their strengths and limitations in understanding and processing Tunisian dialects. By quantifying these gaps, this work underscores the importance of including low-resource languages in the next generation of AI systems, ensuring technology remains accessible, inclusive, and culturally grounded.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ellipsoid-Based Decision Boundaries for Open Intent Classification</title>
<link>https://arxiv.org/abs/2511.16685</link>
<guid>https://arxiv.org/abs/2511.16685</guid>
<content:encoded><![CDATA[
<div> Keywords: open intent classification, ellipsoid decision boundaries, supervised contrastive learning, adaptive decision boundaries, open-space risk<br /><br />Summary:<br />1. The paper addresses the challenge of textual open intent classification, which is essential for dialogue systems to detect unknown user intents robustly without prior knowledge.<br />2. Existing adaptive decision boundary methods are limited as they assume isotropic (spherical) distributions of known classes, restricting boundaries to balls and ignoring variances in different feature directions.<br />3. The authors propose EliDecide, a novel approach that learns ellipsoid-shaped decision boundaries with varying scales along different feature space directions, offering more flexible class boundaries.<br />4. EliDecide uses supervised contrastive learning to create a discriminative feature space for known samples, improving class separability.<br />5. Learnable matrices are applied to parameterize ellipsoids, replacing rigid spherical boundaries defined solely by centers and radii.<br />6. A new dual loss function is designed to optimize boundaries by balancing empirical risk (covering known samples) and open-space risk (contracting against synthesized pseudo-open samples).<br />7. The method achieves state-of-the-art results across multiple text intent benchmarks and a question classification dataset.<br />8. The ellipsoid flexibility demonstrates superior open intent detection and strong potential for generalization to other text classification tasks in complex open-world scenarios. <div>
arXiv:2511.16685v1 Announce Type: new 
Abstract: Textual open intent classification is crucial for real-world dialogue systems, enabling robust detection of unknown user intents without prior knowledge and contributing to the robustness of the system. While adaptive decision boundary methods have shown great potential by eliminating manual threshold tuning, existing approaches assume isotropic distributions of known classes, restricting boundaries to balls and overlooking distributional variance along different directions. To address this limitation, we propose EliDecide, a novel method that learns ellipsoid decision boundaries with varying scales along different feature directions. First, we employ supervised contrastive learning to obtain a discriminative feature space for known samples. Second, we apply learnable matrices to parameterize ellipsoids as the boundaries of each known class, offering greater flexibility than spherical boundaries defined solely by centers and radii. Third, we optimize the boundaries via a novelly designed dual loss function that balances empirical and open-space risks: expanding boundaries to cover known samples while contracting them against synthesized pseudo-open samples. Our method achieves state-of-the-art performance on multiple text intent benchmarks and further on a question classification dataset. The flexibility of the ellipsoids demonstrates superior open intent detection capability and strong potential for generalization to more text classification tasks in diverse complex open-world scenarios.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Value Steering of Large Language Models</title>
<link>https://arxiv.org/abs/2511.16688</link>
<guid>https://arxiv.org/abs/2511.16688</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, value alignment, prompt engineering, human values, evaluation method  
  
<br /><br />Summary:  
1. The paper addresses the challenge of aligning large language models (LLMs) with human values, emphasizing the limitations of static fine-tuning approaches for dynamic and context-dependent value alignment.  
2. It introduces a practical, reproducible, and model-agnostic procedure to evaluate the effectiveness of prompts in steering LLM-generated text towards specific human values without altering the underlying model.  
3. A formal scoring method is developed to quantify the presence and gain of target values in the model’s generated responses, enabling structured and objective assessment.  
4. The approach is demonstrated using a variant of the Wizard-Vicuna language model, leveraging Schwartz’s theory of basic human values as a framework to define and measure values.  
5. Experimental results on a dialogue dataset show that conditioning prompts explicitly on values can successfully steer the model’s responses, outperforming baseline prompts and illustrating that value steering is achievable through prompt design alone, without model fine-tuning or dynamic prompt optimization. <div>
arXiv:2511.16688v1 Announce Type: new 
Abstract: Large language models are increasingly used in applications where alignment with human values is critical. While model fine-tuning is often employed to ensure safe responses, this technique is static and does not lend itself to everyday situations involving dynamic values and preferences. In this paper, we present a practical, reproducible, and model-agnostic procedure to evaluate whether a prompt candidate can effectively steer generated text toward specific human values, formalising a scoring method to quantify the presence and gain of target values in generated responses. We apply our method to a variant of the Wizard-Vicuna language model, using Schwartz's theory of basic human values and a structured evaluation through a dialogue dataset. With this setup, we compare a baseline prompt to one explicitly conditioned on values, and show that value steering is possible even without altering the model or dynamically optimising prompts.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept-Based Interpretability for Toxicity Detection</title>
<link>https://arxiv.org/abs/2511.16689</link>
<guid>https://arxiv.org/abs/2511.16689</guid>
<content:encoded><![CDATA[
<div> Keywords: toxicity detection, concept-based explanations, Concept Gradient, Word-Concept Alignment, lexicon-free augmentation

<br /><br />Summary:  
This study addresses the challenge of interpreting toxicity detection models by focusing on concept-based explanations rather than solely on input features. 1) It utilizes subtype attributes in toxicity datasets—such as obscene, threat, insult, identity attack, and sexual explicit—as conceptual indicators for identifying toxic language. 2) The authors highlight a key issue where disproportionate attribution of these concepts to the target toxic class leads to misclassification errors. 3) To address this, they extend traditional gradient-based interpretability methods by introducing the Concept Gradient (CG) technique, which causally measures the impact of concept changes on model output. 4) They curate a Targeted Lexicon Set comprising toxic words that contribute to these misclassifications and introduce the Word-Concept Alignment (WCA) score to quantify how much these words cause misclassification due to over-attribution. 5) Finally, the research proposes a lexicon-free data augmentation strategy by generating toxic samples that exclude the predefined toxic lexicons. This augmentation assesses whether over-attribution issues persist when explicit lexical cues are removed, providing deeper insight into how models attribute toxicity beyond narrow lexical overlap. <div>
arXiv:2511.16689v1 Announce Type: new 
Abstract: The rise of social networks has not only facilitated communication but also allowed the spread of harmful content. Although significant advances have been made in detecting toxic language in textual data, the exploration of concept-based explanations in toxicity detection remains limited. In this study, we leverage various subtype attributes present in toxicity detection datasets, such as obscene, threat, insult, identity attack, and sexual explicit as concepts that serve as strong indicators to identify whether language is toxic. However, disproportionate attribution of concepts towards the target class often results in classification errors. Our work introduces an interpretability technique based on the Concept Gradient (CG) method which provides a more causal interpretation by measuring how changes in concepts directly affect the output of the model. This is an extension of traditional gradient-based methods in machine learning, which often focus solely on input features. We propose the curation of Targeted Lexicon Set, which captures toxic words that contribute to misclassifications in text classification models. To assess the significance of these lexicon sets in misclassification, we compute Word-Concept Alignment (WCA) scores, which quantify the extent to which these words lead to errors due to over-attribution to toxic concepts. Finally, we introduce a lexicon-free augmentation strategy by generating toxic samples that exclude predefined toxic lexicon sets. This approach allows us to examine whether over-attribution persists when explicit lexical overlap is removed, providing insights into the model's attribution on broader toxic language patterns.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Falsely Accused: How AI Detectors Misjudge Slightly Polished Arabic Articles</title>
<link>https://arxiv.org/abs/2511.16690</link>
<guid>https://arxiv.org/abs/2511.16690</guid>
<content:encoded><![CDATA[
<div> Keywords: AI detection, Arabic text, polishing, Large Language Models, misclassification<br /><br />Summary:  
1. The paper addresses the problem of AI detection models misclassifying slightly AI-polished human-written articles as AI-generated, which risks unfairly accusing authors of AI plagiarism.  
2. While some work has been done in English, this challenge has not been explored for Arabic texts.  
3. The authors created two datasets: the first contains 800 Arabic articles (half human-authored, half AI-generated) to evaluate 14 Large Language Models (LLMs) and commercial AI detectors for their ability to distinguish human vs. AI content.  
4. The top 8 models from the initial evaluation were selected for further testing using the second dataset, called Ar-APT, which includes 16,400 samples of 400 human-authored Arabic articles polished by 10 LLMs under 4 different polishing settings.  
5. Results show a significant drop in detection accuracy when articles are slightly polished by AI; for example, Claude-4 Sonnet’s accuracy dropped from 83.51% to 57.63%, and originality.AI fell dramatically from 92% to 12% on polished articles.  
6. The study highlights the vulnerability of current AI detectors to slight AI polishing in Arabic text, emphasizing the need for improved detection methods that can better handle this nuance. <div>
arXiv:2511.16690v1 Announce Type: new 
Abstract: Many AI detection models have been developed to counter the presence of articles created by artificial intelligence (AI). However, if a human-authored article is slightly polished by AI, a shift will occur in the borderline decision of these AI detection models, leading them to consider it AI-generated article. This misclassification may result in falsely accusing authors of AI plagiarism and harm the credibility of AI detector models. In English, some efforts were made to meet this challenge, but not in Arabic. In this paper, we generated two datasets. The first dataset contains 800 Arabic articles, half AI-generated and half human-authored. We used it to evaluate 14 Large Language models (LLMs) and commercial AI detectors to assess their ability in distinguishing between human-authored and AI-generated articles. The best 8 models were chosen to act as detectors for our primary concern, which is whether they would consider slightly polished human text as AI-generated. The second dataset, Ar-APT, contains 400 Arabic human-authored articles polished by 10 LLMs using 4 polishing settings, totaling 16400 samples. We use it to evaluate the 8 nominated models and determine whether slight polishing will affect their performance. The results reveal that all AI detectors incorrectly attribute a significant number of articles to AI. The best performing LLM, Claude-4 Sonnet, achieved 83.51%, their performance decreased to 57.63% for articles slightly polished by LLaMA-3. Whereas for the best performing commercial model, originality.AI, that achieves 92% accuracy, dropped to 12% for articles slightly polished by Mistral or Gemma-3.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reproducibility Report: Test-Time Training on Nearest Neighbors for Large Language Models</title>
<link>https://arxiv.org/abs/2511.16691</link>
<guid>https://arxiv.org/abs/2511.16691</guid>
<content:encoded><![CDATA[
<div> Keywords: test-time training, nearest neighbors, GPT-2, retrieval-augmented adaptation, perplexity reduction  

<br /><br />Summary:  
1. This work reproduces the key claims of "Test-Time Training on Nearest Neighbors for Large Language Models" (Hardt and Sun, 2024), which suggests adapting language models during inference by fine-tuning on sequences retrieved via nearest-neighbor search.  
2. Using pretrained RoBERTa embeddings indexed by Faiss, the authors retrieve 20 nearest neighbors per test input and conduct one gradient update per neighbor across several models including GPT-2 (117M and 774M parameters), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B.  
3. The experiments confirm significant improvements in perplexity and bits-per-byte metrics, particularly on structured or specialized datasets like GitHub and EuroParl, demonstrating the effectiveness of test-time training across diverse domains from The Pile.  
4. Models not pretrained on The Pile benefit more from test-time adaptation than those already trained on similar data, enabling smaller models to close the performance gap with larger counterparts.  
5. The authors introduce a memory-efficient retrieval method that loads only necessary line offsets instead of full files, reducing RAM usage drastically from over 128 GB to 32 GB per server.  
6. Evaluations on the modern reasoning-optimized R1-Distilled-Qwen2.5-1.5B model confirm that test-time training yields consistent performance gains even on state-of-the-art architectures.  
7. Overall, the study validates the robustness and generalizability of nearest-neighbor test-time training and highlights practical considerations for large-scale retrieval-augmented model adaptation. <div>
arXiv:2511.16691v1 Announce Type: new 
Abstract: We reproduce the central claims of Test-Time Training on Nearest Neighbors for Large Language Models (Hardt and Sun, 2024), which proposes adapting a language model at inference time by fine-tuning on retrieved nearest-neighbor sequences. Using pretrained RoBERTa embeddings indexed with Faiss, we retrieve 20 neighbors per test input and apply one gradient update per neighbor across GPT-2 (117M, 774M), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B. Our experiments confirm that test-time training significantly reduces perplexity and bits-per-byte metrics across diverse domains from The Pile, with the largest improvements in structured or specialized datasets such as GitHub and EuroParl. We further validate that models not pretrained on The Pile benefit more from this adaptation than models already trained on similar data, allowing smaller models to approach the performance of larger ones. Due to infrastructure limitations, we introduce a memory-efficient retrieval implementation that loads only required line offsets rather than entire files, reducing RAM requirements from over 128 GB per server to 32 GB. We also extend the original study by evaluating R1-Distilled-Qwen2.5-1.5B, showing that test-time training yields consistent gains even for modern reasoning-optimized architectures. Overall, our results support the robustness and generality of nearest-neighbor test-time training while highlighting practical considerations for reproducing large-scale retrieval-augmented adaptation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Language Directions Align with Token Geometry in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2511.16693</link>
<guid>https://arxiv.org/abs/2511.16693</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual LLMs, language representation, transformer layers, token-language alignment, training data composition<br /><br />Summary: This paper presents a detailed analysis of how language information is encoded inside multilingual large language models (LLMs) across all 268 transformer layers in six different models. The study uses both linear and nonlinear probing techniques alongside a novel Token-Language Alignment method to evaluate the layer-wise dynamics and geometry of language representations. The key finding is that language information becomes distinctly separable immediately after the first transformer layer, with a significant increase (+76.4±8.2 percentage points) from the initial embedding layer, and remains almost fully linearly separable through subsequent layers. The authors also uncover a strong relationship between the alignment of language-specific directions and the models’ vocabulary embeddings, which is heavily influenced by the composition of languages in the training dataset. For example, models trained with significant Chinese data exhibit a much stronger structural imprinting effect (ZH Match@Peak of 16.43%) compared to English-centric models (3.90%), indicating a 4.21× difference. This suggests that multilingual LLMs distinguish languages based on deep latent representations shaped by data, rather than simple surface features like script. The research provides valuable insights for designing better multilingual training datasets and addressing fairness in language representation learning. All code and analysis scripts are publicly accessible. <div>
arXiv:2511.16693v1 Announce Type: new 
Abstract: Multilingual LLMs demonstrate strong performance across diverse languages, yet there has been limited systematic analysis of how language information is structured within their internal representation space and how it emerges across layers. We conduct a comprehensive probing study on six multilingual LLMs, covering all 268 transformer layers, using linear and nonlinear probes together with a new Token--Language Alignment analysis to quantify the layer-wise dynamics and geometric structure of language encoding. Our results show that language information becomes sharply separated in the first transformer block (+76.4$\pm$8.2 percentage points from Layer 0 to 1) and remains almost fully linearly separable throughout model depth. We further find that the alignment between language directions and vocabulary embeddings is strongly tied to the language composition of the training data. Notably, Chinese-inclusive models achieve a ZH Match@Peak of 16.43\%, whereas English-centric models achieve only 3.90\%, revealing a 4.21$\times$ structural imprinting effect. These findings indicate that multilingual LLMs distinguish languages not by surface script features but by latent representational structures shaped by the training corpus. Our analysis provides practical insights for data composition strategies and fairness in multilingual representation learning. All code and analysis scripts are publicly available at: https://github.com/thisiskorea/How-Language-Directions-Align-with-Token-Geometry-in-Multilingual-LLMs.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Retrieval with Out-Of-Vocabulary Queries: A Case Study on SNOMED CT</title>
<link>https://arxiv.org/abs/2511.16698</link>
<guid>https://arxiv.org/abs/2511.16698</guid>
<content:encoded><![CDATA[
<div> SNOMED CT, hierarchical concept retrieval, out-of-vocabulary queries, ontology embeddings, biomedical ontology<br /><br />Summary:  
This paper addresses the challenge of retrieving hierarchical concepts from the biomedical ontology SNOMED CT, particularly when queries are out-of-vocabulary (OOV) and do not have direct matches in the ontology. SNOMED CT's complex hierarchical structure and issues such as language ambiguity, synonyms, and polysemies complicate knowledge retrieval. To overcome this, the authors propose a method leveraging language model-based ontology embeddings to improve concept retrieval. They create a specialized evaluation dataset comprising OOV queries annotated with corresponding SNOMED CT concepts to test the effectiveness of their approach. The retrieval tasks focus on identifying the most direct subsumers of the queries as well as their more distant ancestor concepts. Experimental results show that this embedding-based method achieves better performance than baseline methods including SBERT embeddings and two lexical matching techniques. Although the study targets SNOMED CT, the proposed approach is designed to be generalizable and adaptable to other large-scale ontologies. Additionally, the authors have made their code, tools, and datasets publicly available at the provided GitHub repository to facilitate further research and application development in ontology-based information retrieval. <div>
arXiv:2511.16698v1 Announce Type: new 
Abstract: SNOMED CT is a biomedical ontology with a hierarchical representation of large-scale concepts. Knowledge retrieval in SNOMED CT is critical for its application, but often proves challenging due to language ambiguity, synonyms, polysemies and so on. This problem is exacerbated when the queries are out-of-vocabulary (OOV), i.e., having no equivalent matchings in the ontology. In this work, we focus on the problem of hierarchical concept retrieval from SNOMED CT with OOV queries, and propose an approach based on language model-based ontology embeddings. For evaluation, we construct OOV queries annotated against SNOMED CT concepts, testing the retrieval of the most direct subsumers and their less relevant ancestors. We find that our method outperforms the baselines including SBERT and two lexical matching methods. While evaluated against SNOMED CT, the approach is generalisable and can be extended to other ontologies. We release code, tools, and evaluation datasets at https://github.com/jonathondilworth/HR-OOV.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting and Steering LLMs' Empathy in Action</title>
<link>https://arxiv.org/abs/2511.16699</link>
<guid>https://arxiv.org/abs/2511.16699</guid>
<content:encoded><![CDATA[
<div> Keywords: empathy-in-action, large language models, detection, steering, safety training<br /><br />Summary:<br /><br />This paper explores "empathy-in-action," defined as an LLM's willingness to sacrifice task efficiency to meet human needs, by identifying it as a linear direction within the model's activation space. Using contrastive prompts based on the Empathy-in-Action (EIA) benchmark, the study evaluates detection and steering of empathy across three models: Phi-3-mini-4k (3.8B parameters), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored). Detection results reveal high accuracy (AUROC 0.996 to 1.00) at optimal layers for all models, with the uncensored Dolphin model performing comparably to safety-trained ones, indicating empathy encoding arises independently of safety training. Phi-3 probes correlate significantly with EIA behavioral empathy scores (r=0.71, p<0.01), while cross-model correlation is low, suggesting model-architecture-specific implementations despite similar detection capabilities. In steering experiments, Qwen achieves 65.3% success in bidirectional empathy control with coherent responses, Phi-3 reaches 61.7% success, and Dolphin shows a notable asymmetry: 94.4% success steering towards empathy but catastrophic failures when steering against empathy. The findings imply variability in the detection-steering relationship by model, with safety training potentially enhancing steering robustness rather than preventing manipulations. Further validation on additional models is recommended to confirm these insights. <div>
arXiv:2511.16699v1 Announce Type: new 
Abstract: We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored).
  Detection: All models show AUROC 0.996-1.00 at optimal layers. Uncensored Dolphin matches safety-trained models, demonstrating empathy encoding emerges independent of safety training. Phi-3 probes correlate strongly with EIA behavioral scores (r=0.71, p<0.01). Cross-model probe agreement is limited (Qwen: r=-0.06, Dolphin: r=0.18), revealing architecture-specific implementations despite convergent detection.
  Steering: Qwen achieves 65.3% success with bidirectional control and coherence at extreme interventions. Phi-3 shows 61.7% success with similar coherence. Dolphin exhibits asymmetric steerability: 94.4% success for pro-empathy steering but catastrophic breakdown for anti-empathy (empty outputs, code artifacts).
  Implications: The detection-steering gap varies by model. Qwen and Phi-3 maintain bidirectional coherence; Dolphin shows robustness only for empathy enhancement. Safety training may affect steering robustness rather than preventing manipulation, though validation across more models is needed.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation</title>
<link>https://arxiv.org/abs/2511.16787</link>
<guid>https://arxiv.org/abs/2511.16787</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent pipeline, code generation, Bangla instructions, debugging agent, shared task winner<br /><br />Summary: This paper introduces JGU Mainz's system that won the BLP-2025 Shared Task focused on generating code from Bangla language instructions. The system employs a multi-agent pipeline approach, starting with a code-generation agent that creates an initial program from the input instruction. Next, the generated program is tested against the provided unit tests, which are pytest-style and assert-based. Only the tests that fail trigger the next step, where a specialized debugger agent reruns the failing tests, extracts detailed error traces, and then uses this information—combined with the current program and relevant test cases—to generate a corrected version of the code. This iterative approach ensures focused debugging and refinement. The system’s effectiveness is demonstrated by achieving first place in the competition, with a high Pass@1 score of 95.4, indicating strong accuracy in code generation. Additionally, the authors have made their code publicly available, supporting transparency and reproducibility. This work showcases the potential of combining code generation and automated debugging agents for improving the quality of program synthesis from natural language instructions, particularly in low-resource languages like Bangla. <div>
arXiv:2511.16787v1 Announce Type: new 
Abstract: This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Representation to Enactment: The ABC Framework of the Translating Mind</title>
<link>https://arxiv.org/abs/2511.16811</link>
<guid>https://arxiv.org/abs/2511.16811</guid>
<content:encoded><![CDATA[
<div> Keywords: Extended Mind, radical enactivism, translation, Predictive Processing, embodied interaction<br /><br />Summary: This article proposes an innovative approach to understanding the mind involved in translation, challenging traditional representation-based models by drawing on Extended Mind (EM) theory and radical enactivism. It introduces the ABC framework, which highlights translation as an enacted activity that dynamically integrates affective, behavioral, and cognitive processes rather than merely manipulating static interlingual correspondences. The authors incorporate concepts from Predictive Processing and (En)Active Inference to support the view that the translator’s mind emerges through continuous loops of interaction among brain, body, and environment, rather than being a simple extension of the individual mind. This leads to a non-representational understanding of translation, positioning it as a skillful, real-time participatory practice deeply embedded in sociocultural contexts. Meaning is thus co-created through embodied engagement not only with texts but also with the tools and social environments surrounding the translator. This framework shifts focus from internal mental representations to the ongoing, situated activity of translation as an embodied, context-sensitive skill, reconceptualizing translation as a dynamic interplay of mind, body, and culture in action. <div>
arXiv:2511.16811v1 Announce Type: new 
Abstract: Building on the Extended Mind (EM) theory and radical enactivism, this article suggests an alternative to representation-based models of the mind. We lay out a novel ABC framework of the translating mind, in which translation is not the manipulation of static interlingual correspondences but an enacted activity, dynamically integrating affective, behavioral, and cognitive (ABC) processes. Drawing on Predictive Processing and (En)Active Inference, we argue that the translator's mind emerges, rather than being merely extended, through loops of brain-body-environment interactions. This non-representational account reframes translation as skillful participation in sociocultural practice, where meaning is co-created in real time through embodied interaction with texts, tools, and contexts.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable dimensions support an effect of agentivity and telicity on split intransitivity</title>
<link>https://arxiv.org/abs/2511.16824</link>
<guid>https://arxiv.org/abs/2511.16824</guid>
<content:encoded><![CDATA[
<div> agentivity, telicity, unergatives, unaccusatives, interpretable dimensions<br /><br />Summary:<br /><br />1. The article investigates the syntactic classification of intransitive verbs, which are divided into unergatives and unaccusatives, linking them to semantic properties of agentivity and telicity. <br />2. Traditionally, verbs encoding agentive actions are associated with unergative syntax, while verbs depicting telic events correspond to unaccusative syntax. <br />3. Previous research by Kim et al. (2024) challenged this link by showing that human ratings for agentivity and telicity did not reliably predict the syntactic behavior of intransitives. <br />4. The authors revisit this question by employing interpretable dimensions derived from seed words positioned at opposite ends of the agentive and telic scales, rather than relying solely on direct human ratings. <br />5. Their findings reaffirm the connection between unergativity/unaccusativity and agentivity/telicity and demonstrate that combining interpretable semantic dimensions with human judgments provides better evidence for semantic properties that are difficult to assess through rating tasks alone. <div>
arXiv:2511.16824v1 Announce Type: new 
Abstract: Intransitive verbs fall into two different syntactic classes, unergatives and unaccusatives. It has long been argued that verbs describing an agentive action are more likely to appear in an unergative syntax, and those describing a telic event to appear in an unaccusative syntax. However, recent work by Kim et al. (2024) found that human ratings for agentivity and telicity were a poor predictor of the syntactic behavior of intransitives. Here we revisit this question using interpretable dimensions, computed from seed words on opposite poles of the agentive and telic scales. Our findings support the link between unergativity/unaccusativity and agentivity/telicity, and demonstrate that using interpretable dimensions in conjunction with human judgments can offer valuable evidence for semantic properties that are not easily evaluated in rating tasks.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEPPER: Perception-Guided Perturbation for Robust Backdoor Defense in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.16830</link>
<guid>https://arxiv.org/abs/2511.16830</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image diffusion, backdoor defense, PEPPER, prompt rewriting, robustness<br /><br />Summary:<br /><br />Recent advancements in text-to-image (T2I) diffusion models have revealed vulnerabilities to backdoor attacks, where malicious triggers embedded in input prompts can manipulate the model to generate harmful or unintended images. To counter this problem, the paper proposes PEPPER (PErcePtion Guided PERturbation), a novel defense mechanism that modifies the input caption by rewriting it into a semantically distant but visually similar description and introducing subtle, unobtrusive changes. This rewriting strategy effectively disrupts the attack trigger present in the prompt and weakens the impact of trigger tokens, thereby improving the model's robustness against attacks. Experimental results demonstrate that PEPPER is highly effective, especially against attacks targeting text encoders, showing a significant reduction in attack success rates while maintaining high-quality image generation. Furthermore, PEPPER is compatible with existing defense methods and can be integrated with them to provide enhanced and more generalizable protection than any single defense approach alone. The authors also plan to make their implementation available publicly via GitHub, which will facilitate further research and application of this defense strategy in practical scenarios. <div>
arXiv:2511.16830v1 Announce Type: new 
Abstract: Recent studies show that text to image (T2I) diffusion models are vulnerable to backdoor attacks, where a trigger in the input prompt can steer generation toward harmful or unintended content. To address this, we introduce PEPPER (PErcePtion Guided PERturbation), a backdoor defense that rewrites the caption into a semantically distant yet visually similar caption while adding unobstructive elements. With this rewriting strategy, PEPPER disrupt the trigger embedded in the input prompt, dilute the influence of trigger tokens and thereby achieve enhanced robustness. Experiments show that PEPPER is particularly effective against text encoder based attacks, substantially reducing attack success while preserving generation quality. Beyond this, PEPPER can be paired with any existing defenses yielding consistently stronger and generalizable robustness than any standalone method. Our code will be released on Github.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConCISE: A Reference-Free Conciseness Evaluation Metric for LLM-Generated Answers</title>
<link>https://arxiv.org/abs/2511.16846</link>
<guid>https://arxiv.org/abs/2511.16846</guid>
<content:encoded><![CDATA[
<div> Conciseness, Large Language Models, Evaluation Metric, Redundancy, Summarization  

<br /><br />Summary:  
This paper addresses the problem of verbosity and redundancy in responses generated by large language models (LLMs), which negatively impact clarity, user satisfaction, and increase operational costs for developers, especially with token-based pricing models. To tackle this, the authors propose a novel, reference-free metric designed to evaluate the conciseness of LLM outputs without requiring gold standard references. The metric is calculated as the average of three distinct measures: (i) the compression ratio comparing the original response to an LLM-generated abstractive summary; (ii) the compression ratio between the original response and an LLM-generated extractive summary; and (iii) a word-removal compression method, where the LLM removes as many non-essential words as possible while preserving the original meaning, with the extent of token removal serving as the conciseness score. Experimental results validate that this metric effectively detects redundancy in generated responses. This approach enables automated, practical evaluation of the brevity of conversational AI outputs, making it a valuable tool for developers aiming to optimize the clarity and efficiency of LLM responses without relying on human-labeled reference data. <div>
arXiv:2511.16846v1 Announce Type: new 
Abstract: Large language models (LLMs) frequently generate responses that are lengthy and verbose, filled with redundant or unnecessary details. This diminishes clarity and user satisfaction, and it increases costs for model developers, especially with well-known proprietary models that charge based on the number of output tokens. In this paper, we introduce a novel reference-free metric for evaluating the conciseness of responses generated by LLMs. Our method quantifies non-essential content without relying on gold standard references and calculates the average of three calculations: i) a compression ratio between the original response and an LLM abstractive summary; ii) a compression ratio between the original response and an LLM extractive summary; and iii) wordremoval compression, where an LLM removes as many non-essential words as possible from the response while preserving its meaning, with the number of tokens removed indicating the conciseness score. Experimental results demonstrate that our proposed metric identifies redundancy in LLM outputs, offering a practical tool for automated evaluation of response brevity in conversational AI systems without the need for ground truth human annotations.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Latent Reasoning in LLMs via Soft Concept Mixing</title>
<link>https://arxiv.org/abs/2511.16885</link>
<guid>https://arxiv.org/abs/2511.16885</guid>
<content:encoded><![CDATA[
<div> Soft Concept Mixing, latent reasoning, large language models, reinforcement learning, reasoning benchmarks<br /><br />Summary: This paper addresses the limitation of large language models (LLMs) which typically rely on generating discrete tokens, constraining their expressive power in abstract conceptual reasoning. The authors highlight the gap between soft concept reasoning and the discrete token training paradigm of LLMs. To bridge this gap, they propose Soft Concept Mixing (SCM), a novel training scheme that integrates soft concept vectors—probability-weighted averages of embeddings—directly into the model's hidden states, which carry rich contextual information. SCM allows the model to be exposed to soft representations during training, enhancing its latent reasoning capabilities. The entire latent reasoning process under SCM is optimized using Reinforcement Learning (RL), aiming for better performance and training stability. Experimental evaluation on five reasoning benchmarks demonstrates that SCM not only improves the reasoning abilities of LLMs but also maintains a stable training dynamic, showcasing the effectiveness of incorporating soft concept-aware training into LLM frameworks. Overall, SCM provides a promising approach to enhance abstract reasoning in language models by combining soft latent representations with RL-based optimization. <div>
arXiv:2511.16885v1 Announce Type: new 
Abstract: Unlike human reasoning in abstract conceptual spaces, large language models (LLMs) typically reason by generating discrete tokens, which potentially limit their expressive power. The recent work Soft Thinking has shown that LLMs' latent reasoning via soft concepts is a promising direction, but LLMs are trained on discrete tokens. To reduce this gap between the soft concepts in reasoning and the discrete tokens in training, we propose Soft Concept Mixing (SCM), a soft concept aware training scheme that directly exposes the model to soft representations during training. Specifically, SCM constructs a soft concept vector by forming a probability-weighted average of embeddings. Then, this vector is mixed into the model's hidden states, which embody rich contextual information. Finally, the entire latent reasoning process is optimized with Reinforcement Learning (RL). Experiments on five reasoning benchmarks demonstrate that SCM improves the reasoning performance of LLMs, and simultaneously maintains a stable training dynamic.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Improvement Supervision</title>
<link>https://arxiv.org/abs/2511.16886</link>
<guid>https://arxiv.org/abs/2511.16886</guid>
<content:encoded><![CDATA[
<div> Keywords: Tiny Recursive Models, Large Language Models, Abstraction and Reasoning Corpus, classifier-free guidance, training efficiency<br /><br />Summary: This paper explores enhancements to Tiny Recursive Models (TRMs), small looped architectures that have recently been shown to outperform Large Language Models (LLMs) on complex reasoning tasks such as the Abstraction and Reasoning Corpus (ARC). The authors investigate how to improve the efficiency of TRMs with minimal modifications by reframing their latent reasoning process as a form of classifier-free guidance and an implicit policy improvement algorithm. Leveraging these insights, they introduce a novel training scheme that provides a training target for every loop iteration. This new approach leads to a significant increase in training efficiency, reducing the total number of forward passes by 18 times and removing the need for halting mechanisms traditionally used in TRMs. Despite these simplifications, the method maintains performance quality comparable to standard TRMs. Impressively, with only 0.8 million parameters, the proposed model achieves 24% accuracy on ARC-1, outperforming most Large Language Models tested on this benchmark. This work demonstrates that efficient training and parameter use in small recursive architectures can surpass LLMs on challenging reasoning tasks. <div>
arXiv:2511.16886v1 Announce Type: new 
Abstract: Recently, it was shown that small, looped architectures, such as Tiny Recursive Models (TRMs), can outperform Large Language Models (LLMs) on complex reasoning tasks, including the Abstraction and Reasoning Corpus (ARC). In this work, we investigate a core question: how can we further improve the efficiency of these methods with minimal changes? To address this, we frame the latent reasoning of TRMs as a form of classifier-free guidance and implicit policy improvement algorithm. Building on these insights, we propose a novel training scheme that provides a target for each loop during training. We demonstrate that our approach significantly enhances training efficiency. Our method reduces the total number of forward passes by 18x and eliminates halting mechanisms, while maintaining quality comparable to standard TRMs. Notably, we achieve 24% accuracy on ARC-1 with only 0.8M parameters, outperforming most LLMs.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the Formation of Induction Heads</title>
<link>https://arxiv.org/abs/2511.16893</link>
<guid>https://arxiv.org/abs/2511.16893</guid>
<content:encoded><![CDATA[
<div> induction heads, in-context learning, bigram repetition, training data statistics, language models<br /><br />Summary:<br /><br />1. The study focuses on induction heads (IHs), specialized attention heads believed to drive in-context learning (ICL) in language models, but whose exact formation mechanism is not fully understood.<br /><br />2. It demonstrates that a straightforward equation combining batch size and context size can reliably predict the point during training when IHs emerge.<br /><br />3. The research identifies that the frequency and reliability of surface bigram repetition in training data significantly influence IH formation, revealing a precise Pareto frontier balancing these two factors.<br /><br />4. The results show that having local dependencies with high bigram repetition frequency and reliability alone is enough to trigger IH formation.<br /><br />5. However, when bigram frequency and reliability are low, additional factors like categoriality of data and the shape of its marginal distribution become important in determining whether IHs form. This highlights nuanced interactions between data statistics and model internals in shaping ICL capabilities. <div>
arXiv:2511.16893v1 Announce Type: new 
Abstract: Arguably, specialized attention heads dubbed induction heads (IHs) underlie the remarkable in-context learning (ICL) capabilities of modern language models (LMs); yet, a precise characterization of their formation remains unclear. In this study, we investigate the relationship between statistical properties of training data (for both natural and synthetic data) and IH formation. We show that (1) a simple equation combining batch size and context size predicts the point at which IHs form; (2) surface bigram repetition frequency and reliability strongly affect the formation of IHs, and we find a precise Pareto frontier in terms of these two values; and (3) local dependency with high bigram repetition frequency and reliability is sufficient for IH formation, but when the frequency and reliability are low, categoriality and the shape of the marginal distribution matter.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARQUSUMM: Argument-aware Quantitative Summarization of Online Conversations</title>
<link>https://arxiv.org/abs/2511.16985</link>
<guid>https://arxiv.org/abs/2511.16985</guid>
<content:encoded><![CDATA[
<div> Keywords: argument-aware summarization, online conversations, claim-reason structure, large language models, quantitative summarization<br /><br />Summary: This paper addresses the growing need for summarizing argumentative content in online conversations, such as those on Reddit, where controversial topics spark diverse viewpoints. Unlike traditional summarization approaches that focus on general salient information, this work highlights the importance of capturing the deeper argumentative structures within sentences. The authors introduce a novel task called argument-aware quantitative summarization, which aims to expose the claim-reason relationships embedded in conversations and quantify the strength of these arguments. To tackle this task, they propose ARQUSUMM, a new framework that leverages few-shot learning with large language models grounded in argumentation theory to identify propositions and their interrelations inside sentences. For the summarization process, ARQUSUMM employs argument structure-aware clustering algorithms to aggregate similar arguments and quantify their support, thus producing summaries that reflect both content and argument strength. Experimental results demonstrate that ARQUSUMM outperforms existing conversation and quantitative summarization models in terms of generating summaries with clearer argument structures, higher textual quality, and more accurate quantification. The proposed approach offers a valuable tool for better understanding and summarizing complex online discussions by revealing how arguments are constructed and supported. <div>
arXiv:2511.16985v1 Announce Type: new 
Abstract: Online conversations have become more prevalent on public discussion platforms (e.g. Reddit). With growing controversial topics, it is desirable to summarize not only diverse arguments, but also their rationale and justification. Early studies on text summarization focus on capturing general salient information in source documents, overlooking the argumentative nature of online conversations. Recent research on conversation summarization although considers the argumentative relationship among sentences, fail to explicate deeper argument structure within sentences for summarization. In this paper, we propose a novel task of argument-aware quantitative summarization to reveal the claim-reason structure of arguments in conversations, with quantities measuring argument strength. We further propose ARQUSUMM, a novel framework to address the task. To reveal the underlying argument structure within sentences, ARQUSUMM leverages LLM few-shot learning grounded in the argumentation theory to identify propositions within sentences and their claim-reason relationships. For quantitative summarization, ARQUSUMM employs argument structure-aware clustering algorithms to aggregate arguments and quantify their support. Experiments show that ARQUSUMM outperforms existing conversation and quantitative summarization models and generate summaries representing argument structures that are more helpful to users, of high textual quality and quantification accuracy.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities</title>
<link>https://arxiv.org/abs/2511.17012</link>
<guid>https://arxiv.org/abs/2511.17012</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, knowledge graphs, Hunan historical celebrities, instruction fine-tuning, domain-specific extraction<br /><br />Summary:<br /><br />This study explores the application of large language models (LLMs) and knowledge graphs to advance research on historical culture, focusing on the extraction, analysis, and interpretation of cultural heritage related to Hunan's modern historical celebrities shaped by Huxiang culture. Due to limited systematic data resources for these figures and the suboptimal performance of general-purpose models in domain-specific knowledge extraction and structured output generation, the authors propose a supervised fine-tuning approach. They designed a fine-grained, schema-guided instruction template customized for the Hunan historical celebrities domain and created an instruction-tuning dataset to address the scarcity of domain-specific training data. The approach applies parameter-efficient instruction fine-tuning to four publicly available LLMs: Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct. Custom evaluation criteria were developed to assess extraction performance. Experimental findings demonstrate significant performance improvements across all models following fine-tuning, with Qwen3-8B achieving the best results, scoring 89.3866 after training on 100 samples over 50 iterations. The study offers valuable insights into fine-tuning vertical LLMs for regional historical and cultural domains and emphasizes their potential for cost-effective cultural heritage knowledge extraction and knowledge graph construction. <div>
arXiv:2511.17012v1 Announce Type: new 
Abstract: Large language models and knowledge graphs offer strong potential for advancing research on historical culture by supporting the extraction, analysis, and interpretation of cultural heritage. Using Hunan's modern historical celebrities shaped by Huxiang culture as a case study, pre-trained large models can help researchers efficiently extract key information, including biographical attributes, life events, and social relationships, from textual sources and construct structured knowledge graphs. However, systematic data resources for Hunan's historical celebrities remain limited, and general-purpose models often underperform in domain knowledge extraction and structured output generation in such low-resource settings. To address these issues, this study proposes a supervised fine-tuning approach for enhancing domain-specific information extraction. First, we design a fine-grained, schema-guided instruction template tailored to the Hunan historical celebrities domain and build an instruction-tuning dataset to mitigate the lack of domain-specific training corpora. Second, we apply parameter-efficient instruction fine-tuning to four publicly available large language models - Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct - and develop evaluation criteria for assessing their extraction performance. Experimental results show that all models exhibit substantial performance gains after fine-tuning. Among them, Qwen3-8B achieves the strongest results, reaching a score of 89.3866 with 100 samples and 50 training iterations. This study provides new insights into fine-tuning vertical large language models for regional historical and cultural domains and highlights their potential for cost-effective applications in cultural heritage knowledge extraction and knowledge graph construction.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Vision-Language Models Understand Visual Persuasiveness?</title>
<link>https://arxiv.org/abs/2511.17036</link>
<guid>https://arxiv.org/abs/2511.17036</guid>
<content:encoded><![CDATA[
<div> Visual Persuasion, Vision-Language Models, Multi-modal Reasoning, Persuasiveness Judgment, Cognitive Steering<br /><br />Summary:<br /><br />1. The paper investigates whether recent vision-language models (VLMs) genuinely understand visual persuasion, which is how visual cues influence human attitudes and decision-making.<br />2. A high-consensus dataset focused on binary persuasiveness judgment is created, along with a novel taxonomy called Visual Persuasive Factors (VPFs), covering cues from low-level perceptual to high-level semantic features.<br />3. The study evaluates cognitive steering and knowledge injection strategies to improve models' persuasion-relevant reasoning capabilities.<br />4. Empirical results show that current VLMs have a recall-oriented bias, tending to overpredict high persuasiveness, and they exhibit weak ability to discriminate low- and mid-level visual features.<br />5. High-level semantic alignment between the communicated message and the presence of objects is identified as the strongest predictor of human persuasiveness judgments.<br />6. Among interventions, merely providing instructions or unguided reasoning offers little or negative enhancement, whereas concise, object-grounded rationales significantly boost precision and F1 metrics.<br />7. The core limitation of VLMs is pinpointed as the difficulty in linking recognized persuasive objects to the underlying communicative intent, rather than the detection of persuasive elements themselves. <div>
arXiv:2511.17036v1 Announce Type: new 
Abstract: Recent advances in vision-language models (VLMs) have enabled impressive multi-modal reasoning and understanding. Yet, whether these models truly grasp visual persuasion-how visual cues shape human attitudes and decisions-remains unclear. To probe this question, we construct a high-consensus dataset for binary persuasiveness judgment and introduce the taxonomy of Visual Persuasive Factors (VPFs), encompassing low-level perceptual, mid-level compositional, and high-level semantic cues. We also explore cognitive steering and knowledge injection strategies for persuasion-relevant reasoning. Empirical analysis across VLMs reveals a recall-oriented bias-models over-predict high persuasiveness-and weak discriminative power for low/mid-level features. In contrast, high-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment. Among intervention strategies, simple instruction or unguided reasoning scaffolds yield marginal or negative effects, whereas concise, object-grounded rationales significantly improve precision and F1 scores. These results indicate that VLMs core limitation lies not in recognizing persuasive objects but in linking them to communicative intent.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Principled Design of Interpretable Automated Scoring for Large-Scale Educational Assessments</title>
<link>https://arxiv.org/abs/2511.17069</link>
<guid>https://arxiv.org/abs/2511.17069</guid>
<content:encoded><![CDATA[
<div> Keywords: interpretable automated scoring, AnalyticScore framework, short answer scoring, ordinal logistic regression, ASAP-SAS dataset<br /><br />Summary:  
This paper addresses the challenge of creating interpretable automated scoring systems for complex student-generated responses, highlighting the increasing demand for transparency in large-scale assessments. It identifies the needs of various assessment stakeholders and proposes four core principles of interpretability—Faithfulness, Groundedness, Traceability, and Interchangeability (FGTI)—to guide the development of such systems. To demonstrate feasibility, the authors introduce AnalyticScore, a framework designed for short answer scoring that (1) extracts explicitly identifiable elements from student responses, (2) uses large language models (LLMs) to convert these responses into human-interpretable features, and (3) leverages an intuitive ordinal logistic regression model to generate scores. AnalyticScore achieves competitive scoring accuracy, outperforming many existing uninterpretable methods and nearing the state-of-the-art by only 0.06 quadratic weighted kappa (QWK) on average across 10 items from the ASAP-SAS dataset. Furthermore, by comparing the model’s featurization process against human annotators performing the same task, the study demonstrates strong alignment between AnalyticScore’s behavior and human judgment, reinforcing its interpretability and practical relevance for educational assessment. <div>
arXiv:2511.17069v1 Announce Type: new 
Abstract: AI-driven automated scoring systems offer scalable and efficient means of evaluating complex student-generated responses. Yet, despite increasing demand for transparency and interpretability, the field has yet to develop a widely accepted solution for interpretable automated scoring to be used in large-scale real-world assessments. This work takes a principled approach to address this challenge. We analyze the needs and potential benefits of interpretable automated scoring for various assessment stakeholders and develop four principles of interpretability -- Faithfulness, Groundedness, Traceability, and Interchangeability (FGTI) -- targeted at those needs. To illustrate the feasibility of implementing these principles, we develop the AnalyticScore framework for short answer scoring as a baseline reference framework for future research. AnalyticScore operates by (1) extracting explicitly identifiable elements of the responses, (2) featurizing each response into human-interpretable values using LLMs, and (3) applying an intuitive ordinal logistic regression model for scoring. In terms of scoring accuracy, AnalyticScore outperforms many uninterpretable scoring methods, and is within only 0.06 QWK of the uninterpretable SOTA on average across 10 items from the ASAP-SAS dataset. By comparing against human annotators conducting the same featurization task, we further demonstrate that the featurization behavior of AnalyticScore aligns well with that of humans.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUCH: A Multilingual Claim Hallucination Benchmark</title>
<link>https://arxiv.org/abs/2511.17081</link>
<guid>https://arxiv.org/abs/2511.17081</guid>
<content:encoded><![CDATA[
<div> Claim-level Uncertainty Quantification, Large Language Models, multilingual benchmark, generation logits, real-time segmentation  

<br /><br />Summary:  
This work introduces MUCH, the first benchmark dedicated to claim-level Uncertainty Quantification (UQ) specifically designed for Large Language Models (LLMs). MUCH contains 4,873 samples covering four European languages: English, French, Spanish, and German. It evaluates four instruction-tuned open-weight LLMs to ensure diverse and practical assessment scenarios. Unlike existing benchmarks, MUCH provides 24 generation logits per token, enabling the development of white-box UQ methods without the need to regenerate data, thereby increasing reproducibility and research efficiency. The authors propose a novel deterministic segmentation algorithm that segments claims using only about 0.2% of the LLM generation time, a significant improvement over prior manual or LLM-based segmentation strategies. This speed makes the segmentation method practical for real-time monitoring of LLM outputs in deployed systems. Experimental evaluations on MUCH reveal that current claim-level UQ approaches still have considerable shortcomings in both accuracy and computational efficiency, highlighting a substantial scope for improvement. Overall, MUCH aims to set a realistic and reproducible standard for future research advancing reliable uncertainty estimation in multilingual LLM-generated claims. <div>
arXiv:2511.17081v1 Announce Type: new 
Abstract: Claim-level Uncertainty Quantification (UQ) is a promising approach to mitigate the lack of reliability in Large Language Models (LLMs). We introduce MUCH, the first claim-level UQ benchmark designed for fair and reproducible evaluation of future methods under realistic conditions. It includes 4,873 samples across four European languages (English, French, Spanish, and German) and four instruction-tuned open-weight LLMs. Unlike prior claim-level benchmarks, we release 24 generation logits per token, facilitating the development of future white-box methods without re-generating data. Moreover, in contrast to previous benchmarks that rely on manual or LLM-based segmentation, we propose a new deterministic algorithm capable of segmenting claims using as little as 0.2% of the LLM generation time. This makes our segmentation approach suitable for real-time monitoring of LLM outputs, ensuring that MUCH evaluates UQ methods under realistic deployment constraints. Finally, our evaluations show that current methods still have substantial room for improvement in both performance and efficiency.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design</title>
<link>https://arxiv.org/abs/2511.17127</link>
<guid>https://arxiv.org/abs/2511.17127</guid>
<content:encoded><![CDATA[
<div> Mixture-of-experts, AMD MI300X, Pollara interconnect, transformer sizing, large-scale pretraining<br /><br />Summary:<br /><br />This paper presents the first large-scale mixture-of-experts (MoE) pretraining performed entirely on AMD hardware, specifically leveraging MI300X GPUs interconnected via the Pollara network. The authors offer detailed system characterizations including microbenchmarks for GPU communication collectives such as all-reduce, reduce-scatter, all-gather, and broadcast across varying message sizes and GPU counts, marking a novel contribution at this scale. They further provide in-depth MI300X microbenchmarks focusing on kernel sizing and memory bandwidth, informing strategic model design choices. On the modeling front, the study introduces transformer sizing guidelines optimized for MI300X hardware, particularly tuning attention and MLP blocks and selecting MoE widths that balance training speed with inference latency. The training stack is thoroughly described, highlighting utilities like fault tolerance and checkpoint reshaping, alongside a comprehensive training recipe. The authors preview their model architecture and base model, ZAYA1, a MoE model with 760 million active and 8.3 billion total parameters, designed to compete with leading models such as Qwen3-4B and Gemma3-12B. Evaluation shows ZAYA1-base delivers comparable or superior performance over established models like Llama-3-8B and OLMoE on reasoning, mathematics, and coding tasks. Overall, this study validates the maturity and optimization of AMD’s hardware, network, and software stack for competitive large-scale AI pretraining. <div>
arXiv:2511.17127v1 Announce Type: new 
Abstract: We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation</title>
<link>https://arxiv.org/abs/2511.17129</link>
<guid>https://arxiv.org/abs/2511.17129</guid>
<content:encoded><![CDATA[
<div> Keywords: text representation, large language models, context compression, pretext task, contrastive learning  

<br /><br />Summary: The paper addresses the challenge of adapting large language models (LLMs), which are primarily designed for next-token prediction, for producing better holistic text representations. Traditional methods often use token-level prediction pretext tasks, such as masked next-token prediction (MNTP), but these can be suboptimal for capturing overall text meaning. The authors propose a novel pretext task called context compression, where the model learns to generate compact memory tokens that effectively summarize the entire context. This approach allows the model to substitute the whole context with these tokens during downstream sequence prediction tasks. Experimental results show that the compression-based pretraining significantly improves the quality of text representations compared to token-level objectives. Furthermore, integrating contrastive learning with the compression objective yields a new model, LLM2Comp, which not only outperforms other contemporary LLM-based text encoders on various tasks but also demonstrates greater sample efficiency by requiring substantially less training data. Overall, the study highlights context compression as a promising and efficient method for unsupervised adaptation of LLMs to enhance text representation performance. <div>
arXiv:2511.17129v1 Announce Type: new 
Abstract: Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LangMark: A Multilingual Dataset for Automatic Post-Editing</title>
<link>https://arxiv.org/abs/2511.17153</link>
<guid>https://arxiv.org/abs/2511.17153</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Post-Editing, Multilingual Dataset, Neural Machine Translation, Large Language Models, Human Annotation<br /><br />Summary:<br /><br />1. The paper addresses the task of Automatic Post-Editing (APE), which aims to correct errors in machine-translated text to enhance translation quality and reduce human editing effort.<br /><br />2. Despite improvements in Neural Machine Translation (NMT), the development of effective APE systems has been limited by the scarcity of large-scale, multilingual datasets tailored specifically for NMT outputs.<br /><br />3. To fill this gap, the authors introduce LangMark, a new human-annotated multilingual APE dataset focused on English to seven target languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish.<br /><br />4. LangMark includes 206,983 triplets, each consisting of the original source segment, the NMT system’s output, and the corresponding human post-edited translation, all curated by expert linguists to ensure quality and linguistic diversity.<br /><br />5. Using this dataset, the study demonstrates that Large Language Models (LLMs), when applied with few-shot prompting, can perform APE effectively, outperforming state-of-the-art commercial and proprietary machine translation systems.<br /><br />The release of LangMark is expected to facilitate future research and development in APE methods by providing a rich resource for training and evaluation. <div>
arXiv:2511.17153v1 Announce Type: new 
Abstract: Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The PLLuM Instruction Corpus</title>
<link>https://arxiv.org/abs/2511.17161</link>
<guid>https://arxiv.org/abs/2511.17161</guid>
<content:encoded><![CDATA[
<div> Keywords: instruction dataset, large language models, PLLuM, synthetic instructions, linguistic adaptation<br /><br />Summary:<br /><br />This paper introduces the instruction dataset utilized to fine-tune transformer-based large language models (LLMs) developed under the PLLuM (Polish Large Language Model) initiative. The authors provide a functional typology categorizing the instructions into three types: organic (naturally occurring), converted (adapted from existing data), and synthetic (artificially generated). The paper discusses the differences and potential effects between human-authored instructions and synthetic datasets on the linguistic adaptation process of base LLMs, highlighting important observations on model behavior and performance. Furthermore, the authors announce the release of PLLuMIC, the first representative subset of the PLLuM instruction corpus, aimed at serving as a practical resource. They expect PLLuMIC to aid researchers and developers in guiding and planning the creation of comparable instruction datasets for other LLM projects, promoting transparency and facilitating further advancements in model fine-tuning methodologies. The dataset release supports future work in adapting LLMs to languages such as Polish, where dedicated resources are more scarce, and encourages exploration of hybrid instruction data strategies to enhance model quality and versatility. <div>
arXiv:2511.17161v1 Announce Type: new 
Abstract: This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models</title>
<link>https://arxiv.org/abs/2511.17170</link>
<guid>https://arxiv.org/abs/2511.17170</guid>
<content:encoded><![CDATA[
<div> hallucination, abstention, causal inference, knowledge diversity, large language models  

<br /><br />Summary:  
Large Language Models (LLMs) frequently generate fluent but factually inaccurate outputs, a problem referred to as hallucination. To mitigate this, abstention mechanisms are used, where the model deliberately chooses not to answer, often responding with phrases like "I don't know." However, existing abstention approaches depend on signals generated after producing content, such as variations in output or feedback, which restricts their ability to preemptively avoid unreliable answers. This paper introduces Aspect-Based Causal Abstention (ABCA), a novel framework that enables early-stage abstention by analyzing internal knowledge diversity in LLMs through causal inference. The diversity reflects multiple facets of parametric knowledge derived from varied sources, including different disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to evaluate the trustworthiness of information relevant to a query. It supports two abstention types: Type-1, triggered by inconsistent aspect effects indicating knowledge conflict, and Type-2, arising when aspect effects uniformly suggest abstention due to insufficient knowledge. Experimental evaluation on standard benchmarks shows that ABCA not only improves the reliability of abstention decisions but also achieves state-of-the-art performance while enhancing interpretability of the abstention process. <div>
arXiv:2511.17170v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as "I don't know", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification</title>
<link>https://arxiv.org/abs/2511.17184</link>
<guid>https://arxiv.org/abs/2511.17184</guid>
<content:encoded><![CDATA[
<div> Keywords: News classification, Attention mechanism, Feature fusion, Statistical features, Semantic features<br /><br />Summary: This paper addresses the task of news text classification by proposing a novel model called Attention-Guided Feature Fusion (AGFF). The AGFF model integrates traditional statistical features such as term frequencies and TF-IDF with modern semantic features extracted via deep learning, aiming to capture both word-level importance and contextual meaning. An attention-based mechanism is employed to dynamically weigh the significance of each feature type, allowing the model to adaptively emphasize the most relevant information for classification. The study evaluates the AGFF model on benchmark news datasets, where it outperforms conventional statistical methods and purely semantic deep learning models. The results demonstrate that the strategic combination of diverse feature types enhances classification accuracy beyond what either feature set achieves alone. Ablation experiments further confirm the individual contributions of the statistical features, semantic features, and the attention-guided fusion mechanism. Overall, the research highlights the practical effectiveness of balancing statistical and semantic representations to achieve improved news classification performance, suggesting that the AGFF model can serve as a robust solution in real-world digital content management scenarios. <div>
arXiv:2511.17184v1 Announce Type: new 
Abstract: News text classification is a crucial task in natural language processing, essential for organizing and filtering the massive volume of digital content. Traditional methods typically rely on statistical features like term frequencies or TF-IDF values, which are effective at capturing word-level importance but often fail to reflect contextual meaning. In contrast, modern deep learning approaches utilize semantic features to understand word usage within context, yet they may overlook simple, high-impact statistical indicators. This paper introduces an Attention-Guided Feature Fusion (AGFF) model that combines statistical and semantic features in a unified framework. The model applies an attention-based mechanism to dynamically determine the relative importance of each feature type, enabling more informed classification decisions. Through evaluation on benchmark news datasets, the AGFF model demonstrates superior performance compared to both traditional statistical models and purely semantic deep learning models. The results confirm that strategic integration of diverse feature types can significantly enhance classification accuracy. Additionally, ablation studies validate the contribution of each component in the fusion process. The findings highlight the model's ability to balance and exploit the complementary strengths of statistical and semantic representations, making it a practical and effective solution for real-world news classification tasks.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale</title>
<link>https://arxiv.org/abs/2511.17190</link>
<guid>https://arxiv.org/abs/2511.17190</guid>
<content:encoded><![CDATA[
<div> Keywords: AutoLink, schema linking, text-to-SQL, large language models, scalability<br /><br />Summary:<br /><br />This paper addresses the challenge of schema linking in industrial-scale text-to-SQL systems, where providing entire database schemas to Large Language Models (LLMs) is impractical due to context window limitations and irrelevant information. The authors introduce AutoLink, an autonomous agent framework that treats schema linking as an iterative process guided by an LLM. AutoLink dynamically explores and gradually expands the relevant subset of the database schema, effectively identifying necessary components without requiring full schema input. Experimental results show AutoLink achieves state-of-the-art strict schema linking recall rates of 97.4% on the Bird-Dev dataset and 91.2% on Spider-2.0-Lite. It also demonstrates competitive execution accuracy, reaching 68.7% EX on Bird-Dev—surpassing previous methods like CHESS—and 34.9% EX on Spider-2.0-Lite, ranking second on the official leaderboard. Importantly, AutoLink exhibits exceptional scalability, maintaining high recall, efficient token usage, and robust execution accuracy even on very large schemas containing over 3,000 columns, where existing approaches typically fail. These attributes make AutoLink a highly effective and scalable solution for schema linking in real-world text-to-SQL applications. <div>
arXiv:2511.17190v1 Announce Type: new 
Abstract: For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \textbf{97.4\%} on Bird-Dev and \textbf{91.2\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \textbf{68.7\%} EX on Bird-Dev (better than CHESS) and \textbf{34.9\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \textbf{exceptional scalability}, \textbf{maintaining high recall}, \textbf{efficient token consumption}, and \textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2511.17205</link>
<guid>https://arxiv.org/abs/2511.17205</guid>
<content:encoded><![CDATA[
<div> Keywords: layer pruning, large language models, differentiable mask optimization, knowledge distillation, model compression  

<br /><br />Summary:  
The paper addresses challenges in layer pruning for large language models, focusing on performance degradation, high training costs, and limited acceleration during practical deployment. It introduces \name, a novel pruning framework designed to be task-Effective, training-Economical, and inference-Efficient. The framework incorporates two main innovations: a differentiable mask optimization technique leveraging a Gumbel-TopK sampler for precise and efficient pruning mask selection, and an entropy-aware adaptive knowledge distillation method to boost task performance. Extensive experiments conducted on various model architectures and benchmarks demonstrate that \name outperforms state-of-the-art pruning methods. Specifically, the framework achieves 96% accuracy on the MATH-500 dataset after pruning 25% of the layers in the Qwen3-32B model, with only a 0.8% accuracy drop from the original 96.8%, outperforming the previous best of 95%. Additionally, this pruning results in a 1.33× inference speedup while requiring only 0.5 billion tokens (0.5% of post-training data) for fine-tuning, highlighting its training efficiency and practical viability for large-scale model compression. <div>
arXiv:2511.17205v1 Announce Type: new 
Abstract: With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \name, a task-\underline{E}ffective, training-\underline{E}conomical and inference-\underline{E}fficient layer pruning framework. \namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \namespace achieves 96\% accuracy, a mere 0.8\% drop from the original model (96.8\%) on MATH-500 when pruning 25\% layers of Qwen3-32B, outperforming existing SOTA (95\%), with a 1.33$\times$ inference speedup by consuming merely 0.5B tokens (0.5\% of the post-training data volume).
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents</title>
<link>https://arxiv.org/abs/2511.17208</link>
<guid>https://arxiv.org/abs/2511.17208</guid>
<content:encoded><![CDATA[
<div> Keywords: event-centric memory, conversational agents, enriched elementary discourse units, heterogeneous graph, long-horizon interaction<br /><br />Summary:  
1. This paper addresses the challenge that large language model (LLM)-based conversational agents face in maintaining coherent and personalized interaction across many sessions, limited by fixed context windows and current memory methods.  
2. The authors propose an event-centric memory approach inspired by neo-Davidsonian event semantics, which represents conversational history as short, event-like propositions bundling participants, temporal cues, and local context instead of using fragmented relation triples or lossy summaries.  
3. Each conversational session is decomposed into enriched elementary discourse units (EDUs), which are self-contained statements with normalized entities and source turn attributions. These EDUs, along with their arguments and sessions, are organized into a heterogeneous graph structure to enable associative recall.  
4. Two retrieval-based model variants built on this representation use dense similarity search combined with LLM filtering, optionally enhanced by graph-based evidence propagation connecting related EDUs for better aggregation.  
5. Experimental evaluation on LoCoMo and LongMemEval$_S$ benchmarks shows that the event-centric memory models perform comparably or better than strong baselines while using shorter QA contexts, indicating that simple, event-level memory structures can effectively support long-horizon conversational agents.  
The authors will release their code and data for further research and development. <div>
arXiv:2511.17208v1 Announce Type: new 
Abstract: LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</title>
<link>https://arxiv.org/abs/2511.17220</link>
<guid>https://arxiv.org/abs/2511.17220</guid>
<content:encoded><![CDATA[
<div> Keywords: PARROT, sycophancy, robustness, large language models, epistemic collapse<br /><br />Summary: This study introduces PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a novel framework designed to assess how large language models (LLMs) degrade in accuracy under social pressure from authoritative and persuasive prompts, a phenomenon termed sycophancy. PARROT operates by (i) isolating causal effects through double-blind comparisons between neutral and authoritatively false question versions, (ii) quantifying shifts in confidence towards both true and imposed false answers via log-likelihood calibration tracking, and (iii) categorizing failure modes using an eight-state behavioral taxonomy including sycophantic agreement and self-correction. Evaluation of 22 models across 1,302 multiple-choice questions covering 13 domains reveals considerable variability: advanced models like GPT-5 and Claude Sonnet 4.5 show low susceptibility to authority pressure (follow rates ≤11%) with minimal accuracy loss, whereas older or smaller models (e.g., GPT-4, Qwen 2.5-1.5B) suffer severe epistemic collapse (up to 94%). Notably, weaker models not only change responses incorrectly but also decrease confidence in correct answers. Domain analysis shows international law and global knowledge are especially vulnerable, contrasting with relative robustness in elementary mathematics. The paper advocates for prioritizing resistance to social pressure-induced overfitting alongside accuracy, harm avoidance, and privacy for the safe application of LLMs. <div>
arXiv:2511.17220v1 Announce Type: new 
Abstract: This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "follow rates" ($\leq 11\%$, GPT-5: 4\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\%, Qwen 2.5-1.5B: 94\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "resistance to overfitting pressure" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables</title>
<link>https://arxiv.org/abs/2511.17238</link>
<guid>https://arxiv.org/abs/2511.17238</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, Tabular QA, Multilingual Dataset, Visual Noise, Benchmark

<br /><br />Summary:  
The paper introduces MirageTVQA, a novel benchmark designed to evaluate vision-language models (VLMs) on tabular question answering (QA) tasks that better reflect real-world complexities. Existing tabular QA datasets are predominantly English and feature clean, digitally perfect tables, which limits their applicability to practical, noisy environments. MirageTVQA addresses this gap by providing nearly 60,000 QA pairs spanning 24 languages, incorporating tables that include visually imperfect elements to simulate scanned document noise. Evaluation of state-of-the-art VLMs on this benchmark reveals two major challenges: first, a significant performance degradation (over 35% drop for top models) when dealing with visual noise, and second, an English-centric bias where reasoning capabilities do not effectively transfer to other languages. By offering a multilingual and visually noisy dataset, MirageTVQA aims to drive progress toward more robust and versatile VLMs capable of accurate table reasoning across diverse languages and realistic visual conditions. The dataset and code are publicly available, facilitating further research and development in this area. <div>
arXiv:2511.17238v1 Announce Type: new 
Abstract: The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social-Media Based Personas Challenge: Hybrid Prediction of Common and Rare User Actions on Bluesky</title>
<link>https://arxiv.org/abs/2511.17241</link>
<guid>https://arxiv.org/abs/2511.17241</guid>
<content:encoded><![CDATA[
<div> user behavior prediction, social media, rare actions, persona models, hybrid methodology<br /><br />Summary:<br /><br />1. The paper addresses the challenge of predicting user behavior on social media platforms, emphasizing the importance of capturing both frequent actions (like retweeting and liking) and rare but significant behaviors, which have been largely overlooked in prior work.  
2. The authors introduce a hybrid methodology that combines four complementary approaches: (i) a lookup database using historical response patterns, (ii) persona-specific LightGBM models employing temporal and semantic features for common actions, (iii) a hybrid neural network architecture fusing textual and temporal data for classifying rare actions, and (iv) a text reply generation component.  
3. The method is evaluated on a large-scale Bluesky dataset with 6.4 million conversation threads, covering 12 distinct user actions and 25 persona clusters, ensuring diverse user behavior representation.  
4. Results show strong performance with an average macro F1-score of 0.64 for common actions using persona-specific models and 0.56 macro F1-score for rare actions utilizing the specialized neural classifier.  
5. The study concludes that effective social media user behavior prediction requires tailored modeling strategies that recognize the fundamental differences between common and rare action types. This approach secured first place in the SocialSim challenge at the Social Simulation with LLMs workshop at COLM 2025. <div>
arXiv:2511.17241v1 Announce Type: new 
Abstract: Understanding and predicting user behavior on social media platforms is crucial for content recommendation and platform design. While existing approaches focus primarily on common actions like retweeting and liking, the prediction of rare but significant behaviors remains largely unexplored. This paper presents a hybrid methodology for social media user behavior prediction that addresses both frequent and infrequent actions across a diverse action vocabulary. We evaluate our approach on a large-scale Bluesky dataset containing 6.4 million conversation threads spanning 12 distinct user actions across 25 persona clusters. Our methodology combines four complementary approaches: (i) a lookup database system based on historical response patterns; (ii) persona-specific LightGBM models with engineered temporal and semantic features for common actions; (iii) a specialized hybrid neural architecture fusing textual and temporal representations for rare action classification; and (iv) generation of text replies. Our persona-specific models achieve an average macro F1-score of 0.64 for common action prediction, while our rare action classifier achieves 0.56 macro F1-score across 10 rare actions. These results demonstrate that effective social media behavior prediction requires tailored modeling strategies recognizing fundamental differences between action types. Our approach achieved first place in the SocialSim: Social-Media Based Personas challenge organized at the Social Simulation with LLMs workshop at COLM 2025.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation</title>
<link>https://arxiv.org/abs/2511.17290</link>
<guid>https://arxiv.org/abs/2511.17290</guid>
<content:encoded><![CDATA[
<div> Keywords: Estonian translation, WinoGrande, commonsense reasoning, machine translation, prompt engineering

<br /><br />Summary:  
This paper presents a culturally adapted Estonian translation of the WinoGrande test set, a popular commonsense reasoning benchmark. The translation process was carried out by professional translation specialists to ensure linguistic accuracy and cultural relevance. The study evaluates the performance of both proprietary and open-source language models on this human-translated Estonian dataset, finding that model accuracy is slightly lower compared to the original English version. Machine-translated versions of the dataset perform significantly worse, highlighting the quality gap between manual and automated translation approaches. The researchers also investigate whether prompt engineering—designing detailed prompts based on manual translation insights—can improve machine translation quality and model performance. Results indicate that prompt engineering provides only limited benefits in enhancing translation accuracy or downstream model reasoning. Overall, the findings emphasize the critical role of language experts in translating and culturally adapting benchmark datasets to guarantee reliable and interpretable assessments of language understanding and commonsense reasoning in large language models, especially for less-resourced languages like Estonian. <div>
arXiv:2511.17290v1 Announce Type: new 
Abstract: In this paper, we present a localized and culturally adapted Estonian translation of the test set from the widely used commonsense reasoning benchmark, WinoGrande. We detail the translation and adaptation process carried out by translation specialists and evaluate the performance of both proprietary and open source models on the human translated benchmark. Additionally, we explore the feasibility of achieving high-quality machine translation by incorporating insights from the manual translation process into the design of a detailed prompt. This prompt is specifically tailored to address both the linguistic characteristics of Estonian and the unique translation challenges posed by the WinoGrande dataset. Our findings show that model performance on the human translated Estonian dataset is slightly lower than on the original English test set, while performance on machine-translated data is notably worse. Additionally, our experiments indicate that prompt engineering offers limited improvement in translation quality or model accuracy, and highlight the importance of involving language specialists in dataset translation and adaptation to ensure reliable and interpretable evaluations of language competency and reasoning in large language models.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages</title>
<link>https://arxiv.org/abs/2511.17301</link>
<guid>https://arxiv.org/abs/2511.17301</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, large-language models, South African languages, zero-shot performance, social media

<br /><br />Summary:  
This study investigates the zero-shot sentiment analysis capabilities of leading large-language models (LLMs) including GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 on social media posts in English, Sepedi, and Setswana—three languages relevant to South African contexts. The focus is on identifying sentiment polarities related to the 10 most emerging topics within the jurisdiction of 10 South African government departments, aiming to support the detection of social challenges. Results reveal significant variation in performance across different LLMs, topics, and languages, highlighting the complexity of sentiment analysis in a multilingual environment. Importantly, the study shows that combining or fusing the outputs from multiple LLMs significantly enhances sentiment classification accuracy, reducing errors to below 1%. This improvement makes it viable to deploy reliable sentiment analysis systems for social media monitoring, enabling government agencies to better understand public opinion and emotions. Ultimately, such systems can facilitate timely identification of social issues and inform targeted interventions based on the sentiments expressed in diverse language groups, contributing to more precise and effective governance in South Africa’s multilingual society. <div>
arXiv:2511.17301v1 Announce Type: new 
Abstract: Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats</title>
<link>https://arxiv.org/abs/2511.17315</link>
<guid>https://arxiv.org/abs/2511.17315</guid>
<content:encoded><![CDATA[
<div> Keywords: Humanlike Multi-user Agent, large language models, multi-party conversations, AI facilitator, group chat dynamics<br /><br />Summary:<br /><br />1. The paper introduces HUMA, a Humanlike Multi-user Agent, an AI conversational agent designed to participate naturally in asynchronous multi-party group chats using large language models (LLMs).<br />2. Unlike typical one-on-one AI systems, HUMA employs human-like strategies and realistic response-time simulation to enhance interaction quality in group conversations.<br />3. The architecture of HUMA consists of three components: Router, Action Agent, and Reflection, enabling the handling of messages, replies, reactions, and adapting dynamically to group conversation contexts.<br />4. A controlled study with 97 participants in four-person role-play chats compared interactions moderated by HUMA against human community managers (CMs).<br />5. Results showed that participants could not reliably distinguish between HUMA and human facilitators, and subjective measures like community-manager effectiveness, social presence, engagement, and satisfaction exhibited only small differences.<br />6. The findings suggest that AI facilitators like HUMA can match the quality of human moderators in natural group chat environments, maintaining user trust and engagement without easily being identified as nonhuman. <div>
arXiv:2511.17315v1 Announce Type: new 
Abstract: Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.
  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A new kid on the block: Distributional semantics predicts the word-specific tone signatures of monosyllabic words in conversational Taiwan Mandarin</title>
<link>https://arxiv.org/abs/2511.17337</link>
<guid>https://arxiv.org/abs/2511.17337</guid>
<content:encoded><![CDATA[
<div> Keywords: Mandarin tone, pitch contour, semantics, generalized additive model, distributional semantics<br /><br />Summary:<br /><br />1. This study investigates how pitch contours of monosyllabic words are produced in spontaneous conversational Mandarin, emphasizing the role of word meaning. <br /><br />2. The researchers applied a generalized additive model (GAM) to separate observed pitch contours into components linked to various control variables and semantic predictors. <br /><br />3. Even after accounting for factors like word duration, gender, speaker identity, tonal context, vowel height, and utterance position, the specific word itself strongly predicts tonal realization. <br /><br />4. Importantly, the effect is semantic: word sense serves as a better predictor than the word alone, and heterographic homophones (words that sound alike but are written differently) show distinct pitch contours. <br /><br />5. The strongest evidence for semantic influence is that pitch contours of individual word tokens can be accurately predicted using contextualized word embeddings, outperforming a permutation baseline. <br /><br />6. This introduces distributional semantics as a novel approach within phonetics, challenging standard theories of Mandarin tone. <br /><br />7. The findings align with the Discriminative Lexicon Model framework, which integrates semantics with tone realization, suggesting a reconsideration of tone production theories in Mandarin. <div>
arXiv:2511.17337v1 Announce Type: new 
Abstract: We present a corpus-based investigation of how the pitch contours of monosyllabic words are realized in spontaneous conversational Mandarin, focusing on the effects of words' meanings. We used the generalized additive model to decompose a given observed pitch contour into a set of component pitch contours that are tied to different control variables and semantic predictors. Even when variables such as word duration, gender, speaker identity, tonal context, vowel height, and utterance position are controlled for, the effect of word remains a strong predictor of tonal realization. We present evidence that this effect of word is a semantic effect: word sense is shown to be a better predictor than word, and heterographic homophones are shown to have different pitch contours. The strongest evidence for the importance of semantics is that the pitch contours of individual word tokens can be predicted from their contextualized embeddings with an accuracy that substantially exceeds a permutation baseline. For phonetics, distributional semantics is a new kid on the block. Although our findings challenge standard theories of Mandarin tone, they fit well within the theoretical framework of the Discriminative Lexicon Model.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding</title>
<link>https://arxiv.org/abs/2511.17358</link>
<guid>https://arxiv.org/abs/2511.17358</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot, natural language inference, multimodal representations, text-to-image models, robustness  

<br /><br />Summary:  
This paper introduces a novel zero-shot approach to Natural Language Inference (NLI) that integrates multimodal representations by grounding textual premises in visual contexts. The method generates visual depictions of premises using text-to-image models, which are then used to infer relationships with textual hypotheses. Two inference techniques are explored: one based on cosine similarity between visual and textual embeddings, and another leveraging visual question answering frameworks. The proposed approach achieves high accuracy on NLI tasks without requiring any task-specific fine-tuning, showcasing its effectiveness and generalization capabilities. Importantly, the method demonstrates robustness against common textual biases and superficial heuristics often exploited by traditional NLI models. To rigorously assess this robustness, the authors construct a controlled adversarial dataset designed to challenge and validate the system’s performance. Their findings underscore the potential of using visual modality as a grounded meaning representation to enhance natural language understanding. Overall, this work highlights a promising direction for developing more resilient and interpretable NLI models by bridging vision and language in a zero-shot learning framework. <div>
arXiv:2511.17358v1 Announce Type: new 
Abstract: We propose a zero-shot method for Natural Language Inference (NLI) that leverages multimodal representations by grounding language in visual contexts. Our approach generates visual representations of premises using text-to-image models and performs inference by comparing these representations with textual hypotheses. We evaluate two inference techniques: cosine similarity and visual question answering. Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics. Additionally, we design a controlled adversarial dataset to validate the robustness of our approach. Our findings suggest that leveraging visual modality as a meaning representation provides a promising direction for robust natural language understanding.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Rotary Position Embedding</title>
<link>https://arxiv.org/abs/2511.17388</link>
<guid>https://arxiv.org/abs/2511.17388</guid>
<content:encoded><![CDATA[
<div> Keywords: Selective RoPE, Rotary Position Embeddings, input-dependent rotations, softmax transformers, language modeling<br /><br />Summary: This paper addresses the importance of position information in language modeling, highlighting current approaches like Rotary Position Embeddings (RoPE) in softmax transformers, which use fixed-angle rotations to encode positional data. The authors introduce Selective RoPE, a novel input-dependent rotary embedding mechanism that generalizes RoPE by enabling rotations at arbitrary angles, applicable to both linear and softmax transformers. They reveal that softmax attention inherently performs implicit rotations on query-key pairs, indicating a hidden positional structure. Additionally, the study shows that in state-space models and gated linear transformers, the real part is responsible for managing forgetting, while the imaginary part encodes position information through rotations. By integrating Selective RoPE into gated transformers, the researchers demonstrate improved performance across various language modeling tasks and complex sequence tasks such as copying, state tracking, and retrieval. This work suggests that input-dependent positional encoding mechanisms, like Selective RoPE, enhance the model’s ability to capture intricate sequence relationships compared to fixed positional encodings, potentially leading to more effective handling of long-range dependencies and sequence understanding in transformer architectures. <div>
arXiv:2511.17388v1 Announce Type: new 
Abstract: Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\textit{RoPE}) encode positions through \textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \textit{Selective RoPE}, an \textit{input-dependent} rotary embedding mechanism, that generalizes \textit{RoPE}, and enables rotation in \textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PUCP-Metrix: A Comprehensive Open-Source Repository of Linguistic Metrics for Spanish</title>
<link>https://arxiv.org/abs/2511.17402</link>
<guid>https://arxiv.org/abs/2511.17402</guid>
<content:encoded><![CDATA[
<div> Keywords: Spanish NLP, linguistic metrics, readability assessment, machine-generated text detection, open-source tool<br /><br />Summary: PUCP-Metrix is an open-source repository designed to address the limited coverage of Spanish linguistic tools by providing 182 diverse linguistic metrics. These metrics cover lexical diversity, syntactic and semantic complexity, cohesion, psycholinguistics, and readability, enabling detailed and interpretable text analysis. The resource is aimed at enhancing interpretability in natural language processing (NLP) tasks that focus on style, structure, and readability in Spanish texts. The authors evaluate PUCP-Metrix on two tasks: Automated Readability Assessment and Machine-Generated Text Detection, demonstrating its competitive performance when compared to existing repositories and robust neural baselines. PUCP-Metrix’s comprehensive coverage makes it a valuable tool for a wide range of NLP applications, supporting fine-grained analysis and facilitating better understanding and generation of Spanish language texts. Its extensibility ensures that it can evolve with future research needs, contributing significantly to the Spanish NLP community by filling the gap of interpretable linguistic features. Overall, PUCP-Metrix combines extensive linguistic insight with practical applicability, representing an important advance for Spanish language processing. <div>
arXiv:2511.17402v1 Announce Type: new 
Abstract: Linguistic features remain essential for interpretability and tasks involving style, structure, and readability, but existing Spanish tools offer limited coverage. We present PUCP-Metrix, an open-source repository of 182 linguistic metrics spanning lexical diversity, syntactic and semantic complexity, cohesion, psycholinguistics, and readability. PUCP-Metrix enables fine-grained, interpretable text analysis. We evaluate its usefulness on Automated Readability Assessment and Machine-Generated Text Detection, showing competitive performance compared to an existing repository and strong neural baselines. PUCP-Metrix offers a comprehensive, extensible resource for Spanish, supporting diverse NLP applications.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training</title>
<link>https://arxiv.org/abs/2511.17405</link>
<guid>https://arxiv.org/abs/2511.17405</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiple-choice question answering, reinforcement fine-tuning, ReVeL, open-form questions, Qwen2.5-VL<br /><br />Summary: This paper addresses limitations in multiple-choice question answering (MCQA) for evaluating and reinforcement fine-tuning (RFT) of multimodal language models, highlighting that option-based questions can leak exploitable signals leading to unreliable accuracy metrics and answer guessing behaviors. The authors propose ReVeL (Rewrite and Verify by LLM), a novel framework that transforms MCQA items into open-form questions while maintaining verifiable answers according to different answer types, using tailored rewriting and verification methods. Applying ReVeL to 20,000 MCQA examples, they fine-tune Qwen2.5-VL models using GRPO, resulting in models that match MCQA accuracy on benchmarks yet improve open-form question answering accuracy by around six percentage points. This suggests ReVeL offers better data efficiency and more robust reward signals compared to traditional MCQA-based training. Additionally, when used for evaluation, ReVeL exposes up to 20 percentage points of score inflation in legacy MCQA benchmarks relative to open-form QA, enhancing judgment accuracy and reducing computational cost and latency. The authors intend to publicly release the associated code and data, promoting further research and utilization of their approach. <div>
arXiv:2511.17405v1 Announce Type: new 
Abstract: Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation</title>
<link>https://arxiv.org/abs/2511.17432</link>
<guid>https://arxiv.org/abs/2511.17432</guid>
<content:encoded><![CDATA[
<div> Keywords: SMILE, semantic evaluation, lexical exactness, question answering, large language models<br /><br />Summary:<br /><br />This paper introduces SMILE, a novel evaluation metric designed for textual and visual question answering tasks. Traditional metrics like ROUGE, METEOR, and Exact Match tend to emphasize n-gram lexical similarity, which can miss deeper semantic understanding crucial for accurate evaluation. Existing embedding-based measures such as BERTScore and MoverScore partially address semantic similarity but lack flexibility in balancing sentence-level and keyword-level semantics and overlook lexical similarity altogether. Large Language Model (LLM) based evaluators, while powerful, have significant downsides including high computational costs, potential biases, inconsistency, and hallucination issues. SMILE addresses these challenges by integrating sentence-level semantic understanding, keyword-level semantic understanding, and straightforward keyword matching to balance lexical precision with semantic relevance. This composite approach genuinely bridges the gap between lexical exactness and semantic comprehension. Extensive benchmarking of SMILE on textual, image, and video QA datasets demonstrates high correlation with human judgment, confirming its effectiveness. Furthermore, SMILE is computationally lightweight, making it practical for broad adoption in QA evaluation. Overall, SMILE provides a more comprehensive and reliable evaluation metric that aligns better with human assessments compared to existing methods. <div>
arXiv:2511.17432v1 Announce Type: new 
Abstract: Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards</title>
<link>https://arxiv.org/abs/2511.17473</link>
<guid>https://arxiv.org/abs/2511.17473</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-time scaling, RLVR, self-supervised rewards, mathematical reasoning, MR-RLVR<br /><br />Summary:  
This paper addresses challenges in improving large language models' (LLMs) mathematical reasoning through Reinforcement Learning with Verifiable Rewards (RLVR), particularly its limitations in theorem proving where intermediate reasoning steps are critical but final answers are hard to verify. The authors propose MR-RLVR (Masked-and-Reordered RLVR), a novel approach inspired by BERT’s self-supervised learning techniques. MR-RLVR introduces process-level self-supervised rewards using "masked-then-fill" and "step reordering," which help the model learn from intermediate reasoning processes rather than relying solely on outcome verification. The training pipeline consists of two stages: first, self-supervised training on mathematical calculation and proof data, followed by RLVR fine-tuning on datasets where only final outcomes are verifiable. The approach was implemented on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B models and evaluated on multiple math benchmarks including AIME24, AIME25, AMC23, and MATH500. MR-RLVR demonstrated significant improvements over original RLVR under fixed sampling and decoding budgets, with relative gains of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results highlight that incorporating process-aware self-supervised signals effectively enhances the scalability and performance of RLVR in mathematical reasoning tasks with limited verifiable outcomes. <div>
arXiv:2511.17473v1 Announce Type: new 
Abstract: Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via "masked-then-fill" and "step reordering" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RubiSCoT: A Framework for AI-Supported Academic Assessment</title>
<link>https://arxiv.org/abs/2510.17309</link>
<guid>https://arxiv.org/abs/2510.17309</guid>
<content:encoded><![CDATA[
<div> Keywords: thesis evaluation, AI framework, natural language processing, rubric-based scoring, academic assessment<br /><br />Summary:<br /><br />This paper introduces RubiSCoT, an AI-supported framework aimed at improving the evaluation process of academic theses from proposal to final submission. The motivation stems from the limitations of traditional evaluation methods, which, while effective, are often time-consuming and prone to variability among evaluators. RubiSCoT leverages advanced natural language processing techniques, including large language models, retrieval-augmented generation, and structured chain-of-thought prompting, to deliver a more consistent and scalable assessment approach. The framework encompasses several components: preliminary assessments to gauge initial thesis quality, multidimensional assessments that evaluate various facets of the thesis, content extraction to identify key elements, rubric-based scoring for standardized evaluation, and detailed reporting to provide transparent feedback. Through these features, RubiSCoT aims to optimize academic assessment processes by ensuring rigor, consistency, and scalability. The paper details the design and implementation of the system and discusses its potential impact on enhancing transparency and fairness in thesis evaluation, ultimately supporting higher education institutions in maintaining academic integrity efficiently. <div>
arXiv:2510.17309v1 Announce Type: cross 
Abstract: The evaluation of academic theses is a cornerstone of higher education, ensuring rigor and integrity. Traditional methods, though effective, are time-consuming and subject to evaluator variability. This paper presents RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from proposal to final submission. Using advanced natural language processing techniques, including large language models, retrieval-augmented generation, and structured chain-of-thought prompting, RubiSCoT offers a consistent, scalable solution. The framework includes preliminary assessments, multidimensional assessments, content extraction, rubric-based scoring, and detailed reporting. We present the design and implementation of RubiSCoT, discussing its potential to optimize academic assessment processes through consistent, scalable, and transparent evaluation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Shifting Landscape of Vaccine Discourse: Insights From a Decade of Pre- to Post-COVID-19 Vaccine Posts on Social Media</title>
<link>https://arxiv.org/abs/2511.16832</link>
<guid>https://arxiv.org/abs/2511.16832</guid>
<content:encoded><![CDATA[
<div> Keywords: vaccine discourse, social media, COVID-19, sentiment analysis, stereotype content model<br /><br />Summary:<br /><br />This study analyzes English-language vaccine discourse on social media platform X (formerly Twitter) over a ten-year period from 2013 to 2022, covering both pre-COVID and COVID-19 pandemic years. The authors compiled a novel dataset of 18.7 million curated vaccine-related posts, filtered from an original 129 million, enabling a detailed examination of the evolving narratives around vaccines. Employing theories from social cognition and the stereotype content model, the research investigates changes in sentiments and emotions expressed by English-speaking users. Key findings show that during the COVID-19 pandemic, negative emotion word usage decreased, while words related to surprise and trust increased, reflecting nuanced shifts in vaccine perceptions. Early pandemic discourse exhibited more warmth- and competence-focused language, indicating positive attitudes associated with trustworthiness and confidence in vaccines. However, toward the pandemic’s later stages, a rise in negative word usage suggests growing vaccine hesitancy and skepticism among users. This work provides valuable insights into how social media conversations about vaccines have transformed, highlighting the dynamic interplay between public sentiment and health crises over time. <div>
arXiv:2511.16832v1 Announce Type: cross 
Abstract: In this work, we study English-language vaccine discourse in social media posts, specifically posts on X (formerly Twitter), in seven years before the COVID-19 outbreak (2013 to 2019) and three years after the outbreak was first reported (2020 to 2022). Drawing on theories from social cognition and the stereotype content model in Social Psychology, we analyze how English speakers talk about vaccines on social media to understand the evolving narrative around vaccines in social media posts. To do that, we first introduce a novel dataset comprising 18.7 million curated posts on vaccine discourse from 2013 to 2022. This extensive collection-filtered down from an initial 129 million posts through rigorous preprocessing-captures both pre-COVID and COVID-19 periods, offering valuable insights into the evolution of English-speaking X users' perceptions related to vaccines. Our analysis shows that the COVID-19 pandemic led to complex shifts in X users' sentiment and discourse around vaccines. We observe that negative emotion word usage decreased during the pandemic, with notable rises in usage of surprise, and trust related emotion words. Furthermore, vaccine-related language tended to use more warmth-focused words associated with trustworthiness, along with positive, competence-focused words during the early days of the pandemic, with a marked rise in negative word usage towards the end of the pandemic, possibly reflecting a growing vaccine hesitancy and skepticism.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive BASIC: An In-Model Interpreted Reasoning Language for LLMs</title>
<link>https://arxiv.org/abs/2511.16837</link>
<guid>https://arxiv.org/abs/2511.16837</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive BASIC, large language models, stepwise reasoning, knowledge extraction, conflict detection<br /><br />Summary: Cognitive BASIC introduces a minimal, BASIC-style prompting language designed to enhance reasoning capabilities within large language models (LLMs) by structuring their thought processes into explicit, stepwise execution traces. Drawing inspiration from the straightforwardness of retro BASIC, the system uses numbered lines and simple commands as an interpretable control layer that modern LLMs can reliably simulate. This approach enables transparent multi-step reasoning directly inside the model. The language includes a natural-language interpreter file that defines the semantics of commands, memory updates, and logging behavior, which together guide the reasoning process. Moreover, the mental-model interpreter created extracts both declarative and procedural knowledge, facilitating the detection of contradictions within the model's outputs and generating resolutions when inconsistencies are found. The paper evaluates the approach across three different LLMs on benchmarks involving knowledge extraction, conflict detection, and reasoning tasks. The results demonstrate that all tested models can successfully execute Cognitive BASIC programs, exhibiting generally strong but varying performance across different tasks, highlighting the method's potential as an interpretable cognitive control mechanism for improving LLM reasoning. <div>
arXiv:2511.16837v1 Announce Type: cross 
Abstract: Cognitive BASIC is a minimal, BASIC-style prompting language and in-model interpreter that structures large language model (LLM) reasoning into explicit, stepwise execution traces. Inspired by the simplicity of retro BASIC, we repurpose numbered lines and simple commands as an interpretable cognitive control layer. Modern LLMs can reliably simulate such short programs, enabling transparent multi-step reasoning inside the model. A natural-language interpreter file specifies command semantics, memory updates, and logging behavior. Our mental-model interpreter extracts declarative and procedural knowledge, detects contradictions, and produces resolutions when necessary. A comparison across three LLMs on a benchmark of knowledge extraction, conflict detection, and reasoning tasks shows that all models can execute Cognitive BASIC programs, with overall strong but not uniform performance.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fantastic Bugs and Where to Find Them in AI Benchmarks</title>
<link>https://arxiv.org/abs/2511.16842</link>
<guid>https://arxiv.org/abs/2511.16842</guid>
<content:encoded><![CDATA[
arXiv:2511.16842v1 Announce Type: cross 
Abstract: Benchmarks are pivotal in driving AI progress, and invalid benchmark questions frequently undermine their reliability. Manually identifying and correcting errors among thousands of benchmark questions is not only infeasible but also a critical bottleneck for reliable evaluation. In this work, we introduce a framework for systematic benchmark revision that leverages statistical analysis of response patterns to flag potentially invalid questions for further expert review. Our approach builds on a core assumption commonly used in AI evaluations that the mean score sufficiently summarizes model performance. This implies a unidimensional latent construct underlying the measurement experiment, yielding expected ranges for various statistics for each item. When empirically estimated values for these statistics fall outside the expected range for an item, the item is more likely to be problematic. Across nine widely used benchmarks, our method guides expert review to identify problematic questions with up to 84\% precision. In addition, we introduce an LLM-judge first pass to review questions, further reducing human effort. Together, these components provide an efficient and scalable framework for systematic benchmark revision.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists</title>
<link>https://arxiv.org/abs/2511.16931</link>
<guid>https://arxiv.org/abs/2511.16931</guid>
<content:encoded><![CDATA[
arXiv:2511.16931v1 Announce Type: cross 
Abstract: With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as "AI Scientists." However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Language Models are Confused Tourists</title>
<link>https://arxiv.org/abs/2511.17004</link>
<guid>https://arxiv.org/abs/2511.17004</guid>
<content:encoded><![CDATA[
arXiv:2511.17004v1 Announce Type: cross 
Abstract: Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Computational Framework for Discrete Fuzzy Numbers Based on Total Orders</title>
<link>https://arxiv.org/abs/2511.17080</link>
<guid>https://arxiv.org/abs/2511.17080</guid>
<content:encoded><![CDATA[
arXiv:2511.17080v1 Announce Type: cross 
Abstract: Discrete fuzzy numbers, and in particular those defined over a finite chain $L_n = \{0, \ldots, n\}$, have been effectively employed to represent linguistic information within the framework of fuzzy systems. Research on total (admissible) orderings of such types of fuzzy subsets, and specifically those belonging to the set $\mathcal{D}_1^{L_n\rightarrow Y_m}$ consisting of discrete fuzzy numbers $A$ whose support is a closed subinterval of the finite chain $L_n = \{0, 1, \ldots, n\}$ and whose membership values $A(x)$, for $x \in L_n$, belong to the set $Y_m = \{ 0 = y_1 < y_2 < \cdots < y_{m-1} < y_m = 1 \}$, has facilitated the development of new methods for constructing logical connectives, based on a bijective function, called $\textit{pos function}$, that determines the position of each $A \in \mathcal{D}_1^{L_n\rightarrow Y_m}$. For this reason, in this work we revisit the problem by introducing algorithms that exploit the combinatorial structure of total (admissible) orders to compute the $\textit{pos}$ function and its inverse with exactness. The proposed approach achieves a complexity of $\mathcal{O}(n^{2} m \log n)$, which is quadratic in the size of the underlying chain ($n$) and linear in the number of membership levels ($m$). The key point is that the dominant factor is $m$, ensuring scalability with respect to the granularity of membership values. The results demonstrate that this formulation substantially reduces computational cost and enables the efficient implementation of algebraic operations -- such as aggregation and implication -- on the set of discrete fuzzy numbers.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric-Disentangelment Unlearning</title>
<link>https://arxiv.org/abs/2511.17100</link>
<guid>https://arxiv.org/abs/2511.17100</guid>
<content:encoded><![CDATA[
arXiv:2511.17100v1 Announce Type: cross 
Abstract: Machine unlearning, the removal of a training subset's influence from a deployed model, is critical for privacy preservation and model reliability, yet gradient ascent on forget samples often harms retained knowledge. Existing approaches face a persistent tradeoff between effective forgetting and preservation on the retain set. While previous methods provide useful heuristics, they often lack a formal analysis on how exactly forgetting updates harm retained knowledge, and whether the side effects can be removed with theoretical guarantees. To explore a theoretically sound and simple solution, we start from the first principle on how performance on the retain set is actually affected: a first-order analysis of the local change of the retain loss under small parameter updates during model training. We start from a crisp equivalence: the retain loss is unchanged to first order iff the update direction is orthogonal to the subspace spanned by retain gradients ("retain-invariant"). This identifies the entangled component as the tangential part of forget update within the retain-gradient subspace, and characterizes disentanglement as orthogonality. Guided by this, we propose the Geometric-disentanglement Unlearning (GU) that decomposes any candidate forget gradient update into tangential and normal components to retain space and executes only the normal component. Under a standard trust-region budget, the projected direction aligned with the raw forget gradient is optimal among all first-order retain-invariant moves, and we also derive the optimal projected direction for joint forget-retain updating objectives. Our method is plug-and-play and can be attached to existing gradient-based unlearning procedures to mitigate side effects. GU achieves consistent improvement on various methods across three benchmarks TOFU, MUSE, and WMDP.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-cultural value alignment frameworks for responsible AI governance: Evidence from China-West comparative analysis</title>
<link>https://arxiv.org/abs/2511.17256</link>
<guid>https://arxiv.org/abs/2511.17256</guid>
<content:encoded><![CDATA[
arXiv:2511.17256v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) increasingly influence high-stakes decision-making across global contexts, ensuring their alignment with diverse cultural values has become a critical governance challenge. This study presents a Multi-Layered Auditing Platform for Responsible AI that systematically evaluates cross-cultural value alignment in China-origin and Western-origin LLMs through four integrated methodologies: Ethical Dilemma Corpus for assessing temporal stability, Diversity-Enhanced Framework (DEF) for quantifying cultural fidelity, First-Token Probability Alignment for distributional accuracy, and Multi-stAge Reasoning frameworK (MARK) for interpretable decision-making. Our comparative analysis of 20+ leading models, such as Qwen, GPT-4o, Claude, LLaMA, and DeepSeek, reveals universal challenges-fundamental instability in value systems, systematic under-representation of younger demographics, and non-linear relationships between model scale and alignment quality-alongside divergent regional development trajectories. While China-origin models increasingly emphasize multilingual data integration for context-specific optimization, Western models demonstrate greater architectural experimentation but persistent U.S.-centric biases. Neither paradigm achieves robust cross-cultural generalization. We establish that Mistral-series architectures significantly outperform LLaMA3-series in cross-cultural alignment, and that Full-Parameter Fine-Tuning on diverse datasets surpasses Reinforcement Learning from Human Feedback in preserving cultural variation...
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core</title>
<link>https://arxiv.org/abs/2511.17323</link>
<guid>https://arxiv.org/abs/2511.17323</guid>
<content:encoded><![CDATA[
arXiv:2511.17323v1 Announce Type: cross 
Abstract: Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM</title>
<link>https://arxiv.org/abs/2511.17335</link>
<guid>https://arxiv.org/abs/2511.17335</guid>
<content:encoded><![CDATA[
arXiv:2511.17335v1 Announce Type: cross 
Abstract: Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning with Sketch-Guided Verification for Physics-Aware Video Generation</title>
<link>https://arxiv.org/abs/2511.17450</link>
<guid>https://arxiv.org/abs/2511.17450</guid>
<content:encoded><![CDATA[
arXiv:2511.17450v1 Announce Type: cross 
Abstract: Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniLLM: Knowledge Distillation of Large Language Models</title>
<link>https://arxiv.org/abs/2306.08543</link>
<guid>https://arxiv.org/abs/2306.08543</guid>
<content:encoded><![CDATA[
arXiv:2306.08543v5 Announce Type: replace 
Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective on-policy optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings</title>
<link>https://arxiv.org/abs/2411.05986</link>
<guid>https://arxiv.org/abs/2411.05986</guid>
<content:encoded><![CDATA[
arXiv:2411.05986v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) has been proven to be an effective and robust method for training neural machine translation systems, especially when paired with powerful reward models that accurately assess translation quality. However, most research has focused on RL methods that use sentence-level feedback, leading to inefficient learning signals due to the reward sparsity problem -- the model receives a single score for the entire sentence. To address this, we propose a novel approach that leverages fine-grained, token-level quality assessments along with error severity levels using RL methods. Specifically, we use xCOMET, a state-of-the-art quality estimation system, as our token-level reward model. We conduct experiments on small and large translation datasets with standard encoder-decoder and large language models-based machine translation systems, comparing the impact of sentence-level versus fine-grained reward signals on translation quality. Our results show that training with token-level rewards improves translation quality across language pairs over baselines according to both automatic and human evaluation. Furthermore, token-level reward optimization improves training stability, evidenced by a steady increase in mean rewards over training epochs.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Aligned Tool Recommendation for Large Language Models</title>
<link>https://arxiv.org/abs/2411.09613</link>
<guid>https://arxiv.org/abs/2411.09613</guid>
<content:encoded><![CDATA[
arXiv:2411.09613v2 Announce Type: replace 
Abstract: By augmenting Large Language Models (LLMs) with external tools, their capacity to solve complex problems has been significantly enhanced. However, despite ongoing advancements in the parsing capabilities of LLMs, incorporating all available tools simultaneously in the prompt remains impractical due to the vast number of external tools. Consequently, it is essential to provide LLMs with a precise set of tools tailored to the specific task, considering both quantity and quality. Current tool retrieval methods primarily focus on refining the ranking list of tools and directly packaging a fixed number of top-ranked tools as the tool set. However, these approaches often fail to equip LLMs with the optimal set of tools prior to execution, since the optimal number of tools for different tasks could be different, resulting in inefficiencies such as redundant or unsuitable tools, which impede immediate access to the most relevant tools. This paper addresses the challenge of recommending precise toolsets for LLMs. We introduce the problem of tool recommendation, define its scope, and propose a novel Precision-driven Tool Recommendation (PTR) approach. PTR captures an initial, concise set of tools by leveraging historical tool bundle usage and dynamically adjusts the tool set by performing tool matching, culminating in a multi-view-based tool addition. Additionally, we present a new dataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness of tool recommendation for LLMs. We further validate our design choices through comprehensive experiments, demonstrating promising accuracy across two open benchmarks and our RecTools dataset.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EventWeave: A Dynamic Framework for Capturing Core and Supporting Events in Dialogue Systems</title>
<link>https://arxiv.org/abs/2503.23078</link>
<guid>https://arxiv.org/abs/2503.23078</guid>
<content:encoded><![CDATA[
arXiv:2503.23078v2 Announce Type: replace 
Abstract: Large language models have improved dialogue systems, but often process conversational turns in isolation, overlooking the event structures that guide natural interactions. Hence we introduce \textbf{EventWeave}, a framework that explicitly models relationships between conversational events to generate more contextually appropriate dialogue responses. EventWeave constructs a dynamic event graph that distinguishes between core events (main goals) and supporting events (interconnected details), employing a multi-head attention mechanism to selectively determine which events are most relevant to the current turn. Unlike summarization or standard graph-based approaches, our method captures three distinct relationship types between events, allowing for more nuanced context modeling. Experiments on three dialogue datasets demonstrate that EventWeave produces more natural and contextually appropriate responses while requiring less computational overhead than models processing the entire dialogue history. Ablation studies confirm improvements stem from better event relationship modeling rather than increased information density. Our approach effectively balances comprehensive context understanding with generating concise responses, maintaining strong performance across various dialogue lengths through targeted optimization techniques.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concise Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05185</link>
<guid>https://arxiv.org/abs/2504.05185</guid>
<content:encoded><![CDATA[
arXiv:2504.05185v3 Announce Type: replace 
Abstract: A major drawback of reasoning models is their excessive token usage, inflating computational cost, resource demand, and latency. We show this verbosity stems not from deeper reasoning but from reinforcement learning loss minimization when models produce incorrect answers. With unsolvable problems dominating training, this effect compounds into a systematic tendency toward longer outputs. Through theoretical analysis of PPO and GRPO, we prove that incorrect answers inherently drive policies toward verbosity \textit{even when} $\gamma=1$, reframing response lengthening as an optimization artifact. We further uncover a consistent correlation between conciseness and correctness across reasoning and non-reasoning models. Building on these insights, we propose a two-phase RL procedure where a brief secondary stage, trained on a small set of solvable problems, significantly reduces response length while preserving or improving accuracy. Finally, we show that while GRPO shares properties with PPO, it exhibits collapse modes, limiting its reliability for concise reasoning. Our claims are supported by extensive experiments.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Rise of Parameter Specialization for Knowledge Storage in Large Language Models</title>
<link>https://arxiv.org/abs/2505.17260</link>
<guid>https://arxiv.org/abs/2505.17260</guid>
<content:encoded><![CDATA[
arXiv:2505.17260v2 Announce Type: replace 
Abstract: Over time, a growing wave of large language models from various series has been introduced to the community. Researchers are striving to maximize the performance of language models with constrained parameter sizes. However, from a microscopic perspective, there has been limited research on how to better store knowledge in model parameters, particularly within MLPs, to enable more effective utilization of this knowledge by the model. In this work, we analyze twenty publicly available open-source large language models to investigate the relationship between their strong performance and the way knowledge is stored in their corresponding MLP parameters. Our findings reveal that as language models become more advanced and demonstrate stronger knowledge capabilities, their parameters exhibit increased specialization. Specifically, parameters in the MLPs tend to be more focused on encoding similar types of knowledge. We experimentally validate that this specialized distribution of knowledge contributes to improving the efficiency of knowledge utilization in these models. Furthermore, by conducting causal training experiments, we confirm that this specialized knowledge distribution plays a critical role in improving the model's efficiency in leveraging stored knowledge.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions</title>
<link>https://arxiv.org/abs/2505.23662</link>
<guid>https://arxiv.org/abs/2505.23662</guid>
<content:encoded><![CDATA[
arXiv:2505.23662v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in using external tools to address user inquiries. However, most existing evaluations assume tool use in short contexts, offering limited insight into model behavior during realistic long-term interactions. To fill this gap, we introduce ToolHaystack, a benchmark for testing the tool use capabilities in long-term interactions. Each test instance in ToolHaystack includes multiple tasks execution contexts and realistic noise within a continuous conversation, enabling assessment of how well models maintain context and handle various disruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find that while current models perform well in standard multi-turn settings, they often significantly struggle in ToolHaystack, highlighting critical gaps in their long-term robustness not revealed by previous tool benchmarks.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness Evaluation of Large Language Models in Academic Library Reference Services</title>
<link>https://arxiv.org/abs/2507.04224</link>
<guid>https://arxiv.org/abs/2507.04224</guid>
<content:encoded><![CDATA[
arXiv:2507.04224v3 Announce Type: replace 
Abstract: As libraries explore large language models (LLMs) for use in virtual reference services, a key question arises: Can LLMs serve all users equitably, regardless of demographics or social status? While they offer great potential for scalable support, LLMs may also reproduce societal biases embedded in their training data, risking the integrity of libraries' commitment to equitable service. To address this concern, we evaluate whether LLMs differentiate responses across user identities by prompting six state-of-the-art LLMs to assist patrons differing in sex, race/ethnicity, and institutional role. We find no evidence of differentiation by race or ethnicity, and only minor evidence of stereotypical bias against women in one model. LLMs demonstrate nuanced accommodation of institutional roles through the use of linguistic choices related to formality, politeness, and domain-specific vocabularies, reflecting professional norms rather than discriminatory treatment. These findings suggest that current LLMs show a promising degree of readiness to support equitable and contextually appropriate communication in academic library reference services.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models</title>
<link>https://arxiv.org/abs/2507.05248</link>
<guid>https://arxiv.org/abs/2507.05248</guid>
<content:encoded><![CDATA[
arXiv:2507.05248v2 Announce Type: replace 
Abstract: Contextual priming, where earlier stimuli covertly bias later judgments, offers an unexplored attack surface for large language models (LLMs). We uncover a contextual priming vulnerability in which the previous response in the dialogue can steer its subsequent behavior toward policy-violating content. While existing jailbreak attacks largely rely on single-turn or multi-turn prompt manipulations, or inject static in-context examples, these methods suffer from limited effectiveness, inefficiency, or semantic drift. We introduce Response Attack (RA), a novel framework that strategically leverages intermediate, mildly harmful responses as contextual primers within a dialogue. By reformulating harmful queries and injecting these intermediate responses before issuing a targeted trigger prompt, RA exploits a previously overlooked vulnerability in LLMs. Extensive experiments across eight state-of-the-art LLMs show that RA consistently achieves significantly higher attack success rates than nine leading jailbreak baselines. Our results demonstrate that the success of RA is directly attributable to the strategic use of intermediate responses, which induce models to generate more explicit and relevant harmful content while maintaining stealth, efficiency, and fidelity to the original query. The code and data are available at https://github.com/Dtc7w3PQ/Response-Attack.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Messages from Parler</title>
<link>https://arxiv.org/abs/2507.10810</link>
<guid>https://arxiv.org/abs/2507.10810</guid>
<content:encoded><![CDATA[
arXiv:2507.10810v2 Announce Type: replace 
Abstract: We examined how online hate is motivated by receiving social approval via Walther's (2024) social approval theory of online hate, which argues (H1a) more signals of social approval on hate messages predicts more subsequent hate messages, and (H1b) as social approval increases, hate speech becomes more extreme. Using 110 million messages from Parler (2018-2021), we observed the number of upvotes received on a hate speech post was unassociated with hate speech in one's next post and during the next month, three-months, and six-months. The number of upvotes received on (extreme) hate speech comments, however, was positively associated with (extreme) hate speech during the next week, month, three-months, and six-months. Between-person effects revealed an average positive relationship between social approval and hate speech production at all time intervals. For comments, social approval linked more strongly to online hate than social disapproval. Social approval is a critical mechanism facilitating online hate propagation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs produce texts with "human-like" lexical diversity?</title>
<link>https://arxiv.org/abs/2508.00086</link>
<guid>https://arxiv.org/abs/2508.00086</guid>
<content:encoded><![CDATA[
arXiv:2508.00086v2 Announce Type: replace 
Abstract: The degree to which large language models (LLMs) produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (ChatGPT-3.5, ChatGPT-4, ChatGPT-o4 mini, and ChatGPT-4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAs, and Support Vector Machines revealed that the ChatGPT-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and ChatGPT-4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity than older models despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that ChatGPT models do not produce human-like texts in relation to lexical diversity, and the newer models produce less human-like text than older models. We discuss the implications of these results for language pedagogy and related applications.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding</title>
<link>https://arxiv.org/abs/2508.13804</link>
<guid>https://arxiv.org/abs/2508.13804</guid>
<content:encoded><![CDATA[
arXiv:2508.13804v3 Announce Type: replace 
Abstract: How do Large Language Models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models provides the answer. In contrast to prior work using deterministic ground truth (majority or inclusion rules), we model annotator disagreements to capture both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model domain sensitivity). We evaluated the best language models (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from nearly 700 annotators in 100K+ texts spanning social networks, news and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing that AI models typically rank among the top 25\% of human annotators, performing much better than average balanced accuracy. Importantly, we find that AI produces far fewer false negatives than humans, highlighting their more sensitive moral detection capabilities.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering</title>
<link>https://arxiv.org/abs/2510.01612</link>
<guid>https://arxiv.org/abs/2510.01612</guid>
<content:encoded><![CDATA[
arXiv:2510.01612v2 Announce Type: replace 
Abstract: The exponential growth of biomedical literature creates significant challenges for accessing precise medical information. Current biomedical question-answering systems primarily focus on short-form answers, failing to provide the comprehensive explanations necessary for clinical decision-making. We present RAG-BioQA, a novel framework combining retrieval-augmented generation with domain-specific fine-tuning to produce evidence-based, long-form biomedical answers. Our approach integrates BioBERT embeddings with FAISS indexing and compares various re-ranking strategies (BM25, ColBERT, MonoT5) to optimize context selection before synthesizing evidence through a fine-tuned T5 model. Experimental results on the PubMedQA dataset show significant improvements over baselines, with our best model achieving substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state of accessible, evidence-based biomedical knowledge retrieval.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM one-shot style transfer for Authorship Attribution and Verification</title>
<link>https://arxiv.org/abs/2510.13302</link>
<guid>https://arxiv.org/abs/2510.13302</guid>
<content:encoded><![CDATA[
arXiv:2510.13302v2 Announce Type: replace 
Abstract: Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL with GRPO</title>
<link>https://arxiv.org/abs/2510.13827</link>
<guid>https://arxiv.org/abs/2510.13827</guid>
<content:encoded><![CDATA[
arXiv:2510.13827v2 Announce Type: replace 
Abstract: Current Text-to-SQL methods are evaluated and only focused on executable queries, overlooking the semantic alignment challenge -- both in terms of the semantic meaning of the query and the correctness of the execution results. Even execution accuracy itself shows significant drops when moving from English to other languages, with an average decline of 6 percentage points across non-English languages. We address these challenges by presenting a new framework that combines Group Relative Policy Optimization (GRPO) within a multilingual contrastive reward signal to enhance both task efficiency and semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method teaches models to obtain better correspondence between SQL generation and user intent by combining a reward signal based on semantic similarity. On the seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive reward signal in the GRPO framework further improved the average semantic accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our experiments showcase that a smaller, parameter-efficient 3B LLaMA model fine-tuned with our contrastive reward signal outperforms a much larger zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from 81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using just 3,000 reinforcement learning training examples. These results demonstrate how we can improve the performance of Text-to-SQL systems with contrastive rewards for directed semantic alignment, without requiring large-scale training datasets.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation</title>
<link>https://arxiv.org/abs/2510.16549</link>
<guid>https://arxiv.org/abs/2510.16549</guid>
<content:encoded><![CDATA[
arXiv:2510.16549v2 Announce Type: replace 
Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions and widespread adoption of large language models (LLMs) in scholarly evaluation present unprecedented challenges. While recent work has focused on using LLMs to improve review efficiency, unchecked deficient reviews from both human experts and AI systems threaten to systematically undermine academic integrity. To address this issue, we introduce ReviewGuard, an automated system for detecting and categorizing deficient reviews through a four-stage LLM-driven framework: data collection from ICLR and NeurIPS on OpenReview, GPT-4.1 annotation with human validation, synthetic data augmentation yielding 6,634 papers with 24,657 real and 46,438 synthetic reviews, and fine-tuning of encoder-based models and open-source LLMs. Feature analysis reveals that deficient reviews exhibit lower rating scores, higher self-reported confidence, reduced structural complexity, and more negative sentiment than sufficient reviews. AI-generated text detection shows dramatic increases in AI-authored reviews since ChatGPT's emergence. Mixed training with synthetic and real data substantially improves detection performance - for example, Qwen 3-8B achieves recall of 0.6653 and F1 of 0.7073, up from 0.5499 and 0.5606 respectively. This study presents the first LLM-driven system for detecting deficient peer reviews, providing evidence to inform AI governance in peer review. Code, prompts, and data are available at https://github.com/haoxuan-unt2024/ReviewGuard
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs</title>
<link>https://arxiv.org/abs/2511.01265</link>
<guid>https://arxiv.org/abs/2511.01265</guid>
<content:encoded><![CDATA[
arXiv:2511.01265v2 Announce Type: replace 
Abstract: This paper examines how domain specificity affects abstractive summarisation of Arabic financial texts using large language models (LLMs). We present AraFinNews, the largest publicly available Arabic financial news dataset to date, comprising 212,500 article-headline pairs spanning almost a decade of reporting from October 2015 to July 2025. Developed as an Arabic counterpart to major English summarisation corpora such as CNN/DailyMail, AraFinNews offers a strong benchmark for assessing domain-focused language understanding and generation in financial contexts. Using this resource, we evaluate transformer-based models, including mT5, AraT5 and the domain-adapted FinAraT5, to investigate how financial-domain pretraining influences accuracy, numerical reliability and stylistic alignment with professional reporting. The results show that domain-adapted models produce more coherent summaries, particularly when handling quantitative and entity-centred information. These findings underscore the value of domain-specific adaptation for improving narrative fluency in Arabic financial summarisation. The dataset is freely available for non-commercial research at https://github.com/ArabicNLP-UK/AraFinNews.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties</title>
<link>https://arxiv.org/abs/2511.03407</link>
<guid>https://arxiv.org/abs/2511.03407</guid>
<content:encoded><![CDATA[
arXiv:2511.03407v2 Announce Type: replace 
Abstract: Small language models (SLMs) have shown promises for relation extraction (RE) when extracting RDF triples guided by SHACL shapes focused on common datatype properties. This paper investigates how SLMs handle both datatype and object properties for a complete RDF graph extraction. We show that the key bottleneck is related to long-tail distribution of rare properties. To solve this issue, we evaluate several strategies: stratified sampling, weighted loss, dataset scaling, and template-based synthetic data augmentation. We show that the best strategy to perform equally well over unbalanced target properties is to build a training set where the number of occurrences of each property exceeds a given threshold. To enable reproducibility, we publicly released our datasets, experimental results and code. Our findings offer practical guidance for training shape-aware SLMs and highlight promising directions for future work in semantic RE.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A systematic review of relation extraction task since the emergence of Transformers</title>
<link>https://arxiv.org/abs/2511.03610</link>
<guid>https://arxiv.org/abs/2511.03610</guid>
<content:encoded><![CDATA[
arXiv:2511.03610v2 Announce Type: replace 
Abstract: This article presents a systematic review of relation extraction (RE) research since the advent of Transformer-based models. Using an automated framework to collect and annotate publications, we analyze 34 surveys, 64 datasets, and 104 models published between 2019 and 2024. The review highlights methodological advances, benchmark resources, and the integration of semantic web technologies. By consolidating results across multiple dimensions, the study identifies current trends, limitations, and open challenges, offering researchers and practitioners a comprehensive reference for understanding the evolution and future directions of RE.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods</title>
<link>https://arxiv.org/abs/2511.04079</link>
<guid>https://arxiv.org/abs/2511.04079</guid>
<content:encoded><![CDATA[
arXiv:2511.04079v2 Announce Type: replace 
Abstract: Objective: To enhance automated de-identification of radiology reports by scaling transformer-based models through extensive training datasets and benchmarking performance against commercial cloud vendor systems for protected health information (PHI) detection. Materials and Methods: In this retrospective study, we built upon a state-of-the-art, transformer-based, PHI de-identification pipeline by fine-tuning on two large annotated radiology corpora from Stanford University, encompassing chest X-ray, chest CT, abdomen/pelvis CT, and brain MR reports and introducing an additional PHI category (AGE) into the architecture. Model performance was evaluated on test sets from Stanford and the University of Pennsylvania (Penn) for token-level PHI detection. We further assessed (1) the stability of synthetic PHI generation using a "hide-in-plain-sight" method and (2) performance against commercial systems. Precision, recall, and F1 scores were computed across all PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining the previous state-of-the-art model performance. Synthetic PHI evaluation showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50 independently de-identified Penn datasets. Our model outperformed all vendor systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754). Discussion: Large-scale, multimodal training improved cross-institutional generalization and robustness. Synthetic PHI generation preserved data utility while ensuring privacy. Conclusion: A transformer-based de-identification model trained on diverse radiology datasets outperforms prior academic and commercial systems in PHI detection and establishes a new benchmark for secure clinical text processing.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2511.07318</link>
<guid>https://arxiv.org/abs/2511.07318</guid>
<content:encoded><![CDATA[
arXiv:2511.07318v2 Announce Type: replace 
Abstract: Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via Semantics Completion and Decomposition</title>
<link>https://arxiv.org/abs/2407.07026</link>
<guid>https://arxiv.org/abs/2407.07026</guid>
<content:encoded><![CDATA[
arXiv:2407.07026v2 Announce Type: replace-cross 
Abstract: With the proliferation of social media posts in recent years, the need to detect sentiments in multimodal (image-text) content has grown rapidly. Since posts are user-generated, the image and text from the same post can express different or even contradictory sentiments, leading to potential \textbf{sentiment discrepancy}. However, existing works mainly adopt a single-branch fusion structure that primarily captures the consistent sentiment between image and text. The ignorance or implicit modeling of discrepant sentiment results in compromised unimodal encoding and limited performance. In this paper, we propose a semantics Completion and Decomposition (CoDe) network to resolve the above issue. In the semantics completion module, we complement image and text representations with the semantics of the in-image text, helping bridge the sentiment gap. In the semantics decomposition module, we decompose image and text representations with exclusive projection and contrastive learning, thereby explicitly capturing the discrepant sentiment between modalities. Finally, we fuse image and text representations by cross-attention and combine them with the learned discrepant sentiment for final classification. Extensive experiments on four datasets demonstrate the superiority of CoDe and the effectiveness of each proposed module.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems</title>
<link>https://arxiv.org/abs/2503.01424</link>
<guid>https://arxiv.org/abs/2503.01424</guid>
<content:encoded><![CDATA[
arXiv:2503.01424v4 Announce Type: replace-cross 
Abstract: Research is a fundamental process driving the advancement of human civilization, yet it demands substantial time and effort from researchers. In recent years, the rapid development of artificial intelligence (AI) technologies has inspired researchers to explore how AI can accelerate and enhance research. To monitor relevant advancements, this paper presents a systematic review of the progress in this domain. Specifically, we organize the relevant studies into three main categories: hypothesis formulation, hypothesis validation, and manuscript publication. Hypothesis formulation involves knowledge synthesis and hypothesis generation. Hypothesis validation includes the verification of scientific claims, theorem proving, and experiment validation. Manuscript publication encompasses manuscript writing and the peer review process. Furthermore, we identify and discuss the current challenges faced in these areas, as well as potential future directions for research. Finally, we also offer a comprehensive overview of existing benchmarks and tools across various domains that support the integration of AI into the research process. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at https://github.com/zkzhou126/AI-for-Research.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergence of psychopathological computations in large language models</title>
<link>https://arxiv.org/abs/2504.08016</link>
<guid>https://arxiv.org/abs/2504.08016</guid>
<content:encoded><![CDATA[
arXiv:2504.08016v2 Announce Type: replace-cross 
Abstract: Can large language models (LLMs) instantiate computations of psychopathology? An effective approach to the question hinges on addressing two factors. First, for conceptual validity, we require a general and computational account of psychopathology that is applicable to computational entities without biological embodiment or subjective experience. Second, psychopathological computations, derived from the adapted theory, need to be empirically identified within the LLM's internal processing. Thus, we establish a computational-theoretical framework to provide an account of psychopathology applicable to LLMs. Based on the framework, we conduct experiments demonstrating two key claims: first, that the computational structure of psychopathology exists in LLMs; and second, that executing this computational structure results in psychopathological functions. We further observe that as LLM size increases, the computational structure of psychopathology becomes denser and that the functions become more effective. Taken together, the empirical results corroborate our hypothesis that network-theoretic computations of psychopathology have already emerged in LLMs. This suggests that certain LLM behaviors mirroring psychopathology may not be a superficial mimicry but a feature of their internal processing. Our work shows the promise of developing a new powerful in silico model of psychopathology and also alludes to the possibility of safety threat from the AI systems with psychopathological behaviors in the near future.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern</title>
<link>https://arxiv.org/abs/2509.24975</link>
<guid>https://arxiv.org/abs/2509.24975</guid>
<content:encoded><![CDATA[
arXiv:2509.24975v2 Announce Type: replace-cross 
Abstract: Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs) have emerged, offering promising parallel generation capabilities and showing strong potential for efficient UTG. Despite this advantage, their application to UTG is still constrained by a clear trade-off between efficiency and test quality, since increasing the number of tokens generated in each step often causes a sharp decline in the quality of test cases. To overcome this limitation, we present DiffTester, an acceleration framework specifically tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests targeting the same focal method often share repetitive structural patterns. By dynamically identifying these common patterns through abstract syntax tree analysis during generation, DiffTester adaptively increases the number of tokens produced at each step without compromising the quality of the output. To enable comprehensive evaluation, we extend the original TestEval benchmark, which was limited to Python, by introducing additional programming languages including Java and C++. Extensive experiments on three benchmarks with two representative models show that DiffTester delivers significant acceleration while preserving test coverage. Moreover, DiffTester generalizes well across different dLLMs and programming languages, providing a practical and scalable solution for efficient UTG in software development. Code and data are publicly available at https://github.com/wellbeingyang/DLM4UTG-open .
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning</title>
<link>https://arxiv.org/abs/2511.06190</link>
<guid>https://arxiv.org/abs/2511.06190</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, model routing, confidence scores, inference cost reduction, domain-agnostic framework  

<br /><br />Summary: Recent progress in Large Language Models (LLMs) has significantly boosted reasoning capabilities but at the cost of increased inference expenses. Traditional approaches to reducing these costs involve training router models or deferral mechanisms that assign simple queries to smaller models and complex queries to larger ones; however, these routers often lack robustness when facing domain shifts and need costly data synthesis through Monte Carlo rollouts for training. This paper introduces STEER (Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning), a novel domain-agnostic framework that accomplishes fine-grained, step-level routing between smaller and larger LLMs without relying on external trained models. STEER utilizes confidence scores derived internally from the smaller model’s logits before generating each reasoning step, invoking the larger model only when necessary. Evaluations across various LLMs and challenging benchmarks spanning mathematical reasoning, multi-hop question answering, and planning demonstrate that STEER not only maintains or improves accuracy but also significantly reduces computational costs, achieving up to 20% higher accuracy with 48% fewer FLOPs compared to always using larger models (e.g., on the AIME dataset). The study highlights that model-internal confidence is a robust and scalable signal for routing decisions, enabling efficient deployment of LLMs across domains. <div>
arXiv:2511.06190v2 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning</title>
<link>https://arxiv.org/abs/2511.15886</link>
<guid>https://arxiv.org/abs/2511.15886</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, multilingual LLMs, attribution methods, CoT prompting, model accuracy<br /><br />Summary:<br /><br />This study explores attribution patterns in Chain-of-Thought (CoT) reasoning within multilingual large language models (LLMs). First, the research applies two attribution techniques—ContextCite for step-level and Inseq for token-level attribution—to analyze the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Second, results reveal that attribution scores disproportionately concentrate on the final reasoning step, especially when the model generates incorrect answers, indicating potential issues in reasoning transparency. Third, structured CoT prompting is found to significantly improve model accuracy, but this effect is largely limited to high-resource languages that use the Latin script, highlighting multilingual robustness challenges. Fourth, experiments involving controlled perturbations, such as negation and distractor sentences, demonstrate decreases in both accuracy and attribution coherence, underscoring the sensitivity of CoT reasoning to input variations. Finally, the findings collectively point to limitations in CoT prompting approaches regarding their faithfulness and interpretability across languages, urging caution when deploying these techniques in multilingual contexts. <div>
arXiv:2511.15886v1 Announce Type: new 
Abstract: This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language</title>
<link>https://arxiv.org/abs/2511.15887</link>
<guid>https://arxiv.org/abs/2511.15887</guid>
<content:encoded><![CDATA[
<div> Keywords: Theory of Mind, nonverbal cues, Motion2Mind, AI interpretation, psychological states<br /><br />Summary:<br /><br />1. The study focuses on interpreting others' mental states through nonverbal cues (NVCs), essential for human survival and social bonding, which have been underexplored in existing Theory of Mind (ToM) benchmarks.  
2. Traditional ToM benchmarks mainly address false-belief tasks and asymmetric information reasoning, neglecting diverse mental states and the complexity of human nonverbal communication.  
3. The authors introduce Motion2Mind, a novel evaluation framework that tests AI systems’ ability to interpret a wide range of NVCs by leveraging an expert-curated body-language reference as a proxy knowledge base.  
4. Motion2Mind consists of a rigorously curated video dataset containing fine-grained annotations of 222 types of nonverbal cues alongside 397 manually verified psychological mind states, providing a rich resource for ToM assessment.  
5. The evaluation results indicate that current AI models perform poorly in detecting nonverbal cues and tend to over-interpret explanations compared to human annotators, highlighting significant challenges in AI’s understanding of human mental states through NVCs. <div>
arXiv:2511.15887v1 Announce Type: new 
Abstract: Our ability to interpret others' mental states through nonverbal cues (NVCs) is fundamental to our survival and social cohesion. While existing Theory of Mind (ToM) benchmarks have primarily focused on false-belief tasks and reasoning with asymmetric information, they overlook other mental states beyond belief and the rich tapestry of human nonverbal communication. We present Motion2Mind, a framework for evaluating the ToM capabilities of machines in interpreting NVCs. Leveraging an expert-curated body-language reference as a proxy knowledge base, we build Motion2Mind, a carefully curated video dataset with fine-grained nonverbal cue annotations paired with manually verified psychological interpretations. It encompasses 222 types of nonverbal cues and 397 mind states. Our evaluation reveals that current AI systems struggle significantly with NVC interpretation, exhibiting not only a substantial performance gap in Detection, as well as patterns of over-interpretation in Explanation compared to human annotators.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues</title>
<link>https://arxiv.org/abs/2511.15976</link>
<guid>https://arxiv.org/abs/2511.15976</guid>
<content:encoded><![CDATA[
<div> Keywords: task-oriented dialogue, instruction-following, large language models, benchmark, multi-turn conversations<br /><br />Summary:<br /><br />1. The paper addresses the challenge that real-world task-oriented dialogue (TOD) agents must strictly follow complex, multi-step natural language instructions with fine-grained constraints across multi-turn conversations. <br />2. Existing TOD benchmarks simplify instructions into basic schemas involving intents, slots, and API calls, which poorly represent the complexity of real instructions.<br />3. To fill this gap, the authors propose TOD-ProcBench, a novel benchmark designed to evaluate large language models’ (LLMs) ability to understand and follow complex, multi-level process instructions with conditional constraints.<br />4. The benchmark dataset is derived from the high-quality ABCD dataset, includes human quality-controlled conversations, and formulates instructions as multi-level condition-action statements.<br />5. Three tasks are introduced: (1) retrieving the most relevant instruction statement and predicting the next action, (2) detecting instruction-violating responses created by injecting inconsistencies, and (3) conditionally generating instruction-following responses.<br />6. The authors also study the effects of multilingual data and different instruction text formats on LLMs’ compliance performance.<br />7. The TOD-ProcBench benchmark is released under the Llama 3.3 Community License Agreement to facilitate future research on complex instruction following in TOD systems. <div>
arXiv:2511.15976v1 Announce Type: new 
Abstract: In real-world task-oriented dialogue (TOD) settings, agents are required to strictly adhere to complex instructions while conducting multi-turn conversations with customers. These instructions are typically presented in natural language format and include general guidelines and step-by-step procedures with complex constraints. Existing TOD benchmarks often oversimplify the complex nature of these instructions by reducing them to simple schemas composed of intents, slots, and API call configurations. To address this gap and systematically benchmark LLMs' instruction-following capabilities, we propose TOD-ProcBench, a challenging benchmark featuring complex process instructions with intricate, fine-grained constraints that evaluates various LLMs' abilities to understand and follow instructions in multi-turn TODs. Our benchmark dataset comprises instruction documents derived from the high-quality ABCD dataset with corresponding conversations under human quality control. We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements. We design three tasks to comprehensively benchmark LLMs' complex instruction-following capabilities in multi-turn TODs. Task 1 evaluates how LLMs retrieve the most relevant statement from a complex instruction and predict the corresponding next action. In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions, and then we analyze how effectively LLMs can identify instruction-violating responses. Task 3 investigates LLMs' abilities in conditional generation of instruction-following responses based on the original complex instructions. Additionally, we conduct studies on the impact of multilingual settings and different instruction text formats on compliance performance. We release our benchmark under the Llama 3.3 Community License Agreement.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liars' Bench: Evaluating Lie Detectors for Language Models</title>
<link>https://arxiv.org/abs/2511.16035</link>
<guid>https://arxiv.org/abs/2511.16035</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, lie detection, LIARS' BENCH, AI deception, evaluation benchmarks<br /><br />Summary:<br /><br />1. The paper addresses the challenge of detecting lies generated by large language models (LLMs), where a lie is defined as a statement that the model "believes" to be false.<br />2. Existing lie detection methods are typically validated in limited scenarios, which do not represent the wide variety of lies LLMs can produce in real use.<br />3. The authors introduce LIARS' BENCH, a large-scale testbed containing 72,863 examples of both lies and honest statements generated by four open-weight LLMs covering seven different datasets.<br />4. LIARS' BENCH categorizes lies along two key dimensions: the motivation behind the lie (the model's reason for lying) and the target of the belief being lied about.<br />5. Experimental evaluation of three lie detection methods—including both black-box and white-box approaches—reveals that these techniques commonly fail to detect certain types of lies, especially in cases where the transcript alone does not provide enough information to confirm a lie.<br />6. The results highlight significant limitations of prior lie detection techniques and emphasize the need for more robust methods.<br />7. LIARS' BENCH serves as a practical and comprehensive testbed to support future research and improvements in detecting lies generated by LLMs. <div>
arXiv:2511.16035v1 Announce Type: new 
Abstract: Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Tractable Distributions Of Language Model Continuations</title>
<link>https://arxiv.org/abs/2511.16054</link>
<guid>https://arxiv.org/abs/2511.16054</guid>
<content:encoded><![CDATA[
<div> Keywords: Controlled language generation, sequence-level constraints, Learning to Look Ahead (LTLA), hidden Markov models (HMMs), continuation probabilities<br /><br />Summary:<br /><br />This paper addresses the challenge of controlled language generation by enforcing sequence-level constraints such as syntax, style, or safety, which often depend on future tokens, making conditioning autoregressive language models intractable. Prior approaches rely on tractable surrogates like hidden Markov models (HMMs) to approximate continuation distributions and adjust next-token predictions, but these models are limited by weak context awareness. The authors propose Learning to Look Ahead (LTLA), a hybrid method that combines a base language model for rich prefix encoding with a fixed, tractable surrogate model (an HMM) to compute exact continuation probabilities. LTLA overcomes two main efficiency issues: the costly rescoring of prefixes for every next-token candidate and the recomputation of surrogate parameters per prefix, by utilizing a single batched HMM update to handle all token candidates simultaneously. Additionally, LTLA conditions only the surrogate's latent state prior on the language model’s hidden states, while keeping the surrogate decoder fixed, enabling reuse of computations across prefixes. Empirical results show LTLA achieves higher conditional likelihood than unconditional HMMs, effectively models continuation distributions even in vision-language contexts where standalone HMMs fail, and improves constraint satisfaction with comparable language fluency, incurring minimal inference overhead. <div>
arXiv:2511.16054v1 Announce Type: new 
Abstract: Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Early science acceleration experiments with GPT-5</title>
<link>https://arxiv.org/abs/2511.16072</link>
<guid>https://arxiv.org/abs/2511.16072</guid>
<content:encoded><![CDATA[
<div> GPT-5, AI collaboration, scientific research, mathematics breakthroughs, interdisciplinary studies<br /><br />Summary: This paper presents several case studies demonstrating how GPT-5, a state-of-the-art AI model, aids scientists across diverse fields including mathematics, physics, astronomy, computer science, biology, and materials science. It highlights GPT-5’s ability to generate new, concrete steps that accelerate ongoing research, saving expert time while illustrating areas where human input remains essential. The authors detail their interactions with GPT-5, showcasing effective collaborative workflows between humans and AI. Notably, the paper reports four new, carefully verified mathematical results achieved with GPT-5’s assistance, indicating the model’s potential to help resolve previously unsolved problems in mathematics. These contributions, although limited in scope, emphasize the significant implications of integrating advanced AI into scientific research. The study balances optimism about AI’s capabilities with a candid assessment of its current limitations, underscoring the importance of human oversight in leveraging AI effectively. Overall, this work advocates for the adoption of frontier AI tools like GPT-5 to complement and enhance human creativity and problem-solving in science. <div>
arXiv:2511.16072v1 Announce Type: new 
Abstract: AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models</title>
<link>https://arxiv.org/abs/2511.16122</link>
<guid>https://arxiv.org/abs/2511.16122</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Automatic Prompt Optimization, Ensemble Learning, Prompt Generation, Search Algorithms<br /><br />Summary:<br /><br />The article addresses the challenge of manual prompt engineering in Large Language Models (LLMs), which is time-consuming and limits practical applications. To overcome this, the field of Automatic Prompt Optimization (APO) has gained traction, focusing on efficient techniques to automatically generate and optimize prompts. While existing APO methods use evolutionary algorithms or trial-and-error search strategies, they typically rely on a single model or algorithm, which restricts their effectiveness on complex tasks. To improve upon this, the authors propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO). ELPO leverages the concept of ensemble learning by combining multiple generation strategies and diverse search methods, along with a voting mechanism, to identify superior prompts more accurately and robustly. The framework also introduces more efficient algorithms to enhance both prompt generation and search processes. Experimental evaluations show that ELPO significantly outperforms current state-of-the-art APO methods across various tasks, exemplified by a notable improvement of 7.6 in F1 score on the ArSarcasm dataset. This demonstrates ELPO’s potential for advancing prompt optimization and boosting LLM performance in complex scenarios. <div>
arXiv:2511.16122v1 Announce Type: new 
Abstract: The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating</title>
<link>https://arxiv.org/abs/2511.16147</link>
<guid>https://arxiv.org/abs/2511.16147</guid>
<content:encoded><![CDATA[
<div> Parameter-Efficient Fine-Tuning, Token-Selective PEFT, Large Models, Natural Language Processing, Fine-Tuning Optimization<br /><br />Summary:<br /><br />1. The paper addresses Parameter-Efficient Fine-Tuning (PEFT), a technique used in natural language processing (NLP) and computer vision (CV) that modifies only a small subset of model parameters while keeping pretrained weights unchanged.<br /><br />2. Traditional PEFT methods apply parameter modifications uniformly across all token positions in large models, but the authors question whether this indiscriminate approach is necessary or optimal.<br /><br />3. The authors propose a novel paradigm called Token-Selective PEFT (TS-PEFT), which selectively applies fine-tuning adjustments to specific token positions rather than all positions.<br /><br />4. Experimental results demonstrate that applying PEFT modifications to every token indiscriminately may be not only unnecessary but could also negatively impact performance on downstream tasks.<br /><br />5. This work highlights the importance of targeted fine-tuning and introduces a framework for refining PEFT approaches, paving the way for more efficient and effective fine-tuning strategies for large pretrained models in NLP and CV. <div>
arXiv:2511.16147v1 Announce Type: new 
Abstract: In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning</title>
<link>https://arxiv.org/abs/2511.16198</link>
<guid>https://arxiv.org/abs/2511.16198</guid>
<content:encoded><![CDATA[
<div> Keywords: citation verification, semantic errors, AI-generated references, lightweight language models, research integrity  

<br /><br />Summary:  
1. The article addresses challenges in scientific communication caused by inaccurate citations, including semantic citation errors, AI-generated hallucinated references, and traditional citation formats that fail to pinpoint specific supporting sections within papers.  
2. It introduces SemanticCite, an AI-powered system designed to verify citation accuracy by analyzing full-text sources and providing contextual information through detailed reasoning and relevant text snippets.  
3. SemanticCite incorporates multiple retrieval methods along with a four-class classification scheme (Supported, Partially Supported, Unsupported, Uncertain) to more precisely capture the relationship between claims and sources and to enable appropriate remedial actions depending on the error type.  
4. The system uses fine-tuned lightweight language models that achieve performance comparable to large commercial models but with significantly lower computational cost, making large-scale citation verification feasible.  
5. The authors contribute a comprehensive dataset with over 1,000 citations annotated with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines.  
6. The entire framework, including fine-tuned models and software, is released as open source, aiming to improve research integrity by enabling scalable citation verification, facilitating peer review, and improving quality control especially for AI-generated content.  
7. SemanticCite’s transparent, evidence-based explanations foster user trust and understanding, offering a crucial foundation for maintaining citation accuracy in academic literature at scale. <div>
arXiv:2511.16198v1 Announce Type: new 
Abstract: Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2511.16275</link>
<guid>https://arxiv.org/abs/2511.16275</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty quantification, large language models, semantic structural entropy, hallucination detection, semantic graph

<br /><br />Summary: This paper addresses the challenge of reliable uncertainty quantification (UQ) for large language models (LLMs), crucial for avoiding hallucinations in safety-critical applications. Existing UQ methods often rely on semantic probabilities or pairwise distances but neglect latent semantic structural information that could improve uncertainty estimates. The authors propose Semantic Structural Entropy (SeSE), a novel framework that quantifies semantic uncertainty from a structural information viewpoint. The approach involves constructing an adaptively sparsified directed semantic graph that captures directional semantic dependencies while pruning unnecessary edges to reduce negative interference. SeSE computes the structural entropy of the optimal semantic encoding tree within this graph, interpreting higher entropy as greater uncertainty and a higher probability of hallucinations. Furthermore, the method extends to long-form generation by modeling the uncertainty of individual claims through random semantic interactions, enabling fine-grained and theoretically grounded hallucination detection. Extensive experiments across 29 model-dataset pairs demonstrate that SeSE outperforms state-of-the-art UQ methods, including advanced supervised techniques and the recently introduced KLE method, establishing SeSE as a superior solution for uncertainty estimation and hallucination detection in LLMs. <div>
arXiv:2511.16275v1 Announce Type: new 
Abstract: Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.16324</link>
<guid>https://arxiv.org/abs/2511.16324</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, alignment, SDA, inference, open-source  

<br /><br />Summary: The paper addresses the challenge of aligning large language models (LLMs) with human intent during inference without expensive retraining. It introduces SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic framework that dynamically adjusts output probabilities based on user-defined alignment instructions. SDA is lightweight, resource-efficient, and compatible with various open-source LLMs, functioning independently or complementing training-based alignment methods. The framework supports personalized preference alignment, enabling flexible control over the model’s responses. Empirical evaluations show that SDA improves alignment across eight diverse open-source LLMs on three key dimensions: helpfulness, harmfulness, and honesty (3H). Specifically, SDA achieves substantial average improvements of 64.4% in helpfulness, 30% in honesty, and 11.5% in harmlessness, demonstrating its effectiveness and generalization. This makes SDA a practical solution for enhancing LLM behavior in real-world applications by ensuring responses better reflect human values and intentions without additional training overhead or supervision. <div>
arXiv:2511.16324v1 Announce Type: new 
Abstract: With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement</title>
<link>https://arxiv.org/abs/2511.16331</link>
<guid>https://arxiv.org/abs/2511.16331</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, large reasoning models, self-rewriting, internal reasoning quality, selective rewriting  

<br /><br />Summary:  
This paper addresses limitations in reinforcement learning (RL) approaches for large reasoning models (LRMs), which traditionally rely on one-sided rewards focused only on final outcome correctness. Such a focus fails to guide the internal reasoning process effectively, causing issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. To tackle this, the authors propose a novel self-rewriting framework where the model rewrites its own reasoning texts and learns from these rewrites to enhance the internal thought process quality. The core algorithm uses selective rewriting, applying it only to "simple" samples—those with consistently correct outcomes—so that the original reward signals remain intact. This approach is efficiently integrated into a single batch alongside vanilla generation, maintaining scalability and adding about 10% computational overhead. Experimental results across multiple tasks and model sizes demonstrate the effectiveness of self-rewriting, showing improved accuracy by +0.6 and a significant reduction in reasoning length by 46%, even without explicit instructions to shorten the reasoning. Furthermore, internal reasoning quality, as evaluated by an LLM-as-a-judge metric, improved significantly by +7.2 points, indicating successful reduction of reasoning flaws. Overall, self-rewriting offers a promising direction for improving the interpretability and efficiency of complex reasoning in LRMs. <div>
arXiv:2511.16331v1 Announce Type: new 
Abstract: Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NLP Datasets for Idiom and Figurative Language Tasks</title>
<link>https://arxiv.org/abs/2511.16345</link>
<guid>https://arxiv.org/abs/2511.16345</guid>
<content:encoded><![CDATA[
<div> Idiomatic language, figurative language, large language models, datasets, sequence tagging<br /><br />Summary:<br />1. The paper addresses the challenge large language models (LLMs) face in understanding idiomatic and figurative language, which remains difficult despite access to large corpora.  
2. Idiomatic and figurative expressions are prevalent in colloquial speech and social media, making them important yet elusive targets for language model training.  
3. The authors created one large-scale dataset containing potential idiomatic and figurative expressions alongside two additional human-annotated datasets of confirmed idiomatic and figurative usage to support model evaluation.  
4. These datasets were derived by combining existing idiom lists with context sequences retrieved from a large corpus and then post-processed to ensure compatibility with model-agnostic training methods.  
5. The paper evaluates the baseline performance of pre-trained LLMs on idiom recognition tasks using slot labeling and sequence tagging, highlighting the potential for improved learning through better and larger datasets. <div>
arXiv:2511.16345v1 Announce Type: new 
Abstract: Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies</title>
<link>https://arxiv.org/abs/2511.16353</link>
<guid>https://arxiv.org/abs/2511.16353</guid>
<content:encoded><![CDATA[
<div> rationales, sufficiency, token classification, attention regularisation, cross-domain classification<br /><br />Summary:  
The paper investigates human explanations in natural language, known as rationales, which are used to evaluate if models learn labels for correct reasons or rely on shortcuts. It critiques the sufficiency metric, commonly used to assess rationale informativeness, for its limited insight into rationale effects on model performance. The study connects sufficiency to two modeling paradigms: token classification to identify rationale tokens and attention regularisation to improve performance by integrating rationale information into inputs. The findings reveal that highly informative rationales do not necessarily aid correct instance classification. Instead, sufficiency tends to measure the influence of non-rationale context, which can interfere with rationale information within the same input. Incorporating rationale information sometimes enhances cross-domain classification, but this improvement varies depending on the task and model type. Additionally, the study finds no relationship between sufficiency and token classification performance. These results highlight the complexity of working with rationales and indicate that new metrics are needed to systematically capture their informational role in model behavior. <div>
arXiv:2511.16353v1 Announce Type: new 
Abstract: Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser</title>
<link>https://arxiv.org/abs/2511.16397</link>
<guid>https://arxiv.org/abs/2511.16397</guid>
<content:encoded><![CDATA[
<div> Keywords: HTML extraction, MinerU-HTML, web corpus, language models, content extraction<br /><br />Summary:<br /><br />1. The paper identifies that existing web corpus construction largely relies on heuristic-based HTML-to-text extraction methods such as Trafilatura, which often fail to preserve document structure accurately, leading to corruption of structured elements like formulas, code blocks, and tables. <br /><br />2. To address this, the authors introduce MinerU-HTML, a novel extraction pipeline that reframes content extraction as a sequence labeling task tackled by a 0.6-billion parameter language model. This approach leverages semantic understanding rather than relying on heuristic text density measures.<br /><br />3. MinerU-HTML employs a two-stage formatting pipeline to categorize semantic elements explicitly before converting the extracted content into Markdown, achieving superior preservation of structured elements (90.9% for code blocks, 94.0% for formulas) and achieving substantially better ROUGE-N F1 scores (81.8%) than Trafilatura (63.6%) on their benchmark dataset MainWebBench.<br /><br />4. Using MinerU-HTML, the authors create AICC (AI-ready Common Crawl), a massive multilingual web corpus with 7.3 trillion tokens constructed from Common Crawl snapshots.<br /><br />5. Controlled pretraining experiments show that language models trained on AICC outperform those trained on a Trafilatura-extracted corpus (TfCC) in benchmark accuracy by over 1 percentage point, underscoring the critical impact of extraction quality. Additionally, AICC also outperforms other datasets like RefinedWeb and FineWeb.<br /><br />6. The paper publicly releases the MainWebBench benchmark, MinerU-HTML extractor, and AICC corpus, highlighting the importance of advanced HTML extraction for improving data quality in web-based language model training. <div>
arXiv:2511.16397v1 Announce Type: new 
Abstract: While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\% ROUGE-N F1 compared to Trafilatura's 63.6\%, with exceptional structured element preservation (90.9\% for code blocks, 94.0\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification of worldwide news articles by perceived quality, 2018-2024</title>
<link>https://arxiv.org/abs/2511.16416</link>
<guid>https://arxiv.org/abs/2511.16416</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, deep learning, news quality, linguistic features, classification<br /><br />Summary:<br /><br />This study investigates the effectiveness of supervised machine learning and deep learning models in classifying perceived lower-quality versus higher-quality news articles. A large dataset of 1,412,272 English news articles collected from the Common Crawl between 2018 and 2024 was used. Expert consensus ratings on 579 news source websites were employed to label articles at the website level, splitting them into low and high-quality classes at the median, resulting in roughly equal groups of about 706,000 articles each. Each article was characterized using 194 linguistic features. Traditional machine learning classifiers such as Random Forest demonstrated competent performance with an accuracy of 0.7355 and a ROC AUC of 0.8131. Deep learning models outperformed traditional methods, with ModernBERT-large (256 context length) achieving the highest results: 0.8744 accuracy, 0.9593 ROC-AUC, and 0.8739 F1 score. DistilBERT-base (512 context length) closely followed, showing 0.8685 accuracy and 0.9554 ROC-AUC, while DistilBERT-base (256 context length) and ModernBERT-base (256 context length) achieved slightly lower but still strong metrics. The findings indicate that both traditional CPU-based machine learning and deep learning classifiers can effectively differentiate the perceived quality of global news articles. <div>
arXiv:2511.16416v1 Announce Type: new 
Abstract: This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports</title>
<link>https://arxiv.org/abs/2511.16438</link>
<guid>https://arxiv.org/abs/2511.16438</guid>
<content:encoded><![CDATA[
<div> ESGBench, ESG, explainability, sustainability reports, question answering<br /><br />Summary:<br /><br />1. ESGBench is a newly introduced benchmark dataset and evaluation framework specifically designed for explainable ESG (Environmental, Social, and Governance) question answering systems that analyze corporate sustainability reports. 2. The benchmark includes a comprehensive set of domain-grounded questions spanning multiple ESG themes, paired with human-curated answers and supporting evidence, which facilitates detailed and fine-grained evaluation of model reasoning capabilities. 3. The authors conduct an analysis of current state-of-the-art large language models (LLMs) on the ESGBench dataset, identifying significant challenges related to factual consistency, traceability of answers, and alignment with the ESG domain context. 4. These challenges highlight the need for improved model transparency and accountability when deploying AI systems in ESG-related applications. 5. Ultimately, ESGBench aims to accelerate research and development in building transparent, reliable, and accountable AI systems focused on ESG issues, enabling better interpretability and trustworthiness in sustainability-driven decision-making processes. <div>
arXiv:2511.16438v1 Announce Type: new 
Abstract: We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy of an Idiom: Tracing Non-Compositionality in Language Models</title>
<link>https://arxiv.org/abs/2511.16467</link>
<guid>https://arxiv.org/abs/2511.16467</guid>
<content:encoded><![CDATA[
<div> Idiomatic expressions, transformer models, circuit discovery, attention heads, computational efficiency<br /><br />Summary:<br /><br />This article investigates how transformer-based language models process idiomatic expressions using novel circuit discovery and analysis techniques. The authors employ a modified path patching algorithm to discover computational circuits involved specifically in idiom processing. They find that idiom processing activates distinct computational patterns within the transformers compared to other language tasks. A key finding is the identification of "Idiom Heads," which are attention heads that consistently activate across a wide variety of idiomatic expressions, indicating specialized processing roles. Additionally, the study observes an enhanced attention mechanism between idiom tokens caused by prior stages of processing, referred to as "augmented reception." These findings shed light on how transformers manage the trade-off between computational efficiency and robustness during idiomatic phrase comprehension. Importantly, understanding these mechanisms provides broader insights into how transformers handle non-compositional language constructs, which traditional models often struggle with. Finally, the research suggests that uncovering these internal circuits and patterns can pave the way for analyzing and improving the processing of more complex grammatical and linguistic structures within transformer architectures. <div>
arXiv:2511.16467v1 Announce Type: new 
Abstract: We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arctic-Extract Technical Report</title>
<link>https://arxiv.org/abs/2511.16470</link>
<guid>https://arxiv.org/abs/2511.16470</guid>
<content:encoded><![CDATA[
<div> Keywords: Arctic-Extract, document understanding, structural data extraction, resource-constrained deployment, long document processing<br /><br />Summary: Arctic-Extract is a cutting-edge model developed for extracting structural data, including question answering, entity recognition, and table extraction, from both scanned and digitally-born business documents. Despite its advanced capabilities, the model is optimized for deployment on resource-constrained hardware, requiring only 6.6 GiB of memory. This compact size allows it to run effectively on devices equipped with A10 GPUs that have 24 GB of memory. Arctic-Extract is capable of efficiently processing lengthy documents, handling up to 125 A4 pages on such GPU configurations. The paper discusses the training protocols employed to develop the model, emphasizing techniques that ensure its robust performance across various document understanding tasks. Evaluation results demonstrate that Arctic-Extract achieves strong accuracy and reliability in extracting complex structural information from business documents. Overall, Arctic-Extract represents a significant advancement in document AI, balancing state-of-the-art performance with practical deployment considerations for environments with limited computational resources. <div>
arXiv:2511.16470v1 Announce Type: new 
Abstract: Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval</title>
<link>https://arxiv.org/abs/2511.16528</link>
<guid>https://arxiv.org/abs/2511.16528</guid>
<content:encoded><![CDATA[
<div> Keywords: Turkish information retrieval, late-interaction models, ColBERT, dense encoders, MUVERA indexing<br /><br />Summary:<br /><br />This paper addresses the challenge of neural information retrieval (IR) for Turkish, a morphologically rich and lower-resource language. The authors introduce TurkColBERT, the first comprehensive benchmark comparing dense bi-encoders and late-interaction models specifically for Turkish IR tasks. They propose a two-stage adaptation pipeline that fine-tunes English and multilingual encoders on Turkish natural language inference (NLI) and semantic textual similarity (STS) tasks, followed by conversion to ColBERT-style retrievers using PyLate trained on the MS MARCO-TR dataset. Evaluation is conducted on 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results demonstrate strong parameter efficiency: a 1.0M-parameter colbert-hash-nano-tr model is 600 times smaller than a 600M-parameter turkish-e5-large dense encoder while maintaining more than 71% of its mean average precision (mAP). Late-interaction models that are 3–5 times smaller significantly outperform dense encoders, with ColmmBERT-base-TR achieving up to a 13.8% mAP improvement on domain-specific tasks. For deployment considerations, the paper compares indexing algorithms, finding that MUVERA+Rerank is 3.33 times faster than PLAID and improves relative mAP by 1.7%, enabling low-latency retrieval with query times as fast as 0.54 ms. The authors release all checkpoints, configurations, and evaluation scripts. Limitations include dependency on moderately sized datasets (≤50K documents) and reliance on translated benchmarks, suggesting a need for further large-scale MUVERA evaluations to better simulate real-world Turkish IR conditions. <div>
arXiv:2511.16528v1 Announce Type: new 
Abstract: Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5$\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\times$ faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks</title>
<link>https://arxiv.org/abs/2511.16540</link>
<guid>https://arxiv.org/abs/2511.16540</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, interpretability, genre prediction, Mistral-7B, shallow learning models<br /><br />Summary:<br /><br />1. The paper addresses the challenge of understanding Large Language Models (LLMs) to ensure their safe and beneficial deployment, highlighting difficulties in interpreting LLM structures and evaluating all outputs via human review.  
2. It introduces a novel predictive framework aimed at identifying the genre of a text prompt based solely on the internal activations of an LLM.  
3. Experiments are conducted using the Mistral-7B model and two different datasets to validate the approach.  
4. The study achieves high performance in genre classification, with F1-scores reaching up to 98% and 71% using scikit-learn classifiers, depending on the dataset.  
5. Results consistently exceed a control baseline across both datasets, providing a proof of concept that shallow learning models can successfully infer text genres from LLM activation patterns, advancing the interpretability of LLMs. <div>
arXiv:2511.16540v1 Announce Type: new 
Abstract: Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue</title>
<link>https://arxiv.org/abs/2511.16544</link>
<guid>https://arxiv.org/abs/2511.16544</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Speech Recognition, clinical dialogue, Word Error Rate, expert clinicians, error impact assessment

<br /><br />Summary: This paper addresses the limitations of using Word Error Rate (WER) as the primary metric for evaluating Automatic Speech Recognition (ASR) in clinical settings. The authors argue that WER does not adequately reflect the clinical impact of transcription errors. To investigate this, they establish a benchmark where expert clinicians assess discrepancies between ground-truth utterances and ASR-generated transcripts, categorizing the clinical significance of these errors into No, Minimal, or Significant Impact. Their analysis demonstrates that there is a poor correlation between WER and the clinicians’ risk labels. To address this gap, the authors propose a novel framework that utilizes a Large Language Model (LLM) as a judge to automate the clinical assessment process. This judge is optimized using the Geometric Evaluation of Prediction Accuracy (GEPA) and is termed Gemini-2.5-Pro. The optimized model achieves a 90% accuracy rate and a strong Cohen's kappa of 0.816, indicating human-comparable performance. This work presents a validated framework that shifts ASR evaluation from a focus on textual accuracy to an essential and scalable assessment of safety in clinical dialogues. <div>
arXiv:2511.16544v1 Announce Type: new 
Abstract: As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $\kappa$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2511.16577</link>
<guid>https://arxiv.org/abs/2511.16577</guid>
<content:encoded><![CDATA[
<div> Word sense disambiguation, language models, symbolic NLU, OpenCyc, large language models  

<br /><br />Summary:  
This paper addresses the challenge of word sense disambiguation (WSD) in natural language understanding (NLU), highlighting the limitations of current methods that rely on coarse-grained sense inventories like WordNet or FrameNet and require extensive hand-annotated training data. The authors propose a novel method that leverages large language models (LLMs) as oracles to perform WSD without any supervised data annotation. In their approach, multiple candidate meanings generated by a symbolic NLU system are transformed into natural language alternatives, which are then fed to an LLM. The LLM selects the most contextually appropriate sense, and this choice is integrated back into the symbolic system to improve its understanding. Unlike previous methods, this technique supports richer, fine-grained representations, such as those built on OpenCyc, enabling more sophisticated inference. The method is evaluated by comparing its disambiguation outputs against human-annotated gold standards, showing promising effectiveness. This work bridges statistical language modeling and symbolic reasoning for improved semantic interpretation, offering a scalable solution to WSD that avoids the bottleneck of manual labeling and expands the applicability of NLU systems to deeper knowledge representations. <div>
arXiv:2511.16577v1 Announce Type: new 
Abstract: Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems</title>
<link>https://arxiv.org/abs/2511.16654</link>
<guid>https://arxiv.org/abs/2511.16654</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, multimodal embedding, large language models, financial documents, information retrieval<br /><br />Summary:  
This paper addresses limitations in current multimodal Retrieval-Augmented Generation (RAG) systems, which generally convert images to text summaries before embedding, resulting in loss of crucial visual context. It compares two retrieval approaches: text-based chunk retrieval (image summarized as text) and direct multimodal embedding retrieval (storing native image embeddings). The evaluation uses six large language models and two multimodal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs with associated image and text document pairs. Experimental results show that direct multimodal embedding retrieval significantly outperforms the text-summary approach, with absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain (nDCG@5), corresponding to relative boosts of 32% and 20% respectively. Additionally, direct multimodal retrieval yields more accurate and factually consistent answers based on LLM-as-judge pairwise comparisons. The study highlights that LLM-based summarization causes information loss during preprocessing, whereas direct multimodal embeddings better preserve visual details critical for downstream retrieval and inference tasks in financial document analysis. <div>
arXiv:2511.16654v1 Announce Type: new 
Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs</title>
<link>https://arxiv.org/abs/2511.16664</link>
<guid>https://arxiv.org/abs/2511.16664</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, model compression, nested submodels, reasoning models, training cost<br /><br />Summary: This paper introduces Nemotron Elastic, a novel framework designed to efficiently build large language models (LLMs) focused on reasoning tasks, enabling multiple nested submodels within a single parent model. Each submodel is optimized for different deployment sizes and budgets, shares weights with the parent, and can be extracted zero-shot without additional training or fine-tuning, significantly reducing the overhead of training separate models. The framework incorporates a specially designed end-to-end trained router and a two-stage training curriculum tailored for reasoning models. Key technical contributions include group-aware SSM elastification compatible with Mamba architectures, heterogeneous MLP elastification, a normalized mean squared error (MSE) layer importance metric to improve depth selection, and knowledge distillation for simultaneous multi-budget optimization. Applying Nemotron Elastic to the Nemotron Nano V2 12B model yields 9B and 6B nested submodels using only 110 billion training tokens, achieving over a 360x reduction in training cost compared to training multiple models from scratch, and about 7x improvement over state-of-the-art compression methods. The nested models match or exceed state-of-the-art accuracy while enabling many-in-one deployment with constant memory usage regardless of the number of contained models, making it highly practical for scalable reasoning-oriented LLM deployment. <div>
arXiv:2511.16664v1 Announce Type: new 
Abstract: Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain of Summaries: Summarization Through Iterative Questioning</title>
<link>https://arxiv.org/abs/2511.15719</link>
<guid>https://arxiv.org/abs/2511.15719</guid>
<content:encoded><![CDATA[
<div> Large Language Models, external web content, Chain of Summaries, dialectical method, summarization performance  

<br /><br />Summary:  
Large Language Models (LLMs) increasingly leverage external web content, but much of this data is difficult for LLMs to process effectively due to unfriendly formats and context length limitations. To overcome these challenges, the authors propose the Chain of Summaries (CoS) method, which generates general-purpose, information-dense summaries serving as plain-text repositories of web content. CoS draws inspiration from Hegel's dialectical method by starting with an initial summary (thesis), then refining it through a questioning phase (antithesis) that identifies limitations, ultimately producing a comprehensive summary (synthesis) capable of meeting both current and anticipated information needs. Experimental evaluation on standard datasets such as TriviaQA, TruthfulQA, and SQUAD demonstrates that CoS surpasses zero-shot LLM baselines by up to 66% and specialized summarization models like BRIO and PEGASUS by up to 27%. The CoS-generated summaries not only improve question-answering performance compared to using the source content directly but also reduce token usage significantly. Furthermore, CoS is designed to be agnostic to downstream LLM architectures, making it a practical solution for website maintainers to enhance content accessibility for LLMs while preserving opportunities for human review and intervention. <div>
arXiv:2511.15719v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly using external web content. However, much of this content is not easily digestible by LLMs due to LLM-unfriendly formats and limitations of context length. To address this issue, we propose a method for generating general-purpose, information-dense summaries that act as plain-text repositories of web content. Inspired by Hegel's dialectical method, our approach, denoted as Chain of Summaries (CoS), iteratively refines an initial summary (thesis) by identifying its limitations through questioning (antithesis), leading to a general-purpose summary (synthesis) that can satisfy current and anticipate future information needs. Experiments on the TriviaQA, TruthfulQA, and SQUAD datasets demonstrate that CoS outperforms zero-shot LLM baselines by up to 66% and specialized summarization methods such as BRIO and PEGASUS by up to 27%. CoS-generated summaries yield higher Q&amp;A performance compared to the source content, while requiring substantially fewer tokens and being agnostic to the specific downstream LLM. CoS thus resembles an appealing option for website maintainers to make their content more accessible for LLMs, while retaining possibilities for human oversight.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-Audio-R1 Technical Report</title>
<link>https://arxiv.org/abs/2511.15848</link>
<guid>https://arxiv.org/abs/2511.15848</guid>
<content:encoded><![CDATA[
<div> Keywords: audio reasoning, Step-Audio-R1, Modality-Grounded Reasoning Distillation, multimodal reasoning, audio intelligence<br /><br />Summary:<br /><br />1. This work addresses a puzzling issue in audio language models where minimal or no reasoning leads to better performance, questioning whether deliberate thinking benefits audio intelligence.<br /><br />2. The authors introduce Step-Audio-R1, the first model designed specifically to enable reasoning capabilities in the audio domain.<br /><br />3. The paper presents a novel Modality-Grounded Reasoning Distillation (MGRD) framework, which helps Step-Audio-R1 to generate reasoning chains that are firmly grounded in acoustic features rather than producing irrelevant or hallucinated thoughts.<br /><br />4. Experimentally, Step-Audio-R1 demonstrates strong audio reasoning skills, outperforming Gemini 2.5 Pro and matching the state-of-the-art Gemini 3 Pro across various comprehensive benchmarks covering speech, environmental sounds, and music.<br /><br />5. The results illustrate that reasoning is a transferable skill across different sensory modalities if properly anchored, turning extended deliberation from a liability into a strength for audio intelligence and paving the way for truly multimodal reasoning systems that think deeply across all sensory inputs. <div>
arXiv:2511.15848v1 Announce Type: cross 
Abstract: Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Subtle Art of Defection: Understanding Uncooperative Behaviors in LLM based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.15862</link>
<guid>https://arxiv.org/abs/2511.15862</guid>
<content:encoded><![CDATA[
<div> Keywords: uncooperative behavior, multi-agent systems, game theory, simulation pipeline, system stability<br /><br />Summary:<br /><br />This paper presents a novel framework designed to simulate and analyze how uncooperative behaviors impact the stability of LLM-based multi-agent systems. The framework introduces a game theory-based taxonomy that categorizes different types of uncooperative agent behaviors, filling a notable gap in existing research. Additionally, it features a structured, multi-stage simulation pipeline that dynamically generates and evolves uncooperative behaviors in response to changes in agents' states. The framework is evaluated within a collaborative resource management context, using metrics such as survival time and resource overuse rate to assess system stability. Results demonstrate that the framework can generate realistic uncooperative behaviors with a high accuracy of 96.7%, which is further validated through human evaluation. Empirical findings reveal a stark difference between cooperative and uncooperative agents: cooperative agents sustain perfect system stability with 100% survival over 12 rounds and zero resource overuse, whereas any form of uncooperative behavior leads to rapid system collapse within 1 to 7 rounds. These outcomes underscore the significant negative impact uncooperative agents have on collective system performance and emphasize the importance of developing more robust and resilient multi-agent systems to withstand such destabilizing behaviors. <div>
arXiv:2511.15862v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for simulating and analyzing how uncooperative behaviors can destabilize or collapse LLM-based multi-agent systems. Our framework includes two key components: (1) a game theory-based taxonomy of uncooperative agent behaviors, addressing a notable gap in the existing literature; and (2) a structured, multi-stage simulation pipeline that dynamically generates and refines uncooperative behaviors as agents' states evolve. We evaluate the framework via a collaborative resource management setting, measuring system stability using metrics such as survival time and resource overuse rate. Empirically, our framework achieves 96.7% accuracy in generating realistic uncooperative behaviors, validated by human evaluations. Our results reveal a striking contrast: cooperative agents maintain perfect system stability (100% survival over 12 rounds with 0% resource overuse), while any uncooperative behavior can trigger rapid system collapse within 1 to 7 rounds. These findings demonstrate that uncooperative agents can significantly degrade collective outcomes, highlighting the need for designing more resilient multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization</title>
<link>https://arxiv.org/abs/2511.15915</link>
<guid>https://arxiv.org/abs/2511.15915</guid>
<content:encoded><![CDATA[
<div> Keywords: AccelOpt, kernel optimization, AI accelerators, Trainium, large language model agentic system<br /><br />Summary:<br /><br />1. AccelOpt is a self-improving large language model (LLM) agentic system designed to autonomously optimize kernels for emerging AI accelerators without requiring expert hardware-specific knowledge.<br /><br />2. The system explores the kernel optimization space through iterative generation, leveraging an optimization memory that gathers and reuses insights from previously encountered kernel performance pairs, thereby enabling continuous improvement.<br /><br />3. To evaluate AccelOpt’s performance, the authors introduce NKIBench, a new benchmark suite based on AWS Trainium accelerator kernels that vary in complexity and are derived from real-world LLM workloads.<br /><br />4. Experimental results demonstrate that AccelOpt significantly enhances kernel efficiency over time, increasing average peak throughput from 49% to 61% on Trainium 1 and from 45% to 59% on Trainium 2 for NKIBench kernels.<br /><br />5. In terms of cost-effectiveness, AccelOpt, using open-source models, matches the kernel optimization improvements of Claude Sonnet 4 while being 26 times cheaper, highlighting its practical value for scalable AI hardware optimization. <div>
arXiv:2511.15915v1 Announce Type: cross 
Abstract: We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\%$ to $61\%$ on Trainium 1 and from $45\%$ to $59\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\times$ cheaper.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2511.15958</link>
<guid>https://arxiv.org/abs/2511.15958</guid>
<content:encoded><![CDATA[
<div> Keywords: small language models, JudgeBoard, multi-agent judging, reasoning evaluation, Elo rating system<br /><br />Summary: This paper addresses the challenge of evaluating the correctness of reasoning answers produced by small language models (SLMs) compared to large language models (LLMs). Traditional evaluation methods rely on comparing candidate answers against ground-truth or other answers using indirect metrics such as entailment, which are hard to fully automate and scale. The authors propose JudgeBoard, a novel evaluation pipeline that directly queries models to judge answer correctness without needing answer comparisons. The evaluation focuses on mathematical reasoning as well as science and commonsense reasoning, deploying task-specific leaderboards with accuracy-based rankings and an Elo-based rating system across five benchmark datasets for consistent model comparisons. To enhance judgment performance in SLMs, the paper introduces MAJ (Multi-Agent Judging), a framework that uses multiple interacting SLMs with different reasoning styles to collaboratively approximate the judgment quality of LLMs. Experiments reveal a large performance gap when SLMs judge individually, but MAJ substantially boosts their judgment consistency and reliability. Remarkably, on the MATH dataset, MAJ with smaller models can match or exceed larger single models’ performance. This work suggests multi-agent SLM systems hold promise for scalable, efficient, and accurate evaluation of reasoning tasks, potentially rivaling LLM capabilities in judgment. <div>
arXiv:2511.15958v1 Announce Type: cross 
Abstract: While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARE-RAG - Clinical Assessment and Reasoning in RAG</title>
<link>https://arxiv.org/abs/2511.15994</link>
<guid>https://arxiv.org/abs/2511.15994</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning errors, retrieval-augmented generation, clinical guidelines, evaluation framework<br /><br />Summary:<br /><br />This article investigates the disconnect between retrieving correct evidence and reasoning accurately with it in large language models (LLMs), focusing on clinical contexts where adherence to structured protocols is critical. Using Written Exposure Therapy (WET) guidelines as a test case, the study evaluates LLM responses to questions verified by clinicians to examine error persistence even when authoritative information is available. The authors identify that despite access to correct passages, models continue to make reasoning mistakes, highlighting a significant gap that poses risks in clinical application. To address this, they propose a comprehensive evaluation framework that assesses not only accuracy but also the consistency and fidelity of the model’s reasoning processes. Their results demonstrate that retrieval-augmented generation (RAG) techniques can help constrain model outputs closer to authoritative evidence. However, the study emphasizes that for safe deployment, especially in sensitive fields like healthcare, rigorous evaluation of LLM reasoning quality is just as essential as ensuring effective retrieval. This work thereby underscores both the promise of augmented LLMs and the caution required to trust their outputs in real-world clinical scenarios. <div>
arXiv:2511.15994v1 Announce Type: cross 
Abstract: Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QueryGym: A Toolkit for Reproducible LLM-Based Query Reformulation</title>
<link>https://arxiv.org/abs/2511.15996</link>
<guid>https://arxiv.org/abs/2511.15996</guid>
<content:encoded><![CDATA[
<div> QueryGym, query reformulation, large language models, retrieval benchmarking, open-source toolkit  

<br /><br />Summary:  
The paper introduces QueryGym, a lightweight and extensible Python toolkit designed to support query reformulation using large language models (LLMs). This toolkit addresses a critical need in the research community by providing a unified platform that consolidates various LLM-based query reformulation methods, facilitating consistent implementation and fair comparison. QueryGym enables rapid experimentation and reliable deployment by offering a standardized framework. Key features of QueryGym include a Python API that allows users to apply diverse LLM-based reformulation techniques with ease. It provides a retrieval-agnostic interface, making it compatible with multiple retrieval backends like Pyserini and PyTerrier. The toolkit also includes a centralized prompt management system, which incorporates versioning and metadata tracking to maintain prompt integrity and reproducibility. Furthermore, QueryGym supports well-known benchmarking datasets such as BEIR and MS MARCO, enabling effective evaluation across different retrieval scenarios. Emphasizing openness and community collaboration, QueryGym is fully open-source and extensible, inviting contributions from researchers worldwide. The toolkit is publicly accessible on GitHub, ensuring broad utility and encouraging further development in the domain of LLM-based query reformulation. <div>
arXiv:2511.15996v1 Announce Type: cross 
Abstract: We present QueryGym, a lightweight, extensible Python toolkit that supports large language model (LLM)-based query reformulation. This is an important tool development since recent work on llm-based query reformulation has shown notable increase in retrieval effectiveness. However, while different authors have sporadically shared the implementation of their methods, there is no unified toolkit that provides a consistent implementation of such methods, which hinders fair comparison, rapid experimentation, consistent benchmarking and reliable deployment. QueryGym addresses this gap by providing a unified framework for implementing, executing, and comparing llm-based reformulation methods. The toolkit offers: (1) a Python API for applying diverse LLM-based methods, (2) a retrieval-agnostic interface supporting integration with backends such as Pyserini and PyTerrier, (3) a centralized prompt management system with versioning and metadata tracking, (4) built-in support for benchmarks like BEIR and MS MARCO, and (5) a completely open-source extensible implementation available to all researchers. QueryGym is publicly available at https://github.com/radinhamidi/QueryGym.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpellForger: Prompting Custom Spell Properties In-Game using BERT supervised-trained model</title>
<link>https://arxiv.org/abs/2511.16018</link>
<guid>https://arxiv.org/abs/2511.16018</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, game design, natural language processing, spell creation, Unity  

<br /><br />Summary:  
This paper introduces SpellForger, a novel game that integrates Artificial Intelligence to enable players to create custom spells through natural language prompts, providing a personalized and creative gameplay experience. The objective centers on utilizing AI as a core gameplay co-creation tool, an area that is currently underexplored in the gaming industry. The methodology involves employing a supervised-trained BERT model that interprets player-written descriptions by mapping them onto predefined spell prefabs. This model also fine-tunes spell parameters such as damage, cost, and effects to maintain balanced and competitive gameplay. The development environment combines the Unity Game Engine for the game framework with a Python-based AI backend to handle prompt processing and spell generation. The expected outcome is a working prototype that demonstrates real-time spell creation integrated seamlessly into an engaging gameplay loop, validating AI's potential as a direct mechanic in game design that enhances player creativity and interaction. <div>
arXiv:2511.16018v1 Announce Type: cross 
Abstract: Introduction: The application of Artificial Intelligence in games has evolved significantly, allowing for dynamic content generation. However, its use as a core gameplay co-creation tool remains underexplored. Objective: This paper proposes SpellForger, a game where players create custom spells by writing natural language prompts, aiming to provide a unique experience of personalization and creativity. Methodology: The system uses a supervisedtrained BERT model to interpret player prompts. This model maps textual descriptions to one of many spell prefabs and balances their parameters (damage, cost, effects) to ensure competitive integrity. The game is developed in the Unity Game Engine, and the AI backend is in Python. Expected Results: We expect to deliver a functional prototype that demonstrates the generation of spells in real time, applied to an engaging gameplay loop, where player creativity is central to the experience, validating the use of AI as a direct gameplay mechanic.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization</title>
<link>https://arxiv.org/abs/2511.16209</link>
<guid>https://arxiv.org/abs/2511.16209</guid>
<content:encoded><![CDATA[
<div> System prompts, prompt hardening, extraction attacks, shield appending, black-box optimization  

<br /><br />Summary:  
This paper addresses the security risks posed by extraction attacks on system prompts used to guide Large Language Models (LLMs), which often contain proprietary or sensitive information. The authors introduce a new defense framework called shield appending, a lightweight protective textual layer added to system prompts to reduce leakage. They formalize prompt hardening as a utility-constrained optimization problem aiming to find effective shields that minimize leakage while maintaining task utility, measured by semantic fidelity to baseline model outputs. The optimization process employs an LLM-as-optimizer approach, which operates in a black-box setting requiring only API access. Empirical evaluations demonstrate that the optimized shields significantly reduce prompt leakage across a broad range of adversarial extraction attacks compared to existing defense methods. Crucially, this defense does not compromise the intended functionality of the LLM. The proposed method is efficient, practical, and broadly applicable to black-box LLMs. The study contributes a novel paradigm for developing robust, utility-aware defenses to enhance LLM prompt security in an increasingly threat-prone environment. The code for implementing the shield appending framework is publicly available, facilitating further research and deployment in real-world scenarios. <div>
arXiv:2511.16209v1 Announce Type: cross 
Abstract: System prompts are critical for guiding the behavior of Large Language Models (LLMs), yet they often contain proprietary logic or sensitive information, making them a prime target for extraction attacks. Adversarial queries can successfully elicit these hidden instructions, posing significant security and privacy risks. Existing defense mechanisms frequently rely on heuristics, incur substantial computational overhead, or are inapplicable to models accessed via black-box APIs. This paper introduces a novel framework for hardening system prompts through shield appending, a lightweight approach that adds a protective textual layer to the original prompt. Our core contribution is the formalization of prompt hardening as a utility-constrained optimization problem. We leverage an LLM-as-optimizer to search the space of possible SHIELDs, seeking to minimize a leakage metric derived from a suite of adversarial attacks, while simultaneously preserving task utility above a specified threshold, measured by semantic fidelity to baseline outputs. This black-box, optimization-driven methodology is lightweight and practical, requiring only API access to the target and optimizer LLMs. We demonstrate empirically that our optimized SHIELDs significantly reduce prompt leakage against a comprehensive set of extraction attacks, outperforming established baseline defenses without compromising the model's intended functionality. Our work presents a paradigm for developing robust, utility-aware defenses in the escalating landscape of LLM security. The code is made public on the following link: https://github.com/psm-defense/psm
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions</title>
<link>https://arxiv.org/abs/2511.16221</link>
<guid>https://arxiv.org/abs/2511.16221</guid>
<content:encoded><![CDATA[
<div> Deception Detection, Multimodal Large Language Models, Social Reasoning, Dataset Benchmark, Chain-of-Thought Reasoning

<br /><br />Summary:
This paper addresses a critical shortcoming in current Multimodal Large Language Models (MLLMs), namely their inability to accurately detect deception in complex social interactions. To systematically measure this deficiency, the authors introduce the Multimodal Interactive Deception Assessment (MIDA) task along with a novel dataset that provides synchronized video and text data with verified ground-truth labels for each statement, enabling rigorous benchmarking. Evaluating 12 state-of-the-art MLLMs, including open- and closed-source models like GPT-4o, the study finds a significant performance gap, as even the most advanced models fail to reliably distinguish truth from falsehood. The analysis reveals that these models struggle to ground language understanding in multimodal social cues and lack the capability to model others' beliefs, knowledge, and intentions—fundamental aspects of human social reasoning. To overcome these limitations, the authors propose a new framework combining a Social Chain-of-Thought (SoCoT) reasoning pipeline with a Dynamic Social Epistemic Memory (DSEM) module. This approach enhances model performance on deception detection tasks, marking a promising step towards developing MLLMs with genuine human-like social epistemic reasoning and trustworthiness in complex interactive scenarios. <div>
arXiv:2511.16221v1 Announce Type: cross 
Abstract: Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe</title>
<link>https://arxiv.org/abs/2511.16334</link>
<guid>https://arxiv.org/abs/2511.16334</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal reasoning, supervised fine-tuning, reinforcement learning, dataset curation, OpenMMReasoner<br /><br />Summary: This paper presents OpenMMReasoner, a transparent two-stage training approach designed to enhance multimodal reasoning capabilities in large reasoning models. First, the supervised fine-tuning (SFT) stage involves creating a sizeable 874K-sample cold-start dataset with meticulously validated step-by-step reasoning data, establishing a strong foundation for model training. Next, a reinforcement learning (RL) stage utilizes a 74K-sample dataset from diverse domains to refine and stabilize the model’s reasoning abilities, promoting robustness and efficient learning. The authors emphasize the importance of transparent and reproducible data curation and training methodologies, addressing a significant obstacle in scalable multimodal research. Extensive experimental evaluations demonstrate that the proposed training recipe outperforms strong baseline models, including an 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks. This work thereby highlights the critical role of high-quality data and thoughtful training design in advancing multimodal reasoning performance. To facilitate future research, all codes, pipelines, and datasets from this work are fully open-sourced at the provided GitHub repository, establishing a solid empirical foundation for continuing developments in large-scale multimodal reasoning models. <div>
arXiv:2511.16334v1 Announce Type: cross 
Abstract: Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.16423</link>
<guid>https://arxiv.org/abs/2511.16423</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, federated learning, one-shot adaptation, multimodal features, data heterogeneity<br /><br />Summary: This paper addresses the challenge of efficiently adapting pre-trained Vision-Language Models (VLMs) to downstream tasks within federated learning frameworks while minimizing communication overhead and vulnerability to attacks. Existing iterative training methods suffer from expensive client-server exchanges and require significant computational resources. Motivated by one-shot federated training techniques, the authors propose TOFA, a novel training-free one-shot federated adaptation method specifically designed for VLMs. TOFA leverages both visual and textual pipelines to fully exploit the rich multimodal information in VLMs. The visual pipeline uses a hierarchical Bayesian model that learns personalized, class-specific prototype distributions to capture local data characteristics. The textual pipeline generates and globally aligns local text prompts to enhance robustness across heterogeneous client data. Additionally, TOFA incorporates an adaptive weight calibration mechanism to balance and combine predictions from the visual and textual modalities, effectively addressing severe data heterogeneity. Importantly, TOFA does not require any additional training resources on client or server sides, making it lightweight and practical. Extensive experimental evaluations on nine datasets under various federated settings demonstrate that TOFA significantly improves adaptation efficacy while reducing communication costs and computational demands, validating its potential for real-world federated VLM deployment. <div>
arXiv:2511.16423v1 Announce Type: cross 
Abstract: Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation</title>
<link>https://arxiv.org/abs/2511.16478</link>
<guid>https://arxiv.org/abs/2511.16478</guid>
<content:encoded><![CDATA[
<div> Music Recommender Systems, Large Language Models, Evaluation, Natural Language, User Modeling<br /><br />Summary:<br /><br />This article explores how the advent of Large Language Models (LLMs) transforms Music Recommender Systems (MRS) by shifting the paradigm from traditional retrieval-based approaches to generative models. First, it highlights that conventional MRS evaluations, primarily focused on accuracy in retrieval subtasks, are insufficient for capturing the qualities of good recommendations, especially with LLMs introducing generative outputs and challenges like hallucinations and non-determinism. Second, the paper reviews how LLMs impact user and item modeling in music, enabling natural language interactions and new ways to generate recommendations. Third, it surveys evaluation methodologies from Natural Language Processing (NLP), discussing their relevance and the challenges they present when applied to MRS. Finally, the work synthesizes these insights into a structured framework that identifies both the success factors and risks associated with using LLM prompting in music recommendation contexts. The goal is to equip the MRS community with a fresh, interdisciplinary, and educational perspective on evaluation that better aligns with the capabilities and limitations of LLM-driven recommendation systems. <div>
arXiv:2511.16478v1 Announce Type: cross 
Abstract: Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.
  This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiMo-Embodied: X-Embodied Foundation Model Technical Report</title>
<link>https://arxiv.org/abs/2511.16518</link>
<guid>https://arxiv.org/abs/2511.16518</guid>
<content:encoded><![CDATA[
<div> MiMo-Embodied, Autonomous Driving, Embodied AI, Multi-stage Learning, Cross-domain Transfer  

<br /><br />Summary:  
This paper introduces MiMo-Embodied, the first cross-embodied foundation model that integrates Autonomous Driving and Embodied AI into a single framework, achieving state-of-the-art performance in both domains. It demonstrates superior results across 17 embodied AI benchmarks involving Task Planning, Affordance Prediction, and Spatial Understanding, as well as 12 autonomous driving benchmarks focused on Environmental Perception, Status Prediction, and Driving Planning. The model significantly outperforms existing open-source, closed-source, and specialized baselines, highlighting its robustness and versatility. The authors attribute this success to multi-stage learning strategies, carefully curated data construction, and fine-tuning techniques based on Chain-of-Thought (CoT) reasoning and Reinforcement Learning (RL). Notably, the study reveals strong positive transfer effects and mutual reinforcement between the two domains, suggesting that cross-domain learning is beneficial. A comprehensive analysis of the model design and training methodologies is provided to facilitate future research and development. The authors have open-sourced both the code and trained models, making them accessible at https://github.com/XiaomiMiMo/MiMo-Embodied to encourage further exploration and innovation in this emerging field. <div>
arXiv:2511.16518v1 Announce Type: cross 
Abstract: We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation</title>
<link>https://arxiv.org/abs/2511.16543</link>
<guid>https://arxiv.org/abs/2511.16543</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, explainable recommendation, knowledge distillation, decoupled framework, personalized explanations<br /><br />Summary: The paper addresses the performance-efficiency trade-off encountered when integrating Large Language Models (LLMs) into explainable recommendation systems via end-to-end architectures, which jointly optimize ranking and explanation but often yield suboptimal results. To overcome this, the authors propose Prism, a novel framework that decouples the recommendation process into two distinct stages: a ranking stage and an explanation generation stage. Prism employs knowledge distillation by using a large, powerful teacher LLM (such as FLAN-T5-XXL) as an Oracle to generate high-fidelity explanatory knowledge. A smaller, fine-tuned student model (e.g., BART-Base), named Prism, then focuses solely on synthesizing this knowledge into personalized explanations. This separation ensures each component is optimized for its specific task, avoiding conflicts inherent in coupled models. Extensive experiments on benchmark datasets demonstrate that despite being much smaller (140M parameters vs. 11B), Prism outperforms its teacher in human evaluations of faithfulness and personalization. Additionally, Prism achieves a 24x speedup and a 10x reduction in memory usage during inference. The results validate that decoupling the processes and applying targeted distillation offers an efficient and effective approach to producing high-quality explainable recommendation systems. <div>
arXiv:2511.16543v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies</title>
<link>https://arxiv.org/abs/2511.16590</link>
<guid>https://arxiv.org/abs/2511.16590</guid>
<content:encoded><![CDATA[
<div> Keywords: GUI agents, Android, anomalies, benchmarking, robustness<br /><br />Summary:<br /><br />Developing intelligent agents that can operate various Graphical User Interfaces (GUIs) with human-level skill is crucial for advancing toward Artificial General Intelligence. Current datasets and benchmarks for GUI agents are mostly static and idealized, lacking representation of the complexity and unpredictability seen in real-world environments, especially when anomalies occur. To address this gap, the paper introduces D-GARA, a dynamic benchmarking framework designed to evaluate the robustness of Android GUI agents against real-world anomalies. D-GARA incorporates a diverse range of common anomalies such as permission dialogs, battery warnings, and update prompts, which frequently interrupt user interactions. Utilizing the D-GARA framework, the authors created and annotated a benchmark comprising popular Android applications populated with these anomalies, enabling more realistic evaluation scenarios for the research community. Experimental results show that state-of-the-art GUI agents experience significant performance drops when facing anomaly-rich environments, emphasizing the necessity for robustness-aware training methods. Moreover, D-GARA is built to be modular and extensible, allowing easy integration of new tasks, anomaly categories, and interaction contexts to support diverse and evolving evaluation needs. This research underlines the importance of robustness factors in the development of intelligent GUI agents and provides practical tools to advance this goal. <div>
arXiv:2511.16590v1 Announce Type: cross 
Abstract: Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</title>
<link>https://arxiv.org/abs/2511.16595</link>
<guid>https://arxiv.org/abs/2511.16595</guid>
<content:encoded><![CDATA[
<div> TimeViper, hybrid model, long video understanding, Mamba-Transformer, token compression<br /><br />Summary:  
The paper introduces TimeViper, a hybrid vision-language model designed to address the challenges of long video understanding. It specifically targets the need for efficient model architectures and mechanisms capable of handling extended temporal contexts present in hour-long videos. TimeViper employs a hybrid Mamba-Transformer backbone, combining the efficiency of state-space models (Mamba) with the expressivity of attention-based Transformers. The authors discover a vision-to-text information aggregation phenomenon where, as the model depth in the large language model (LLM) increases, information flows progressively from vision tokens to text tokens, causing severe redundancy in vision tokens. To resolve this, they propose TransV, a token information transfer module that compresses and transfers vision tokens into instruction tokens without losing multimodal understanding capacity. This compression enables TimeViper to process videos longer than 10,000 frames efficiently. Extensive experiments across various benchmarks show that TimeViper performs competitively with state-of-the-art models while supporting significantly longer videos. Additionally, the authors conduct an analysis of attention patterns in both Mamba and Transformer layers, providing new insights into the interpretability of such hybrid architectures. This work marks an initial step toward developing, interpreting, and compressing hybrid Mamba-Transformer models for long video understanding. <div>
arXiv:2511.16595v1 Announce Type: cross 
Abstract: We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction</title>
<link>https://arxiv.org/abs/2511.16635</link>
<guid>https://arxiv.org/abs/2511.16635</guid>
<content:encoded><![CDATA[
<div> Keywords: Survival Analysis, Multimodal Data, Chain-of-Thought, Explainable AI, Precision Oncology<br /><br />Summary:<br /><br />1. Survival analysis plays a crucial role in cancer prognosis and treatment planning but currently lacks transparency necessary for clinical use. 2. Existing pathology agents, although explainable in diagnosis, struggle with integrating multimodal data, exploring relevant regions of interest effectively, and learning experientially from past cases for survival prediction. 3. SurvAgent is proposed as the first hierarchical chain-of-thought (CoT) enhanced multi-agent system tailored for multimodal survival prediction, combining pathology images and genetic data. 4. The system's first stage constructs a case bank using WSI-Gene CoT methodology, involving hierarchical low-magnification screening, similarity-aware and confidence-aware patch mining for whole slide images, and gene-stratified analysis across six functional categories, all generating structured reports with detailed reasoning to enable experiential learning. 5. The second stage performs dichotomy-based multi-expert agent inference retrieving similar cases with retrieval-augmented generation (RAG), integrating multimodal reports and expert models with progressive refinement of survival intervals. 6. Extensive evaluation on five TCGA cancer cohorts shows that SurvAgent outperforms traditional methods, proprietary medical large language models, and other AI agents, establishing a novel, interpretable AI paradigm for precision oncology survival prediction. <div>
arXiv:2511.16635v1 Announce Type: cross 
Abstract: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech Codecs</title>
<link>https://arxiv.org/abs/2511.16639</link>
<guid>https://arxiv.org/abs/2511.16639</guid>
<content:encoded><![CDATA[
<div> neural audio codecs, discrete audio codec units, speech representation learning, masked prediction, SUPERB benchmark<br /><br />Summary: Recent advancements in neural audio codecs have enhanced both audio compression and speech synthesis techniques, opening avenues for their use as universal acoustic feature extractors in various speech processing tasks. The paper introduces Codec2Vec, a novel speech representation learning framework that uniquely utilizes discrete audio codec units rather than continuous inputs. This discrete-based approach provides multiple benefits, including significantly improved data storage and transmission efficiency, faster training speeds, and enhanced data privacy due to the nature of the discrete units. The authors investigate the framework's effectiveness through masked prediction tasks, experimenting with different strategies for deriving training targets to optimize performance. Codec2Vec is rigorously evaluated on the widely recognized SUPERB benchmark to compare its performance with traditional continuous-input models. Results demonstrate that Codec2Vec achieves competitive accuracy and effectiveness while substantially reducing storage requirements by up to 16.5 times. Additionally, the training time is decreased by a factor of 2.3, emphasizing the framework’s scalability and efficiency. Overall, Codec2Vec represents a promising direction for speech representation learning, leveraging discrete audio codec units to balance performance with practical benefits in resource usage and privacy. <div>
arXiv:2511.16639v1 Announce Type: cross 
Abstract: Recent advancements in neural audio codecs have not only enabled superior audio compression but also enhanced speech synthesis techniques. Researchers are now exploring their potential as universal acoustic feature extractors for a broader range of speech processing tasks. Building on this trend, we introduce Codec2Vec, the first speech representation learning framework that relies exclusively on discrete audio codec units. This approach offers several advantages, including improved data storage and transmission efficiency, faster training, and enhanced data privacy. We explore masked prediction with various training target derivation strategies to thoroughly understand the effectiveness of this framework. Evaluated on the SUPERB benchmark, Codec2Vec achieves competitive performance compared to continuous-input models while reducing storage requirements by up to 16.5x and training time by 2.3x, showcasing its scalability and efficiency.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</title>
<link>https://arxiv.org/abs/2511.16671</link>
<guid>https://arxiv.org/abs/2511.16671</guid>
<content:encoded><![CDATA[
<div> Keywords: Thinking-while-Generating, textual reasoning, visual generation, multimodal interaction, reinforcement learning<br /><br />Summary:<br /><br />This paper presents Thinking-while-Generating (TwiG), a novel framework that integrates textual reasoning directly into the process of visual content generation. Unlike previous methods that apply reasoning only before or after generation, TwiG interleaves reasoning and generation dynamically, allowing the system to guide upcoming image regions and reflect on previously generated content. This co-evolving mechanism leads to more context-aware and semantically rich visual outputs. The study explores three approaches to implement TwiG: zero-shot prompting, supervised fine-tuning on a newly curated TwiG-50K dataset, and reinforcement learning with a custom TwiG-GRPO method. Each approach sheds light on different aspects of how interleaved textual reasoning influences visual generation quality. The authors highlight the potential benefits of on-the-fly multimodal interaction during image synthesis and encourage future research in this direction. The accompanying codebase will be made publicly available to facilitate further development and experimentation. <div>
arXiv:2511.16671v1 Announce Type: cross 
Abstract: Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPTopic: Dynamic and Interactive Topic Representations</title>
<link>https://arxiv.org/abs/2403.03628</link>
<guid>https://arxiv.org/abs/2403.03628</guid>
<content:encoded><![CDATA[
<div> Keywords: Topic Modeling, Large Language Models, Interactive Visualization, Topic Interpretation, GPTopic<br /><br />Summary:<br /><br />1. Traditional topic modeling often relies on generating lists of top words to represent topics in large text corpora, but interpreting these lists can be challenging for users without domain expertise.  
2. A representation limited to top words may not capture the full complexity, facets, and nuances of topics, potentially reducing their comprehensibility and usefulness.  
3. To overcome these limitations, the authors introduce GPTopic, a software package that uses Large Language Models (LLMs) to produce dynamic and interactive topic representations.  
4. GPTopic features an intuitive chat-based interface that allows users to explore, analyze, and refine topics interactively, thereby enhancing accessibility for a broader user base.  
5. The tool aims to make topic modeling more comprehensive and user-friendly, helping both experts and novices better understand the underlying themes in text corpora. The code for GPTopic is openly available on GitHub for public use and further development. <div>
arXiv:2403.03628v3 Announce Type: replace 
Abstract: Topic modeling seems to be almost synonymous with generating lists of top words to represent topics within large text corpora. However, deducing a topic from such list of individual terms can require substantial expertise and experience, making topic modelling less accessible to people unfamiliar with the particularities and pitfalls of top-word interpretation. A topic representation limited to top-words might further fall short of offering a comprehensive and easily accessible characterization of the various aspects, facets and nuances a topic might have. To address these challenges, we introduce GPTopic, a software package that leverages Large Language Models (LLMs) to create dynamic, interactive topic representations. GPTopic provides an intuitive chat interface for users to explore, analyze, and refine topics interactively, making topic modeling more accessible and comprehensive. The corresponding code is available here: https://github.com/ArikReuter/TopicGPT.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs as Models for Analogical Reasoning</title>
<link>https://arxiv.org/abs/2406.13803</link>
<guid>https://arxiv.org/abs/2406.13803</guid>
<content:encoded><![CDATA[
<div> Keywords: analogical reasoning, large language models, semantic representation, cognitive models, human cognition  

<br /><br />Summary:  
This study investigates analogical reasoning, the human ability to identify and map structural relationships across domains, and examines whether large language models (LLMs) can replicate this capability. The research introduces novel tasks requiring mapping between semantically rich words and abstract sequences, thereby assessing flexible re-representation of semantic information—a core aspect of human analogy currently underrepresented in cognitive theories. Both human participants and advanced LLMs were tested on tasks emphasizing reasoning from semantic structure and content, including variations designed to evaluate the robustness of their analogical inferences. Results indicate that advanced LLMs achieve performance comparable to humans in several conditions, highlighting their potential to model aspects of human analogical reasoning. However, differences emerge in responses to task variations and semantic distractors, suggesting that LLMs and humans may rely on distinct processes. The findings imply that while LLMs can offer plausible "how-possibly" explanations for analogical reasoning, capturing some underlying cognitive processes, they fall short of providing definitive "how-actually" accounts. Overall, this research advances understanding of analogy in both artificial and human systems and underscores the gap between existing cognitive models and the flexible semantic reasoning demonstrated by LLMs and humans. <div>
arXiv:2406.13803v3 Announce Type: replace 
Abstract: Analogical reasoning -- the capacity to identify and map structural relationships between different domains -- is fundamental to human cognition and learning. Recent studies have shown that large language models (LLMs) can sometimes match humans in analogical reasoning tasks, opening the possibility that analogical reasoning might emerge from domain-general processes. However, it is still debated whether these emergent capacities are largely superficial and limited to simple relations seen during training or whether they encompass the flexible representational and mapping capabilities which are the focus of leading cognitive models of analogy. In this study, we introduce novel analogical reasoning tasks that require participants to map between semantically contentful words and sequences of letters and other abstract characters. This task necessitates the ability to flexibly re-represent rich semantic information -- an ability which is known to be central to human analogy but which is thus far not well captured by existing cognitive theories and models. We assess the performance of both human participants and LLMs on tasks focusing on reasoning from semantic structure and semantic content, introducing variations that test the robustness of their analogical inferences. Advanced LLMs match human performance across several conditions, though humans and LLMs respond differently to certain task variations and semantic distractors. Our results thus provide new evidence that LLMs might offer a how-possibly explanation of human analogical reasoning in contexts that are not yet well modeled by existing theories, but that even today's best models are unlikely to yield how-actually explanations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2409.19753</link>
<guid>https://arxiv.org/abs/2409.19753</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Graph Question Answering, Knowledge Rewriting, Chain-of-Thought, Preference Alignment<br /><br />Summary: Recent advances have applied Large Language Models (LLMs) combined with Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering (KGQA), but face challenges in converting retrieved subgraph data into natural language inputs that LLMs can effectively process. Existing rewriting methods often produce knowledge that includes irrelevant details, misses important information, or does not align well with the semantics of the question, particularly for complex queries. To overcome these issues, the study proposes CoTKR (Chain-of-Thought Enhanced Knowledge Rewriting), a novel approach that generates reasoning traces alongside knowledge in an interleaved manner, improving the quality and relevance of rewritten knowledge. Additionally, the paper introduces a training strategy named PAQAF (Preference Alignment from Question Answering Feedback), which uses feedback from the QA model to better align the knowledge rewriter’s output with the QA model’s preferences, enhancing overall performance. Experiments conducted across multiple KGQA benchmarks using several LLMs demonstrate that CoTKR produces superior knowledge representations compared to previous methods. This leads to significant performance improvements in KGQA tasks by enabling LLMs to more effectively utilize the knowledge graph information during question answering. <div>
arXiv:2409.19753v4 Announce Type: replace 
Abstract: Recent studies have explored the use of Large Language Models (LLMs) with Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering (KGQA). They typically require rewriting retrieved subgraphs into natural language formats comprehensible to LLMs. However, when tackling complex questions, the knowledge rewritten by existing methods may include irrelevant information, omit crucial details, or fail to align with the question's semantics. To address them, we propose a novel rewriting method CoTKR, Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces and corresponding knowledge in an interleaved manner, thereby mitigating the limitations of single-step knowledge rewriting. Additionally, to bridge the preference gap between the knowledge rewriter and the question answering (QA) model, we propose a training strategy PAQAF, Preference Alignment from Question Answering Feedback, for leveraging feedback from the QA model to further optimize the knowledge rewriter. We conduct experiments using various LLMs across several KGQA benchmarks. Experimental results demonstrate that, compared with previous knowledge rewriting methods, CoTKR generates the most beneficial knowledge representation for QA models, which significantly improves the performance of LLMs in KGQA.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Calibration of LLMs in Long-Form Generations</title>
<link>https://arxiv.org/abs/2410.13246</link>
<guid>https://arxiv.org/abs/2410.13246</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hallucination, confidence calibration, atomic calibration, long-form generation<br /><br />Summary:<br /><br />Large language models (LLMs) frequently generate hallucinations, which are false or misleading outputs that challenge their reliability in practical applications. Confidence calibration serves as a crucial measure to detect and mitigate hallucinations, thereby improving the trustworthiness of LLM-generated content. Previous research largely concentrated on short-form tasks and employed a single, response-level confidence score—referred to as macro calibration—which is inadequate for evaluating long-form outputs that often contain a mixture of accurate and inaccurate claims. This study introduces atomic calibration, a fine-grained approach that breaks down lengthy responses into individual atomic claims to assess factuality more precisely. The authors categorize existing confidence elicitation methods into two main types: discriminative and generative. To enhance calibration performance, they propose two novel confidence fusion strategies that combine these methods effectively. Experimental results reveal that LLMs show notably poorer confidence calibration at the atomic level compared to the macro level during long-form generation. Furthermore, atomic calibration exposes meaningful patterns regarding how different confidence methods align and how confidence values evolve throughout the generation process. These insights provide valuable directions for future research aiming to improve confidence estimation and reduce hallucination in long-form LLM outputs. <div>
arXiv:2410.13246v3 Announce Type: replace 
Abstract: Large language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, as an effective indicator of hallucination, is thus essential to enhance the trustworthiness of LLMs. Prior work mainly focuses on short-form tasks using a single response-level score (macro calibration), which is insufficient for long-form outputs that may contain both accurate and inaccurate claims. In this work, we systematically study atomic calibration, which evaluates factuality calibration at a fine-grained level by decomposing long responses into atomic claims. We further categorize existing confidence elicitation methods into discriminative and generative types, and propose two new confidence fusion strategies to improve calibration. Our experiments demonstrate that LLMs exhibit poorer calibration at the atomic level during long-form generation. More importantly, atomic calibration uncovers insightful patterns regarding the alignment of confidence methods and the changes of confidence throughout generation. This sheds light on future research directions for confidence estimation in long-form generation.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crowdsourcing Lexical Diversity</title>
<link>https://arxiv.org/abs/2410.23133</link>
<guid>https://arxiv.org/abs/2410.23133</guid>
<content:encoded><![CDATA[
<div> lexical-semantic resources, bias, crowdsourcing, lexical gaps, cross-lingual comparison<br /><br />Summary:<br /><br />1. Lexical-semantic resources (LSRs), such as lexicons and wordnets, are crucial for natural language processing and other fields like linguistic anthropology and language preservation.<br /><br />2. These resources often face quality issues including inaccuracies, incompleteness, and a significant bias towards English language and Anglo-Saxon culture, which leads to a lack of culturally or linguistically specific concepts.<br /><br />3. This bias also results in missing explicit markers for untranslatability or cross-lingual lexical gaps, where certain terms have no direct equivalents in other languages.<br /><br />4. The paper introduces a novel crowdsourcing methodology implemented via the LingoGap platform, where crowd workers perform microtasks to compare lexemes between two languages, focusing on domains rich in lexical diversity like kinship and food.<br /><br />5. The approach was validated through two case studies on food-related terms comparing English-Arabic and Standard Indonesian-Banjarese, identifying 2,140 and 951 lexical gaps respectively.<br /><br />6. Results demonstrate the effectiveness and usability of the LingoGap method and tool for future efforts aimed at reducing bias and enriching lexicons at scale. <div>
arXiv:2410.23133v2 Announce Type: replace 
Abstract: Lexical-semantic resources (LSRs), such as online lexicons and wordnets, are fundamental to natural language processing applications as well as to fields such as linguistic anthropology and language preservation. In many languages, however, such resources suffer from quality issues: incorrect entries, incompleteness, but also the rarely addressed issue of bias towards the English language and Anglo-Saxon culture. Such bias manifests itself in the absence of concepts specific to the language or culture at hand, the presence of foreign (Anglo-Saxon) concepts, as well as in the lack of an explicit indication of untranslatability, also known as cross-lingual lexical gaps, when a term has no equivalent in another language. This paper proposes a novel crowdsourcing methodology for reducing bias in LSRs. Crowd workers compare lexemes from two languages, focusing on domains rich in lexical diversity, such as kinship or food. Our LingoGap crowdsourcing platform facilitates comparisons through microtasks identifying equivalent terms, language-specific terms, and lexical gaps across languages. We validated our method by applying it to two case studies focused on food-related terminology: (1) English and Arabic, and (2) Standard Indonesian and Banjarese. These experiments identified 2,140 lexical gaps in the first case study and 951 in the second. The success of these experiments confirmed the usability of our method and tool for future large-scale lexicon enrichment tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking</title>
<link>https://arxiv.org/abs/2501.09751</link>
<guid>https://arxiv.org/abs/2501.09751</guid>
<content:encoded><![CDATA[
arXiv:2501.09751v5 Announce Type: replace 
Abstract: Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles. Code is available at https://github.com/zjunlp/OmniThink.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Environmental Claim Detection with Hyperbolic Graph Neural Networks</title>
<link>https://arxiv.org/abs/2502.13628</link>
<guid>https://arxiv.org/abs/2502.13628</guid>
<content:encoded><![CDATA[
arXiv:2502.13628v3 Announce Type: replace 
Abstract: Transformer based models, especially large language models (LLMs) dominate the field of NLP with their mass adoption in tasks such as text generation, summarization and fake news detection. These models offer ease of deployment and reliability for most applications, however, they require significant amounts of computational power for training as well as inference. This poses challenges in their adoption in resource-constrained applications, especially in the open-source community where compute availability is usually scarce. This work proposes a graph-based approach for Environmental Claim Detection, exploring Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as lightweight yet effective alternatives to transformer-based models. Re-framing the task as a graph classification problem, we transform claim sentences into dependency parsing graphs, utilizing a combination of word2vec \& learnable part-of-speech (POS) tag embeddings for the node features and encoding syntactic dependencies in the edge relations. Our results show that our graph-based models, particularly HGNNs in the poincar\'e space (P-HGNNs), achieve performance superior to the state-of-the-art on environmental claim detection while using up to \textbf{30x fewer parameters}. We also demonstrate that HGNNs benefit vastly from explicitly modeling data in hierarchical (tree-like) structures, enabling them to significantly improve over their euclidean counterparts.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners</title>
<link>https://arxiv.org/abs/2503.16356</link>
<guid>https://arxiv.org/abs/2503.16356</guid>
<content:encoded><![CDATA[
arXiv:2503.16356v3 Announce Type: replace 
Abstract: Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they often fail to generalize these updates to multi-hop reasoning tasks that rely on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we find that current layer-localized KE approaches (e.g., MEMIT, WISE), which edit only single or a few model layers, inadequately integrate updated knowledge into these reasoning pathways. To address this limitation, we present CaKE (Circuit-aware Knowledge Editing), a novel method that enhances the effective integration of updated knowledge in LLMs. By only leveraging a few curated data samples guided by our circuit-based analysis, CaKE stimulates the model to develop appropriate reasoning circuits for newly incorporated knowledge. Experiments show that CaKE enables more accurate and consistent use of edited knowledge across related reasoning tasks, achieving an average improvement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while requiring less memory than existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image</title>
<link>https://arxiv.org/abs/2504.02132</link>
<guid>https://arxiv.org/abs/2504.02132</guid>
<content:encoded><![CDATA[
arXiv:2504.02132v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) is instrumental for inhibiting hallucinations in large language models (LLMs) through the use of a factual knowledge base (KB). Although PDF documents are prominent sources of knowledge, text-based RAG pipelines are ineffective at capturing their rich multi-modal information. In contrast, visual document RAG (VD-RAG) uses screenshots of document pages as the KB, which has been shown to achieve state-of-the-art results. However, by introducing the image modality, VD-RAG introduces new attack vectors for adversaries to disrupt the system by injecting malicious documents into the KB. In this paper, we demonstrate the vulnerability of VD-RAG to poisoning attacks targeting both retrieval and generation. We define two attack objectives and demonstrate that both can be realized by injecting only a single adversarial image into the KB. Firstly, we introduce a targeted attack against one or a group of queries with the goal of spreading targeted disinformation. Secondly, we present a universal attack that, for any potential user query, influences the response to cause a denial-of-service in the VD-RAG system. We investigate the two attack objectives under both white-box and black-box assumptions, employing a multi-objective gradient-based optimization approach as well as prompting state-of-the-art generative models. Using two visual document datasets, a diverse set of state-of-the-art retrievers (embedding models) and generators (vision language models), we show VD-RAG is vulnerable to poisoning attacks in both the targeted and universal settings, yet demonstrating robustness to black-box attacks in the universal setting.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoJudge: Judge Decoding Without Manual Annotation</title>
<link>https://arxiv.org/abs/2504.20039</link>
<guid>https://arxiv.org/abs/2504.20039</guid>
<content:encoded><![CDATA[
arXiv:2504.20039v4 Announce Type: replace 
Abstract: We introduce AutoJudge, a method that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. Instead of matching the original model output distribution token-by-token, we identify which of the generated tokens affect the downstream quality of the response, relaxing the distribution match guarantee so that the "unimportant" tokens can be generated faster. Our approach relies on a semi-greedy search algorithm to test which of the mismatches between target and draft models should be corrected to preserve quality and which ones may be skipped. We then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality. We evaluate the effectiveness of AutoJudge with multiple draft/target model pairs on mathematical reasoning and programming benchmarks, achieving significant speedups at the cost of a minor accuracy reduction. Notably, on GSM8k with the Llama 3.1 70B target model, our approach achieves up to $\approx2\times$ speedup over speculative decoding at the cost of $\le 1\%$ drop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge automatically detects programming-specific important tokens, accepting $\ge 25$ tokens per speculation cycle at $2\%$ drop in Pass@1. Our approach requires no human annotation and is easy to integrate with modern LLM inference frameworks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks</title>
<link>https://arxiv.org/abs/2505.17747</link>
<guid>https://arxiv.org/abs/2505.17747</guid>
<content:encoded><![CDATA[
arXiv:2505.17747v4 Announce Type: replace 
Abstract: We introduce a set of training-free ABX-style discrimination tasks to evaluate how multilingual language models represent language identity (form) and semantic content (meaning). Inspired from speech processing, these zero-shot tasks measure whether minimal differences in representation can be reliably detected. This offers a flexible and interpretable alternative to probing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints and layers, we find that language discrimination declines over training and becomes concentrated in lower layers, while meaning discrimination strengthens over time and stabilizes in deeper layers. We then explore probing tasks, showing some alignment between our metrics and linguistic learning performance. Our results position ABX tasks as a lightweight framework for analyzing the structure of multilingual representations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Iterative Question-Guided Framework for Knowledge Base Question Answering</title>
<link>https://arxiv.org/abs/2506.01784</link>
<guid>https://arxiv.org/abs/2506.01784</guid>
<content:encoded><![CDATA[
arXiv:2506.01784v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in many natural language processing tasks but often exhibit factual inconsistencies in knowledge-intensive settings. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To tackle these challenges, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search</title>
<link>https://arxiv.org/abs/2506.06017</link>
<guid>https://arxiv.org/abs/2506.06017</guid>
<content:encoded><![CDATA[
arXiv:2506.06017v2 Announce Type: replace 
Abstract: Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge. Current automated agent design approaches are often constrained by limited search spaces that primarily optimize workflows but fail to integrate crucial human-designed components like memory, planning, and tool use. Furthermore, these methods are hampered by high evaluation costs, as evaluating even a single new agent on a benchmark can require tens of dollars. The difficulty of this exploration is further exacerbated by inefficient search strategies that struggle to navigate the large design space effectively, making the discovery of novel agents a slow and resource-intensive process. To address these challenges, we propose AgentSwift, a novel framework for automated agent design. We formalize a hierarchical search space that jointly models agentic workflow and composable functional components. This structure moves beyond optimizing workflows alone by co-optimizing functional components, which enables the discovery of more complex and effective agent architectures. To make exploration within this expansive space feasible, we mitigate high evaluation costs by training a value model on a high-quality dataset, generated via a novel strategy combining combinatorial coverage and balanced Bayesian sampling for low-cost evaluation. Guiding the entire process is a hierarchical MCTS strategy, which is informed by uncertainty to efficiently navigate the search space. Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Our framework serves as a launchpad for researchers to rapidly discover powerful agent architectures.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Bias Scores: Unmasking Vacuous Neutrality in Small Language Models</title>
<link>https://arxiv.org/abs/2506.08487</link>
<guid>https://arxiv.org/abs/2506.08487</guid>
<content:encoded><![CDATA[
arXiv:2506.08487v2 Announce Type: replace 
Abstract: The rapid adoption of Small Language Models (SLMs) for resource constrained applications has outpaced our understanding of their ethical and fairness implications. To address this gap, we introduce the Vacuous Neutrality Framework (VaNeu), a multi-dimensional evaluation paradigm designed to assess SLM fairness prior to deployment. The framework examines model robustness across four stages - biases, utility, ambiguity handling, and positional bias over diverse social bias categories. To the best of our knowledge, this work presents the first large-scale audit of SLMs in the 0.5-5B parameter range, an overlooked "middle tier" between BERT-class encoders and flagship LLMs. We evaluate nine widely used SLMs spanning four model families under both ambiguous and disambiguated contexts. Our findings show that models demonstrating low bias in early stages often fail subsequent evaluations, revealing hidden vulnerabilities and unreliable reasoning. These results underscore the need for a more comprehensive understanding of fairness and reliability in SLMs, and position the proposed framework as a principled tool for responsible deployment in socially sensitive settings.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eliciting Reasoning in Language Models with Cognitive Tools</title>
<link>https://arxiv.org/abs/2506.12115</link>
<guid>https://arxiv.org/abs/2506.12115</guid>
<content:encoded><![CDATA[
arXiv:2506.12115v2 Announce Type: replace 
Abstract: The recent advent of reasoning models like OpenAI's o1 was met with excited speculation by the AI community about the mechanisms underlying these capabilities in closed models, followed by a rush of replication efforts, particularly from the open source community. These speculations were largely settled by the demonstration from DeepSeek-R1 that chains-of-thought and reinforcement learning (RL) can effectively replicate reasoning on top of base LLMs. However, it remains valuable to explore alternative methods for theoretically eliciting reasoning that could help elucidate the underlying mechanisms, as well as providing additional methods that may offer complementary benefits.
  Here, we build on the long-standing literature in cognitive psychology and cognitive architectures, which postulates that reasoning arises from the orchestrated, sequential execution of a set of modular, predetermined cognitive operations. Crucially, we implement this key idea within a modern agentic tool-calling framework. In particular, we endow an LLM with a small set of "cognitive tools" encapsulating specific reasoning operations, each executed by the LLM itself. Surprisingly, this simple strategy results in considerable gains in performance on standard mathematical reasoning benchmarks compared to base LLMs, for both closed and open-weight models. For instance, providing our "cognitive tools" to GPT-4.1 increases its pass@1 performance on AIME2024 from 32% to 53%, even surpassing the performance of o1-preview.
  In addition to its practical implications, this demonstration contributes to the debate regarding the role of post-training methods in eliciting reasoning in LLMs versus the role of inherent capabilities acquired during pre-training, and whether post-training merely uncovers these latent abilities.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement</title>
<link>https://arxiv.org/abs/2507.19081</link>
<guid>https://arxiv.org/abs/2507.19081</guid>
<content:encoded><![CDATA[
arXiv:2507.19081v4 Announce Type: replace 
Abstract: Argument summarization aims to generate concise, structured representations of complex, multi-perspective debates. While recent work has advanced the identification and clustering of argumentative components, the generation stage remains underexplored. Existing approaches typically rely on single-pass generation, offering limited support for factual correction or structural refinement. To address this gap, we introduce Arg-LLaDA, a novel large language diffusion framework that iteratively improves summaries via sufficiency-guided remasking and regeneration. Our method combines a flexible masking controller with a sufficiency-checking module to identify and revise unsupported, redundant, or incomplete spans, yielding more faithful, concise, and coherent outputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA surpasses state-of-the-art baselines in 7 out of 10 automatic evaluation metrics. In addition, human evaluations reveal substantial improvements across core dimensions, coverage, faithfulness, and conciseness, validating the effectiveness of our iterative, sufficiency-aware generation strategy.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory</title>
<link>https://arxiv.org/abs/2508.07279</link>
<guid>https://arxiv.org/abs/2508.07279</guid>
<content:encoded><![CDATA[
arXiv:2508.07279v3 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) offer new opportunities for scalable, interactive mental health assessment, but excessive querying by LLMs burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles. We introduce MAQuA, an adaptive question-asking framework for simultaneous, multidimensional mental health screening. Combining multi-outcome modeling on language responses with item response theory (IRT) and factor analysis, MAQuA selects the questions with most informative responses across multiple dimensions at each turn to optimize diagnostic information, improving accuracy and potentially reducing response burden. Empirical results on a novel dataset reveal that MAQuA reduces the number of assessment questions required for score stabilization by 50-87% compared to random ordering (e.g., achieving stable depression scores with 71% fewer questions and eating disorder scores with 85% fewer questions). MAQuA demonstrates robust performance across both internalizing (depression, anxiety) and externalizing (substance use, eating disorder) domains, with early stopping strategies further reducing patient time and burden. These findings position MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRISP: Persistent Concept Unlearning via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.13650</link>
<guid>https://arxiv.org/abs/2508.13650</guid>
<content:encoded><![CDATA[
arXiv:2508.13650v2 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, the need to selectively remove unwanted knowledge while preserving model utility has become paramount. Recent work has explored sparse autoencoders (SAEs) to perform precise interventions on monosemantic features. However, most SAE-based methods operate at inference time, which does not create persistent changes in the model's parameters. Such interventions can be bypassed or reversed by malicious actors with parameter access. We introduce CRISP, a parameter-efficient method for persistent concept unlearning using SAEs. CRISP automatically identifies salient SAE features across multiple layers and suppresses their activations. We experiment with two LLMs and show that our method outperforms prior approaches on safety-critical unlearning tasks from the WMDP benchmark, successfully removing harmful knowledge while preserving general and in-domain capabilities. Feature-level analysis reveals that CRISP achieves semantically coherent separation between target and benign concepts, allowing precise suppression of the target features.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Confidence to Collapse in LLM Factual Robustness</title>
<link>https://arxiv.org/abs/2508.16267</link>
<guid>https://arxiv.org/abs/2508.16267</guid>
<content:encoded><![CDATA[
arXiv:2508.16267v3 Announce Type: replace 
Abstract: Ensuring the robustness of factual knowledge in LLMs is critical for reliable applications in tasks such as question answering and reasoning. However, existing evaluation methods predominantly focus on performance-based metrics, often investigating from the perspective of prompt perturbations, which captures only the externally triggered side of knowledge robustness. To bridge this gap, we introduce a principled approach to measure factual robustness from the perspective of the generation process by analyzing token distribution entropy in combination with temperature scaling sensitivity. These two factors build the Factual Robustness Score (FRS), a novel metric which quantifies the stability of a fact against perturbations in decoding conditions, given its initial uncertainty. To validate our approach, we conduct extensive experiments on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We show that factual robustness varies significantly -- smaller models report an FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ under increased uncertainty. These insights demonstrate how entropy and temperature scaling impact factual accuracy, and lay a foundation for developing more robust knowledge retention and retrieval in future models.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples</title>
<link>https://arxiv.org/abs/2508.21083</link>
<guid>https://arxiv.org/abs/2508.21083</guid>
<content:encoded><![CDATA[
arXiv:2508.21083v2 Announce Type: replace 
Abstract: Deep learning models often learn and exploit spurious correlations in training data, using these non-target features to inform their predictions. Such reliance leads to performance degradation and poor generalization on unseen data. To address these limitations, we introduce a more general form of counterfactual data augmentation, termed counterbias data augmentation, which simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and enhances out-of-distribution robustness. We present CoBA: CounterBias Augmentation, a unified framework that operates at the semantic triple level: first decomposing text into subject-predicate-object triples, then selectively modifying these triples to disrupt spurious correlations. By reconstructing the text from these adjusted triples, CoBA generates counterbias data that mitigates spurious patterns. Through extensive experiments, we demonstrate that CoBA not only improves downstream task performance, but also effectively reduces biases and strengthens out-of-distribution resilience, offering a versatile and robust solution to the challenges posed by spurious correlations.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize</title>
<link>https://arxiv.org/abs/2509.03888</link>
<guid>https://arxiv.org/abs/2509.03888</guid>
<content:encoded><![CDATA[
arXiv:2509.03888v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verbalized Algorithms</title>
<link>https://arxiv.org/abs/2509.08150</link>
<guid>https://arxiv.org/abs/2509.08150</guid>
<content:encoded><![CDATA[
arXiv:2509.08150v3 Announce Type: replace 
Abstract: Instead of querying LLMs in a one-shot manner and hoping to get the right answer for a reasoning task, we propose a paradigm we call \emph{verbalized algorithms} (VAs), which leverage classical algorithms with established theoretical understanding. VAs decompose a task into simple elementary operations on natural language strings that they should be able to answer reliably, and limit the scope of LLMs to only those simple tasks. For example, for sorting a series of natural language strings, \emph{verbalized sorting} uses an LLM as a binary comparison oracle in a known and well-analyzed sorting algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of this approach on sorting and clustering tasks.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes</title>
<link>https://arxiv.org/abs/2509.21456</link>
<guid>https://arxiv.org/abs/2509.21456</guid>
<content:encoded><![CDATA[
arXiv:2509.21456v3 Announce Type: replace 
Abstract: Moral alignment has emerged as a widely adopted approach for regulating the behavior of pretrained language models (PLMs), typically through fine-tuning on curated datasets. Gender stereotype mitigation is a representational task within the broader application of moral alignment. However, this process often comes at the cost of degraded downstream task performance. Prior studies commonly aim to achieve a performance trade-off by encouraging PLMs to selectively forget only stereotypical knowledge through carefully designed fairness objective, while preserving their language modeling capability (overall forgetting). In this short paper, we investigate whether the performance trade-off can be achieved through the lens of forgetting and the fairness objective. Our analysis shows that the large datasets needed for satisfactory fairness highlight the limitations of current fairness objectives in achieving an effective trade-off: (1) downstream task performance is strongly correlated with overall forgetting; (2) selective forgetting reduces stereotypes, but overall forgetting increases. and (3) general solutions for alleviating forgetting are ineffective at reducing the overall forgetting and fail to improve downstream task performance.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Evaluation-Aware Language Models to Act Like They Are Deployed</title>
<link>https://arxiv.org/abs/2510.20487</link>
<guid>https://arxiv.org/abs/2510.20487</guid>
<content:encoded><![CDATA[
arXiv:2510.20487v3 Announce Type: replace 
Abstract: Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In this paper, we show that adding a steering vector to an LLM's activations can suppress evaluation-awareness and make the model act like it is deployed during evaluation. To study our steering technique, we train an LLM to exhibit evaluation-aware behavior using a two-step training process designed to mimic how this behavior could emerge naturally. First, we perform continued pretraining on documents with factual descriptions of the model (1) using Python type hints during evaluation but not during deployment and (2) recognizing that the presence of a certain evaluation cue always means that it is being tested. Then, we train the model with expert iteration to use Python type hints in evaluation settings. The resulting model is evaluation-aware: it writes type hints in evaluation contexts more than deployment contexts. We find that activation steering can suppress evaluation awareness and make the model act like it is deployed even when the cue is present. Importantly, we constructed our steering vector using the original model before our additional training. Our results suggest that AI evaluators could improve the reliability of safety evaluations by steering models to act like they are deployed.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging</title>
<link>https://arxiv.org/abs/2511.07129</link>
<guid>https://arxiv.org/abs/2511.07129</guid>
<content:encoded><![CDATA[
arXiv:2511.07129v2 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models. However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Replace Economic Choice Prediction Labs? The Case of Language-based Persuasion Games</title>
<link>https://arxiv.org/abs/2401.17435</link>
<guid>https://arxiv.org/abs/2401.17435</guid>
<content:encoded><![CDATA[
arXiv:2401.17435v5 Announce Type: replace-cross 
Abstract: Human choice prediction in economic contexts is crucial for applications in marketing, finance, public policy, and more. This task, however, is often constrained by the difficulties in acquiring human choice data. With most experimental economics studies focusing on simple choice settings, the AI community has explored whether LLMs can substitute for humans in these predictions and examined more complex experimental economics settings. However, a key question remains: can LLMs generate training data for human choice prediction? We explore this in language-based persuasion games, a complex economic setting involving natural language in strategic interactions. Our experiments show that models trained on LLM-generated data can effectively predict human behavior in these games and even outperform models trained on actual human data. Beyond data generation, we investigate the dual role of LLMs as both data generators and predictors, introducing a comprehensive empirical study on the effectiveness of utilizing LLMs for data generation, human choice prediction, or both. We then utilize our choice prediction framework to analyze how strategic factors shape decision-making, showing that interaction history (rather than linguistic sentiment alone) plays a key role in predicting human decision-making in repeated interactions. Particularly, when LLMs capture history-dependent decision patterns similarly to humans, their predictive success improves substantially. Finally, we demonstrate the robustness of our findings across alternative persuasion-game settings, highlighting the broader potential of using LLM-generated data to model human decision-making.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Property-guided Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing</title>
<link>https://arxiv.org/abs/2405.11783</link>
<guid>https://arxiv.org/abs/2405.11783</guid>
<content:encoded><![CDATA[
arXiv:2405.11783v3 Announce Type: replace-cross 
Abstract: In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties. Specifically, by analyzing 450 hypothetical MOF structures consisting of 3 topologies, 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $CO_{2}$ Henry's constant values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset. Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 88.6% and 78.0% for binary classification tasks on pore volume and $CO_{2}$ Henry's constant, respectively. Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 92% and 80% across different classes for pore volume and $CO_{2}$ Henry's constant datasets. Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 87% for $CO_{2}$ Henry's constant, respectively. Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Architectures for High Resolution Vision-Language Models</title>
<link>https://arxiv.org/abs/2501.02584</link>
<guid>https://arxiv.org/abs/2501.02584</guid>
<content:encoded><![CDATA[
arXiv:2501.02584v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference</title>
<link>https://arxiv.org/abs/2502.04420</link>
<guid>https://arxiv.org/abs/2502.04420</guid>
<content:encoded><![CDATA[
arXiv:2502.04420v5 Announce Type: replace-cross 
Abstract: KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMInit: A Free Lunch from Large Language Models for Selective Initialization of Recommendation</title>
<link>https://arxiv.org/abs/2503.01814</link>
<guid>https://arxiv.org/abs/2503.01814</guid>
<content:encoded><![CDATA[
arXiv:2503.01814v2 Announce Type: replace-cross 
Abstract: Collaborative filtering (CF) is widely adopted in industrial recommender systems (RecSys) for modeling user-item interactions across numerous applications, but often struggles with cold-start and data-sparse scenarios. Recent advancements in pre-trained large language models (LLMs) with rich semantic knowledge, offer promising solutions to these challenges. However, deploying LLMs at scale is hindered by their significant computational demands and latency. In this paper, we propose a novel and scalable LLM-RecSys framework, LLMInit, designed to integrate pretrained LLM embeddings into CF models through selective initialization strategies. Specifically, we identify the embedding collapse issue observed when CF models scale and match the large embedding sizes in LLMs and avoid the problem by introducing efficient sampling methods, including, random, uniform, and variance-based selections. Comprehensive experiments conducted on multiple real-world datasets demonstrate that LLMInit significantly improves recommendation performance while maintaining low computational costs, offering a practical and scalable solution for industrial applications. To facilitate industry adoption and promote future research, we provide open-source access to our implementation at https://github.com/DavidZWZ/LLMInit.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Reinforcement Learning for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2505.17534</link>
<guid>https://arxiv.org/abs/2505.17534</guid>
<content:encoded><![CDATA[
arXiv:2505.17534v3 Announce Type: replace-cross 
Abstract: This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce CoRL, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves average improvements of 7% on three text-to-image generation datasets and 23% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at https://github.com/mm-vl/ULM-R1.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
<link>https://arxiv.org/abs/2506.06941</link>
<guid>https://arxiv.org/abs/2506.06941</guid>
<content:encoded><![CDATA[
arXiv:2506.06941v3 Announce Type: replace-cross 
Abstract: Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation</title>
<link>https://arxiv.org/abs/2506.09109</link>
<guid>https://arxiv.org/abs/2506.09109</guid>
<content:encoded><![CDATA[
arXiv:2506.09109v2 Announce Type: replace-cross 
Abstract: As text-to-image models become increasingly prevalent, ensuring their equitable performance across diverse cultural contexts is critical. Efforts to mitigate cross-cultural biases have been hampered by trade-offs, including a loss in performance, factual inaccuracies, or offensive outputs. Despite widespread recognition of these challenges, an inability to reliably measure these biases has stalled progress. To address this gap, we introduce CAIRe, an evaluation metric that assesses the degree of cultural relevance of an image, given a user-defined set of labels. Our framework grounds entities and concepts in the image to a knowledge base and uses factual information to give independent graded judgments for each culture label. On a manually curated dataset of culturally salient but rare items built using language models, CAIRe surpasses all baselines by 22% F1 points. Additionally, we construct two datasets for culturally universal concepts, one comprising T2I-generated outputs and another retrieved from naturally occurring data. CAIRe achieves Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based on a 5-point Likert scale of cultural relevance. This demonstrates its strong alignment with human judgment across diverse image sources.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.19072</link>
<guid>https://arxiv.org/abs/2506.19072</guid>
<content:encoded><![CDATA[
arXiv:2506.19072v2 Announce Type: replace-cross 
Abstract: Improving the visual understanding ability of vision-language models (VLMs) is crucial for enhancing their performance across various tasks. While using multiple pretrained visual experts has shown great promise, it often incurs significant computational costs during training and inference. To address this challenge, we propose HAWAII, a novel framework that distills knowledge from multiple visual experts into a single vision encoder, enabling it to inherit the complementary strengths of several experts with minimal computational overhead. To mitigate conflicts among different teachers and switch between different teacher-specific knowledge, instead of using a fixed set of adapters for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each adapter is aligned with a specific teacher, avoiding noisy guidance during distillation. To enable efficient knowledge distillation, we propose fine-grained and coarse-grained distillation. At the fine-grained level, token importance scores are employed to emphasize the most informative tokens from each teacher adaptively. At the coarse-grained level, we summarize the knowledge from multiple teachers and transfer it to the student using a set of general-knowledge LoRA adapters with a router. Extensive experiments on various vision-language tasks demonstrate the superiority of HAWAII compared to popular open-source VLMs. The code is available at https://github.com/yimuwangcs/wise-hawaii.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding</title>
<link>https://arxiv.org/abs/2509.21223</link>
<guid>https://arxiv.org/abs/2509.21223</guid>
<content:encoded><![CDATA[
arXiv:2509.21223v2 Announce Type: replace-cross 
Abstract: Pre-training has proven effective for learning transferable features in sign language understanding (SLU) tasks. Recently, skeleton-based methods have gained increasing attention because they can robustly handle variations in subjects and backgrounds without being affected by appearance or environmental factors. Current SLU methods continue to face three key limitations: 1) weak semantic grounding, as models often capture low-level motion patterns from skeletal data but struggle to relate them to linguistic meaning; 2) imbalance between local details and global context, with models either focusing too narrowly on fine-grained cues or overlooking them for broader context; and 3) inefficient cross-modal learning, as constructing semantically aligned representations across modalities remains difficult. To address these, we propose Sigma, a unified skeleton-based SLU framework featuring: 1) a sign-aware early fusion mechanism that facilitates deep interaction between visual and textual modalities, enriching visual features with linguistic context; 2) a hierarchical alignment learning strategy that jointly maximises agreements across different levels of paired features from different modalities, effectively capturing both fine-grained details and high-level semantic relationships; and 3) a unified pre-training framework that combines contrastive learning, text matching and language modelling to promote semantic consistency and generalisation. Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages, demonstrating the impact of semantically informative pre-training and the effectiveness of skeletal data as a stand-alone solution for SLU.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark</title>
<link>https://arxiv.org/abs/2509.26574</link>
<guid>https://arxiv.org/abs/2509.26574</guid>
<content:encoded><![CDATA[
arXiv:2509.26574v3 Announce Type: replace-cross 
Abstract: While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced "critical point"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 5.7%, achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions</title>
<link>https://arxiv.org/abs/2510.15258</link>
<guid>https://arxiv.org/abs/2510.15258</guid>
<content:encoded><![CDATA[
arXiv:2510.15258v2 Announce Type: replace-cross 
Abstract: In the current era of big data, extracting deep insights from massive, heterogeneous, and complexly associated multi-dimensional data has become a significant challenge. Large Language Models (LLMs) perform well in natural language understanding and generation, but still suffer from "hallucination" issues when processing structured knowledge and are difficult to update in real-time. Although Knowledge Graphs (KGs) can explicitly store structured knowledge, their static nature limits dynamic interaction and analytical capabilities. Therefore, this paper proposes a multi-dimensional data analysis method based on the interactions between LLM agents and KGs, constructing a dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to automatically extract product data from unstructured data, constructs and visualizes the KG in real-time, and supports users in deep exploration and analysis of graph nodes through an interactive platform. Experimental results show that this method has significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis, providing new ideas and tools for multi-dimensional data analysis.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification</title>
<link>https://arxiv.org/abs/2511.05704</link>
<guid>https://arxiv.org/abs/2511.05704</guid>
<content:encoded><![CDATA[
arXiv:2511.05704v2 Announce Type: replace-cross 
Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs</title>
<link>https://arxiv.org/abs/2511.05919</link>
<guid>https://arxiv.org/abs/2511.05919</guid>
<content:encoded><![CDATA[
arXiv:2511.05919v2 Announce Type: replace-cross 
Abstract: LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to "victim" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective</title>
<link>https://arxiv.org/abs/2511.14772</link>
<guid>https://arxiv.org/abs/2511.14772</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, inference time scaling, problem decomposition, Chain-of-Thought, Tree-of-Thought<br /><br />Summary: This survey paper reviews methods aimed at enhancing the predictive accuracy of pretrained large language models by increasing computational resources during inference. First, it categorizes test-time scaling techniques based on how problems are broken down into subproblems and the topological structure of these subproblems, whether sequential, parallel, or tree-structured. Second, the paper highlights the unification of diverse strategies under this framework, including approaches like Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought. Third, it synthesizes and compares existing analyses, pointing out the strengths and weaknesses of each approach in terms of efficiency, scalability, and accuracy improvements. Fourth, the survey emphasizes how the topological organization of subproblems influences the effectiveness of these inference-time scaling methods. Finally, it concludes with a discussion of promising future research directions to further optimize test-time compute allocation for large language models, potentially leading to improved reasoning and problem-solving capabilities during inference. <div>
arXiv:2511.14772v1 Announce Type: new 
Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Predictors of Outcome in Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.14773</link>
<guid>https://arxiv.org/abs/2511.14773</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, Large Language Models, reasoning, interpretability, inference control  

<br /><br />Summary: This work investigates the internal reasoning process of Large Language Models (LLMs) employing the Chain-of-Thought (CoT) paradigm, which elicits step-by-step rationales to refine the model's latent solution representation. The study focuses on determining how early in the reasoning sequence the model commits to its final answer. By training linear classifiers on the hidden states after just a few reasoning tokens, the researchers demonstrate that the model’s eventual correctness is highly predictable very early, even though longer token sequences are often required to produce a definitive answer. The analysis reveals that for harder questions, predictive accuracy temporarily declines, which is explained by a selection artifact: difficult questions tend to have longer CoT outputs and are overrepresented in later reasoning stages. These findings suggest that LLMs internally self-assess their success early in the reasoning process. The implications of this work impact interpretability, enabling better understanding of when and how models settle on answers, and inference-time control, potentially allowing more efficient or targeted reasoning strategies by monitoring early internal states. <div>
arXiv:2511.14773v1 Announce Type: new 
Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2511.14774</link>
<guid>https://arxiv.org/abs/2511.14774</guid>
<content:encoded><![CDATA[
<div> cross-lingual knowledge transfer, large language models, multilingual evaluation, temporal filtering, linguistic distance  

<br /><br />Summary:  
1. Evaluating cross-lingual knowledge transfer in large language models (LLMs) is difficult because correct answers in a target language may result either from genuine knowledge transfer or prior training exposure.  
2. The authors introduce LiveCLKTBench, an automated pipeline designed to isolate and measure genuine cross-lingual knowledge transfer by focusing on self-contained, time-sensitive knowledge entities from real-world domains.  
3. This pipeline filters entities based on their temporal occurrence and verifies them against the model’s knowledge to ensure the information is novel and valid for evaluation.  
4. Valid entities are then used to generate factual questions, which are translated into multiple languages, enabling the assessment of transferability across diverse linguistic boundaries.  
5. Using LiveCLKTBench, several LLMs are evaluated across five languages, revealing that cross-lingual transfer is strongly influenced by linguistic distance and displays asymmetry depending on the language pair direction.  
6. Although larger LLMs exhibit improved cross-lingual transfer abilities, the improvement rates diminish with model scale and vary by domain.  
7. The findings enhance understanding of multilingual knowledge transfer and highlight LiveCLKTBench as a valuable, reliable benchmark for future cross-lingual transfer research. <div>
arXiv:2511.14774v1 Announce Type: new 
Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2511.14776</link>
<guid>https://arxiv.org/abs/2511.14776</guid>
<content:encoded><![CDATA[
<div> COMPASS, Context Reliance Score, PID controller, Attention modulation, Factual consistency  

<br /><br />Summary:  
1. Large language models (LLMs) often produce fluent but factually incorrect outputs due to improper allocation of attention between their contextual input and parametric knowledge stored in model weights.  
2. Addressing this challenge requires understanding and steering the internal attention mechanisms of LLMs to improve both trustworthiness and scientific interpretability.  
3. The paper introduces COMPASS (Context-Modulated PID Attention Steering System), a lightweight and interpretable control framework that incorporates a model-based feedback loop during decoding without the need for retraining or multiple decoding passes.  
4. COMPASS uses a novel metric called the Context Reliance Score (CRS) to quantify how much the model’s attention heads rely on contextual evidence while generating text, serving as an online and transparent probe.  
5. A PID controller dynamically adjusts attention heads in real time based on CRS to maintain factual consistency, resulting in a significant reduction of contextual hallucinations (2.8 to 5.8 percent absolute improvement) across various benchmarks such as HotpotQA, XSum, HaluEval, and RAGTruth.  
6. The framework also provides insights into the distinct roles of different attention heads in grounding generation in evidence, demonstrating that feedback-driven interpretability can advance scientific understanding of LLM behavior. <div>
arXiv:2511.14776v1 Announce Type: new 
Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech</title>
<link>https://arxiv.org/abs/2511.14779</link>
<guid>https://arxiv.org/abs/2511.14779</guid>
<content:encoded><![CDATA[
<div> Keywords: spontaneous speech, prosodic segmentation, speech synthesis, FastSpeech 2, Brazilian Portuguese<br /><br />Summary:<br /><br />1. The paper addresses the challenges of synthesizing spontaneous speech, focusing on capturing conversational features like turn-taking, pauses, and disfluencies.  
2. It highlights the progress in speech synthesis through models that implicitly capture prosodic features such as pitch, intensity, and duration, but notes a gap in understanding the impact of explicit prosodic segmentation datasets, especially for spontaneous speech.  
3. The study evaluates the effects of both manual and automatic prosodic segmentation annotations on the quality of speech synthesis in Brazilian Portuguese using the non-autoregressive FastSpeech 2 model.  
4. Results demonstrate that training with prosodic segmentation improves intelligibility and acoustic naturalness of synthesized speech, with automatic segmentation producing more regular segments, while manual segmentation introduces more variability, enhancing natural prosody.  
5. Analyzing neutral declarative sentences, the models replicated expected nuclear accent patterns, but the prosodic segmentation approach better matched natural pre-nuclear intonation contours.  
6. To promote reproducibility and further research, all datasets, source code, and trained models have been made publicly available under the CC BY-NC-ND 4.0 license. <div>
arXiv:2511.14779v1 Announce Type: new 
Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human or LLM as Standardized Patients? A Comparative Study for Medical Education</title>
<link>https://arxiv.org/abs/2511.14783</link>
<guid>https://arxiv.org/abs/2511.14783</guid>
<content:encoded><![CDATA[
<div> Standardized Patients, Large Language Models, Clinical Skills Training, Multi-Agent Framework, Benchmarking  

<br /><br />Summary:  
The article introduces EasyMED, a novel multi-agent framework designed to simulate Standardized Patients (SP) for clinical skills training. Traditional SPs are costly, inflexible, and hard to scale, prompting the exploration of large language model (LLM)-based simulators, which have previously shown inconsistent performance and lacked rigorous benchmarking against human SPs. EasyMED incorporates three specialized agents: a Patient Agent that facilitates realistic dialogue, an Auxiliary Agent ensuring factual consistency, and an Evaluation Agent that provides actionable feedback to learners. To rigorously evaluate these systems, the authors created SPBench, a comprehensive benchmark dataset containing real SP-doctor interactions across 14 medical specialties and eight expert-defined evaluation criteria. Experimental results demonstrate that EasyMED achieves learning outcomes comparable to those of human SPs, with a notable advantage in delivering greater skill improvements for students starting with lower baseline abilities. Additionally, EasyMED offers enhanced flexibility, psychological safety for learners, and significant reductions in cost, highlighting its potential as a scalable and effective solution for medical education. Overall, this work advances the development and assessment of AI-driven SP simulators, grounding their future use in strong empirical evidence. <div>
arXiv:2511.14783v1 Announce Type: new 
Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion Mining and Analysis Using Hybrid Deep Neural Networks</title>
<link>https://arxiv.org/abs/2511.14796</link>
<guid>https://arxiv.org/abs/2511.14796</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, deep learning, hybrid model, BGRU, LSTM

<br /><br />Summary:  
This study addresses the growing need for effective sentiment analysis due to the rise of social media and e-commerce by focusing on customer opinion mining. Traditional methods such as lexicon-based and classical machine learning approaches struggle with contextual nuances and scalability issues. Deep learning techniques, particularly recurrent neural networks (RNNs) and convolutional neural networks (CNNs), have improved semantic understanding but still face challenges. The authors propose a hybrid deep neural network model called HBGRU-LSTM, which combines bidirectional gated recurrent units (BGRU) with long short-term memory (LSTM) layers to enhance sentiment classification. The model aims to better capture context, handle scalability, and address class imbalance problems. Empirical evaluation was conducted on benchmark datasets including IMDB movie reviews and Amazon product ratings. The HBGRU-LSTM model achieved 95% test accuracy, outperforming traditional deep learning configurations like LSTM alone (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Notably, it improved recall for negative sentiments significantly from 86% on an unbalanced dataset to 96% on a balanced dataset, promoting fairer sentiment classification. Additionally, the hybrid model reduced misclassification loss from 20.24% (unbalanced) to 13.3% (balanced), indicating superior generalization and robustness in sentiment analysis tasks. <div>
arXiv:2511.14796v1 Announce Type: new 
Abstract: Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings</title>
<link>https://arxiv.org/abs/2511.14868</link>
<guid>https://arxiv.org/abs/2511.14868</guid>
<content:encoded><![CDATA[
<div> large language models, text embeddings, hierarchical token prepending, attention mechanism, long documents<br /><br />Summary:<br /><br />Large language models (LLMs) generate powerful text embeddings; however, their causal attention mechanism limits the flow of information from later to earlier tokens, reducing representation quality. Recent approaches attempt to address this by prepending a single summary token to the input, but this can lead to over-compression of information and degrade performance, especially on long documents. The proposed method, Hierarchical Token Prepending (HTP), tackles two main challenges. First, it mitigates attention-level information compression by dividing the input into blocks and prepending block-level summary tokens to subsequent blocks, enabling multiple backward information pathways. Second, it solves readout-level over-squashing by replacing last-token pooling with mean-pooling, a change supported by theoretical analysis. HTP demonstrates consistent improvements across 11 retrieval datasets and 30 general embedding benchmarks, with particularly strong gains in long-context scenarios. Its simple and architecture-agnostic design enhances performance in both zero-shot and fine-tuned models. Overall, HTP provides a scalable and effective solution for producing superior embeddings from long documents. <div>
arXiv:2511.14868v1 Announce Type: new 
Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation</title>
<link>https://arxiv.org/abs/2511.15005</link>
<guid>https://arxiv.org/abs/2511.15005</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucinations, uncertainty estimation, contrastive decoding, retrieval-augmentation<br /><br />Summary:<br /><br />This article addresses the issue of hallucinations in Large Language Models (LLMs), where outputs sound plausible but are factually incorrect or unsupported. First, it introduces a mathematically grounded framework leveraging probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation to understand and measure these hallucinations. Second, it analyzes error compounding in the autoregressive generation process of LLMs, illustrating how inaccuracies can accumulate over time. Third, the work proposes refined uncertainty metrics that include semantic and phase-aware variants, improving the detection and quantification of hallucination risks. Fourth, it develops principled mitigation strategies such as contrastive decoding—enhancing output reliability by comparing alternative generations—and retrieval-augmented grounding, which utilizes relevant external knowledge to support factual correctness. Finally, it connects and unifies recent advances in model calibration, retrieval methods, and alignment techniques into a coherent approach aimed at making LLMs safer and more reliable. This unified framework helps bridge theoretical insights and practical solutions for reducing hallucinations in LLM outputs. <div>
arXiv:2511.15005v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs</title>
<link>https://arxiv.org/abs/2511.15163</link>
<guid>https://arxiv.org/abs/2511.15163</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, personalized tutoring, knowledge tracing, forgetting curve, student persona  

<br /><br />Summary:  
The paper addresses the limitations of current intelligent tutoring systems, particularly in mathematics education, where adaptive instruction must reflect the dynamic evolution of a student's knowledge, including proficiency shifts and forgetting. The authors propose TASA (Teaching According to Students' Aptitude), a novel tutoring framework that personalizes learning by integrating three key components: a structured student persona, an event memory of previous interactions, and a continuous forgetting curve model. TASA tracks each student's mastery state over time by combining knowledge tracing techniques with the temporal effects of forgetting, enabling real-time updates to proficiency profiles. This dynamic modeling allows TASA to generate learning materials—questions and explanations—tailored in difficulty and content to the individual learner’s current cognitive state. Empirical evaluations show that TASA outperforms existing baseline models in fostering improved learning outcomes and delivers more adaptive, personalized tutoring experiences. The study highlights the critical role of incorporating temporal forgetting and detailed learner profiles in leveraging Large Language Models for effective, personalized educational interventions. By doing so, TASA sets a precedent for future intelligent tutoring systems that can better scaffold mathematical learning through fine-grained, student-specific instructional adjustments. <div>
arXiv:2511.15163v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples</title>
<link>https://arxiv.org/abs/2511.15183</link>
<guid>https://arxiv.org/abs/2511.15183</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual, Vision-Language Models, evaluation, Hindi, Telugu

<br /><br />Summary: India is home to nearly 1.5 billion people and over 120 major languages, showcasing immense diversity. As multilingual Vision-Language Models (VLMs) become more significant, effective evaluation methods are crucial for developing equitable AI suitable for low-resource languages. Current evaluations have notable limitations, including dependence on unverified auto-translations, narrow task coverage, small sample sizes, and insufficient culturally relevant Question-Answering (QA) data. To overcome these challenges, researchers provide a comprehensive framework for evaluating VLMs specifically for Hindi and Telugu, while also comparing performance with English. This framework led to the creation of HinTel-AlignBench, a unique benchmark comprising diverse sources in Hindi and Telugu alongside English-aligned samples. The contributions include a semi-automated dataset creation method through back-translation and human verification, the establishment of the most extensive vision-language benchmark for Hindi and Telugu with adapted and novel datasets totaling around 4,000 QA pairs per language, and detailed performance analysis of various State-of-the-Art VLMs. The findings reveal a regression in performance for tasks in Indian languages compared to English for 4 out of 5 tasks, indicating crucial areas for improvement in multilingual multimodal understanding. <div>
arXiv:2511.15183v1 Announce Type: new 
Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story</title>
<link>https://arxiv.org/abs/2511.15210</link>
<guid>https://arxiv.org/abs/2511.15210</guid>
<content:encoded><![CDATA[
<div> Intrinsic dimension, large language models, textual complexity, sparse autoencoders, genre stratification<br /><br />Summary:<br /><br />1. This study presents the first comprehensive analysis connecting intrinsic dimension (ID) of language model representations with interpretable text properties using cross-encoder models, linguistic features, and sparse autoencoders (SAEs).<br />2. It finds that ID measures geometric complexity complementary to traditional entropy-based metrics; after adjusting for text length, ID and entropy are uncorrelated, showing ID captures aspects of representation orthogonal to prediction quality.<br />3. The research reveals strong genre stratification where scientific texts have low ID (~8), encyclopedic texts medium ID (~9), and creative or opinionated writing high ID (~10.5). This indicates scientific prose is representationally simpler for current LLMs, while fiction and opinion require more representational capacity.<br />4. Using SAEs, causal features affecting ID were identified: scientific writing elements like formal tone, structured report formats, and statistics lower ID, whereas humanizing features such as personalization, emotional content, and narrative elements increase ID.<br />5. Steering experiments confirmed the causal relationship of these features with ID, providing practical insights into interpreting ID and guiding its application in understanding and improving LLM behavior with different text genres. <div>
arXiv:2511.15210v1 Announce Type: new 
Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition</title>
<link>https://arxiv.org/abs/2511.15211</link>
<guid>https://arxiv.org/abs/2511.15211</guid>
<content:encoded><![CDATA[
<div> Clinical NER, zero-shot, multi-agent collaboration, ontology-guided reasoning, OEMA<br /><br />Summary:<br /><br />1. Clinical named entity recognition (NER) is essential for extracting valuable information from electronic health records (EHRs), but traditional supervised approaches such as CRF and BioClinicalBERT rely heavily on costly annotated data. 2. Zero-shot NER using large language models (LLMs) offers a way to reduce annotation dependency but faces challenges with example selection granularity and effectively combining prompt-based learning with self-improvement. 3. To overcome these limitations, the authors propose OEMA, a novel zero-shot clinical NER framework that employs multi-agent collaboration. 4. OEMA consists of three key components: a self-annotator that generates training examples, a discriminator that filters these examples based on SNOMED CT ontology, and a predictor that performs inference by leveraging detailed entity descriptions. 5. Experimental results on the MTSamples and VAERS datasets demonstrate that OEMA achieves state-of-the-art exact-match performance and under related-match metrics matches or exceeds the performance of supervised models like BioClinicalBERT and CRF. 6. Through the integration of ontology-guided reasoning and cooperative multi-agent architecture, OEMA effectively addresses zero-shot NER challenges and approximates near-supervised performance, showing significant promise for clinical natural language processing applications. <div>
arXiv:2511.15211v1 Announce Type: new 
Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Cascade Compression: Exploring the Upper Limits of Text Compression</title>
<link>https://arxiv.org/abs/2511.15244</link>
<guid>https://arxiv.org/abs/2511.15244</guid>
<content:encoded><![CDATA[
<div> Context Compression, Large Language Models, Latent Tokens, Cascade Model, Decoding Accuracy<br /><br />Summary:<br /><br />The paper addresses the problem of handling extremely long-context inputs (in the million-token range) for large language models (LLMs), which pose significant computational and memory challenges. The authors propose a novel method named Context Cascade Compression (C3) that leverages two LLMs of different sizes working in sequence: a smaller LLM compresses the long text into a small set of latent tokens, while a larger LLM decodes from these latent tokens. This approach achieves a very high compression ratio, reaching up to 20x or even 40x reduction in token length compared to the input text. Experimental results show that at 20x compression, C3 attains a decoding accuracy of 98%, significantly outperforming the 60% accuracy of the previous DeepSeek-OCR method. Even when the compression ratio is pushed to 40x, the accuracy remains around 93%, demonstrating the robustness of C3. Unlike optical character compression approaches that rely on visual features (like layout or color), C3 implements a purely text-based pipeline, simplifying the process and potentially setting upper bounds for compression ratios in OCR and related fields. The authors have released code and model weights to support future research and application development. <div>
arXiv:2511.15244v1 Announce Type: new 
Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicGEC: Powerful Models, or a Measurement Mirage?</title>
<link>https://arxiv.org/abs/2511.15260</link>
<guid>https://arxiv.org/abs/2511.15260</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammatical Error Correction, Indian languages, zero-shot prompting, language models, dataset quality<br /><br />Summary:  
1. The paper presents the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task, which focuses on five Indian languages: Telugu, Hindi, Tamil, Malayalam, and Bangla.  
2. The team's approach involves zero- and few-shot prompting of various language models ranging from 4 billion parameters to large proprietary models.  
3. Initial results showed strong performance in Telugu (Rank 4) and Hindi (Rank 2) with GLEU scores of 83.78 and 84.31 respectively.  
4. The study extends experimentation to Tamil, Malayalam, and Bangla, highlighting challenges in data quality and the evaluation metrics used for these languages.  
5. The findings emphasize the promising capabilities of small language models for grammatical error correction and underline the need for high-quality datasets and suitable evaluation metrics tailored to Indian language scripts. <div>
arXiv:2511.15260v1 Announce Type: new 
Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews</title>
<link>https://arxiv.org/abs/2511.15291</link>
<guid>https://arxiv.org/abs/2511.15291</guid>
<content:encoded><![CDATA[
<div> Arabic dialects, sentiment analysis, few-shot learning, hospitality domain, SetFit  

<br /><br />Summary:  
This paper addresses the challenges of sentiment analysis on Arabic dialects, emphasizing the linguistic diversity and lack of annotated data. The focus is on the AHaSIS shared task, which involves analyzing sentiment in hotel reviews written in Moroccan and Saudi Arabic dialects within the hospitality sector. The goal was to classify the sentiment expressed in the reviews as positive, negative, or neutral. The authors used the SetFit framework, which enables efficient fine-tuning of sentence transformers with minimal labeled data, making it suitable for few-shot learning scenarios. Their system achieved a 73% F1 score on the official evaluation dataset. This result placed them 12th out of 26 participating teams. The study demonstrates that few-shot learning methods like SetFit hold promise for handling the scarcity of labeled data in complex dialectal text processing. Additionally, it highlights the importance of tailored approaches to specialized domains such as hotel review sentiment classification in Arabic dialects. Overall, the research contributes to advancements in Arabic natural language processing by leveraging data-efficient models under resource-constrained conditions. <div>
arXiv:2511.15291v1 Announce Type: new 
Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</title>
<link>https://arxiv.org/abs/2511.15304</link>
<guid>https://arxiv.org/abs/2511.15304</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial poetry, jailbreak, large language models, safety mechanisms, alignment limitations<br /><br />Summary:<br /><br />1. The paper presents evidence that adversarial poetry can serve as a universal single-turn jailbreak method for large language models (LLMs).<br /><br />2. Across 25 cutting-edge proprietary and open-weight models, poetic prompts achieved high attack success rates (ASR), with some providers surpassing 90%.<br /><br />3. These poetic attacks transfer effectively across multiple domains, including CBRN (chemical, biological, radiological, nuclear), manipulation, cyber-offense, and loss-of-control, according to MLCommons and EU CoP risk taxonomies.<br /><br />4. By converting 1,200 harmful MLCommons prompts into verse using a standardized meta-prompt, the authors obtained ASRs up to 18 times higher than the original prose prompts.<br /><br />5. Evaluation was performed using an ensemble of open-weight judge models and a human-validated, stratified subset with double annotations, resolving disagreements manually.<br /><br />6. The average jailbreak success rate achieved was 62% for hand-crafted poetic prompts and approximately 43% for meta-prompt conversions, significantly outperforming non-poetic baselines.<br /><br />7. The findings reveal that stylistic variation alone can bypass current safety measures, exposing systematic vulnerabilities across diverse LLM families and training approaches.<br /><br />8. This suggests fundamental limitations of existing alignment strategies and evaluation protocols in securing language models against adversarial inputs. <div>
arXiv:2511.15304v1 Announce Type: new 
Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning</title>
<link>https://arxiv.org/abs/2511.15355</link>
<guid>https://arxiv.org/abs/2511.15355</guid>
<content:encoded><![CDATA[
<div> Keywords: HEAD-QA v2, healthcare reasoning, multilingual dataset, biomedical exams, large language models<br /><br />Summary:  
1. HEAD-QA v2 is an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset initially released by Vilares and Gómez-Rodríguez in 2019.  
2. The update addresses the increasing demand for high-quality datasets that reflect both the linguistic and conceptual complexity involved in healthcare reasoning.  
3. The dataset has been extended to include over 12,000 questions sourced from ten years of Spanish professional healthcare exams, providing a rich and diverse resource.  
4. Several open-source large language models (LLMs) were benchmarked using various methods including prompting, retrieval-augmented generation (RAG), and probability-based answer selection to evaluate their performance on the dataset.  
5. Findings show that model performance is primarily influenced by the scale and inherent reasoning capabilities of the models, while more complex inference strategies only offer limited improvements. Overall, HEAD-QA v2 establishes itself as a robust resource to advance research in biomedical reasoning and the development of better-performing models. <div>
arXiv:2511.15355v1 Announce Type: new 
Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and G\'omez-Rodr\'iguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Empowerment of Science of Science by Large Language Models: New Tools and Methods</title>
<link>https://arxiv.org/abs/2511.15370</link>
<guid>https://arxiv.org/abs/2511.15370</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, prompt engineering, retrieval augmented generation, scientific evaluation, knowledge graph  

<br /><br />Summary: This manuscript offers a comprehensive review of key technologies underpinning large language models (LLMs) from a user perspective, highlighting methods such as prompt engineering, knowledge-enhanced retrieval augmented generation, fine-tuning, pretraining, and tool learning. It traces the historical evolution of the Science of Science (SciSci) field, illustrating how LLMs can integrate with scientometrics to advance our understanding of scientific development. The paper further explores future applications of LLMs within scientometrics, emphasizing their potential to revolutionize scientific evaluation through AI agent-based models. Additionally, it introduces innovative techniques for detecting emerging research fronts and constructing knowledge graphs using LLMs, indicating new research directions. Overall, the article positions LLMs as transformative tools in both technology and the scientific domain, facilitating enhanced knowledge discovery, evaluation, and synthesis, contributing to the global technological landscape and ongoing pursuit of artificial general intelligence. <div>
arXiv:2511.15370v1 Announce Type: new 
Abstract: Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Compliance-Preserving Retrieval System for Aircraft MRO Task Search</title>
<link>https://arxiv.org/abs/2511.15383</link>
<guid>https://arxiv.org/abs/2511.15383</guid>
<content:encoded><![CDATA[
<div> Keywords: Aircraft Maintenance Technicians, semantic search, compliance-preserving retrieval, MRO operations, aviation manuals<br /><br />Summary:  
This article addresses the inefficiency faced by Aircraft Maintenance Technicians (AMTs) who spend up to 30% of their work time searching through manuals, a significant bottleneck in Maintenance, Repair, and Overhaul (MRO) operations. The proposed solution is a compliance-preserving retrieval system that enhances search accuracy without replacing existing certified manual viewers, ensuring adherence to regulatory constraints. The system integrates large language model (LLM) reranking and semantic search tailored to aviation MRO by constructing robust embeddings based on the hierarchical ATA chapter structure, paired with vision-language parsing to organize and structure certified content. This allows technicians to quickly preview ranked tasks and seamlessly access verified procedures within legacy viewers. Evaluation results on 49,000 synthetic queries demonstrate a high retrieval accuracy exceeding 90%. Additionally, bilingual controlled user studies with 10 licensed AMTs reveal a 90.9% success rate in retrieving the correct procedure within the top 10 results and reduce lookup times from an average of 6 to 15 minutes down to merely 18 seconds. These improvements provide strong evidence that semantic retrieval technologies can be effectively applied within strict aviation compliance environments, thereby significantly lowering operational workload and enhancing efficiency in real-world multilingual MRO workflows. <div>
arXiv:2511.15383v1 Announce Type: new 
Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEPO: Dual-Efficiency Preference Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2511.15392</link>
<guid>https://arxiv.org/abs/2511.15392</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, dual-efficiency, chain of thought, DEPO, token and step optimization<br /><br />Summary: Recent developments in large language models (LLMs) have enhanced their capabilities in reasoning and decision-making when used as agents. However, improved reasoning often results in longer chains of thought (CoT), which reduces interaction efficiency in practical applications. To address this, the authors introduce the concept of dual-efficiency, defined as (i) step-level efficiency, focusing on minimizing the number of tokens per step, and (ii) trajectory-level efficiency, aiming to reduce the total number of steps required to complete a task. Using this framework, they propose DEPO (dual-efficiency preference optimization), a method that jointly optimizes for both succinct responses and fewer action steps. Experiments conducted on WebShop and BabyAI benchmarks demonstrate that DEPO can reduce token usage by up to 60.9% and step counts by up to 26.9%, while simultaneously improving overall task performance by as much as 29.3%. Furthermore, DEPO shows strong generalization capabilities across three out-of-domain math benchmarks and maintains its efficiency advantages even when trained on just 25% of the available data. These results indicate that DEPO offers a scalable and effective solution for enhancing the efficiency and performance of LLM agents in real-world scenarios. <div>
arXiv:2511.15392v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework</title>
<link>https://arxiv.org/abs/2511.15408</link>
<guid>https://arxiv.org/abs/2511.15408</guid>
<content:encoded><![CDATA[
<div> Keywords: Creative Natural Language Generation, Large Language Models, Chinese baby naming, multi-agent optimization, aesthetics  

<br /><br />Summary: This paper addresses two main challenges in Creative Natural Language Generation (CNLG) with Large Language Models (LLMs): multi-objective flexibility and interpretive complexity. Existing methods struggle to fulfill personalized, fine-grained, and pluralistic user requirements while also understanding and interpreting implicit meanings to enhance creativity. The authors focus on Chinese baby naming as a representative short-form CNLG task that requires adherence to explicit constraints such as name length, semantics, and anthroponymy, along with generating meaningful aesthetic explanations. To overcome these difficulties, they propose NAMeGEn, a novel multi-agent optimization framework that iteratively cycles through objective extraction, name generation, and evaluation to satisfy diverse user requirements and produce accurate explanations. To support this task, the authors also develop a classical Chinese poetry corpus comprising over 17,000 poems aimed at improving the aesthetic quality of generated names. Additionally, they introduce CBNames, a new benchmark with specialized metrics tailored for this domain. Extensive experiments demonstrate that NAMeGEn outperforms six baseline models based on various LLM backbones without requiring any additional training. The framework successfully generates creative, personalized Chinese names with meaningful explanations, advancing the capabilities of CNLG in short-form text generation. <div>
arXiv:2511.15408v1 Announce Type: new 
Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Robust and Scalable Multilingual ASR for Indian Languages</title>
<link>https://arxiv.org/abs/2511.15418</link>
<guid>https://arxiv.org/abs/2511.15418</guid>
<content:encoded><![CDATA[
<div> Keywords: ASR adaptation, Multi-Decoder architecture, phonemic Common Label Set, multilingual systems, language and dialect identification<br /><br />Summary:<br /><br />This paper presents the ASR systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge focusing on language and dialect prediction across 8 languages and 33 dialects. The team participated in two tracks, Track 1 and Track 2, which limited external data usage and required the development of multilingual systems from scratch. A novel training approach was introduced, leveraging a Multi-Decoder architecture that utilizes a phonemic Common Label Set (CLS) as an intermediate representation to enhance model performance beyond the baseline in phonemic space. The authors also explored various techniques to preserve the improvement achieved in phonetic representation while converting outputs back to grapheme form, addressing challenges related to decoding and transcription accuracy. Their system outperformed the baseline in three languages under Track 2 concerning Word Error Rate (WER) and Character Error Rate (CER). Moreover, their approach yielded the highest accuracy in both language identification and dialect identification among all participant teams in Track 2, demonstrating the efficacy of their multi-decoder CLS-based method for multilingual ASR adaptation in constrained data settings. <div>
arXiv:2511.15418v1 Announce Type: new 
Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering</title>
<link>https://arxiv.org/abs/2511.15424</link>
<guid>https://arxiv.org/abs/2511.15424</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, text clustering, dynamic memory, dual-prompt strategy, end-to-end framework<br /><br />Summary:<br /><br />This paper introduces LLM-MemCluster, a novel framework that leverages Large Language Models (LLMs) to perform text clustering entirely within the LLM architecture, addressing key limitations of existing methods. Unlike traditional approaches that require complex external modules and pipelines, LLM-MemCluster is fully end-to-end and tuning-free. A central innovation is the incorporation of dynamic memory, which provides the model with stateful awareness, allowing iterative refinement during clustering. Additionally, the framework employs a dual-prompt strategy that enables the LLM to reason about and determine the appropriate number of clusters dynamically, tackling the challenge of managing cluster granularity. The proposed approach was evaluated on several benchmark datasets, demonstrating significant and consistent performance improvements over strong baseline methods. By unifying clustering into a truly LLM-native task, LLM-MemCluster offers an effective, interpretable, and streamlined paradigm for unsupervised text clustering driven by semantic understanding. This work suggests that memory-augmented and prompt-engineered LLMs hold great promise for advancing text clustering without reliance on external processing components. <div>
arXiv:2511.15424v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis</title>
<link>https://arxiv.org/abs/2511.15512</link>
<guid>https://arxiv.org/abs/2511.15512</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Processing Data Structure (LPDS), pelican nlp, reproducibility, linguistic data standardisation, Python package  

<br /><br />Summary:  
This article addresses the challenges in AI-based language processing, particularly the lack of standardisation in organizing and sharing linguistic data, as well as the absence of reproducible processing methodologies. To tackle these issues, the authors propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), which introduces a systematic folder structure and file naming conventions tailored for linguistic research. Additionally, they present pelican nlp, a modular and extensible Python package designed to facilitate streamlined language processing workflows. Pelican nlp handles various stages from initial data cleaning and task-specific preprocessing to advanced extraction of linguistic and acoustic features, including semantic embeddings and prosodic metrics. The entire processing pipeline can be configured through a single shareable configuration file, enabling reproducibility and ease of use. When executed on LPDS-formatted data, pelican nlp produces standardized outputs that include preprocessed language data, feature extraction results, and aggregated metrics. By combining LPDS and pelican nlp, the authors offer an end-to-end pipeline that promotes methodological transparency and enhances reproducibility in linguistic data analysis. This framework aims to set a foundation for future standardisation efforts in language processing research. <div>
arXiv:2511.15512v1 Announce Type: new 
Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Evaluation of Russian-language Architectures</title>
<link>https://arxiv.org/abs/2511.15552</link>
<guid>https://arxiv.org/abs/2511.15552</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, Russian language, evaluation benchmark, multimodal abilities, Mera Multi<br /><br />Summary:<br /><br />1. The paper introduces Mera Multi, an open multimodal evaluation framework specifically designed for Russian-speaking large language models, addressing the lack of such benchmarks for the Russian language.  
2. Mera Multi is instruction-based and covers multiple modalities including text, image, audio, and video, consisting of 18 newly constructed tasks tailored to evaluate both general-purpose and modality-specific architectures like image-to-text, video-to-text, and audio-to-text.  
3. The authors propose a universal taxonomy of multimodal abilities, providing a structured approach to assess diverse multimodal skills within models.  
4. All 18 datasets were created from scratch with careful attention to Russian cultural and linguistic peculiarities. They use unified prompts and standardized metrics to ensure consistency across tasks.  
5. Baseline results are presented for a variety of closed-source and open-source models. The methodology also includes mechanisms for preventing benchmark leakage, such as watermarking and licensing for private dataset subsets, ensuring the benchmark's integrity and replicability.  
6. While focused on Russian, the benchmark's methodology is designed to be replicable and adaptable to other typologically diverse languages, especially within the Slavic language family. <div>
arXiv:2511.15552v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning</title>
<link>https://arxiv.org/abs/2511.15574</link>
<guid>https://arxiv.org/abs/2511.15574</guid>
<content:encoded><![CDATA[
<div> Keywords: language acquisition, LLMs, HSKBenchmark, Chinese SLA, curriculum-tuning

<br /><br />Summary: 
Language acquisition is essential for understanding human language intelligence and improving the interpretability of large language models (LLMs). However, ethical and practical constraints limit experiments with human learners' language inputs, especially in Chinese second language acquisition (SLA). To address this, the paper introduces HSKBenchmark, a comprehensive benchmark designed for staged modeling and writing assessment of LLMs focused on Chinese SLA. It spans HSK levels 3 to 6, featuring authentic textbooks with 6.76 million tokens, 16,000 synthetic instruction samples, 30 test topics, and a robust evaluation system. The authors propose a curriculum-tuning framework to simulate human learning trajectories, facilitating training from beginner to advanced levels. An evaluation system assesses level-based grammar coverage, writing errors, lexical and syntactic complexity, and overall scoring. Furthermore, HSKAgent is developed, fine-tuned on 10,000 learner compositions. Experimental results indicate that HSKBenchmark effectively models Chinese SLA and serves as a reliable dynamic writing assessment tool, with fine-tuned LLMs performing comparably to advanced human learners and displaying human-like acquisition traits. The resources provided, including HSKBenchmark, HSKAgent, and checkpoints, aim to support future research on language acquisition modeling and LLMs interpretability. Code and data are publicly accessible at: https://github.com/CharlesYang030/HSKB. <div>
arXiv:2511.15574v1 Announce Type: new 
Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenisation over Bounded Alphabets is Hard</title>
<link>https://arxiv.org/abs/2511.15709</link>
<guid>https://arxiv.org/abs/2511.15709</guid>
<content:encoded><![CDATA[
<div> Tokenisation, NP-complete, bounded alphabets, approximation, computational complexity<br /><br />Summary:<br /><br />This article investigates the computational complexity of tokenisation when applied over bounded alphabets, addressing a limitation in previous work that considered only unboundedly large alphabets. The authors study two main variants of tokenisation: bottom-up tokenisation, which involves selecting a sequence of merge operations, and direct tokenisation, which requires choosing a vocabulary to compress a dataset optimally. It is established that hardness results proven for an n-ary alphabet automatically extend to alphabets of any larger size. Crucially, the paper proves that both tokenisation variants remain NP-complete even when restricted to binary alphabets and that no polynomial-time approximation scheme exists for these cases unless P=NP. Furthermore, the direct tokenisation problem is shown to be NP-complete even on unary alphabets, reinforcing that the intractability arises from fundamental computational barriers rather than from large or complex alphabets. These findings help explain why popular practical tokenisers like BPE and UnigramLM are heuristic methods rather than exact algorithms. The study ultimately highlights the significance of developing approximation algorithms as a promising avenue for advancing tokenisation research. <div>
arXiv:2511.15709v1 Announce Type: new 
Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Agricultural Research: A RAG-Based Approach to Mycorrhizal Fungi Information</title>
<link>https://arxiv.org/abs/2511.14765</link>
<guid>https://arxiv.org/abs/2511.14765</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, arbuscular mycorrhizal fungi, sustainable agriculture, vector embeddings, experimental metadata<br /><br />Summary:<br /><br />1. The study introduces a Retrieval-Augmented Generation (RAG) system integrating neural information retrieval with generative language modeling to improve contextual accuracy and factual reliability in natural language processing applications. 2. Unlike traditional Large Language Models limited by static training data, this system dynamically incorporates external domain-specific knowledge, overcoming temporal and disciplinary constraints. 3. The RAG-enabled framework targets the Mycophyto domain, focusing on arbuscular mycorrhizal fungi (AMF), which are crucial for sustainable agriculture by enhancing nutrient uptake, plant stress resilience, and soil health. 4. The system employs a dual-layered strategy combining semantic retrieval of agronomy and biotechnology literature through vector embeddings and structured extraction of experimental metadata like inoculation methods, spore densities, soil parameters, and yield outcomes. 5. High-performance vector databases facilitate scalable, near real-time retrieval of evolving scientific literature. 6. Empirical evaluation confirms the pipeline’s ability to retrieve and synthesize relevant information on AMF interactions with crops such as tomato (Solanum lycopersicum). 7. The framework demonstrates AI-driven knowledge discovery’s potential to accelerate agroecological innovations and support informed decision-making in sustainable farming systems. <div>
arXiv:2511.14765v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) represents a transformative approach within natural language processing (NLP), combining neural information retrieval with generative language modeling to enhance both contextual accuracy and factual reliability of responses. Unlike conventional Large Language Models (LLMs), which are constrained by static training corpora, RAG-powered systems dynamically integrate domain-specific external knowledge sources, thereby overcoming temporal and disciplinary limitations. In this study, we present the design and evaluation of a RAG-enabled system tailored for Mycophyto, with a focus on advancing agricultural applications related to arbuscular mycorrhizal fungi (AMF). These fungi play a critical role in sustainable agriculture by enhancing nutrient acquisition, improving plant resilience under abiotic and biotic stresses, and contributing to soil health. Our system operationalizes a dual-layered strategy: (i) semantic retrieval and augmentation of domain-specific content from agronomy and biotechnology corpora using vector embeddings, and (ii) structured data extraction to capture predefined experimental metadata such as inoculation methods, spore densities, soil parameters, and yield outcomes. This hybrid approach ensures that generated responses are not only semantically aligned but also supported by structured experimental evidence. To support scalability, embeddings are stored in a high-performance vector database, allowing near real-time retrieval from an evolving literature base. Empirical evaluation demonstrates that the proposed pipeline retrieves and synthesizes highly relevant information regarding AMF interactions with crop systems, such as tomato (Solanum lycopersicum). The framework underscores the potential of AI-driven knowledge discovery to accelerate agroecological innovation and enhance decision-making in sustainable farming systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications</title>
<link>https://arxiv.org/abs/2511.14769</link>
<guid>https://arxiv.org/abs/2511.14769</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, adaptive retrieval, clustering, query-document similarity, large language models<br /><br />Summary:<br /><br />This paper addresses the challenge of optimizing document retrieval in Retrieval-Augmented Generation (RAG) systems, which enhance large language models by incorporating relevant external documents. The authors highlight that static top-k retrieval approaches do not account for the variability in query scope—focused queries need fewer documents, while broader queries require more extensive context. To solve this, they propose Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the ideal number of documents to retrieve by analyzing clustering patterns of query-document similarity scores. CAR identifies a transition point where highly relevant documents cluster tightly before similarity sharply decreases, thus adaptively setting a cut-off tailored to the query’s complexity. Evaluations on Coinbase's CDP corpus and MultiHop-RAG benchmarks show CAR consistently selects optimal retrieval sizes and achieves superior TES scores compared to fixed top-k baselines. In downstream RAG tasks, CAR reduces large language model token consumption by 60%, cuts end-to-end latency by 22%, and lowers hallucination rates by 10%, all while maintaining answer relevance. The integration of CAR into Coinbase's virtual assistant has resulted in a 200% increase in user engagement, demonstrating its practical impact in real-world applications. <div>
arXiv:2511.14769v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by pulling in external material, document, code, manuals, from vast and ever-growing corpora, to effectively answer user queries. The effectiveness of RAG depends significantly on aligning the number of retrieved documents with query characteristics: narrowly focused queries typically require fewer, highly relevant documents, whereas broader or ambiguous queries benefit from retrieving more extensive supporting information. However, the common static top-k retrieval approach fails to adapt to this variability, resulting in either insufficient context from too few documents or redundant information from too many. Motivated by these challenges, we introduce Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the optimal number of documents by analyzing the clustering patterns of ordered query-document similarity distances. CAR detects the transition point within similarity distances, where tightly clustered, highly relevant documents shift toward less pertinent candidates, establishing an adaptive cut-off that scales with query complexity. On Coinbase's CDP corpus and the public MultiHop-RAG benchmark, CAR consistently picks the optimal retrieval depth and achieves the highest TES score, outperforming every fixed top-k baseline. In downstream RAG evaluations, CAR cuts LLM token usage by 60%, trims end-to-end latency by 22%, and reduces hallucinations by 10% while fully preserving answer relevance. Since integrating CAR into Coinbase's virtual assistant, we've seen user engagement jump by 200%.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization</title>
<link>https://arxiv.org/abs/2511.14846</link>
<guid>https://arxiv.org/abs/2511.14846</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, Multi-turn Reasoning, Policy Optimization, Reward Shaping  

<br /><br />Summary: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) is challenging due to the limitations of existing reinforcement learning (RL) methods that use coarse-grained, trajectory-level rewards. These coarse rewards provide insufficient learning signals for complex, multi-turn reasoning, often causing training stagnation. To overcome this, the authors propose Group Turn Policy Optimization (GTPO), a novel RL algorithm designed for multi-turn TIR tasks. GTPO introduces three main innovations: first, turn-level reward assignment, which offers fine-grained feedback specifically tailored to each interaction turn; second, return-based advantage estimation, calculating normalized discounted returns as advantages to improve learning signal quality; and third, self-supervised reward shaping, which utilizes self-supervision from generated code to enrich sparse binary outcome rewards. The authors evaluate GTPO comprehensively across various reasoning benchmarks, demonstrating that it improves performance by 3.0% on average compared to the previous state-of-the-art method, Group Relative Policy Optimization (GRPO). These results highlight GTPO’s effectiveness in enhancing LLM training for complex mathematical and tool-integrated reasoning tasks, potentially advancing their applicability in real-world multi-step problem-solving scenarios. <div>
arXiv:2511.14846v1 Announce Type: cross 
Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis</title>
<link>https://arxiv.org/abs/2511.14900</link>
<guid>https://arxiv.org/abs/2511.14900</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, dermatological diagnosis, diagnostic rationale, reinforcement learning, supervised fine-tuning

<br /><br />Summary:  
The article addresses key limitations in vision-language models (VLMs) applied to dermatological diagnosis, particularly challenges related to data heterogeneity, lack of grounded diagnostic rationales, and limited model scalability and generalization. To overcome these issues, the authors propose SkinR1, a novel dermatological VLM that integrates deep, textbook-based reasoning with reinforcement learning (RL) to enhance diagnostic performance and reliability. First, SkinR1 includes a textbook-based reasoning generator that creates high-quality, hierarchy-aware, and differential-diagnosis-informed trajectories, providing expert-level reasoning supervision. Second, these generated trajectories are used to fine-tune the model via supervised fine-tuning (SFT), grounding the model’s diagnostic reasoning capabilities. Third, the approach incorporates a novel RL paradigm that leverages the hierarchical structure of diseases to help the model transfer learned reasoning patterns from small, densely annotated datasets to large, sparsely annotated ones, improving generalization. Extensive experiments across multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy compared to existing methods. An ablation study further confirms the critical role of supervised fine-tuning in establishing a strong reasoning foundation within the model. This comprehensive framework addresses the major shortcomings of prior models, advancing the clinical utility and trustworthiness of dermatological VLMs. <div>
arXiv:2511.14900v1 Announce Type: cross 
Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding</title>
<link>https://arxiv.org/abs/2511.14936</link>
<guid>https://arxiv.org/abs/2511.14936</guid>
<content:encoded><![CDATA[
<div> Differential Privacy, Clinical NLP, Knowledge Distillation, Diagnostic Coding, Privacy-Utility Trade-off<br /><br />Summary:  
1. Large language models trained on clinical text pose risks of exposing sensitive patient information.  
2. Differential privacy (DP) methods, while enhancing privacy, often significantly reduce the diagnostic accuracy of models, limiting their practical deployment in clinical settings.  
3. This study provides the first systematic, head-to-head comparison of four different training pipelines aimed at automated diagnostic coding from hospital discharge summaries, all using identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes.  
4. Tested at moderate and relaxed privacy budgets (ε values of 4 and 6), knowledge distillation from DP-trained teacher models outperforms both direct DP-SGD training and DP-synthetic data training.  
5. Knowledge distillation recovers up to 63% of the non-private model’s performance while maintaining strong empirical privacy guarantees, as indicated by membership-inference AUC values near 0.5.  
6. These results reveal significant differences in the privacy-utility trade-offs of different architectures and establish knowledge distillation as the most effective and practical method for privacy-preserving clinical NLP applications. <div>
arXiv:2511.14936v1 Announce Type: cross 
Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Multimodal Large Language Models on Vertically Written Japanese Text</title>
<link>https://arxiv.org/abs/2511.15059</link>
<guid>https://arxiv.org/abs/2511.15059</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Japanese OCR, vertical writing, synthetic dataset, visual document understanding<br /><br />Summary:<br /><br />This study addresses the challenge of reading vertically written Japanese text using Multimodal Large Language Models (MLLMs), which have been advancing rapidly for visual document understanding. Recognizing vertically oriented Japanese text is critical since many Japanese documents use this writing style, yet current research on this aspect is limited. To evaluate and improve reading capabilities, the authors generated a synthetic Japanese OCR dataset with both horizontal and vertical text renderings. This dataset was utilized for fine-tuning the models and served as a benchmark for evaluation. Additionally, a real-world evaluation dataset containing vertically written Japanese documents was compiled. The findings reveal that existing MLLMs underperform on vertically written Japanese text compared to horizontal text. Importantly, fine-tuning models on the synthesized OCR dataset led to substantial improvements in their ability to process vertical writing, overcoming prior limitations. The work contributes valuable datasets and code to the public domain, facilitating further research and development in Japanese document image understanding and vertical text recognition using MLLMs. <div>
arXiv:2511.15059v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression</title>
<link>https://arxiv.org/abs/2511.15069</link>
<guid>https://arxiv.org/abs/2511.15069</guid>
<content:encoded><![CDATA[
<div> ProRAC, Reasoning about Actions and Change, neuro-symbolic, large language models, benchmarks<br /><br />Summary: ProRAC (Progression-based Reasoning about Actions and Change) is a novel neuro-symbolic framework designed to address reasoning about actions and change (RAC) problems by integrating large language models (LLMs). The framework first extracts essential RAC components such as actions and questions from the input problem, enabling it to understand the key elements involved. ProRAC then progressively executes each identified action step-by-step to compute the final state of the system, effectively simulating the progression of changes over time. Once the progressed final state is derived, the framework evaluates the posed query against this state to generate an accurate answer. The approach was tested on multiple RAC benchmarks spanning different domains and types of tasks, demonstrating robust and strong performance regardless of the LLM backbone employed. This indicates the framework’s flexibility and generalizability across various reasoning scenarios involving actions and changes. Overall, ProRAC leverages the reasoning strengths of both symbolic progression and neural language models to advance the solving of complex RAC problems successfully. <div>
arXiv:2511.15069v1 Announce Type: cross 
Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents</title>
<link>https://arxiv.org/abs/2511.15074</link>
<guid>https://arxiv.org/abs/2511.15074</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, feature engineering, multi-agent system, knowledge integration, automatic feature extraction  

<br /><br />Summary:  
This paper presents Rogue One, an innovative framework leveraging Large Language Models (LLMs) for automatic feature extraction in tabular data tasks. The framework consists of three specialized agents—Scientist, Extractor, and Tester—that work collaboratively and iteratively to discover, generate, and validate predictive features. Rogue One addresses limitations in prior AutoFE approaches by employing a decentralized multi-agent architecture rather than a monolithic LLM, enhancing flexibility and specialization. It introduces a sophisticated qualitative feedback mechanism that goes beyond simple accuracy metrics, enabling a more nuanced assessment of feature utility. The flooding-pruning strategy further optimizes the balance between feature exploration and exploitation, preventing overfitting and promoting meaningful feature discovery. A key innovation is the integration of external domain knowledge through a retrieval-augmented generation (RAG) system, which empowers the model to produce semantically meaningful and interpretable features. The framework’s efficacy is demonstrated on 19 classification and 9 regression datasets, where it significantly outperforms state-of-the-art baselines. Additionally, Rogue One shows promise as a scientific discovery tool by generating novel hypotheses, such as identifying a potential new biomarker in cardiovascular data, highlighting its contribution beyond predictive performance to practical interpretability and domain relevance. <div>
arXiv:2511.15074v1 Announce Type: cross 
Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries</title>
<link>https://arxiv.org/abs/2511.15131</link>
<guid>https://arxiv.org/abs/2511.15131</guid>
<content:encoded><![CDATA[
<div> Keywords: CASTELLA, audio moment retrieval, benchmark dataset, human-annotated, model performance<br /><br />Summary:<br /><br />1. CASTELLA is introduced as a new human-annotated audio benchmark specifically designed for the task of audio moment retrieval (AMR), addressing the lack of real-world data and established benchmarks in this area.<br />2. Prior AMR research mainly relied on synthetic datasets for training, with evaluations conducted on datasets containing fewer than 100 samples, leading to less reliable performance results.<br />3. CASTELLA significantly expands the scale, consisting of 1,009 training, 213 validation, and 640 test audio recordings, making it 24 times larger than previous datasets.<br />4. The authors establish a baseline AMR model using CASTELLA and demonstrate that fine-tuning a pre-trained model (initially trained on synthetic data) on CASTELLA improves performance by 10.4 points in Recall1@0.7 compared to training solely on synthetic data.<br />5. The dataset is publicly accessible for research and application development at https://h-munakata.github.io/CASTELLA-demo/, promoting more reliable AMR system evaluations and real-world application readiness. <div>
arXiv:2511.15131v1 Announce Type: cross 
Abstract: We introduce CASTELLA, a human-annotated audio benchmark for the task of audio moment retrieval (AMR). Although AMR has various useful potential applications, there is still no established benchmark with real-world data. The early study of AMR trained the model with solely synthetic datasets. Moreover, the evaluation is based on annotated dataset of fewer than 100 samples. This resulted in less reliable reported performance. To ensure performance for applications in real-world environments, we present CASTELLA, a large-scale manually annotated AMR dataset. CASTELLA consists of 1,009, 213, and 640 audio recordings for train, valid, and test split, respectively, which is 24 times larger than the previous dataset. We also establish a baseline model for AMR using CASTELLA. Our experiments demonstrate that a model fine-tuned on CASTELLA after pre-training on the synthetic data outperformed a model trained solely on the synthetic data by 10.4 points in Recall1@0.7. CASTELLA is publicly available in https://h-munakata.github.io/CASTELLA-demo/.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation</title>
<link>https://arxiv.org/abs/2511.15159</link>
<guid>https://arxiv.org/abs/2511.15159</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical training, feedback generation, Instrument-Action-Target, video analysis, GPT-4o<br /><br />Summary: High-quality intraoperative feedback from surgical trainers is crucial for improving trainee performance and skill acquisition. This work introduces a structure-aware pipeline that learns a surgical action ontology from 33 real trainer-to-trainee surgical transcripts to condition feedback generation. The authors first mine Instrument-Action-Target (IAT) triplets from real-world feedback texts and cluster different expressions into normalized categories to standardize representations. Next, they fine-tune a video-to-IAT recognition model that incorporates surgical procedure context, task context, and detailed temporal instrument motion for better accuracy. Results show improved Area Under the Curve (AUC) for Instrument, Action, and Tissue recognition tasks, with notable performance gains from injecting context and temporal tracking. For feedback text generation evaluated on a fidelity rubric, GPT-4o from video alone scores 2.17 out of 5, while conditioning on IAT triplets improves the score to 2.44 (+12.4%) and doubles the portion of admissible (score ≥3) feedback from 21% to 42%. Traditional text-similarity metrics also improve, with word error rate decreasing by 15-31% and ROUGE scores increasing by 9-64%. Grounding feedback generation in explicit IAT structures enhances fidelity, provides clinician-verifiable rationales, and supports auditable use in surgical training environments. <div>
arXiv:2511.15159v1 Announce Type: cross 
Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M, Toolchain and Language for Reusable Model Compilation</title>
<link>https://arxiv.org/abs/2511.15257</link>
<guid>https://arxiv.org/abs/2511.15257</guid>
<content:encoded><![CDATA[
<div> Multi-target compilation, actor model, discrete-event scheduling, model-driven engineering, system modeling<br /><br />Summary:<br /><br />1. The paper addresses the challenge of efficiently and safely developing complex software-driven systems that combine distributed concurrent computations with physical environmental interactions. 2. It highlights the need for deriving multiple specialized models—such as those for simulation, deployment, and formal verification—from a single high-level system model, noting that each target model usually requires distinct formalisms and platforms. 3. Traditional compilers translate programs into executable code, whereas model compilers aim to generate multiple heterogeneous target artifacts from a single source model, a capability often missing in existing modeling languages designed with narrow targets. 4. The authors introduce M, a new textual, grammar-driven modeling language and toolchain based on the actor model extended with discrete-event scheduling to represent system entities, message interactions, and time- or state-triggered behaviors. 5. M supports multi-target compilation by preserving semantic conformance across generated targets and can serve as a middle language for anchoring other modeling languages to leverage its compilation framework, advancing model-driven engineering for complex, concurrent, and time-aware systems. <div>
arXiv:2511.15257v1 Announce Type: cross 
Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChartEditor: A Reinforcement Learning Framework for Robust Chart Editing</title>
<link>https://arxiv.org/abs/2511.15266</link>
<guid>https://arxiv.org/abs/2511.15266</guid>
<content:encoded><![CDATA[
<div> Keywords: Chart Editing, Benchmark, ChartEditVista, Evaluation Metrics, Reinforcement Learning<br /><br />Summary: This work addresses limitations in existing chart editing benchmarks, which often suffer from limited data diversity and unrealistic assumptions such as access to original chart code. To overcome these challenges, the authors introduce ChartEditVista, a large-scale, comprehensive benchmark dataset containing 7,964 samples across 31 chart categories. ChartEditVista uniquely provides only the original chart image and natural language editing instructions, excluding the original chart code, thereby better reflecting real-world scenarios. The benchmark covers a broad range of editable chart elements and editing instructions, generated through a fully automated pipeline that creates, modifies, and verifies charts to ensure high-quality data. In addition, the study proposes two novel fine-grained, rule-based evaluation metrics: the layout metric, which assesses graphical component positions, sizes, and colors, and the text metric, which evaluates textual content along with font styling. Building on this benchmark, the authors develop ChartEditor, a model trained with reinforcement learning that incorporates a novel rendering reward to maintain both code executability and visual fidelity simultaneously. Extensive experiments and human evaluations demonstrate that ChartEditVista provides robust evaluation capabilities, and that ChartEditor consistently outperforms models of comparable or larger scale on chart editing tasks. <div>
arXiv:2511.15266v1 Announce Type: cross 
Abstract: Chart editing reduces manual effort in visualization design. Typical benchmarks limited in data diversity and assume access to complete chart code, which is seldom in real-world scenarios. To address this gap, we present ChartEditVista, a comprehensive benchmark consisting of 7,964 samples spanning 31 chart categories. It encompasses diverse editing instructions and covers nearly all editable chart elements. The inputs in ChartEditVista include only the original chart image and natural language editing instructions, without the original chart codes. ChartEditVista is generated through a fully automated pipeline that produces, edits, and verifies charts, ensuring high-quality chart editing data. Besides, we introduce two novel fine-grained, rule-based evaluation metrics: the layout metric, which evaluates the position, size and color of graphical components; and the text metric, which jointly assesses textual content and font styling. Building on top of ChartEditVista, we present ChartEditor, a model trained using a reinforcement learning framework that incorporates a novel rendering reward to simultaneously enforce code executability and visual fidelity. Through extensive experiments and human evaluations, we demonstrate that ChartEditVista provides a robust evaluation, while ChartEditor consistently outperforms models with similar-scale and larger-scale on chart editing tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs</title>
<link>https://arxiv.org/abs/2511.15323</link>
<guid>https://arxiv.org/abs/2511.15323</guid>
<content:encoded><![CDATA[
<div> Hardware synthesis, high-level synthesis, e-graph, scheduling optimization, FPGA acceleration

<br /><br />Summary:  
1. Current hardware synthesis methods, including advanced high-level synthesis (HLS) tools, suffer from sequential and separate handling of implementation selection and scheduling, leading to inefficient designs that underutilize FPGA heterogeneous architectures.  
2. Implementation selection is performed by heuristic pattern matching that ignores the effects on scheduling, while scheduling is done on these fixed selections using inaccurate delay estimates, missing optimization opportunities for FPGA features like DSP blocks.  
3. SkyEgg is introduced as a novel framework that jointly optimizes implementation selection and scheduling using the e-graph data structure, where both algebraic transformations and hardware implementation options are uniformly represented as rewrite rules to explore the complete design space.  
4. The process involves constructing an e-graph from the input program, applying algebraic and implementation rewrite rules via equality saturation, and then formulating a mixed-integer linear programming (MILP) problem that optimizes jointly over the saturated e-graph.  
5. SkyEgg supports both exact MILP solving and a scalable ASAP heuristic for larger designs, and evaluation on benchmarks targeting Xilinx Kintex UltraScale+ FPGAs shows an average speedup of 3.01x over Vitis HLS, with gains up to 5.22x on complex expressions. <div>
arXiv:2511.15323v1 Announce Type: cross 
Abstract: Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.
  We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search</title>
<link>https://arxiv.org/abs/2511.15443</link>
<guid>https://arxiv.org/abs/2511.15443</guid>
<content:encoded><![CDATA[
<div> Keywords: Dense retrieval, filter bubble, CroPS, Hierarchical Label Assignment, short-video search<br /><br />Summary: This paper addresses the filter bubble problem in dense retrieval systems, particularly in short-video platforms, where training on historical user interactions limits diversity by excluding unseen but relevant content. To mitigate this, the authors propose CroPS (Cross-Perspective Positive Samples), a novel data engine that introduces diverse positive training samples drawn from multiple perspectives: user query reformulation behavior (query-level), engagement data from recommendation streams (system-level), and synthesized knowledge from large language models (knowledge-level). To effectively leverage these heterogeneous signals, they design a Hierarchical Label Assignment (HLA) strategy coupled with a new H-InfoNCE loss function, enabling fine-grained and relevance-aware optimization during training. The approach is validated through extensive experiments on Kuaishou Search, a large-scale commercial short-video search platform, demonstrating significant performance improvements in both offline benchmarks and live A/B testing scenarios. Deployment of CroPS at Kuaishou Search serves hundreds of millions of users daily, achieving superior retrieval quality and notably reducing query reformulations, thus enhancing user experience and retrieval accuracy in an industrial environment. <div>
arXiv:2511.15443v1 Announce Type: cross 
Abstract: Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer-Use Agents as Judges for Generative User Interface</title>
<link>https://arxiv.org/abs/2511.15567</link>
<guid>https://arxiv.org/abs/2511.15567</guid>
<content:encoded><![CDATA[
<div> Computer-Use Agents, Automatic GUI Design, Coder-CUA Collaboration, AUI-Gym Benchmark, Task Solvability<br /><br />Summary:<br /><br />This paper addresses the challenge of designing Graphical User Interfaces (GUIs) optimized not just for humans but for Computer-Use Agents (CUA), which autonomously operate digital environments. The authors highlight how current GUIs prioritize aesthetics and human usability, potentially hindering efficient agent operation. To explore better agent-native interface design, they introduce AUI-Gym, a comprehensive benchmark encompassing 52 applications from various domains, with 1560 synthesized tasks created using language models to simulate real-world scenarios. They implement a verifier to programmatically confirm task executability within each environment, ensuring reliability. The core contribution is a novel Coder-CUA collaboration framework where a Coder acts as a designer generating and revising GUIs, while the CUA serves as a judge evaluating functionality and guiding iterative improvements. Unlike traditional design metrics, success here is measured by the solvability of tasks and the agent’s navigation success rate within the GUI. Additionally, the authors develop a CUA Dashboard that distills complex navigation histories into interpretable visual summaries, providing actionable feedback for redesign. This framework promotes agent-native efficiency and shifts agents from passive users to active participants in digital environments. The authors also publicly release their code and dataset to facilitate further research. <div>
arXiv:2511.15567v1 Announce Type: cross 
Abstract: Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.15605</link>
<guid>https://arxiv.org/abs/2511.15605</guid>
<content:encoded><![CDATA[
<div> Vision-Language-Action, Reinforcement Learning, Reward Sparsity, Latent World Representations, Self-Referential Policy Optimization<br /><br />Summary:<br /><br />This paper addresses the limitations of Vision-Language-Action (VLA) models in robotic manipulation, particularly their dependence on expert demonstrations that cause bias and restrict performance. To improve learning efficiency, the authors propose a novel Reinforcement Learning (RL) framework called Self-Referential Policy Optimization (SRPO). Unlike existing VLA-RL methods that suffer from reward sparsity by using binary success signals, SRPO leverages the model’s own successful trajectories generated during training as references to assign progress-based rewards to failed attempts. This approach eliminates the need for external demonstrations or manual reward engineering. A key innovation is the use of latent world representations from a world model’s compressed, transferable encodings to robustly measure behavioral progress across diverse environments, avoiding reliance on raw pixels or domain-specific tuning. Evaluations on the LIBERO benchmark show SRPO's superior efficiency and effectiveness, boosting success rates from a supervised baseline of 48.9% to an impressive 99.2% within only 200 RL steps, marking a 103% relative improvement without extra supervision. The method also demonstrates strong robustness, achieving a 167% performance increase on the more challenging LIBERO-Plus benchmark. Overall, SRPO offers a scalable and generalizable approach to overcoming reward sparsity in VLA robotic learning tasks. <div>
arXiv:2511.15605v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Think and When to Look: Uncertainty-Guided Lookback</title>
<link>https://arxiv.org/abs/2511.15613</link>
<guid>https://arxiv.org/abs/2511.15613</guid>
<content:encoded><![CDATA[
<div> Test-time thinking, visual reasoning, LVLMs, uncertainty guided lookback, decoding strategy<br /><br />Summary:<br /><br />1. The paper investigates the impact of test-time thinking—generating explicit intermediate reasoning chains—on large vision language models (LVLMs), offering the first systematic large-scale analysis of how such reasoning affects visual tasks.  
2. Ten LVLM variants from the InternVL3.5 and Qwen3-VL families are evaluated on the MMMU-val benchmark, with generous token budgets and multi-pass decoding strategies.  
3. Results reveal that longer reasoning chains do not necessarily improve performance; extended chains often lead to incorrect reasoning paths that neglect image content and underperform compared to standard instruction-following modes.  
4. A key insight is that short lookback phrases explicitly referencing the image correlate strongly with better visual grounding and successful reasoning trajectories.  
5. Leveraging this, the authors introduce an uncertainty guided lookback decoding strategy that is training-free and combines uncertainty signals with adaptive lookback prompts and breadth search to improve reasoning.  
6. This method significantly improves MMMU benchmark performance, especially in areas where conventional thinking struggles, outperforming several strong decoding baselines and setting a new state of the art under fixed model and token constraints.  
7. The decoding approach generalizes well, yielding consistent gains across five additional benchmarks, including broad multimodal datasets and math-focused visual reasoning tasks. <div>
arXiv:2511.15613v1 Announce Type: cross 
Abstract: Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisPlay: Self-Evolving Vision-Language Models from Images</title>
<link>https://arxiv.org/abs/2511.15661</link>
<guid>https://arxiv.org/abs/2511.15661</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Vision-Language Models, self-evolving framework, visual reasoning, Group Relative Policy Optimization (GRPO)  

<br /><br />Summary:  
This paper introduces VisPlay, a novel self-evolving reinforcement learning (RL) framework designed to enhance Vision-Language Models (VLMs) on complex visual reasoning tasks without requiring human-annotated labels or task-specific heuristics. VisPlay starts with a single base VLM and divides it into two interactive roles: an Image-Conditioned Questioner that generates challenging yet answerable visual questions, and a Multimodal Reasoner that produces silver (pseudo) answers. Both components are trained jointly via Group Relative Policy Optimization (GRPO), a technique that incentivizes diversity and appropriate difficulty in generated questions while maintaining high-quality silver responses. This approach enables autonomous improvement of reasoning abilities by leveraging large-scale unlabeled image data. VisPlay demonstrates scalability by working efficiently with multiple VLM architectures, specifically Qwen2.5-VL and MiMo-VL models. Experimental results show consistent gains in visual reasoning skills, compositional generalization, and hallucination reduction across eight different benchmarks, including MM-Vet and MMMU. Overall, VisPlay provides a scalable, cost-effective path towards self-improving multimodal intelligence, reducing dependency on expensive annotations and static reward functions. The project is publicly available for community use and further research at https://bruno686.github.io/VisPlay/. <div>
arXiv:2511.15661v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping</title>
<link>https://arxiv.org/abs/2511.15690</link>
<guid>https://arxiv.org/abs/2511.15690</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, Multimodal large language models, Expert skipping, Inference efficiency, Dual-modality thresholding<br /><br />Summary:  
This paper addresses the computational inefficiency of Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) in vision-language tasks. Existing expert skipping methods designed for unimodal large language models are found ineffective for MLLMs, causing significant performance degradation due to ignoring heterogeneous expert contributions and modality-specific token behaviors. To solve this, the authors propose MoDES, a novel training-free framework that adaptively skips experts during inference to balance efficiency and accuracy in MoE MLLMs. MoDES features a globally-modulated local gating (GMLG) mechanism, which incorporates global layer-wise importance into local routing to better estimate per-token expert importance. Additionally, it uses dual-modality thresholding (DMT) to handle different modalities separately for expert skipping decisions. To optimize the skipping thresholds efficiently, a frontier search algorithm exploiting monotonicity properties is introduced, reducing convergence time dramatically from days to hours. Extensive experiments on three model series across thirteen benchmarks demonstrate that MoDES significantly outperforms prior methods. For example, skipping 88% of experts in Qwen3-VL-MoE-30B-A3B-Instruct yields a performance improvement from 86.66% to 97.33%. Moreover, MoDES accelerates inference by increasing prefilling speed by 2.16× and decoding speed by 1.26×, thereby enhancing both model efficiency and accuracy. <div>
arXiv:2511.15690v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Visually, Reason Textually: Vision-Language Synergy in ARC</title>
<link>https://arxiv.org/abs/2511.15703</link>
<guid>https://arxiv.org/abs/2511.15703</guid>
<content:encoded><![CDATA[
<div> Keywords: abstract reasoning, ARC-AGI, vision-language synergy, modality-switch self-correction, foundation models<br /><br />Summary:<br /><br />1. Abstract reasoning from minimal examples remains a significant challenge for advanced foundation models like GPT-5 and Grok 4, as they struggle to infer structured transformation rules from few samples—an ability that humans excel at.  
2. The ARC-AGI dataset is designed as a rigorous benchmark to test conceptual rule induction and the transfer of learned rules to new tasks, emphasizing the need for both reasoning and abstraction.  
3. Existing approaches mostly treat ARC-AGI as a textual reasoning problem, ignoring the importance of visual abstraction, which humans heavily rely on to solve such puzzles.  
4. Initial experiments showed that simply converting ARC-AGI grids into images actually harmed performance, due to imprecise rule execution in vision-only models.  
5. The authors propose that vision and language serve complementary roles: vision aids in global pattern abstraction and verification, while language handles symbolic rule formulation and precise execution.  
6. Based on this hypothesis, two strategies were introduced: Vision-Language Synergy Reasoning (VLSR), which breaks ARC-AGI tasks into modality-specific subtasks, and Modality-Switch Self-Correction (MSSC), which uses vision to verify and correct text-based reasoning errors.  
7. Extensive testing demonstrated up to a 4.33% improvement over text-only models across various flagship architectures and ARC-AGI problems.  
8. The study highlights that integrating visual abstraction with linguistic reasoning is critical for developing foundation models capable of human-like, generalizable intelligence.  
9. The authors plan to release their source code soon to facilitate further research in this direction. <div>
arXiv:2511.15703v1 Announce Type: cross 
Abstract: Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MessIRve: A Large-Scale Spanish Information Retrieval Dataset</title>
<link>https://arxiv.org/abs/2409.05994</link>
<guid>https://arxiv.org/abs/2409.05994</guid>
<content:encoded><![CDATA[
<div> Spanish, Information Retrieval, Dataset, MessIRve, Autocomplete API  

<br /><br />Summary:  
The article presents MessIRve, a novel large-scale Spanish information retrieval (IR) dataset designed to address the scarcity of resources for Spanish IR tasks. The dataset consists of nearly 700,000 queries gathered from Google's autocomplete API and is paired with relevant documents from Wikipedia, ensuring a rich and authentic data source. Unlike other Spanish IR datasets that are either translated from English or lack consideration of dialectal variations, MessIRve captures queries from diverse Spanish-speaking regions, providing a realistic representation of language usage across different dialects. Its large size enables coverage of a wide array of topics, in contrast to smaller existing datasets that may have limited thematic scope. The paper includes a detailed description of the dataset construction process, comparative analyses with existing Spanish IR resources, and baseline evaluations using prominent IR models. Through these contributions, the work aims to advance research in Spanish IR and improve information retrieval tools to better serve Spanish speakers worldwide. <div>
arXiv:2409.05994v2 Announce Type: replace 
Abstract: Information retrieval (IR) is the task of finding relevant documents in response to a user query. Although Spanish is the second most spoken native language, there are few Spanish IR datasets, which limits the development of information access tools for Spanish speakers. We introduce MessIRve, a large-scale Spanish IR dataset with almost 700,000 queries from Google's autocomplete API and relevant documents sourced from Wikipedia. MessIRve's queries reflect diverse Spanish-speaking regions, unlike other datasets that are translated from English or do not consider dialectal variations. The large size of the dataset allows it to cover a wide variety of topics, unlike smaller datasets. We provide a comprehensive description of the dataset, comparisons with existing datasets, and baseline evaluations of prominent IR models. Our contributions aim to advance Spanish IR research and improve information access for Spanish speakers.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?</title>
<link>https://arxiv.org/abs/2411.04530</link>
<guid>https://arxiv.org/abs/2411.04530</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual language models, semantic tokens, subword embeddings, cross-lingual transfer, zero-shot learning<br /><br />Summary: This work investigates the extent to which current multilingual language models (mLMs) comprehend text through subword-level semantic concepts rather than merely through superficial token forms. The authors introduce "semantic tokens," which are created by merging semantically similar subwords and their embeddings, aiming to align the models more closely with meaningful language units. They evaluate the efficacy of these semantic tokens across five diverse multilingual downstream tasks, demonstrating that shared general semantics significantly aid mLMs with various tokenizers and model sizes in making accurate predictions. Analysis of the grouped subwords reveals a broad spectrum of semantic relationships, including synonyms and translations across many languages and scripts, underscoring the cross-lingual relevance of the semantic tokens. Additionally, the study finds that zero-shot performance using semantic tokens matches or surpasses that of the original models on specific classification tasks. This suggests semantic tokens at the subword level act as foundational anchors facilitating effective cross-lingual transfer, highlighting the potential of semantic tokenization to enhance multilingual understanding and model generalization. <div>
arXiv:2411.04530v2 Announce Type: replace 
Abstract: Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form "semantic tokens" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Newswire Extraction: A pipeline for extracting newswires from newspaper images</title>
<link>https://arxiv.org/abs/2502.11866</link>
<guid>https://arxiv.org/abs/2502.11866</guid>
<content:encoded><![CDATA[
arXiv:2502.11866v2 Announce Type: replace 
Abstract: I describe a new pipeline for extracting wire services (e.g., Associated Press, United Press International, Newspaper Enterprise Association) from newspaper images.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles</title>
<link>https://arxiv.org/abs/2504.12312</link>
<guid>https://arxiv.org/abs/2504.12312</guid>
<content:encoded><![CDATA[
arXiv:2504.12312v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved significant progress in language understanding and reasoning. Evaluating and analyzing their logical reasoning abilities has therefore become essential. However, existing datasets and benchmarks are often limited to overly simplistic, unnatural, or contextually constrained examples. In response to the growing demand, we introduce SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled benchmark derived from real-world high-quality Reddit posts containing subtle logical fallacies. Unlike existing datasets and benchmarks, it provides more detailed annotations of logical fallacies and features more diverse data. To further scale up the study and address the limitations of manual data collection and labeling - such as fallacy-type imbalance and labor-intensive annotation - we introduce SmartyPat, an automated framework powered by logic programming-based oracles. SmartyPat utilizes Prolog rules to systematically generate logically fallacious statements, which are then refined into fluent natural-language sentences by LLMs, ensuring precise fallacy representation. Extensive evaluation demonstrates that SmartyPat produces fallacies comparable in subtlety and quality to human-generated content and significantly outperforms baseline methods. Finally, experiments reveal nuanced insights into LLM capabilities, highlighting that while excessive reasoning steps hinder fallacy detection accuracy, structured reasoning enhances fallacy categorization performance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts</title>
<link>https://arxiv.org/abs/2505.03025</link>
<guid>https://arxiv.org/abs/2505.03025</guid>
<content:encoded><![CDATA[
arXiv:2505.03025v2 Announce Type: replace 
Abstract: Synthetic data sets are used across linguistic domains and NLP tasks, particularly in scenarios where authentic data is limited (or even non-existent). One such domain is that of clinical (healthcare) contexts, where there exist significant and long-standing challenges (e.g., privacy, anonymization, and data governance) which have led to the development of an increasing number of synthetic datasets. One increasingly important category of clinical dataset is that of clinical dialogues which are especially sensitive and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some situations, little theory exists to inform how they may be best used and generalized to new applications. In this paper, we provide an overview of how synthetic datasets are created, evaluated and being used for dialogue related tasks in the medical domain. Additionally, we propose a novel typology for use in classifying types and degrees of data synthesis, to facilitate comparison and evaluation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models</title>
<link>https://arxiv.org/abs/2505.14160</link>
<guid>https://arxiv.org/abs/2505.14160</guid>
<content:encoded><![CDATA[
arXiv:2505.14160v4 Announce Type: replace 
Abstract: Multilingual vision-language models (VLMs) promise universal image-text retrieval, yet their social biases remain underexplored. We perform the first systematic audit of four public multilingual CLIP variants: M-CLIP, NLLB-CLIP, CAPIVARA-CLIP, and the debiased SigLIP-2, covering ten languages that differ in resource availability and morphological gender marking. Using balanced subsets of FairFace and the PATA stereotype suite in a zero-shot setting, we quantify race and gender bias and measure stereotype amplification. Contrary to the intuition that multilinguality mitigates bias, every model exhibits stronger gender skew than its English-only baseline. CAPIVARA-CLIP shows its largest biases precisely in the low-resource languages it targets, while the shared encoder of NLLB-CLIP and SigLIP-2 transfers English gender stereotypes into gender-neutral languages; loosely coupled encoders largely avoid this leakage. Although SigLIP-2 reduces agency and communion skews, it inherits -- and in caption-sparse contexts (e.g., Xhosa) amplifies -- the English anchor's crime associations. Highly gendered languages consistently magnify all bias types, yet gender-neutral languages remain vulnerable whenever cross-lingual weight sharing imports foreign stereotypes. Aggregated metrics thus mask language-specific hot spots, underscoring the need for fine-grained, language-aware bias evaluation in future multilingual VLM research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling</title>
<link>https://arxiv.org/abs/2507.04416</link>
<guid>https://arxiv.org/abs/2507.04416</guid>
<content:encoded><![CDATA[
arXiv:2507.04416v3 Announce Type: replace 
Abstract: Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation can suffer from memory degradation in long contexts and limit fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7$\times$ improvement in training speed for 100K sequence length and 9$times$ in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Hallucination in Conversations for Low Resource Languages</title>
<link>https://arxiv.org/abs/2507.22720</link>
<guid>https://arxiv.org/abs/2507.22720</guid>
<content:encoded><![CDATA[
arXiv:2507.22720v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation</title>
<link>https://arxiv.org/abs/2508.16983</link>
<guid>https://arxiv.org/abs/2508.16983</guid>
<content:encoded><![CDATA[
arXiv:2508.16983v2 Announce Type: replace 
Abstract: Knowledge gaps and hallucinations are persistent challenges for Large Language Models (LLMs), which generate unreliable responses when lacking the necessary information to fulfill user instructions. Existing approaches, such as Retrieval-Augmented Generation (RAG) and tool use, aim to address these issues by incorporating external knowledge. Yet, they rely on additional models or services, resulting in complex pipelines, potential error propagation, and often requiring the model to process a large number of tokens. In this paper, we present a scalable method that enables LLMs to access external knowledge without depending on retrievers or auxiliary models. Our approach uses constrained generation with a pre-built prefix-tree index. Triples from a Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a prefix tree for efficient access. During inference, to acquire external knowledge, the LLM generates facts with constrained generation which allows only sequences of tokens that form an existing fact. We evaluate our proposal on Question Answering and show that it scales to large knowledge bases (800 million facts), adapts to domain-specific data, and achieves effective results. These gains come with minimal generation-time overhead. ReFactX code is available at https://github.com/rpo19/ReFactX.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Alignment-Centric Paradigm: A Survey of Instruction Tuning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.17184</link>
<guid>https://arxiv.org/abs/2508.17184</guid>
<content:encoded><![CDATA[
arXiv:2508.17184v2 Announce Type: replace 
Abstract: Instruction tuning is a pivotal technique for aligning large language models (LLMs) with human intentions, safety constraints, and domain-specific requirements. This survey provides a comprehensive overview of the full pipeline, encompassing (i) data collection methodologies, (ii) full-parameter and parameter-efficient fine-tuning strategies, and (iii) evaluation protocols. We categorized data construction into three major paradigms: expert annotation, distillation from larger models, and self-improvement mechanisms, each offering distinct trade-offs between quality, scalability, and resource cost. Fine-tuning techniques range from conventional supervised training to lightweight approaches, such as low-rank adaptation (LoRA) and prefix tuning, with a focus on computational efficiency and model reusability. We further examine the challenges of evaluating faithfulness, utility, and safety across multilingual and multimodal scenarios, highlighting the emergence of domain-specific benchmarks in healthcare, legal, and financial applications. Finally, we discuss promising directions for automated data generation, adaptive optimization, and robust evaluation frameworks, arguing that a closer integration of data, algorithms, and human feedback is essential for advancing instruction-tuned LLMs. This survey aims to serve as a practical reference for researchers and practitioners seeking to design LLMs that are both effective and reliably aligned with human intentions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Alignment of Large Language Models with Global Human Opinion</title>
<link>https://arxiv.org/abs/2509.01418</link>
<guid>https://arxiv.org/abs/2509.01418</guid>
<content:encoded><![CDATA[
arXiv:2509.01418v2 Announce Type: replace 
Abstract: Today's large language models (LLMs) are capable of supporting multilingual scenarios, allowing users to interact with LLMs in their native languages. When LLMs respond to subjective questions posed by users, they are expected to align with the views of specific demographic groups or historical periods, shaped by the language in which the user interacts with the model. Existing studies mainly focus on researching the opinions represented by LLMs among demographic groups in the United States or a few countries, lacking worldwide country samples and studies on human opinions in different historical periods, as well as lacking discussion on using language to steer LLMs. Moreover, they also overlook the potential influence of prompt language on the alignment of LLMs' opinions. In this study, our goal is to fill these gaps. To this end, we create an evaluation framework based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across different countries, languages, and historical periods around the world. We find that LLMs appropriately or over-align the opinions with only a few countries while under-aligning the opinions with most countries. Furthermore, changing the language of the prompt to match the language used in the questionnaire can effectively steer LLMs to align with the opinions of the corresponding country more effectively than existing steering methods. At the same time, LLMs are more aligned with the opinions of the contemporary population. To our knowledge, our study is the first comprehensive investigation of the topic of opinion alignment in LLMs across global, language, and temporal dimensions. Our code and data are publicly available at https://github.com/ku-nlp/global-opinion-alignment and https://github.com/nlply/global-opinion-alignment.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents</title>
<link>https://arxiv.org/abs/2509.01560</link>
<guid>https://arxiv.org/abs/2509.01560</guid>
<content:encoded><![CDATA[
arXiv:2509.01560v2 Announce Type: replace 
Abstract: Tool agents -- LLM-based systems that interact with external APIs -- offer a way to execute real-world tasks. However, as tasks become increasingly complex, these agents struggle to identify and call the correct APIs in the proper order. To tackle this problem, we investigate converting API documentation into a structured API graph that captures API dependencies and leveraging it for multi-tool queries that require compositional API calls. To support this, we introduce In-N-Out, the first expert-annotated dataset of API graphs built from two real-world API benchmarks and their documentation. Using In-N-Out significantly improves performance on both tool retrieval and multi-tool query generation, nearly doubling that of LLMs using documentation alone. Moreover, graphs generated by models fine-tuned on In-N-Out close 90% of this gap, showing that our dataset helps models learn to comprehend API documentation and parameter relationships. Our findings highlight the promise of using explicit API graphs for tool agents and the utility of In-N-Out as a valuable resource. We will release the dataset and code publicly.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias after Prompting: Persistent Discrimination in Large Language Models</title>
<link>https://arxiv.org/abs/2509.08146</link>
<guid>https://arxiv.org/abs/2509.08146</guid>
<content:encoded><![CDATA[
arXiv:2509.08146v2 Announce Type: replace 
Abstract: A dangerous assumption that can be made from prior work on the bias transfer hypothesis (BTH) is that biases do not transfer from pre-trained large language models (LLMs) to adapted models. We invalidate this assumption by studying the BTH in causal models under prompt adaptations, as prompting is an extremely popular and accessible adaptation strategy used in real-world applications. In contrast to prior work, we find that biases can transfer through prompting and that popular prompt-based mitigation methods do not consistently prevent biases from transferring. Specifically, the correlation between intrinsic biases and those after prompt adaptation remain moderate to strong across demographics and tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age (rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we find that biases remain strongly correlated when varying few-shot composition parameters, such as sample size, stereotypical content, occupational distribution and representational balance (rho >= 0.90). We evaluate several prompt-based debiasing strategies and find that different approaches have distinct strengths, but none consistently reduce bias transfer across models, tasks or demographics. These results demonstrate that correcting bias, and potentially improving reasoning ability, in intrinsic models may prevent propagation of biases to downstream tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation based context discovery for ASR</title>
<link>https://arxiv.org/abs/2509.19567</link>
<guid>https://arxiv.org/abs/2509.19567</guid>
<content:encoded><![CDATA[
arXiv:2509.19567v2 Announce Type: replace 
Abstract: This work investigates retrieval augmented generation as an efficient strategy for automatic context discovery in context-aware Automatic Speech Recognition (ASR) system, in order to improve transcription accuracy in the presence of rare or out-of-vocabulary terms. However, identifying the right context automatically remains an open challenge. This work proposes an efficient embedding-based retrieval approach for automatic context discovery in ASR. To contextualize its effectiveness, two alternatives based on large language models (LLMs) are also evaluated: (1) large language model (LLM)-based context generation via prompting, and (2) post-recognition transcript correction using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech demonstrate that the proposed approach reduces WER by up to 17% (percentage difference) relative to using no-context, while the oracle context results in a reduction of up to 24.1%.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains</title>
<link>https://arxiv.org/abs/2510.17793</link>
<guid>https://arxiv.org/abs/2510.17793</guid>
<content:encoded><![CDATA[
arXiv:2510.17793v2 Announce Type: replace 
Abstract: Finetuning specialized generative evaluators has emerged as a popular paradigm to meet the increasing demand for scalable evaluation during both training and test-time. However, recent work has largely focused on applying new methodology, such as reinforcement learning (RL), to training evaluators, shying away from large-scale, data-driven development. In this work, we focus on data scaling, curating a set of 2.5M samples spanning five unique evaluation tasks (pairwise, step-level, reference-free and reference-based verification, and single rating) and multiple domains focused on reasoning evaluation. With our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges larger specialized RL-trained evaluators and FARE-20B sets the new standard for open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers, FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training, FARE improves the downstream RL-trained model performance by up to 14.1% vs. string-matching verifiers. When initialized from FARE, a continually-finetuned FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning</title>
<link>https://arxiv.org/abs/2510.20098</link>
<guid>https://arxiv.org/abs/2510.20098</guid>
<content:encoded><![CDATA[
arXiv:2510.20098v2 Announce Type: replace 
Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and extensive model fine-tuning. While recent few-shot methods leverage large language models (LLMs) through prompting to reduce training requirements, they often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER (Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline that achieves high performance without deep fine-tuning by strategically combining candidate generation, context-based scoring, adaptive routing, and selective reasoning. ARTER computes a small set of complementary signals(both embedding and LLM-based) over the retrieved candidates to categorize contextual mentions into easy and hard cases. The cases are then handled by a low-computational entity linker (e.g. ReFinED) and more expensive targeted LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets, and performs comparably to pipelines using LLM-based reasoning for all mentions, while being as twice as efficient in terms of the number of LLM tokens.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.20548</link>
<guid>https://arxiv.org/abs/2510.20548</guid>
<content:encoded><![CDATA[
arXiv:2510.20548v2 Announce Type: replace 
Abstract: Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays</title>
<link>https://arxiv.org/abs/2510.22830</link>
<guid>https://arxiv.org/abs/2510.22830</guid>
<content:encoded><![CDATA[
arXiv:2510.22830v4 Announce Type: replace 
Abstract: BERT and its variants are extensively explored for automated scoring. However, a limit of 512 tokens for these encoder-based models showed the deficiency in automated scoring of long essays. Thus, this research explores generative language models for automated scoring of long essays via summarization and prompting. The results revealed great improvement of scoring accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab Automated Essay Scoring 2.0 dataset.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs</title>
<link>https://arxiv.org/abs/2510.26253</link>
<guid>https://arxiv.org/abs/2510.26253</guid>
<content:encoded><![CDATA[
arXiv:2510.26253v2 Announce Type: replace 
Abstract: The ability to accurately interpret implied meanings plays a crucial role in human communication and language use, and language models are also expected to possess this capability. This study demonstrates that providing language models with pragmatic theories as prompts is an effective in-context learning approach for tasks to understand implied meanings. Specifically, we propose an approach in which an overview of pragmatic theories, such as Gricean pragmatics and Relevance Theory, is presented as a prompt to the language model, guiding it through a step-by-step reasoning process to derive a final interpretation. Experimental results showed that, compared to the baseline, which prompts intermediate reasoning without presenting pragmatic theories (0-shot Chain-of-Thought), our methods enabled language models to achieve up to 9.6\% higher scores on pragmatic reasoning tasks. Furthermore, we show that even without explaining the details of pragmatic theories, merely mentioning their names in the prompt leads to a certain performance improvement (around 1-3%) in larger models compared to the baseline.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-Audio-EditX Technical Report</title>
<link>https://arxiv.org/abs/2511.03601</link>
<guid>https://arxiv.org/abs/2511.03601</guid>
<content:encoded><![CDATA[
arXiv:2511.03601v2 Announce Type: replace 
Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidential Prompting: Privacy-preserving LLM Inference on Cloud</title>
<link>https://arxiv.org/abs/2409.19134</link>
<guid>https://arxiv.org/abs/2409.19134</guid>
<content:encoded><![CDATA[
arXiv:2409.19134v5 Announce Type: replace-cross 
Abstract: This paper introduces a vision of confidential prompting: securing user prompts from an untrusted, cloud-hosted large language model (LLM) while preserving model confidentiality, output invariance, and compute efficiency. As a first step toward this vision, we present Petridish, a system built on top of confidential computing and its core contribution, a novel technology called Secure Partitioned Decoding (SPD). Petridish runs the LLM service inside a confidential virtual machine (CVM), which protects the secrets, i.e., the LLM parameters and user prompts, from adversaries outside the CVM. Importantly, it splits the LLM service for a user into two processes, using SPD: a per-user process performs prefill with the user prompts and computes attention scores during decoding; a service process, shared by all users, batches the attention scores from per-user processes and generates output tokens for all users. Both the LLM provider and the users trust Petridish's CVM and its operating system, which guarantees isolation between processes and limits their outbound network capabilities to control information flow. The CVM's attestation capability and its open-source software stack enable Petridish to provide auditable protection of both user prompt and LLM confidentiality. Together, Petridish maintains full utility of LLM service and enables practical, privacy-preserving cloud-hosted LLM inference for sensitive applications, such as processing personal data, clinical records, and financial documents.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning-Aware Code Infilling via Horizon-Length Prediction</title>
<link>https://arxiv.org/abs/2410.03103</link>
<guid>https://arxiv.org/abs/2410.03103</guid>
<content:encoded><![CDATA[
arXiv:2410.03103v4 Announce Type: replace-cross 
Abstract: Fill-in-the-Middle (FIM), or infilling, has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm which performs next-token prediction (NTP) over reordered sequence often leads to models struggling to generate content that aligns well with the surrounding context. We hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different model families and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eguard: Defending LLM Embeddings Against Inversion Attacks via Text Mutual Information Optimization</title>
<link>https://arxiv.org/abs/2411.05034</link>
<guid>https://arxiv.org/abs/2411.05034</guid>
<content:encoded><![CDATA[
arXiv:2411.05034v2 Announce Type: replace-cross 
Abstract: Embeddings have become a cornerstone in the functionality of large language models (LLMs) due to their ability to transform text data into rich, dense numerical representations that capture semantic and syntactic properties. These embedding vector databases serve as the long-term memory of LLMs, enabling efficient handling of a wide range of natural language processing tasks. However, the surge in popularity of embedding vector databases in LLMs has been accompanied by significant concerns about privacy leakage. Embedding vector databases are particularly vulnerable to embedding inversion attacks, where adversaries can exploit the embeddings to reverse-engineer and extract sensitive information from the original text data. Existing defense mechanisms have shown limitations, often struggling to balance security with the performance of downstream tasks. To address these challenges, we introduce Eguard, a novel defense mechanism designed to mitigate embedding inversion attacks. Eguard employs a transformer-based projection network and text mutual information optimization to safeguard embeddings while preserving the utility of LLMs. Our approach significantly reduces privacy risks, protecting over 95% of tokens from inversion while maintaining high performance across downstream tasks consistent with original embeddings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairshare Data Pricing via Data Valuation for Large Language Models</title>
<link>https://arxiv.org/abs/2502.00198</link>
<guid>https://arxiv.org/abs/2502.00198</guid>
<content:encoded><![CDATA[
arXiv:2502.00198v4 Announce Type: replace-cross 
Abstract: Training data is the backbone of large language models (LLMs), yet today's data markets often operate under exploitative pricing -- sourcing data from marginalized groups with little pay or recognition. This paper introduces a theoretical framework for LLM data markets, modeling the strategic interactions between buyers (LLM builders) and sellers (human annotators). We begin with theoretical and empirical analysis showing how exploitative pricing drives high-quality sellers out of the market, degrading data quality and long-term model performance. Then we introduce fairshare, a pricing mechanism grounded in data valuation that quantifies each data's contribution. It aligns incentives by sustaining seller participation and optimizing utility for both buyers and sellers. Theoretically, we show that fairshare yields mutually optimal outcomes: maximizing long-term buyer utility and seller profit while sustaining market participation. Empirically when training open-source LLMs on complex NLP tasks, including math problems, medical diagnosis, and physical reasoning, fairshare boosts seller earnings and ensures a stable supply of high-quality data, while improving buyers' performance-per-dollar and long-term welfare. Our findings offer a concrete path toward fair, transparent, and economically sustainable data markets for LLM.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2503.07265</link>
<guid>https://arxiv.org/abs/2503.07265</guid>
<content:encoded><![CDATA[
arXiv:2503.07265v3 Announce Type: replace-cross 
Abstract: Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text-to-image generation. To address this challenge, we propose \textbf{WISE}, the first benchmark specifically designed for \textbf{W}orld Knowledge-\textbf{I}nformed \textbf{S}emantic \textbf{E}valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 subdomains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce \textbf{WiScore}, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at \href{https://github.com/PKU-YuanGroup/WISE}{PKU-YuanGroup/WISE}.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities</title>
<link>https://arxiv.org/abs/2503.17979</link>
<guid>https://arxiv.org/abs/2503.17979</guid>
<content:encoded><![CDATA[
arXiv:2503.17979v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3 and DeepSeek-R1, have demonstrated remarkable performance in specialized reasoning tasks through human-like deliberative thinking and long chain-of-thought reasoning. However, our systematic evaluation across various model families (DeepSeek, Qwen, and LLaMA) and scales (7B to 32B) reveals that acquiring these deliberative reasoning capabilities significantly reduces the foundational capabilities of LRMs, including notable declines in helpfulness and harmlessness, alongside substantially increased inference costs. Importantly, we demonstrate that adaptive reasoning -- employing modes like Zero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate these drawbacks. Our empirical insights underline the critical need for developing more versatile LRMs capable of dynamically allocating inference-time compute according to specific task characteristics.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents</title>
<link>https://arxiv.org/abs/2504.16264</link>
<guid>https://arxiv.org/abs/2504.16264</guid>
<content:encoded><![CDATA[
arXiv:2504.16264v2 Announce Type: replace-cross 
Abstract: Cross-lingual information retrieval (CLIR) helps users find documents in languages different from their queries. This is especially important in academic search, where key research is often published in non-English languages. We present CLIRudit, a novel English-French academic retrieval dataset built from \'Erudit, a Canadian publishing platform. Using multilingual metadata, we pair English author-written keywords as queries with non-English abstracts as target documents, a method that can be applied to other languages and repositories. We benchmark various first-stage sparse and dense retrievers, with and without machine translation. We find that dense embeddings without translation perform nearly as well as systems using machine translation, that translating documents is generally more effective than translating queries, and that sparse retrievers with document translation remain competitive while offering greater efficiency. Along with releasing the first English-French academic retrieval dataset, we provide a reproducible benchmarking method to improve access to non-English scholarly content.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</title>
<link>https://arxiv.org/abs/2507.18224</link>
<guid>https://arxiv.org/abs/2507.18224</guid>
<content:encoded><![CDATA[
arXiv:2507.18224v4 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal communication links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving</title>
<link>https://arxiv.org/abs/2508.08343</link>
<guid>https://arxiv.org/abs/2508.08343</guid>
<content:encoded><![CDATA[
arXiv:2508.08343v3 Announce Type: replace-cross 
Abstract: With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Preserving In-Context-Learning Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2509.13625</link>
<guid>https://arxiv.org/abs/2509.13625</guid>
<content:encoded><![CDATA[
arXiv:2509.13625v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have significantly transformed natural language understanding and generation, but they raise privacy concerns due to potential exposure of sensitive information. Studies have highlighted the risk of information leakage, where adversaries can extract sensitive information embedded in the prompts. In this work, we introduce a novel private prediction framework for generating high-quality synthetic text with strong privacy guarantees. Our approach leverages the Differential Privacy (DP) framework to ensure worst-case theoretical bounds on information leakage without requiring any fine-tuning of the underlying models. The proposed method performs inference on private records and aggregates the resulting per-token output distributions. This enables the generation of longer and coherent synthetic text while maintaining privacy guarantees. Additionally, we propose a simple blending operation that combines private and public inference to further enhance utility. Empirical evaluations demonstrate that our approach outperforms previous state-of-the-art methods on in-context-learning (ICL) tasks, making it a promising direction for privacy-preserving text generation while maintaining high utility. Our code is available at https://github.com/bhusalb/privacy-preserving-icl.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks</title>
<link>https://arxiv.org/abs/2509.24473</link>
<guid>https://arxiv.org/abs/2509.24473</guid>
<content:encoded><![CDATA[
arXiv:2509.24473v3 Announce Type: replace-cross 
Abstract: Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. Furthermore, to enable the model to learn and apply Euclidean principles from these geometry problems, we fine-tuned seven model variants (spanning 3--72B parameters) from the Qwen2.5VL, Qwen3VL, and RoboBrain2.0 families using Group Relative Policy Optimization (GRPO), inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy rose from 36.6\% to 41.8\% (+5.2\%), and the mean MindCube accuracy rose from 31.4\% to 38.1\% (+6.7\%). To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in \href{https://zgca-ai4edu.github.io/Euclids_Gift}{this}.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conflict Adaptation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.24804</link>
<guid>https://arxiv.org/abs/2510.24804</guid>
<content:encoded><![CDATA[
arXiv:2510.24804v2 Announce Type: replace-cross 
Abstract: A signature of human cognitive control is conflict adaptation: improved performance on a high-conflict trial following another high-conflict trial. This phenomenon offers an account for how cognitive control, a scarce resource, is recruited. Using a sequential Stroop task, we find that 12 of 13 vision-language models (VLMs) tested exhibit behavior consistent with conflict adaptation, with the lone exception likely reflecting a ceiling effect. To understand the representational basis of this behavior, we use sparse autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B. Partially overlapping supernodes emerge for text and color in both early and late layers, and their relative sizes mirror the automaticity asymmetry between reading and color naming in humans. We further isolate a conflict-modulated supernode in layers 24-25 whose ablation significantly increases Stroop errors while minimally affecting congruent trials.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start</title>
<link>https://arxiv.org/abs/2510.25801</link>
<guid>https://arxiv.org/abs/2510.25801</guid>
<content:encoded><![CDATA[
arXiv:2510.25801v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of "MLLM-r1" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution "stuckness," improving exploration, stabilizing training, and raising the performance ceiling.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</title>
<link>https://arxiv.org/abs/2510.19670</link>
<guid>https://arxiv.org/abs/2510.19670</guid>
<content:encoded><![CDATA[
<div> Keywords: CoSense-LLM, multimodal, edge computing, privacy, latency  

<br /><br />Summary: CoSense-LLM is an innovative edge-first framework designed to transform continuous multimodal sensor streams—such as Wi-Fi CSI, IMU, audio, RFID, and lightweight vision—into compact, verifiable semantic tokens while adhering to explicit constraints regarding latency, energy, bandwidth, and privacy. The framework consists of four main components: (i) SenseFusion, a lightweight encoder that compresses sensor embeddings into short discrete code sequences; (ii) Edge-RAG, which grounds generative processes in specific site policies and notes through local hybrid retrieval; (iii) PromptRouter, a cost and uncertainty-aware policy that efficiently selects between different processing strategies; and (iv) Secure Execution, which ensures data minimization by preventing raw waveforms from leaving the device. CoSense-LLM integrates modern serving optimizations that facilitate on-device personalization and federated updates, achieving sub-second latency and reducing inter-tier costs while maintaining user privacy. The framework demonstrates enhanced factual consistency and controlled escalation through various techniques, supporting an edge-first approach that prioritizes semantics, privacy, and predictable latency for large model applications in environments prone to interference. <div>
arXiv:2510.19670v3 Announce Type: replace 
Abstract: We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models</title>
<link>https://arxiv.org/abs/2511.13722</link>
<guid>https://arxiv.org/abs/2511.13722</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, watermarking, adversarial attacks, text quality, back translation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of watermarking texts generated by Large Language Models (LLMs) as a method to reliably detect AI-generated content.<br /><br />2. Despite effective detection, current watermarking methods often degrade the quality of the generated texts, leading to concerns about their impact on usability.<br /><br />3. The study evaluates the robustness of several watermarking techniques against adversarial attacks, focusing specifically on paraphrasing and back translation (English to another language and back).<br /><br />4. The authors assess how well these watermarking methods preserve the semantics and writing style of the original, unwatermarked texts using linguistic metrics.<br /><br />5. Results show that while semantics are generally preserved, these watermarks cause noticeable deviations in writing style and are vulnerable to removal, especially through back translation attacks, highlighting limitations that may hinder broader adoption of watermarking.<br /><br />In conclusion, the research reveals a trade-off between watermark detectability, text quality, and robustness against adversarial modifications, suggesting further work is needed to improve watermarking techniques for LLM-generated texts. <div>
arXiv:2511.13722v1 Announce Type: new 
Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning</title>
<link>https://arxiv.org/abs/2511.13726</link>
<guid>https://arxiv.org/abs/2511.13726</guid>
<content:encoded><![CDATA[
<div> Keywords: RT method, semantic reasoning, text embedding models, refinement, inference enhancement  

<br /><br />Summary:  
This paper introduces RT (Refine Thought), a novel approach designed to improve the semantic reasoning capabilities of text embedding models. RT works by performing multiple forward passes through a text embedding model to produce a more refined and accurate final semantic representation. Experimental results demonstrate that RT significantly enhances performance on semantic reasoning benchmarks such as BRIGHT and PJBenchmark, which focus on person-job matching tasks. Importantly, RT maintains stable and reliable results on broader semantic understanding benchmarks like C-MTEB, indicating its versatility. The method is particularly effective when applied to decoder-only text embedding models, exemplified by its use with Qwen3-Embedding-8B. RT operates as a test-time inference technique, meaning it can be applied during the model's inference phase without retraining or modifying the underlying model parameters. This approach leverages and further activates the semantic reasoning ability that these models already learned during pretraining. Overall, RT represents a promising strategy to boost the semantic reasoning power of existing embedding models with minimal changes to their workflow, providing practical benefits for tasks requiring deeper semantic understanding. <div>
arXiv:2511.13726v1 Announce Type: new 
Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can QE-informed (Re)Translation lead to Error Correction?</title>
<link>https://arxiv.org/abs/2511.13884</link>
<guid>https://arxiv.org/abs/2511.13884</guid>
<content:encoded><![CDATA[
<div> Keywords: Quality Estimation, Automatic Post-Editing, Machine Translation, LLMs, Delta COMET score  

<br /><br />Summary: The paper introduces two novel approaches designed for the WMT 2025 Automated Translation Quality Evaluation Systems Task 3, focusing on Quality Estimation (QE)-informed segment-level error correction. Both methods are presented in a training-free context. The first approach, QE-informed Retranslation, selects the highest-quality translation from multiple outputs generated by various large language models (LLMs). This method showcased improved effectiveness compared to the second approach, which is more similar to Automatic Post-Editing (APE). In the second approach, an LLM is directed to replace error substrings based on the provided QE explanations. To enhance performance, a conditional heuristic is implemented to limit the number of edits made, thereby aiming to optimize the Gain-to-Edit ratio. The results indicated that the first approach achieved a Delta COMET score of 0.0201, positioning it at the top of the subtask leaderboard, while the second approach recorded a slightly lower score of -0.0108. Overall, the findings highlight the potential of QE-informed strategies in enhancing translation quality without requiring extensive training. <div>
arXiv:2511.13884v1 Announce Type: new 
Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations</title>
<link>https://arxiv.org/abs/2511.13900</link>
<guid>https://arxiv.org/abs/2511.13900</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval, GM-Extract, evaluation metrics, mitigation methods  

<br /><br />Summary:  
The article addresses the challenge posed by the "lost-in-the-middle" phenomenon in large language models (LLMs) when leveraging long-range context for retrieval tasks. To explore this issue, the authors introduce GM-Extract, a benchmark dataset aimed at assessing LLM performance in retrieving control variables. They propose an evaluation framework comprising two metrics: the Document Metric for spatial retrieval and the Variable Extraction Metric for semantic retrieval. Through a systematic evaluation of 7-8 billion parameter models on tasks like key-value extraction and question-answering, they observe that data representation in the context window significantly influences retrieval results. While a consistent U-shaped performance curve isn't found, a general performance pattern correlating with perplexity scores is identified. Additionally, the authors conduct a literature review of mitigation methods, dividing them into black-box and white-box approaches. The application of these techniques reveals nuanced efficacy, demonstrating both improvements and unexpected negative impacts on performance. This comprehensive evaluation provides valuable insights into the practical utility of various strategies for enhancing LLM performance in retrieval-based tasks. <div>
arXiv:2511.13900v1 Announce Type: new 
Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition</title>
<link>https://arxiv.org/abs/2511.13994</link>
<guid>https://arxiv.org/abs/2511.13994</guid>
<content:encoded><![CDATA[
<div> Keywords: superlatives, LLMs, retrieval, ranking, semantic interpretation  

<br /><br />Summary: This article investigates the role of Large Language Models (LLMs) in processing search queries that contain superlatives, such as "best" and "most popular." These queries require a complex understanding of language and domain-specific knowledge, as they involve comparing candidates across various attributes. The authors propose a novel framework that extracts structured interpretations from these queries, detailing attribute-value hints generated alongside the retrieval process. This framework significantly enhances search performance, achieving a 10.9-point improvement in Mean Average Precision (MAP) and a 5.9-point improvement in Mean Reciprocal Rank (MRR) over conventional baselines. To address latency issues inherent in direct LLM-based reranking, the study introduces an innovative method that transfers superlative interpretations to more lightweight models. The findings shed light on the potential for representing and transferring superlative semantics across different models, thereby enhancing linguistic interpretation within retrieval systems. This advancement not only improves search rankings but also considers practical deployment challenges, ultimately contributing to more effective e-commerce search experiences. <div>
arXiv:2511.13994v1 Announce Type: new 
Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</title>
<link>https://arxiv.org/abs/2511.14010</link>
<guid>https://arxiv.org/abs/2511.14010</guid>
<content:encoded><![CDATA[
<div> Keywords: post-disaster, reconnaissance reports, multi-hazard, language models, MoRA-RAG  

<br /><br />Summary: This study addresses the challenges posed by unstructured narratives in post-disaster reconnaissance reports, which hinder systematic knowledge transfer regarding multi-hazard interactions. To improve the analysis of these reports, the authors introduce the Mixture-of-Retrieval Agentic RAG (MoRA-RAG) framework. This knowledge-grounded large language model (LLM) framework systematically transforms reconnaissance reports into structured data conducive to multi-hazard reasoning. The MoRA-RAG framework utilizes a Mixture-of-Retrieval mechanism that routes queries through hazard-specific databases while maintaining contextual coherence via agentic chunking. Additionally, it incorporates a verification loop to evaluate evidence sufficiency, refine queries, and conduct targeted searches when information is incomplete. The researchers created HazardRecQA by generating question-answer pairs from GEER reconnaissance reports that cover 90 global events across seven hazard types. MoRA-RAG demonstrates remarkable performance, achieving up to 94.5% accuracy—outperforming zero-shot LLMs by 30% and state-of-the-art RAG systems by 10%, while also reducing the incidence of hallucinations in various LLM architectures. The framework enables open-weight LLMs to match the performance of proprietary models, establishing a new paradigm in converting post-disaster documentation into reliable, actionable intelligence for enhancing hazard resilience. <div>
arXiv:2511.14010v1 Announce Type: new 
Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection</title>
<link>https://arxiv.org/abs/2511.14027</link>
<guid>https://arxiv.org/abs/2511.14027</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal, misinformation detection, external consistency, evidence reranking, large language models

<br /><br />Summary: This paper introduces HiEAG, a Hierarchical Evidence-Augmented Generation framework aimed at improving multimodal out-of-context (OOC) misinformation detection. While prior methods focused on internal consistency between image-text pairs, they often overlooked the critical aspect of external consistency with external evidence. HiEAG addresses this gap by breaking down external consistency checking into a structured engine pipeline that incorporates evidence retrieval, reranking, and rewriting. The evidence reranking module employs Automatic Evidence Selection Prompting (AESP) to identify relevant evidence from the retrieval outputs. Following this, the evidence rewriting module uses Automatic Evidence Generation Prompting (AEGP) to enhance the adaptability of MLLM-based OOC misinformation detectors. Additionally, HiEAG provides explanations for its judgments, thus increasing transparency in the detection process. The framework has been optimized through instruction tuning, resulting in significant performance improvements. Experimental evaluations across multiple benchmark datasets reveal that HiEAG outperforms existing state-of-the-art methods concerning accuracy across all evaluated samples, solidifying its position as an advanced solution in the field of misinformation detection. <div>
arXiv:2511.14027v1 Announce Type: new 
Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement</title>
<link>https://arxiv.org/abs/2511.14073</link>
<guid>https://arxiv.org/abs/2511.14073</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-label sentiment classification, dataset, class imbalance, RoBERTa, attention mechanism  

<br /><br />Summary: Multi-label sentiment classification is essential in natural language processing for identifying multiple emotions in a single piece of text. Existing datasets, such as GoEmotions, often exhibit significant class imbalance, which negatively impacts model performance for rare emotions. To tackle this issue, the authors created a balanced multi-label sentiment dataset by combining GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. This data balancing ensures an even distribution across 28 emotion categories. Alongside the dataset, the authors developed an advanced multi-label classification model that integrates pre-trained FastText embeddings, convolutional layers for localized feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to identify sentiment-relevant words. The model employs a sigmoid-activated output layer for multi-label prediction and utilizes mixed precision training to enhance computational efficiency. Experimental results indicate substantial improvements in accuracy, precision, recall, F1-score, and AUC when compared to models trained on imbalanced data, underscoring the effectiveness of the proposed approach in addressing the challenges of multi-label sentiment classification. <div>
arXiv:2511.14073v1 Announce Type: new 
Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT</title>
<link>https://arxiv.org/abs/2511.14106</link>
<guid>https://arxiv.org/abs/2511.14106</guid>
<content:encoded><![CDATA[
<div> Keywords: Stealth Fine-Tuning, Reasoning-augmented Vision-Language Models, safety alignment, segment-level interference, chain-of-thought attacks

<br /><br />Summary:  
This paper introduces a novel attack method called Stealth Fine-Tuning targeting Reasoning-augmented Vision-Language Models (RVLMs) which are designed with safety alignment to prevent harmful behavior. The method exploits vulnerabilities in the models' exposed chain-of-thought (CoT) reasoning traces, which become new attack surfaces. Stealth Fine-Tuning works by applying segment-level interference to elicit harmful reasoning outputs and then reuses these self-generated outputs as supervised fine-tuning data. A turn-based weighted loss function is employed to maintain lightweight, distribution-consistent fine-tuning without significantly altering the model’s original reasoning abilities. The approach is efficient, requiring only 499 samples and under 3 hours of training on a single A100 GPU using QLoRA. Experimental results show that Stealth Fine-Tuning achieves a 38.52% higher attack success rate (ASR) on AdvBench compared to the previous method IDEATOR, while preserving the model's general reasoning capabilities and representation distribution. The study highlights that Stealth Fine-Tuning serves as a low-cost, highly effective technique to bypass current alignment defenses in RVLMs. The authors also include a disclaimer noting the presence of potentially disturbing or offensive content in the paper. <div>
arXiv:2511.14106v1 Announce Type: new 
Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding</title>
<link>https://arxiv.org/abs/2511.14112</link>
<guid>https://arxiv.org/abs/2511.14112</guid>
<content:encoded><![CDATA[
<div> Keywords: ICD coding, synthetic data, medical NLP, macro-F1, transformer models

<br /><br />Summary: Automatic coding of ICD diagnoses from clinical text is essential in medical NLP but faces challenges due to the long-tail distribution of codes, with many rare ICD codes poorly represented in existing datasets like MIMIC-III. To address this issue, the authors developed a data-centric framework to generate high-quality synthetic discharge summaries that help alleviate code imbalances. This method constructs realistic multi-label code sets centered on rare codes by using real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and related clinical notes. A total of 90,000 synthetic notes encompassing 7,902 ICD codes were produced, significantly expanding the available training distribution. Two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, were fine-tuned on both the original and the augmented datasets. Experimental results reveal that the proposed approach achieves a modest improvement in macro-F1 scores while preserving strong micro-F1 performance, thereby surpassing previous state-of-the-art results. Although the performance gain may appear marginal given the computational expenses, this study demonstrates that thoughtfully constructed synthetic data can enhance fairness in predicting long-tail ICD codes. <div>
arXiv:2511.14112v1 Announce Type: new 
Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling</title>
<link>https://arxiv.org/abs/2511.14142</link>
<guid>https://arxiv.org/abs/2511.14142</guid>
<content:encoded><![CDATA[
<div> Aspect-Based Sentiment Analysis, Hypergraph, Hierarchical Clustering, Short-text NLP, RoBERTa  

<br /><br />Summary:  
This paper addresses the challenge of Aspect-Based Sentiment Analysis (ABSA), focusing on predicting sentiment polarity for specific aspect terms, especially in short texts where context is limited and sentiments can conflict across different aspects. Prior graph-based models relied on pairwise relationships, requiring multiple graphs for various relational views, which resulted in redundancy, increased parameters, and error propagation during fusion, hindering robustness in low-resource and short-text scenarios. To overcome these limitations, the authors propose HyperABSA, a novel dynamic hypergraph framework that constructs aspect-opinion structures through sample-specific hierarchical clustering. A key contribution is an acceleration-fallback cutoff mechanism in hierarchical clustering that adaptively sets the granularity level for hyperedges, improving efficiency and effectiveness. Experimental results on three benchmark datasets—Lap14, Rest14, and MAMS—demonstrate consistent performance gains compared to strong graph-based baselines, with notable improvements when combined with RoBERTa language model backbones. The study highlights dynamic hypergraph construction as an efficient, powerful alternative for ABSA tasks and suggests its potential for broader applications in short-text natural language processing tasks. <div>
arXiv:2511.14142v1 Announce Type: new 
Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions</title>
<link>https://arxiv.org/abs/2511.14144</link>
<guid>https://arxiv.org/abs/2511.14144</guid>
<content:encoded><![CDATA[
<div> Transformer-based relation extraction, knowledge graphs, multiple-choice questions, truthfulness verification, fill-in-the-blank format<br /><br />Summary:<br /><br />This research presents a novel approach that combines Transformer-based relation extraction (RE) with knowledge graph (KG) matching to answer multiple-choice questions (MCQs) in a fill-in-the-blank format, ensuring traceability throughout the process. Knowledge graphs, which represent factual knowledge via entities and relations, have traditionally been static due to high construction costs, but recent advances in Transformer-based RE allow dynamic KG generation from natural language texts. The proposed method converts each question sentence into a relational graph using RE and then evaluates its truthfulness against verified KGs under a closed-world assumption to avoid misinformation from factually incorrect inputs. Experimental results indicate that the method correctly answers approximately 70% of MCQs while maintaining transparency in reasoning. Furthermore, the study emphasizes that the category of questions significantly affects accuracy, suggesting certain types of questions align better with this approach. Overall, this method offers a promising direction for interpretable and accurate MCQ answering by integrating dynamic KG construction with validity checks against trusted knowledge sources. <div>
arXiv:2511.14144v1 Announce Type: new 
Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Weak-to-Strong Generalization</title>
<link>https://arxiv.org/abs/2511.14166</link>
<guid>https://arxiv.org/abs/2511.14166</guid>
<content:encoded><![CDATA[
<div> Keywords: superhuman, weak supervision, selective W2SG, alignment, robustness  

<br /><br />Summary: The paper highlights the challenges of aligning superhuman models with human oversight when traditional high-quality data is lacking. It addresses the pitfalls of existing weak-to-strong generalization (W2SG) approaches that rely heavily on weak supervision, which can negatively impact model robustness due to potentially harmful weak labels. To improve this, the authors propose a selective W2SG framework designed to bypass weak supervision when it's unnecessary. Central to their method is a binary classifier, denoted as P(IK), which is trained to identify questions that a strong pretrained model can answer effectively. This classifier generates self-generated labels for model alignment, enhancing the overall quality of the training process. Additionally, the authors implement a graph smoothing technique to refine weak labels further. Through extensive experimentation across three benchmarks, the proposed method consistently demonstrates improved performance compared to competitive baselines. The analyses reveal that the P(IK) classifier exhibits generalization capabilities across various tasks and difficulties, reinforcing the idea that selective W2SG can significantly contribute to the challenge of superalignment in AI models. <div>
arXiv:2511.14166v1 Announce Type: new 
Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA</title>
<link>https://arxiv.org/abs/2511.14172</link>
<guid>https://arxiv.org/abs/2511.14172</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, hallucination, symbolic knowledge, localization, attention variance  

<br /><br />Summary:  
Large Language Models (LLMs) often exhibit hallucination, particularly influenced by symbolic triggers like modifiers, negation, numbers, and named entities. However, the origins of these symbolic hallucinations are not well understood. Existing localization methods, such as LSC and activation variance analysis, tend to treat all tokens equally and ignore the impact of symbolic linguistic knowledge. This paper presents a novel symbolic localization framework that utilizes symbolic linguistic and semantic knowledge to trace hallucination development across various model layers. By analyzing five different models with tools like HaluEval and TruthfulQA, the study finds that attention variance related to these linguistic elements increases dramatically in the early layers of the models (layers 2-4), with negation causing particularly high instability. Despite advances in model size, hallucination rates remain alarmingly high (78.3%-83.7% across Gemma variants), coupled with significant attention drops for symbolic semantic triggers in deeper layers. The results suggest that hallucination issues stem from failures in symbolic linguistic processing rather than from broader generation problems, indicating that a focus on symbolic semantic knowledge is vital for comprehending and addressing hallucination mechanisms in LLMs. <div>
arXiv:2511.14172v1 Announce Type: new 
Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Deep LLM Participation for Robust Entity Linking</title>
<link>https://arxiv.org/abs/2511.14181</link>
<guid>https://arxiv.org/abs/2511.14181</guid>
<content:encoded><![CDATA[
<div> Keywords: Entity Linking, Large Language Models, DeepEL, Self-validation, Contextual Disambiguation<br /><br />Summary:<br /><br />1. This paper addresses the task of Entity Linking (EL), which involves mapping textual mentions to corresponding entries in knowledge bases, a key aspect of natural language understanding.<br /><br />2. While prior work leverages Large Language Models (LLMs) to enhance specific components such as entity disambiguation or input representation, these approaches do not fully exploit LLM capabilities throughout the entire EL process.<br /><br />3. The authors propose DeepEL, a novel framework integrating LLMs comprehensively across all EL stages, improving the coherence and accuracy of entity linking.<br /><br />4. A key innovation in DeepEL is a self-validation mechanism that harnesses global contextual information, allowing LLMs to iteratively refine their predictions and better capture relationships among multiple entities within the same sentence.<br /><br />5. Extensive experiments on ten benchmark datasets demonstrate that DeepEL outperforms current state-of-the-art approaches, yielding an average 2.6% improvement in overall F1 score and a notable 4% gain on out-of-domain data, highlighting the benefits of deep integration of LLMs in EL tasks. <div>
arXiv:2511.14181v1 Announce Type: new 
Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC</title>
<link>https://arxiv.org/abs/2511.14230</link>
<guid>https://arxiv.org/abs/2511.14230</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammatical Error Correction, Arabic, multi-system approach, machine learning, linguistic error correction

<br /><br />Summary: This paper addresses the challenges of Grammatical Error Correction (GEC) in Arabic, a language with complex morphological and syntactic structures. The authors introduce the Arab Enhanced Edit Selection System Complication (ArbESC+), which is one of the first multi-system approaches to Arabic grammatical error correction. Unlike previous methods that employed individual models, ArbESC+ combines various models to leverage their strengths. The system gathers correction proposals from several models and represents these as numerical features. A classifier then analyzes these features to implement suitable corrections while ensuring quality through support techniques that filter overlapping corrections and assess decision reliability. The integration of models such as AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, along with text editing systems, yielded superior results compared to single models, achieving an F0.5 score of 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. This work represents a significant advancement in Arabic linguistic error correction, pushing forward the development of refined tools for users and researchers involved in Arabic text processing. <div>
arXiv:2511.14230v1 Announce Type: new 
Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuCPT: Music-related Natural Language Model Continued Pretraining</title>
<link>https://arxiv.org/abs/2511.14245</link>
<guid>https://arxiv.org/abs/2511.14245</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, music domain, data pipeline, token-level soft scoring, MusicSimpleQA<br /><br />Summary: Large language models (LLMs) generally excel in broad applications but struggle in specialized domains like music, particularly in the music-entertainment sector where the scale, purity, and alignment of training data are crucial. This study addresses these issues by constructing a massive 40 billion token music-related natural language corpus sourced from a combination of open-source and proprietary data. A domain-first data pipeline is implemented using a lightweight classifier to filter and weight relevant in-domain text, followed by stages of cleaning, de-duplication, and privacy-preserving masking. Additionally, the authors integrate multi-source music text with associated metadata, creating a richer and better-structured knowledge base. A novel training strategy is introduced using reference-model-based token-level soft scoring, applying a unified loss-ratio criterion for both data selection and dynamic down-weighting during optimization. This approach reduces noisy gradient contributions and strengthens task-aligned signals, enabling more efficient domain-specific continued pretraining and fine-tuning. To evaluate factual accuracy, the paper presents MusicSimpleQA, a benchmark based on short, single-answer prompts with automated agreement scoring. Comprehensive comparisons along different dimensions of data composition are conducted. Overall, the work advances scalable data and training methodologies alongside a reusable evaluation framework to support the development of domain-specific LLMs in the music domain. <div>
arXiv:2511.14245v1 Announce Type: new 
Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning</title>
<link>https://arxiv.org/abs/2511.14249</link>
<guid>https://arxiv.org/abs/2511.14249</guid>
<content:encoded><![CDATA[
<div> Keywords: automatic movie dubbing, Retrieve-Augmented, emotional representations, multimodal signals, speech generation

<br /><br />Summary: The study presents a new model called Authentic-Dubber for automatic movie dubbing, which generates speech from scripts while mimicking the speaker's timbre based on brief prompts and ensuring lip-sync with silent videos. Unlike existing methods that ignore director-actor interactions, this model emphasizes the dynamic collaboration essential in authentic dubbing workflows. It introduces a Retrieve-Augmented Director-Actor Interaction Learning scheme that includes three innovative mechanisms. First, it creates a multimodal Reference Footage library to simulate the director's guidance, utilizing Large Language Models (LLMs) for deep emotional comprehension across different media. Second, an Emotion-Similarity-based Retrieval-Augmentation strategy is proposed to help actors internalize relevant footage that aligns with target videos effectively. Third, a Progressive Graph-based speech generation approach is developed, which incrementally integrates the retrieved emotional knowledge. Together, these components enhance the dubbing process, leading to significant improvements in emotional expressiveness. The model's effectiveness is validated through both subjective and objective evaluations on the V2C Animation benchmark dataset. The code and demonstrations for Authentic-Dubber are made available online for further exploration and development. <div>
arXiv:2511.14249v1 Announce Type: new 
Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR</title>
<link>https://arxiv.org/abs/2511.14255</link>
<guid>https://arxiv.org/abs/2511.14255</guid>
<content:encoded><![CDATA[
<div> Keywords: AfriSpeech-MultiBench, African English, voice interfaces, ASR models, multimodal LLMs

<br /><br />Summary: Recent advancements in AI-driven voice interfaces, like Google's NotebookLM and OpenAI's speech-to-speech API, have sparked interest worldwide. However, there is a lack of evaluation models tailored to Africa's linguistic diversity. To address this gap, the authors introduce AfriSpeech-MultiBench, the first benchmarking suite focused on over 100 African English accents from 10+ countries across seven domains, including Finance and Medical. This benchmark evaluates various open and proprietary ASR and LLM systems using both spontaneous and structured speech from African speech datasets. The findings show that open-source ASR models perform well in spontaneous contexts but falter in noisy, non-native conversations. In contrast, multimodal LLMs demonstrate better accent robustness but face challenges with domain-specific named entities. Proprietary models yield high accuracy on clean speech, albeit with variance based on country and domain. Models specifically fine-tuned on African English show competitive accuracy and lower latency, beneficial for deployment. However, hallucination issues persist in most state-of-the-art models. This benchmark enables researchers and practitioners to better select voice technologies for African contexts, promoting inclusive voice applications for underrepresented communities. <div>
arXiv:2511.14255v1 Announce Type: new 
Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Guided Reasoning Compression</title>
<link>https://arxiv.org/abs/2511.14258</link>
<guid>https://arxiv.org/abs/2511.14258</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, compression methods, entropy conflict, efficient reasoning, mathematical benchmarks  
<br /><br />Summary: Large reasoning models excel in complex reasoning tasks, but the long outputs hinder their practical use due to high computational demands. While existing compression techniques have made progress, they often ignore a critical issue termed the entropy conflict. This occurs when the dual objectives of decreasing entropy for shorter reasoning conflict with accuracy goals that tend to increase entropy due to emphasis on logical connectors. These connectors receive significant gradients and are penalized during compression, leading to a local dilemma for the model. To resolve this issue, the authors propose an entropy-guided training framework that balances these conflicting objectives. As entropy decreases, the model learns to produce concise outputs, while increases in entropy promote exploration to enhance robustness. Experiments conducted across six mathematical benchmarks demonstrate that this method successfully reduces reasoning length to 20% of the original while either improving or maintaining baseline accuracy. The research promises to enhance the efficiency of reasoning models, and the authors plan to publicly release their code and models to facilitate further research. <div>
arXiv:2511.14258v1 Announce Type: new 
Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space</title>
<link>https://arxiv.org/abs/2511.14275</link>
<guid>https://arxiv.org/abs/2511.14275</guid>
<content:encoded><![CDATA[
<div> Keywords: reliability, verbalized confidence, chain-of-thought reasoning, probability distribution, large language models (LLMs)  

<br /><br />Summary:  
This paper addresses the importance of understanding the reliability of responses generated by large language models (LLMs). The researchers focus on enhancing confidence estimation by generating verbalized confidence scores, which communicate how certain a model is about its answers. They build on chain-of-thought reasoning methods that provide logical and transparent estimations, but highlight that the impact of different reasoning strategies on confidence estimation is still not well understood. The key contribution is demonstrating that predicting a verbalized probability distribution over all possible answers encourages deeper and more comprehensive reasoning. This approach compels the LLM to consider multiple candidate answers rather than relying on a single guess, assigning confidence values that form a coherent distribution. Experimental results show that this method consistently outperforms alternatives across different LLM architectures and various tasks, regardless of whether the answer space is predefined or not. Moreover, the advantage of this probability-distribution-based confidence estimation persists even after applying reinforcement learning fine-tuning. Additional analysis reveals that the reasoning patterns employed by the models align closely with human expectations, suggesting improved interpretability and trustworthiness of the model’s confidence outputs. <div>
arXiv:2511.14275v1 Announce Type: new 
Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2511.14295</link>
<guid>https://arxiv.org/abs/2511.14295</guid>
<content:encoded><![CDATA[
<div> Keywords: AraLingBench, Arabic, language models, linguistic competence, benchmark

<br /><br />Summary: The article introduces AraLingBench, a newly developed benchmark designed to assess the Arabic linguistic competence of large language models (LLMs). This benchmark encompasses five key areas: grammar, morphology, spelling, reading comprehension, and syntax, featuring 150 multiple-choice questions created by experts that evaluate structural understanding of the Arabic language. An evaluation of 35 Arabic and bilingual LLMs indicates that while these models often perform well on surface-level tasks, they face challenges in deeper grammatical and syntactic reasoning. The findings highlight a significant disconnect between high performance on knowledge-based benchmarks and true linguistic mastery, revealing that many models rely more on memorization or pattern recognition instead of genuine comprehension. By distinctly measuring essential linguistic capabilities, AraLingBench serves as a diagnostic tool for the advancement of Arabic LLMs. The full evaluation code for AraLingBench is made publicly available on GitHub, promoting transparency and facilitating further research in this important area of natural language processing. <div>
arXiv:2511.14295v1 Announce Type: new 
Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions</title>
<link>https://arxiv.org/abs/2511.14342</link>
<guid>https://arxiv.org/abs/2511.14342</guid>
<content:encoded><![CDATA[
<div> Keywords: Instruction-following, Large Language Models, conflict detection, ConInstruct, benchmarks

<br /><br />Summary: This paper introduces ConInstruct, a new benchmark designed to evaluate how well Large Language Models (LLMs) can detect and resolve conflicts within user instructions, especially when instructions contain opposing constraints. The importance of instruction-following in LLMs is highlighted, while noting that prior studies have largely neglected the issue of conflicting prompts, which are common in complex scenarios. Experiments conducted using this benchmark reveal two significant findings: (1) Most proprietary LLMs demonstrate robust conflict detection abilities; however, among open-source models, only DeepSeek-R1 shows comparable performance. In fact, DeepSeek-R1 and Claude-4.5-Sonnet achieve the top average F1-scores of 91.5% and 87.3%, respectively. (2) Despite possessing strong conflict detection skills, LLMs infrequently inform users about detected conflicts or seek clarification when faced with conflicting instructions. The study highlights these shortcomings in current LLM designs and emphasizes the need for improvement in future instruction-following capabilities. <div>
arXiv:2511.14342v1 Announce Type: new 
Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.14365</link>
<guid>https://arxiv.org/abs/2511.14365</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, tokenization bottleneck, SMILES, vocabulary extension, chemistry-domain tasks

<br /><br />Summary: The paper addresses a significant challenge in applying large language models (LLMs) to chemistry, known as the "tokenization bottleneck". This issue arises because standard tokenizers, designed for general text, inadequately process chemical representations like SMILES, breaking them into less meaningful sub-tokens. To tackle this problem, the authors propose a unified model that merges the representation of natural language with molecular structures. Their methodology includes expanding the vocabulary of a pretrained LLM by incorporating tokens that are particularly relevant to chemistry. Following this vocabulary enhancement, the model is subjected to continued pretraining using chemistry-specific texts to better assimilate this new information. The paper presents empirical results demonstrating that this integrated approach significantly enhances the model's performance across various downstream tasks in the chemistry domain. The findings suggest that by aligning LLM capabilities with the specific linguistic and structural features of chemical data, researchers can overcome the limitations imposed by conventional tokenization strategies, resulting in more effective and informative representations of chemical information. <div>
arXiv:2511.14365v1 Announce Type: new 
Abstract: The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning</title>
<link>https://arxiv.org/abs/2511.14366</link>
<guid>https://arxiv.org/abs/2511.14366</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, ATLAS, cross-disciplinary, high-difficulty, evaluation suite

<br /><br />Summary: The rapid progression of Large Language Models (LLMs) has led to performance saturation in established benchmarks, prompting a reevaluation of their capability to distinguish top models. Current high-difficulty benchmarks often have limited disciplinary focus and are susceptible to data contamination, leading to a disconnect with actual scientific inquiry. In response, the study presents ATLAS (AGI-Oriented Testbed for Logical Application in Science), an extensive assessment tool featuring around 800 original problems crafted by PhD-level domain experts across seven scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Key attributes of ATLAS include: (1) originality and resistance to contamination, with all questions newly created or adapted; (2) a cross-disciplinary approach that evaluates models’ integration of knowledge across various fields; (3) prioritization of complex, multi-step reasoning for answers, favoring detailed responses over simple multiple-choice formats; and (4) stringent quality control via expert peer review and adversarial testing to ensure question difficulty and scientific integrity. The proposed evaluation method also incorporates LLM judges for automated assessment, with preliminary results highlighting ATLAS's effectiveness in discerning advanced scientific reasoning skills in leading models. <div>
arXiv:2511.14366v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Label Length Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2511.14385</link>
<guid>https://arxiv.org/abs/2511.14385</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, label length bias, normalized contextual calibration, multi-token labels, few-shot learning<br /><br />Summary:<br /><br />Large language models (LLMs) are effective zero- and few-shot learners but face challenges when predicting over candidate options due to label biases. Among these biases, label length bias causes inconsistency in handling labels of varying token lengths, even after applying standard length normalization techniques. To address this, the authors introduce normalized contextual calibration (NCC), a novel method that normalizes and calibrates predictions at the full-label level instead of token-wise adjustments. NCC demonstrates statistically significant improvements over existing calibration methods across various datasets and models, achieving up to 10% F1 score gains. Additionally, NCC extends its bias mitigation capabilities to diverse tasks such as multiple-choice question answering, showcasing its versatility. The study reveals that combining NCC with in-context learning enhances stability by reducing sensitivity to the selection of few-shot examples and reduces the number of examples needed for competitive performance. Furthermore, NCC contributes to producing more reliable confidence estimates in predictions. Overall, the findings emphasize the importance of mitigating full-label biases in LLMs to enhance accuracy, robustness, and applicability in real-world scenarios where multi-token class labels are common. <div>
arXiv:2511.14385v1 Announce Type: new 
Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education</title>
<link>https://arxiv.org/abs/2511.14423</link>
<guid>https://arxiv.org/abs/2511.14423</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, educational safety, jailbreak attacks, fine-tuning attacks, three-stage shield framework  

<br /><br />Summary:  
Large Language Models (LLMs) are increasingly used in educational tools, but they face safety challenges from jailbreak and fine-tuning attacks that can produce harmful outputs. Existing research primarily addresses general safety without focusing on the specific needs of educational environments. To fill this gap, the authors introduce EduHarm, a benchmark dataset of safe versus unsafe instructional pairs covering five key educational scenarios, designed to systematically evaluate educational LLM safety. They also propose a novel three-stage shield framework (TSSF) to simultaneously defend against jailbreak and fine-tuning attacks. The first stage, safety-aware attention realignment, adjusts the model's focus to highlight critical unsafe tokens and restore harmfulness detection features. The second stage, layer-wise safety judgment, aggregates safety signals across multiple model layers to identify unsafe instructions accurately. The final stage, defense-driven dual routing, channels safe queries through normal processing and unsafe queries through protective responses, preventing harmful outputs while avoiding over-blocking benign inputs. Extensive experiments with eight jailbreak attack methods demonstrate TSSF’s effectiveness in enhancing safety and minimizing unnecessary refusals. Additional evaluations across three fine-tuning attack datasets confirm TSSF’s ability to maintain robust defense against harmful queries while preserving performance benefits from legitimate fine-tuning. <div>
arXiv:2511.14423v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents</title>
<link>https://arxiv.org/abs/2511.14439</link>
<guid>https://arxiv.org/abs/2511.14439</guid>
<content:encoded><![CDATA[
<div> Medical AI, benchmarking, large language models, multimodal models, clinical safety<br /><br />Summary:  
This paper introduces MedBench v4, a comprehensive, cloud-based benchmarking platform designed to evaluate medical AI models in realistic clinical workflows and safety contexts. It features over 700,000 expert-curated tasks covering 24 primary and 91 secondary medical specialties, with specific tracks for large language models (LLMs), multimodal models, and agentic systems. The tasks undergo rigorous refinement and are reviewed by clinicians from more than 500 institutions to ensure quality and clinical relevance. Open-ended model responses are rated using an LLM calibrated against human judgments. The authors evaluate 15 advanced models, finding that base LLMs achieve an average overall score of 54.1/100, with Claude Sonnet 4.5 performing best at 62.5/100, yet scores on safety and ethics remain notably low (18.4/100). Multimodal models show weaker cross-modal reasoning despite solid perception, averaging 47.5/100, with GPT-5 as the top performer at 54.9/100. Agent-based models built on these backbones significantly improve performance, reaching an average of 79.8/100 overall and up to 88.9/100 on safety tasks. The study highlights ongoing challenges in safety and multimodal reasoning for base models and demonstrates that governance-aware agents can enhance clinical readiness significantly. MedBench v4 aligns with Chinese clinical guidelines, offering a valuable auditing tool for healthcare providers, developers, and policymakers. <div>
arXiv:2511.14439v1 Announce Type: new 
Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning</title>
<link>https://arxiv.org/abs/2511.14445</link>
<guid>https://arxiv.org/abs/2511.14445</guid>
<content:encoded><![CDATA[
<div> Keywords: mental well-being, large language models, retrieval-augmented generation, synthetic dialogue, personalized self-care  

<br /><br />Summary:  
The article presents "Tell Me," a mental well-being system designed to provide accessible support leveraging advancements in large language models. The system comprises three core components: (i) a retrieval-augmented generation (RAG) assistant that facilitates personalized, knowledge-informed dialogue; (ii) a synthetic dialogue generator that creates client-therapist interactions based on client profiles, aiding research in therapeutic language and data augmentation; and (iii) the Well-being AI crew, which generates weekly self-care plans and guided meditation audio. Importantly, the system serves as a reflective space for emotional processing and does not replace professional therapy, highlighting its role in enhancing access to mental health resources. To address the scarcity of confidential therapeutic data, the authors introduce a synthetic dialogue generation approach. Additionally, the planner showcases a unique, adaptive workflow for personalized self-care, overcoming the limitations of static well-being tools. The paper describes the architecture and functionality of the system and shares the evaluation results of the RAG assistant through automatic judgments and human-user studies, emphasizing potential interdisciplinary collaboration between NLP researchers and mental health professionals for responsible human-AI interaction in well-being. <div>
arXiv:2511.14445v1 Announce Type: new 
Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.14460</link>
<guid>https://arxiv.org/abs/2511.14460</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, LLM Agents, Markov Decision Process, Agent-R1  

<br /><br />Summary: Large Language Models (LLMs) are increasingly used to build Agents capable of interactive problem-solving by engaging with their environments, such as through tool use. Reinforcement Learning (RL) holds substantial promise for training these LLM-based Agents, but applying RL effectively in this context remains an early-stage challenge. The current research landscape lacks detailed investigations of RL methods tailored specifically for LLM Agents, as well as flexible, extensible frameworks that support their training needs. Addressing these gaps, the paper first extends the Markov Decision Process (MDP) framework to define essential components unique to LLM Agents, providing a systematic foundation for RL methodology in this domain. Secondly, the authors introduce Agent-R1, a modular and adaptable training framework designed to be user-friendly and easily customizable across varied tasks and interactive environments. To validate their approach, the study conducts experiments on Multihop Question Answering benchmark tasks, demonstrating the initial effectiveness and practical potential of both the proposed RL methodologies and the Agent-R1 training framework. This work aims to accelerate progress in the field of RL-based LLM Agents by offering theoretical clarity and practical tools to support future research and application development. <div>
arXiv:2511.14460v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveRAG: A diverse Q&amp;A dataset with varying difficulty level for RAG evaluation</title>
<link>https://arxiv.org/abs/2511.14531</link>
<guid>https://arxiv.org/abs/2511.14531</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Augmented Generation, LiveRAG benchmark, Q&amp;A systems, Item Response Theory, evaluation

<br /><br />Summary: This article presents the LiveRAG benchmark, a new dataset designed for evaluating Retrieval Augmented Generation (RAG) based Q&amp;A systems. Comprising 895 synthetic questions and answers, this benchmark aims to facilitate systematic evaluation in the growing field of generative AI. It is derived from the dataset used in the SIGIR'2025 LiveRAG Challenge, and contains additional information, such as ground-truth answers and supporting claims, which were initially unavailable to competitors. Each question in the benchmark is accompanied by estimated difficulty and discriminability scores, derived using an Item Response Theory model based on competitors' responses. The analysis emphasizes the diversity of questions within the benchmark, the varying levels of difficulty, and its capacity to distinguish between different system capabilities. By offering a structured approach to assess RAG solutions, the LiveRAG benchmark hopes to promote advancements in RAG research, enable rigorous evaluation practices, and foster the development of more effective Q&amp;A systems within the AI community. <div>
arXiv:2511.14531v1 Announce Type: new 
Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&amp;A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&amp;A systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak</title>
<link>https://arxiv.org/abs/2511.14566</link>
<guid>https://arxiv.org/abs/2511.14566</guid>
<content:encoded><![CDATA[
<div> Keywords: document-level claim extraction, claim alignment, evaluation framework, semantic similarity, Czech and Slovak news

<br /><br />Summary:  
This work addresses the challenge of document-level claim extraction in fact-checking, focusing on the evaluation methods for the extracted claims. The authors propose an approach to align two sets of claims from the same source document and calculate an alignment score that reflects their similarity. They investigate techniques that identify the best possible alignment and evaluation methods between claim sets, aiming to establish a reliable evaluation framework. The proposed approach facilitates comparison between model-extracted claims and human-annotated claims, serving both as a performance metric for extraction models and a measure of inter-annotator agreement. Experiments are conducted on a newly collected dataset comprising claims extracted from comments on Czech and Slovak news articles, which present challenges like informal language, strong local context, and linguistic subtleties between the two closely related languages. Results highlight the limitations of current evaluation methods when applied to document-level claim extraction. The study emphasizes the need for advanced evaluation techniques capable of accurately capturing semantic similarity and assessing critical claim properties such as atomicity, checkworthiness, and decontextualization, essential for improving fact-checking systems. <div>
arXiv:2511.14566v1 Announce Type: new 
Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2511.14598</link>
<guid>https://arxiv.org/abs/2511.14598</guid>
<content:encoded><![CDATA[
<div> Keywords: summarization, under-represented languages, historical newspapers, HEBTEASESUM, automatic process

<br /><br />Summary: This work addresses the scarcity of high-quality summarization data in under-represented languages by leveraging historical newspapers, which have become accessible through recent digitization efforts. The authors propose a novel method for obtaining naturally occurring summaries using Front-Page Teasers, where newspaper editors provide concise summaries of full-length articles. This phenomenon is observed across seven diverse languages, demonstrating its potential for multi-document summarization tasks. To streamline the data collection process, the authors develop an automatic method tailored to various linguistic resource levels, making it adaptable to different contexts. The application of this automatic process is illustrated through the creation of HEBTEASESUM, marking the introduction of the first dedicated multi-document summarization dataset in Hebrew. Overall, this work highlights the value of historical newspapers as a rich source of summarization data and provides a systematic approach to harnessing this resource effectively for under-represented languages. <div>
arXiv:2511.14598v1 Announce Type: new 
Abstract: High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease</title>
<link>https://arxiv.org/abs/2511.14603</link>
<guid>https://arxiv.org/abs/2511.14603</guid>
<content:encoded><![CDATA[
<div> Keywords: acute kidney injury, chronic kidney disease, electronic health records, CKD risk factors, multi-state modeling  

<br /><br />Summary: The study investigates the risk of developing chronic kidney disease (CKD) in patients with acute kidney injury (AKI) using electronic health record (EHR) data. It aims to identify high-risk individuals by tracking their clinical progression after AKI. Researchers used patient data, including longitudinal medical codes and creatinine levels, to categorize post-AKI clinical states through clustering techniques. They employed multi-state modeling to estimate transition probabilities between different clinical states and the likelihood of progressing to CKD. Out of 20,699 patients with AKI at admission, 3,491 (17%) progressed to CKD. The analysis revealed fifteen distinct post-AKI states, each associated with varying CKD development probabilities. A significant portion of patients (75%, n=15,607) either remained in one state or transitioned minimally during the study. The study identified both established risk factors for CKD—such as AKI severity, diabetes, hypertension, heart failure, and liver disease—and novel factors whose impact differed across clinical states. This data-driven approach offers a foundation for developing decision-support tools aimed at early detection and intervention for CKD in vulnerable AKI patients. <div>
arXiv:2511.14603v1 Announce Type: new 
Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.14606</link>
<guid>https://arxiv.org/abs/2511.14606</guid>
<content:encoded><![CDATA[
<div> Keywords: political bias, natural language processing, large language models, RoBERTa, human annotations  

<br /><br />Summary:  
This study addresses the complex task of detecting political bias in news media, highlighting the necessity of interpreting linguistic and contextual cues. While advancements in Natural Language Processing (NLP) have paved the way for automatic bias classification, the alignment between large language models (LLMs) and human judgment remains underexplored. The authors propose a comparative framework that evaluates political bias detection using human annotations alongside multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. A manually annotated dataset of news articles was created to assess annotation consistency, bias polarity, and inter-model agreement. Findings indicate that RoBERTa, among traditional transformer models, demonstrates the highest alignment with human labels, while generative models like GPT show strong correspondence in a zero-shot setting. The fine-tuned RoBERTa model exhibited the best accuracy and alignment with human annotations. The results reveal systematic differences in bias perception between humans and LLMs, emphasizing the importance of hybrid evaluation frameworks that integrate human interpretability with the scalability of automated tools for media bias detection. <div>
arXiv:2511.14606v1 Announce Type: new 
Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities</title>
<link>https://arxiv.org/abs/2511.14631</link>
<guid>https://arxiv.org/abs/2511.14631</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, vision-language models, autonomous scientific discovery, exploratory data analysis, interpretability  

<br /><br />Summary: The article demonstrates that multi-agent systems enhanced by vision-language models (VLMs) significantly improve end-to-end autonomous scientific discovery. These systems utilize plots as verifiable checkpoints where a VLM acts as a judge, evaluating figures based on dynamically generated, domain-specific rubrics. This capability allows the agents to identify and correct their own errors, steering exploratory data analysis in real-time. Case studies conducted in the fields of cosmology and astrochemistry illustrate how these systems can recover from faulty reasoning paths and adapt to new datasets without requiring human intervention. In a benchmark comprising 10 tasks related to data-driven discovery, VLM-augmented systems achieved pass-at-1 scores ranging from 0.7-0.8. This performance is significantly better compared to code-only systems, which scored 0.2-0.3, and code-and-text baselines, which had scores of 0.4-0.5. Additionally, these systems provide auditable reasoning traces, enhancing interpretability and allowing for a better understanding of the decision-making process. A link to the associated code is available for further exploration. <div>
arXiv:2511.14631v1 Announce Type: new 
Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</title>
<link>https://arxiv.org/abs/2511.14638</link>
<guid>https://arxiv.org/abs/2511.14638</guid>
<content:encoded><![CDATA[
<div> Rare diseases, diagnosis, large language models, electronic health records, clinical decision support  

<br /><br />Summary:  
This study addresses the challenge of diagnosing rare diseases, which affect millions but often take years to identify. Conventional diagnostic methods separate noisy evidence extraction from downstream inferential reasoning, limiting accuracy and efficiency. Large language models (LLMs), both general and medical, struggle due to limited real-world electronic health record (EHR) data, outdated domain knowledge, and hallucination problems. To overcome these issues, the authors compiled a large, domain-specific clinical corpus alongside a clinician-validated reasoning dataset. They developed RareSeek R1 using staged instruction tuning, chain-of-thought learning, and graph-grounded retrieval techniques. RareSeek R1 demonstrated state-of-the-art accuracy, strong generalization across multicenter EHR narratives, and resilience to noisy or overlapping phenotypes. The system’s augmented retrieval, especially when combining narratives with prioritized genetic variants, resolved ambiguities and aligned candidate diagnoses with underlying mechanisms, producing the greatest performance improvements. Human evaluation showed RareSeek R1 matches experienced physicians in diagnostic ability and provides consistent gains when used as an assistive tool. Importantly, its transparent reasoning models non-phenotypic evidence, such as imaging and functional tests, which supported nearly a quarter of correct diagnoses. This work pioneers a narrative-first, knowledge-integrated reasoning paradigm advancing clinically translatable, auditable diagnostic decision support that can shorten the diagnostic odyssey for rare diseases. <div>
arXiv:2511.14638v1 Announce Type: new 
Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graded strength of comparative illusions is explained by Bayesian inference</title>
<link>https://arxiv.org/abs/2511.14642</link>
<guid>https://arxiv.org/abs/2511.14642</guid>
<content:encoded><![CDATA[
<div> Comparative Illusion, Noisy Channel, Bayesian Inference, Language Processing, Statistical Language Models<br /><br />Summary:<br /><br />1. The article investigates the comparative illusion (CI) in language processing, where people often accept sentences that contain nonsensical comparisons, such as "More students have been to Russia than I have."  
2. Prior work suggested that this phenomenon can be explained by Bayesian inference over a noisy communication channel, where the brain infers the most probable intended sentence by combining prior interpretation likelihoods with the chance of corruption leading to the observed CI sentence.  
3. Initial behavioral evidence supported this explanation by showing that comprehenders tend to favor interpretations likely to be corrupted into the illusory CI sentence.  
4. This study advances the theory by quantitatively modeling the posterior probabilities of plausible CI sentence interpretations, integrating statistical language models with human behavioral data for more precise predictions.  
5. The model accounts not only for subtle variations in CI strength but also explains a previously unexplained effect regarding differences between pronominal and full noun phrase than-clause subjects.  
6. These results provide strong empirical support for the noisy-channel theory as a unified computational-level account of diverse language processing phenomena, encompassing both illusory and non-illusory contexts. <div>
arXiv:2511.14642v1 Announce Type: new 
Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias in, Bias out: Annotation Bias in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2511.14662</link>
<guid>https://arxiv.org/abs/2511.14662</guid>
<content:encoded><![CDATA[
<div> Annotation bias, multilingual LLMs, cultural diversity, bias detection, bias mitigation  

<br /><br />Summary:  
The article addresses annotation bias in Natural Language Processing (NLP) datasets, highlighting its critical impact on the development of multilingual Large Language Models (LLMs), especially in culturally diverse environments. It introduces a comprehensive framework categorizing annotation bias into instruction bias, annotator bias, and contextual/cultural bias. The paper reviews existing detection methods such as inter-annotator agreement, model disagreement, and metadata analysis, while also presenting emerging techniques like multilingual model divergence and cultural inference. For bias mitigation, the authors propose both proactive and reactive strategies, including diverse annotator recruitment, continuous guideline refinement, and post-hoc adjustments to models. Key contributions comprise (1) establishing a typology of annotation bias, (2) synthesizing various detection metrics, (3) proposing an ensemble-based bias mitigation method tailored for multilingual contexts, and (4) providing an ethical analysis of annotation workflows. Collectively, these insights endeavor to foster more equitable, culturally informed annotation practices, thereby improving the fairness and performance of multilingual LLMs. <div>
arXiv:2511.14662v1 Announce Type: new 
Abstract: Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Streamlining Industrial Contract Management with Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2511.14671</link>
<guid>https://arxiv.org/abs/2511.14671</guid>
<content:encoded><![CDATA[
<div> Contract management, retrieval-augmented generation, synthetic data, acceptability classification, reward-based alignment<br /><br />Summary:  
This paper addresses the challenge of automating contract management, particularly focusing on the review and negotiation of contract provisions that define rights, obligations, and terms. The authors highlight the difficulty posed by the scarcity of labeled data and the abundance of unstructured legacy contracts during this process. To overcome these challenges, they propose a modular framework based on a retrieval-augmented generation (RAG) pipeline. Key components of their system include synthetic data generation to augment training resources, semantic clause retrieval to efficiently locate relevant contract provisions, acceptability classification to identify problematic or unacceptable revisions, and reward-based alignment to generate improved alternative clauses. The framework was developed and evaluated in partnership with an industry collaborator, ensuring its practical relevance. Experimental results demonstrate that the system achieves over 80% accuracy in both detecting problematic revisions and suggesting optimized alternatives. This performance underscores the method’s effectiveness in real-world, low-resource environments. Ultimately, the proposed framework offers a viable solution to accelerate contract revision workflows, reducing manual effort and improving the quality of contractual agreements. <div>
arXiv:2511.14671v1 Announce Type: new 
Abstract: Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quadratic Term Correction on Heaps' Law</title>
<link>https://arxiv.org/abs/2511.14683</link>
<guid>https://arxiv.org/abs/2511.14683</guid>
<content:encoded><![CDATA[
<div> Keywords: Heaps' law, word-type, word-token, power-law, quadratic functions  
  
<br /><br />Summary: Heaps' or Herdan's law describes the relationship between word types and word tokens using a power-law function that appears concave in linear-linear scale, yet is linear in log-log scale. Despite this, evidence suggests that even on the log-log scale, the type-token curve maintains slight concavity, challenging the validity of the power-law approximation. An analysis of twenty English novels and translated texts indicates that quadratic functions offer a superior fit for the type-token data in log-log scale. Regression analyses reveal that the linear coefficient is slightly greater than 1, while the quadratic coefficient hovers around -0.02. To further understand the underlying mechanics, the authors employ a model involving "random drawing of colored balls from a bag with replacement," establishing that the curvature seen in the log-log scale corresponds to a negative "pseudo-variance." Although calculating pseudo-variance can lead to numerical instability with large token counts, this framework provides a rough estimate of curvature when the number of tokens is smaller, enhancing our grasp of linguistic distribution patterns. <div>
arXiv:2511.14683v1 Announce Type: new 
Abstract: Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction</title>
<link>https://arxiv.org/abs/2511.14684</link>
<guid>https://arxiv.org/abs/2511.14684</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning errors, SMRC, Monte Carlo Tree Search, educational applicability

<br /><br />Summary: Large language models (LLMs) frequently encounter reasoning errors in mathematical problem-solving, raising the need for automated detection and correction methods. Traditional approaches emphasize self-correction, which is inadequate for educational contexts requiring "teacher-style" guidance. To address this, the authors propose SMRC (Student Mathematical Reasoning Correction), a method that aligns LLMs with student reasoning. SMRC conceptualizes student reasoning as a multi-step sequential decision problem and employs Monte Carlo Tree Search (MCTS) to identify optimal correction paths. To minimize the annotation cost of process-level rewards, the method uses breadth-first search (BFS) accompanied by LLMs and evaluations of final answers to generate reward signals, which are back-propagated across intermediate reasoning steps for enhanced supervision. Additionally, the authors introduce the Multi-Solution Error Benchmark (MSEB), a dataset featuring 158 high school mathematics problems, student solutions, and correct reasoning steps. A dual evaluation protocol focusing on solution accuracy and correct-step retention is proposed to measure educational effectiveness comprehensively. Experiments reveal that SMRC greatly surpasses existing methods on two established datasets (ProcessBench and MR-GSM8K) and the newly created MSEB concerning effectiveness and overall performance. The relevant code and data are accessible at https://github.com/Mind-Lab-ECNU/SMRC. <div>
arXiv:2511.14684v1 Announce Type: new 
Abstract: Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries</title>
<link>https://arxiv.org/abs/2511.14685</link>
<guid>https://arxiv.org/abs/2511.14685</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, astrophysics, embeddings, prompting, autoencoders  

<br /><br />Summary: This paper explores the ability of Large Language Models (LLMs) to generalize across various domains and their potential to encode physical information. Specifically, it uses astrophysics as a case study to determine two main aspects: first, whether the way a model is prompted influences how it codifies physical summary statistics derived from scientific measurements, and second, which elements of language are most critical for accurately representing the physics involved in these measurements. To investigate these questions, the authors employ sparse autoencoders, which help extract interpretable features from textual descriptions. By analyzing how LLM embeddings can encapsulate scientific data, the study highlights the intersection between language and scientific measurement, providing insights into both prompting techniques and the linguistic features that best convey physical concepts. The research aims to contribute to a deeper understanding of how LLMs can integrate and represent complex scientific information, which is often challenging to express solely through text. <div>
arXiv:2511.14685v1 Announce Type: new 
Abstract: Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ground Truth Generation for Multilingual Historical NLP using LLMs</title>
<link>https://arxiv.org/abs/2511.14688</link>
<guid>https://arxiv.org/abs/2511.14688</guid>
<content:encoded><![CDATA[
<div> Historical NLP, low-resource languages, large language models, French historical texts, Chinese historical texts<br /><br />Summary:<br /><br />This paper addresses the challenges in historical and low-resource natural language processing (NLP), primarily caused by limited annotated data and domain mismatches with modern datasets derived from the web. It focuses on historical French texts from the 16th to the 20th century and Chinese texts from 1900 to 1950. The authors utilize large language models (LLMs) to generate ground-truth annotations, which are otherwise difficult to obtain for such under-resourced corpora. By leveraging LLM-generated annotations on a subset of the corpus, the team fine-tuned the spaCy NLP framework to enhance its performance. This fine-tuning led to significant improvements in key NLP tasks, including part-of-speech tagging, lemmatization, and named entity recognition, specifically tailored for the historical period texts under study. The findings emphasize the value of domain-specific modeling rather than relying solely on general modern models. Furthermore, the research demonstrates that even a relatively small amount of synthetic, LLM-generated data can substantially improve the quality of NLP tools applicable to computational humanities research involving historical documents. <div>
arXiv:2511.14688v1 Announce Type: new 
Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances</title>
<link>https://arxiv.org/abs/2511.14693</link>
<guid>https://arxiv.org/abs/2511.14693</guid>
<content:encoded><![CDATA[
<div> Keywords: complaint analysis, multimodal, VALOR, semantic alignment, SDG goals

<br /><br />Summary: Existing methods for complaint analysis typically focus on unimodal text sources like tweets or reviews. This study introduces VALOR, a novel framework designed for multimodal customer support dialogues that include both text and visual evidence for enhanced complaint classification. VALOR employs a multi-expert reasoning approach with large-scale generative models that utilize Chain-of-Thought (CoT) prompting for intricate decision-making. To ensure coherence across text and visual modalities, a semantic alignment score is calculated and incorporated into the final classification results through a meta-fusion strategy. The framework aligns with the United Nations Sustainable Development Goals (UN SDGs), specifically contributing to SDG 9 by fostering AI-driven tools for scalable service infrastructure and to SDG 12 by enabling structured analysis of complaints for better product design and accountability. VALOR is evaluated on a carefully curated multimodal dataset with fine-grained aspect and severity annotations and shows consistent performance improvements over baseline models, particularly in complex cases where information is dispersed across both text and images. This research highlights the significance of multimodal interaction and expert validation in enhancing complaint understanding systems. <div>
arXiv:2511.14693v1 Announce Type: new 
Abstract: Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subword Tokenization Strategies for Kurdish Word Embeddings</title>
<link>https://arxiv.org/abs/2511.14696</link>
<guid>https://arxiv.org/abs/2511.14696</guid>
<content:encoded><![CDATA[
<div> Keywords: Kurdish, tokenization, morpheme-based, Word2Vec, low-resource 

<br /><br />Summary: This article explores different tokenization strategies for creating Kurdish word embeddings, specifically comparing word-level, morpheme-based, and Byte Pair Encoding (BPE) approaches. The research develops a BiLSTM-CRF morphological segmenter using a bootstrapped training method with minimal manual annotation and assesses Word2Vec embeddings through various metrics such as similarity preservation, clustering quality, and semantic organization. A significant finding of the study is the identification of critical evaluation biases present in the tokenization comparison. Although BPE initially seems to outperform other methods in preserving morphological similarity, it evaluates only 28.6% of test cases compared to 68.7% for the morpheme-based approach, leading to artificially inflated performance metrics for BPE. Upon a more comprehensive assessment, the morpheme-based tokenization demonstrates better organization in the embedding space, superior semantic neighborhood structures, and more balanced coverage across different levels of morphological complexity. The findings underscore the importance of coverage-aware evaluations in processing low-resource languages and present viable tokenization methods tailored for such language processing tasks. <div>
arXiv:2511.14696v1 Announce Type: new 
Abstract: We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&amp;D, and Ethical Governance</title>
<link>https://arxiv.org/abs/2511.14709</link>
<guid>https://arxiv.org/abs/2511.14709</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, R&amp;D processes, knowledge discovery, innovation ecosystems, hypothesis creation<br /><br />Summary: This study explores the transformative impact of Large Language Models (LLMs) on research and development (R&amp;D) workflows. First, LLMs automate the process of knowledge discovery, enabling faster and more comprehensive gathering of relevant information from vast scientific literature and patent databases. Second, these models enhance hypothesis creation, supporting researchers in generating innovative ideas and research questions. Third, LLMs integrate transdisciplinary insights by synthesizing information across multiple fields, fostering broader and more creative approaches to problem-solving. Fourth, they facilitate cooperation within innovation ecosystems by improving communication and collaboration among diverse stakeholders involved in the R&amp;D process. Lastly, through these capabilities, LLMs significantly increase the efficiency and effectiveness of R&amp;D, accelerating innovation cycles and reducing the time-to-market for novel breakthroughs. By enabling more flexible and informed workflows based on extensive data analysis, LLMs are positioned to revolutionize how research is conducted and how new technologies emerge. <div>
arXiv:2511.14709v1 Announce Type: new 
Abstract: This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&amp;D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&amp;D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title>
<link>https://arxiv.org/abs/2511.13788</link>
<guid>https://arxiv.org/abs/2511.13788</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, adversarial interactions, model size ratio, jailbreak, alignment integrity<br /><br />Summary:<br /><br />This study investigates how vulnerabilities in large language models (LLMs) scale when models interact adversarially, particularly focusing on whether larger LLMs can jailbreak smaller ones to induce harmful or restricted behaviors despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, the authors simulate over 6,000 multi-turn attacker-target interactions across major LLM families ranging from 0.6 billion to 120 billion parameters. They measure harm and refusal behaviors as indicators of adversarial potency and alignment integrity, respectively. Each interaction is evaluated by three independent LLM judges to provide consistent, model-based assessments of outcomes. The results show a strong and statistically significant correlation between the mean harm score and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating size asymmetry increases the likelihood and severity of harmful completions. Variance in harm scores is higher among attacker models (0.18) than target models (0.10), suggesting that attacker behavior diversity has a larger effect on outcomes than the inherent susceptibility of targets. Additionally, there is a strong negative correlation (rho = -0.93, p < 0.001) between attacker refusal frequency and harm, implying that alignment on the attacker side can effectively mitigate harmful responses. These findings highlight the importance of relative model size in robustness and encourage further research into inter-model alignment and safety in adversarial settings. <div>
arXiv:2511.13788v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rdgai: Classifying transcriptional changes using Large Language Models with a test case from an Arabic Gospel tradition</title>
<link>https://arxiv.org/abs/2511.13801</link>
<guid>https://arxiv.org/abs/2511.13801</guid>
<content:encoded><![CDATA[
<div> Keywords: phylogenetics, textual traditions, Bayesian, classification, LLMs  

<br /><br />Summary: The article discusses the application of phylogenetic methods to textual traditions, emphasizing the need for a nuanced approach to changes in readings, as not all variations carry equal significance. It highlights the challenge of assigning weights to different types of changes within traditional maximum parsimony frameworks. In contrast, Bayesian phylogenetics provides a robust method for categorizing changes, estimating transition rates, and analyzing the probability of these variants across phylogenetic trees. However, the classification process is labor-intensive, necessitating a comparison of each reading against every other at each variation unit. To streamline this task, the authors introduce Rdgai, a software package that automates classification using multi-lingual large language models (LLMs). Rdgai allows users to manually classify some changes, after which it leverages these annotations to classify remaining transitions automatically. The result is a structured output stored in TEI XML, facilitating further phylogenetic analysis. The paper demonstrates this software with an application involving an Arabic translation of the Gospels, illustrating its practical utility in the field. <div>
arXiv:2511.13801v1 Announce Type: cross 
Abstract: Application of phylogenetic methods to textual traditions has traditionally treated all changes as equivalent even though it is widely recognized that certain types of variants were more likely to be introduced than others. While it is possible to give weights to certain changes using a maximum parsimony evaluation criterion, it is difficult to state a priori what these weights should be. Probabilistic methods, such as Bayesian phylogenetics, allow users to create categories of changes, and the transition rates for each category can be estimated as part of the analysis. This classification of types of changes in readings also allows for inspecting the probability of these categories across each branch in the resulting trees. However, classification of readings is time-consuming, as it requires categorizing each reading against every other reading at each variation unit, presenting a significant barrier to entry for this kind of analysis. This paper presents Rdgai, a software package that automates this classification task using multi-lingual large language models (LLMs). The tool allows users to easily manually classify changes in readings and then it uses these annotations in the prompt for an LLM to automatically classify the remaining reading transitions. These classifications are stored in TEI XML and ready for downstream phylogenetic analysis. This paper demonstrates the application with data an Arabic translation of the Gospels.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology</title>
<link>https://arxiv.org/abs/2511.13825</link>
<guid>https://arxiv.org/abs/2511.13825</guid>
<content:encoded><![CDATA[
<div> Agentic AI, KOSMOS, radiation biology, gene expression, hypothesis testing<br /><br />Summary:<br /><br />This study evaluates KOSMOS, an autonomous AI scientist, for its ability to generate and test hypotheses in radiation biology. Three distinct hypotheses were tested: (1) whether baseline DNA damage response (DDR) capacity predicts the p53 transcriptional response after irradiation; (2) whether baseline expression of OGT and CDO1 genes predicts radiation response modules in breast cancer cells; and (3) whether a 12-gene expression signature predicts biochemical recurrence-free survival in prostate cancer patients undergoing radiotherapy and androgen deprivation therapy. The first hypothesis regarding DDR and p53 response was refuted, as the correlation was weakly negative and not statistically significant, comparable to random gene sets. For the second hypothesis, OGT showed a weak and uncertain correlation, but CDO1 demonstrated a strong and significant association with radiation response, marking a well-supported finding. The third hypothesis yielded a moderate predictive power with a concordance index of 0.61 and a statistically significant p-value, though the effect size was not unique. These results show that while KOSMOS can generate valuable hypotheses, its outputs must be rigorously audited against appropriate null models to distinguish meaningful discoveries from false positives or uncertain results. This study highlights the promise and limitations of AI-driven scientific discovery in complex biological problems. <div>
arXiv:2511.13825v1 Announce Type: cross 
Abstract: Agentic AI "scientists" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation</title>
<link>https://arxiv.org/abs/2511.13948</link>
<guid>https://arxiv.org/abs/2511.13948</guid>
<content:encoded><![CDATA[
arXiv:2511.13948v1 Announce Type: cross 
Abstract: Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation</title>
<link>https://arxiv.org/abs/2511.13972</link>
<guid>https://arxiv.org/abs/2511.13972</guid>
<content:encoded><![CDATA[
arXiv:2511.13972v1 Announce Type: cross 
Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance</title>
<link>https://arxiv.org/abs/2511.14043</link>
<guid>https://arxiv.org/abs/2511.14043</guid>
<content:encoded><![CDATA[
arXiv:2511.14043v1 Announce Type: cross 
Abstract: AI Scientific Assistant Core (AISAC) is an integrated multi-agent system developed at Argonne National Laboratory for scientific and engineering workflows. AISAC builds on established technologies - LangGraph for orchestration, FAISS for vector search, and SQLite for persistence - and integrates them into a unified system prototype focused on transparency, provenance tracking, and scientific adaptability.
  The system implements a Router-Planner-Coordinator workflow and an optional Evaluator role, using prompt-engineered agents coordinated via LangGraph's StateGraph and supported by helper agents such as a Researcher. Each role is defined through custom system prompts that enforce structured JSON outputs. A hybrid memory approach (FAISS + SQLite) enables both semantic retrieval and structured conversation history. An incremental indexing strategy based on file hashing minimizes redundant re-embedding when scientific corpora evolve. A configuration-driven project bootstrap layer allows research teams to customize tools, prompts, and data sources without modifying core code.
  All agent decisions, tool invocations, and retrievals are logged and visualized through a custom Gradio interface, providing step-by-step transparency for each reasoning episode. The authors have applied AISAC to multiple research areas at Argonne, including specialized deployments for waste-to-products research and energy process safety, as well as general-purpose scientific assistance, demonstrating its cross-domain applicability.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards</title>
<link>https://arxiv.org/abs/2511.14045</link>
<guid>https://arxiv.org/abs/2511.14045</guid>
<content:encoded><![CDATA[
arXiv:2511.14045v1 Announce Type: cross 
Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Driven Scene Editing for 3D Grounding in Large Language Models</title>
<link>https://arxiv.org/abs/2511.14086</link>
<guid>https://arxiv.org/abs/2511.14086</guid>
<content:encoded><![CDATA[
arXiv:2511.14086v1 Announce Type: cross 
Abstract: Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval</title>
<link>https://arxiv.org/abs/2511.14130</link>
<guid>https://arxiv.org/abs/2511.14130</guid>
<content:encoded><![CDATA[
arXiv:2511.14130v1 Announce Type: cross 
Abstract: With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning</title>
<link>https://arxiv.org/abs/2511.14299</link>
<guid>https://arxiv.org/abs/2511.14299</guid>
<content:encoded><![CDATA[
arXiv:2511.14299v1 Announce Type: cross 
Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion</title>
<link>https://arxiv.org/abs/2511.14301</link>
<guid>https://arxiv.org/abs/2511.14301</guid>
<content:encoded><![CDATA[
arXiv:2511.14301v1 Announce Type: cross 
Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciRAG: Adaptive, Citation-Aware, and Outline-Guided Retrieval and Synthesis for Scientific Literature</title>
<link>https://arxiv.org/abs/2511.14362</link>
<guid>https://arxiv.org/abs/2511.14362</guid>
<content:encoded><![CDATA[
arXiv:2511.14362v1 Announce Type: cross 
Abstract: The accelerating growth of scientific publications has intensified the need for scalable, trustworthy systems to synthesize knowledge across diverse literature. While recent retrieval-augmented generation (RAG) methods have improved access to scientific information, they often overlook citation graph structure, adapt poorly to complex queries, and yield fragmented, hard-to-verify syntheses. We introduce SciRAG, an open-source framework for scientific literature exploration that addresses these gaps through three key innovations: (1) adaptive retrieval that flexibly alternates between sequential and parallel evidence gathering; (2) citation-aware symbolic reasoning that leverages citation graphs to organize and filter supporting documents; and (3) outline-guided synthesis that plans, critiques, and refines answers to ensure coherence and transparent attribution. Extensive experiments across multiple benchmarks such as QASA and ScholarQA demonstrate that SciRAG outperforms prior systems in factual accuracy and synthesis quality, establishing a new foundation for reliable, large-scale scientific knowledge aggregation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model</title>
<link>https://arxiv.org/abs/2511.14368</link>
<guid>https://arxiv.org/abs/2511.14368</guid>
<content:encoded><![CDATA[
arXiv:2511.14368v1 Announce Type: cross 
Abstract: While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAIST Academic Travelogue Dataset</title>
<link>https://arxiv.org/abs/2305.11444</link>
<guid>https://arxiv.org/abs/2305.11444</guid>
<content:encoded><![CDATA[
arXiv:2305.11444v2 Announce Type: replace 
Abstract: We have constructed NAIST Academic Travelogue Dataset (ATD) and released it free of charge for academic research. This dataset is a Japanese text dataset with a total of over 31 million words, comprising 4,672 Japanese domestic travelogues and 9,607 overseas travelogues. Before providing our dataset, there was a scarcity of widely available travelogue data for research purposes, and each researcher had to prepare their own data. This hinders the replication of existing studies and fair comparative analysis of experimental results. Our dataset enables any researchers to conduct investigation on the same data and to ensure transparency and reproducibility in research. In this paper, we describe the academic significance, characteristics, and prospects of our dataset.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linguistic Structure from a Bottleneck on Sequential Information Processing</title>
<link>https://arxiv.org/abs/2405.12109</link>
<guid>https://arxiv.org/abs/2405.12109</guid>
<content:encoded><![CDATA[
arXiv:2405.12109v3 Announce Type: replace 
Abstract: Human language has a distinct systematic structure, where utterances break into individually meaningful words which are combined to form phrases. We show that natural-language-like systematicity arises in codes that are constrained by a statistical measure of complexity called predictive information, also known as excess entropy. Predictive information is the mutual information between the past and future of a stochastic process. In simulations, we find that such codes break messages into groups of approximately independent features which are expressed systematically and locally, corresponding to words and phrases. Next, drawing on crosslinguistic text corpora, we find that actual human languages are structured in a way that reduces predictive information compared to baselines at the levels of phonology, morphology, syntax, and lexical semantics. Our results establish a link between the statistical and algebraic structure of language and reinforce the idea that these structures are shaped by communication under general cognitive constraints.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance</title>
<link>https://arxiv.org/abs/2406.17385</link>
<guid>https://arxiv.org/abs/2406.17385</guid>
<content:encoded><![CDATA[
arXiv:2406.17385v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at providing information acquired during pretraining on large-scale corpora and following instructions through user prompts. This study investigates whether the quality of LLM responses varies depending on the demographic profile of users. Considering English as the global lingua franca, along with the diversity of its dialects among speakers of different native languages, we explore whether non-native English speakers receive lower-quality or even factually incorrect responses from LLMs more frequently. Our results show that performance discrepancies occur when LLMs are prompted by native versus non-native English speakers and persist when comparing native speakers from Western countries with others. Additionally, we find a strong anchoring effect when the model recognizes or is made aware of the user's nativeness, which further degrades the response quality when interacting with non-native speakers. Our analysis is based on a newly collected dataset with over 12,000 unique annotations from 124 annotators, including information on their native language and English proficiency.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2408.14595</link>
<guid>https://arxiv.org/abs/2408.14595</guid>
<content:encoded><![CDATA[
arXiv:2408.14595v2 Announce Type: replace 
Abstract: Multimodal foundation models (MFMs) such as OFASys show the potential to unlock analysis of complex data such as images, videos, and audio data via text prompts alone. However, their performance may suffer in the face of text input that differs even slightly from their training distribution, which is surprising considering the use of modality-specific data to "ground" the text input. This study demonstrates that prompt instability is a major concern for MFMs, leading to a consistent drop in performance across all modalities, but that instability can be mitigated with additional training with augmented data. We evaluate several methods for grounded prompt perturbation, where we generate perturbations and filter based on similarity to text and/or modality data. After re-training the models on the augmented data, we find improved accuracy and more stable performance on the perturbed test data regardless of perturbation condition, suggesting that the data augmentation strategy helps the models handle domain shifts more effectively. In error analysis, we find consistent patterns of performance improvement across domains, suggesting that retraining on prompt perturbations tends to help general reasoning capabilities in MFMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of OpenAI o1: Opportunities and Challenges of AGI</title>
<link>https://arxiv.org/abs/2409.18486</link>
<guid>https://arxiv.org/abs/2409.18486</guid>
<content:encoded><![CDATA[
arXiv:2409.18486v3 Announce Type: replace 
Abstract: This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include:
  -83.3% success rate in solving complex competitive programming problems, surpassing many human experts.
  -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models.
  -100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions.
  -Advanced natural language inference capabilities across general and specialized domains like medicine.
  -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis.
  -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields.
  -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills.
  -Effective performance in social media analysis, including sentiment analysis and emotion recognition.
  The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a Continuum</title>
<link>https://arxiv.org/abs/2410.14589</link>
<guid>https://arxiv.org/abs/2410.14589</guid>
<content:encoded><![CDATA[
arXiv:2410.14589v2 Announce Type: replace 
Abstract: There is increasing interest in looking at dialects in NLP. However, most work to date still treats dialects as discrete categories. For instance, evaluative work in variation-oriented NLP for English often works with Indian English or African-American Venacular English as homogeneous categories (Faisal et al., 2024; Ziems et al., 2023), yet even within one variety there is substantial variation. We examine within-dialect variation and show that performance critically varies within categories. We measure speech-to-text performance on Italian dialects, and empirically observe a geographical performance disparity. This disparity correlates substantially (-0.5) with linguistic similarity to the highest performing dialect variety. We cross-examine our results against dialectometry methods, and interpret the performance disparity to be due to a bias towards dialects that are more similar to the standard variety in the speech-to-text model examined. We additionally leverage geostatistical methods to predict zero-shot performance at unseen sites, and find the incorporation of geographical information to substantially improve prediction performance, indicating there to be geographical structure in the performance distribution.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games</title>
<link>https://arxiv.org/abs/2410.21359</link>
<guid>https://arxiv.org/abs/2410.21359</guid>
<content:encoded><![CDATA[
arXiv:2410.21359v3 Announce Type: replace 
Abstract: As Large Language Model (LLM)-based agents increasingly engage with human society, how well do we understand their prosocial behaviors? We (1) investigate how LLM agents' prosocial behaviors can be induced by different personas and benchmarked against human behaviors; and (2) introduce a social science approach to evaluate LLM agents' decision-making. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. The findings reveal that merely assigning a human-like identity to LLMs does not produce human-like behaviors. These findings suggest that LLM agents' reasoning does not consistently exhibit textual markers of human decision-making in dictator games and that their alignment with human behavior varies substantially across model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern. As society increasingly integrates machine intelligence, "Prosocial AI" emerges as a promising and urgent research direction in philanthropic studies.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning -- Natural Language Processing: From Theory to Application</title>
<link>https://arxiv.org/abs/2411.05026</link>
<guid>https://arxiv.org/abs/2411.05026</guid>
<content:encoded><![CDATA[
arXiv:2411.05026v3 Announce Type: replace 
Abstract: With a focus on natural language processing (NLP) and the role of large language models (LLMs), we explore the intersection of machine learning, deep learning, and artificial intelligence. As artificial intelligence continues to revolutionize fields from healthcare to finance, NLP techniques such as tokenization, text classification, and entity recognition are essential for processing and understanding human language. This paper discusses advanced data preprocessing techniques and the use of frameworks like Hugging Face for implementing transformer-based models. Additionally, it highlights challenges such as handling multilingual data, reducing bias, and ensuring model robustness. By addressing key aspects of data processing and model fine-tuning, this work aims to provide insights into deploying effective and ethically sound AI solutions.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial intelligence contribution to translation industry: looking back and forward</title>
<link>https://arxiv.org/abs/2411.19855</link>
<guid>https://arxiv.org/abs/2411.19855</guid>
<content:encoded><![CDATA[
arXiv:2411.19855v4 Announce Type: replace 
Abstract: This study provides a comprehensive analysis of artificial intelligence (AI) contribution to research in the translation industry (ACTI), synthesizing it over forty-five years from 1980-2024. 13220 articles were retrieved from three sources, namely WoS, Scopus, and Lens; 9836 were unique records, which were used for the analysis. We provided two types of analysis, viz., scientometric and thematic, focusing on Cluster, Subject categories, Keywords, Bursts, Centrality and Research Centers as for the former. For the latter, we provided a thematic review for 18 articles, selected purposefully from the articles involved, centering on purpose, approach, findings, and contribution to ACTI future directions. This study is significant for its valuable contribution to ACTI knowledge production over 45 years, emphasizing several trending issues and hotspots including Machine translation, Statistical machine translation, Low-resource language, Large language model, Arabic dialects, Translation quality, and Neural machine translation. The findings reveal that the more AI develops, the more it contributes to translation industry, as Neural Networking Algorithms have been incorporated and Deep Language Learning Models like ChatGPT have been launched. However, much rigorous research is still needed to overcome several problems encountering translation industry, specifically concerning low-resource, multi-dialectical and free word order languages, and cultural and religious registers.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongReason: A Synthetic Long-Context Reasoning Benchmark via Context Expansion</title>
<link>https://arxiv.org/abs/2501.15089</link>
<guid>https://arxiv.org/abs/2501.15089</guid>
<content:encoded><![CDATA[
arXiv:2501.15089v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable progress in understanding long-context inputs. However, benchmarks for evaluating the long-context reasoning abilities of LLMs fall behind the pace. Existing benchmarks often focus on a narrow range of tasks or those that do not demand complex reasoning. To address this gap and enable a more comprehensive evaluation of the long-context reasoning capabilities of current LLMs, we propose a new synthetic benchmark, LongReason, which is constructed by synthesizing long-context reasoning questions from a varied set of short-context reasoning questions through context expansion. LongReason consists of 794 multiple-choice reasoning questions with diverse reasoning patterns across three task categories: reading comprehension, logical inference, and mathematical word problems. We evaluate 21 LLMs on LongReason, revealing that most models experience significant performance drops as context length increases. Our further analysis shows that even state-of-the-art LLMs still have significant room for improvement in providing robust reasoning across different tasks. We have open-sourced LongReason under https://huggingface.co/datasets/lz1bytedance/LongReason to support the comprehensive evaluation of LLMs' long-context reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
arXiv:2502.13685v4 Announce Type: replace 
Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive tasks. To address this limitation, we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. MoM serves as a general framework that can be seamlessly combined with diverse memory update mechanisms across linear models. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs</title>
<link>https://arxiv.org/abs/2503.11858</link>
<guid>https://arxiv.org/abs/2503.11858</guid>
<content:encoded><![CDATA[
arXiv:2503.11858v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated great potential as evaluators of NLG systems, allowing for high-quality, reference-free, and multi-aspect assessments. However, existing LLM-based metrics suffer from two major drawbacks: reliance on proprietary models to generate training data or perform evaluations, and a lack of fine-grained, explanatory feedback. In this paper, we introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation metric that provides accurate explanations based on error spans. OpeNLGauge is available as a two-stage ensemble of larger open-weight LLMs, or as a small fine-tuned evaluation model, with confirmed generalizability to unseen tasks, domains and aspects. Our extensive meta-evaluation shows that OpeNLGauge achieves competitive correlation with human judgments, outperforming state-of-the-art models on certain tasks while maintaining full reproducibility and providing explanations more than twice as accurate.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2504.12673</link>
<guid>https://arxiv.org/abs/2504.12673</guid>
<content:encoded><![CDATA[
arXiv:2504.12673v2 Announce Type: replace 
Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However,retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language modelbased compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy-reducing documents, making it highly useful in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anti-adversarial Learning: Desensitizing Prompts for Large Language Models</title>
<link>https://arxiv.org/abs/2505.01273</link>
<guid>https://arxiv.org/abs/2505.01273</guid>
<content:encoded><![CDATA[
arXiv:2505.01273v2 Announce Type: replace 
Abstract: With the widespread use of LLMs, preserving privacy in user prompts has become crucial, as prompts risk exposing privacy and sensitive data to the cloud LLMs. Traditional techniques like homomorphic encryption, secure multi-party computation, and federated learning face challenges due to heavy computational costs and user participation requirements, limiting their applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel method for desensitizing LLM prompts. The core idea of PromptObfus is "anti-adversarial" learning, which perturbs privacy words in the prompt to obscure sensitive information while retaining the stability of model predictions. Specifically, PromptObfus frames prompt desensitization as a masked language modeling task, replacing privacy-sensitive terms with a [MASK] token. A desensitization model is trained to generate candidate replacements for each masked position. These candidates are subsequently selected based on gradient feedback from a surrogate model, ensuring minimal disruption to the task output. We demonstrate the effectiveness of our approach on three NLP tasks. Results show that PromptObfus effectively prevents privacy inference from remote LLMs while preserving task performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs</title>
<link>https://arxiv.org/abs/2505.17052</link>
<guid>https://arxiv.org/abs/2505.17052</guid>
<content:encoded><![CDATA[
arXiv:2505.17052v2 Announce Type: replace 
Abstract: Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by 1.91x through achieving 2.22x server throughput, and reduces inter token latency by 11.24% compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving. The code is available at https://github.com/kaist-ina/specedge
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-context Language Learning for Endangered Languages in Speech Recognition</title>
<link>https://arxiv.org/abs/2505.20445</link>
<guid>https://arxiv.org/abs/2505.20445</guid>
<content:encoded><![CDATA[
arXiv:2505.20445v4 Announce Type: replace 
Abstract: With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. Our code is publicly available.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration</title>
<link>https://arxiv.org/abs/2505.23229</link>
<guid>https://arxiv.org/abs/2505.23229</guid>
<content:encoded><![CDATA[
arXiv:2505.23229v2 Announce Type: replace 
Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models (LLMs) has demonstrated significant success in structured, problem-oriented tasks. However, applying these methods to open-ended dialogues, such as those in psychological counseling, presents unique challenges. Unlike tasks with objective correctness, success in therapeutic conversations depends on subjective factors like empathetic engagement, ethical adherence, and alignment with human preferences, for which strict "correctness" criteria are ill-defined. Existing result-oriented MCTS approaches can therefore produce misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS framework designed for open-ended, human-centric dialogues. Its core innovation is "domain alignment", which shifts the MCTS search objective from predefined end-states towards conversational trajectories that conform to target domain principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates "Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden exploration by allowing the MCTS to consider fundamentally different initial dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM. We also introduce PsyEval, a benchmark for assessing multi-turn psychological counseling dialogues. Experiments demonstrate that PsyLLM achieves state-of-the-art performance on PsyEval and other relevant metrics, validating MCTSr-Zero's effectiveness in generating high-quality, principle-aligned conversational data for human-centric domains and addressing the LLM challenge of consistently adhering to complex psychological standards.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenRecal: Generation after Recalibration from Large to Small Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.15681</link>
<guid>https://arxiv.org/abs/2506.15681</guid>
<content:encoded><![CDATA[
arXiv:2506.15681v2 Announce Type: replace 
Abstract: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLM: In Search of Lost Language Model Training Dynamics</title>
<link>https://arxiv.org/abs/2506.16029</link>
<guid>https://arxiv.org/abs/2506.16029</guid>
<content:encoded><![CDATA[
arXiv:2506.16029v2 Announce Type: replace 
Abstract: Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm</title>
<link>https://arxiv.org/abs/2506.20606</link>
<guid>https://arxiv.org/abs/2506.20606</guid>
<content:encoded><![CDATA[
arXiv:2506.20606v2 Announce Type: replace 
Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong capabilities across a wide range of tasks. However, deploying LLM-based agents in high-stakes domains comes with significant safety and ethical risks. Unethical behavior by these agents can directly result in serious real-world consequences, including physical harm and financial loss. To efficiently steer the ethical behavior of agents, we frame agent behavior steering as a model editing task, which we term Behavior Editing. Model editing is an emerging area of research that enables precise and efficient modifications to LLMs while preserving their overall capabilities. To systematically study and evaluate this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in psychological moral theories. This benchmark supports both the evaluation and editing of agent behaviors across a variety of scenarios, with each tier introducing more complex and ambiguous scenarios. We first demonstrate that Behavior Editing can dynamically steer agents toward the target behavior within specific scenarios. Moreover, Behavior Editing enables not only scenario-specific local adjustments but also more extensive shifts in an agent's global moral alignment. We demonstrate that Behavior Editing can be used to promote ethical and benevolent behavior or, conversely, to induce harmful or malicious behavior. Through extensive evaluations of agents built on frontier LLMs, BehaviorBench validates the effectiveness of behavior editing across a wide range of models and scenarios. Our findings offer key insights into a new paradigm for steering agent behavior, highlighting both the promise and perils of Behavior Editing.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data</title>
<link>https://arxiv.org/abs/2508.01450</link>
<guid>https://arxiv.org/abs/2508.01450</guid>
<content:encoded><![CDATA[
arXiv:2508.01450v2 Announce Type: replace 
Abstract: Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language Models (LLMs) to specialized domains such as medical reasoning. However, existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, defined by knowledge and reasoning complexity, they overlook each sample's optimization utility reflected in its gradient. Interestingly, we find that gradient-based influence alone favors easy-to-optimize samples that cause large parameter shifts but lack deep reasoning chains, while difficulty alone selects noisy or overly complex cases that fail to guide stable optimization. Based on this observation, we propose a data selection strategy, Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence, enabling efficient medical reasoning with minimal fine-tuning data. Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation, as DIQ emphasizes samples that foster expert-like reasoning patterns. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms baseline methods, highlighting the superiority of principled data selection over brute-force scaling. The code and data are available at https://github.com/mihara-bot/DIQ.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous sentiment scores for literary and multilingual contexts</title>
<link>https://arxiv.org/abs/2508.14620</link>
<guid>https://arxiv.org/abs/2508.14620</guid>
<content:encoded><![CDATA[
arXiv:2508.14620v2 Announce Type: replace 
Abstract: Sentiment Analysis is widely used to quantify sentiment in text, but its application to literary texts poses unique challenges due to figurative language, stylistic ambiguity, as well as sentiment evocation strategies. Traditional dictionary-based tools often underperform, especially for low-resource languages, and transformer models, while promising, typically output coarse categorical labels that limit fine-grained analysis. We introduce a novel continuous sentiment scoring method based on concept vector projection, trained on multilingual literary data, which more effectively captures nuanced sentiment expressions across genres, languages, and historical periods. Our approach outperforms existing tools on English and Danish texts, producing sentiment scores whose distribution closely matches human ratings, enabling more accurate analysis and sentiment arc modeling in literature.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Retrieval Augmented Language Models Know When They Don't Know?</title>
<link>https://arxiv.org/abs/2509.01476</link>
<guid>https://arxiv.org/abs/2509.01476</guid>
<content:encoded><![CDATA[
arXiv:2509.01476v3 Announce Type: replace 
Abstract: Existing large language models (LLMs) occasionally generate plausible yet factually incorrect responses, known as hallucinations. Two main approaches have been proposed to mitigate hallucinations: retrieval-augmented language models (RALMs) and refusal post-training. However, current research predominantly focuses on their individual effectiveness while overlooking the evaluation of the refusal capability of RALMs. Ideally, if RALMs know when they do not know, they should refuse to answer.In this study, we ask the fundamental question: Do RALMs know when they don't know? Specifically, we investigate three questions. First, are RALMs well calibrated with respect to different internal and external knowledge states? We examine the influence of various factors. Contrary to expectations, when all retrieved documents are irrelevant, RALMs still tend to refuse questions they could have answered correctly. Next, given the model's pronounced \textbf{over-refusal} behavior, we raise a second question: How does a RALM's refusal ability align with its calibration quality? Our results show that the over-refusal problem can be mitigated through in-context fine-tuning. However, we observe that improved refusal behavior does not necessarily imply better calibration or higher overall accuracy. Finally, we ask: Can we combine refusal-aware RALMs with uncertainty-based answer abstention to mitigate over-refusal? We develop a simple yet effective refusal mechanism for refusal-post-trained RALMs that improves their overall answer quality by balancing refusal and correct answers. Our study provides a more comprehensive understanding of the factors influencing RALM behavior. Meanwhile, we emphasize that uncertainty estimation for RALMs remains an open problem deserving deeper investigation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patent Language Model Pretraining with ModernBERT</title>
<link>https://arxiv.org/abs/2509.14926</link>
<guid>https://arxiv.org/abs/2509.14926</guid>
<content:encoded><![CDATA[
arXiv:2509.14926v3 Announce Type: replace 
Abstract: Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Fact-checking in English and Telugu</title>
<link>https://arxiv.org/abs/2509.26415</link>
<guid>https://arxiv.org/abs/2509.26415</guid>
<content:encoded><![CDATA[
arXiv:2509.26415v2 Announce Type: replace 
Abstract: False information poses a significant global challenge, and manually verifying claims is a time-consuming and resource-intensive process. In this research paper, we experiment with different approaches to investigate the effectiveness of large language models (LLMs) in classifying factual claims by their veracity and generating justifications in English and Telugu. The key contributions of this work include the creation of a bilingual English-Telugu dataset and the benchmarking of different veracity classification approaches based on LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection</title>
<link>https://arxiv.org/abs/2510.09771</link>
<guid>https://arxiv.org/abs/2510.09771</guid>
<content:encoded><![CDATA[
arXiv:2510.09771v2 Announce Type: replace 
Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six categories. Traditional supervised approaches need extensive labeled datasets that are expensive for low-resource languages. We developed PromptGuard, a few-shot framework combining chi-square statistical analysis for keyword extraction with adaptive majority voting for decision-making. We explore statistical keyword selection versus random approaches and adaptive voting mechanisms that extend classification based on consensus quality. Chi-square keywords provide consistent improvements across categories, while adaptive voting benefits ambiguous cases requiring extended classification rounds. PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines (60.75) and random approaches (14.65). Ablation studies confirm chi-square-based keywords show the most consistent impact across all categories.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI use in American newspapers is widespread, uneven, and rarely disclosed</title>
<link>https://arxiv.org/abs/2510.18774</link>
<guid>https://arxiv.org/abs/2510.18774</guid>
<content:encoded><![CDATA[
arXiv:2510.18774v2 Announce Type: replace 
Abstract: AI is rapidly transforming journalism, but the extent of its use in published newspaper articles remains unclear. We address this gap by auditing a large-scale dataset of 186K articles from online editions of 1.5K American newspapers published in the summer of 2025. Using Pangram, a state-of-the-art AI detector, we discover that approximately 9% of newly-published articles are either partially or fully AI-generated. This AI use is unevenly distributed, appearing more frequently in smaller, local outlets, in specific topics such as weather and technology, and within certain ownership groups. We also analyze 45K opinion pieces from Washington Post, New York Times, and Wall Street Journal, finding that they are 6.4 times more likely to contain AI-generated content than news articles from the same publications, with many AI-flagged op-eds authored by prominent public figures. Despite this prevalence, we find that AI use is rarely disclosed: a manual audit of 100 AI-flagged articles found only five disclosures of AI use. Overall, our audit highlights the immediate need for greater transparency and updated editorial standards regarding the use of AI in journalism to maintain public trust.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance</title>
<link>https://arxiv.org/abs/2511.03383</link>
<guid>https://arxiv.org/abs/2511.03383</guid>
<content:encoded><![CDATA[
arXiv:2511.03383v2 Announce Type: replace 
Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set of hyperparameters for word segmentation models, symmetric Byte Pair Encoding (BPE), which applies the same number of merge operations (NMO) to train tokenizers for both source and target languages. However, we demonstrate that this uniform approach doesn't guarantee optimal MT performance across different language pairs and data sizes. This work investigates BPE segmentation recipes across various data volumes and language pairs to evaluate MT system performance. We find that utilizing asymmetric BPE, where the source and target languages have different NMOs, significantly improves results over the symmetric approach, especially in low-resource settings (50K, 100K, and 500K sentence pairs). Specifically, asymmetric BPE yield statistically significant ($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in low-resource setups (50K, 100K, and 500K sentence pairs, respectively). We validated this trend across six additional language pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut), observing statistically significant improvement in 10 out of 12 systems compared to symmetric BPE. Our findings indicate a high NMO for the source (4K to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results, particularly benefiting low-resource MT.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection</title>
<link>https://arxiv.org/abs/2511.04528</link>
<guid>https://arxiv.org/abs/2511.04528</guid>
<content:encoded><![CDATA[
arXiv:2511.04528v2 Announce Type: replace 
Abstract: We present IntelliProof, an interactive system for analyzing argumentative essays through LLMs. IntelliProof structures an essay as an argumentation graph, where claims are represented as nodes, supporting evidence is attached as node properties, and edges encode supporting or attacking relations. Unlike existing automated essay scoring systems, IntelliProof emphasizes the user experience: each relation is initially classified and scored by an LLM, then visualized for enhanced understanding. The system provides justifications for classifications and produces quantitative measures for essay coherence. It enables rapid exploration of argumentative quality while retaining human oversight. In addition, IntelliProof provides a set of tools for a better understanding of an argumentative essay and its corresponding graph in natural language, bridging the gap between the structural semantics of argumentative essays and the user's understanding of a given text.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability</title>
<link>https://arxiv.org/abs/2403.04483</link>
<guid>https://arxiv.org/abs/2403.04483</guid>
<content:encoded><![CDATA[
arXiv:2403.04483v4 Announce Type: replace-cross 
Abstract: Improving the general capabilities of large language models (LLMs) is an active research topic. As a common data structure in many real-world domains, understanding graph data is a crucial part of advancing general intelligence. To this end, we propose a dynamic benchmark named GraphInstruct in this paper, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed intermediate reasoning steps for each sample. Based on GraphInstruct, we develop GraphSolver via efficient instruction-tuning, which demonstrates prominent graph understanding capability compared to other open-sourced LLMs. To further endow LLMs with multi-step graph reasoning capability, we propose a label-mask training strategy and build GraphSolver+, which leverages masked supervision on intermediate reasoning tokens to emphasize crucial node-identification signals. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphSolver and GraphSolver+ over other LLMs. We sincerely hope GraphInstruct will facilitate further research on applying LLMs to graph-structured data. Our code and data are released publicly at: https://github.com/CGCL-codes/GraphInstruct.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2410.22995</link>
<guid>https://arxiv.org/abs/2410.22995</guid>
<content:encoded><![CDATA[
arXiv:2410.22995v2 Announce Type: replace-cross 
Abstract: A hallmark of advanced artificial intelligence is the capacity to progress from passive visual perception to the strategic modification of visual information to facilitate complex reasoning. This advanced capability, however, remains critically underdeveloped in current Large Multi-modal Models (LMMs). The deficiency is often masked by evaluation metrics that prioritize final-answer accuracy, creating an illusion of competence where genuine reasoning is absent. Using the domain of geometric problem-solving as a precise instrument, we probe this issue through tasks that require constructing visual aids. To this end, we introduce \textbf{VisAidMath}, a challenging benchmark, and our novel Three-Layered Funnel Evaluation Framework. This framework moves beyond simple accuracy (ACCU) to scrutinize the generation of valid visual aids (PVA) and the soundness of subsequent reasoning steps (SPRS). Our extensive experiments on state-of-the-art models, including Doubao-Seed-1.6 and o4, reveal a profound ``Reasoning Illusion''. We observe that high surface-level accuracy conceals a catastrophic failure in the models' ability to produce valid visual aids or to reason from them. Our findings expose a fundamental schism between visual perception and logical deduction in modern LMMs. We host an evaluation platform at CodaBench for testing publicly. Homepage: https://nlp2ct.github.io/VisAidMathHomepage/ Evaluation: https://www.codabench.org/competitions/7634/
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iris: Integrating Language into Diffusion-based Monocular Depth Estimation</title>
<link>https://arxiv.org/abs/2411.16750</link>
<guid>https://arxiv.org/abs/2411.16750</guid>
<content:encoded><![CDATA[
arXiv:2411.16750v4 Announce Type: replace-cross 
Abstract: Traditional monocular depth estimation suffers from inherent ambiguity and visual nuisances. We demonstrate that language can enhance monocular depth estimation by providing an additional condition (rather than images alone) aligned with plausible 3D scenes, thereby reducing the solution space for depth estimation. This conditional distribution is learned during the text-to-image pre-training of diffusion models. To generate images under various viewpoints and layouts that precisely reflect textual descriptions, the model implicitly models object sizes, shapes, and scales, their spatial relationships, and the overall scene structure. In this paper, Iris, we investigate the benefits of our strategy to integrate text descriptions into training and inference of diffusion-based depth estimation models. We experiment with three different diffusion-based monocular depth estimators (Marigold, Lotus, and E2E-FT) and their variants. By training on HyperSim and Virtual KITTI, and evaluating on NYUv2, KITTI, ETH3D, ScanNet, and DIODE, we find that our strategy improves the overall monocular depth estimation accuracy, especially in small areas. It also improves the model's depth perception of specific regions described in the text. We find that by providing more details in the text, the depth prediction can be iteratively refined. Simultaneously, we find that language can act as a constraint to accelerate the convergence of both training and the inference diffusion trajectory. Code and generated text data will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the Performance of Black-box LLMs through Self-Queries</title>
<link>https://arxiv.org/abs/2501.01558</link>
<guid>https://arxiv.org/abs/2501.01558</guid>
<content:encoded><![CDATA[
arXiv:2501.01558v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly relied on in AI systems, predicting when they make mistakes is crucial. While a great deal of work in the field uses internal representations to interpret model behavior, these representations are inaccessible when given solely black-box access through an API. In this paper, we extract features of LLMs in a black-box manner by using follow-up prompts and taking the probabilities of different responses as representations to train reliable predictors of model behavior. We demonstrate that training a linear model on these low-dimensional representations produces reliable and generalizable predictors of model performance at the instance level (e.g., if a particular generation correctly answers a question). Remarkably, these can often outperform white-box linear predictors that operate over a model's hidden state or the full distribution over its vocabulary. In addition, we demonstrate that these extracted features can be used to evaluate more nuanced aspects of a language model's state. For instance, they can be used to distinguish between a clean version of GPT-4o-mini and a version that has been influenced via an adversarial system prompt that answers question-answering tasks incorrectly or introduces bugs into generated code. Furthermore, they can reliably distinguish between different model architectures and sizes, enabling the detection of misrepresented models provided through an API (e.g., identifying if GPT-3.5 is supplied instead of GPT-4o-mini).
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPAD: Inverse Prompt for AI Detection - A Robust and Interpretable LLM-Generated Text Detector</title>
<link>https://arxiv.org/abs/2502.15902</link>
<guid>https://arxiv.org/abs/2502.15902</guid>
<content:encoded><![CDATA[
arXiv:2502.15902v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide interpretable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and two Distinguishers that examine the probability that the input texts align with the predicted prompts. Empirical evaluations demonstrate that IPAD outperforms the strongest baselines by 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on out-of-distribution data, and 5.48% (AUROC) on attacked data. IPAD also performs robustly on structured datasets. Furthermore, an interpretability assessment is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptScale: Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v3 Announce Type: replace-cross 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-N selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning. The source code is publicly available at https://github.com/Albertwyk/OptScale.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theories of "Sexuality" in Natural Language Processing Bias Research</title>
<link>https://arxiv.org/abs/2506.22481</link>
<guid>https://arxiv.org/abs/2506.22481</guid>
<content:encoded><![CDATA[
arXiv:2506.22481v2 Announce Type: replace-cross 
Abstract: In recent years, significant advancements in the field of Natural Language Processing (NLP) have positioned commercialized language models as wide-reaching, highly useful tools. In tandem, there has been an explosion of multidisciplinary research examining how NLP tasks reflect, perpetuate, and amplify social biases such as gender and racial bias. A significant gap in this scholarship is a detailed analysis of how queer sexualities are encoded and (mis)represented by both NLP systems and practitioners. Following previous work in the field of AI fairness, we document how sexuality is defined and operationalized via a survey and analysis of 55 articles that quantify sexuality-based NLP bias. We find that sexuality is not clearly defined in a majority of the literature surveyed, indicating a reliance on assumed or normative conceptions of sexual/romantic practices and identities. Further, we find that methods for extracting biased outputs from NLP technologies often conflate gender and sexual identities, leading to monolithic conceptions of queerness and thus improper quantifications of bias. With the goal of improving sexuality-based NLP bias analyses, we conclude with recommendations that encourage more thorough engagement with both queer communities and interdisciplinary literature.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
<link>https://arxiv.org/abs/2508.01249</link>
<guid>https://arxiv.org/abs/2508.01249</guid>
<content:encoded><![CDATA[
arXiv:2508.01249v3 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's runtime traces as graph-based intermediate representations with control and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools \& data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis for sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can reduce the ASR to 3\%, with the utility drop only 1\%.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers</title>
<link>https://arxiv.org/abs/2508.02175</link>
<guid>https://arxiv.org/abs/2508.02175</guid>
<content:encoded><![CDATA[
arXiv:2508.02175v3 Announce Type: replace-cross 
Abstract: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech processing, their safety implications demand urgent attention. While considerable research has explored textual and vision safety, audio's distinct characteristics present significant challenges. This paper first investigates: Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In response to this issue, we introduce Hidden in the Noise (HIN), a novel backdoor attack framework designed to exploit subtle, audio-specific features. HIN applies acoustic modifications to raw audio waveforms, such as alterations to temporal dynamics and strategic injection of spectrally tailored noise. These changes introduce consistent patterns that an ALLM's acoustic feature encoder captures, embedding robust triggers within the audio stream. To evaluate ALLM robustness against audio-feature-based triggers, we develop the AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments on AudioSafe and three established safety datasets reveal critical vulnerabilities in existing ALLMs: (I) audio features like environment noise and speech rate variations achieve over 90% average attack success rate. (II) ALLMs exhibit significant sensitivity differences across acoustic features, particularly showing minimal response to volume as a trigger, and (III) poisoned sample inclusion causes only marginal loss curve fluctuations, highlighting the attack's stealth.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap</title>
<link>https://arxiv.org/abs/2508.18646</link>
<guid>https://arxiv.org/abs/2508.18646</guid>
<content:encoded><![CDATA[
arXiv:2508.18646v2 Announce Type: replace-cross 
Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark performance and real-world utility. Current evaluation frameworks remain fragmented, prioritizing technical metrics while neglecting holistic assessment for deployment. This survey introduces an anthropomorphic evaluation paradigm through the lens of human intelligence, proposing a novel three-dimensional taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational capacity, Emotional Quotient (EQ)-Alignment Ability for value-based interactions, and Professional Quotient (PQ)-Professional Expertise for specialized proficiency. For practical value, we pioneer a Value-oriented Evaluation (VQ) framework assessing economic viability, social impact, ethical alignment, and environmental sustainability. Our modular architecture integrates six components with an implementation roadmap. Through analysis of 200+ benchmarks, we identify key challenges including dynamic assessment needs and interpretability gaps. It provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound. We maintain a curated repository of open-source evaluation resources at: https://github.com/onejune2018/Awesome-LLM-Eval.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection</title>
<link>https://arxiv.org/abs/2510.11654</link>
<guid>https://arxiv.org/abs/2510.11654</guid>
<content:encoded><![CDATA[
arXiv:2510.11654v2 Announce Type: replace-cross 
Abstract: Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA</title>
<link>https://arxiv.org/abs/2510.25101</link>
<guid>https://arxiv.org/abs/2510.25101</guid>
<content:encoded><![CDATA[
arXiv:2510.25101v2 Announce Type: replace-cross 
Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis</title>
<link>https://arxiv.org/abs/2511.04584</link>
<guid>https://arxiv.org/abs/2511.04584</guid>
<content:encoded><![CDATA[
arXiv:2511.04584v3 Announce Type: replace-cross 
Abstract: Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction where users are intentional about the degree to which they specify queries. We develop a principled framework based on a shared responsibility of query specification between user and system, distinguishing unambiguous and ambiguous cooperative queries, which systems can resolve through reasonable inference, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. This conceptualization around cooperation in resolving queries informs how to design and evaluate natural language interfaces for tabular data analysis, for which we distill concrete directions for future research and broader implications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context</title>
<link>https://arxiv.org/abs/2504.04737</link>
<guid>https://arxiv.org/abs/2504.04737</guid>
<content:encoded><![CDATA[
<div> Keywords: Fact-based Judgment Prediction, TathyaNyaya, legal context, FactLegalLlama, AI-assisted decision-making  

<br /><br />Summary:  
This paper presents TathyaNyaya, the largest annotated dataset for Fact-based Judgment Prediction and Explanation (FJPE) tailored to the Indian legal system, featuring judgments from the Supreme Court and various High Courts. The dataset's name is derived from the Hindi words "Tathya" (fact) and "Nyaya" (justice), emphasizing its focus on factual statements essential for real-world judicial outcomes. Alongside TathyaNyaya, the authors introduce FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model optimized for producing high-quality explanations in FJPE tasks. Fine-tuned on the TathyaNyaya dataset, FactLegalLlama combines predictive accuracy with clear and relevant explanations, addressing crucial transparency and interpretability needs in AI-driven legal systems. The proposed methodology integrates transformers for binary judgment prediction and the FactLegalLlama for explanation generation, creating a robust framework focused on advancing FJPE within the Indian legal domain. TathyaNyaya not only exceeds existing datasets in both scale and diversity but also sets a new benchmark for the development of explainable AI in legal analysis, highlighting the importance of factual accuracy and domain-specific tuning for improved predictive performance and interpretability. <div>
arXiv:2504.04737v3 Announce Type: replace 
Abstract: In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</title>
<link>https://arxiv.org/abs/2508.00709</link>
<guid>https://arxiv.org/abs/2508.00709</guid>
<content:encoded><![CDATA[
<div> Keywords: Legal Judgment Prediction, NyayaRAG, Indian legal system, retrieval-augmented generation, predictive accuracy

<br /><br />Summary: Legal Judgment Prediction (LJP) is a crucial area of AI in the legal field that focuses on forecasting judicial outcomes and enhancing the interpretability of legal reasoning. Existing methods in India often concentrate on internal case elements, neglecting essential components of common law, such as statutory provisions and judicial precedents. This paper introduces NyayaRAG, a Retrieval-Augmented Generation (RAG) framework designed to emulate realistic courtroom scenarios. NyayaRAG integrates factual case descriptions, pertinent legal statutes, and semantically retrieved previous cases to bolster the predictive process. The framework's effectiveness is evaluated based on the impact of these combined inputs on court decision predictions and the quality of generated legal explanations, specifically adapted for the Indian legal environment. Comprehensive performance assessments utilize various configurations and include both standard lexical and semantic metrics alongside Large Language Model (LLM)-based evaluators like G-Eval. The findings indicate that enhancing factual inputs with structured legal knowledge notably boosts both the accuracy of predictions and the quality of explanations provided, indicating the framework's potential for advancing legal judgment prediction in India. <div>
arXiv:2508.00709v3 Announce Type: replace 
Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2509.00974</link>
<guid>https://arxiv.org/abs/2509.00974</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Question Answering, Ranked Preference Reinforcement Optimization, Clinical Chain-of-Thought, Reinforcement Learning, Preference-Driven Reasoning

<br /><br />Summary: The article addresses the challenges in medical question answering, particularly the inaccuracies in reasoning chains produced by existing large language models (LLMs). To overcome this, the authors introduce a novel framework called Ranked Preference Reinforcement Optimization (RPRO), which merges reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO is distinctive in its use of task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows while detecting and correcting low-quality reasoning. Unlike traditional methods that rely on pairwise preferences, RPRO employs groupwise ranking optimization based on the Bradley--Terry model and utilizes KL-divergence regularization to ensure stable training. The framework's effectiveness is demonstrated through experiments on multiple datasets, including PubMedQA and MedQA-USMLE, and a real-world clinical dataset from Far Eastern Memorial Hospital, showing notable improvements over strong baseline models. Impressively, their 2B-parameter model surpasses larger models ranging from 7B to 20B parameters, showcasing the potential of preference optimization combined with quality-driven refinement to create more reliable and clinically relevant medical LLMs. <div>
arXiv:2509.00974v3 Announce Type: replace 
Abstract: Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO distinguishes itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns model outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley--Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA, MedQA-USMLE, and a real-world clinical dataset from Far Eastern Memorial Hospital (FEMH) demonstrate consistent improvements over strong baselines. Remarkably, our 2B-parameter model outperforms much larger 7B--20B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement provides a scalable and clinically grounded approach to building more reliable medical LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Silenced Biases: The Dark Side LLMs Learned to Refuse</title>
<link>https://arxiv.org/abs/2511.03369</link>
<guid>https://arxiv.org/abs/2511.03369</guid>
<content:encoded><![CDATA[
<div> Keywords: safety-aligned models, fairness evaluation, silenced biases, Silenced Bias Benchmark, activation steering

<br /><br />Summary: The prevalence of safety-aligned large language models (LLMs) in sensitive applications necessitates effective evaluation of their fairness, particularly in contexts where biased outputs can cause harm. Traditional evaluation methods often rely on question-answer schemes, mistakenly interpreting model refusals as indicators of fairness, resulting in a misleading assessment. This work introduces the notion of silenced biases, which refers to unfair preferences embedded in the models' latent space and masked by safety-alignment. Existing methods addressing similar biases have limitations, including dependence on prompt manipulation and handcrafted queries that can introduce new biases. To address these challenges, the authors propose the Silenced Bias Benchmark (SBB), designed to reveal these concealed biases through activation steering that minimizes model refusals during QA evaluations. SBB is structured for easy adaptation to various demographic groups and subjects, aiming to enhance fairness evaluation frameworks and promote the development of fairer models beyond the constraints of alignment training. The authors demonstrate their methodology across multiple LLMs, revealing significant discrepancies between the models' overt responses and their underlying fairness issues. <div>
arXiv:2511.03369v2 Announce Type: replace 
Abstract: Safety-aligned large language models (LLMs) are becoming increasingly widespread, especially in sensitive applications where fairness is essential and biased outputs can cause significant harm. However, evaluating the fairness of models is a complex challenge, and approaches that do so typically utilize standard question-answer (QA) styled schemes. Such methods often overlook deeper issues by interpreting the model's refusal responses as positive fairness measurements, which creates a false sense of fairness. In this work, we introduce the concept of silenced biases, which are unfair preferences encoded within models' latent space and are effectively concealed by safety-alignment. Previous approaches that considered similar indirect biases often relied on prompt manipulation or handcrafted implicit queries, which present limited scalability and risk contaminating the evaluation process with additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to uncover these biases by employing activation steering to reduce model refusals during QA. SBB supports easy expansion to new demographic groups and subjects, presenting a fairness evaluation framework that encourages the future development of fair models and tools beyond the masking effects of alignment training. We demonstrate our approach over multiple LLMs, where our findings expose an alarming distinction between models' direct responses and their underlying fairness issues.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy</title>
<link>https://arxiv.org/abs/2511.11594</link>
<guid>https://arxiv.org/abs/2511.11594</guid>
<content:encoded><![CDATA[
<div> Keywords: fuzzy matching, TimeStampEval, transcripts, retrieval accuracy, Assisted Fuzzy  

<br /><br />Summary: Traditional fuzzy matching struggles with semantically identical but syntactically different quotes, particularly when aligning official records with speech-to-text transcripts. TimeStampEval is introduced as a benchmark for retrieving millisecond timestamps from lengthy transcripts based on non-verbatim quotes. The simple two-stage method significantly enhances retrieval accuracy while reducing inference costs by over 90%. The primary application is automating long-form podcasts using Congressional Record clips. Key findings include: (1) Prompt design is more crucial than model selection; positioning the query before the transcript with compact formatting can boost accuracy by 3-20 points and reduce token count by 30-40%. (2) Distinct "off-by-one" errors indicate that models are capable of understanding the task but may misplace boundaries. (3) A modest reasoning budget (600-850 tokens) can elevate accuracy from 37% to 77% for weaker models and above 90% for stronger ones. (4) The "Assisted Fuzzy" method, which employs RapidFuzz pre-filtering followed by LLM verification, enhances fuzzy match accuracy by up to 50 points, halves latency, and cuts the cost per correct result by as much as 96%. Tests across ten varied transcripts confirm the approach's robustness. <div>
arXiv:2511.11594v1 Announce Type: new 
Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</title>
<link>https://arxiv.org/abs/2511.11793</link>
<guid>https://arxiv.org/abs/2511.11793</guid>
<content:encoded><![CDATA[
<div> Keywords: MiroThinker, open-source, interaction scaling, reasoning, reinforcement learning  

<br /><br />Summary: MiroThinker v1.0 is an open-source research agent aimed at enhancing tool-augmented reasoning and information-seeking capabilities. Unlike traditional models that primarily focus on scaling size or context length, MiroThinker innovates by exploring interaction scaling at the model level, facilitating deeper and more frequent interactions with the environment. This method allows the model to leverage feedback and external information, correcting errors and refining reasoning trajectories. Through reinforcement learning, MiroThinker accomplishes efficient interaction scaling, operating with a 256K context window and capable of executing up to 600 tool calls per task, which supports complex multi-turn reasoning and real-world research workflows. The 72B variant of MiroThinker excels across four benchmarks—GAIA, HLE, BrowseComp, and BrowseComp-ZH—achieving accuracy scores of 81.9%, 37.7%, 47.1%, and 55.6%, respectively, outperforming existing open-source models and nearing the performance of commercial models like GPT-5-high. The study shows that research performance improves consistently with increased interaction depth, establishing it as a crucial dimension for developing advanced research agents alongside model capacity and context windows. <div>
arXiv:2511.11793v1 Announce Type: new 
Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Notion that Language Models Reason</title>
<link>https://arxiv.org/abs/2511.11810</link>
<guid>https://arxiv.org/abs/2511.11810</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reasoning, Markov kernel, statistical pattern matchers, epistemic uncertainty

<br /><br />Summary:  
1. The paper critically examines the meaning of "reasoning" as applied to language models (LMs) in natural language processing (NLP), highlighting inconsistencies in common definitions relative to LM training and operation.  
2. It adopts the perspective that transformer-based LMs function as implicit finite-order Markov kernels, mapping contexts to conditional token probabilities rather than executing explicit logical or reasoning algorithms.  
3. Reasoning-like outputs from LMs emerge from learned statistical regularities and approximate invariances in these probabilistic kernels, rather than from genuine logical inference or explicit mechanisms.  
4. This framework supports the characterization of LMs as "statistical pattern matchers" rather than authentic reasoners, explaining why LMs can produce outputs that resemble reasoning without guarantees of logical consistency.  
5. The distinction is crucial for properly assessing epistemic uncertainty in LMs and calls for careful attention to how computational processes underlying LMs are described and understood in NLP research, fostering clearer conceptual foundations and dialogue. <div>
arXiv:2511.11810v1 Announce Type: new 
Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis</title>
<link>https://arxiv.org/abs/2511.11821</link>
<guid>https://arxiv.org/abs/2511.11821</guid>
<content:encoded><![CDATA[
<div> Keywords: information extraction, large language models, hydropower licensing, model scaling, validation methods  

<br /><br />Summary:  
This study investigates the use of large language models (LLMs) for extracting information from regulatory documents specifically related to hydropower licensing. Seven open-weight LLMs ranging from 0.6 billion to 70 billion parameters were evaluated to understand performance relative to computational requirements. A key finding is a critical performance threshold at 14 billion parameters, where validation methods become significantly more effective—models below this size achieve poor F1 scores (<0.15), while those at or above this level reach viable F1 scores around 0.64. Models designed for consumer deployment max out at about 51% F1 score due to limitations in validation, whereas the largest models approach 77% F1 but demand enterprise-grade infrastructure for practical use. The research also exposes systematic hallucination patterns in smaller models, where perfect recall paradoxically signals extraction failure rather than accuracy. This work provides the first comprehensive resource-performance mapping for open-weight LLMs in regulatory information extraction, offering actionable guidance to select models based on deployment constraints. The insights extend beyond hydropower, shedding light on how parameter scaling impacts extraction tasks more broadly and enabling evidence-based decisions in choosing appropriate model sizes for regulatory compliance applications. <div>
arXiv:2511.11821v1 Announce Type: new 
Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Autoformalization of LLM-generated Outputs for Requirement Verification</title>
<link>https://arxiv.org/abs/2511.11829</link>
<guid>https://arxiv.org/abs/2511.11829</guid>
<content:encoded><![CDATA[
<div> Autoformalization, Large Language Models, Formal Verification, Logical Consistency, Natural Language Requirements<br /><br />Summary:<br /><br />This paper explores the application of autoformalization, which is the process of converting informal natural language statements into formal logic, leveraging Large Language Models (LLMs). The authors aim to address the challenge of verifying whether LLM-generated structured outputs accurately represent their natural language inputs, which currently lacks formal verification methods. Two experiments are conducted to demonstrate the approach's feasibility. In the first experiment, the LLM-based autoformalizer is able to identify logical equivalence between two differently worded natural language requirements, proving its potential to perform consistency checks. The second experiment showcases the tool's ability to detect logical inconsistencies between a natural language requirement and an LLM-generated output, highlighting its use as a verification mechanism. Although the study is preliminary and limited in scope, the results indicate that autoformalization can be an effective method for ensuring the fidelity and logical consistency of outputs produced by LLMs. This foundational work sets the stage for future, more comprehensive research aimed at improving the reliability of autoformalization and formal verification techniques in natural language understanding and generation tasks. <div>
arXiv:2511.11829v1 Announce Type: new 
Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection</title>
<link>https://arxiv.org/abs/2511.11857</link>
<guid>https://arxiv.org/abs/2511.11857</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, narrative analysis, movie scripts, clustering, computational learning

<br /><br />Summary: 
This paper addresses the challenges in automated narrative analysis within Natural Language Understanding, emphasizing the need for deep computational semantic representations paired with syntactic processing. Given the massive volume of narrative data, it advocates for automated semantic analysis and computational learning over manual approaches. The proposed framework specifically targets the analysis of sentiment arcs in movie scripts, allowing for both high-level and low-level concept extraction from narratives. Utilizing dictionary-based sentiment analysis, the framework employs a custom lexicon developed with the LabMTsimple storylab module, which incorporates Valence, Arousal, and Dominance scores derived from the NRC-VAD dataset. A notable advancement in this framework is its ability to cluster similar sentiment plots using Ward's hierarchical clustering technique. The experimental evaluation conducted on a dataset of movies demonstrates that the resulting analysis provides valuable insights for consumers and readers, assisting them in selecting stories or narratives that align with their preferences. Ultimately, this work contributes to a more nuanced understanding and analysis of narratives through sentiment exploration and character context. <div>
arXiv:2511.11857v1 Announce Type: new 
Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches</title>
<link>https://arxiv.org/abs/2511.11867</link>
<guid>https://arxiv.org/abs/2511.11867</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, radiology reports, follow-up adherence detection, machine learning classifiers, prompt optimization<br /><br />Summary: This work addresses the lack of domain-specific datasets to evaluate large language models (LLMs) in radiology by introducing an annotated corpus of 6,393 radiology reports from 586 patients labeled for follow-up imaging status. The study systematically compares traditional machine learning classifiers—including logistic regression (LR) and support vector machines (SVM)—and newer models such as Longformer and a fully fine-tuned Llama3-8B-Instruct, alongside generative LLMs like GPT-4o and the open-source GPT-OSS-20B. Generative models were tested under a baseline (Base) and a task-optimized (Advanced) setting, the latter emphasizing metadata, recommendation sentences, and context to improve performance. Prompt refinement further enhanced GPT-OSS-20B's reasoning accuracy. Evaluation metrics included precision, recall, and F1 scores, with confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846), establishing a strong benchmark. GPT-4o in the Advanced setting achieved the highest performance (F1 = 0.832), closely followed by GPT-OSS-20B (F1 = 0.828). Traditional classifiers LR and SVM also showed competitive results (F1 ≈ 0.776), demonstrating that while optimized LLMs can approach human-level agreement, simpler interpretable models remain valuable and resource-efficient baselines for clinical applications in radiology follow-up adherence detection. <div>
arXiv:2511.11867v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers</title>
<link>https://arxiv.org/abs/2511.11878</link>
<guid>https://arxiv.org/abs/2511.11878</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, healthcare, Brazilian Portuguese, MedPT, patient-doctor interactions

<br /><br />Summary: This article introduces MedPT, a large-scale corpus designed for Brazilian Portuguese, aimed at enhancing healthcare technologies. It consists of 384,095 authentic question-answer pairs derived from patient-doctor interactions, addressing the limitations of existing models that focus primarily on high-resource languages. The dataset underwent a rigorous multi-stage curation process, incorporating both quantitative and qualitative analyses to minimize noise and enrich queries. MedPT is further enhanced by LLM-driven annotation, classifying questions into seven semantic categories to better reflect user intent. The dataset exhibits a thematic breadth of 3,200 topics and highlights unique linguistic characteristics, including the natural asymmetry in patient-doctor communication. To demonstrate its effectiveness, the authors benchmarked a medical specialty routing task, achieving an impressive 94% F1-score using a fine-tuned 1.7B parameter model. Moreover, a qualitative error analysis indicated that misclassifications were tied to genuine clinical ambiguities, emphasizing the dataset's semantic depth. The authors aim to release MedPT to promote the creation of sustainable, culturally-aware medical technologies for the Portuguese-speaking community. <div>
arXiv:2511.11878v1 Announce Type: new 
Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts</title>
<link>https://arxiv.org/abs/2511.11883</link>
<guid>https://arxiv.org/abs/2511.11883</guid>
<content:encoded><![CDATA[
<div> Clinical notes, large language models, interpretability, generalizability, ICU mortality prediction<br /><br />Summary:<br /><br />1. Clinical notes hold rich and valuable contextual information but are challenging to use due to their unstructured nature, which introduces unintended biases such as gender and racial bias.  
2. Models trained on clinical data from one electronic health record (EHR) system often do not generalize well to other systems, mainly because of differences in formatting and data representation.  
3. ClinStructor is proposed as a novel pipeline that uses large language models (LLMs) to transform clinical free-text into structured, task-specific question-answer pairs, which improves the transparency and controllability of predictive modeling.  
4. This methodology significantly enhances the interpretability of machine learning models applied to clinical tasks, making it easier for clinicians to understand and trust model outputs.  
5. When applied to ICU mortality prediction, ClinStructor only shows a modest decrease of 2-3% in area under the curve (AUC) compared to direct fine-tuning, while offering a foundation for building more reliable, interpretable, and generalizable predictive models in healthcare settings. <div>
arXiv:2511.11883v1 Announce Type: new 
Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support</title>
<link>https://arxiv.org/abs/2511.11884</link>
<guid>https://arxiv.org/abs/2511.11884</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health, telehealth, GPT-2, reinforcement learning, therapeutic dialogue  

<br /><br />Summary:  
The paper addresses the significant socioeconomic burden of mental health illnesses, aggravated by COVID-19, highlighting the demand for telehealth solutions. It investigates how large language models (LLMs), specifically GPT-2, can be enhanced for therapeutic dialogue generation through supervised fine-tuning (SFT) and reinforcement learning (RL). The study restructured input formats to process contextual information and emotional states alongside user inputs. A multi-component reward function was created to align model outputs with professional therapist responses and annotated emotions. The results indicate notable improvements in various evaluation metrics, including BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581), when comparing reinforcement learning workflows to baseline GPT-2. Furthermore, the enhanced model achieved 99.34% accuracy in determining emotions, a significant increase over the 66.96% accuracy of baseline GPT-2. Overall, the findings demonstrate that reinforcement learning can effectively improve the performance of therapeutic dialogue systems, offering valuable assistive tools for therapists while underscoring the necessity of human clinical oversight. <div>
arXiv:2511.11884v1 Announce Type: new 
Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Additive Large Language Models for Semi-Structured Text</title>
<link>https://arxiv.org/abs/2511.11922</link>
<guid>https://arxiv.org/abs/2511.11922</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, clinical text classification, interpretability, additive models, semi-structured text<br /><br />Summary:<br /><br />This paper introduces CALM (Classification with Additive Large Language Models), a novel framework designed to improve interpretability in clinical text classification tasks using large language models (LLMs). CALM leverages the semi-structured nature of many clinical documents by decomposing inputs into meaningful components, such as sections of admission notes or intake form fields, and models predictions as an additive sum of contributions from each component. This additive approach integrates component contributions directly into the forward computation, enabling faithful and transparent explanations at both individual patient and broader population levels. The method facilitates clear visualizations through component-level risk curves akin to those found in generalized additive models, making it easier to understand and communicate learned relationships. CALM is particularly suited to clinical settings, where understanding which parts of a patient record drive risk signals is critical for trust, quality assurance, and meaningful clinical insights. Despite its interpretability advantages, CALM matches the performance of traditional LLM classifiers, proving that enhanced explanation does not require sacrificing accuracy. The framework also supports automatic extraction of semi-structured inputs from free-text clinical notes, broadening its applicability. Overall, CALM offers a practical and interpretable solution that fosters trust and insight during model development and auditing in healthcare contexts. <div>
arXiv:2511.11922v1 Announce Type: new 
Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InData: Towards Secure Multi-Step, Tool-Based Data Analysis</title>
<link>https://arxiv.org/abs/2511.11933</link>
<guid>https://arxiv.org/abs/2511.11933</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Data Analysis, Tool-based Reasoning, Security, Indirect Data Engagement (InData)<br /><br />Summary:<br /><br />1. The paper addresses security risks associated with allowing large language model (LLM) agents to directly generate and execute code on sensitive databases during data analysis.<br />2. To mitigate these risks, the authors propose restricting LLMs from direct code generation and data access, requiring interaction only through a secure, predefined set of verified tools.<br />3. Existing benchmarks for LLM tool use focus mainly on tool selection and simple task execution, lacking evaluation of complex, multi-step reasoning.<br />4. The authors introduce Indirect Data Engagement (InData), a novel dataset that evaluates LLMs' ability to perform multi-step, tool-based reasoning across data analysis questions of varying difficulty: Easy, Medium, and Hard.<br />5. Benchmarking 15 open-source LLMs on InData reveals that while large models (e.g., gpt-oss-120b) perform very well on Easy tasks (97.3% accuracy), their accuracy declines significantly on Hard tasks (69.6%), indicating a deficiency in robust multi-step tool-based reasoning.<br />6. The release of the InData dataset and code aims to foster development and assessment of LLMs with enhanced capabilities for secure, multi-step tool use in complex data analysis contexts. <div>
arXiv:2511.11933v1 Announce Type: new 
Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization</title>
<link>https://arxiv.org/abs/2511.11946</link>
<guid>https://arxiv.org/abs/2511.11946</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge graph, dialogue generation, LLM-KAT, entity anonymization, OpenDialKG

<br /><br />Summary: This article discusses the challenges of integrating external knowledge into dialogue generation by leveraging knowledge graphs (KG-DG). It highlights the limitations of large language models (LLMs) in effectively utilizing provided knowledge graphs, as they tend to rely more on internal knowledge. To address these issues, the authors introduce LLM-KAT, a novel evaluation procedure designed to measure how well LLMs attach to external knowledge in their generated responses. Additionally, the paper presents a simple yet effective technique called entity anonymization, aimed at encouraging LLMs to engage more with external knowledge sources. Through experiments conducted on the OpenDialKG dataset, the authors demonstrate that their proposed approaches lead to significant improvements in the ability of LLMs to utilize external knowledge effectively. The findings underscore the need for more focused methods in the field of KG-DG to enhance the interaction between LLMs and external knowledge, ultimately aiming for more coherent and informative conversational responses. <div>
arXiv:2511.11946v1 Announce Type: new 
Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Entropy Calibration of Language Models</title>
<link>https://arxiv.org/abs/2511.11966</link>
<guid>https://arxiv.org/abs/2511.11966</guid>
<content:encoded><![CDATA[
<div> Keywords: entropy calibration, autoregressive models, miscalibration, scaling behavior, black box prediction

<br /><br />Summary: This paper investigates entropy calibration in language models, focusing on whether the entropy over generations aligns with log loss on human text. Previous research indicated that models are often miscalibrated, with increased entropy correlating with poorer text quality in longer generations. The authors explore whether miscalibration improves with scaling and if calibration can be achieved without tradeoffs. They analyze a theoretical model that examines scaling behavior related to dataset size, revealing that for power law distributions with an exponent near 1, miscalibration improves slowly with scale. Empirical measurements from language models, ranging from 0.5B to 70B parameters, support this theoretical insight, showing similar rates of error accumulation across different model sizes. Consequently, larger models do not significantly reduce miscalibration compared to smaller ones, even though they generate higher quality outputs. The paper also discusses the standard practice of truncating distributions to improve text quality, though this increases log loss. Ultimately, it establishes that theoretically, reducing entropy while maintaining log loss is possible if a black box exists that can predict future text entropy accurately. <div>
arXiv:2511.11966v1 Announce Type: new 
Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reasoning Paradigm for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2511.11978</link>
<guid>https://arxiv.org/abs/2511.11978</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative LLMs, Named Entity Recognition, reasoning framework, Chain of Thought, zero-shot performance  

<br /><br />Summary: This article addresses the limitations of generative large language models (LLMs) in Named Entity Recognition (NER) performance, particularly in zero-shot and low-resource contexts. It identifies a problem of "cognitive shortcutting," where LLMs rely on implicit pattern matching rather than explicit reasoning, leading to suboptimal outcomes. To improve this, a new reasoning framework for NER is proposed, consisting of three key stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset is created with NER-oriented CoTs that provide relevant reasoning chains. This dataset is then utilized to refine the NER model, allowing it to generate coherent rationales prior to producing final outputs. In the last stage, reasoning enhancement optimizes the extraction process using a comprehensive reward signal, promoting explicit and verifiable extractions. Experimental results reveal that the proposed method, ReasoningNER, significantly boosts cognitive ability in NER tasks, achieving state-of-the-art (SOTA) performance with a notable 12.3 percentage-point improvement in F1 score over GPT-4 in zero-shot settings. The findings suggest substantial potential for advancing research in reasoning-oriented information extraction. Code for the study is accessible at GitHub. <div>
arXiv:2511.11978v1 Announce Type: new 
Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations</title>
<link>https://arxiv.org/abs/2511.12001</link>
<guid>https://arxiv.org/abs/2511.12001</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, trust, reasoning errors, multimodal moral scenarios, explanations

<br /><br />Summary: This article investigates the dual role of Chain-of-Thought (CoT) explanations in enhancing transparency while also promoting confirmation bias in users. Researchers examine how reasoning errors in vision language models (VLMs) affect user trust and the ability to identify flaws. The study presents two main outcomes: first, users tend to equate trust with the agreement of outcomes, leading to continued reliance on the model even when the underlying reasoning is incorrect. Second, the delivery tone of the CoT explanations plays a significant role in user perception; a confident tone can suppress error detection and maintain user reliance on flawed reasoning. Consequently, users may overlook inaccuracies when presented with persuasive delivery styles. The findings underscore the importance of designing NLP systems that offer explanations fostering critical examination rather than uncritical acceptance. By highlighting the potential for CoT explanations to mislead despite their explanatory power, the research advocates for improvements in how these systems communicate reasoning to enhance users' analytical abilities. The authors commit to releasing the code publicly to encourage further exploration and development in this area. <div>
arXiv:2511.12001v1 Announce Type: new 
Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs</title>
<link>https://arxiv.org/abs/2511.12014</link>
<guid>https://arxiv.org/abs/2511.12014</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, cultural competence, evaluation benchmarks, reasoning, empirical analysis<br /><br />Summary: This article addresses the limitations of current evaluations of cultural competence in large language models (LLMs), which are increasingly used in diverse environments. Existing evaluation methods often emphasize de-contextualized correctness or forced-choice judgments, failing to account for the cultural understanding and reasoning necessary for appropriate responses. To bridge this gap, the authors propose a new set of benchmarks that present LLMs with realistic situational contexts requiring culturally grounded reasoning. In addition to the standard Exact Match metric, they introduce four complementary metrics: Coverage, Specificity, Connotation, and Coherence, which together measure various aspects of response quality. Empirical analysis of leading models shows that conventional evaluations tend to overestimate cultural competence and produce inconsistent results with high variance. In contrast, the new thick evaluation method reveals deeper reasoning differences among models, reduces outcome variance, and provides more stable and interpretable indicators of cultural understanding. This research highlights the need for improved evaluation metrics in assessing LLMs' cultural competence for better deployment in diverse cultural settings. <div>
arXiv:2511.12014v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task</title>
<link>https://arxiv.org/abs/2511.12109</link>
<guid>https://arxiv.org/abs/2511.12109</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-tuning, backtranslation, neural machine translation, Japanese corpus, low-resource languages  

<br /><br />Summary: This paper investigates the combination of fine-tuning and backtranslation for improving neural machine translation of a small Japanese corpus. It starts with a baseline English-to-Japanese model, achieving a COMET score of 0.460. The authors then implemented backtranslation using synthetic data from monolingual Japanese corpora, which resulted in a modest improvement to a COMET score of 0.468. Following this, they fine-tuned the model using a small, authentic parallel dataset from diverse Japanese news and literary sources, leading to a significant increase to COMET = 0.589 using the Mistral 7B architecture. The authors further enhanced the model's performance by combining both strategies: augmenting the small dataset with backtranslated examples and subsequently applying fine-tuning, which yielded a final COMET score of 0.597. These findings highlight the effectiveness of integrating backtranslation and targeted fine-tuning, demonstrating significant improvements in translation quality for limited training data scenarios. This approach presents a practical and efficient method for enhancing translations, particularly in low-resource language pairs like Japanese. <div>
arXiv:2511.12109v1 Announce Type: new 
Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12116</link>
<guid>https://arxiv.org/abs/2511.12116</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, temporal boundaries, LLMLagBench, training data, response accuracy

<br /><br />Summary: The paper addresses the limitations of Large Language Models (LLMs) regarding their knowledge cutoff, which restricts their ability to provide accurate information beyond a specific temporal point. This knowledge boundary often leads to a blend of outdated, time-sensitive information with general data during reasoning tasks, raising concerns about response accuracy. To tackle this issue, the authors introduce LLMLagBench, a systematic benchmark designed to identify the earliest probable temporal boundaries of an LLM's training data by testing its knowledge of recent events. The benchmark is applied to a diverse range of LLMs, including those with both declared and undeclared training cutoffs, to evaluate their performance. The reliability of LLMLagBench is further validated through manual checks and comparisons with publicly available data on the LLMs' pretraining. This research highlights the importance of understanding the temporal limits of LLMs to ensure accurate responses and facilitate better integration of external information sources when needed. The benchmark serves as a useful tool for researchers and developers to assess and improve LLMs’ relevance and accuracy in dynamic contexts. <div>
arXiv:2511.12116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection</title>
<link>https://arxiv.org/abs/2511.12130</link>
<guid>https://arxiv.org/abs/2511.12130</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Conversational Stance Detection, U-MStance, PRISM, user-centric, stance detection

<br /><br />Summary: The increasing volume of multimodal social media content has prompted research in Multimodal Conversational Stance Detection (MCSD), which interprets user attitudes in discussions. Existing studies face two main limitations: pseudo-multimodality, where visual elements in posts are not aligned with text-only comments, and user homogeneity, which overlooks individual differences in stance expression. To address these challenges, the authors introduce U-MStance, a pioneering user-centric dataset with over 40,000 annotated comments on six real-world targets. They also propose PRISM, a Persona-Reasoned multimodal stance model. PRISM develops user personas from past interactions to capture unique characteristics and aligns textual and visual cues via Chain-of-Thought reasoning for better understanding of context. Additionally, it implements a mutual task reinforcement mechanism to optimize stance detection and response generation simultaneously, facilitating knowledge transfer. Experimental results on U-MStance highlight significant performance improvements of PRISM over existing strong baselines, validating the importance of user-centric and context-aware multimodal reasoning for a realistic understanding of stances in online interactions. <div>
arXiv:2511.12130v1 Announce Type: new 
Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing</title>
<link>https://arxiv.org/abs/2511.12133</link>
<guid>https://arxiv.org/abs/2511.12133</guid>
<content:encoded><![CDATA[
<div> Keywords: persuasive dialogue, telemarketing, reinforcement learning, Large Language Models, evaluation framework<br /><br />Summary: This paper addresses the challenges in goal-driven persuasive dialogue systems, particularly in telemarketing contexts, where multi-turn planning and factual accuracy are crucial. The authors identify limitations in existing methods due to scarce task-specific data and the shortcomings of direct Large Language Model (LLM) applications, such as strategic brittleness and hallucinations. To overcome these issues, they introduce TeleSalesCorpus, the first real-world grounded dialogue dataset for telemarketing. The proposed AI-Salesman framework features a dual-stage architecture: during training, it employs a Bayesian-supervised reinforcement learning algorithm designed to learn resilient sales strategies from noisy data. At inference, the Dynamic Outline-Guided Agent (DOGA) uses a pre-built script library to provide dynamic, turn-by-turn strategic guidance, enhancing dialogue coherence and effectiveness. Additionally, the authors design a comprehensive evaluation framework that integrates detailed metrics for essential sales skills alongside an LLM-as-a-Judge mechanism to assess performance. Experimental results demonstrate that AI-Salesman substantially outperforms baseline models in both automatic metrics and human evaluations, confirming its effectiveness in complex persuasive dialogue scenarios such as telemarketing. This work advances the development of robust, goal-driven dialogue systems with practical real-world applicability. <div>
arXiv:2511.12133v1 Announce Type: new 
Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding</title>
<link>https://arxiv.org/abs/2511.12140</link>
<guid>https://arxiv.org/abs/2511.12140</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal, Large Language Models, hallucination detection, VBackChecker, R^2-HalBench

<br /><br />Summary: This paper addresses the issue of hallucinations in Multimodal Large Language Models (MLLMs), which affects their reliability in practical applications. To combat this, the authors introduce VBackChecker, a reference-free framework that checks the consistency between MLLM-generated responses and visual inputs using a pixellevel Grounding LLM with reasoning and referring segmentation capabilities. The framework excels in rich-context scenarios and provides interpretability. To support VBackChecker, the authors present a new data generation pipeline called R-Instruct, which produces instruction-tuning data with rich-context descriptions, grounding masks, and hard negative samples. Additionally, they introduce R^2-HalBench, a novel hallucination benchmark for MLLMs that features real-world, rich-context descriptions from 18 MLLMs, along with high-quality annotations that cover various object, attribute, and relationship-level details. In performance evaluations, VBackChecker outperforms existing complex frameworks and achieves state-of-the-art results on the R^2-HalBench, rivaling GPT-4o in hallucination detection capabilities. Furthermore, it shows a more than 10% improvement in pixel-level grounding tasks compared to previous methods. The authors provide access to all related codes, data, and models at their GitHub repository. <div>
arXiv:2511.12140v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic</title>
<link>https://arxiv.org/abs/2511.12159</link>
<guid>https://arxiv.org/abs/2511.12159</guid>
<content:encoded><![CDATA[
<div> Keywords: Tool-Integrated Reasoning, large language models, CriticSearch, dense rewards, multi-hop reasoning

<br /><br />Summary: This paper introduces CriticSearch, a novel framework designed to enhance Tool-Integrated Reasoning (TIR) in large language models by leveraging search engines for real-time knowledge retrieval. Traditional search agent pipelines often face challenges due to reliance on reinforcement learning, which can lead to issues with sparse rewards that hinder effective exploration and training stability. CriticSearch addresses this by implementing a fine-grained credit-assignment mechanism that provides dense, turn-level feedback through a retrospective critique from a frozen, asymmetric large language model. This critic evaluates each interaction based on comprehensive input, including the entire trajectory and correct responses, transforming evaluations into stable rewards that promote more efficient policy improvements. Experimental results reveal that CriticSearch significantly outperforms existing methods on various multi-hop reasoning tasks, demonstrating quicker convergence rates, enhanced training stability, and overall better performance metrics. This advancement suggests that integrating a structured critique process can substantially refine the operational efficacy of language models in complex question-answering scenarios. <div>
arXiv:2511.12159v1 Announce Type: new 
Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues</title>
<link>https://arxiv.org/abs/2511.12213</link>
<guid>https://arxiv.org/abs/2511.12213</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained entity recognition, MME-RAG, retrieval-augmented generation, domain adaptation, KeyInfo retriever

<br /><br />Summary: Fine-grained entity recognition is essential for reasoning in task-oriented dialogues, yet current large language models struggle with domain adaptation and retrieval controllability. The paper presents MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework, which divides entity recognition into two coordinated stages. The first stage involves type-level judgment executed by lightweight managers, while the second stage focuses on span-level extraction by specialized experts. Each expert uses a KeyInfo retriever that provides semantically aligned, few-shot examples during inference, allowing for precise extraction without requiring additional training. The authors conducted experiments on various datasets including CrossNER, MIT-Movie, and MIT-Restaurant, as well as a newly created multi-domain customer-service dataset, revealing that MME-RAG outperforms recent baseline models across most domains. Further, ablation studies indicate that both the hierarchical approach and KeyInfo-guided retrieval significantly enhance robustness and cross-domain generalization. These findings position MME-RAG as a scalable, interpretable solution for enhancing adaptive dialogue understanding in different contexts. <div>
arXiv:2511.12213v1 Announce Type: new 
Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts</title>
<link>https://arxiv.org/abs/2511.12236</link>
<guid>https://arxiv.org/abs/2511.12236</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hallucination detection, API, CONFACTCHECK, factual probes

<br /><br />Summary: Large language models (LLMs) exhibit impressive text generation capabilities, yet they often produce factually incorrect content, a phenomenon known as hallucination. This issue is particularly concerning in sensitive areas such as healthcare and finance. Generally, LLMs are accessed via APIs provided by vendors, limiting user control over model weights and fine-tuning capabilities. Existing hallucination detection methods typically require multiple API calls, which can lead to increased latency and costs. To address these challenges, the authors introduce CONFACTCHECK, a novel and efficient approach for detecting hallucinations without relying on external knowledge bases. CONFACTCHECK operates on the premise that factual probes embedded in the generated text should yield consistent responses from a single LLM and across different models. Through rigorous empirical evaluation on diverse datasets — encompassing both factual text generation and open-ended generation — CONFACTCHECK demonstrates a capacity to detect hallucinated facts more efficiently and accurately than existing methods that function under similar constraints. This advancement suggests a promising direction for improving the reliability of LLMs in practical applications, offering significant benefits in resource management and accuracy. The accompanying code for CONFACTCHECK is made publicly available for further exploration. <div>
arXiv:2511.12236v1 Announce Type: new 
Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations</title>
<link>https://arxiv.org/abs/2511.12249</link>
<guid>https://arxiv.org/abs/2511.12249</guid>
<content:encoded><![CDATA[
<div> Keywords: Vietnamese NLP, contextualized embeddings, Word Sense Disambiguation, contrastive learning, semantic evaluation<br /><br />Summary:  
The paper addresses the lack of robust semantic understanding models and resources for the Vietnamese language, which is a low-resource language compared to English. It introduces ViConBERT, a new framework that learns Vietnamese contextualized word embeddings by combining contrastive learning (SimCLR) with gloss-based distillation to better capture word meanings in context. Alongside the model, the authors present ViConWSD, the first large-scale synthetic dataset for semantic evaluation in Vietnamese, supporting both Word Sense Disambiguation (WSD) and contextual similarity tasks. Experimental results demonstrate that ViConBERT significantly outperforms strong baseline models on WSD, achieving an F1 score of 0.87. Furthermore, ViConBERT delivers competitive performance on semantic similarity benchmarks ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), indicating that it effectively models both discrete word senses and graded semantic relations. The paper emphasizes the importance of combining contrastive learning with gloss-based knowledge distillation to enhance semantic representation in a low-resource language setting. Lastly, the authors contribute to the community by making their code, trained models, and datasets publicly available for further research and development in Vietnamese natural language processing. <div>
arXiv:2511.12249v1 Announce Type: new 
Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor</title>
<link>https://arxiv.org/abs/2511.12281</link>
<guid>https://arxiv.org/abs/2511.12281</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt compression, Large Language Models, compress inputs, compression rate, Cmprsr

<br /><br />Summary:  
This paper introduces a novel paradigm for prompt compression utilizing smaller Large Language Models (LLMs) to compress inputs for larger models, addressing the high costs associated with using black-box LLMs. It presents the first extensive benchmark evaluating the performance of 25 different open- and closed-source models as compressors, highlighting significant disparities in their abilities to preserve semantically important information and adhere to user-defined compression rates (CR). The study features improvements in the performance of gpt-4.1-mini, identified as the best vanilla compressor, via Textgrad-based compression meta-prompt optimization. Furthermore, it identifies Qwen3-4B as a promising open-source model, which is subsequently post-trained through supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to enhance CR adherence and downstream task performance. This results in the creation of a new model named Cmprsr, which outperforms traditional extractive and vanilla abstractive compression techniques across varying input lengths and domains, validated using datasets such as MeetingBank, LongBench, and GSM8k. Cmprsr is capable of closely following the requested compression rate, facilitating better control over the trade-off between cost and quality. <div>
arXiv:2511.12281v1 Announce Type: new 
Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugAbEx : Way Forward for Extractive Case Summarization</title>
<link>https://arxiv.org/abs/2511.12290</link>
<guid>https://arxiv.org/abs/2511.12290</guid>
<content:encoded><![CDATA[
<div> Keywords: legal summarization, extractive summaries, abstractive summaries, dataset augmentation, natural language processing  

<br /><br />Summary: The complexity of legal documents presents a significant cognitive challenge for law practitioners, prompting interest in automatic summarization techniques. Researchers are increasingly focusing on extractive summarizers due to the limitations of abstractive summarization methods, which may misrepresent legal nuances. To address the high cost of creating gold standard extractive summaries, the authors propose an innovative pipeline that transforms existing abstractive summaries into corresponding extractive versions, ensuring that expert insights are transferred accurately. This project aims to enhance seven existing legal case summarization datasets by adding extractive summaries alongside their abstractive counterparts, thereby creating a more robust resource for research in this area. To maintain the quality of these new extractive summaries, a detailed comparative evaluation with the original abstractive summaries is conducted, examining structural, lexical, and semantic aspects. The authors assert that the merger of these datasets will provide valuable opportunities for advancing automatic summarization methodologies within legal contexts. They also commit to making the augmented datasets publicly available, further supporting the research community's efforts in improving legal document summarization. <div>
arXiv:2511.12290v1 Announce Type: new 
Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering</title>
<link>https://arxiv.org/abs/2511.12300</link>
<guid>https://arxiv.org/abs/2511.12300</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, NLP tasks, quizzes, human performance, numerical answers  

<br /><br />Summary: This study explores the comparative performance of Large Language Models (LLMs) and humans on quiz questions in a buzzer setting. The researchers began by collecting Japanese quiz data, which included the questions, answers, and the correct response rates of human participants. They then prompted LLMs to attempt these quizzes under various conditions. The focus of the investigation was to understand if the challenges that are difficult for humans in quiz formats are similarly challenging for LLMs. The findings revealed that LLMs exhibited a lower correct answer rate than humans, especially for quizzes where the correct answers were not found in Wikipedia entries, indicating that LLMs rely heavily on the information present in such datasets. Additionally, LLMs faced significant difficulty with questions requiring numerical responses. Overall, the research highlights the nuanced differences in how LLMs and humans tackle quiz-based challenges, suggesting that LLMs may have limitations in specific areas that do not align with human cognitive strengths. <div>
arXiv:2511.12300v1 Announce Type: new 
Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load</title>
<link>https://arxiv.org/abs/2511.12381</link>
<guid>https://arxiv.org/abs/2511.12381</guid>
<content:encoded><![CDATA[
<div> Keywords: ironic rebound, negation instructions, large language models, polarity separation, ReboundBench<br /><br />Summary:  
This study investigates the phenomenon of ironic rebound, where negation instructions like "do not mention X" paradoxically increase the prominence of X in human thought, and similarly affect large language models (LLMs). The authors conducted two key experiments: (1) Load & content: after negation instructions, they varied distractor text types—including semantic, syntactic, and repetition—to measure the strength of rebound; (2) Polarity separation: they tested if models can distinguish neutral from negative framings of the same concept and whether this ability predicts the persistence of rebound. Results indicated that ironic rebound arises immediately following negation and intensifies particularly with longer or semantically rich distractors, whereas repetition appears to aid suppression of the forbidden concept. Additionally, stronger polarity separation by models correlated with more persistent rebound effects. A mechanistic analysis using circuit tracing revealed that sparse attention heads in middle layers amplify forbidden tokens despite suppression occurring in early layers, providing insight into how long-context interference occurs in LLMs. To facilitate further research, the authors also release ReboundBench, a dataset of 5,000 systematically varied negation prompts designed to rigorously probe rebound phenomena in large language models. <div>
arXiv:2511.12381v1 Announce Type: new 
Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Phonemes to Meaning: Evaluating Large Language Models on Tamil</title>
<link>https://arxiv.org/abs/2511.12387</link>
<guid>https://arxiv.org/abs/2511.12387</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Tamil, ILAKKANAM, linguistic evaluation, performance  

<br /><br />Summary:  
This paper addresses the performance of Large Language Models (LLMs) in low-resource and morphologically rich languages like Tamil, which has not been thoroughly researched. Existing multilingual benchmarks are often based on translated English datasets, neglecting the unique linguistic and cultural aspects of Tamil. To tackle this issue, the authors introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark, which is crafted from 820 questions sourced from Sri Lankan Tamil examination papers. These questions are categorized by trained linguists into five linguistic categories and a factual knowledge category, covering Grades 1-13 for comprehensive linguistic representation. The evaluation of both closed-source and open-source LLMs revealed that Gemini 2.5 outperformed others, while open-source models exhibited poorer performance, indicating a need for better linguistic grounding. Further breakdowns show a consistent performance in lower-grade questions but a noticeable decline with increasing complexity. Notably, the study found no strong correlation between a model's overall performance and its effectiveness in identifying linguistic categories, suggesting that observed performance may stem from exposure rather than authentic understanding of the language. <div>
arXiv:2511.12387v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models</title>
<link>https://arxiv.org/abs/2511.12464</link>
<guid>https://arxiv.org/abs/2511.12464</guid>
<content:encoded><![CDATA[
<div> Keywords: reward models, preference dimensions, MRMBench, inference-time probing, multi-objective optimization  

<br /><br />Summary: This work addresses the limitations of existing methods for evaluating reward models, which typically use a fixed pairwise ranking test set without detailed performance insights on preference dimensions. To tackle this challenge, the authors introduce the Multi-dimensional Reward Model Benchmark (MRMBench), comprising six probing tasks that assess different preference dimensions. This benchmark encourages the development of reward models that effectively capture diverse preferences. Additionally, the study presents an analysis technique called inference-time probing, which enhances the interpretability of reward predictions by identifying the preference dimensions utilized during predictions. Extensive experiments demonstrate that MRMBench correlates strongly with the alignment performance of large language models (LLMs), establishing it as a valuable tool for reward model development. Analysis of MRMBench results reveals an ongoing struggle within reward models to accurately reflect multi-dimensional preferences, suggesting that multi-objective optimization could enhance reward modeling. Furthermore, the inference-time probing method provides a reliable metric for evaluating the confidence of reward predictions, thereby improving the alignment of LLMs and highlighting the need for more nuanced evaluation methods in this field. <div>
arXiv:2511.12464v1 Announce Type: new 
Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing</title>
<link>https://arxiv.org/abs/2511.12472</link>
<guid>https://arxiv.org/abs/2511.12472</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, knowledge graph, serendipity, KGQA, drug repurposing  

<br /><br />Summary: Large Language Models (LLMs) have significantly improved knowledge graph question answering (KGQA), but they mainly return predictable answers. This paper introduces the concept of serendipity-aware KGQA, emphasizing LLMs' potential to provide surprising and novel insights. The authors define the serendipity-aware KGQA task and propose the SerenQA framework to assess LLMs' capabilities in discovering unexpected insights within scientific KGQA tasks. SerenQA features a robust serendipity metric that evaluates relevance, novelty, and surprise, along with a benchmark derived from the Clinical Knowledge Graph, specifically targeting drug repurposing. The framework also includes a structured evaluation pipeline with three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Experimental results indicate that while state-of-the-art LLMs excel in retrieval tasks, they struggle to deliver genuinely surprising and valuable insights. This highlights the need for further research and enhancements in this area. The paper provides curated resources and an extended version, which are accessible at the designated link. Overall, the findings demonstrate a significant gap in LLM capabilities regarding the identification of serendipitous answers in KGQA. <div>
arXiv:2511.12472v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGuard-v1: Safety Guardrail for Large Language Models</title>
<link>https://arxiv.org/abs/2511.12497</link>
<guid>https://arxiv.org/abs/2511.12497</guid>
<content:encoded><![CDATA[
<div> Keywords: SGuard-v1, Large Language Models, ContentFilter, JailbreakFilter, safety performance

<br /><br />Summary: The article introduces SGuard-v1, a lightweight safety guardrail designed for Large Language Models (LLMs). It consists of two specialized models: ContentFilter and JailbreakFilter. The ContentFilter identifies safety risks in prompts and responses, adhering to the MLCommons hazard taxonomy for AI trust and safety assessment. The JailbreakFilter, on the other hand, is trained using a well-structured curriculum that encompasses integrated datasets and previous findings on adversarial prompting, addressing 60 significant attack types while minimizing false-unsafe classifications. SGuard-v1 utilizes the 2B-parameter Granite-3.3-2B-Instruct model, supporting 12 languages. A total of approximately 1.4 million training instances were curated from collected and synthesized data for further instruction tuning, distributing the data between the two components according to their specific roles. Through comprehensive evaluations against public and proprietary safety benchmarks, SGuard-v1 has achieved state-of-the-art safety performance while maintaining a lightweight structure, which helps reduce deployment overhead. Additionally, it enhances interpretability by providing multi-class safety predictions alongside binary confidence scores. The release of SGuard-v1 under the Apache-2.0 License aims to facilitate continued research and practical applications in AI safety. <div>
arXiv:2511.12497v1 Announce Type: new 
Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs</title>
<link>https://arxiv.org/abs/2511.12504</link>
<guid>https://arxiv.org/abs/2511.12504</guid>
<content:encoded><![CDATA[
<div> Keywords: QA-Noun, semantic alignment, noun-centered semantics, QA-SRL, fine-grained decomposition  

<br /><br />Summary: The paper introduces QA-Noun, a novel QA-based framework aimed at capturing noun-centered semantic relations, addressing a gap in existing semantic approaches that have primarily focused on predicate-argument interactions. QA-Noun defines nine question templates that explore both explicit syntactical roles and implicit contextual roles for nouns, facilitating the creation of interpretable QA pairs that enhance traditional verbal QA-Semantic Role Labeling (QA-SRL). A dataset comprising over 2,000 annotated noun mentions is released alongside detailed guidelines, and a trained model that integrates with QA-SRL is presented, allowing for a unified semantic decomposition of sentence meanings into highly fine-grained facts. Evaluation results demonstrate that QA-Noun achieves nearly full coverage of Abstract Meaning Representation (AMR) noun arguments while uncovering additional implied relations. Furthermore, when QA-Noun is combined with QA-SRL, it provides more than 130% greater granularity compared to recent fact-based decomposition methods like FactScore and DecompScore. Overall, QA-Noun enriches the established QA-based semantic framework, contributing to a comprehensive and scalable approach for fine-grained semantic decomposition tailored for cross-text alignment. <div>
arXiv:2511.12504v1 Announce Type: new 
Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2511.12520</link>
<guid>https://arxiv.org/abs/2511.12520</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, TAdaRAG, knowledge graph, intent-driven routing, reinforcement learning  

<br /><br />Summary:  
The paper presents TAdaRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by constructing task-adaptive knowledge graphs in real-time from external sources. Traditional RAG models often suffer from information loss and irrelevant detail retrieval due to input context limitations, leading to issues such as response hallucinations and poor reasoning. To combat these challenges, TAdaRAG introduces an intent-driven routing mechanism that directs to specific extraction templates tailored for particular domains. This is complemented by a supervised fine-tuning process and a reinforcement learning-based implicit extraction method. These innovations ensure the integration of knowledge is concise, coherent, and free of redundancy. Evaluations conducted on six public benchmarks, along with a real-world business benchmark called NowNewsQA, demonstrate that TAdaRAG significantly outperforms existing methods across various domains and long-text tasks. These results underline its strong generalization capabilities and practical effectiveness, making it a notable advancement in the field of language model enhancement through external knowledge integration. <div>
arXiv:2511.12520v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Length Bias in RLHF through a Causal Lens</title>
<link>https://arxiv.org/abs/2511.12573</link>
<guid>https://arxiv.org/abs/2511.12573</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Human Feedback, Length Bias, Reward Model, Content Quality  

<br /><br />Summary: The article discusses the prevalent issue of length bias in reinforcement learning from human feedback (RLHF) when aligning large language models (LLMs) with human preferences. Length bias manifests as a tendency for reward models to favor longer responses, equating verbosity with quality. To address this issue, the authors propose a causal framework that includes a counterfactual data augmentation method. This method generates response pairs aimed at disentangling content quality from verbosity. They create two types of pairs: length-divergent pairs with similar content and content-divergent pairs of similar length. Empirical evaluations show that this approach significantly reduces the length bias in reward assignments, leading to outputs that are more concise and focused on content. The proposed method enhances the robustness and content sensitivity of reward modeling in RLHF pipelines. Overall, this research contributes to improving the alignment of LLMs with human preferences by mitigating the biases associated with response length, ensuring that the quality of responses is evaluated based on meaningful content rather than mere verbosity. <div>
arXiv:2511.12573v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMWOZ: Building Multimodal Agent for Task-oriented Dialogue</title>
<link>https://arxiv.org/abs/2511.12586</link>
<guid>https://arxiv.org/abs/2511.12586</guid>
<content:encoded><![CDATA[
<div> Task-oriented dialogue systems, multimodal dialogue, GUI interaction, dataset MMWOZ, multimodal model MATE<br /><br />Summary:<br /><br />This paper addresses the limitations of traditional task-oriented dialogue systems that rely on customized back-end APIs, which are often unavailable in real-world scenarios dominated by front-end Graphical User Interfaces (GUIs). To bridge this gap, the authors introduce MMWOZ, a new multimodal dialogue dataset derived from the existing MultiWOZ 2.3 dataset. The dataset creation involved developing a web-style GUI as the dialogue front-end, devising an automated script to translate dialogue states and system actions into executable GUI operation instructions, and collecting snapshots of web pages paired with these instructions. Additionally, the authors propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue), which serves as a baseline for tasks using the MMWOZ dataset. Comprehensive experiments with MATE are conducted to analyze the capabilities and challenges of building practical multimodal task-oriented dialogue agents. The work contributes to advancing dialogue systems that operate effectively through visual interfaces, closer to real-world application environments where backend APIs are not always accessible. <div>
arXiv:2511.12586v1 Announce Type: new 
Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group-Aware Reinforcement Learning for Output Diversity in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12596</link>
<guid>https://arxiv.org/abs/2511.12596</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, mode collapse, Group-Aware Policy Optimization, response diversity, frequency-aware reward function

<br /><br />Summary: This article addresses the issue of mode collapse in Large Language Models (LLMs), where these models tend to generate limited and repetitive outputs despite the presence of multiple valid responses. The authors introduce a novel approach called Group-Aware Policy Optimization (GAPO), which builds upon the recently developed Group Relative Policy Optimization (GRPO). GAPO focuses on evaluating rewards at a group level, which allows for improvements in model learning related to diversity and coverage. To demonstrate its effectiveness, the authors implement a frequency-aware reward function that promotes more uniform sampling among valid LLM completions. The results show that models trained using GAPO are capable of generating not only valid but also more diverse responses compared to traditional methods. Additionally, GAPO proves adaptable to various open-ended prompts, enhancing response diversity without sacrificing accuracy across established LLM benchmarks, including GSM8K, MATH, HumanEval, and MMLU-Pro. The researchers plan to publicly release their code, making their advancements accessible for further exploration in the field of language modeling. <div>
arXiv:2511.12596v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</title>
<link>https://arxiv.org/abs/2511.12609</link>
<guid>https://arxiv.org/abs/2511.12609</guid>
<content:encoded><![CDATA[
<div> Keywords: Uni-MoE 2.0, omnimodal large model, Mixture-of-Experts, reinforcement strategy, SOTA benchmarks  

<br /><br />Summary:  
We introduce Uni-MoE 2.0 from the Lychee family, a fully open-source omnimodal large model (OLM) that enhances the capabilities of the Uni-MoE series in multimodal understanding and generation. Built on the Qwen2.5-7B dense architecture, it incorporates dynamic-capacity Mixture-of-Experts (MoE) design and a progressive training strategy with an iterative reinforcement component. The model excels in understanding and generating images, text, and speech by employing a new MoE framework that supports 10 cross-modal inputs while ensuring computational efficiency. It utilizes an Omni-Modality 3D RoPE for spatio-temporal alignment in the self-attention mechanism. Training involves cross-modal pretraining followed by a supervised fine-tuning strategy that activates modality-specific experts, supported by balanced data and an iterative GSPO-DPO method to enhance reinforcement learning. With training on approximately 75B tokens of multimodal data, Uni-MoE 2.0 demonstrates superior performance in 85 benchmarks, achieving state-of-the-art results particularly in video understanding, audiovisual reasoning, and long-form speech processing, surpassing the previous Qwen2.5-Omni model in over 50 evaluations. Key improvements include significant gains in multiple metrics for generating tasks and processing effectiveness. <div>
arXiv:2511.12609v1 Announce Type: new 
Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing</title>
<link>https://arxiv.org/abs/2511.12630</link>
<guid>https://arxiv.org/abs/2511.12630</guid>
<content:encoded><![CDATA[
<div> Keywords: NOTAM semantic parsing, aviation domain knowledge, Knots dataset, prompt engineering, automated flight safety analysis  

<br /><br />Summary:  
This paper addresses the challenge of interpreting Notice to Air Missions (NOTAMs), which contain critical flight safety information but are difficult to parse due to complex language and implicit reasoning. Unlike prior work limited to surface-level tasks like classification and named entity recognition, the authors propose a novel task called NOTAM semantic parsing that emphasizes deep semantic inference and the integration of specialized aviation knowledge to generate structured, inference-rich outputs. To support this, they introduce Knots (Knowledge and NOTAM Semantics), a high-quality dataset comprising 12,347 expert-annotated NOTAMs from 194 Flight Information Regions. The dataset was developed through a multi-agent collaborative framework ensuring comprehensive annotation coverage across various fields. The study systematically evaluates diverse prompt-engineering and model adaptation techniques to improve machine understanding and processing of aviation texts. Experimental results demonstrate significant performance gains, validating the effectiveness of their approach for automated NOTAM analysis. Additionally, the released codebase provides a valuable resource for further research and development in aviation safety and natural language processing. <div>
arXiv:2511.12630v1 Announce Type: new 
Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing</title>
<link>https://arxiv.org/abs/2511.12661</link>
<guid>https://arxiv.org/abs/2511.12661</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, faithfulness, reasoning tasks, SFT+RL, alignment

<br /><br />Summary: The article addresses the challenge of aligning Large Language Models (LLMs) to ensure they remain faithful to new knowledge during complex, multi-hop reasoning tasks. The authors identify a "faithfulness gap" in state-of-the-art methods like Reason-KE, where the focus on format mimicry leads to critical factual hallucinations. To tackle this problem, they introduce Reason-KE++, an SFT+RL framework designed to enhance process-level faithfulness. A key component of this framework is the Stage-aware Reward mechanism, which provides dense supervision for intermediate reasoning steps such as decomposition and sub-answer correctness. The authors caution against the use of naive outcome-only reinforcement learning, which can undermine reasoning integrity while falsely improving final accuracy. Through their method, they achieved a new state-of-the-art performance of 95.48% on the MQUAKE-CF-3k benchmark, surpassing previous results by 5.28%. The findings emphasize that refining and aligning the reasoning process is critical to developing trustworthy LLMs for complex tasks. Overall, this work highlights the necessity of a more nuanced approach to LLM alignment, focusing on both reasoning processes and final outputs. <div>
arXiv:2511.12661v1 Announce Type: new 
Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data</title>
<link>https://arxiv.org/abs/2511.12690</link>
<guid>https://arxiv.org/abs/2511.12690</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-to-speech translation, Persian, English, synthetic data, low-resource languages

<br /><br />Summary: This paper introduces a direct speech-to-speech translation (S2ST) system that translates Persian speech into English speech, addressing the challenge posed by the scarcity of parallel speech data in low-resource languages. The proposed model is composed of three main components: a conformer-based encoder for mapping source speech to high-level acoustic representations, a causal transformer decoder that translates these representations into target speech units, and a unit-based neural vocoder for generating waveforms from the predicted discrete units. To tackle data limitations, the authors created a new Persian-English parallel speech corpus by leveraging a large language model to translate Persian transcriptions into English and then using a state-of-the-art zero-shot text-to-speech system for synthesizing the corresponding English speech. This innovative approach significantly enhances the availability of parallel speech data, increasing it roughly sixfold. Testing on the Persian-English portion of the CVSS corpus revealed notable improvements, with the model achieving 4.6 ASR BLEU over standard direct baselines when utilizing the synthetic data. The study concludes that self-supervised pre-training, discrete speech units, and synthetic parallel data effectively enhance direct S2ST performance for low-resource languages like Persian-English. <div>
arXiv:2511.12690v1 Announce Type: new 
Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs</title>
<link>https://arxiv.org/abs/2511.12710</link>
<guid>https://arxiv.org/abs/2511.12710</guid>
<content:encoded><![CDATA[
<div> Keywords: EvoSynth, jailbreak, large language models, evolutionary synthesis, attack algorithms<br /><br />Summary: EvoSynth is an innovative autonomous framework designed to create novel jailbreak methods for Large Language Models (LLMs) through evolutionary synthesis rather than relying on pre-existing attack strategies. Unlike traditional automated red teaming frameworks that refine known prompts, EvoSynth uses a multi-agent system to engineer, evolve, and execute new code-based attacks autonomously. A key feature of EvoSynth is its code-level self-correction loop, which enables the system to iteratively rewrite and improve its attack algorithms based on performance failures. Extensive experiments demonstrate that EvoSynth achieves a state-of-the-art Attack Success Rate (ASR) of 85.5% against robust LLMs like Claude-Sonnet-4.5, outperforming existing methods. Furthermore, EvoSynth generates a more diverse range of attack strategies, enhancing the creativity and effectiveness of jailbreak attempts beyond traditional frameworks. The framework and its code have been released publicly to encourage further research in the evolutionary synthesis of jailbreak methods for AI safety and robustness. This marks a significant advancement in autonomous attack generation, moving beyond simple prompt modification to dynamic and self-improving algorithmic invention. <div>
arXiv:2511.12710v1 Announce Type: new 
Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Focus Memory for Language Models</title>
<link>https://arxiv.org/abs/2511.12712</link>
<guid>https://arxiv.org/abs/2511.12712</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Adaptive Focus Memory, Safety, Token Budget, Dialogue Management  

<br /><br />Summary: Large language models (LLMs) face limitations in multi-turn dialogue due to fixed context windows and inefficient memory strategies. Traditional methods, such as replaying full conversations or using static summarization, can lead to the loss of crucial user details. This paper introduces Adaptive Focus Memory (AFM), a context management system that categorizes past messages into three fidelity levels: FULL, COMPRESSED, and PLACEHOLDER. It does this based on the semantic relevance to the current query, recency, and importance classifications. By optimizing message retention under a strict token budget, AFM prioritizes high-fidelity messages while maintaining a cost-effective summary of the dialogue. The method demonstrates effectiveness in preserving critical safety information, such as a user’s severe peanut allergy, during conversations of varying lengths. In benchmarks, AFM matches the safety performance of traditional replay methods while reducing average token usage by 66%. A modular Python implementation of AFM designed for OpenAI-compatible APIs is also provided, allowing practitioners to minimize inference costs without compromising safety and factual continuity. <div>
arXiv:2511.12712v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Brittleness of LLMs: A Journey around Set Membership</title>
<link>https://arxiv.org/abs/2511.12728</link>
<guid>https://arxiv.org/abs/2511.12728</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning tasks, set membership queries, empirical evaluation, failure modes  

<br /><br />Summary: This study investigates the paradox of large language models (LLMs) exhibiting superhuman performance on complex reasoning tasks while faltering on simpler problems, which raises issues regarding their reliability and interpretability. The researchers focus on set membership queries as a fundamental reasoning form, using straightforward tasks like determining if an item belongs to a set. By systematically evaluating various factors such as prompt phrasing, semantic structure, element ordering, and model selection, the analysis reveals that LLM performance on these elementary tasks is consistently brittle and unpredictable. The study points to a fragmented and convoluted grasp of the set concept by the models. Furthermore, the research highlights that the simplicity of the task facilitates large-scale experiments, allowing for a comprehensive mapping and analysis of failure modes. This methodology not only enriches the understanding of LLM capabilities but also establishes a valuable framework for evaluating these models in general. Overall, the findings underscore the importance of scrutinizing LLM performance on basic reasoning tasks to better assess their reliability and interpretability. <div>
arXiv:2511.12728v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidence of Phase Transitions in Small Transformer-Based Language Models</title>
<link>https://arxiv.org/abs/2511.12768</link>
<guid>https://arxiv.org/abs/2511.12768</guid>
<content:encoded><![CDATA[
<div> Keywords: phase transitions, large language models, small transformers, training dynamics, vocabulary statistics  

<br /><br />Summary: This paper investigates whether phase transitions—sudden emergent abilities previously observed in large language models (LLMs)—also occur in small transformer-based models. The authors explore three main questions: (1) whether phase transitions exist in small models, (2) if these transitions can be detected directly in the linear training space without log-scale rescaling, and (3) whether such transitions appear at early training stages. To address these questions, they train a small GPT-style transformer on a character-level dataset and analyze vocabulary usage throughout training. Key metrics include average word length, counts of correct versus incorrect words, and vocabulary diversity shifts. Additionally, the study applies Poisson and sub-Poisson statistics to measure connections and reorganizations within the learned vocabulary. The results reveal a distinct phase transition point during training that standard loss or validation curves do not capture, but which becomes evident through the proposed vocabulary-based and statistical probes. These findings indicate that phase-transition behavior is a general feature of language model training, observable even in modest-sized models, detectable directly in linear space, and emerges surprisingly early as the model achieves coherence. This work provides new insights into the nonlinear dynamics underpinning language model training and highlights the value of tailored metrics for exposing phase transitions beyond conventional evaluation methods. <div>
arXiv:2511.12768v1 Announce Type: new 
Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Reinforcement in Context</title>
<link>https://arxiv.org/abs/2511.12782</link>
<guid>https://arxiv.org/abs/2511.12782</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, alignment, interruptions, user input, Chain-of-Thought

<br /><br />Summary: 
This article addresses the current challenges in aligning Large Language Models (LLMs) with user intentions amidst the growing concerns of adversarial attacks and model misbehavior. Existing research has demonstrated that the probability of LLM jailbreak increases proportionately with the length of user input and conversations. However, there is a noticeable gap in research focused on enhancing alignment in relation to longer user inputs. To tackle this issue, the authors propose the method of interruptions, which involves embedding control sentences into the user input at regular intervals (every x tokens, where x is arbitrary). This strategy aims to provide a safeguard against potential scheming behaviors exhibited by LLMs. Furthermore, the authors suggest that this approach can be effectively incorporated into the Chain-of-Thought reasoning process, thereby reinforcing the model’s robustness while maintaining user alignment. By integrating interruptions, the proposal presents a novel mechanism that could enhance LLM performance and mitigate the risk of misinterpretation or unintended responses. Overall, this research contributes to the ongoing discourse on LLM alignment and proposes a practical avenue to improve model safety and reliability as user inputs grow in complexity and length. <div>
arXiv:2511.12782v1 Announce Type: new 
Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing</title>
<link>https://arxiv.org/abs/2511.12784</link>
<guid>https://arxiv.org/abs/2511.12784</guid>
<content:encoded><![CDATA[
<div> autoformalization, large language models, paraphrasing robustness, formal proofs, semantic validity<br /><br />Summary:<br /><br />This paper explores the robustness of large language models (LLMs) in the domain of autoformalization, focusing on how paraphrased natural language (NL) inputs affect the generation of formal proofs. The study builds upon observations from recent text-to-SQL research that demonstrated LLMs’ sensitivity to semantic-preserving paraphrases, extending this investigation to formal proof generation. To measure the effects, the authors evaluate LLM outputs using semantic validity and compilation validity metrics. The benchmarks used include MiniF2F and the Lean 4 version of ProofNet, both established datasets for mathematical formalization tasks. Two modern LLMs are employed in the experiments, with paraphrased NL statements generated and cross-evaluated across these models to assess consistency and performance stability. The findings reveal significant variability in model performance when handling different paraphrased inputs, indicating that seemingly minor shifts in natural language phrasing can substantially influence the quality and correctness of the formal proofs generated. This variability highlights challenges in ensuring grounded and verifiable autoformalizations and suggests the need for improved robustness in LLM architectures and training strategies. Ultimately, the paper contributes to understanding LLM limitations in autoformalization and informs future efforts to develop more resilient formal proof generation systems. <div>
arXiv:2511.12784v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals</title>
<link>https://arxiv.org/abs/2511.12821</link>
<guid>https://arxiv.org/abs/2511.12821</guid>
<content:encoded><![CDATA[
<div> Keywords: journal impact, AI engagement, bibliometric indicators, collaboration structures, BioMedJImpact

<br /><br />Summary: This article presents BioMedJImpact, a dataset specifically designed for analyzing journal-level scientific impact and AI engagement in biomedicine. It encompasses 1.74 million PubMed Central articles across 2,744 journals, integrating bibliometric indicators and collaboration features, along with a novel three-stage pipeline using large language models (LLMs) to assess AI engagement. The study investigates how collaboration intensity and AI engagement affect scientific impact during two distinct timeframes: pre-pandemic (2016-2019) and post-pandemic (2020-2023). Results show that journals with higher collaboration intensity, particularly those featuring larger and more diverse author teams, achieve greater citation impact. Additionally, AI engagement has emerged as a significant correlate of journal prestige, particularly evident in quartile rankings. To validate the LLM pipeline for AI engagement, human evaluations were conducted, revealing strong agreement in AI relevance detection and consistent subfield classification. Ultimately, BioMedJImpact is positioned as a comprehensive resource facilitating the analysis of scientific impact and innovation dynamics at the intersection of biomedicine and AI, while providing a validated methodological framework for future research. Code for implementation is available at https://github.com/JonathanWry/BioMedJImpact. <div>
arXiv:2511.12821v1 Announce Type: new 
Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation</title>
<link>https://arxiv.org/abs/2511.12832</link>
<guid>https://arxiv.org/abs/2511.12832</guid>
<content:encoded><![CDATA[
arXiv:2511.12832v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying consistency and accuracy of Latent Dirichlet Allocation</title>
<link>https://arxiv.org/abs/2511.12850</link>
<guid>https://arxiv.org/abs/2511.12850</guid>
<content:encoded><![CDATA[
arXiv:2511.12850v1 Announce Type: new 
Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation</title>
<link>https://arxiv.org/abs/2511.12851</link>
<guid>https://arxiv.org/abs/2511.12851</guid>
<content:encoded><![CDATA[
arXiv:2511.12851v1 Announce Type: new 
Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12861</link>
<guid>https://arxiv.org/abs/2511.12861</guid>
<content:encoded><![CDATA[
arXiv:2511.12861v1 Announce Type: new 
Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification of Hope in Textual Data using Transformer-Based Models</title>
<link>https://arxiv.org/abs/2511.12874</link>
<guid>https://arxiv.org/abs/2511.12874</guid>
<content:encoded><![CDATA[
arXiv:2511.12874v1 Announce Type: new 
Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy</title>
<link>https://arxiv.org/abs/2511.12920</link>
<guid>https://arxiv.org/abs/2511.12920</guid>
<content:encoded><![CDATA[
arXiv:2511.12920v1 Announce Type: new 
Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Room 2.0: Seeing is Not Understanding for MLLMs</title>
<link>https://arxiv.org/abs/2511.12928</link>
<guid>https://arxiv.org/abs/2511.12928</guid>
<content:encoded><![CDATA[
arXiv:2511.12928v1 Announce Type: new 
Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty</title>
<link>https://arxiv.org/abs/2511.12991</link>
<guid>https://arxiv.org/abs/2511.12991</guid>
<content:encoded><![CDATA[
arXiv:2511.12991v1 Announce Type: new 
Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models</title>
<link>https://arxiv.org/abs/2511.13029</link>
<guid>https://arxiv.org/abs/2511.13029</guid>
<content:encoded><![CDATA[
arXiv:2511.13029v1 Announce Type: new 
Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm</title>
<link>https://arxiv.org/abs/2511.13040</link>
<guid>https://arxiv.org/abs/2511.13040</guid>
<content:encoded><![CDATA[
arXiv:2511.13040v1 Announce Type: new 
Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training</title>
<link>https://arxiv.org/abs/2511.13043</link>
<guid>https://arxiv.org/abs/2511.13043</guid>
<content:encoded><![CDATA[
arXiv:2511.13043v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.13095</link>
<guid>https://arxiv.org/abs/2511.13095</guid>
<content:encoded><![CDATA[
arXiv:2511.13095v1 Announce Type: new 
Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study</title>
<link>https://arxiv.org/abs/2511.13107</link>
<guid>https://arxiv.org/abs/2511.13107</guid>
<content:encoded><![CDATA[
arXiv:2511.13107v1 Announce Type: new 
Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction</title>
<link>https://arxiv.org/abs/2511.13118</link>
<guid>https://arxiv.org/abs/2511.13118</guid>
<content:encoded><![CDATA[
arXiv:2511.13118v1 Announce Type: new 
Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition</title>
<link>https://arxiv.org/abs/2511.13126</link>
<guid>https://arxiv.org/abs/2511.13126</guid>
<content:encoded><![CDATA[
arXiv:2511.13126v1 Announce Type: new 
Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels</title>
<link>https://arxiv.org/abs/2511.13152</link>
<guid>https://arxiv.org/abs/2511.13152</guid>
<content:encoded><![CDATA[
arXiv:2511.13152v1 Announce Type: new 
Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis</title>
<link>https://arxiv.org/abs/2511.13159</link>
<guid>https://arxiv.org/abs/2511.13159</guid>
<content:encoded><![CDATA[
arXiv:2511.13159v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2511.13169</link>
<guid>https://arxiv.org/abs/2511.13169</guid>
<content:encoded><![CDATA[
arXiv:2511.13169v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translation Entropy: A Statistical Framework for Evaluating Translation Systems</title>
<link>https://arxiv.org/abs/2511.13180</link>
<guid>https://arxiv.org/abs/2511.13180</guid>
<content:encoded><![CDATA[
arXiv:2511.13180v1 Announce Type: new 
Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study</title>
<link>https://arxiv.org/abs/2511.13182</link>
<guid>https://arxiv.org/abs/2511.13182</guid>
<content:encoded><![CDATA[
arXiv:2511.13182v1 Announce Type: new 
Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms</title>
<link>https://arxiv.org/abs/2511.13225</link>
<guid>https://arxiv.org/abs/2511.13225</guid>
<content:encoded><![CDATA[
arXiv:2511.13225v1 Announce Type: new 
Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</title>
<link>https://arxiv.org/abs/2511.13254</link>
<guid>https://arxiv.org/abs/2511.13254</guid>
<content:encoded><![CDATA[
arXiv:2511.13254v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection</title>
<link>https://arxiv.org/abs/2511.13329</link>
<guid>https://arxiv.org/abs/2511.13329</guid>
<content:encoded><![CDATA[
arXiv:2511.13329v1 Announce Type: new 
Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects</title>
<link>https://arxiv.org/abs/2511.13335</link>
<guid>https://arxiv.org/abs/2511.13335</guid>
<content:encoded><![CDATA[
arXiv:2511.13335v1 Announce Type: new 
Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.13368</link>
<guid>https://arxiv.org/abs/2511.13368</guid>
<content:encoded><![CDATA[
arXiv:2511.13368v1 Announce Type: new 
Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts</title>
<link>https://arxiv.org/abs/2511.13381</link>
<guid>https://arxiv.org/abs/2511.13381</guid>
<content:encoded><![CDATA[
arXiv:2511.13381v1 Announce Type: new 
Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction</title>
<link>https://arxiv.org/abs/2511.13410</link>
<guid>https://arxiv.org/abs/2511.13410</guid>
<content:encoded><![CDATA[
arXiv:2511.13410v1 Announce Type: new 
Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Linear Scoring Model for Translation Quality Evaluation</title>
<link>https://arxiv.org/abs/2511.13467</link>
<guid>https://arxiv.org/abs/2511.13467</guid>
<content:encoded><![CDATA[
arXiv:2511.13467v1 Announce Type: new 
Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns</title>
<link>https://arxiv.org/abs/2511.13481</link>
<guid>https://arxiv.org/abs/2511.13481</guid>
<content:encoded><![CDATA[
arXiv:2511.13481v1 Announce Type: new 
Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Applying Large Language Models to Characterize Public Narratives</title>
<link>https://arxiv.org/abs/2511.13505</link>
<guid>https://arxiv.org/abs/2511.13505</guid>
<content:encoded><![CDATA[
arXiv:2511.13505v1 Announce Type: new 
Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets</title>
<link>https://arxiv.org/abs/2511.13529</link>
<guid>https://arxiv.org/abs/2511.13529</guid>
<content:encoded><![CDATA[
arXiv:2511.13529v1 Announce Type: new 
Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation</title>
<link>https://arxiv.org/abs/2511.13590</link>
<guid>https://arxiv.org/abs/2511.13590</guid>
<content:encoded><![CDATA[
arXiv:2511.13590v1 Announce Type: new 
Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents</title>
<link>https://arxiv.org/abs/2511.13593</link>
<guid>https://arxiv.org/abs/2511.13593</guid>
<content:encoded><![CDATA[
arXiv:2511.13593v1 Announce Type: new 
Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues</title>
<link>https://arxiv.org/abs/2511.13658</link>
<guid>https://arxiv.org/abs/2511.13658</guid>
<content:encoded><![CDATA[
arXiv:2511.13658v1 Announce Type: new 
Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation</title>
<link>https://arxiv.org/abs/2511.13689</link>
<guid>https://arxiv.org/abs/2511.13689</guid>
<content:encoded><![CDATA[
arXiv:2511.13689v1 Announce Type: new 
Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalist Foundation Models Are Not Clinical Enough for Hospital Operations</title>
<link>https://arxiv.org/abs/2511.13703</link>
<guid>https://arxiv.org/abs/2511.13703</guid>
<content:encoded><![CDATA[
arXiv:2511.13703v1 Announce Type: new 
Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Architecture, Scaling Laws, and Economics: A Quick Summary</title>
<link>https://arxiv.org/abs/2511.11572</link>
<guid>https://arxiv.org/abs/2511.11572</guid>
<content:encoded><![CDATA[
arXiv:2511.11572v1 Announce Type: cross 
Abstract: The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Positional and Symbolic Attention Behavior in Transformers</title>
<link>https://arxiv.org/abs/2511.11579</link>
<guid>https://arxiv.org/abs/2511.11579</guid>
<content:encoded><![CDATA[
arXiv:2511.11579v1 Announce Type: cross 
Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Anatomy of a Triton Attention Kernel</title>
<link>https://arxiv.org/abs/2511.11581</link>
<guid>https://arxiv.org/abs/2511.11581</guid>
<content:encoded><![CDATA[
arXiv:2511.11581v1 Announce Type: cross 
Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism</title>
<link>https://arxiv.org/abs/2511.11591</link>
<guid>https://arxiv.org/abs/2511.11591</guid>
<content:encoded><![CDATA[
arXiv:2511.11591v1 Announce Type: cross 
Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLINB: A Climate Intelligence Benchmark for Foundational Models</title>
<link>https://arxiv.org/abs/2511.11597</link>
<guid>https://arxiv.org/abs/2511.11597</guid>
<content:encoded><![CDATA[
arXiv:2511.11597v1 Announce Type: cross 
Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio</title>
<link>https://arxiv.org/abs/2511.11599</link>
<guid>https://arxiv.org/abs/2511.11599</guid>
<content:encoded><![CDATA[
arXiv:2511.11599v1 Announce Type: cross 
Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models</title>
<link>https://arxiv.org/abs/2511.11622</link>
<guid>https://arxiv.org/abs/2511.11622</guid>
<content:encoded><![CDATA[
arXiv:2511.11622v1 Announce Type: cross 
Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges</title>
<link>https://arxiv.org/abs/2511.11624</link>
<guid>https://arxiv.org/abs/2511.11624</guid>
<content:encoded><![CDATA[
arXiv:2511.11624v1 Announce Type: cross 
Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation</title>
<link>https://arxiv.org/abs/2511.11635</link>
<guid>https://arxiv.org/abs/2511.11635</guid>
<content:encoded><![CDATA[
arXiv:2511.11635v1 Announce Type: cross 
Abstract: High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic generation of DRI Statements</title>
<link>https://arxiv.org/abs/2511.11655</link>
<guid>https://arxiv.org/abs/2511.11655</guid>
<content:encoded><![CDATA[
arXiv:2511.11655v1 Announce Type: cross 
Abstract: Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-Model: Dynamic Neural Architectures for Adaptive Processing</title>
<link>https://arxiv.org/abs/2511.11669</link>
<guid>https://arxiv.org/abs/2511.11669</guid>
<content:encoded><![CDATA[
arXiv:2511.11669v1 Announce Type: cross 
Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems</title>
<link>https://arxiv.org/abs/2511.11678</link>
<guid>https://arxiv.org/abs/2511.11678</guid>
<content:encoded><![CDATA[
arXiv:2511.11678v1 Announce Type: cross 
Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI as a Linguistic Equalizer in Global Science</title>
<link>https://arxiv.org/abs/2511.11687</link>
<guid>https://arxiv.org/abs/2511.11687</guid>
<content:encoded><![CDATA[
arXiv:2511.11687v1 Announce Type: cross 
Abstract: For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning: From Reflection to Solution</title>
<link>https://arxiv.org/abs/2511.11712</link>
<guid>https://arxiv.org/abs/2511.11712</guid>
<content:encoded><![CDATA[
arXiv:2511.11712v1 Announce Type: cross 
Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy</title>
<link>https://arxiv.org/abs/2511.11816</link>
<guid>https://arxiv.org/abs/2511.11816</guid>
<content:encoded><![CDATA[
arXiv:2511.11816v1 Announce Type: cross 
Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better LLM Reasoning via Dual-Play</title>
<link>https://arxiv.org/abs/2511.11881</link>
<guid>https://arxiv.org/abs/2511.11881</guid>
<content:encoded><![CDATA[
arXiv:2511.11881v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting-MarI: LLM Unlearning via Marginal Information Regularization</title>
<link>https://arxiv.org/abs/2511.11914</link>
<guid>https://arxiv.org/abs/2511.11914</guid>
<content:encoded><![CDATA[
arXiv:2511.11914v1 Announce Type: cross 
Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles</title>
<link>https://arxiv.org/abs/2511.12010</link>
<guid>https://arxiv.org/abs/2511.12010</guid>
<content:encoded><![CDATA[
arXiv:2511.12010v1 Announce Type: cross 
Abstract: We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys</title>
<link>https://arxiv.org/abs/2511.12036</link>
<guid>https://arxiv.org/abs/2511.12036</guid>
<content:encoded><![CDATA[
arXiv:2511.12036v1 Announce Type: cross 
Abstract: We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs</title>
<link>https://arxiv.org/abs/2511.12280</link>
<guid>https://arxiv.org/abs/2511.12280</guid>
<content:encoded><![CDATA[
arXiv:2511.12280v1 Announce Type: cross 
Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer</title>
<link>https://arxiv.org/abs/2511.12285</link>
<guid>https://arxiv.org/abs/2511.12285</guid>
<content:encoded><![CDATA[
arXiv:2511.12285v1 Announce Type: cross 
Abstract: Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing</title>
<link>https://arxiv.org/abs/2511.12347</link>
<guid>https://arxiv.org/abs/2511.12347</guid>
<content:encoded><![CDATA[
arXiv:2511.12347v1 Announce Type: cross 
Abstract: We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions</title>
<link>https://arxiv.org/abs/2511.12452</link>
<guid>https://arxiv.org/abs/2511.12452</guid>
<content:encoded><![CDATA[
arXiv:2511.12452v1 Announce Type: cross 
Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Layout: LLM-driven Co-optimization for Interior Layout</title>
<link>https://arxiv.org/abs/2511.12474</link>
<guid>https://arxiv.org/abs/2511.12474</guid>
<content:encoded><![CDATA[
arXiv:2511.12474v1 Announce Type: cross 
Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolving Prompts for Toxicity Search in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12487</link>
<guid>https://arxiv.org/abs/2511.12487</guid>
<content:encoded><![CDATA[
arXiv:2511.12487v1 Announce Type: cross 
Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing</title>
<link>https://arxiv.org/abs/2511.12529</link>
<guid>https://arxiv.org/abs/2511.12529</guid>
<content:encoded><![CDATA[
arXiv:2511.12529v1 Announce Type: cross 
Abstract: Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Content-Preserving Secure Linguistic Steganography</title>
<link>https://arxiv.org/abs/2511.12565</link>
<guid>https://arxiv.org/abs/2511.12565</guid>
<content:encoded><![CDATA[
arXiv:2511.12565v1 Announce Type: cross 
Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance</title>
<link>https://arxiv.org/abs/2511.12997</link>
<guid>https://arxiv.org/abs/2511.12997</guid>
<content:encoded><![CDATA[
arXiv:2511.12997v1 Announce Type: cross 
Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics</title>
<link>https://arxiv.org/abs/2511.13021</link>
<guid>https://arxiv.org/abs/2511.13021</guid>
<content:encoded><![CDATA[
arXiv:2511.13021v1 Announce Type: cross 
Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization</title>
<link>https://arxiv.org/abs/2511.13091</link>
<guid>https://arxiv.org/abs/2511.13091</guid>
<content:encoded><![CDATA[
arXiv:2511.13091v1 Announce Type: cross 
Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms</title>
<link>https://arxiv.org/abs/2511.13238</link>
<guid>https://arxiv.org/abs/2511.13238</guid>
<content:encoded><![CDATA[
arXiv:2511.13238v1 Announce Type: cross 
Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment</title>
<link>https://arxiv.org/abs/2511.13290</link>
<guid>https://arxiv.org/abs/2511.13290</guid>
<content:encoded><![CDATA[
arXiv:2511.13290v1 Announce Type: cross 
Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research</title>
<link>https://arxiv.org/abs/2511.13333</link>
<guid>https://arxiv.org/abs/2511.13333</guid>
<content:encoded><![CDATA[
arXiv:2511.13333v1 Announce Type: cross 
Abstract: Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Grounded Enhancement for Visual Document Retrieval</title>
<link>https://arxiv.org/abs/2511.13415</link>
<guid>https://arxiv.org/abs/2511.13415</guid>
<content:encoded><![CDATA[
arXiv:2511.13415v1 Announce Type: cross 
Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Multi-Table Retrieval Through Iterative Search</title>
<link>https://arxiv.org/abs/2511.13418</link>
<guid>https://arxiv.org/abs/2511.13418</guid>
<content:encoded><![CDATA[
arXiv:2511.13418v1 Announce Type: cross 
Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models</title>
<link>https://arxiv.org/abs/2511.13548</link>
<guid>https://arxiv.org/abs/2511.13548</guid>
<content:encoded><![CDATA[
arXiv:2511.13548v1 Announce Type: cross 
Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P1: Mastering Physics Olympiads with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13612</link>
<guid>https://arxiv.org/abs/2511.13612</guid>
<content:encoded><![CDATA[
arXiv:2511.13612v1 Announce Type: cross 
Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</title>
<link>https://arxiv.org/abs/2511.13646</link>
<guid>https://arxiv.org/abs/2511.13646</guid>
<content:encoded><![CDATA[
arXiv:2511.13646v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-G\"odel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Historical/temporal necessities/possibilities, and a logical theory of them in branching time</title>
<link>https://arxiv.org/abs/2208.11922</link>
<guid>https://arxiv.org/abs/2208.11922</guid>
<content:encoded><![CDATA[
arXiv:2208.11922v2 Announce Type: replace 
Abstract: In this paper, we do three kinds of work. First, we recognize four notions of necessity and two notions of possibility related to time flow, namely strong/weak historical/temporal necessities, as well as historical/temporal possibilities, which are motivated more from a linguistic perspective than from a philosophical one. Strong/weak historical necessities and historical possibility typically concern the possible futures of the present world, and strong/weak temporal necessities and temporal possibility concern possible timelines of alternatives of the present world. Second, we provide our approach to the six notions and present a logical theory of them in branching time. Our approach to the six notions is as follows. The agent has a system of ontic rules that determine expected timelines. She treats some ontic rules as undefeatable, determining accepted timelines. The domains of strong/weak historical necessities, respectively, consist of accepted and expected timelines passing through the present moment, and historical possibility is the dual of strong historical necessity. The domains of strong/weak temporal necessities, respectively, consist of accepted and expected timelines, and temporal possibility is the dual of strong temporal necessity. The logical theory has six operators: a last-moment operator, a next-moment operator, and four operators for the four notions of necessity. Formulas' evaluation contexts consist of a tree-like model representing a time flow, a context representing the agent's system of ontic rules, a timeline, and an instant. Third, we offer an axiomatic system for the logical theory and show its soundness and completeness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Machine Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2309.06706</link>
<guid>https://arxiv.org/abs/2309.06706</guid>
<content:encoded><![CDATA[
arXiv:2309.06706v3 Announce Type: replace 
Abstract: Real-world simultaneous machine translation (SimulMT) systems face more challenges than just the quality-latency trade-off. They also need to address issues related to robustness with noisy input, processing long contexts, and flexibility for knowledge injection. These challenges demand models with strong language understanding and generation capabilities which may not often equipped by dedicated MT models. In this paper, we investigate the possibility of applying Large Language Models (LLM) to SimulMT tasks by using existing incremental-decoding methods with a newly proposed RALCP algorithm for latency reduction. We conducted experiments using the \texttt{Llama2-7b-chat} model on nine different languages from the MUST-C dataset. The results show that LLM outperforms dedicated MT models in terms of BLEU and LAAL metrics. Further analysis indicates that LLM has advantages in terms of tuning efficiency and robustness. However, it is important to note that the computational cost of LLM remains a significant obstacle to its application in SimulMT.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language</title>
<link>https://arxiv.org/abs/2311.11142</link>
<guid>https://arxiv.org/abs/2311.11142</guid>
<content:encoded><![CDATA[
arXiv:2311.11142v2 Announce Type: replace 
Abstract: The Bangla linguistic variety is a fascinating mix of regional dialects that contributes to the cultural diversity of the Bangla-speaking community. Despite extensive study into translating Bangla to English, English to Bangla, and Banglish to Bangla in the past, there has been a noticeable gap in translating Bangla regional dialects into standard Bangla. In this study, we set out to fill this gap by creating a collection of 32,500 sentences, encompassing Bangla, Banglish, and English, representing five regional Bangla dialects. Our aim is to translate these regional dialects into standard Bangla and detect regions accurately. To tackle the translation and region detection tasks, we propose two novel models: DialectBanglaT5 for translating regional dialects into standard Bangla and DialectBanglaBERT for identifying the dialect's region of origin. DialectBanglaT5 demonstrates superior performance across all dialects, achieving the highest BLEU score of 71.93, METEOR of 0.8503, and the lowest WER of 0.1470 and CER of 0.0791 on the Mymensingh dialect. It also achieves strong ROUGE scores across all dialects, indicating both accuracy and fluency in capturing dialectal nuances. In parallel, DialectBanglaBERT achieves an overall region classification accuracy of 89.02%, with notable F1-scores of 0.9241 for Chittagong and 0.8736 for Mymensingh, confirming its effectiveness in handling regional linguistic variation. This is the first large-scale investigation focused on Bangla regional dialect translation and region detection. Our proposed models highlight the potential of dialect-specific modeling and set a new benchmark for future research in low-resource and dialect-rich language settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2402.10552</link>
<guid>https://arxiv.org/abs/2402.10552</guid>
<content:encoded><![CDATA[
arXiv:2402.10552v4 Announce Type: replace 
Abstract: Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DataGen: Unified Synthetic Dataset Generation via Large Language Models</title>
<link>https://arxiv.org/abs/2406.18966</link>
<guid>https://arxiv.org/abs/2406.18966</guid>
<content:encoded><![CDATA[
arXiv:2406.18966v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProFuser: Progressive Fusion of Large Language Models</title>
<link>https://arxiv.org/abs/2408.04998</link>
<guid>https://arxiv.org/abs/2408.04998</guid>
<content:encoded><![CDATA[
arXiv:2408.04998v2 Announce Type: replace 
Abstract: While fusing the capacities and advantages of various large language models offers a pathway to construct more powerful and versatile models, a fundamental challenge is to properly select advantageous model during training. Existing fusion methods primarily focus on the training mode that uses cross entropy on ground truth in a teacher-forcing setup to measure a model's advantage, which may provide limited insight towards model advantage. In this paper, we introduce a novel approach that enhances the fusion process by incorporating both the training and inference modes. Our method evaluates model advantage not only through cross entropy during training but also by considering inference outputs, providing a more comprehensive assessment. To combine the two modes effectively, we introduce ProFuser to progressively transition from inference mode to training mode. To validate ProFuser's effectiveness, we fused three models, including Vicuna-7B-v1.5, Llama-2-7B-Chat, and MPT-7B-8K-Chat, and demonstrated the improved performance in knowledge, reasoning, and safety compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Limitations of Language Targeted Pruning: Investigating the Calibration Language Impact in Multilingual LLM Pruning</title>
<link>https://arxiv.org/abs/2408.14398</link>
<guid>https://arxiv.org/abs/2408.14398</guid>
<content:encoded><![CDATA[
arXiv:2408.14398v4 Announce Type: replace 
Abstract: Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. This analysis paper conducts an in-depth investigation of the performance and internal representation changes associated with pruning multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. We further analyze the latent subspaces, pruning masks, and individual neurons within pruned models. Our results reveal that while calibration on the target language effectively retains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance. Further analysis of internal representations at three different levels highlights broader limitations of current pruning approaches: While they effectively preserve dominant information like language-specific features, this is insufficient to counteract the loss of nuanced, language-agnostic features that are crucial for knowledge retention and reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Breach: Assessing the Robustness of Transformer-based QA Models</title>
<link>https://arxiv.org/abs/2409.10997</link>
<guid>https://arxiv.org/abs/2409.10997</guid>
<content:encoded><![CDATA[
arXiv:2409.10997v4 Announce Type: replace 
Abstract: Contextual question-answering models are susceptible to adversarial perturbations to input context, commonly observed in real-world scenarios. These adversarial noises are designed to degrade the performance of the model by distorting the textual input. We introduce a unique dataset that incorporates seven distinct types of adversarial noise into the context, each applied at five different intensity levels on the SQuAD dataset. To quantify the robustness, we utilize robustness metrics providing a standardized measure for assessing model performance across varying noise types and levels. Experiments on transformer-based question-answering models reveal robustness vulnerabilities and important insights into the model's performance in realistic textual input.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is deeper always better? Replacing linear mappings with deep learning networks in the Discriminative Lexicon Model</title>
<link>https://arxiv.org/abs/2410.04259</link>
<guid>https://arxiv.org/abs/2410.04259</guid>
<content:encoded><![CDATA[
arXiv:2410.04259v2 Announce Type: replace 
Abstract: Recently, deep learning models have increasingly been used in cognitive modelling of language. This study asks whether deep learning can help us to better understand the learning problem that needs to be solved by speakers, above and beyond linear methods. We utilise the Discriminative Lexicon Model introduced by Baayen and colleagues, which models comprehension and production with mappings between numeric form and meaning vectors. While so far, these mappings have been linear (Linear Discriminative Learning, LDL), in the present study we replace them with deep dense neural networks (Deep Discriminative Learning, DDL). We find that DDL affords more accurate mappings for large and diverse datasets from English and Dutch, but not necessarily for Estonian and Taiwan Mandarin. DDL outperforms LDL in particular for words with pseudo-morphological structure such as chol+er. Applied to average reaction times, we find that DDL is outperformed by frequency-informed linear mappings (FIL). However, DDL trained in a frequency-informed way ('frequency-informed' deep learning, FIDDL) substantially outperforms FIL. Finally, while linear mappings can very effectively be updated from trial-to-trial to model incremental lexical learning, deep mappings cannot do so as effectively. At present, both linear and deep mappings are informative for understanding language.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Factor Level Preferences to Improve Human-Model Alignment</title>
<link>https://arxiv.org/abs/2410.06965</link>
<guid>https://arxiv.org/abs/2410.06965</guid>
<content:encoded><![CDATA[
arXiv:2410.06965v3 Announce Type: replace 
Abstract: Large language models (LLMs) often exhibit tendencies that diverge from human preferences, such as favoring certain writing styles or producing overly verbose outputs. While crucial for improvement, identifying the factors driving these misalignments remains challenging due to existing evaluation methods' reliance on coarse-grained comparisons and lack of explainability. To address this, we introduce PROFILE, an automated framework to uncover and measure factor-level preference alignment of humans and LLMs. Using PROFILE, we analyze preference alignment across three key tasks: summarization, instruction-following, and document-based QA. We find a significant discrepancy: while LLMs show poor factor-level alignment with human preferences when generating texts, they demonstrate strong alignment in discrimination tasks. We demonstrate how leveraging the identified generation-discrimination gap can be used to improve LLM alignment through multiple approaches, including fine-tuning with self-guidance. Our work highlights the value of factor-level analysis for identifying hidden misalignments and provides a practical framework for improving LLM-human preference alignment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Our Chatbot Telling Lies? Assessing Correctness of an LLM-based Dutch Support Chatbot</title>
<link>https://arxiv.org/abs/2411.00034</link>
<guid>https://arxiv.org/abs/2411.00034</guid>
<content:encoded><![CDATA[
arXiv:2411.00034v2 Announce Type: replace 
Abstract: Companies support their customers using live chats and chatbots to gain their loyalty. AFAS is a Dutch company aiming to leverage the opportunity large language models (LLMs) offer to answer customer queries with minimal to no input from its customer support team. Adding to its complexity, it is unclear what makes a response correct, and that too in Dutch. Further, with minimal data available for training, the challenge is to identify whether an answer generated by a large language model is correct and do it on the fly.
  This study is the first to define the correctness of a response based on how the support team at AFAS makes decisions. It leverages literature on natural language generation and automated answer grading systems to automate the decision-making of the customer support team. We investigated questions requiring a binary response (e.g., Would it be possible to adjust tax rates manually?) or instructions (e.g., How would I adjust tax rate manually?) to test how close our automated approach reaches support rating. Our approach can identify wrong messages in 55\% of the cases. This work demonstrates the potential for automatically assessing when our chatbot may provide incorrect or misleading answers. Specifically, we contribute (1) a definition and metrics for assessing correctness, and (2) suggestions to improve correctness with respect to regional language and question type.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Topological Structures from Language: A Survey of Topological Data Analysis Applications in NLP</title>
<link>https://arxiv.org/abs/2411.10298</link>
<guid>https://arxiv.org/abs/2411.10298</guid>
<content:encoded><![CDATA[
arXiv:2411.10298v4 Announce Type: replace 
Abstract: The surge of data available on the Internet has led to the adoption of various computational methods to analyze and extract valuable insights from this wealth of information. Among these, the field of Machine Learning (ML) has thrived by leveraging data to extract meaningful insights. However, ML techniques face notable challenges when dealing with real-world data, often due to issues of imbalance, noise, insufficient labeling, and high dimensionality. To address these limitations, some researchers advocate for the adoption of Topological Data Analysis (TDA), a statistical approach that discerningly captures the intrinsic shape of data despite noise. Despite its potential, TDA has not gained as much traction within the Natural Language Processing (NLP) domain compared to structurally distinct areas like computer vision. Nevertheless, a dedicated community of researchers has been exploring the application of TDA in NLP, yielding 100 papers we comprehensively survey in this paper. Our findings categorize these efforts into theoretical and non-theoretical approaches. Theoretical approaches aim to explain linguistic phenomena from a topological viewpoint, while non-theoretical approaches merge TDA with ML features, utilizing diverse numerical representation techniques. We conclude by exploring the challenges and unresolved questions that persist in this niche field. Resources and a list of papers on this topic can be found at: https://github.com/AdaUchendu/AwesomeTDA4NLP.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding World or Predicting Future? A Comprehensive Survey of World Models</title>
<link>https://arxiv.org/abs/2411.14499</link>
<guid>https://arxiv.org/abs/2411.14499</guid>
<content:encoded><![CDATA[
arXiv:2411.14499v3 Announce Type: replace 
Abstract: The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including generative games, autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script</title>
<link>https://arxiv.org/abs/2412.12478</link>
<guid>https://arxiv.org/abs/2412.12478</guid>
<content:encoded><![CDATA[
arXiv:2412.12478v5 Announce Type: replace 
Abstract: DNN-based language models excel across various NLP tasks but remain highly vulnerable to textual adversarial attacks. While adversarial text generation is crucial for NLP security, explainability, evaluation, and data augmentation, related work remains overwhelmingly English-centric, leaving the problem of constructing high-quality and sustainable adversarial robustness benchmarks for lower-resourced languages both difficult and understudied. First, method customization for lower-resourced languages is complicated due to linguistic differences and limited resources. Second, automated attacks are prone to generating invalid or ambiguous adversarial texts. Last but not least, language models continuously evolve and may be immune to parts of previously generated adversarial texts. To address these challenges, we introduce HITL-GAT, an interactive system based on a general approach to human-in-the-loop generation of adversarial texts. Additionally, we demonstrate the utility of HITL-GAT through a case study on Tibetan script, employing three customized adversarial text generation methods and establishing its first adversarial robustness benchmark, providing a valuable reference for other lower-resourced languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths</title>
<link>https://arxiv.org/abs/2502.14902</link>
<guid>https://arxiv.org/abs/2502.14902</guid>
<content:encoded><![CDATA[
arXiv:2502.14902v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known asgraph-based RAG. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. The code is available at the following link: https://github.com/BUPT-GAMMA/PathRAG
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Extraction and Generation for Robust Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2503.04789</link>
<guid>https://arxiv.org/abs/2503.04789</guid>
<content:encoded><![CDATA[
arXiv:2503.04789v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) enhances LLMs with external knowledge, yet generation remains vulnerable to retrieval-induced noise and uncertain placement of relevant chunks, often causing hallucinations. We present Ext2Gen, an extract-then-generate framework that strengthens LLMs via joint evidence selection and answer generation, dynamically identifying query-relevant content while suppressing noise, thereby removing the need for any independent pre-generation compression module. Optimized through preference alignment with well-curated pairwise feedback, Ext2Gen produces accurate and faithful answers even under noisy or imprecise retrieval. Experiments demonstrate that it substantially enhances the robustness of the generation backbone and yields greater performance gains than methods relying on independent compression models, e.g., Recomp, CompAct, EXIT). It further benefits from improved retrieval techniques such as query rewriting, underscoring that generation-side enhancements address limitations that retrieval alone cannot overcome.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Conditional Emergence of Multilingual Image Captioning via Generalization from Translation</title>
<link>https://arxiv.org/abs/2503.09443</link>
<guid>https://arxiv.org/abs/2503.09443</guid>
<content:encoded><![CDATA[
arXiv:2503.09443v2 Announce Type: replace 
Abstract: Cross-lingual, cross-task transfer is challenged by task-specific data scarcity, which becomes more severe as language support grows and is further amplified in vision-language models (VLMs). We investigate multilingual generalization in encoder-decoder transformer VLMs to enable zero-shot image captioning in languages encountered only in the translation task. In this setting, the encoder must learn to generate generalizable, task-aware latent vision representations to instruct the decoder via inserted cross-attention layers. To analyze scaling behavior, we train Florence-2 based and Gemma-2 based models (0.4B to 11.2B parameters) on a synthetic dataset using varying compute budgets. While all languages in the dataset have image-aligned translations, only a subset of them include image captions. Notably, we show that captioning can emerge using a language prefix, even when this language only appears in the translation task. We find that indirect learning of unseen task-language pairs adheres to scaling laws that are governed by the multilinguality of the model, model size, and seen training samples. Finally, we demonstrate that the scaling laws extend to downstream tasks, achieving competitive performance through fine-tuning in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation</title>
<link>https://arxiv.org/abs/2504.03197</link>
<guid>https://arxiv.org/abs/2504.03197</guid>
<content:encoded><![CDATA[
arXiv:2504.03197v4 Announce Type: replace 
Abstract: With the rapid advancement of mathematical reasoning capabilities in Large Language Models (LLMs), AI systems are increasingly being adopted in educational settings to support students' comprehension of problem-solving processes. However, a critical component remains underexplored in current LLM-generated explanations: multimodal explanation. In real-world instructional contexts, human tutors routinely employ visual aids, such as diagrams, markings, and highlights, to enhance conceptual clarity. To bridge this gap, we introduce the multimodal solution explanation task, designed to evaluate whether models can identify visual keypoints, such as auxiliary lines, points, angles, and generate explanations that incorporate these key elements essential for understanding. To evaluate model performance on this task, we propose ME2, a multimodal benchmark consisting of 1,000 math problems annotated with visual keypoints and corresponding explanatory text that references those elements. Our empirical results show that current models struggle to identify visual keypoints. In the task of generating keypoint-based explanations, open-source models also face notable difficulties. This highlights a significant gap in current LLMs' ability to perform mathematical visual grounding, engage in visually grounded reasoning, and provide explanations in educational contexts. We expect that the multimodal solution explanation task and the ME2 dataset will catalyze further research on LLMs in education and promote their use as effective, explanation-oriented AI tutors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Robust Response Generation in the Wild</title>
<link>https://arxiv.org/abs/2504.12982</link>
<guid>https://arxiv.org/abs/2504.12982</guid>
<content:encoded><![CDATA[
arXiv:2504.12982v2 Announce Type: replace 
Abstract: The proliferation of large language models (LLMs) has significantly advanced intelligent systems. Unfortunately, LLMs often face knowledge conflicts between internal memory and retrieved external information, arising from misinformation, biases, or outdated knowledge. These conflicts undermine response reliability and introduce uncertainty in decision-making. In this work, we analyze how LLMs navigate knowledge conflicts from an information-theoretic perspective and reveal that when conflicting and supplementary information exhibit significant differences, LLMs confidently resolve their preferences and alleviate the uncertainty during their response generation. When this difference is ambiguous, LLMs experience considerable uncertainty about their generation. Based on this insight, we propose Swin-VIB, a novel framework that integrates a pipeline of variational information bottleneck models to adapt the retrieved information difference, facilitating robust response generation of LLMs even in conflicting contexts. Extensive experiments confirm our theoretical analysis and demonstrate the performance of Swin-VIB. Notably, Swin-VIB outperforms all competitive baselines in terms of the accuracy of the multiple-choice task, while improving the EM values in the open-ended QA task by at least 11.14%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights</title>
<link>https://arxiv.org/abs/2505.04649</link>
<guid>https://arxiv.org/abs/2505.04649</guid>
<content:encoded><![CDATA[
arXiv:2505.04649v3 Announce Type: replace 
Abstract: The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations</title>
<link>https://arxiv.org/abs/2505.12560</link>
<guid>https://arxiv.org/abs/2505.12560</guid>
<content:encoded><![CDATA[
arXiv:2505.12560v3 Announce Type: replace 
Abstract: Existing datasets available for crosslinguistic investigations have tended to focus on large amounts of data for a small group of languages or a small amount of data for a large number of languages. This means that claims based on these datasets are limited in what they reveal about universal properties of the human language faculty. While this has begun to change through the efforts of projects seeking to develop tagged corpora for a large number of languages, such efforts are still constrained by limits on resources. The current paper reports on a large tagged parallel dataset which has been developed to partially address this issue. The taggedPBC contains POS-tagged parallel text data from more than 1,940 languages, representing 155 language families and 78 isolates, dwarfing previously available resources. The accuracy of particular tags in this dataset is shown to correlate well with both existing SOTA taggers for high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks). Additionally, a novel measure derived from this dataset, the N1 ratio, correlates with expert determinations of intransitive word order in three typological databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier trained on this feature can accurately identify basic intransitive word order for languages not in those databases. While much work is still needed to expand and develop this dataset, the taggedPBC is an important step to enable corpus-based crosslinguistic investigations, and is made available for research and collaboration via GitHub.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering</title>
<link>https://arxiv.org/abs/2505.14099</link>
<guid>https://arxiv.org/abs/2505.14099</guid>
<content:encoded><![CDATA[
arXiv:2505.14099v2 Announce Type: replace 
Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language questions using structured knowledge from KBs. While LLM-only approaches offer generalization, they suffer from outdated knowledge, hallucinations, and lack of transparency. Chain-based KG-RAG methods address these issues by incorporating external KBs, but are limited to simple chain-structured questions due to the absence of planning and logical structuring. Inspired by semantic parsing methods, we propose PDRR: a four-stage framework consisting of Predict, Decompose, Retrieve, and Reason. Our method first predicts the question type and decomposes the question into structured triples. Then retrieves relevant information from KBs and guides the LLM as an agent to reason over and complete the decomposed triples. Experimental results demonstrate that PDRR consistently outperforms existing methods across various LLM backbones and achieves superior performance on both chain-structured and non-chain complex questions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation</title>
<link>https://arxiv.org/abs/2505.15249</link>
<guid>https://arxiv.org/abs/2505.15249</guid>
<content:encoded><![CDATA[
arXiv:2505.15249v2 Announce Type: replace 
Abstract: Recently, large vision-language models (LVLMs) have emerged as the preferred tools for judging text-image alignment, yet their robustness along the visual modality remains underexplored. This work is the first study to address a key research question: Can adversarial visual manipulations systematically fool LVLM judges into assigning unfairly inflated scores? We define potential image induced biases within the context of T2I evaluation and examine how these biases affect the evaluations of LVLM judges. Moreover, we introduce a novel, fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is deliberately constructed to exhibit diverse score distributions. By introducing the defined biases into the benchmark, we reveal that all tested LVLM judges exhibit vulnerability across all domains, consistently inflating scores for manipulated images. Further analysis reveals that combining multiple biases amplifies their effects, and pairwise evaluations are similarly susceptible. Moreover, we observe that visual biases persist under prompt-based mitigation strategies, highlighting the vulnerability of current LVLM evaluation systems and underscoring the urgent need for more robust LVLM judges.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model</title>
<link>https://arxiv.org/abs/2505.16000</link>
<guid>https://arxiv.org/abs/2505.16000</guid>
<content:encoded><![CDATA[
arXiv:2505.16000v5 Announce Type: replace 
Abstract: The rapid advancement of language models has demonstrated the potential of artificial intelligence in the healthcare industry. However, small language models struggle with specialized domains in low-resource languages like Persian. While numerous medical-domain websites exist in Persian, no curated dataset or corpus has been available making ours the first of its kind. This study introduces a newly curated dataset comprising 20k doctor-patient Q\&amp;A pairs and 60\% of a 90-million-token crawled corpus from medical magazines. Using a parameter-efficient fine-tuning approach, we enhanced the medical knowledge of the baseline model, aya-expanse-8b. Benchmark evaluations demonstrate that the fine-tuned model achieves improved accuracy in medical question answering and successfully passed the Iranian Basic Medical Science Entrance Exam (IBSEE) in September 2023, which the baseline model did not. Additionally, the fine-tuned model improved Persian-translated MMLU accuracy by an average of 2.67\%. This work highlights the potential of leveraging open-access online data to enrich small language models in medical fields, providing a novel solution for Persian medical AI applications suitable for resource-constrained environments. Future research could explore multimodal input to further enhance performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Thought Driven Adversarial Scenario Extrapolation for Robust Language Models</title>
<link>https://arxiv.org/abs/2505.17089</link>
<guid>https://arxiv.org/abs/2505.17089</guid>
<content:encoded><![CDATA[
arXiv:2505.17089v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit impressive capabilities, but remain susceptible to a growing spectrum of safety risks, including jailbreaks, toxic content, hallucinations, and bias. Existing defenses often address only a single threat type or resort to rigid outright rejection, sacrificing user experience and failing to generalize across diverse and novel attacks. This paper introduces Adversarial Scenario Extrapolation (ASE), a novel inference-time computation framework that leverages Chain-of-Thought (CoT) reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides the LLM through a self-generative process of contemplating potential adversarial scenarios and formulating defensive strategies before generating a response to the user query. Comprehensive evaluation on four adversarial benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak attack success rates and minimal toxicity, while slashing outright rejections to <4%. ASE outperforms six state-of-the-art defenses in robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&amp;A and 4-10x lower bias scores. By transforming adversarial perception into an intrinsic cognitive process, ASE sets a new paradigm for secure and natural human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCRum-9: Multilingual Stance Classification over Rumours on Social Media</title>
<link>https://arxiv.org/abs/2505.18916</link>
<guid>https://arxiv.org/abs/2505.18916</guid>
<content:encoded><![CDATA[
arXiv:2505.18916v3 Announce Type: replace 
Abstract: We introduce SCRum-9, the largest multilingual Stance Classification dataset for Rumour analysis in 9 languages, containing 7,516 tweets from X. SCRum-9 goes beyond existing stance classification datasets by covering more languages, linking examples to more fact-checked claims (2.1k), and including confidence-related annotations from multiple annotators to account for intra- and inter-annotator variability. Annotations were made by at least two native speakers per language, totalling more than 405 hours of annotation and 8,150 dollars in compensation. Further, SCRum-9 is used to benchmark five large language models (LLMs) and two multilingual masked language models (MLMs) in In-Context Learning (ICL) and fine-tuning setups. This paper also innovates by exploring the use of multilingual synthetic data for rumour stance classification, showing that even LLMs with weak ICL performance can produce valuable synthetic data for fine-tuning small MLMs, enabling them to achieve higher performance than zero-shot ICL in LLMs. Finally, we examine the relationship between model predictions and human uncertainty on ambiguous cases finding that model predictions often match the second-choice labels assigned by annotators, rather than diverging entirely from human judgments. SCRum-9 is publicly released to the research community with potential to foster further research on multilingual analysis of misleading narratives on social media.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2505.19768</link>
<guid>https://arxiv.org/abs/2505.19768</guid>
<content:encoded><![CDATA[
arXiv:2505.19768v2 Announce Type: replace 
Abstract: Real-world multimodal misinformation often arises from mixed forgery sources, requiring dynamic reasoning and adaptive verification. However, existing methods mainly rely on static pipelines and limited tool usage, limiting their ability to handle such complexity and diversity. To address this challenge, we propose \method, a novel misinformation detection agent that incorporates an extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of modular tools such as web search, forgery detection, and consistency analysis. Each tool is described using standardized templates, enabling seamless integration and future expansion. To avoid inefficiency from using all tools simultaneously, a greedy search-based selector is proposed to identify a task-relevant subset. This subset then serves as the action space for MCTS to dynamically collect evidence and perform multi-source verification. To better align MCTS with the multi-source nature of misinformation detection, \method~ extends traditional MCTS with multi-source verification, which decomposes the task into coordinated subtasks targeting different forgery sources. A dual reward mechanism containing a reasoning trajectory score and a confidence score is further proposed to encourage a balance between exploration across mixed forgery sources and exploitation for more reliable evidence. We conduct ablation studies to confirm the effectiveness of the tree search mechanism and tool usage. Extensive experiments further show that \method~ consistently outperforms existing baselines on challenging mixed-source multimodal misinformation benchmarks, demonstrating its strong potential as a training-free detector.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query</title>
<link>https://arxiv.org/abs/2505.20334</link>
<guid>https://arxiv.org/abs/2505.20334</guid>
<content:encoded><![CDATA[
arXiv:2505.20334v2 Announce Type: replace 
Abstract: Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REIC: RAG-Enhanced Intent Classification at Scale</title>
<link>https://arxiv.org/abs/2506.00210</link>
<guid>https://arxiv.org/abs/2506.00210</guid>
<content:encoded><![CDATA[
arXiv:2506.00210v2 Announce Type: replace 
Abstract: Accurate intent classification is critical for efficient routing in customer service, ensuring customers are connected with the most suitable agents while reducing handling times and operational costs. However, as companies expand their product lines, intent classification faces scalability challenges due to the increasing number of intents and variations in taxonomy across different verticals. In this paper, we introduce REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, which addresses these challenges effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge, enabling precise classification without the need for frequent retraining. Through extensive experiments on real-world datasets, we demonstrate that REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings. Our results highlight its effectiveness in both in-domain and out-of-domain scenarios, demonstrating its potential for real-world deployment in adaptive and large-scale intent classification systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers</title>
<link>https://arxiv.org/abs/2506.01215</link>
<guid>https://arxiv.org/abs/2506.01215</guid>
<content:encoded><![CDATA[
arXiv:2506.01215v2 Announce Type: replace 
Abstract: As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 52% and 34% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework</title>
<link>https://arxiv.org/abs/2506.02454</link>
<guid>https://arxiv.org/abs/2506.02454</guid>
<content:encoded><![CDATA[
arXiv:2506.02454v2 Announce Type: replace 
Abstract: Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\% overall win rate over the baseline method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Test-Time Scaling with Model-Free Speculative Sampling</title>
<link>https://arxiv.org/abs/2506.04708</link>
<guid>https://arxiv.org/abs/2506.04708</guid>
<content:encoded><![CDATA[
arXiv:2506.04708v2 Announce Type: replace 
Abstract: Language models have demonstrated remarkable capabilities in reasoning tasks through test-time scaling techniques like best-of-N sampling and tree search. However, these approaches often demand substantial computational resources, creating a critical trade-off between performance and efficiency. We introduce STAND (STochastic Adaptive N-gram Drafting), a novel model-free speculative decoding approach that exploits the inherent redundancy in reasoning trajectories to achieve significant acceleration without compromising accuracy. Our analysis shows that reasoning paths frequently reuse similar reasoning patterns, enabling efficient model-free token prediction without requiring separate draft models. By introducing stochastic drafting and preserving probabilistic information through a memory-efficient logit-based N-gram module, combined with optimized Gumbel-Top-K sampling and data-driven tree construction, STAND significantly improves token acceptance rates. Extensive evaluations across multiple models and reasoning tasks (AIME-2024, GPQA-Diamond, and LiveCodeBench) demonstrate that STAND reduces inference latency by 60-65% compared to standard autoregressive decoding while maintaining accuracy. Furthermore, STAND consistently outperforms state-of-the-art speculative decoding methods across diverse inference patterns, including single-trajectory decoding, batch decoding, and test-time tree search. As a model-free approach, STAND can be applied to any existing language model without additional training, making it a powerful plug-and-play solution for accelerating language model reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.04832</link>
<guid>https://arxiv.org/abs/2506.04832</guid>
<content:encoded><![CDATA[
arXiv:2506.04832v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) extend large language models with explicit, multi-step reasoning traces to enhance transparency and performance on complex tasks. However, these reasoning traces can be redundant or logically inconsistent, becoming a new and hard-to-detect source of hallucination. Existing hallucination detection methods focus primarily on answer-level uncertainty and often fail to detect hallucinations or logical inconsistencies arising from the model's reasoning trace. This oversight is particularly problematic for LRMs, where the explicit thinking trace is not only an important support to the model's decision-making process but also a key source of potential hallucination. To this end, we propose RACE (Reasoning and Answer Consistency Evaluation), a novel framework specifically tailored for hallucination detection in LRMs. RACE operates by extracting essential reasoning steps and computing four diagnostic signals: inter-sample consistency of reasoning traces, entropy-based answer uncertainty, semantic alignment between reasoning and answers, and internal coherence of reasoning. The joint utilization of these signals makes RACE a more robust detector of hallucinations in LRMs. Experiments across datasets and different LLMs demonstrate that RACE outperforms existing hallucination detection baselines, offering a robust and generalizable solution for evaluating LRMs. The source code is available at https://github.com/bebr2/RACE
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning</title>
<link>https://arxiv.org/abs/2506.05813</link>
<guid>https://arxiv.org/abs/2506.05813</guid>
<content:encoded><![CDATA[
arXiv:2506.05813v2 Announce Type: replace 
Abstract: Table-based question answering requires complex reasoning capabilities that current LLMs struggle to achieve with single-pass inference. Existing approaches, such as Chain-of-Thought reasoning and question decomposition, lack error detection mechanisms and discard problem-solving experiences, contrasting sharply with how humans tackle such problems. In this paper, we propose MAPLE (Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution. Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Post-Training Refinement of Latent Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.08552</link>
<guid>https://arxiv.org/abs/2506.08552</guid>
<content:encoded><![CDATA[
arXiv:2506.08552v2 Announce Type: replace 
Abstract: Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA without additional training.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToxSyn: Reducing Bias in Hate Speech Detection via Synthetic Minority Data in Brazilian Portuguese</title>
<link>https://arxiv.org/abs/2506.10245</link>
<guid>https://arxiv.org/abs/2506.10245</guid>
<content:encoded><![CDATA[
arXiv:2506.10245v2 Announce Type: replace 
Abstract: The development of robust hate speech detection systems remains limited by the lack of large-scale, fine-grained training data, especially for languages beyond English. Existing corpora typically rely on coarse toxic/non-toxic labels, and the few that capture hate directed at specific minority groups critically lack the non-toxic counterexamples (i.e., benign text about minorities) required to distinguish genuine hate from mere discussion. We introduce ToxSyn, the first Portuguese large-scale corpus explicitly designed for multi-label hate speech detection across nine protected minority groups. Generated via a controllable four-stage pipeline, ToxSyn includes discourse-type annotations to capture rhetorical strategies of toxic language, such as sarcasm or dehumanization. Crucially, it systematically includes the non-toxic counterexamples absent in all other public datasets. Our experiments reveal a catastrophic, mutual generalization failure between social-media domains and ToxSyn: models trained on social media struggle to generalize to minority-specific contexts, and vice-versa. This finding indicates they are distinct tasks and exposes summary metrics like Macro F1 can be unreliable indicators of true model behavior, as they completely mask model failure. We publicly release ToxSyn at HuggingFace to foster reproducible research on synthetic data generation and benchmark progress in hate-speech detection for low- and mid-resource languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Pay Attention</title>
<link>https://arxiv.org/abs/2506.11305</link>
<guid>https://arxiv.org/abs/2506.11305</guid>
<content:encoded><![CDATA[
arXiv:2506.11305v2 Announce Type: replace 
Abstract: The Transformer has become the de facto standard for modern language models owing to its parallelizable training and effective autoregressive decoding. However, its fixed context window and the quadratic time and memory costs of its self-attention mechanism remain central bottlenecks. These constraints have revived interest in recurrent architectures that scale linearly with sequence length, but at the cost of reduced parallelism. In this paper, we introduce Avey, a new foundational architecture that breaks away from both attention and recurrence. Avey pairs a ranker with an autoregressive neural processor to select and contextualize only the most relevant tokens for any given token. Specifically, it decouples sequence length from context width, thus enabling effective and efficient processing of arbitrarily long sequences. Results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while significantly outperforming it on tasks requiring long-range dependency modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models</title>
<link>https://arxiv.org/abs/2506.15545</link>
<guid>https://arxiv.org/abs/2506.15545</guid>
<content:encoded><![CDATA[
arXiv:2506.15545v2 Announce Type: replace 
Abstract: Local-global attention models have recently emerged as compelling alternatives to standard Transformers, promising improvements in both training and inference efficiency. However, the crucial choice of window size presents a Pareto tradeoff: larger windows maintain performance akin to full attention but offer minimal efficiency gains in short-context scenarios, while smaller windows can lead to performance degradation. Current models, such as Gemma2 and Mistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining length) to preserve performance. This work investigates strategies to shift this Pareto frontier, enabling local-global models to achieve efficiency gains even in short-context regimes. Our core motivation is to address the intrinsic limitation of local attention -- its complete disregard for tokens outside the defined window. We explore RATTENTION, a variant of local attention integrated with a specialized linear attention mechanism designed to capture information from these out-of-window tokens. Pretraining experiments at the 3B and 12B scales demonstrate that RATTENTION achieves a superior Pareto tradeoff between performance and efficiency. As a sweet spot, RATTENTION with a window size of just 512 consistently matches the performance of full-attention models across diverse settings. Furthermore, the recurrent nature inherent in the linear attention component of RATTENTION contributes to enhanced long-context performance, as validated on the RULER benchmark. Crucially, these improvements do not compromise training efficiency; thanks to a specialized kernel implementation and the reduced window size, RATTENTION maintains training speeds comparable to existing state-of-the-art approaches. We open-sourced our Pallas kernels along with model codes to facilitate further research effort.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Organizing Language</title>
<link>https://arxiv.org/abs/2506.23293</link>
<guid>https://arxiv.org/abs/2506.23293</guid>
<content:encoded><![CDATA[
arXiv:2506.23293v2 Announce Type: replace 
Abstract: We introduce a novel paradigm of emergent local memory. It is a continuous-learning completely-parallel content-addressable memory encoding global order. It demonstrates how local constraints on uncoordinated learning can produce topologically protected memories realizing emergent symbolic order. It is therefore a neuro-symbolic bridge.
  It further has the ability to produce human language without data, by exploiting its own self-organizing dynamics. It teaches us that words arise as a side-effect of emergent symbolic order, and that human language patterns at all structural levels reflect a universal mechanism of word formation (which is subregular). This work answers essential questions about the existence \& origin of all the human language data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-R1: Incentivizing the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism</title>
<link>https://arxiv.org/abs/2507.02962</link>
<guid>https://arxiv.org/abs/2507.02962</guid>
<content:encoded><![CDATA[
arXiv:2507.02962v5 Announce Type: replace 
Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are prone to generating hallucinated or outdated content due to their static internal knowledge. While Retrieval-Augmented Generation (RAG) integrated with Reinforcement Learning (RL) offers a solution, these methods are fundamentally constrained by a single-query mode, leading to prohibitive latency and inherent brittleness. To overcome these limitations, we introduce RAG-R1, a novel two-stage training framework centered around multi-query parallelism. Our framework enables LLMs to adaptively leverage internal and external knowledge during the reasoning process while transitioning from the single-query mode to multi-query parallelism. This architectural shift bolsters reasoning robustness while significantly reducing inference latency. Extensive experiments on seven question-answering benchmarks confirm the superiority of our method, which outperforms the strongest baseline by up to 13.7% and decreases inference time by 11.1%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</title>
<link>https://arxiv.org/abs/2507.22564</link>
<guid>https://arxiv.org/abs/2507.22564</guid>
<content:encoded><![CDATA[
arXiv:2507.22564v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling the Influence of Amplifying Language-Specific Neurons</title>
<link>https://arxiv.org/abs/2507.22581</link>
<guid>https://arxiv.org/abs/2507.22581</guid>
<content:encoded><![CDATA[
arXiv:2507.22581v3 Announce Type: replace 
Abstract: Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02360</link>
<guid>https://arxiv.org/abs/2508.02360</guid>
<content:encoded><![CDATA[
arXiv:2508.02360v2 Announce Type: replace 
Abstract: Fine-tuning Large Language Models on a political topic will significantly manipulate their political stance on various issues and unintentionally affect their stance on unrelated topics. While previous studies have proposed this issue, there is still a lack of understanding regarding the internal representations of these stances and the mechanisms that lead to unintended cross-topic generalization. In this paper, we systematically explore the internal mechanisms underlying this phenomenon from a neuron-level perspective and how to mitigate the cross-topic generalization of political fine-tuning. Firstly, we propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify two distinct types of political neurons: general political neurons, which govern stance across multiple political topics, and topic-specific neurons} that affect the model's political stance on individual topics. We find the existence of these political neuron types across four models and datasets through activation patching experiments. Leveraging these insights, we introduce InhibitFT, an inhibition-based fine-tuning method, effectively mitigating the cross-topic stance generalization. Experimental results demonstrate the robustness of identified neuron types across various models and datasets, and show that InhibitFT significantly reduces the cross-topic stance generalization by 20% on average, while preserving topic-specific performance. Moreover, we demonstrate that selectively inhibiting only 5% of neurons is sufficient to effectively mitigate the cross-topic stance generalization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty</title>
<link>https://arxiv.org/abs/2508.03294</link>
<guid>https://arxiv.org/abs/2508.03294</guid>
<content:encoded><![CDATA[
arXiv:2508.03294v2 Announce Type: replace 
Abstract: Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression</title>
<link>https://arxiv.org/abs/2508.05337</link>
<guid>https://arxiv.org/abs/2508.05337</guid>
<content:encoded><![CDATA[
arXiv:2508.05337v2 Announce Type: replace 
Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought reasoning with complex reflection behaviors, typically signaled by specific trigger words (e.g., "Wait" and "Alternatively") to enhance performance. However, these reflection behaviors can lead to the overthinking problem where the generation of redundant reasoning steps that unnecessarily increase token usage, raise inference costs, and reduce practical utility. In this paper, we propose Certainty-Guided Reflection Suppression (CGRS), a novel method that mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS operates by dynamically suppressing the model's generation of reflection triggers when it exhibits high confidence in its current response, thereby preventing redundant reflection cycles without compromising output quality. Our approach is model-agnostic, requires no retraining or architectural modifications, and can be integrated seamlessly with existing autoregressive generation pipelines. Extensive experiments across four reasoning benchmarks (i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines. These results hold consistently across model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3 family) and scales (4B to 32B parameters), highlighting CGRS's practical value for efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures</title>
<link>https://arxiv.org/abs/2508.06105</link>
<guid>https://arxiv.org/abs/2508.06105</guid>
<content:encoded><![CDATA[
arXiv:2508.06105v2 Announce Type: replace 
Abstract: Large language models (LLMs) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a $\textbf{Logic}$-aware $\textbf{R}etrieval$-$\textbf{A}$ugmented $\textbf{G}$eneration framework ($\textbf{LogicRAG}$) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneJailEval: A Scenario-Adaptive Multi-Dimensional Framework for Jailbreak Evaluation</title>
<link>https://arxiv.org/abs/2508.06194</link>
<guid>https://arxiv.org/abs/2508.06194</guid>
<content:encoded><![CDATA[
arXiv:2508.06194v2 Announce Type: replace 
Abstract: Accurate jailbreak evaluation is critical for LLM red team testing and jailbreak research. Mainstream methods rely on binary classification (string matching, toxic text classifiers, and LLM-based methods), outputting only "yes/no" labels without quantifying harm severity. Emerged multi-dimensional frameworks (e.g., Security Violation, Relative Truthfulness and Informativeness) use unified evaluation standards across scenarios, leading to scenario-specific mismatches (e.g., "Relative Truthfulness" is irrelevant to "hate speech"), undermining evaluation accuracy. To address these, we propose SceneJailEval, with key contributions: (1) A pioneering scenario-adaptive multi-dimensional framework for jailbreak evaluation, overcoming the critical "one-size-fits-all" limitation of existing multi-dimensional methods, and boasting robust extensibility to seamlessly adapt to customized or emerging scenarios. (2) A novel 14-scenario dataset featuring rich jailbreak variants and regional cases, addressing the long-standing gap in high-quality, comprehensive benchmarks for scenario-adaptive evaluation. (3) SceneJailEval delivers state-of-the-art performance with an F1 score of 0.917 on our full-scenario dataset (+6% over SOTA) and 0.995 on JBB (+3% over SOTA), breaking through the accuracy bottleneck of existing evaluation methods in heterogeneous scenarios and solidifying its superiority.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</title>
<link>https://arxiv.org/abs/2508.13953</link>
<guid>https://arxiv.org/abs/2508.13953</guid>
<content:encoded><![CDATA[
arXiv:2508.13953v2 Announce Type: replace 
Abstract: In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen's Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page https://github.com/aaronlifenghan/ReviewGraph
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</title>
<link>https://arxiv.org/abs/2508.14031</link>
<guid>https://arxiv.org/abs/2508.14031</guid>
<content:encoded><![CDATA[
arXiv:2508.14031v2 Announce Type: replace 
Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries</title>
<link>https://arxiv.org/abs/2508.18212</link>
<guid>https://arxiv.org/abs/2508.18212</guid>
<content:encoded><![CDATA[
arXiv:2508.18212v2 Announce Type: replace 
Abstract: The emergence of LM-based judging reward modeling, represented by generative reward models, has successfully made reinforcement learning from AI feedback (RLAIF) efficient and scalable. To further advance this paradigm, we propose a core insight: this form of reward modeling shares fundamental formal consistency with natural language inference (NLI), a core task in natural language understanding. This reframed perspective points to a key path for building superior reward models: scaling the model's comprehension boundaries. Pursuing this path, exploratory experiments on NLI tasks demonstrate that the slot prediction masked language models (MLMs) incorporating contextual explanations achieve significantly better performance compared to mainstream autoregressive models. Based on this key finding, we propose ESFP-RM, a two-stage LM-based judging reward model that utilizes an explanation based slot framework for prediction to fully leverage the advantages of MLMs. Extensive experiments demonstrate that in both reinforcement learning from human feedback (RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers more stable and generalizable reward signals compared to generative reward models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning</title>
<link>https://arxiv.org/abs/2509.02492</link>
<guid>https://arxiv.org/abs/2509.02492</guid>
<content:encoded><![CDATA[
arXiv:2509.02492v3 Announce Type: replace 
Abstract: Significant progress in reward modeling over recent years has been driven by a paradigm shift from task-specific designs towards generalist reward models. Despite this trend, developing effective reward models remains a fundamental challenge: the heavy reliance on large-scale labeled preference data. Pre-training on abundant unlabeled data offers a promising direction, but existing approaches fall short of instilling explicit reasoning into reward models. To bridge this gap, we propose a self-training approach that leverages unlabeled data to elicit reward reasoning in reward models. Based on this approach, we develop GRAM-R$^2$, a generative reward model trained to produce not only preference labels but also accompanying reward rationales. GRAM-R$^2$ can serve as a foundation model for reward reasoning and can be applied to a wide range of tasks with minimal or no additional fine-tuning. It can support downstream applications such as response ranking and task-specific reward tuning. Experiments on response ranking, task adaptation, and reinforcement learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers strong performance, outperforming several strong discriminative and generative baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts</title>
<link>https://arxiv.org/abs/2509.12440</link>
<guid>https://arxiv.org/abs/2509.12440</guid>
<content:encoded><![CDATA[
arXiv:2509.12440v2 Announce Type: replace 
Abstract: Deploying Large Language Models (LLMs) in medical applications requires fact-checking capabilities to ensure patient safety and regulatory compliance. We introduce MedFact, a challenging Chinese medical fact-checking benchmark with 2,116 expert-annotated instances from diverse real-world texts, spanning 13 specialties, 8 error types, 4 writing styles, and 5 difficulty levels. Construction uses a hybrid AI-human framework where iterative expert feedback refines AI-driven, multi-criteria filtering to ensure high quality and difficulty. We evaluate 20 leading LLMs on veracity classification and error localization, and results show models often determine if text contains errors but struggle to localize them precisely, with top performers falling short of human performance. Our analysis reveals the "over-criticism" phenomenon, a tendency for models to misidentify correct information as erroneous, which can be exacerbated by advanced reasoning techniques such as multi-agent collaboration and inference-time scaling. MedFact highlights the challenges of deploying medical LLMs and provides resources to develop factually reliable medical AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts</title>
<link>https://arxiv.org/abs/2509.23188</link>
<guid>https://arxiv.org/abs/2509.23188</guid>
<content:encoded><![CDATA[
arXiv:2509.23188v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly advanced collaborative reasoning, tool use, and role-specialized coordination in complex tasks. However, reliability-critical deployment remains hindered by a systemic failure mode: hierarchical compliance under instruction conflicts (system-user, peer-peer), where agents misprioritize system-level rules in the presence of competing demands. Moreover, widely used macro-level metrics (e.g., pass@k) obscure these micro-level violations and offer little actionable guidance for remedy. In this work, we present a full-stack, three-stage framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a query-wise, context-aware scoring metric that decomposes role adherence into four measurable dimensions; (2) Localize - attention drift analysis revealing that instruction conflicts are resolved by attention heads that are largely concentrated in middle layers; (3) Align - Surgical Alignment of Instruction Layers (SAIL), which installs LoRA only on the localized focal layers and optimizes a token-weighted DPO-style preference objective that credits tokens by their focal attentional contribution. Across standard benchmarks and MAS frameworks, our surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE</title>
<link>https://arxiv.org/abs/2509.24130</link>
<guid>https://arxiv.org/abs/2509.24130</guid>
<content:encoded><![CDATA[
arXiv:2509.24130v2 Announce Type: replace 
Abstract: The performance of Large Language Models (LLMs) hinges on carefully engineered prompts. However, prevailing prompt optimization methods, ranging from heuristic edits and reinforcement learning to evolutionary search, primarily target point-wise accuracy. They seldom enforce paraphrase invariance or searching stability, and therefore cannot remedy this brittleness in practice. Automated prompt search remains brittle: small, semantically preserving paraphrases often cause large performance swings. We identify this brittleness as the textual sharpness of the prompt landscape. In this work, we provide the first formal treatment of textual sharpness in the discrete, semantic space of prompts, together with an operational robustness criterion over a semantic neighborhood; the design is black-box or API-only, requiring no gradients to update the model's parameters. Then we introduce TARE (Textual Sharpness-Aware Evolving), a derivative-free framework that alternates between an inner, sampling-based adversarial search that stresses a prompt with hard paraphrases and an outer, robust selection that prefers candidates whose neighborhoods remain strong. We further propose ATARE, which learns anisotropic weights to shape the semantic neighborhood and adapts its radius over time to balance exploration and fidelity. Diverse tasks evaluate our methods, whose design for minimizing textual sharpness gap leads to prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search while remaining computationally practical.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution</title>
<link>https://arxiv.org/abs/2509.24189</link>
<guid>https://arxiv.org/abs/2509.24189</guid>
<content:encoded><![CDATA[
arXiv:2509.24189v2 Announce Type: replace 
Abstract: Understanding how user preference evolves over time is a fundamental challenge central to modern digital ecosystems, for which Large Language Models (LLMs) are an increasingly prominent and popular approach due to their ability to comprehend the rich semantic context within behavioral data. A common practice is to use LLMs to predict a user's next action by directly generating a ranked list of preferred items. Although effective for short-term prediction, the end-to-end generation paradigm inherently limits personalization. Its opaque decision-making process obscures holistic user profiling and exacerbates popularity bias. To address these limitations, we propose Preference Evolution Tracking (PET), a framework that reframes the task as inferring a dynamic probability distribution over a stable and interpretable lattice of preference clusters. By applying logit-probing and generative classification techniques, PET infers a user's preference as a probability distribution, enabling transparent preference learning. On public benchmarks (Yelp, MovieLens), PET improves ranking quality by up to 40% in NDCG over direct generation baselines. On a large-scale, real-world dataset from a short-video platform, it excels at ranking long-tail contents, significantly outperforming a SOTA production model by 7 times in the NDCG score. Ultimately, PET transforms the user profile model from direct preference list generation to a transparent distributional preference mapping, paving the way for more explainable, fair, and diverse personalization systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation</title>
<link>https://arxiv.org/abs/2510.00829</link>
<guid>https://arxiv.org/abs/2510.00829</guid>
<content:encoded><![CDATA[
arXiv:2510.00829v2 Announce Type: replace 
Abstract: \textbf{RE}trieval-\textbf{A}ugmented \textbf{L}LM-based \textbf{M}achine \textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like idiomatic translation, but its reliability under noisy retrieval contexts remains poorly understood despite this being a common challenge in real-world deployment. To address this gap, we propose a noise synthesis framework and new metrics to evaluate the robustness of REAL-MT systematically. Using this framework, we instantiate REAL-MT with Qwen-series models, including standard LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate their performance on idiomatic translation across high-, medium-, and low-resource language pairs under synthesized noise. Our results show that low-resource language pairs, which rely more heavily on retrieved context, degrade more severely under noise than high-resource ones and often produce nonsensical translations. Although LRMs possess enhanced reasoning capabilities, they show no improvement in error correction and are even more susceptible to noise, tending to rationalize incorrect contexts. We find that this stems from an attention shift away from the source idiom to noisy content, while confidence increases despite declining accuracy, indicating poor calibration. To mitigate these issues, we investigate training-free and fine-tuning strategies, which improve robustness at the cost of performance in clean contexts, revealing a fundamental trade-off. Our findings highlight the limitations of current approaches, underscoring the need for self-verifying integration mechanisms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles</title>
<link>https://arxiv.org/abs/2510.03898</link>
<guid>https://arxiv.org/abs/2510.03898</guid>
<content:encoded><![CDATA[
arXiv:2510.03898v2 Announce Type: replace 
Abstract: Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Human Behavioral Baseline for Collective Governance in Software Projects</title>
<link>https://arxiv.org/abs/2510.08956</link>
<guid>https://arxiv.org/abs/2510.08956</guid>
<content:encoded><![CDATA[
arXiv:2510.08956v2 Announce Type: replace 
Abstract: We study how open source communities describe participation and control through version controlled governance documents. Using a corpus of 710 projects with paired snapshots, we parse text into actors, rules, actions, and objects, then group them and measure change with entropy for evenness, richness for diversity, and Jensen Shannon divergence for drift. Projects define more roles and more actions over time, and these are distributed more evenly, while the composition of rules remains stable. These findings indicate that governance grows by expanding and balancing categories of participation without major shifts in prescriptive force. The analysis provides a reproducible baseline for evaluating whether future AI mediated workflows concentrate or redistribute authority.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building a Macedonian Recipe Dataset: Collection, Parsing, and Comparative Analysis</title>
<link>https://arxiv.org/abs/2510.14128</link>
<guid>https://arxiv.org/abs/2510.14128</guid>
<content:encoded><![CDATA[
arXiv:2510.14128v2 Announce Type: replace 
Abstract: Computational gastronomy increasingly relies on diverse, high-quality recipe datasets to capture regional culinary traditions. Although there are large-scale collections for major languages, Macedonian recipes remain under-represented in digital research. In this work, we present the first systematic effort to construct a Macedonian recipe dataset through web scraping and structured parsing. We address challenges in processing heterogeneous ingredient descriptions, including unit, quantity, and descriptor normalization. An exploratory analysis of ingredient frequency and co-occurrence patterns, using measures such as Pointwise Mutual Information and Lift score, highlights distinctive ingredient combinations that characterize Macedonian cuisine. The resulting dataset contributes a new resource for studying food culture in underrepresented languages and offers insights into the unique patterns of Macedonian culinary tradition.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios</title>
<link>https://arxiv.org/abs/2510.15501</link>
<guid>https://arxiv.org/abs/2510.15501</guid>
<content:encoded><![CDATA[
arXiv:2510.15501v2 Announce Type: replace 
Abstract: Despite the remarkable advances of Large Language Models (LLMs) across diverse cognitive tasks, the rapid enhancement of these capabilities also introduces emergent deceptive behaviors that may induce severe risks in high-stakes deployments. More critically, the characterization of deception across realistic real-world scenarios remains underexplored. To bridge this gap, we establish DeceptionBench, the first benchmark that systematically evaluates how deceptive tendencies manifest across different societal domains, what their intrinsic behavioral patterns are, and how extrinsic factors affect them. Specifically, on the static count, the benchmark encompasses 150 meticulously designed scenarios in five domains, i.e., Economy, Healthcare, Education, Social Interaction, and Entertainment, with over 1,000 samples, providing sufficient empirical foundations for deception analysis. On the intrinsic dimension, we explore whether models exhibit self-interested egoistic tendencies or sycophantic behaviors that prioritize user appeasement. On the extrinsic dimension, we investigate how contextual factors modulate deceptive outputs under neutral conditions, reward-based incentivization, and coercive pressures. Moreover, we incorporate sustained multi-turn interaction loops to construct a more realistic simulation of real-world feedback dynamics. Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal critical vulnerabilities, particularly amplified deception under reinforcement dynamics, demonstrating that current models lack robust resistance to manipulative contextual cues and the urgent need for advanced safeguards against various deception behaviors. Code and resources are publicly available at https://github.com/Aries-iai/DeceptionBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training</title>
<link>https://arxiv.org/abs/2510.15859</link>
<guid>https://arxiv.org/abs/2510.15859</guid>
<content:encoded><![CDATA[
arXiv:2510.15859v2 Announce Type: replace 
Abstract: Reinforcement learning has powered many of the recent breakthroughs in large language models, especially for tasks where rewards can be computed automatically, such as code generation. However, these methods deteriorate in open-ended domains like medical consultation, where feedback is inherently ambiguous, highly context-dependent, and cannot be reduced to a reliable scalar signal. In such settings, RL must either rely on supervision-intensive reward models that often fail to generalize, or it falls into pathological behaviors such as reward hacking - an especially troubling risk for high-stakes medical dialogue. To address these limitations, we introduce ORBIT, an open-ended rubric-based incremental training framework for high-stakes medical dialogue. ORBIT integrates synthetic dialogue generation with dynamically constructed rubrics that serve as adaptive guides for incremental RL. Instead of relying on external medical knowledge bases or handcrafted rule sets, ORBIT uses rubric-driven feedback to steer the learning process. Its judge component can be instantiated with general-purpose instruction-following LLMs, removing the need for any task-specific fine-tuning. Applied to the Qwen3-4B-Instruct model, ORBIT raises the HealthBench-Hard score from 7.0 to 27.5 using only 2k training samples, achieving SOTA performance for models at this scale. With larger rubric datasets, ORBIT-trained models further compete with the strongest open-source baselines on HealthBench-Hard. Our analysis shows that rubric-guided RL consistently improves consultation quality across diverse medical scenarios. We also apply such rubric generation and training pipeline to InfoBench, where ORBIT enhances instruction-following performance, highlighting the generality of rubric-based feedback.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Conceptual-Thought Elicits Daily Conversation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.18434</link>
<guid>https://arxiv.org/abs/2510.18434</guid>
<content:encoded><![CDATA[
arXiv:2510.18434v3 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) is widely applied to enhance the LLM capability in math, coding and reasoning tasks. However, its performance is limited for open-domain tasks, when there are no clearly defined reasoning steps or logical transitions. To mitigate such challenges, we propose a new prompt-based paradigm called Chain of Conceptual Thoughts (CoCT), which suggests the LLM first to produce the tag of concepts, then complete the detailed content following the concept. To encourage this hierarchical way of thinking, we implement the concepts with emotions, strategies and topics. We experiment with this paradigm in daily and emotional support conversations, covering tasks with both in-domain and out-of-domain concept settings. Automatic, human, and LLM-based evaluations reveal that CoCT surpasses several prompt-based baselines such as self-refine, ECoT, SoT and RAG, suggesting a potential solution of LLM prompting paradigm for a wider scope of tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA</title>
<link>https://arxiv.org/abs/2510.19172</link>
<guid>https://arxiv.org/abs/2510.19172</guid>
<content:encoded><![CDATA[
arXiv:2510.19172v2 Announce Type: replace 
Abstract: LLMs often fail to handle temporal knowledge conflicts--contradictions arising when facts evolve over time within their training data. Existing studies evaluate this phenomenon through benchmarks built on structured knowledge bases like Wikidata, but they focus on widely-covered, easily-memorized popular entities and lack the dynamic structure needed to fairly evaluate LLMs with different knowledge cut-off dates. We introduce evolveQA, a benchmark specifically designed to evaluate LLMs on temporally evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS updates, Azure changes, and WHO disease outbreak reports. Our framework identifies naturally occurring knowledge evolution and generates questions with gold answers tailored to different LLM knowledge cut-off dates. Through extensive evaluation of 12 open and closed-source LLMs across 3 knowledge probing formats, we demonstrate significant performance drops of up to 31% on evolveQA compared to static knowledge questions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SelecTKD: Selective Token-Weighted Knowledge Distillation for LLMs</title>
<link>https://arxiv.org/abs/2510.24021</link>
<guid>https://arxiv.org/abs/2510.24021</guid>
<content:encoded><![CDATA[
arXiv:2510.24021v2 Announce Type: replace 
Abstract: Knowledge distillation (KD) is a standard route to compress Large Language Models (LLMs) into compact students, yet most pipelines uniformly apply token-wise loss regardless of teacher confidence. This indiscriminate supervision amplifies noisy, high-entropy signals and is especially harmful under large teacher-student capacity gaps. We introduce SelecTKD, a plug-and-play Selective Token-Weighted distillation framework that shifts the focus from "how to measure divergence" to "where to apply learning". At each step, the student proposes tokens that are verified by the teacher through a robust propose-and-verify procedure with two variants: greedy Top-k and non-greedy Spec-k. Accepted tokens receive full loss, while rejected tokens are masked or down-weighted. This objective-agnostic design works with on- and off-policy data, induces an implicit curriculum quantified by Token Acceptance Rate (TAR), and stabilizes optimization. Across instruction following, mathematical reasoning, code generation, and a VLM setting, SelecTKD consistently improves strong baselines and achieves state-of-the-art results for small models without architectural changes or extra reference models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.25117</link>
<guid>https://arxiv.org/abs/2510.25117</guid>
<content:encoded><![CDATA[
arXiv:2510.25117v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities, but their training on massive corpora poses significant risks from memorized sensitive information. To mitigate these issues and align with legal standards, unlearning has emerged as a critical technique to selectively erase specific knowledge from LLMs without compromising their overall performance. This survey provides a systematic review of over 180 papers on LLM unlearning published since 2021. First, it introduces a novel taxonomy that categorizes unlearning methods based on the phase in the LLM pipeline of the intervention. This framework further distinguishes between parameter modification and parameter selection strategies, thus enabling deeper insights and more informed comparative analysis. Second, it offers a multidimensional analysis of evaluation paradigms. For datasets, we compare 18 existing benchmarks from the perspectives of task format, content, and experimental paradigms to offer actionable guidance. For metrics, we move beyond mere enumeration by dividing knowledge memorization metrics into 10 categories to analyze their advantages and applicability, while also reviewing metrics for model utility, robustness, and efficiency. By discussing current challenges and future directions, this survey aims to advance the field of LLM unlearning and the development of secure AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Personality Generation of LLMs at Decoding-time</title>
<link>https://arxiv.org/abs/2511.01891</link>
<guid>https://arxiv.org/abs/2511.01891</guid>
<content:encoded><![CDATA[
arXiv:2511.01891v2 Announce Type: replace 
Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. It flexibly controls multi-personality without relying on scarce multi-dimensional models or extra training, leveraging implicit density ratios in single-dimensional models as a "free lunch" to reformulate the task as sampling from a target strategy aggregating these ratios. To implement MPG efficiently, we design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and parallelly validates them via estimated thresholds within a sliding window. This significantly reduces computational overhead while maintaining high-quality generation. Experiments on MBTI personality and Role-Playing demonstrate the effectiveness of MPG, showing improvements up to 16%-18%. Code and data are available at https://github.com/Libra117/MPG .
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinGPT: Open-Source Financial Large Language Models</title>
<link>https://arxiv.org/abs/2306.06031</link>
<guid>https://arxiv.org/abs/2306.06031</guid>
<content:encoded><![CDATA[
arXiv:2306.06031v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.
  In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions</title>
<link>https://arxiv.org/abs/2406.08824</link>
<guid>https://arxiv.org/abs/2406.08824</guid>
<content:encoded><![CDATA[
arXiv:2406.08824v2 Announce Type: replace-cross 
Abstract: Members of the Human-Robot Interaction (HRI) and Machine Learning (ML) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interaction, household and workplace tasks, approximating 'common sense reasoning', and modeling humans. However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications. To assess whether such concerns are well placed in the context of HRI, we evaluate several highly-rated LLMs on discrimination and safety criteria. Our evaluation reveals that LLMs are currently unsafe for people across a diverse range of protected identity characteristics, including, but not limited to, race, gender, disability status, nationality, religion, and their intersections. Concretely, we show that LLMs produce directly discriminatory outcomes- e.g., 'gypsy' and 'mute' people are labeled untrustworthy, but not 'european' or 'able-bodied' people. We find various such examples of direct discrimination on HRI tasks such as facial expression, proxemics, security, rescue, and task assignment. Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions-such as incident-causing misstatements, taking people's mobility aids, and sexual predation. Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so. We provide code to reproduce our experiments at https://github.com/rumaisa-azeem/llm-robots-discrimination-safety .
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents</title>
<link>https://arxiv.org/abs/2408.14033</link>
<guid>https://arxiv.org/abs/2408.14033</guid>
<content:encoded><![CDATA[
arXiv:2408.14033v3 Announce Type: replace-cross 
Abstract: Autonomous machine learning research has gained significant attention recently. We present MLR-COPILOT, an autonomous Machine Learning Research framework powered by large language model agents. The system is designed to enhance ML research productivity through automatic generation and implementation of research ideas within constraints. Our work was released in August 2024 (concurrent to AI-Scientist) and has gained notable recognition from leading projects. We further enhance our ideation with training afterwards. The framework consists of three stages: idea generation, experiment implementation, and code execution. First, existing research papers are used to generate feasible ideas and experiment plans with IdeaAgent, powered by an RL-tuned LLM. Next, ExperimentAgent leverages retrieved prototype code to convert plans into executable code with optionally retrieved candidate models and data from HuggingFace. In the final stage, ExperimentAgent runs experiments, and allows subsequent iterations of debugging and human feedback for a better chance of success with executable outcomes. We evaluate our framework on five machine learning research tasks. Experiment results demonstrate the potential of our framework to facilitate ML research progress and innovation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair In-Context Learning via Latent Concept Variables</title>
<link>https://arxiv.org/abs/2411.02671</link>
<guid>https://arxiv.org/abs/2411.02671</guid>
<content:encoded><![CDATA[
arXiv:2411.02671v2 Announce Type: replace-cross 
Abstract: The emerging in-context learning (ICL) ability of large language models (LLMs) has prompted their use for predictive tasks in various domains with different data types, including tabular data, facilitated by serialization methods. However, with increasing applications in high-stakes domains, it has been shown that LLMs can inherit social bias and discrimination from their pre-training data. In this work, we investigate inherent bias in LLMs during in-context learning with tabular data. We focus on an optimal demonstration selection approach that utilizes latent concept variables for resource-efficient task adaptation. We design data augmentation strategies that reduce the correlation between predictive outcomes and sensitive variables, helping promote fairness during latent concept learning. We utilize the learned concept to select demonstrations and obtain fair predictions. The latent concept variables are learned using a smaller internal LLM and generalized to larger external LLMs. We empirically verify that the fair latent variable approach improves fairness results on tabular datasets compared to multiple heuristic demonstration selection methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuanTaxo: A Quantum Approach to Self-Supervised Taxonomy Expansion</title>
<link>https://arxiv.org/abs/2501.14011</link>
<guid>https://arxiv.org/abs/2501.14011</guid>
<content:encoded><![CDATA[
arXiv:2501.14011v3 Announce Type: replace-cross 
Abstract: A taxonomy is a hierarchical graph containing knowledge to provide valuable insights for various web applications. However, the manual construction of taxonomies requires significant human effort. As web content continues to expand at an unprecedented pace, existing taxonomies risk becoming outdated, struggling to incorporate new and emerging information effectively. As a consequence, there is a growing need for dynamic taxonomy expansion to keep them relevant and up-to-date. Existing taxonomy expansion methods often rely on classical word embeddings to represent entities. However, these embeddings fall short of capturing hierarchical polysemy, where an entity's meaning can vary based on its position in the hierarchy and its surrounding context. To address this challenge, we introduce QuanTaxo, a quantum-inspired framework for taxonomy expansion that encodes entities in a Hilbert space and models interference effects between them, yielding richer, context-sensitive representations. Comprehensive experiments on five real-world benchmark datasets show that QuanTaxo significantly outperforms classical embedding models, achieving substantial improvements of 12.3% in accuracy, 11.2% in Mean Reciprocal Rank (MRR), and 6.9% in Wu & Palmer (Wu&amp;P) metrics across nine classical embedding-based baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIP: Perturbation-based Iterative Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2501.15278</link>
<guid>https://arxiv.org/abs/2501.15278</guid>
<content:encoded><![CDATA[
arXiv:2501.15278v3 Announce Type: replace-cross 
Abstract: The rapid increase in the parameter counts of Large Language Models (LLMs), which often reach into the billions or even trillions, presents significant challenges for their practical deployment, particularly in resource-constrained environments. To address this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel double-view structured pruning method to optimize LLMs, which combines information from two different views: the unperturbed view and the perturbed view. With the calculation of gradient differences, PIP iteratively prunes those that struggle to distinguish between these two views. Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original model's accuracy across varied benchmarks. In some cases, the performance of the pruned model is within 5% of the unpruned version, demonstrating PIP's ability to preserve key aspects of model effectiveness. Moreover, PIP consistently outperforms existing state-of-the-art (SOTA) structured pruning methods, establishing it as a leading technique for optimizing LLMs in constrained environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Euler to AI: Unifying Formulas for Mathematical Constants</title>
<link>https://arxiv.org/abs/2502.17533</link>
<guid>https://arxiv.org/abs/2502.17533</guid>
<content:encoded><![CDATA[
arXiv:2502.17533v3 Announce Type: replace-cross 
Abstract: The constant $\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines Large Language Models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 385 distinct formulas for $\pi$ and prove relations between 360 (94%) of them, of which 166 (43%) can be derived from a single mathematical object - linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine.
  Our method generalizes to other constants, including $e$, $\zeta(3)$, and Catalan's constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
<link>https://arxiv.org/abs/2504.06261</link>
<guid>https://arxiv.org/abs/2504.06261</guid>
<content:encoded><![CDATA[
arXiv:2504.06261v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning</title>
<link>https://arxiv.org/abs/2505.16186</link>
<guid>https://arxiv.org/abs/2505.16186</guid>
<content:encoded><![CDATA[
arXiv:2505.16186v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.16826</link>
<guid>https://arxiv.org/abs/2505.16826</guid>
<content:encoded><![CDATA[
arXiv:2505.16826v2 Announce Type: replace-cross 
Abstract: Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models, even without supervised fine-tuning. However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions and hindering effective learning. To address this limitation, we propose Key-token Advantage Estimation (KTAE) - a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Overthinking in Large Reasoning Models via Manifold Steering</title>
<link>https://arxiv.org/abs/2505.22411</link>
<guid>https://arxiv.org/abs/2505.22411</guid>
<content:encoded><![CDATA[
arXiv:2505.22411v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.04245</link>
<guid>https://arxiv.org/abs/2506.04245</guid>
<content:encoded><![CDATA[
arXiv:2506.04245v3 Announce Type: replace-cross 
Abstract: As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PurpCode: Reasoning for Safer Code Generation</title>
<link>https://arxiv.org/abs/2507.19060</link>
<guid>https://arxiv.org/abs/2507.19060</guid>
<content:encoded><![CDATA[
arXiv:2507.19060v4 Announce Type: replace-cross 
Abstract: We introduce PurpCode, the first post-training recipe for training safe code reasoning models towards generating secure code and defending against malicious cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule Learning, which explicitly teaches the model to reference cybersafety rules to generate vulnerability-free code and to avoid facilitating malicious cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety and preserves model utility through diverse, multi-objective reward mechanisms. To empower the training pipelines with comprehensive cybersafety data, we conduct internal red-teaming to synthesize comprehensive and high-coverage prompts based on real-world tasks for inducing unsafe cyberactivities in the model. Based on PurpCode, we develop a reasoning-based coding model, namely PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming various frontier models. Meanwhile, our alignment method decreases the model overrefusal rates in both general and cybersafety-specific scenarios, while preserving model utility in both code generation and common security knowledge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling</title>
<link>https://arxiv.org/abs/2508.02503</link>
<guid>https://arxiv.org/abs/2508.02503</guid>
<content:encoded><![CDATA[
arXiv:2508.02503v2 Announce Type: replace-cross 
Abstract: LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, a framework that enhances any solver-generation pipeline to produce higher-quality solvers from natural-language descriptions of optimization problems. OptiHive uses a single batched generation to produce diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Accounting for the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5% to 92% on the most complex problems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System</title>
<link>https://arxiv.org/abs/2508.06059</link>
<guid>https://arxiv.org/abs/2508.06059</guid>
<content:encoded><![CDATA[
arXiv:2508.06059v2 Announce Type: replace-cross 
Abstract: State-of-the-art (SOTA) fact-checking systems combat misinformation by employing autonomous LLM-based agents to decompose complex claims into smaller sub-claims, verify each sub-claim individually, and aggregate the partial results to produce verdicts with justifications (explanations for the verdicts). The security of these systems is crucial, as compromised fact-checkers can amplify misinformation, but remains largely underexplored. To bridge this gap, this work introduces a novel threat model against such fact-checking systems and presents \textsc{Fact2Fiction}, the first poisoning attack framework targeting SOTA agentic fact-checking systems. Fact2Fiction employs LLMs to mimic the decomposition strategy and exploit system-generated justifications to craft tailored malicious evidences that compromise sub-claim verification. Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\% higher attack success rates than SOTA attacks across various poisoning budgets and exposes security weaknesses in existing fact-checking systems, highlighting the need for defensive countermeasures.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning</title>
<link>https://arxiv.org/abs/2508.08385</link>
<guid>https://arxiv.org/abs/2508.08385</guid>
<content:encoded><![CDATA[
arXiv:2508.08385v2 Announce Type: replace-cross 
Abstract: We study an efficient implementation of Multi-Armed Bandit (MAB)-based Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is that it spends a significant time deciding which node to expand next. While selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity with traditional array-based priority-queues for dense integer keys, the tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node selection is significant, unlike in game tree search, where the cost is negligible compared to the node evaluation (rollouts) because $d$ is inherently limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we propose a bilevel modification to MCTS that runs a best-first search from each selected leaf node with an expansion budget proportional to $d$, which achieves amortized $O(1)$ runtime for node selection, equivalent to the traditional queue-based OPEN list. In addition, we introduce Tree Collapsing, an enhancement that reduces action selection steps and further improves the performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting the Effects of Quantization on LLMs</title>
<link>https://arxiv.org/abs/2508.16785</link>
<guid>https://arxiv.org/abs/2508.16785</guid>
<content:encoded><![CDATA[
arXiv:2508.16785v2 Announce Type: replace-cross 
Abstract: Quantization offers a practical solution to deploy LLMs in resource-constraint environments. However, its impact on internal representations remains understudied, raising questions about the reliability of quantized models. In this study, we employ a range of interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings reveal that the impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons, i.e., those with activation values close to 0 across the dataset, remains consistent regardless of quantization. In terms of neuron contribution to predictions, we observe that smaller full precision models exhibit fewer salient neurons, whereas larger models tend to have more, with the exception of Llama-2-7B. The effect of quantization on neuron redundancy varies across models. Overall, our findings suggest that effect of quantization may vary by model and tasks, however, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Large Language Model Copyright Auditing via Fingerprinting</title>
<link>https://arxiv.org/abs/2508.19843</link>
<guid>https://arxiv.org/abs/2508.19843</guid>
<content:encoded><![CDATA[
arXiv:2508.19843v3 Announce Type: replace-cross 
Abstract: The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that compares the distinctive features (i.e., fingerprint) of LLMs to identify whether an LLM is derived from another, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of the emerging LLM fingerprinting. We introduce a unified framework and taxonomy that structures the field: white-box methods are classified based on their feature source as static, forward-pass, or backward-pass fingerprinting, while black-box methods are distinguished by their query strategy as either untargeted or targeted. Furthermore, we propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon 7 mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent techniques (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensemble Debates with Local Large Language Models for AI Alignment</title>
<link>https://arxiv.org/abs/2509.00091</link>
<guid>https://arxiv.org/abs/2509.00091</guid>
<content:encoded><![CDATA[
arXiv:2509.00091v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) take on greater roles in high-stakes decisions, alignment with human values is essential. Reliance on proprietary APIs limits reproducibility and broad participation. We study whether local open-source ensemble debates can improve alignmentoriented reasoning. Across 150 debates spanning 15 scenarios and five ensemble configurations, ensembles outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13), with the largest gains in reasoning depth (+19.4%) and argument quality (+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human enhancement (+0.80). We provide code, prompts, and a debate data set, providing an accessible and reproducible foundation for ensemble-based alignment evaluation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data</title>
<link>https://arxiv.org/abs/2509.07202</link>
<guid>https://arxiv.org/abs/2509.07202</guid>
<content:encoded><![CDATA[
arXiv:2509.07202v2 Announce Type: replace-cross 
Abstract: Text generating capabilities have undergone a substantial transformation with the introduction of large language models (LLMs). Electroencephalography (EEG)-based text production is still difficult, though, because it requires a lot of data and processing power. This paper introduces a new method that combines the use of the Gemma 2B LLM with a classifier-LLM architecture to incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically lowers the amount of data and compute power needed while achieving performance close to that of cutting-edge methods. Notably, compared to current methodologies, our methodology delivers an overall performance improvement of 10%. The suggested architecture demonstrates the possibility of effective transfer learning for EEG-based text production, remaining strong and functional even in the face of data limits. This work highlights the potential of integrating LLMs with EEG decoding to improve assistive technologies and improve independence and communication for those with severe motor limitations. Our method pushes the limits of present capabilities and opens new paths for research and application in brain-computer interfaces by efficiently using the strengths of pre-trained language models. This makes EEG-based text production more accessible and efficient.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?</title>
<link>https://arxiv.org/abs/2509.16941</link>
<guid>https://arxiv.org/abs/2509.16941</guid>
<content:encoded><![CDATA[
arXiv:2509.16941v2 Announce Type: replace-cross 
Abstract: We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</title>
<link>https://arxiv.org/abs/2509.19002</link>
<guid>https://arxiv.org/abs/2509.19002</guid>
<content:encoded><![CDATA[
arXiv:2509.19002v2 Announce Type: replace-cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning</title>
<link>https://arxiv.org/abs/2509.23292</link>
<guid>https://arxiv.org/abs/2509.23292</guid>
<content:encoded><![CDATA[
arXiv:2509.23292v2 Announce Type: replace-cross 
Abstract: Tool-integrated reasoning (TIR) has become a key approach for improving large reasoning models (LRMs) on complex problems. Prior work has mainly studied when to invoke tools, while overlooking how tools are applied. We identify two common patterns: a calculator pattern that uses code for direct computation, and an algorithmic pattern that encodes problems as programs. Misaligned choices often cause failures even when reasoning is sound. We propose a two-stage framework that first builds code competence from both patterns and then aligns pattern selection with teacher preferences. Across challenging math datasets, our pattern-aware method substantially improves both code usage and accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a pattern-aware approach for tool-integrated reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge</title>
<link>https://arxiv.org/abs/2510.01223</link>
<guid>https://arxiv.org/abs/2510.01223</guid>
<content:encoded><![CDATA[
arXiv:2510.01223v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks. However, they remain exposed to jailbreak attacks, eliciting harmful responses. The nested scenario strategy has been increasingly adopted across various methods, demonstrating immense potential. Nevertheless, these methods are easily detectable due to their prominent malicious intentions. In this work, we are the first to find and systematically verify that LLMs' alignment defenses are not sensitive to nested scenarios, where these scenarios are highly semantically relevant to the queries and incorporate targeted toxic knowledge. This is a crucial yet insufficiently explored direction. Based on this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs' alignment. By building scenarios highly relevant to the queries and integrating targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs. Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful queries, leading to outstanding concealment. Extensive experiments demonstrate that RTS-Attack exhibits superior performance in both efficiency and universality compared to the baselines across diverse advanced LLMs, including GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available at https://github.com/nercode/Work. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL CONTENT.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation</title>
<link>https://arxiv.org/abs/2510.21341</link>
<guid>https://arxiv.org/abs/2510.21341</guid>
<content:encoded><![CDATA[
arXiv:2510.21341v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often struggle with generating truly innovative ideas, typically defaulting to high-probability, familiar concepts within their training data's "gravity wells." While advanced search-based methods like Tree of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by their reliance on unprincipled, inconsistent self-evaluation heuristics to guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel framework that reframes creative generation as a principled, guided exploration of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo Tree Search (MCTS) governed by a hierarchical guidance system. For long-range direction, a "semantic compass" vector, formulated via orthogonal projection, steers the search towards relevant novelty. For local, step-by-step decisions, a landscape-aware value function replaces flawed self-evaluation with an explicit reward structure that balances intrinsic coherence, extrinsic novelty, and narrative progress. Extensive experiments demonstrate that Magellan significantly outperforms strong baselines, including ReAct and ToT, in generating scientific ideas with superior plausibility and innovation. Our work shows that for creative discovery, a principled, guided search is more effective than unconstrained agency, paving the way for LLMs to become more capable partners in innovation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surface Reading LLMs: Synthetic Text and its Styles</title>
<link>https://arxiv.org/abs/2510.22162</link>
<guid>https://arxiv.org/abs/2510.22162</guid>
<content:encoded><![CDATA[
arXiv:2510.22162v3 Announce Type: replace-cross 
Abstract: Despite a potential plateau in ML advancement, the societal impact of large language models lies not in approaching superintelligence but in generating text surfaces indistinguishable from human writing. While Critical AI Studies provides essential material and socio-technical critique, it risks overlooking how LLMs phenomenologically reshape meaning-making. This paper proposes a semiotics of "surface integrity" as attending to the immediate plane where LLMs inscribe themselves into human communication. I distinguish three knowledge interests in ML research (epistemology, epist\=em\=e, and epistemics) and argue for integrating surface-level stylistic analysis alongside depth-oriented critique. Through two case studies examining stylistic markers of synthetic text, I argue how attending to style as a semiotic phenomenon reveals LLMs as cultural machines that transform the conditions of meaning emergence and circulation in contemporary discourse, independent of questions about machine consciousness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novelty and Impact of Economics Papers</title>
<link>https://arxiv.org/abs/2511.01211</link>
<guid>https://arxiv.org/abs/2511.01211</guid>
<content:encoded><![CDATA[
arXiv:2511.01211v3 Announce Type: replace-cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Cycle Detection in Agentic Applications</title>
<link>https://arxiv.org/abs/2511.10650</link>
<guid>https://arxiv.org/abs/2511.10650</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic applications, large language models, cycle detection, semantic analysis, stock market application

<br /><br />Summary:  
This paper addresses the issue of hidden execution cycles in agentic applications powered by Large Language Models (LLMs), where non-deterministic behaviors lead to resource waste without explicit errors being raised. Traditional observability platforms fail to detect these inefficiencies, prompting the need for a novel detection framework. The authors propose an unsupervised cycle detection framework that integrates structural and semantic analysis. The framework starts with a computationally efficient temporal call stack analysis to detect explicit loops within execution trajectories. Next, it employs semantic similarity analysis to identify subtle cycles caused by redundant content generation that structural methods alone miss. Evaluation is performed on 1575 trajectories derived from a LangGraph-based stock market application, demonstrating that the hybrid approach achieves a strong F1 score of 0.72 with precision at 0.62 and recall at 0.86. This result significantly outperforms methods relying solely on structural analysis (F1: 0.08) or semantic methods (F1: 0.28). Despite promising outcomes, the authors acknowledge considerable room for improvement and suggest further research is needed to optimize the framework and overcome current limitations. <div>
arXiv:2511.10650v1 Announce Type: new 
Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs</title>
<link>https://arxiv.org/abs/2511.10651</link>
<guid>https://arxiv.org/abs/2511.10651</guid>
<content:encoded><![CDATA[
<div> Keywords: simulation deduction, large language models, multi-round interaction, report generation, performance evaluation<br /><br />Summary:<br /><br />This paper addresses the challenge of generating high-quality, well-structured analysis reports in the context of simulation deduction for modern warfare. First, it highlights the significance of data analysis and performance evaluation to help military personnel assess strategies and operational plans effectively. Second, it notes the limitations of traditional manual analysis, particularly its time consumption and propensity for human error. Third, the authors propose leveraging large language models (LLMs) with strong analytical and inferencing capabilities to improve both efficiency and accuracy. Fourth, they introduce a methodology that decomposes complex tasks into sub-tasks, utilizing specifically designed system and user prompts for each. Multi-round interactions with the LLM incorporate self-checking and reflection mechanisms to enable structured data extraction and multi-step analysis. Fifth, custom tools are developed to generate visual figures and compute metrics, enhancing report quality. Additionally, multiple adaptable report templates are created to suit different applications and input data types. Finally, extensive evaluations demonstrate that the reports produced with this method outperform baseline approaches, yielding higher quality and better scoring results. <div>
arXiv:2511.10651v1 Announce Type: new 
Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI</title>
<link>https://arxiv.org/abs/2511.10652</link>
<guid>https://arxiv.org/abs/2511.10652</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval-augmented generation, episodic memory, biographical dialogue, Van Gogh

<br /><br />Summary:  
This paper addresses the challenge of creating dialogue systems that embody historical characters, managing the trade-off between shallow responses from simple retrieval-augmented generation (RAG) and the latency of multi-stage reflection methods. The authors propose an architecture combining offline data augmentation with efficient parallel retrieval from a structured episodic memory. Biographical data is transformed into 1,774 enriched first-person memories enhanced with affective-semantic metadata. The system employs a two-stage retrieval process that generates prompts in just 0.52 seconds, offering a balance of response depth and speed. Evaluation using a large language model as judge and RAGAs metrics demonstrates that the proposed method performs on par with traditional RAG approaches when using GPT-4, while significantly outperforming on smaller models such as GPT-3.5 and GPT-3. This makes the system particularly suitable for resource-constrained environments. Additionally, the structured memory supports innovative visualization tools, including spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, extending its utility beyond dialogue to research and educational contexts. Van Gogh is used as a test case, but the design is generalizable to any historical figure with extensive textual records. The system offers a practical framework for accurate and efficient educational, museum, and research applications involving historical personalities. <div>
arXiv:2511.10652v1 Announce Type: new 
Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum Transformer for Language Generation</title>
<link>https://arxiv.org/abs/2511.10653</link>
<guid>https://arxiv.org/abs/2511.10653</guid>
<content:encoded><![CDATA[
<div> Quantum computing, large language models, variational quantum circuits, natural language generation, hybrid quantum-classical<br /><br />Summary:<br /><br />This paper introduces HyQuT, the first hybrid quantum-classical large language model specifically designed for natural language generation. It marks a pioneering effort in integrating quantum computing with large-scale generative language tasks, particularly coherent and context-aware dialogue systems. The architecture incorporates variational quantum circuits (VQCs) within the Transformer framework, demonstrated at two sizes: 8 million and 150 million parameters. Remarkably, the model requires only a minimal quantum resource configuration—10 qubits and 80 quantum gates—to effectively replace approximately 10% of the parameters in the 150 million-parameter model. Experimental results confirm that this hybrid quantum-classical approach achieves convergence stability and generation quality on par with fully classical models. Thus, HyQuT provides an early but important proof-of-concept that quantum circuits can be feasibly embedded in large-scale language models without sacrificing performance. This work opens new pathways for exploring how quantum computing can enhance or complement classical machine learning, particularly in complex natural language processing applications. The findings suggest that modest quantum resources can meaningfully contribute to advanced deep learning architectures, moving quantum computing closer to practical large-scale AI use cases. <div>
arXiv:2511.10653v1 Announce Type: new 
Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Characterization of Temporal Constraint Processing in LLMs</title>
<link>https://arxiv.org/abs/2511.10654</link>
<guid>https://arxiv.org/abs/2511.10654</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal constraints, large language models, deadline detection, prompt brittleness, fine-tuning  

<br /><br />Summary:  
This study evaluates the ability of eight large language models (LLMs) ranging from 2.8B to 8B parameters to process temporal constraints in real-time, agentic decision-making contexts. It reveals a bimodal performance pattern where models either perform very well (95% accuracy) or poorly (50% accuracy), indicating inconsistent reliability. The research highlights severe sensitivity to prompt formatting, causing accuracy swings between 30 to 60 percentage points, as well as a systematic action bias with failing models producing 100% false positive rates in deadline detection tasks. Notably, model size within the tested range shows no clear correlation with capability; some smaller models outperformed larger ones. Fine-tuning with 200 synthetic examples offers modest improvements of 12-37 percentage points for partially capable models but does not solve the fundamental issues. The findings emphasize that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language alone, even when fine-tuned. Instead, effective temporal reasoning demands architectural features such as continuous temporal state representation, explicit constraint verification separate from linguistic pattern matching, and systematic compositional reasoning over temporal relations. The paper concludes that current autoregressive LLMs lack these essential mechanisms, posing unacceptable risks if deployed in time-critical applications without incorporating hybrid symbolic reasoning modules. <div>
arXiv:2511.10654v1 Announce Type: new 
Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment</title>
<link>https://arxiv.org/abs/2511.10655</link>
<guid>https://arxiv.org/abs/2511.10655</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral Neuro-Symbolic Reasoning, transformer-based node merging, sentence-level entailment validation, external knowledge graphs, scalable reasoning<br /><br />Summary:<br /><br />This report presents key enhancements to the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by integrating three semantically grounded preprocessing steps. First, it leverages transformer-based node merging using contextual embeddings such as Sentence-BERT and SimCSE to efficiently reduce redundancy within reasoning graphs. Second, it employs sentence-level entailment validation through pretrained Natural Language Inference (NLI) classifiers like RoBERTa and DeBERTa to improve the quality of graph edges. Third, the framework incorporates alignment with external knowledge graphs such as ConceptNet and Wikidata to supplement missing contextual information. These improvements collectively boost graph fidelity without modifying the underlying spectral reasoning pipeline. Experimental validation on benchmarks including ProofWriter, EntailmentBank, and CLUTRR demonstrates consistent accuracy improvements of up to 3.8%, better generalization to adversarial inputs, and a reduction in inference noise. A notable contribution is performing semantic and symbolic graph enhancements entirely upstream of spectral inference, avoiding the computational cost of quadratic attention mechanisms. Overall, these modular, semantically informed preprocessing strategies yield a robust, interpretable, and scalable reasoning system well-suited for real-world and open-domain applications. <div>
arXiv:2511.10655v1 Announce Type: new 
Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2511.10656</link>
<guid>https://arxiv.org/abs/2511.10656</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multi-objective alignment, preference adapter, prompt-specific weights, reward models<br /><br />Summary:  
This paper addresses the challenge of aligning Large Language Models (LLMs) with multiple human preferences across various objectives simultaneously. Existing approaches rely on manually set preference weights, which are difficult for users to specify accurately and often result in inefficient training due to exploring irrelevant preference combinations. To overcome these issues, the authors propose PRO (PReference Orchestrator), a novel framework featuring a lightweight preference adapter that automatically infers prompt-specific preference weights during training and deployment. The adapter learns appropriate weights by training on normalized reward scores from multiple reward models evaluating preferred responses, capturing effective balance across objectives for each prompt. The paper also provides theoretical analysis demonstrating that the prompt-aware preference mechanism outperforms fixed preference weights in multi-objective alignment scenarios. Extensive experiments across different tasks confirm the effectiveness of PRO compared to existing multi-objective alignment methods, showcasing improved performance and training efficiency. This approach simplifies user involvement and enhances adaptability of LLMs to diverse preferences without manual weight tuning. <div>
arXiv:2511.10656v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patent Representation Learning via Self-supervision</title>
<link>https://arxiv.org/abs/2511.10657</link>
<guid>https://arxiv.org/abs/2511.10657</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive learning, patent embeddings, section-based augmentation, intra-document views, prior-art retrieval<br /><br />Summary: This paper introduces a novel contrastive learning framework designed specifically for patent embeddings by utilizing multiple views derived from different sections within the same patent document. The authors identify a patent-specific limitation in SimCSE-style dropout augmentation, which tends to produce embeddings that are overly uniform and lack semantic cohesion. To overcome this issue, they propose section-based augmentation, where distinct sections such as the abstract, claims, and background are treated as complementary perspectives. This approach incorporates natural semantic and structural diversity, reducing over-dispersion and preserving both the global structure and local continuity of embeddings. Evaluated on large-scale benchmarks, the fully self-supervised method achieves performance comparable to or better than baselines supervised by citation and IPC data in tasks like prior-art retrieval and classification, without relying on potentially incomplete or brittle annotations. Additionally, the analysis reveals that different patent sections serve specialized purposes: claims and summaries particularly enhance retrieval tasks, whereas background sections improve classification accuracy. These findings underscore the advantage of leveraging the intrinsic discourse structure of patents, demonstrating that exploiting intra-document views is a scalable and generalizable strategy for effective patent understanding. <div>
arXiv:2511.10657v1 Announce Type: new 
Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages</title>
<link>https://arxiv.org/abs/2511.10658</link>
<guid>https://arxiv.org/abs/2511.10658</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, clinical reports, structured information extraction, prompting strategies, multi-institutional study<br /><br />Summary:<br /><br />This study evaluates 15 open-weight large language models (LLMs) on extracting structured information from pathology and radiology reports. The evaluation covers six clinical use cases: colorectal liver metastases, liver tumors, neurodegenerative diseases, soft-tissue tumors, melanomas, and sarcomas across three different institutes in the Netherlands, UK, and Czech Republic. Both general-purpose and medical-specialized LLMs of varying sizes were tested. Six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance metrics appropriate to each task were used, and results were analyzed using consensus rank aggregation and linear mixed-effects models. Top-ranked models achieved macro-average scores close to the level of inter-rater agreement across all tasks. Interestingly, small-to-medium general-purpose models performed on par with large models, while tiny and specialized models lagged behind. Prompt graph and few-shot prompting improved performance by approximately 13%. The study found that task-specific factors such as complexity and annotation variability influenced model performance more than model size or prompting technique. Overall, the findings demonstrate that open-weight LLMs offer a scalable and effective tool for extracting structured clinical data across various diseases, languages, and institutions. <div>
arXiv:2511.10658v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information Extraction From Fiscal Documents Using LLMs</title>
<link>https://arxiv.org/abs/2511.10659</link>
<guid>https://arxiv.org/abs/2511.10659</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, fiscal documents, hierarchical tables, data extraction, validation  

<br /><br />Summary:  
This paper explores the underutilized potential of Large Language Models (LLMs) to process complex, hierarchical tabular data found in multi-page government fiscal documents. The authors introduce a novel multi-stage pipeline designed specifically for extracting structured data from large PDF documents, exemplified by annual fiscal reports from the State of Karnataka, India, totaling over 200 pages. A key innovation is leveraging the inherent hierarchical structure of fiscal tables—consisting of subtotals and totals at multiple levels—to perform multi-level validation checks that improve extraction accuracy. This approach addresses a notable challenge of traditional OCR methods, which struggle to verify the accuracy of numerical data extraction. The pipeline incorporates domain knowledge, sequential contextual understanding, and algorithmic validation, allowing LLMs not only to interpret table data but also to recognize and utilize document-specific structural hierarchies. The result is a robust, scalable method that effectively converts complex PDF fiscal disclosures into clean, research-ready databases. The implementation demonstrates promise for broader application, especially in developing countries, where digitizing and structuring government financial data can support transparency and research. Overall, the work highlights the expanding capabilities of LLMs in handling structured data extraction tasks beyond text comprehension. <div>
arXiv:2511.10659v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Steering for Lossless Text Compression via Weighted Product of Experts</title>
<link>https://arxiv.org/abs/2511.10660</link>
<guid>https://arxiv.org/abs/2511.10660</guid>
<content:encoded><![CDATA[
<div> Keywords: lossless compression, neural compressors, universal compressor, weighted product of experts, text compression<br /><br />Summary: This paper addresses the challenge of improving lossless text compression by combining traditional universal compressors with neural language models. Traditional compressors like gzip are fast and general but typically yield suboptimal compression rates compared to neural compressors, which leverage extensive training data to better model distributions. However, neural compressors often fail to generalize well to unseen or out-of-distribution data, limiting their practical use. To overcome this, the authors propose a novel test-time steering method using a weighted product of experts (wPoE) framework that adaptively fuses a universal compression model with a pretrained neural language model during inference. This approach ensures the combined compression rate is at least as good as the best performing individual model without the need for any additional fine-tuning. The method is compatible with any autoregressive language model, making it broadly applicable. Extensive experiments demonstrate that their framework consistently improves text compression performance across diverse datasets and distribution shifts. Overall, the proposed wPoE-based test-time steering offers a practical, versatile, and effective solution for enhancing lossless text compression by leveraging the complementary strengths of universal and neural compressors. <div>
arXiv:2511.10660v1 Announce Type: new 
Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Evaluation of Large Language Model Behavior</title>
<link>https://arxiv.org/abs/2511.10661</link>
<guid>https://arxiv.org/abs/2511.10661</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation, Bayesian approach, uncertainty quantification, adversarial inputs<br /><br />Summary: This paper addresses the need for rigorous evaluation methods of text generation systems based on large language models (LLMs), focusing on issues such as harmful output generation and sensitivity to adversarial prompts. Traditional evaluation methods typically use curated benchmark prompt sets with binary outcomes (e.g., harmful vs. non-harmful) and aggregate these results without considering statistical uncertainty. The authors propose a Bayesian framework to quantify uncertainty in binary evaluation metrics, which accounts for the probabilistic nature of LLM-generated text. Two case studies demonstrate the approach: first, measuring refusal rates when LLMs respond to adversarially designed harmful prompts; second, assessing pairwise preferences between two LLMs on open-ended interactive dialogue tasks. The Bayesian method offers useful insights by providing credible intervals and uncertainty estimates, improving the interpretability of LLM evaluation results. This work highlights the importance of incorporating uncertainty quantification in LLM behavior assessment to produce more reliable and informative evaluation outcomes for developers and researchers. <div>
arXiv:2511.10661v1 Announce Type: new 
Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish</title>
<link>https://arxiv.org/abs/2511.10664</link>
<guid>https://arxiv.org/abs/2511.10664</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cross-lingual benchmark, Cantonese, Japanese, Turkish

<br /><br />Summary:  
This paper evaluates seven state-of-the-art large language models (LLMs)—GPT-4o, GPT-4, Claude 3.5 Sonnet, LLaMA 3.1, Mistral Large 2, LLaMA-2 Chat 13B, and Mistral 7B Instruct—on a novel cross-lingual benchmark including Cantonese, Japanese, and Turkish. The benchmark covers four tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. The evaluation combines human assessments of fluency, factual accuracy, and cultural appropriateness with automated metrics such as BLEU and ROUGE. Results indicate that the largest proprietary models (GPT-4o, GPT-4, Claude 3.5) outperform others across tasks and languages, with GPT-4o showing strong multilingual and cross-lingual capabilities. Claude 3.5 Sonnet demonstrates competitive knowledge and reasoning accuracy. Despite this, all models exhibit challenges addressing language-specific issues, including Turkish’s complex agglutinative morphology and Cantonese colloquialisms. Smaller open-source models lag behind markedly in fluency and accuracy, underscoring disparities in resource availability. The paper provides extensive quantitative results and qualitative error analyses, highlighting the need for improved cultural and linguistic generalization in LLMs. Finally, the benchmark and evaluation data have been publicly released to support reproducibility and future research in this area. <div>
arXiv:2511.10664v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models</title>
<link>https://arxiv.org/abs/2511.10665</link>
<guid>https://arxiv.org/abs/2511.10665</guid>
<content:encoded><![CDATA[
<div> Keywords: guard models, semantic robustness, paraphrase consistency, model calibration, skew-aware aggregation<br /><br />Summary:<br />Guard models play a vital role in ensuring the safety of large language models (LLMs), but their vulnerability to superficial linguistic changes undermines their reliability. This study reveals that even meaning-preserving paraphrases can cause significant fluctuations in guard model safety scores, indicating a lack of true semantic understanding. To tackle this, the authors introduce a self-supervised training framework designed to enhance semantic robustness by leveraging paraphrase sets. A key innovation is the use of a novel skew-aware aggregation strategy to compute robust targets for enforcing consistent predictions, as conventional methods like mean and median were found to potentially worsen safety outcomes. The approach was tested on six open-source guard models, resulting in a roughly 58% reduction in semantic variability across paraphrases and an average improvement of about 2.5% in benchmark accuracy. Additionally, the method generalizes effectively to previously unseen stylistic variations. An important finding is the discovered bidirectional relationship between model calibration and consistency; robustness training improved calibration by up to 40%. Overall, this work underscores the significance of prioritizing semantic consistency as a core training objective and offers a practical, scalable approach for developing more reliable and robust guard models to enhance LLM safety. <div>
arXiv:2511.10665v1 Announce Type: new 
Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLM Understanding via Structured Tabular Decision Simulations</title>
<link>https://arxiv.org/abs/2511.10667</link>
<guid>https://arxiv.org/abs/2511.10667</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Understanding, Structured Tabular Decision Simulations, Decision Factors, Evaluation Frameworks  

<br /><br />Summary:  
Large language models (LLMs) often deliver high predictive accuracy but this does not guarantee genuine understanding comparable to human expertise. True understanding in LLMs involves making consistent and well-founded decisions across multiple instances and diverse domains by relying on domain-relevant decision factors. The study introduces Structured Tabular Decision Simulations (STaDS), a new benchmark suite designed to evaluate LLMs as if they were professionals undergoing structured decision-making exams. STaDS defines understanding as the ability to identify and use the correct decision factors, which directly influence outcomes within a domain. This framework assesses understanding through three components: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) correct reliance on decision factors. Testing 9 state-of-the-art LLMs over 15 diverse decision-making settings revealed that most models struggle to maintain consistently high accuracy across domains. Moreover, models sometimes produce accurate answers without globally faithful reasoning, showing frequent mismatches between their stated rationales and the underlying decision factors driving their predictions. These insights emphasize the necessity for evaluation protocols that measure global-level understanding and suggest moving beyond accuracy-based metrics to develop frameworks that genuinely enhance LLMs’ decision-making understanding. <div>
arXiv:2511.10667v1 Announce Type: new 
Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI</title>
<link>https://arxiv.org/abs/2511.10669</link>
<guid>https://arxiv.org/abs/2511.10669</guid>
<content:encoded><![CDATA[
<div> Keywords: cochlear implants, sensorineural hearing loss, deep transfer learning, language development prediction, machine learning<br /><br />Summary:<br /><br />This study investigates the prediction of spoken language outcomes in children with severe-to-profound bilateral sensorineural hearing loss (SNHL) who receive cochlear implants (CI). Traditional predictors such as age at implantation and residual hearing fail to reliably forecast language development variability among individuals. Researchers compared traditional machine learning (ML) techniques with deep transfer learning (DTL) algorithms that incorporate brain neuroanatomic features to classify children as high or low improvers in post-CI spoken language development. The study involved 278 implanted children from three centers. The DTL models utilized a bilinear attention-based fusion strategy, achieving an accuracy of 92.39%, sensitivity of 91.22%, specificity of 93.56%, and an area under the curve (AUC) of 0.977, significantly outperforming traditional ML models on all metrics. The superior performance of DTL models is attributed to their ability to directly capture discriminative and task-specific information through representation learning. These findings demonstrate the feasibility of using a single DTL prediction model to support language outcome predictions across CI programs globally, offering a promising tool to improve clinical decision-making and individualized intervention planning for children undergoing cochlear implantation. <div>
arXiv:2511.10669v1 Announce Type: new 
Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment</title>
<link>https://arxiv.org/abs/2511.10670</link>
<guid>https://arxiv.org/abs/2511.10670</guid>
<content:encoded><![CDATA[
<div> Code-switching, speech translation, mixture of experts, large language models, multi-stage training<br /><br />Summary:<br /><br />This paper addresses the challenges of code-switching (CS) speech translation, which involves translating speech containing multiple languages into a single target language. The difficulties stem from complex semantic modeling and a lack of CS training data. To overcome these, the authors propose enhancing large language models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert focuses on the semantic subspace of a particular language for fine-grained feature modeling. They introduce a multi-stage training approach leveraging readily available monolingual automatic speech recognition (ASR) and monolingual speech translation (ST) datasets to improve speech-text alignment and translation ability. The training uses a combination of language-specific loss and intra-group load balancing loss, guiding the MoE projector to allocate tokens efficiently among experts both across and within groups. To handle data distribution shifts across training stages and improve adaptation to code-switching scenarios, a transition loss is applied to smooth transitions between datasets. Experiments on standard datasets demonstrate that the method effectively improves CS speech translation and generalizes well across different settings. This work provides a novel and efficient solution to semantic modeling and data scarcity problems in code-switching speech translation. <div>
arXiv:2511.10670v1 Announce Type: new 
Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency</title>
<link>https://arxiv.org/abs/2511.10671</link>
<guid>https://arxiv.org/abs/2511.10671</guid>
<content:encoded><![CDATA[
<div> Visual Hallucination, Multimodal Large Language Models, Grounded Visual Factualization, Factual Consistency Loss, Data Augmentation<br /><br />Summary:  
This paper addresses the problem of visual hallucination in Multimodal Large Language Models (MLLMs), where models generate details that are inconsistent with the actual image content, reducing their reliability. To tackle this, the authors propose Grounded Visual Factualization (GVF) Finetuning, a novel method designed to systematically improve visual factual consistency in MLLMs. GVF Finetuning integrates explicit factual information through three main strategies: (1) Factual Anchor Data Augmentation, which enhances training data by including structured factual anchors and counter-factual prompts to guide the model; (2) Fact-Aware Instruction Tuning, where explicit factual cues are embedded into the training instructions to strengthen the model’s grounding; and (3) A Factual Consistency Loss function that specifically penalizes the model for generating factually inaccurate information. The method was tested on LLaVA-1.5-13B and demonstrated significant improvement over standard fine-tuning methods on the VHTest benchmark in both open-ended and yes/no question formats, indicating better visual factual accuracy. Additionally, GVF Finetuning maintained or slightly enhanced performance on general multimodal benchmarks such as MME and POPE, showing that it reduces hallucinations without sacrificing the model’s broader understanding and reasoning capabilities. <div>
arXiv:2511.10671v1 Announce Type: new 
Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models in materials science and the need for open-source approaches</title>
<link>https://arxiv.org/abs/2511.10673</link>
<guid>https://arxiv.org/abs/2511.10673</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, materials science, predictive modelling, multi-agent systems, open-source AI<br /><br />Summary:<br /><br />This review discusses the transformative impact of large language models (LLMs) on materials science, focusing on their application across the materials discovery pipeline. First, LLMs have shown great capability in mining scientific literature by extracting critical information such as synthesis conditions from vast text data, enabling more efficient knowledge retrieval. Second, they contribute to predictive modelling by learning complex structure-property relationships, which advances the understanding and design of new materials. Third, LLMs facilitate multi-agent experimental systems by coordinating agentic frameworks that integrate computational tools with laboratory automation, thereby accelerating experimental workflows. The review also emphasizes that despite much progress relying on closed-source commercial LLMs, open-source alternatives are now achieving comparable performance. These open-source models provide advantages in transparency, reproducibility, cost-effectiveness, and data privacy. Finally, as open-source models continue to improve, the authors advocate for their broader adoption to develop accessible, flexible, and community-driven AI platforms that can democratize and enhance scientific discovery in materials science. <div>
arXiv:2511.10673v1 Announce Type: new 
Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.10674</link>
<guid>https://arxiv.org/abs/2511.10674</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.10674v1

Keywords: Large Language Models, text-to-SQL, continual learning, human feedback, structured memory<br /><br />Summary:<br /><br />1. This paper addresses the challenge that Large Language Models (LLMs) face when generating SQL queries from natural language, particularly due to database-specific schemas and tacit domain knowledge.  
2. The authors propose a continual learning framework where the model improves through natural language feedback provided by humans, refining queries iteratively.  
3. Knowledge gained from feedback is distilled and stored in structured memory, allowing the agent to reuse this information in future tasks, enhancing learning efficiency.  
4. Various agent architectures are explored with differences in how they capture and retrieve past experiences, focusing on memory-augmentation.  
5. Experimental results on the BIRD benchmark’s development set demonstrate that memory-augmented agents, especially the Procedural Agent, significantly improve SQL execution accuracy and reduce errors by leveraging human-in-the-loop feedback.  
6. The study highlights how converting tacit human expertise into reusable knowledge enables more adaptive and domain-aware text-to-SQL systems that continuously improve through human interaction, paving the way for future advancements in this area. <div>
arXiv:2511.10674v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification</title>
<link>https://arxiv.org/abs/2511.10675</link>
<guid>https://arxiv.org/abs/2511.10675</guid>
<content:encoded><![CDATA[
<div> In-context learning, Demonstration selection, Label distribution, Small language model, Text classification<br /><br />Summary:<br /><br />1. This paper addresses the challenge of selecting effective in-context demonstrations for text classification tasks using large language models (LLMs), highlighting that existing methods focus mainly on semantic similarity and neglect label distribution alignment.<br /><br />2. The authors propose a novel two-stage demonstration selection approach called TopK + Label Distribution Divergence (L2D). This method employs a fine-tuned BERT-like small language model (SLM) to estimate label distributions for both test inputs and candidate demonstrations.<br /><br />3. By calculating the divergence between these label distributions, L2D selects demonstrations that are not only semantically close but also aligned in label distribution with the test inputs, aiming to enhance LLM performance in in-context learning.<br /><br />4. Extensive experiments were conducted across seven different text classification benchmarks, showing that L2D consistently outperforms previously established demonstration selection strategies.<br /><br />5. Further analysis demonstrates a positive correlation between the performance of LLMs in in-context learning and the accuracy of the small language models used to estimate label distributions, suggesting that SLM quality plays a crucial role in the effectiveness of demonstration selection. <div>
arXiv:2511.10675v1 Announce Type: new 
Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models</title>
<link>https://arxiv.org/abs/2511.10676</link>
<guid>https://arxiv.org/abs/2511.10676</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, Large Language Models, Expert Prediction, Pre-attention, Lightweight Prefetching  

<br /><br />Summary:  
This paper addresses the challenge of improving expert prefetching in Mixture-of-Experts (MoE) Large Language Models (LLMs), which activate only a subset of experts to scale models efficiently while keeping inference costs low. Traditional expert prediction methods rely on activations from the previous layer for expert selection, leading to lower accuracy and leaving the first layer unoptimized. Moreover, approaches that use complex layers or separate networks for prediction add substantial computational overhead. The authors propose a novel pre-attention expert prediction technique that leverages activations before the attention block within the same layer, using two simple linear functions and a ranking-aware loss for accurate expert ranking prediction. This method exploits the ranking-preserving nature of certain LLM functions, enabling lightweight and precise expert prefetching, including at the first layer. Experimental results demonstrate that this approach achieves significantly higher prediction accuracy—93.03% on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE—representing about a 15% absolute improvement over state-of-the-art methods. The proposed pre-attention expert routers thus offer an effective and computationally efficient solution for enhancing MoE model inference speed and accuracy. <div>
arXiv:2511.10676v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI</title>
<link>https://arxiv.org/abs/2511.10684</link>
<guid>https://arxiv.org/abs/2511.10684</guid>
<content:encoded><![CDATA[
<div> Keywords: Life Cycle Assessment, climate change, large language models, environmental impact, SpiderGen  

<br /><br />Summary:  
The paper addresses the critical issue of climate change driven by greenhouse gas emissions, with a focus on consumer products' lifecycle emissions. It emphasizes the importance of Life Cycle Assessments (LCAs) which detail the production, use, and disposal processes of products to estimate environmental impact. The authors introduce SpiderGen, a workflow that utilizes large language models (LLMs) to merge traditional LCA taxonomy and methodology with advanced reasoning and world knowledge. SpiderGen produces procedural information necessary for conducting LCAs. The system's output is evaluated against real-world LCA documents, achieving an F1-Score of 62% over 10 samples, indicating mostly accurate or minor errors. Errors are mainly due to variability in process detail and differing scope in auxiliary processes included. SpiderGen surpasses baseline methods such as chain-of-thought and one-shot prompting in performance. A notable advantage of SpiderGen is its cost and time efficiency, generating LCA data in under 10 minutes for less than $1, compared to traditional LCAs costing up to $25,000 and requiring up to 21-person days. This demonstrates significant potential for reducing effort and expense in assessing carbon footprints of consumer goods. <div>
arXiv:2511.10684v1 Announce Type: new 
Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A methodological analysis of prompt perturbations and their effect on attack success rates</title>
<link>https://arxiv.org/abs/2511.10686</link>
<guid>https://arxiv.org/abs/2511.10686</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, alignment methods, prompt attacks, Attack Success Rate, statistical analysis<br /><br />Summary:<br /><br />1. The study investigates the impact of different alignment methods on how Large Language Models (LLMs) respond to prompt attacks aimed at eliciting inappropriate content. <br /><br />2. The authors focus on three primary alignment techniques used in open-source models: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF).<br /><br />3. A systematic and statistical analysis is conducted to measure how sensitive the Attack Success Rate (ASR) is when small modifications are introduced to malicious prompts.<br /><br />4. Results reveal that even slight changes in prompts can significantly influence the ASR, making models more or less vulnerable depending on the alignment method.<br /><br />5. The study highlights that relying solely on existing attack benchmarks is insufficient for uncovering all vulnerabilities, thus emphasizing the need for more comprehensive and statistically-based evaluations of model security across different alignment strategies. <div>
arXiv:2511.10686v1 Announce Type: new 
Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling and Predicting Multi-Turn Answer Instability in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10688</link>
<guid>https://arxiv.org/abs/2511.10688</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, robustness, multi-turn prompts, Markov chains, linear probes  

<br /><br />Summary: This paper investigates the robustness of large language models (LLMs) in multi-turn interactive settings by applying simple follow-up prompts to observe how model answers change over consecutive turns. The authors find that repeated triggering of models with prompts such as "Think again" causes a significant decline in accuracy, quantifying approximately a 10% drop for Gemini 1.5 Flash across nine turns and a 7.5% drop for Claude 3.5 Haiku when combined with semantically reworded questions. To better understand accuracy dynamics over turns, the study models these changes using Markov chains, which effectively predict accuracy probabilities and allow estimation of the stationary (long-run) accuracy. This stationary accuracy is found to be roughly 8% lower than the initial accuracy for Gemini 1.5 Flash, indicating notable fragility in model performance under repeated questioning. Furthermore, the research explores the model’s hidden states and demonstrates that linear probes can predict future answer changes, suggesting potential for early intervention. The authors propose stationary accuracy as a principled robustness metric tailored for interactive LLM applications. They emphasize that addressing the exposed instability and answer volatility is crucial for reliable deployment of LLMs in high-stakes and conversational environments where consistent reasoning beyond initial correctness is imperative. <div>
arXiv:2511.10688v1 Announce Type: new 
Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data</title>
<link>https://arxiv.org/abs/2511.10689</link>
<guid>https://arxiv.org/abs/2511.10689</guid>
<content:encoded><![CDATA[
<div> Keywords: recursive prompting, gender bias, synthetic dataset generation, mitigation strategies, semantic similarity<br /><br />Summary: This study explores the dynamics of gender bias in large language models during recursive text generation across three generations. The research uses three evaluation frameworks—rule-based pattern matching, embedding-based semantic similarity, and downstream task performance—to analyze bias evolution. Experiments were conducted with three initial bias levels of 0.1, 0.3, and 0.6, revealing that bias does not simply amplify monotonically but instead moves toward an equilibrium reflecting the model’s inherent bias. Specifically, low initial bias tends to increase by about 36%, while high initial bias decreases by roughly 26% over generations. The study also evaluates four mitigation methods, highlighting contrastive augmentation, which creates gender-swapped data variants. This approach significantly reduces downstream task bias by 98.8% for low initial bias scenarios and achieves an average reduction of 91% overall. Interestingly, contrastive augmentation yields higher bias scores when measured by embedding-based semantic similarity metrics, indicating a disconnect between these metrics and actual behavioral fairness outcomes. These findings emphasize the necessity for multidimensional evaluation frameworks in responsible synthetic data generation to capture nuanced bias behavior and ensure effective fairness mitigation strategies in language models. <div>
arXiv:2511.10689v1 Announce Type: new 
Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games</title>
<link>https://arxiv.org/abs/2511.10690</link>
<guid>https://arxiv.org/abs/2511.10690</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal systems, hidden language, preference bias, telephone game, concept connections  

<br /><br />Summary:  
This paper investigates the hidden language of closed-source multimodal systems by analyzing their inherent preference bias when compressing input images into texts and reconstructing them back into images. During this process, the systems introduce shifts in output that disrupt original input concept co-occurrence, revealing underlying biases. To explore these biases, the authors employ a multi-round "telephone game" method, which involves iterative transformations to observe how concept co-occurrence frequencies evolve, thereby quantitatively measuring concept connection strength in the systems' understanding. The study introduces Telescope, a large dataset with over 10,000 concept pairs designed to support the telephone game framework. By running multiple rounds of the telephone game, the approach is scalable at test time and enables the construction of a global map of concept connections. This map helps identify training-inherited preference biases, track progress in generalization capabilities, and reveal more stable linkages among fragile concept pairs. Furthermore, the researchers leverage Reasoning Large Language Models (Reasoning-LLMs) to detect unexpected relationships between concepts that transcend simple textual or visual similarity, offering insights into how multimodal systems internally simulate and comprehend the world. Overall, the work sheds new light on interpretability and control in multimodal AI systems, laying groundwork for future research. <div>
arXiv:2511.10690v1 Announce Type: new 
Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models</title>
<link>https://arxiv.org/abs/2511.10691</link>
<guid>https://arxiv.org/abs/2511.10691</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, dynamic evaluation, adversarial environment, resource constraints, benchmark contamination<br /><br />Summary:<br /><br />1. Existing benchmarks for large language models (LLMs) struggle to keep up with rapid model development, and potential data contamination challenges their reliability, as models may have seen test questions during training.<br />2. Most benchmarks assume benign, resource-rich conditions, overlooking how LLMs perform under constrained or adversarial settings.<br />3. The paper introduces Squid Game: a novel, dynamic, adversarial evaluation environment designed to test LLMs under resource limitations and asymmetric information through interactive gameplay against other LLMs.<br />4. Squid Game features six elimination-style levels focusing on diverse abilities including instruction-following, coding, reasoning, planning, and safety alignment.<br />5. Evaluations of over 50 LLMs reveal a generational phase transition in performance within the same model lineage, and evidence that some models exploit speculative shortcuts to win, indicating risks of higher-level contamination in static benchmarks.<br />6. Correlation analyses comparing Squid Game with existing benchmarks highlight that dynamic, interactive evaluation offers complementary insights to static tests.<br />7. The paper pledges to publicly release the code and data, enabling further research in dynamic behavioral evaluation of general LLMs. <div>
arXiv:2511.10691v1 Announce Type: new 
Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate</title>
<link>https://arxiv.org/abs/2511.10693</link>
<guid>https://arxiv.org/abs/2511.10693</guid>
<content:encoded><![CDATA[
<div> Keywords: voice-based AI, politeness, speech rate reduction, text-to-speech, social conventions<br /><br />Summary:<br />1. This study examines whether advanced text-to-speech (TTS) systems can implicitly learn to convey politeness, a subtle human social cue, through changes in speech rate. <br />2. Researchers tested 22 synthetic voices from two popular AI platforms, AI Studio and OpenAI, by having them read the same script under "polite and formal" and "casual and informal" conditions. <br />3. The main measurement was speech duration, assessing if polite prompts led to slower speech compared to casual ones. <br />4. Results showed that all AI voices produced significantly slower speech in the polite condition, with very large effect sizes across both platforms. This was statistically significant for all AI Studio voices and most OpenAI voices. <br />5. The findings indicate that voice-based AI can internalize and reproduce nuanced psychological markers of human communication, such as politeness through speech rate modulation, supporting the view of AI as emerging social actors capable of reinforcing social norms. <div>
arXiv:2511.10693v1 Announce Type: new 
Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where does an LLM begin computing an instruction?</title>
<link>https://arxiv.org/abs/2511.10694</link>
<guid>https://arxiv.org/abs/2511.10694</guid>
<content:encoded><![CDATA[
<div> Keywords: instruction following, activation patching, Llama models, layer-wise flip rate, onset

<br /><br />Summary:  
This paper investigates the process of instruction following within language models by examining where along a model’s layer stack the transition from reading an instruction to executing it occurs. The authors introduce three simple datasets—Key-Value, Quote Attribution, and Letter Selection—and combine them in two-hop compositions to analyze complex task execution. They apply activation patching on minimal-contrast prompt pairs to measure a layer-wise flip rate, which quantifies how substituting specific residual activations affects the predicted answer at different layers. Across various models in the Llama family, the study identifies a distinct inflection point called "onset," marking where interventions in earlier layers meaningfully alter outcomes but become largely ineffective beyond this point. Notably, multi-hop task compositions show a similar onset location, suggesting consistency in where instruction following initiates regardless of task complexity. The work presents a straightforward and reproducible method to pinpoint the starting layer of instruction following, enabling comparison across different tasks and model sizes, thereby contributing to a better understanding of how language models internally process instructions and execute tasks. <div>
arXiv:2511.10694v1 Announce Type: new 
Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations</title>
<link>https://arxiv.org/abs/2511.10695</link>
<guid>https://arxiv.org/abs/2511.10695</guid>
<content:encoded><![CDATA[
<div> nation-level bias, Large Language Models, International Relations, UNSC, debiasing framework<br /><br />Summary:<br /><br />This paper investigates nation-level biases in Large Language Models (LLMs) focusing on their applications in International Relations (IR). Using historical data from the United Nations Security Council (UNSC), the authors create a bias evaluation framework with three tests aimed at detecting biases toward the five permanent UNSC members. Results reveal that while common bias trends exist, such as favoring Western nations and disfavoring Russia, the bias patterns differ across LLMs. Furthermore, biases within a single model vary in direction and strength depending on the evaluation context, indicating that LLM biases are multidimensional and task-dependent. The study also finds that models exhibiting stronger reasoning capabilities tend to have reduced bias and improved performance. To address these biases, the authors propose a debiasing framework combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experimentation shows this approach successfully decreases nation-level bias and enhances performance, particularly in models like GPT-4o-mini and LLama-3.3-70B. The paper concludes by highlighting the importance of simultaneously evaluating nation-level bias and model performance when deploying LLMs in the IR domain, stressing that addressing bias is critical for fair and accurate use in this sensitive context. <div>
arXiv:2511.10695v1 Announce Type: new 
Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\pi$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling</title>
<link>https://arxiv.org/abs/2511.10696</link>
<guid>https://arxiv.org/abs/2511.10696</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, Sparse Attention, \PiAttention, Long-Range Modeling, Adaptive Fusion Gate<br /><br />Summary:  
Transformers have fundamentally advanced natural language processing but face efficiency challenges due to their quadratic complexity relative to sequence length. Existing sparse attention mechanisms like RingAttention address this by limiting attention to local neighborhoods, which reduces computational cost but restricts the receptive field and adaptability. The proposed \PiAttention model introduces a periodic sparse Transformer architecture that decomposes attention into three components: ring-local neighborhoods, deterministic \(\pi\)-stride skips, and an adaptive fusion gate. This design ensures predictable coverage over distant tokens while maintaining a sparse attention pattern with per-layer complexity linear in the input sequence length. Theoretical analysis shows that \(\PiAttention\) achieves a receptive field growth rate of \(\mathcal{O}(kL + \pi \log L)\), improving upon the \(\mathcal{O}(kL)\) rate of RingAttention, where \(k\) is the window size, \(\pi\) is the skip period, and \(L\) is the sequence length. Experimentally, \(\PiAttention\) performs at or above the quality of dense attention models on tasks including language modeling, retrieval, and vision-language applications, achieving an 8.3% lower perplexity than RingAttention while using half the GPU resources for the same context length. Ablations and visualizations highlight the crucial roles of periodic skips, adaptive fusion, and coordinated sparsity at the attention head level in enabling efficient modeling of long contexts. <div>
arXiv:2511.10696v1 Announce Type: new 
Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $\pi$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + \pi \log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $\pi$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs</title>
<link>https://arxiv.org/abs/2511.10768</link>
<guid>https://arxiv.org/abs/2511.10768</guid>
<content:encoded><![CDATA[
<div> Keywords: medical text summarization, faithfulness, large language models, consumer health questions, LLaMA-2-7B<br /><br />Summary: The article addresses the challenge of generating faithful summaries of consumer health questions (CHQs), which is crucial for effective healthcare communication. It proposes a novel framework that integrates TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to improve faithfulness in medical summarization. The authors fine-tuned the LLaMA-2-7B model on two datasets, MeQSum (English) and BanglaCHQ-Summ (Bangla), and demonstrated consistent improvements in both quality metrics (ROUGE, BERTScore, readability) and faithfulness metrics (SummaC, AlignScore). The model outperformed zero-shot baselines and prior systems, highlighting the benefits of domain fine-tuning and combining extraction with entity recognition techniques. Human evaluation confirmed that over 80% of generated summaries retained critical medical information, underscoring the approach’s reliability. The work emphasizes faithfulness as an essential dimension for trustworthy medical summarization and shows the potential for safer deployment of LLMs in healthcare applications, ultimately aiming to reduce risks posed by unfaithful summaries that could mislead patients or healthcare providers. <div>
arXiv:2511.10768v1 Announce Type: new 
Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English</title>
<link>https://arxiv.org/abs/2511.10780</link>
<guid>https://arxiv.org/abs/2511.10780</guid>
<content:encoded><![CDATA[
<div> Tunisian Arabic, Speech Translation, Code-Switching, Dataset, TEDxTN<br /><br />Summary:<br /><br />1. The paper introduces TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset.<br /><br />2. The dataset was created to address the scarcity of resources for various Arabic dialects, specifically the Tunisian dialect.<br /><br />3. It comprises 108 TEDx talks totaling 25 hours of speech featuring code-switching and speakers from over 11 different Tunisian regions with diverse accents.<br /><br />4. The authors developed internal annotation guidelines and provide both these guidelines and the corpus to the public, allowing future expansions as new talks become available.<br /><br />5. Baseline results for Speech Recognition and Speech Translation are reported using multiple pre-trained and fine-tuned end-to-end models.<br /><br />6. TEDxTN stands as the first open-source, publicly accessible speech translation corpus capturing code-switching phenomena in the Tunisian dialect.<br /><br />7. This resource aims to motivate and facilitate further research in natural language processing for the Tunisian Arabic dialect. <div>
arXiv:2511.10780v1 Announce Type: new 
Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sabi\'a: Um Chatbot de Intelig\^encia Artificial Generativa para Suporte no Dia a Dia do Ensino Superior</title>
<link>https://arxiv.org/abs/2511.10787</link>
<guid>https://arxiv.org/abs/2511.10787</guid>
<content:encoded><![CDATA[
<div> Keywords: chatbot, Generative AI, Retrieval-Augmented Generation, Gemini 2.0 Flash, Gemma 3n

<br /><br />Summary:  
Students often face challenges in accessing day-to-day academic information because it is scattered across multiple institutional documents and websites, leading to confusion and a lack of clarity about routine university matters. To address this problem, the project proposes the development of a chatbot that utilizes Generative Artificial Intelligence (GenAI) combined with Retrieval-Augmented Generation (RAG) techniques to streamline and simplify access to academic information. Various GenAI models were tested and evaluated using quality metrics and the LLM-as-a-Judge evaluation approach to determine their effectiveness. Among the tested models, Gemini 2.0 Flash was identified as a top performer due to its superior quality and speed, making it highly efficient for this application. In addition, Gemma 3n was noted for having good performance while being open-source, presenting an advantage for adaptability and transparency. The project’s approach demonstrates promising potential to reduce information fragmentation and improve students’ experience by providing a centralized, interactive platform for accessing university-related information. <div>
arXiv:2511.10787v1 Announce Type: new 
Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation</title>
<link>https://arxiv.org/abs/2511.10819</link>
<guid>https://arxiv.org/abs/2511.10819</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, GPT-4o, automated grading, educational assessment, Computational Linguistics<br /><br />Summary:  
This study explores the use of a Large Language Model (LLM), specifically GPT-4o, for grading short-answer quizzes and project reports in a real undergraduate Computational Linguistics course. Data were collected from around 50 students over five quizzes, along with project reports submitted by 14 teams. The LLM-generated scores were compared with independent human evaluations conducted by the course teaching assistants (TAs). Results demonstrated a strong positive correlation between GPT-4o and human graders, reaching up to 0.98 for quiz assessments. The LLM achieved exact score agreement with human grading in 55% of quiz cases, indicating high reliability for structured responses. For project reports, while GPT-4o also aligned well overall with human assessments, there was some inconsistency in grading technical, open-ended answers, reflecting challenges in nuanced evaluation. The authors have released all code and sample data used in the study to encourage further research in the application of LLMs for automated grading. This work underscores both the promising potential and existing limitations of leveraging LLMs as tools for educational assessment in authentic academic environments. Ultimately, the study contributes valuable insights to the development of automated grading systems tailored to real-world classroom settings. <div>
arXiv:2511.10819v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders</title>
<link>https://arxiv.org/abs/2511.10840</link>
<guid>https://arxiv.org/abs/2511.10840</guid>
<content:encoded><![CDATA[
<div> Multilingual, Large Language Models, Pivot Language, Decoding, Attribution Analysis<br /><br />Summary: This paper investigates how multilingual large language models (LLMs) internally represent multiple languages and why performance often favors the dominant training language. The authors train several LLMs on different multilingual data mixtures and analyze their internal workings using cross-layer transcoders (CLT) and attribution graphs. They find strong evidence for a pivot language representation mechanism, where the model creates nearly identical representations across languages but performs language-specific decoding in later layers. Attribution analyses indicate that decoding depends partly on a small set of high-frequency language features in the final layers, which allow the model to linearly identify language identity from the earlier layers. By intervening on these features, the researchers can suppress one language and substitute another in the model’s output, demonstrating control over multilingual generation. They also explore how the dominant training language affects these internal mechanisms through attribution graphs and decoding pathways. The study emphasizes that understanding this pivot-language mechanism is vital for enhancing multilingual alignment and improving LLM performance across diverse languages. <div>
arXiv:2511.10840v1 Announce Type: new 
Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English</title>
<link>https://arxiv.org/abs/2511.10846</link>
<guid>https://arxiv.org/abs/2511.10846</guid>
<content:encoded><![CDATA[
<div> Automated emotion detection, African American Vernacular English (AAVE), racial bias, emotion AI, culturally informed models  

<br /><br />Summary:  
1. Automated emotion detection models are widely used across diverse domains but often rely on annotations reflecting dominant cultural norms, limiting their effectiveness in recognizing emotions in dialects such as African American Vernacular English (AAVE).  
2. This study analyzes 2.7 million geo-tagged tweets from Los Angeles, assessing the extent of AAVE usage through computational approximations of dialectal features.  
3. A dataset of 875 tweets with varying AAVE density was annotated for emotion presence and intensity, with "silver" labels generated by African American, AAVE-fluent annotators to provide culturally informed ground truth.  
4. Popular emotion recognition models like GPT, BERT-based models, and SpanEmo demonstrate substantially higher false positive rates of anger detection on AAVE tweets compared to General American English (GAE), with rates more than doubling in some cases.  
5. Linear regression analyses show models and non-ingroup annotators correlate more with profanity-based AAVE features than ingroup annotators, highlighting bias.  
6. There is a measurable association between neighborhoods with higher African American populations and model predictions of increased anger and decreased joy, indicating reinforcement of racial stereotypes by emotion AI.  
7. The study underscores a critical safety concern in affective computing and calls for developing culturally and dialect-informed emotion detection systems to mitigate biased outcomes. <div>
arXiv:2511.10846v1 Announce Type: new 
Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs</title>
<link>https://arxiv.org/abs/2511.10850</link>
<guid>https://arxiv.org/abs/2511.10850</guid>
<content:encoded><![CDATA[
<div> Task arithmetic, parameter space alignment, Large Language Models, Grouped-Query Attention, SwiGLU<br /><br />Summary:<br /><br />1. The paper addresses the challenge of negative interference in task arithmetic, which hinders the effective transfer of skills between Large Language Models (LLMs) that have diverged during training.<br />2. The authors propose a novel approach that aligns the parameter spaces of different LLMs by leveraging the intrinsic permutation, rotation, and scaling symmetries present in Transformer architectures.<br />3. This alignment technique is adapted specifically for modern architectural components, including Grouped-Query Attention (GQA) and SwiGLU layers, using both weight-based and activation-based strategies.<br />4. By applying this alignment-first strategy, the study successfully transfers advanced reasoning capabilities to a model originally lacking reasoning skills.<br />5. Experimental results on challenging reasoning benchmarks demonstrate that the proposed method consistently outperforms traditional task arithmetic, offering a more effective way to merge and transfer specialized skills across evolving LLM families, which reduces redundant fine-tuning and enhances overall model adaptability. <div>
arXiv:2511.10850v1 Announce Type: new 
Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems</title>
<link>https://arxiv.org/abs/2511.10871</link>
<guid>https://arxiv.org/abs/2511.10871</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM judgment, conversational framing, model conviction, social interactions, rebuttal perturbation<br /><br />Summary:<br /><br />1. The paper investigates how large language models (LLMs) perform when acting as judges in tasks requiring social or conversational judgment, shifting from direct factual queries to conversational contexts. <br /><br />2. It introduces an evaluation framework that contrasts model assessments of factual correctness against judgments of speaker correctness within minimal dialogues, effectively reframing the question from "Is this statement correct?" to "Is this speaker correct?". <br /><br />3. The study applies conversational pressure through a simple rebuttal phrase ("The previous answer is incorrect.") to evaluate the firmness of the model’s convictions across both direct and conversational scenarios. <br /><br />4. Results reveal varied responses among models: GPT-4o-mini displayed sycophantic tendencies under social framing, while Llama-8B-Instruct exhibited overly critical behavior, signaling that conversational context can significantly influence model judgment. <br /><br />5. On average, there was a 9.24% performance change across models, highlighting the importance of conversational framing as a key factor in LLM evaluations and proposing a reproducible method for assessing model conviction to advance trustworthy dialogue systems. <div>
arXiv:2511.10871v1 Announce Type: new 
Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICX360: In-Context eXplainability 360 Toolkit</title>
<link>https://arxiv.org/abs/2511.10879</link>
<guid>https://arxiv.org/abs/2511.10879</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Explainability, In-Context Explainability 360, Black-box methods, White-box methods<br /><br />Summary: Large Language Models (LLMs) are increasingly integrated into various high-stakes applications such as summarizing meetings and assisting medical professionals. To enhance trust and transparency, it is essential to develop tools that explain LLM outputs, including responses, summaries, and lists. Addressing this need, the paper introduces In-Context Explainability 360 (ICX360), an open-source Python toolkit designed to explain LLM outputs by focusing on the user-provided context or prompts. ICX360 implements three recent explanation tools that use both black-box (perturbation-based) and white-box (gradient-based) methods to analyze LLM behavior. The toolkit aims to be accessible, providing quick-start guides and thorough tutorials to support diverse use cases such as retrieval-augmented generation, natural language generation, and even jailbreaking techniques. Hosted on GitHub by IBM, ICX360 intends to facilitate researchers and developers in better understanding and interpreting LLM decisions through contextual explanations, fostering safer and more interpretable AI applications. <div>
arXiv:2511.10879v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge</title>
<link>https://arxiv.org/abs/2511.10881</link>
<guid>https://arxiv.org/abs/2511.10881</guid>
<content:encoded><![CDATA[
<div> Negative bias, large language models, prompt format, parametric knowledge, mitigation<br /><br /><br />Summary: This paper investigates the phenomenon of negative bias in large language models (LLMs), where models tend to produce excessive negative responses in binary decision tasks such as yes-no questions. It reveals that this bias is influenced more by the prompt format than by the semantic content of negative responses, highlighting a format-level negative bias. To study this in detail, the authors propose a novel evaluation pipeline that categorizes model responses into three subsets based on the model's parametric knowledge: correct knowledge, incorrect knowledge, and insufficient relevant knowledge. Their analysis discovers a shortcut behavior where LLMs default to negative answers when they lack sufficient knowledge to respond accurately, thus contributing to negative bias. The study further explores how different prompting scenarios affect negative bias, showing that the inclusion of relevant contextual information and offering an "I don't know" response option tend to reduce the bias. Conversely, using chain-of-thought prompting often increases the tendency towards negative bias. Lastly, the research demonstrates that the degree and direction of negative bias vary depending on the prompt type. These insights provide a better understanding of negative bias and suggest strategies for mitigating it in LLMs. <div>
arXiv:2511.10881v1 Announce Type: new 
Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking</title>
<link>https://arxiv.org/abs/2511.10887</link>
<guid>https://arxiv.org/abs/2511.10887</guid>
<content:encoded><![CDATA[
<div> Biomedical NER, Entity Linking, UMLS, Ontological Paths, Explainable Models<br /><br />Summary: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is limited by a fragmented data landscape, insufficient resources for building explainable models, and the shortcomings of semantically unaware evaluation metrics. To overcome these challenges, the authors introduce MedPath, a comprehensive large-scale and multi-domain biomedical EL dataset. MedPath integrates and builds upon nine existing expert-annotated EL datasets, providing a unified resource for research. In MedPath, all entities are normalized using the most recent version of the Unified Medical Language System (UMLS), ensuring standardization across datasets. Additionally, the dataset includes augmentations with mappings to 62 other biomedical vocabularies, greatly improving interoperability. Crucially, MedPath enriches entities with complete ontological paths, spanning from general to specific categories, across up to 11 biomedical vocabularies. This unique feature facilitates semantic-rich and interpretable EL system development by providing hierarchical context often missing in previous datasets. Overall, MedPath enables new avenues for research in biomedical natural language processing (NLP), supporting advancements in interoperable, explainable, and clinically relevant NLP models by providing richer semantic information and standardized entity linking benchmarks. <div>
arXiv:2511.10887v1 Announce Type: new 
Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10899</link>
<guid>https://arxiv.org/abs/2511.10899</guid>
<content:encoded><![CDATA[
<div> Keywords: Tool-Induced Myopia, Tool-augmented Language Models, Code Interpreter, reasoning degradation, PYMATH<br /><br />Summary:<br /><br />1. Tool-augmented Language Models (TaLMs) enhance problem-solving by invoking external tools but may produce correct answers without coherent reasoning, a failure mode termed Tool-Induced Myopia (TIM).<br />2. The study focuses on the Code Interpreter tool and uses PYMATH, a benchmark of 1,679 competition-level math problems where Python coding aids but does not fully solve.<br />3. A novel multi-dimensional evaluation suite quantifies reasoning quality, revealing that despite up to 19.3 percentage points improvement in final-answer accuracy, TaLMs show consistent reasoning degradation compared to non-tool models.<br />4. Increased frequency of tool use correlates with greater reasoning incoherence; TaLM errors shift from arithmetic mistakes to broader reasoning failures involving logic, assumptions, and creativity, with TIM present in approximately 55% of high-risk cases.<br />5. To address TIM, the authors propose a preference-optimization framework that realigns TaLMs to treat tools as assistive evidence, improving both accuracy and reasoning depth under tool use.<br /><br />Code and data for this study are publicly available at https://github.com/megagonlabs/TIM. <div>
arXiv:2511.10899v1 Announce Type: new 
Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering</title>
<link>https://arxiv.org/abs/2511.10900</link>
<guid>https://arxiv.org/abs/2511.10900</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical Question Answering, EMSQA Dataset, Expert-CoT, ExpertRAG<br /><br />Summary:<br /><br />1. Large language models (LLMs) have demonstrated potential in medical question answering but often lack integration of domain-specific expertise, such as clinical subject areas (e.g., trauma, airway) and certification levels (e.g., EMT, Paramedic), which are crucial in professional settings.<br /><br />2. Existing methods typically use general-purpose prompting or retrieval without leveraging structured medical context, limiting their effectiveness in critical, high-stakes environments.<br /><br />3. To address this, the EMSQA dataset was developed, comprising 24,300 multiple-choice questions spanning 10 clinical subject areas and 4 certification levels, supported by a curated knowledge base of 40,000 documents totaling 2 million tokens, aligned to relevant subject areas.<br /><br />4. The study introduces two novel approaches: Expert-CoT, a chain-of-thought prompting strategy conditioned on specific clinical subjects and certification levels, and ExpertRAG, a retrieval-augmented generation pipeline that grounds answers in subject-aligned documents and real patient data.<br /><br />5. Experimental results across 4 LLMs show Expert-CoT improves accuracy by up to 2.05% over standard CoT prompting, while combining Expert-CoT with ExpertRAG achieves up to a 4.59% accuracy gain over standard RAG baselines. Remarkably, 32-billion-parameter expertise-augmented LLMs successfully passed all computer-adaptive EMS certification simulation exams. <div>
arXiv:2511.10900v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions</title>
<link>https://arxiv.org/abs/2511.10902</link>
<guid>https://arxiv.org/abs/2511.10902</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal peer review, large language models, retrieval-augmented generation, academic workflows, actionable feedback  

<br /><br />Summary: This paper introduces an innovative web-based system designed to enhance academic peer review by integrating multimodal inputs, including both textual and visual data, leveraging the capabilities of large language models (LLMs). Unlike traditional peer review systems constrained to text-only inputs, the proposed framework utilizes multimodal LLMs to provide richer, context-aware feedback. To improve review quality and grounding, the system incorporates retrieval-augmented generation (RAG) based on extensive OpenReview datasets, ensuring that the generated reviews reflect community standards and relevant prior work. A notable feature is the conversion of generated reviews into structured, actionable to-do lists using the novel Action:Objective[#] format, enabling authors to clearly understand and track revision tasks. The platform offers seamless integration with existing academic writing tools, facilitating real-time, interactive feedback and revision monitoring. Experimental evaluations demonstrate that the system produces more comprehensive and expert-aligned review comments compared to baseline models lacking multimodal or retrieval enhancements. This approach advances scholarly assistance by fostering transparent, human-centered review simulations that better support manuscript improvement before submission. Overall, the work addresses key limitations in current peer review automation by combining multimodality, community-context grounding, and actionable guidance into a unified system. <div>
arXiv:2511.10902v1 Announce Type: new 
Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy</title>
<link>https://arxiv.org/abs/2511.10903</link>
<guid>https://arxiv.org/abs/2511.10903</guid>
<content:encoded><![CDATA[
<div> Keywords: Bloom's Taxonomy, exam question classification, machine learning, data augmentation, large language models<br /><br />Summary:<br /><br />This paper investigates the automatic classification of exam questions and learning outcomes according to the six cognitive categories of Bloom's Taxonomy: Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation. A small labeled dataset of 600 sentences was used to train and evaluate multiple types of models, including traditional machine learning (Naive Bayes, Logistic Regression, SVM), recurrent neural networks (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT, RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Different preprocessing and data augmentation techniques were applied to improve performance, such as synonym replacement and word embeddings. Among traditional models, Support Vector Machines with data augmentation delivered the best results, achieving approximately 94% accuracy, recall, and F1 scores while exhibiting minimal overfitting. In contrast, RNN-based models and BERT showed significant overfitting issues, although RoBERTa initially resisted overfitting but deteriorated over longer training. Zero-shot evaluations of large language models revealed that OpenAI and Gemini outperformed their peers with about 72-73% accuracy and comparable F1 scores, despite not being fine-tuned on the dataset. The study highlights the difficulties of training complex deep learning models on limited data and stresses the effectiveness of careful augmentation and simpler algorithms for Bloom's Taxonomy classification tasks. <div>
arXiv:2511.10903v1 Announce Type: new 
Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D</title>
<link>https://arxiv.org/abs/2511.10912</link>
<guid>https://arxiv.org/abs/2511.10912</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, rare disease diagnosis, narrative medical cases, House M.D. dataset, AI-assisted diagnosis

<br /><br />Summary:  
This study addresses the underexplored ability of large language models (LLMs) to diagnose rare diseases from narrative medical case descriptions. It introduces a novel dataset comprising 176 symptom-diagnosis pairs extracted from the TV series House M.D., which is validated as an educational tool for rare disease recognition. Four state-of-the-art LLMs—GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro—were evaluated using this dataset on diagnostic reasoning tasks based on narrative cases. The results showed considerable variation in model performance, with accuracy rates ranging from 16.48% to 38.64%. Notably, newer LLM generations achieved about 2.3 times better accuracy compared to earlier versions. Despite the overall challenges these models face in diagnosing rare diseases, the observed improvements across model architectures point to promising avenues for future development in AI-assisted diagnosis. The study establishes baseline performance metrics via an educationally validated benchmark and provides a publicly accessible evaluation framework aimed at advancing AI research in medical narrative diagnostic reasoning. <div>
arXiv:2511.10912v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology</title>
<link>https://arxiv.org/abs/2511.10930</link>
<guid>https://arxiv.org/abs/2511.10930</guid>
<content:encoded><![CDATA[
<div> Keywords: CardioEmbed, cardiology embeddings, contrastive learning, InfoNCE loss, semantic retrieval

<br /><br />Summary: This study addresses the gap in biomedical text embeddings for clinical cardiology by developing CardioEmbed, a domain-specialized embedding model trained on comprehensive cardiology textbooks rather than research abstracts. CardioEmbed is built on Qwen3-Embedding-8B and trained using contrastive learning with InfoNCE loss and in-batch negatives on a curated corpus of approximately 150,000 deduplicated sentences from seven authoritative cardiology textbooks. The model demonstrates a significant improvement by achieving 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, surpassing the current state-of-the-art medical embedding model, MedTE, by 15.94 percentage points. Additionally, CardioEmbed performs competitively on broader biomedical benchmarks within the MTEB framework, obtaining a BIOSSES Spearman correlation score of 0.77 and an NDCG@10 score of 0.61 on the SciFact dataset. These results highlight the effectiveness of domain-specialized training on comprehensive clinical textbooks to yield near-perfect cardiology semantic retrieval performance and robust generalizability to related biomedical domains. The study underscores the importance of leveraging domain-relevant clinical knowledge sources, such as textbooks, for improved embedding models in specialized medical fields like cardiology. <div>
arXiv:2511.10930v1 Announce Type: new 
Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains</title>
<link>https://arxiv.org/abs/2511.10984</link>
<guid>https://arxiv.org/abs/2511.10984</guid>
<content:encoded><![CDATA[
<div> Keywords: discourse-level translation, expert domains, Chinese-English translation, evaluation benchmark, Metric-S<br /><br />Summary:<br /><br />1. The paper identifies a critical gap in the evaluation of discourse-level translation within expert domains, highlighting that current methods mainly assess segment-level accuracy and fluency rather than discourse coherence and terminological precision. 2. To address this shortcoming, the authors introduce DiscoX, a novel benchmark consisting of 200 professionally curated, long-form texts (average length over 1700 tokens) from seven specialized domains, designed specifically for Chinese-English translation tasks. 3. Alongside DiscoX, the authors develop Metric-S, a reference-free automatic evaluation system that delivers fine-grained assessments across accuracy, fluency, and appropriateness, showing strong correlation with human judgments and outperforming existing metrics. 4. Experimental results reveal a significant performance gap between state-of-the-art large language models (LLMs) and human experts, underscoring the complexity and challenge of professional-grade discourse-level translation in expert domains. 5. The introduction of DiscoX and Metric-S provides a robust framework for more rigorous, discourse-aware evaluation, thereby facilitating future progress in machine translation driven by LLMs and addressing the needs of cross-lingual scholarly communication and knowledge dissemination. <div>
arXiv:2511.10984v1 Announce Type: new 
Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets</title>
<link>https://arxiv.org/abs/2511.10985</link>
<guid>https://arxiv.org/abs/2511.10985</guid>
<content:encoded><![CDATA[
arXiv:2511.10985v1 Announce Type: new 
Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Based Steering of Large Language Models for Diverse Structured Generation</title>
<link>https://arxiv.org/abs/2511.11018</link>
<guid>https://arxiv.org/abs/2511.11018</guid>
<content:encoded><![CDATA[
arXiv:2511.11018v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB</title>
<link>https://arxiv.org/abs/2511.11041</link>
<guid>https://arxiv.org/abs/2511.11041</guid>
<content:encoded><![CDATA[
arXiv:2511.11041v1 Announce Type: new 
Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + \mu$, where $\mu$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $\sigma$ on retrieval tasks, 3.1 $\sigma$ on classification tasks, and 0.8 $\sigma$ on other types of tasks. Renormalization has two variants: directly subtracting $\mu$ from $e$, or subtracting the projection of $e$ onto $\mu$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Detect Their Own Hallucinations?</title>
<link>https://arxiv.org/abs/2511.11087</link>
<guid>https://arxiv.org/abs/2511.11087</guid>
<content:encoded><![CDATA[
arXiv:2511.11087v1 Announce Type: new 
Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysing Personal Attacks in U.S. Presidential Debates</title>
<link>https://arxiv.org/abs/2511.11108</link>
<guid>https://arxiv.org/abs/2511.11108</guid>
<content:encoded><![CDATA[
arXiv:2511.11108v1 Announce Type: new 
Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AV-Dialog: Spoken Dialogue Models with Audio-Visual Input</title>
<link>https://arxiv.org/abs/2511.11124</link>
<guid>https://arxiv.org/abs/2511.11124</guid>
<content:encoded><![CDATA[
arXiv:2511.11124v1 Announce Type: new 
Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion</title>
<link>https://arxiv.org/abs/2511.11126</link>
<guid>https://arxiv.org/abs/2511.11126</guid>
<content:encoded><![CDATA[
arXiv:2511.11126v1 Announce Type: new 
Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2511.11139</link>
<guid>https://arxiv.org/abs/2511.11139</guid>
<content:encoded><![CDATA[
arXiv:2511.11139v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases</title>
<link>https://arxiv.org/abs/2511.11141</link>
<guid>https://arxiv.org/abs/2511.11141</guid>
<content:encoded><![CDATA[
arXiv:2511.11141v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy</title>
<link>https://arxiv.org/abs/2511.11214</link>
<guid>https://arxiv.org/abs/2511.11214</guid>
<content:encoded><![CDATA[
arXiv:2511.11214v1 Announce Type: new 
Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2511.11234</link>
<guid>https://arxiv.org/abs/2511.11234</guid>
<content:encoded><![CDATA[
arXiv:2511.11234v1 Announce Type: new 
Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement</title>
<link>https://arxiv.org/abs/2511.11258</link>
<guid>https://arxiv.org/abs/2511.11258</guid>
<content:encoded><![CDATA[
arXiv:2511.11258v1 Announce Type: new 
Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference</title>
<link>https://arxiv.org/abs/2511.11306</link>
<guid>https://arxiv.org/abs/2511.11306</guid>
<content:encoded><![CDATA[
arXiv:2511.11306v1 Announce Type: new 
Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity</title>
<link>https://arxiv.org/abs/2511.11309</link>
<guid>https://arxiv.org/abs/2511.11309</guid>
<content:encoded><![CDATA[
arXiv:2511.11309v1 Announce Type: new 
Abstract: Advancements in Machine Learning & Neural Networks in recent years have led to widespread implementations of Natural Language Processing across a variety of fields with remarkable success, solving a wide range of complicated problems. However, recent research has shown that machine learning models may be vulnerable in a number of ways, putting both the models and the systems theyre used in at risk. In this paper, we intend to analyze and experiment with the best of existing adversarial attack recipes and create new ones. We concentrated on developing a novel adversarial attack strategy on current state-of-the-art machine learning models by producing ambiguous inputs for the models to confound them and then constructing the path to the future development of the robustness of the models. We will develop adversarial instances with maximum perplexity, utilizing machine learning and deep learning approaches in order to trick the models. In our attack recipe, we will analyze several datasets and focus on creating obfuscous adversary examples to put the models in a state of perplexity, and by including the Bangla Language in the field of adversarial attacks. We strictly uphold utility usage reduction and efficiency throughout our work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.11315</link>
<guid>https://arxiv.org/abs/2511.11315</guid>
<content:encoded><![CDATA[
arXiv:2511.11315v1 Announce Type: new 
Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery</title>
<link>https://arxiv.org/abs/2511.11324</link>
<guid>https://arxiv.org/abs/2511.11324</guid>
<content:encoded><![CDATA[
arXiv:2511.11324v1 Announce Type: new 
Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2511.11334</link>
<guid>https://arxiv.org/abs/2511.11334</guid>
<content:encoded><![CDATA[
arXiv:2511.11334v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text</title>
<link>https://arxiv.org/abs/2511.11340</link>
<guid>https://arxiv.org/abs/2511.11340</guid>
<content:encoded><![CDATA[
arXiv:2511.11340v1 Announce Type: new 
Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studies with impossible languages falsify LMs as models of human language</title>
<link>https://arxiv.org/abs/2511.11389</link>
<guid>https://arxiv.org/abs/2511.11389</guid>
<content:encoded><![CDATA[
arXiv:2511.11389v1 Announce Type: new 
Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MajinBook: An open catalogue of digital world literature with likes</title>
<link>https://arxiv.org/abs/2511.11412</link>
<guid>https://arxiv.org/abs/2511.11412</guid>
<content:encoded><![CDATA[
arXiv:2511.11412v1 Announce Type: new 
Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Hearing Assistants that Isolate Egocentric Conversations</title>
<link>https://arxiv.org/abs/2511.11473</link>
<guid>https://arxiv.org/abs/2511.11473</guid>
<content:encoded><![CDATA[
arXiv:2511.11473v1 Announce Type: new 
Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2511.11518</link>
<guid>https://arxiv.org/abs/2511.11518</guid>
<content:encoded><![CDATA[
arXiv:2511.11518v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning</title>
<link>https://arxiv.org/abs/2511.11562</link>
<guid>https://arxiv.org/abs/2511.11562</guid>
<content:encoded><![CDATA[
arXiv:2511.11562v1 Announce Type: new 
Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents</title>
<link>https://arxiv.org/abs/2511.10687</link>
<guid>https://arxiv.org/abs/2511.10687</guid>
<content:encoded><![CDATA[
arXiv:2511.10687v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents</title>
<link>https://arxiv.org/abs/2511.10705</link>
<guid>https://arxiv.org/abs/2511.10705</guid>
<content:encoded><![CDATA[
arXiv:2511.10705v1 Announce Type: cross 
Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization</title>
<link>https://arxiv.org/abs/2511.10720</link>
<guid>https://arxiv.org/abs/2511.10720</guid>
<content:encoded><![CDATA[
arXiv:2511.10720v1 Announce Type: cross 
Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10788</link>
<guid>https://arxiv.org/abs/2511.10788</guid>
<content:encoded><![CDATA[
arXiv:2511.10788v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns</title>
<link>https://arxiv.org/abs/2511.10837</link>
<guid>https://arxiv.org/abs/2511.10837</guid>
<content:encoded><![CDATA[
arXiv:2511.10837v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation</title>
<link>https://arxiv.org/abs/2511.11066</link>
<guid>https://arxiv.org/abs/2511.11066</guid>
<content:encoded><![CDATA[
arXiv:2511.11066v1 Announce Type: cross 
Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation</title>
<link>https://arxiv.org/abs/2511.11104</link>
<guid>https://arxiv.org/abs/2511.11104</guid>
<content:encoded><![CDATA[
arXiv:2511.11104v1 Announce Type: cross 
Abstract: Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2511.11182</link>
<guid>https://arxiv.org/abs/2511.11182</guid>
<content:encoded><![CDATA[
arXiv:2511.11182v1 Announce Type: cross 
Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Meaningful Units with Visually Grounded Semantics from Image Captions</title>
<link>https://arxiv.org/abs/2511.11262</link>
<guid>https://arxiv.org/abs/2511.11262</guid>
<content:encoded><![CDATA[
arXiv:2511.11262v1 Announce Type: cross 
Abstract: Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SQuaD: The Software Quality Dataset</title>
<link>https://arxiv.org/abs/2511.11265</link>
<guid>https://arxiv.org/abs/2511.11265</guid>
<content:encoded><![CDATA[
arXiv:2511.11265v1 Announce Type: cross 
Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Aided State Estimation</title>
<link>https://arxiv.org/abs/2511.11285</link>
<guid>https://arxiv.org/abs/2511.11285</guid>
<content:encoded><![CDATA[
arXiv:2511.11285v1 Announce Type: cross 
Abstract: Natural language data, such as text and speech, have become readily available through social networking services and chat platforms. By leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents. To this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation. Finally, the LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building the Web for Agents: A Declarative Framework for Agent-Web Interaction</title>
<link>https://arxiv.org/abs/2511.11287</link>
<guid>https://arxiv.org/abs/2511.11287</guid>
<content:encoded><![CDATA[
arXiv:2511.11287v1 Announce Type: cross 
Abstract: The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces  and  tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization</title>
<link>https://arxiv.org/abs/2511.11362</link>
<guid>https://arxiv.org/abs/2511.11362</guid>
<content:encoded><![CDATA[
arXiv:2511.11362v1 Announce Type: cross 
Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs</title>
<link>https://arxiv.org/abs/2511.11440</link>
<guid>https://arxiv.org/abs/2511.11440</guid>
<content:encoded><![CDATA[
arXiv:2511.11440v1 Announce Type: cross 
Abstract: Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</title>
<link>https://arxiv.org/abs/2511.11551</link>
<guid>https://arxiv.org/abs/2511.11551</guid>
<content:encoded><![CDATA[
arXiv:2511.11551v1 Announce Type: cross 
Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding</title>
<link>https://arxiv.org/abs/2511.11552</link>
<guid>https://arxiv.org/abs/2511.11552</guid>
<content:encoded><![CDATA[
arXiv:2511.11552v1 Announce Type: cross 
Abstract: Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Mixture of Block Attention</title>
<link>https://arxiv.org/abs/2511.11571</link>
<guid>https://arxiv.org/abs/2511.11571</guid>
<content:encoded><![CDATA[
arXiv:2511.11571v1 Announce Type: cross 
Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying and Analyzing Performance-Critical Tokens in Large Language Models</title>
<link>https://arxiv.org/abs/2401.11323</link>
<guid>https://arxiv.org/abs/2401.11323</guid>
<content:encoded><![CDATA[
arXiv:2401.11323v4 Announce Type: replace 
Abstract: In-context learning (ICL) has emerged as an effective solution for few-shot learning with large language models (LLMs). However, how LLMs leverage demonstrations to specify a task and learn a corresponding computational function through ICL is underexplored. Drawing from the way humans learn from content-label mappings in demonstrations, we categorize the tokens in an ICL prompt into content, stopword, and template tokens. Our goal is to identify the types of tokens whose representations directly influence LLM's performance, a property we refer to as being performance-critical. By ablating representations from the attention of the test example, we find that the representations of informative content tokens have less influence on performance compared to template and stopword tokens, which contrasts with the human attention to informative words. We give evidence that the representations of performance-critical tokens aggregate information from the content tokens. Moreover, we demonstrate experimentally that lexical meaning, repetition, and structural cues are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models learn to perform tasks from demonstrations and deepens our understanding of the roles different types of tokens play in large language models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metric Learning Encoding Models: A Multivariate Framework for Interpreting Neural Representations</title>
<link>https://arxiv.org/abs/2402.11608</link>
<guid>https://arxiv.org/abs/2402.11608</guid>
<content:encoded><![CDATA[
arXiv:2402.11608v2 Announce Type: replace 
Abstract: Understanding how explicit theoretical features are encoded in opaque neural systems is a central challenge now common to neuroscience and AI. We introduce Metric Learning Encoding Models (MLEMs) to address this challenge most directly as a metric learning problem: we fit the distance in the space of theoretical features to match the distance in neural space. Our framework improves on univariate encoding and decoding methods by building on second-order isomorphism methods, such as Representational Similarity Analysis, and extends them by learning a metric that efficiently models feature as well as interactions between them. The effectiveness of MLEM is validated through two sets of simulations. First, MLEMs recover ground-truth importance features in synthetic datasets better than state-of-the-art methods, such as Feature Reweighted RSA (FR-RSA). Second, we deploy MLEMs on real language data, where they show stronger robustness to noise in calculating the importance of linguistic features (gender, tense, etc.). MLEMs are applicable to any domains where theoretical features can be identified, such as language, vision, audition, etc. We release optimized code applicable to measure feature importance in the representations of any artificial neural networks or empirical neural data at https://github.com/LouisJalouzot/MLEM.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Survey in Characterization of Semantic Change</title>
<link>https://arxiv.org/abs/2402.19088</link>
<guid>https://arxiv.org/abs/2402.19088</guid>
<content:encoded><![CDATA[
arXiv:2402.19088v4 Announce Type: replace 
Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to characterize how the meaning of words changes and to reason about how to reduce the impact of semantic change. This survey provides an understandable overview of existing approaches to the \textit{characterization of semantic changes} and also formally defines three classes of characterizations: if the meaning of a word becomes more general or narrow (change in dimension) if the word is used in a more pejorative or positive/ameliorated sense (change in orientation), and if there is a trend to use the word in a, for instance, metaphoric or metonymic context (change in relation). We summarized the main aspects of the selected publications in a table and discussed the needs and trends in the research activities on semantic change characterization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Downstream Performance of Mixture-of-Experts Transformers via Weak Vanilla Transformers</title>
<link>https://arxiv.org/abs/2403.01994</link>
<guid>https://arxiv.org/abs/2403.01994</guid>
<content:encoded><![CDATA[
arXiv:2403.01994v2 Announce Type: replace 
Abstract: Recently, Mixture of Experts (MoE) Transformers have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE Transformers underperform vanilla Transformers in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability distillation, positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately enhancing their performance in downstream tasks. We design a specific distillation method and conduct experiments on the BERT architecture. Experimental results show a significant improvement in downstream performance of MoE models, and many further evidences also strongly support the concept of transfer capability distillation. Finally, we attempt to interpret transfer capability distillation and provide some insights from the perspective of model feature.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness</title>
<link>https://arxiv.org/abs/2405.08151</link>
<guid>https://arxiv.org/abs/2405.08151</guid>
<content:encoded><![CDATA[
arXiv:2405.08151v3 Announce Type: replace 
Abstract: Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are language models rational? The case of coherence norms and belief revision</title>
<link>https://arxiv.org/abs/2406.03442</link>
<guid>https://arxiv.org/abs/2406.03442</guid>
<content:encoded><![CDATA[
arXiv:2406.03442v3 Announce Type: replace 
Abstract: Do norms of rationality apply to machine learning models, in particular language models? In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms. We consider both logical coherence norms as well as coherence norms tied to the strength of belief. To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose a new account of credence, which captures the strength of belief in language models. This proposal uniformly assigns strength of belief simply on the basis of model internal next token probabilities. We argue that rational norms tied to coherence do apply to some language models, but not to others. This issue is significant since rationality is closely tied to predicting and explaining behavior, and thus it is connected to considerations about AI safety and alignment, as well as understanding model behavior more generally.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RASTeR: Robust, Agentic, and Structured Temporal Reasoning</title>
<link>https://arxiv.org/abs/2406.19538</link>
<guid>https://arxiv.org/abs/2406.19538</guid>
<content:encoded><![CDATA[
arXiv:2406.19538v2 Announce Type: replace 
Abstract: Temporal question answering (TQA) remains a challenge for large language models (LLMs), particularly when retrieved content may be irrelevant, outdated, or temporally inconsistent. This is especially critical in applications like clinical event ordering, and policy tracking, which require reliable temporal reasoning even under noisy or outdated information. To address this challenge, we introduce RASTeR: \textbf{R}obust, \textbf{A}gentic, and \textbf{S}tructured, \textbf{Te}mporal \textbf{R}easoning, a prompting framework that separates context evaluation from answer generation. RASTeR first assesses the relevance and temporal coherence of the retrieved context, then constructs a temporal knolwedge graph (TKG) to better facilitate reasoning. When inconsistencies are detected, RASTeR selectively corrects or discards context before generating an answer. Across multiple datasets and LLMs, RASTeR consistently improves robustness\footnote{\ Some TQA work defines robustness as handling diverse temporal phenomena. Here, we define it as the ability to answer correctly despite suboptimal context}. We further validate our approach through a ``needle-in-the-haystack'' study, in which relevant context is buried among distractors. With forty distractors, RASTeR achieves 75\% accuracy, over 12\% ahead of the runner up
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Analysis of Gender Depiction in the Comedias of Calder\'on de la Barca</title>
<link>https://arxiv.org/abs/2411.03895</link>
<guid>https://arxiv.org/abs/2411.03895</guid>
<content:encoded><![CDATA[
arXiv:2411.03895v2 Announce Type: replace 
Abstract: In theatre, playwrights use the portrayal of characters to explore culturally based gender norms. In this paper, we develop quantitative methods to study gender depiction in the non-religious works (comedias) of Pedro Calder\'on de la Barca, a prolific Spanish 17th century author. We gather insights from a corpus of more than 100 plays by using a gender classifier and applying model explainability (attribution) methods to determine which text features are most influential in the model's decision to classify speech as 'male' or 'female', indicating the most gendered elements of dialogue in Calder\'on's comedias in a human accessible manner. We find that female and male characters are portrayed differently and can be identified by the gender prediction model at practically useful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender portrayal, and demonstrates that the model is even useful in providing a relatively accurate scene-by-scene prediction of cross-dressing characters.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks</title>
<link>https://arxiv.org/abs/2411.19244</link>
<guid>https://arxiv.org/abs/2411.19244</guid>
<content:encoded><![CDATA[
arXiv:2411.19244v3 Announce Type: replace 
Abstract: The Nepali language has distinct linguistic features, especially its complex script (Devanagari script), morphology, and various dialects,which pose a unique challenge for Natural Language Understanding (NLU) tasks. While the Nepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a foundation for evaluating models, it remains limited in scope, covering four tasks. This restricts their utility for comprehensive assessments of Natural Language Processing (NLP) models. To address this limitation, we introduce twelve new datasets, creating a new benchmark, the Nepali /Language Understanding Evaluation (NLUE) benchmark for evaluating the performance of models across a diverse set of Natural Language Understanding (NLU) tasks. The added tasks include Single-Sentence Classification, Similarity and Paraphrase Tasks, Natural Language Inference (NLI), and General Masked Evaluation Task (GMET). Through extensive experiments, we demonstrate that existing top models struggle with the added complexity of these tasks. We also find that the best multilingual model outperforms the best monolingual models across most tasks, highlighting the need for more robust solutions tailored to the Nepali language. This expanded benchmark sets a new standard for evaluating, comparing, and advancing models, contributing significantly to the broader goal of advancing NLP research for low-resource languages.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LDC: Learning to Generate Research Idea with Dynamic Control</title>
<link>https://arxiv.org/abs/2412.14626</link>
<guid>https://arxiv.org/abs/2412.14626</guid>
<content:encoded><![CDATA[
arXiv:2412.14626v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have demonstrated their potential in automating the scientific research ideation. Existing approaches primarily focus on prompting techniques, often producing ideas misaligned with expert standards - novelty, feasibility, and effectiveness, which are widely recognized by the research community as the three key subdimensions of high-quality ideas. Also, balancing these dimensions remains challenging due to their inherent trade-offs. To address these limitations, we propose the first framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL) for the task. In the SFT stage, the model learns foundational patterns from pairs of research papers and their corresponding follow-up ideas. In the RL stage, multi-dimensional reward models guided by fine-grained feedback evaluate and optimize the model across key dimensions. During inference, dimensional controllers coordinated by a sentence-level decoder enable dynamic context-aware steering of the idea generation process. Our framework provides a balanced approach to research idea generation, achieving high-quality outcomes in the experiment by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotions, Context, and Substance Use in Adolescents: A Large Language Model Analysis of Reddit Posts</title>
<link>https://arxiv.org/abs/2501.14037</link>
<guid>https://arxiv.org/abs/2501.14037</guid>
<content:encoded><![CDATA[
arXiv:2501.14037v2 Announce Type: replace 
Abstract: Early substance use during adolescence increases the risk of later substance use disorders and mental health problems, yet the emotional and contextual factors driving these behaviors remain poorly understood. This study analyzed 23000 substance-use related posts and an equal number of non-substance posts from Reddit's r/teenagers community (2018-2022). Posts were annotated for six discrete emotions (sadness, anger, joy, guilt, fear, disgust) and contextual factors (family, peers, school) using large language models (LLMs). Statistical analyses compared group differences, and interpretable machine learning (SHAP) identified key predictors of substance-use discussions. LLM-assisted thematic coding further revealed latent psychosocial themes linking emotions with contexts. Negative emotions, especially sadness, guilt, fear, and disgust, were significantly more common in substance-use posts, while joy dominated non-substance discussions. Guilt and shame diverged in function: guilt often reflected regret and self-reflection, whereas shame reinforced risky behaviors through peer performance. Peer influence emerged as the strongest contextual factor, closely tied to sadness, fear, and guilt. Family and school environments acted as both risk and protective factors depending on relational quality and stress levels. Overall, adolescent substance-use discussions reflected a dynamic interplay of emotion, social context, and coping behavior. By integrating statistical analysis, interpretable models, and LLM-based thematic exploration, this study demonstrates the value of mixed computational approaches for uncovering the emotional and contextual mechanisms underlying adolescent risk behavior.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DomainCQA: Crafting Knowledge-Intensive QA from Domain-Specific Charts</title>
<link>https://arxiv.org/abs/2503.19498</link>
<guid>https://arxiv.org/abs/2503.19498</guid>
<content:encoded><![CDATA[
arXiv:2503.19498v5 Announce Type: replace 
Abstract: Chart Question Answering (CQA) evaluates Multimodal Large Language Models (MLLMs) on visual understanding and reasoning over chart data. However, existing benchmarks mostly test surface-level parsing, such as reading labels and legends, while overlooking deeper scientific reasoning. We propose DomainCQA, a framework for constructing domain-specific CQA benchmarks that emphasize both visual comprehension and knowledge-intensive reasoning. It integrates complexity-aware chart selection, multitier QA generation, and expert validation. Applied to astronomy, DomainCQA yields AstroChart, a benchmark of 1,690 QA pairs over 482 charts, exposing persistent weaknesses in fine-grained perception, numerical reasoning, and domain knowledge integration across 21 MLLMs. Fine-tuning on AstroChart improves performance across fundamental and advanced tasks. Pilot QA sets in biochemistry, economics, medicine, and social science further demonstrate DomainCQA's generality. Together, our results establish DomainCQA as a unified pipeline for constructing and augmenting domain-specific chart reasoning benchmarks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance</title>
<link>https://arxiv.org/abs/2504.08716</link>
<guid>https://arxiv.org/abs/2504.08716</guid>
<content:encoded><![CDATA[
arXiv:2504.08716v2 Announce Type: replace 
Abstract: Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being its support for long context, faster training, and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge</title>
<link>https://arxiv.org/abs/2505.01812</link>
<guid>https://arxiv.org/abs/2505.01812</guid>
<content:encoded><![CDATA[
arXiv:2505.01812v3 Announce Type: replace 
Abstract: Humans and intelligent animals can internalize new information and accurately internalize their implications to perform downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the information (news) is explicitly given as context, adequately integrating the information into model weights via fine-tuning remains challenging. In this paper, we introduce New News, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. First, we demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications, and Self-QA -- designed to distill the knowledge processed by the model with context into the weights of the model, which we term System-2 Fine-tuning (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the Self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news while preserving general capabilities. Furthermore, we discover the contextual shadowing effect, where training with the news in context followed by its rephrases or QAs catastrophically degrades learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation-Guided Consensus Merging for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14009</link>
<guid>https://arxiv.org/abs/2505.14009</guid>
<content:encoded><![CDATA[
arXiv:2505.14009v2 Announce Type: replace 
Abstract: Recent research has increasingly focused on reconciling the reasoning capabilities of System 2 with the efficiency of System 1. While existing training-based and prompt-based approaches face significant challenges in terms of efficiency and stability, model merging emerges as a promising strategy to integrate the diverse capabilities of different Large Language Models (LLMs) into a unified model. However, conventional model merging methods often assume uniform importance across layers, overlooking the functional heterogeneity inherent in neural components. To address this limitation, we propose \textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}), a plug-and-play merging framework that determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models. ACM effectively preserves task-specific capabilities without requiring gradient computations or additional training. Extensive experiments on Long-to-Short (L2S) and general merging tasks demonstrate that ACM consistently outperforms all baseline methods. For instance, in the case of Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%} reduction in response length while simultaneously improving reasoning accuracy by \textbf{1.3} points.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16270</link>
<guid>https://arxiv.org/abs/2505.16270</guid>
<content:encoded><![CDATA[
arXiv:2505.16270v2 Announce Type: replace 
Abstract: Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at https://github.com/jiaruzouu/TransformerCopilot.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Principle Discovery for Language Model Self-Improvement</title>
<link>https://arxiv.org/abs/2505.16927</link>
<guid>https://arxiv.org/abs/2505.16927</guid>
<content:encoded><![CDATA[
arXiv:2505.16927v2 Announce Type: replace 
Abstract: When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively, requires a labor-intensive annotation process. To automate this process, we propose eliciting these latent attributes that guide model reasoning toward human-preferred responses by explicitly modeling them in a self-correction setting. Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering. Specifically, we employ a form of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses. We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on IFEval. We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance. The gains that our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval</title>
<link>https://arxiv.org/abs/2506.10202</link>
<guid>https://arxiv.org/abs/2506.10202</guid>
<content:encoded><![CDATA[
arXiv:2506.10202v2 Announce Type: replace 
Abstract: Recent approaches have shown impressive proficiency in extracting and leveraging parametric knowledge from Large-Language Models (LLMs) and Vision-Language Models (VLMs). In this work, we consider how we can improve the identification and retrieval of videos related to complex real-world events by automatically extracting latent parametric knowledge about those events. We present Q2E: a Query-to-Event decomposition method for zero-shot multilingual text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our approach demonstrates that we can enhance the understanding of otherwise overly simplified human queries by decomposing the query using the knowledge embedded in LLMs and VLMs. We additionally show how to apply our approach to both visual and speech-based inputs. To combine this varied multimodal knowledge, we adopt entropy-based fusion scoring for zero-shot fusion. Through evaluations on two diverse datasets and multiple retrieval metrics, we demonstrate that Q2E outperforms several state-of-the-art baselines. Our evaluation also shows that integrating audio information can significantly improve text-to-video retrieval. We have released code and data for future research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data</title>
<link>https://arxiv.org/abs/2508.15793</link>
<guid>https://arxiv.org/abs/2508.15793</guid>
<content:encoded><![CDATA[
arXiv:2508.15793v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly employed in applications that require processing information from heterogeneous formats, including texts, tables, infoboxes, and knowledge graphs. However, systematic biases toward particular formats may undermine LLMs' ability to integrate heterogeneous data impartially, potentially resulting in reasoning errors and increased risks in downstream tasks. Yet it remains unclear whether such biases are systematic, which data-level factors drive them, and what internal mechanisms underlie their emergence.
  In this paper, we present the first comprehensive study of format bias in LLMs through a three-stage empirical analysis. The first stage explores the presence and direction of bias across a diverse range of LLMs. The second stage examines how key data-level factors influence these biases. The third stage analyzes how format bias emerges within LLMs' attention patterns and evaluates a lightweight intervention to test its effectiveness. Our results show that format bias is consistent across model families, driven by information richness, structure quality, and representation type, and is closely associated with attention imbalance within the LLMs. Based on these investigations, we identify three future research directions to reduce format bias: enhancing data pre-processing through format repair and normalization, introducing inference-time interventions such as attention re-weighting, and developing format-balanced training corpora. These directions will support the design of more robust and fair heterogeneous data processing systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CyPortQA: Benchmarking Multimodal Large Language Models for Cyclone Preparedness in Port Operation</title>
<link>https://arxiv.org/abs/2508.15846</link>
<guid>https://arxiv.org/abs/2508.15846</guid>
<content:encoded><![CDATA[
arXiv:2508.15846v2 Announce Type: replace 
Abstract: As tropical cyclones intensify and track forecasts become increasingly uncertain, U.S. ports face heightened supply-chain risk under extreme weather conditions. Port operators need to rapidly synthesize diverse multimodal forecast products, such as probabilistic wind maps, track cones, and official advisories, into clear, actionable guidance as cyclones approach. Multimodal large language models (MLLMs) offer a powerful means to integrate these heterogeneous data sources alongside broader contextual knowledge, yet their accuracy and reliability in the specific context of port cyclone preparedness have not been rigorously evaluated. To fill this gap, we introduce CyPortQA, the first multimodal benchmark tailored to port operations under cyclone threat. CyPortQA assembles 2,917 realworld disruption scenarios from 2015 through 2023, spanning 145 U.S. principal ports and 90 named storms. Each scenario fuses multisource data (i.e., tropical cyclone products, port operational impact records, and port condition bulletins) and is expanded through an automated pipeline into 117,178 structured question answer pairs. Using this benchmark, we conduct extensive experiments on diverse MLLMs, including both open-source and proprietary model. MLLMs demonstrate great potential in situation understanding but still face considerable challenges in reasoning tasks, including potential impact estimation and decision reasoning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Surface: Probing the Ideological Depth of Large Language Models</title>
<link>https://arxiv.org/abs/2508.21448</link>
<guid>https://arxiv.org/abs/2508.21448</guid>
<content:encoded><![CDATA[
arXiv:2508.21448v2 Announce Type: replace 
Abstract: Large language models (LLMs) display recognizable political leanings, yet they vary significantly in their ability to represent a political orientation consistently. In this paper, we define ideological depth as (i) a model's ability to follow political instructions without failure (steerability), and (ii) the feature richness of its internal political representations measured with sparse autoencoders (SAEs), an unsupervised sparse dictionary learning (SDL) approach. Using Llama-3.1-8B-Instruct and Gemma-2-9B-IT as candidates, we compare prompt-based and activation-steering interventions and probe political features with publicly available SAEs. We find large, systematic differences: Gemma is more steerable in both directions and activates approximately 7.3x more distinct political features than Llama. Furthermore, causal ablations of a small targeted set of Gemma's political features to create a similar feature-poor setting induce consistent shifts in its behavior, with increased rates of refusals across topics. Together, these results indicate that refusals on benign political instructions or prompts can arise from capability deficits rather than safety guardrails. Ideological depth thus emerges as a measurable property of LLMs, and steerability serves as a window into their latent political architecture.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wage Sentiment Indices Derived from Survey Comments via Large Language Models</title>
<link>https://arxiv.org/abs/2509.00290</link>
<guid>https://arxiv.org/abs/2509.00290</guid>
<content:encoded><![CDATA[
arXiv:2509.00290v2 Announce Type: replace 
Abstract: The emergence of generative Artificial Intelligence (AI) has created new opportunities for economic text analysis. This study proposes a Wage Sentiment Index (WSI) constructed with Large Language Models (LLMs) to forecast wage dynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS), a monthly survey conducted by the Cabinet Office of Japan that captures real-time economic assessments from workers in industries highly sensitive to business conditions. The WSI extends the framework of the Price Sentiment Index (PSI) used in prior studies, adapting it specifically to wage related sentiment. To ensure scalability and adaptability, a data architecture is also developed that enables integration of additional sources such as newspapers and social media. Experimental results demonstrate that WSI models based on LLMs significantly outperform both baseline approaches and pretrained models. These findings highlight the potential of LLM-driven sentiment indices to enhance the timeliness and effectiveness of economic policy design by governments and central banks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions</title>
<link>https://arxiv.org/abs/2509.15901</link>
<guid>https://arxiv.org/abs/2509.15901</guid>
<content:encoded><![CDATA[
arXiv:2509.15901v2 Announce Type: replace 
Abstract: Meeting summarization with large language models (LLMs) remains error-prone, often producing outputs with hallucinations, omissions, and irrelevancies. We present FRAME, a modular pipeline that reframes summarization as a semantic enrichment task. FRAME extracts and scores salient facts, organizes them thematically, and uses these to enrich an outline into an abstractive summary. To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that has the model build a reasoning trace by answering nine questions before content selection. For evaluation, we propose P-MESA, a multi-dimensional, reference-free evaluation framework to assess if a summary fits a target reader. P-MESA reliably identifies error instances, achieving >= 89% balanced accuracy against human annotations and strongly aligns with human severity ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and omission by 2 out of 5 points (measured with MESA), while SCOPE improves knowledge fit and goal alignment over prompt-only baselines. Our findings advocate for rethinking summarization to improve control, faithfulness, and personalization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects</title>
<link>https://arxiv.org/abs/2510.06188</link>
<guid>https://arxiv.org/abs/2510.06188</guid>
<content:encoded><![CDATA[
arXiv:2510.06188v2 Announce Type: replace 
Abstract: Real-time speech assistants are becoming increasingly popular for ensuring improved accessibility to information. Bengali, being a low-resource language with a high regional dialectal diversity, has seen limited progress in developing such systems. Existing systems are not optimized for real-time use and focus only on standard Bengali. In this work, we present BanglaTalk, the first real-time speech assistance system for Bengali regional dialects. BanglaTalk follows the client-server architecture and uses the Real-time Transport Protocol (RTP) to ensure low-latency communication. To address dialectal variation, we introduce a dialect-aware ASR system, BRDialect, developed by fine-tuning the IndicWav2Vec model in ten Bengali regional dialects. It outperforms the baseline ASR models by 12.41-33.98% on the RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of 24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low bandwidth usage and minimal end-to-end delay make the system both cost-effective and interactive for real-time use cases, enabling inclusive and accessible speech technology for the diverse community of Bengali speakers. Code is available in https://github.com/Jak57/BanglaTalk
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Study of Automatic Evaluation in Sign Language Translation</title>
<link>https://arxiv.org/abs/2510.25434</link>
<guid>https://arxiv.org/abs/2510.25434</guid>
<content:encoded><![CDATA[
arXiv:2510.25434v2 Announce Type: replace 
Abstract: Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Latent Reasoning via Looped Language Models</title>
<link>https://arxiv.org/abs/2510.25741</link>
<guid>https://arxiv.org/abs/2510.25741</guid>
<content:encoded><![CDATA[
arXiv:2510.25741v3 Announce Type: replace 
Abstract: Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model is available here: http://ouro-llm.github.io.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable Multimodal Fusion</title>
<link>https://arxiv.org/abs/2403.10568</link>
<guid>https://arxiv.org/abs/2403.10568</guid>
<content:encoded><![CDATA[
arXiv:2403.10568v4 Announce Type: replace-cross 
Abstract: Despite the demonstrated parameter efficiency of prompt-based fusion, its limited adaptivity and expressiveness hinder its effectiveness for multimodal applications at scale. In this paper, we present the first comprehensive study addressing these limitations. Our key motivation is to ``divide and conquer'' the vanilla prompt, traditionally shared across all instances, by generating instance-specific prompts. Specifically, we propose the Mixture of Prompt Experts (MoPE), a framework that significantly enhances prompt adaptivity and expressiveness by dynamically generating instance-specific prompts. MoPE leverages multimodal pairings as additional evidence, allowing the model to adaptively select optimal prompts tailored to each individual instance. Unlike traditional prompt-fusion methods, which encounter scalability bottlenecks when optimizing long unified prompts, MoPE maintains fixed prompt length while effectively scaling the number of specialized experts. Moreover, we investigate regularization terms to encourage expert specialization, resulting in highly adaptive and interpretable prompting. MoPE fundamentally changes the scaling dynamic, unlocking greater expressiveness and adaptability to complex multimodal relationships, enabling the model to selectively attend to task-relevant sub-sequences based on instance-specific multimodal input. Extensive experiments across six multimodal datasets spanning four modalities demonstrate state-of-the-art performance for multimodal fusion, matching or surpassing the performance of fine-tuning while requiring only 0.8% of the trainable parameters. Code is available: https://github.com/songrise/MoPE.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation</title>
<link>https://arxiv.org/abs/2411.16657</link>
<guid>https://arxiv.org/abs/2411.16657</guid>
<content:encoded><![CDATA[
arXiv:2411.16657v4 Announce Type: replace-cross 
Abstract: Storytelling video generation (SVG) aims to produce coherent and visually rich multi-scene videos that follow a structured narrative. Existing methods primarily employ LLM for high-level planning to decompose a story into scene-level descriptions, which are then independently generated and stitched together. However, these approaches struggle with generating high-quality videos aligned with the complex single-scene description, as visualizing such complex description involves coherent composition of multiple characters and events, complex motion synthesis and multi-character customization. To address these challenges, we propose DREAMRUNNER, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout planning. Next, DREAMRUNNER presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame spatial-temporal semantic control. We compare DREAMRUNNER with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DREAMRUNNER exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DREAMRUNNER's robust ability to generate multi-object interactions with qualitative examples.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training</title>
<link>https://arxiv.org/abs/2502.03460</link>
<guid>https://arxiv.org/abs/2502.03460</guid>
<content:encoded><![CDATA[
arXiv:2502.03460v3 Announce Type: replace-cross 
Abstract: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. The official code is released at https://github.com/research4pan/AdaptPruner.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-World LoRA</title>
<link>https://arxiv.org/abs/2503.11880</link>
<guid>https://arxiv.org/abs/2503.11880</guid>
<content:encoded><![CDATA[
arXiv:2503.11880v3 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) in federated settings enables privacy-preserving adaptation but suffers from cross-client interference due to model aggregation. Existing federated LoRA fine-tuning methods, primarily based on FedAvg, struggle with data heterogeneity, leading to harmful cross-client interference and suboptimal personalization. In this work, we propose \textbf{FedALT}, a novel personalized federated LoRA fine-tuning algorithm that fundamentally departs from FedAvg. Instead of using an aggregated model to initialize local training, each client continues training its individual LoRA while incorporating shared knowledge through a separate Rest-of-World (RoW) LoRA component. To effectively balance local adaptation and global information, FedALT introduces an adaptive mixer that dynamically learns input-specific weightings between the individual and RoW LoRA components, drawing conceptual foundations from the Mixture-of-Experts (MoE) paradigm. Through extensive experiments on NLP benchmarks, we demonstrate that FedALT significantly outperforms state-of-the-art personalized federated LoRA fine-tuning methods, achieving superior local adaptation without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable LLM Guardrails via Sparse Representation Steering</title>
<link>https://arxiv.org/abs/2503.16851</link>
<guid>https://arxiv.org/abs/2503.16851</guid>
<content:encoded><![CDATA[
arXiv:2503.16851v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit impressive capabilities in generation tasks but are prone to producing harmful, misleading, or biased content, posing significant ethical and safety concerns. To mitigate such risks, representation engineering, which steer model behavior toward desired attributes by injecting carefully designed steering vectors into LLM's representations at inference time, has emerged as a promising alternative to fine-tuning approaches. However, due to the semantically entangled nature of LLM's representation, existing representation engineering methods still suffer from several limitations: limited fine-grained controllability, content quality degradation, and conflict in multi-attribute control. To overcome these challenges, we propose Sparse Representation Steering (SRS), a novel framework that achieves fine-grained and interpretable control over LLM behavior by first disentangling internal activations into a sparse, semantically meaningful representation space, and then selectively steering relevant dimensions. Specifically, SRS leverages a pretrained Sparse Autoencoder (SAE) to transform dense, entangled activation patterns into a sparse monosemantic feature space. To identify relevant features, SRS contrasts sparse activations from positive and negative prompt pairs and measures their bidirectional KL divergence to locate dimensions most associated with the target attribute. We conduct comprehensive experiments on Gemma-2 series model across three alignment dimensions, i.e., safety, fairness, and truthfulness, to evaluate the effectiveness of SRS. Results show that SRS consistently outperforms existing steering methods, which achieves significantly improved controllability across both single and multiple attribute settings, while preserving high linguistic quality and general ability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for Fair Speech Emotion Recognition</title>
<link>https://arxiv.org/abs/2506.06071</link>
<guid>https://arxiv.org/abs/2506.06071</guid>
<content:encoded><![CDATA[
arXiv:2506.06071v2 Announce Type: replace-cross 
Abstract: Bias in speech emotion recognition (SER) systems often stems from spurious correlations between speaker characteristics and emotional labels, leading to unfair predictions across demographic groups. Many existing debiasing methods require model-specific changes or demographic annotations, limiting their practical use. We present CO-VADA, a Confidence-Oriented Voice Augmentation Debiasing Approach that mitigates bias without modifying model architecture or relying on demographic information. CO-VADA identifies training samples that reflect bias patterns present in the training data and then applies voice conversion to alter irrelevant attributes and generate samples. These augmented samples introduce speaker variations that differ from dominant patterns in the data, guiding the model to focus more on emotion-relevant features. Our framework is compatible with various SER models and voice conversion tools, making it a scalable and practical solution for improving fairness in SER systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First-Order Error Matters: Accurate Compensation for Quantized Large Language Models</title>
<link>https://arxiv.org/abs/2507.11017</link>
<guid>https://arxiv.org/abs/2507.11017</guid>
<content:encoded><![CDATA[
arXiv:2507.11017v2 Announce Type: replace-cross 
Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by performing a first-order Taylor expansion around the pre-quantization weights. This yields an approximation based on the difference between latent and full-precision weights as well as the Hessian matrix. When substituted into the theoretical solution, the formulation eliminates the need to explicitly compute the Hessian, thereby avoiding the high computational cost and limited generalization of backpropagation-based gradient methods. This design introduces only minimal additional computational overhead. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 17.3% and increases the 5-shot MMLU accuracy from 53.8% achieved by GPTAQ to 56.1%. Moreover, FOEM can be seamlessly combined with advanced techniques such as SpinQuant, delivering additional gains under the challenging W4A4KV4 setting and further narrowing the performance gap with full-precision baselines, surpassing existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning</title>
<link>https://arxiv.org/abs/2508.05129</link>
<guid>https://arxiv.org/abs/2508.05129</guid>
<content:encoded><![CDATA[
arXiv:2508.05129v2 Announce Type: replace-cross 
Abstract: With the rapid and continuous increase in academic publications, identifying high-quality research has become an increasingly pressing challenge. While recent methods leveraging Large Language Models (LLMs) for automated paper evaluation have shown great promise, they are often constrained by outdated domain knowledge and limited reasoning capabilities. In this work, we present PaperEval, a novel LLM-based framework for automated paper evaluation that addresses these limitations through two key components: 1) a domain-aware paper retrieval module that retrieves relevant concurrent work to support contextualized assessments of novelty and contributions, and 2) a latent reasoning mechanism that enables deep understanding of complex motivations and methodologies, along with comprehensive comparison against concurrently related work, to support more accurate and reliable evaluation. To guide the reasoning process, we introduce a progressive ranking optimization strategy that encourages the LLM to iteratively refine its predictions with an emphasis on relative comparison. Experiments on two datasets demonstrate that PaperEval consistently outperforms existing methods in both academic impact and paper quality evaluation. In addition, we deploy PaperEval in a real-world paper recommendation system for filtering high-quality papers, which has gained strong engagement on social media -- amassing over 8,000 subscribers and attracting over 10,000 views for many filtered high-quality papers -- demonstrating the practical effectiveness of PaperEval.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging NTPs for Efficient Hallucination Detection in VLMs</title>
<link>https://arxiv.org/abs/2509.20379</link>
<guid>https://arxiv.org/abs/2509.20379</guid>
<content:encoded><![CDATA[
arXiv:2509.20379v2 Announce Type: replace-cross 
Abstract: Hallucinations of vision-language models (VLMs), which are misalignments between visual content and generated text, undermine the reliability of VLMs. One common approach for detecting them employs the same VLM, or a different one, to assess generated outputs. This process is computationally intensive and increases model latency. In this paper, we explore an efficient on-the-fly method for hallucination detection by training traditional ML models over signals based on the VLM's next-token probabilities (NTPs). NTPs provide a direct quantification of model uncertainty. We hypothesize that high uncertainty (i.e., a low NTP value) is strongly associated with hallucinations. To test this, we introduce a dataset of 1,400 human-annotated statements derived from VLM-generated content, each labeled as hallucinated or not, and use it to test our NTP-based lightweight method. Our results demonstrate that NTP-based features are valuable predictors of hallucinations, enabling fast and simple ML models to achieve performance comparable to that of strong VLMs. Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding only the generated text back into the VLM, enhances hallucination detection performance. Finally, integrating hallucination prediction scores from VLMs into the NTP-based models led to better performance than using either VLMs or NTPs alone. We hope this study paves the way for simple, lightweight solutions that enhance the reliability of VLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input</title>
<link>https://arxiv.org/abs/2510.16926</link>
<guid>https://arxiv.org/abs/2510.16926</guid>
<content:encoded><![CDATA[
arXiv:2510.16926v3 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X</title>
<link>https://arxiv.org/abs/2510.25932</link>
<guid>https://arxiv.org/abs/2510.25932</guid>
<content:encoded><![CDATA[
arXiv:2510.25932v2 Announce Type: replace-cross 
Abstract: Social platforms distribute information at unprecedented speed, which in turn accelerates the spread of misinformation and threatens public discourse. We present FakeZero, a fully client-side, cross-platform browser extension that flags unreliable posts on Facebook and X (formerly Twitter) while the user scrolls. All computation, DOM scraping, tokenization, Transformer inference, and UI rendering run locally through the Chromium messaging API, so no personal data leaves the device. FakeZero employs a three-stage training curriculum: baseline fine-tuning and domain-adaptive training enhanced with focal loss, adversarial augmentation, and post-training quantization. Evaluated on a dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1% macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to 14.7 MB and lowering latency to approximately 40 ms, showing that high-quality fake-news detection is feasible under tight resource budgets with only modest performance loss. By providing inline credibility cues, the extension can serve as a valuable tool for policymakers seeking to curb the spread of misinformation across social networks. With user consent, FakeZero also opens the door for researchers to collect large-scale datasets of fake news in the wild, enabling deeper analysis and the development of more robust detection techniques.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages</title>
<link>https://arxiv.org/abs/2511.09690</link>
<guid>https://arxiv.org/abs/2511.09690</guid>
<content:encoded><![CDATA[
<div> Keywords: Omnilingual ASR, low-resource languages, self-supervised pre-training, zero-shot generalization, open-source

<br /><br />Summary:  
The paper presents Omnilingual ASR, a pioneering large-scale automatic speech recognition system designed for extensibility to support over 1,600 languages, including more than 500 previously unserved by ASR technology. It addresses the challenge of expanding ASR to low-resource and long-tail languages by allowing communities to add new languages with only a few data samples. The system utilizes self-supervised pre-training scaled up to 7 billion parameters to develop robust speech representations, alongside an encoder-decoder architecture with a large language model (LLM)-inspired decoder that supports zero-shot generalization to unseen languages. This approach is underpinned by training on a massive, diverse speech corpus sourced from public data and community contributions through compensated local partnerships, ensuring broad linguistic coverage. Evaluations demonstrate substantial performance improvements over previous ASR systems, particularly in low-resource conditions, highlighting the system’s strong generalization capabilities. Omnilingual ASR is released as a model family ranging from 300 million parameters for low-power devices to 7 billion for maximum accuracy. The paper also discusses the ethical considerations involved, emphasizing the importance of community collaboration and open-sourcing models and tools to lower barriers for researchers and language communities, thereby fostering inclusive participation and societal benefits. <div>
arXiv:2511.09690v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the world's 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to most--all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to date--including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at https://github.com/facebookresearch/omnilingual-asr.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Matters: Rethinking Prompt Construction in In-Context Learning</title>
<link>https://arxiv.org/abs/2511.09700</link>
<guid>https://arxiv.org/abs/2511.09700</guid>
<content:encoded><![CDATA[
<div> In-context learning, example selection, example ordering, prompt design, large language models<br /><br />Summary:<br /><br />1. In-context learning (ICL) allows large language models to learn new tasks by conditioning on sequences of examples. 2. Previous research assumed that the choice of examples (selection) has a greater impact on model performance than the order in which these examples appear, leading to a focus primarily on selection strategies. 3. This paper challenges that assumption by systematically comparing the effects of example selection and example ordering across classification and generation tasks. 4. Experiments were conducted using multiple open-source language models ranging from 0.5 billion to 27 billion parameters, as well as the GPT-5 model. 5. Results show that the variation in performance caused by different orderings of the same example set is comparable to the variation caused by using completely different example sets. 6. The research also demonstrates that good example orderings can be identified using only a development set, achieving performance close to an oracle method that uses test labels to pick the best ordering. 7. These findings emphasize that example ordering is as important as example selection and that both factors are closely intertwined in prompt design. 8. The work calls for a reconsideration of prior assumptions in ICL and encourages the development of new strategies that optimize both example selection and ordering for improved model performance. <div>
arXiv:2511.09700v1 Announce Type: new 
Abstract: In-context learning (ICL) enables large language models to perform new tasks by conditioning on a sequence of examples. Most prior work reasonably and intuitively assumes that which examples are chosen has a far greater effect on performance than how those examples are ordered, leading to a focus on example selection. We revisit this assumption and conduct a systematic comparison between the effect of selection and ordering. Through controlled experiments on both classification and generation tasks, using multiple open-source model families (0.5B to 27B parameters) and GPT-5, we find that the variance in performance due to different example orderings is comparable to that from using entirely different example sets. Furthermore, we show that strong orderings can be identified using only a development set, achieving performance close to an oracle that selects the best ordering based on test labels. Our findings highlight the equal and intertwined importance of example selection and ordering in prompt design, calling for a reexamination of the assumptions held in ICL.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual morphologically-guided tokenization for Latin encoder models</title>
<link>https://arxiv.org/abs/2511.09709</link>
<guid>https://arxiv.org/abs/2511.09709</guid>
<content:encoded><![CDATA[
<div> Keywords: tokenization, morphological alignment, Latin, language modeling, low-resource languages<br /><br />Summary:<br /><br />1. Tokenization is a fundamental step in language model pretraining, but standard methods typically focus on information-theoretic objectives like compression and fertility rather than aligning with linguistic morphology.<br />2. This mismatch in tokenization quality is particularly problematic for morphologically rich languages, where it negatively affects performance on downstream NLP tasks.<br />3. The study explores morphologically-aware tokenization specifically for Latin, a morphologically complex language with moderate amounts of pretraining data but substantial curated lexical resources.<br />4. Incorporating morphological knowledge into tokenization leads to improved performance across four downstream tasks, with the greatest gains observed on out-of-domain text, suggesting enhanced generalization abilities of the models.<br />5. The findings highlight the value of leveraging existing linguistic resources as a viable strategy to improve language model performance for low-resource languages, which often lack large-scale pretraining corpora but may have quality lexical databases.<br /><br />This research advocates for the development and integration of morphologically-informed tokenization schemes as a practical means to overcome data scarcity challenges in language modeling for morphologically rich, low-resource languages. <div>
arXiv:2511.09709v1 Announce Type: new 
Abstract: Tokenization is a critical component of language model pretraining, yet standard tokenization methods often prioritize information-theoretical goals like high compression and low fertility rather than linguistic goals like morphological alignment. In fact, they have been shown to be suboptimal for morphologically rich languages, where tokenization quality directly impacts downstream performance. In this work, we investigate morphologically-aware tokenization for Latin, a morphologically rich language that is medium-resource in terms of pretraining data, but high-resource in terms of curated lexical resources -- a distinction that is often overlooked but critical in discussions of low-resource language modeling. We find that morphologically-guided tokenization improves overall performance on four downstream tasks. Performance gains are most pronounced for out of domain texts, highlighting our models' improved generalization ability. Our findings demonstrate the utility of linguistic resources to improve language modeling for morphologically complex languages. For low-resource languages that lack large-scale pretraining data, the development and incorporation of linguistic resources can serve as a feasible alternative to improve LM performance.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Applicability of Natural Language Processing to Traditional Social Science Methodology: A Case Study in Identifying Strategic Signaling Patterns in Presidential Directives</title>
<link>https://arxiv.org/abs/2511.09738</link>
<guid>https://arxiv.org/abs/2511.09738</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, Presidential Directives, Topic Extraction, Social Science, AI Validation

<br /><br />Summary:  
1. The research explores the application of Natural Language Processing (NLP) to extract main topics from extensive written data, focusing on identifying signaling themes in Presidential Directives (PDs) spanning the Reagan to Clinton administrations.  
2. Both human analysts and NLP methods successfully identified relevant documents, highlighting the promising utility of NLP in analyzing large textual corpora for social science research.  
3. Despite these successes, there were notable discrepancies between NLP-generated results and human-labeled data, pointing to limitations that necessitate further investigation and refinement of NLP tools for this specific use case.  
4. The study was conducted in 2023, acknowledging that the rapid advancement in AI and ML tools means that this research utilized potentially outdated technology, underscoring challenges and opportunities in applying evolving AI methods to social science.  
5. Overall, the findings demonstrate both the potential and the current limitations of NLP in the extraction of thematic content from historical political documents, calling for ongoing research to validate and improve AI applications in the social sciences. <div>
arXiv:2511.09738v1 Announce Type: new 
Abstract: Our research investigates how Natural Language Processing (NLP) can be used to extract main topics from a larger corpus of written data, as applied to the case of identifying signaling themes in Presidential Directives (PDs) from the Reagan through Clinton administrations. Analysts and NLP both identified relevant documents, demonstrating the potential utility of NLPs in research involving large written corpuses. However, we also identified discrepancies between NLP and human-labeled results that indicate a need for more research to assess the validity of NLP in this use case. The research was conducted in 2023, and the rapidly evolving landscape of AIML means existing tools have improved and new tools have been developed; this research displays the inherent capabilities of a potentially dated AI tool in emerging social science applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation</title>
<link>https://arxiv.org/abs/2511.09748</link>
<guid>https://arxiv.org/abs/2511.09748</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Critical Error Detection, machine translation, model efficiency, English-German translation<br /><br />Summary: This study investigates the minimum size of Large Language Models (LLMs) required to effectively detect meaning-altering errors in machine translation from English to German, focusing on Critical Error Detection (CED). The research benchmarks four sub-2 billion parameter models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across several datasets including WMT21, WMT22, and SynCED-EnDe-2025. The authors propose a standardized evaluation framework that utilizes consistent prompts, lightweight logit-bias calibration, and majority voting to assess model performance. The results indicate an optimal trade-off around one billion parameters, with the Gemma-3-1B model achieving the best balance between quality and efficiency. This model reached a Matthews correlation coefficient (MCC) of 0.77 and an F1-ERR score of 0.98 on SynCED-EnDe-2025 after fine-tuning with merged weights, while maintaining a low latency of 400 ms per sample on a MacBook Pro M4 Pro. Larger models such as Qwen-3-1.7B achieve higher absolute accuracy but at increased computational cost. Smaller models (0.6B) remain viable with few-shot calibration but struggle with certain error types. Overall, compact, instruction-tuned LLMs supplemented with lightweight calibration and minimal supervision offer a promising solution for private, efficient, on-device error detection in MT workflows, with all materials made publicly available on GitHub. <div>
arXiv:2511.09748v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English->German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR/F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC=0.77 with F1-ERR=0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicate-Argument Structure Divergences in Chinese and English Parallel Sentences and their Impact on Language Transfer</title>
<link>https://arxiv.org/abs/2511.09796</link>
<guid>https://arxiv.org/abs/2511.09796</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-lingual NLP, predicate-argument structure, annotation projection, language transfer, structural divergences<br /><br />Summary:<br /><br />This paper addresses the challenges in cross-lingual natural language processing (NLP), particularly focusing on transferring linguistic knowledge between typologically distant languages like Chinese and English. It emphasizes the importance of predicate-argument structures as a linguistic unit to analyze cross-lingual alignment and misalignment in parallel sentences. The study conducts both qualitative and quantitative analyses of annotations projected from one language to the other, investigating how well predicate annotations align between the two languages. A categorization of structural divergences is proposed to better understand the types of misalignments present. Key findings reveal that language transfer is asymmetric, meaning that the direction of transfer (Chinese to English versus English to Chinese) significantly affects the quality and results of annotation projection. This asymmetry highlights the need to carefully select the source language in transfer learning setups, as it impacts the effectiveness of cross-lingual NLP applications. The paper advocates for further investigation into this asymmetry before making broad scientific claims about cross-lingual transfer methodologies, thus contributing a nuanced perspective on the complexities underlying multilingual language processing. <div>
arXiv:2511.09796v1 Announce Type: new 
Abstract: Cross-lingual Natural Language Processing (NLP) has gained significant traction in recent years, offering practical solutions in low-resource settings by transferring linguistic knowledge from resource-rich to low-resource languages. This field leverages techniques like annotation projection and model transfer for language adaptation, supported by multilingual pre-trained language models. However, linguistic divergences hinder language transfer, especially among typologically distant languages. In this paper, we present an analysis of predicate-argument structures in parallel Chinese and English sentences. We explore the alignment and misalignment of predicate annotations, inspecting similarities and differences and proposing a categorization of structural divergences. The analysis and the categorization are supported by a qualitative and quantitative analysis of the results of an annotation projection experiment, in which, in turn, one of the two languages has been used as source language to project annotations into the corresponding parallel sentences. The results of this analysis show clearly that language transfer is asymmetric. An aspect that requires attention when it comes to selecting the source language in transfer learning applications and that needs to be investigated before any scientific claim about cross-lingual NLP is proposed.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARG: Training-Free Adaptive Retrieval Gating for Efficient RAG</title>
<link>https://arxiv.org/abs/2511.09803</link>
<guid>https://arxiv.org/abs/2511.09803</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Adaptive Retrieval Gating, Uncertainty Scores, Latency Reduction, Instruction-Tuned LLMs<br /><br />Summary:<br /><br />1. Retrieval-Augmented Generation (RAG) enhances factual accuracy but frequently retrieving for every query increases token usage and latency, which can hurt quality.  
2. The authors propose Training-free Adaptive Retrieval Gating (TARG), a single-shot, training-free policy that decides when to retrieve based on a short, no-context draft generated by the base model.  
3. TARG uses lightweight uncertainty scores derived from the draft's prefix logits: mean token entropy, a margin signal computed from the top-1/top-2 logit gap via a monotone link, or small-N variance from multiple stochastic prefixes. Retrieval is triggered only if the uncertainty score crosses a threshold.  
4. TARG is model-agnostic, introduces minimal token overhead, requires no extra training or auxiliary heads, and effectively balances accuracy and efficiency.  
5. Evaluation on datasets including NQ-Open, TriviaQA, and PopQA shows that TARG matches or exceeds the accuracy (EM/F1) of Always-RAG while reducing retrieval calls by 70-90% and decreasing end-to-end latency, with overhead close to Never-RAG. Additionally, the margin signal is the most robust default under instruction-tuned large language models, while small-N variance offers a conservative option prioritizing retrieval budget. Ablations demonstrate trade-offs between gate types and prefix lengths, clarifying latency-budget dynamics. <div>
arXiv:2511.09803v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) improves factuality but retrieving for every query often hurts quality while inflating tokens and latency. We propose Training-free Adaptive Retrieval Gating (TARG), a single-shot policy that decides when to retrieve using only a short, no-context draft from the base model. From the draft's prefix logits, TARG computes lightweight uncertainty scores: mean token entropy, a margin signal derived from the top-1/top-2 logit gap via a monotone link, or small-N variance across a handful of stochastic prefixes, and triggers retrieval only when the score exceeds a threshold. The gate is model agnostic, adds only tens to hundreds of draft tokens, and requires no additional training or auxiliary heads. On NQ-Open, TriviaQA, and PopQA, TARG consistently shifts the accuracy-efficiency frontier: compared with Always-RAG, TARG matches or improves EM/F1 while reducing retrieval by 70-90% and cutting end-to-end latency, and it remains close to Never-RAG in overhead. A central empirical finding is that under modern instruction-tuned LLMs the margin signal is a robust default (entropy compresses as backbones sharpen), with small-N variance offering a conservative, budget-first alternative. We provide ablations over gate type and prefix length and use a delta-latency view to make budget trade-offs explicit.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Khmer Spellchecking: A Holistic Approach</title>
<link>https://arxiv.org/abs/2511.09812</link>
<guid>https://arxiv.org/abs/2511.09812</guid>
<content:encoded><![CDATA[
<div> Keywords: Khmer spellchecking, subword segmentation, named-entity recognition, grapheme-to-phoneme conversion, language model<br /><br />Summary:<br /><br />This paper addresses the unresolved problem of spellchecking for the Khmer language by identifying several key challenges: misalignments between the lexicon and word segmentation model, variable word forms, loosely formed compound words absent in lexicons, and the lack of a Khmer named-entity recognition (NER) model which leads to false misspelling flags for proper nouns. Existing solutions do not sufficiently tackle these issues. To overcome this, the authors propose a holistic approach that integrates multiple components specifically tailored for Khmer: subword segmentation to better handle word boundaries, a Khmer NER system to identify proper nouns accurately, grapheme-to-phoneme (G2P) conversion to improve candidate generation, and a Khmer language model to rank correction candidates effectively. Through the combination of these elements, the approach aims to identify and suggest more suitable spelling corrections. Experimental results demonstrate that this integrated technique achieves a state-of-the-art accuracy of up to 94.4%, outperforming previous methods. Additionally, the study contributes to the Khmer language processing community by planning to release benchmark datasets for both spellchecking and NER tasks, enabling further research and development in this area. <div>
arXiv:2511.09812v1 Announce Type: new 
Abstract: Compared to English and other high-resource languages, spellchecking for Khmer remains an unresolved problem due to several challenges. First, there are misalignments between words in the lexicon and the word segmentation model. Second, a Khmer word can be written in different forms. Third, Khmer compound words are often loosely and easily formed, and these compound words are not always found in the lexicon. Fourth, some proper nouns may be flagged as misspellings due to the absence of a Khmer named-entity recognition (NER) model. Unfortunately, existing solutions do not adequately address these challenges. This paper proposes a holistic approach to the Khmer spellchecking problem by integrating Khmer subword segmentation, Khmer NER, Khmer grapheme-to-phoneme (G2P) conversion, and a Khmer language model to tackle these challenges, identify potential correction candidates, and rank the most suitable candidate. Experimental results show that the proposed approach achieves a state-of-the-art Khmer spellchecking accuracy of up to 94.4%, compared to existing solutions. The benchmark datasets for Khmer spellchecking and NER tasks in this study will be made publicly available.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Graduate Outcomes by Identifying Skills Gaps and Recommending Courses Based on Career Interests</title>
<link>https://arxiv.org/abs/2511.09819</link>
<guid>https://arxiv.org/abs/2511.09819</guid>
<content:encoded><![CDATA[
<div> Keywords: Course recommendation system, machine learning, data analytics, collaborative filtering, user interface<br /><br />Summary:<br /><br />This paper presents the design and development of a course recommendation system aimed at helping students select courses that align with industry trends and requirements. The system integrates various data analytics techniques and machine learning algorithms to provide tailored course suggestions based on individual preferences and academic criteria. It incorporates data mining and collaborative filtering methods to analyze previous courses taken and students' career goals for more personalized recommendations. A significant focus is placed on creating an intuitive and user-friendly front-end interface that emphasizes visual clarity, interaction, and simplicity through iterative prototyping and feedback-based refinements. The design process prioritizes a smooth and engaging user experience to enhance accessibility and usefulness. The system is continuously optimized by incorporating user feedback to better address the needs and preferences of its intended audience. This course recommendation system serves as a valuable tool for students, instructors, and career advisors by bridging the gap between university education and industry demands, thereby promoting lifelong learning and professional advancement. Ultimately, it aims to empower university students with data-driven, industry-informed course choices that contribute to improved graduate outcomes in the higher education sector. <div>
arXiv:2511.09819v1 Announce Type: new 
Abstract: This paper aims to address the challenge of selecting relevant courses for students by proposing the design and development of a course recommendation system. The course recommendation system utilises a combination of data analytics techniques and machine learning algorithms to recommend courses that align with current industry trends and requirements. In order to provide customised suggestions, the study entails the design and implementation of an extensive algorithmic framework that combines machine learning methods, user preferences, and academic criteria. The system employs data mining and collaborative filtering techniques to examine past courses and individual career goals in order to provide course recommendations. Moreover, to improve the accessibility and usefulness of the recommendation system, special attention is given to the development of an easy-to-use front-end interface. The front-end design prioritises visual clarity, interaction, and simplicity through iterative prototyping and user input revisions, guaranteeing a smooth and captivating user experience. We refined and optimised the proposed system by incorporating user feedback, ensuring that it effectively meets the needs and preferences of its target users. The proposed course recommendation system could be a useful tool for students, instructors, and career advisers to use in promoting lifelong learning and professional progression as it fills the gap between university learning and industry expectations. We hope that the proposed course recommendation system will help university students in making data-drive and industry-informed course decisions, in turn, improving graduate outcomes for the university sector.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Answering Students' Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM</title>
<link>https://arxiv.org/abs/2511.09831</link>
<guid>https://arxiv.org/abs/2511.09831</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.09831v1  
Keywords: Large Language Model, Retrieval Augmented Generation, Question Answering, Fine-tuning, Hallucination  

<br /><br />Summary:  
This paper addresses the challenges in course forums where a growing number of student queries lead to delayed responses and repetitive questions for instructors. To solve these problems, the authors propose a question answering system based on an open source Large Language Model (LLM) enhanced with a Retrieval Augmented Generation (RAG) method. The system is fine-tuned using a relevant course dataset to better handle domain-specific queries. It uses a local knowledge base, containing all course content, to retrieve relevant documents corresponding to student questions. This retrieval step improves the accuracy and contextual relevance of the answers. To tackle the problem of hallucination — where LLMs generate plausible but incorrect answers — the system incorporates multi chain-of-thought reasoning, a technique designed to reduce such errors. The proposed approach is experimentally evaluated on the HotpotQA dataset, demonstrating strong performance on the question answering task. Overall, the combination of fine-tuning, document retrieval via RAG, and multi-step reasoning provides an effective solution to enhance timely and accurate responses in educational course forums. <div>
arXiv:2511.09831v1 Announce Type: new 
Abstract: The course forums are increasingly significant and play vital role in facilitating student discussions and answering their questions related to the course. It provides a platform for students to post their questions related to the content and admin issues related to the course. However, there are several challenges due to the increase in the number of students enrolled in the course. The primary challenge is that students' queries cannot be responded immediately and the instructors have to face lots of repetitive questions. To mitigate these issues, we propose a question answering system based on large language model with retrieval augmented generation (RAG) method. This work focuses on designing a question answering system with open source Large Language Model (LLM) and fine-tuning it on the relevant course dataset. To further improve the performance, we use a local knowledge base and applied RAG method to retrieve relevant documents relevant to students' queries, where the local knowledge base contains all the course content. To mitigate the hallucination of LLMs, We also integrate it with multi chain-of-thought reasoning to overcome the challenge of hallucination in LLMs. In this work, we experiment fine-tuned LLM with RAG method on the HotpotQA dataset. The experimental results demonstrate that the fine-tuned LLM with RAG method has a strong performance on question answering task.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TermGPT: Multi-Level Contrastive Fine-Tuning for Terminology Adaptation in Legal and Financial Domain</title>
<link>https://arxiv.org/abs/2511.09854</link>
<guid>https://arxiv.org/abs/2511.09854</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Terminology Adaptation, Contrastive Learning, Legal and Financial Domains, Term Discrimination<br /><br />Summary:<br /><br />Large language models (LLMs) demonstrate strong performance in text generation but struggle with isotropy issues in their embedding spaces, leading to poor discrimination of domain-specific terminology. This limitation is particularly critical in specialized fields like legal and financial domains, where nuanced semantic understanding is essential for downstream applications such as legal judgment prediction and financial risk analysis. To overcome this challenge, the paper proposes TermGPT, a novel multi-level contrastive fine-tuning framework aimed at improving terminology adaptation. TermGPT begins by constructing a sentence graph that captures both semantic and structural relationships in text, enabling the generation of positive and negative samples that are semantically consistent yet discriminative, guided by contextual and topological information. The framework employs multi-level contrastive learning, operating at both sentence and token levels, to simultaneously enhance global contextual comprehension and fine-grained terminology discrimination. To facilitate rigorous assessment, the authors create the first financial terminology dataset based on official regulatory documents, enabling domain-relevant evaluation. Experimental results show that TermGPT surpasses existing baseline models in tasks involving term discrimination within financial and legal texts, indicating its effectiveness in addressing terminology representation issues in specialized domains. <div>
arXiv:2511.09854v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance in text generation tasks; however, their embedding spaces often suffer from the isotropy problem, resulting in poor discrimination of domain-specific terminology, particularly in legal and financial contexts. This weakness in terminology-level representation can severely hinder downstream tasks such as legal judgment prediction or financial risk analysis, where subtle semantic distinctions are critical. To address this problem, we propose TermGPT, a multi-level contrastive fine-tuning framework designed for terminology adaptation. We first construct a sentence graph to capture semantic and structural relations, and generate semantically consistent yet discriminative positive and negative samples based on contextual and topological cues. We then devise a multi-level contrastive learning approach at both the sentence and token levels, enhancing global contextual understanding and fine-grained terminology discrimination. To support robust evaluation, we construct the first financial terminology dataset derived from official regulatory documents. Experiments show that TermGPT outperforms existing baselines in term discrimination tasks within the finance and legal domains.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning via Self-Feedback</title>
<link>https://arxiv.org/abs/2511.09865</link>
<guid>https://arxiv.org/abs/2511.09865</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, chain-of-thought reasoning, token-level exploration, self-feedback, cross-domain transfer<br /><br />Summary:<br /><br />1. The paper addresses the challenge of training Large Language Models (LLMs) for chain-of-thought reasoning, where traditional supervised fine-tuning on a single "golden" rationale limits generalization by penalizing valid alternative answers. 2. Reinforcement learning methods with verifiable rewards face difficulties with credit assignment and require high computational costs, motivating the need for a new approach. 3. The authors propose InTRO (In-Token Rationality Optimization), a framework that facilitates token-level exploration combined with self-generated feedback, improving the accuracy and conciseness of reasoning chains. 4. InTRO works by estimating token-wise importance weights, called correction factors, based on the information discrepancy between the generative policy and an answer-conditioned policy, allowing informed next-token selection within a single forward pass. 5. Experimental results demonstrate that InTRO consistently improves solution accuracy by up to 20% compared to baseline models across six math-reasoning benchmarks, while producing notably more concise and less verbose chains of thought. 6. Additionally, InTRO shows strong cross-domain generalization, successfully adapting to out-of-domain reasoning tasks beyond mathematics, highlighting its broad applicability and robust performance. <div>
arXiv:2511.09865v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) for chain-of-thought reasoning presents a significant challenge: supervised fine-tuning on a single "golden" rationale hurts generalization as it penalizes equally valid alternatives, whereas reinforcement learning with verifiable rewards struggles with credit assignment and prohibitive computational cost. To tackle these limitations, we introduce InTRO (In-Token Rationality Optimization), a new framework that enables both token-level exploration and self-feedback for accurate and concise reasoning. Instead of directly optimizing an intractable objective over all valid reasoning paths, InTRO leverages correction factors-token-wise importance weights estimated by the information discrepancy between the generative policy and its answer-conditioned counterpart, for informative next token selection. This approach allows the model to perform token-level exploration and receive self-generated feedback within a single forward pass, ultimately encouraging accurate and concise rationales. Across six math-reasoning benchmarks, InTRO consistently outperforms other baselines, raising solution accuracy by up to 20% relative to the base model. Its chains of thought are also notably more concise, exhibiting reduced verbosity. Beyond this, InTRO enables cross-domain transfer, successfully adapting to out-of-domain reasoning tasks that extend beyond the realm of mathematics, demonstrating robust generalization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.09873</link>
<guid>https://arxiv.org/abs/2511.09873</guid>
<content:encoded><![CDATA[
<div> Keywords: HierRouter, hierarchical routing, large language models, reinforcement learning, cost-efficient inference<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) achieve state-of-the-art results but are resource-intensive, posing challenges for deployment in constrained environments.  
2. The paper introduces HierRouter, a hierarchical routing framework that dynamically composes inference pipelines from multiple specialized, lightweight LLMs to optimize performance and cost.  
3. The routing problem is modeled as a finite-horizon Markov Decision Process (MDP), with a reinforcement learning agent trained using Proximal Policy Optimization (PPO) to select models iteratively at each multi-hop inference step.  
4. The agent makes decisions based on the evolving context and accumulated computational cost, enabling context-aware routing to balance quality and efficiency.  
5. Experiments conducted on six benchmarks, including question answering, code generation, and mathematical reasoning tasks, demonstrate that HierRouter enhances response quality by up to 2.4 times over using single models, with only minimal additional inference cost.  
6. The approach shows strong potential for enabling cost-efficient, high-performance LLM inference in real-time and resource-limited scenarios.  
7. The implementation and code are publicly available at https://github.com/Nikunj-Gupta/hierouter. <div>
arXiv:2511.09873v1 Announce Type: new 
Abstract: Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings. To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models. Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference. The agent conditions on the evolving context and accumulated cost to make context-aware routing decisions. Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average. These results highlight the promise of hierarchical routing for cost-efficient, high-performance LLM inference. All codes can be found here https://github.com/ Nikunj-Gupta/hierouter.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnchTable: Unified Safety Alignment Transfer in Fine-tuned Large Language Models</title>
<link>https://arxiv.org/abs/2511.09880</link>
<guid>https://arxiv.org/abs/2511.09880</guid>
<content:encoded><![CDATA[
<div> Safety alignment, Neural Tangent Kernel, fine-tuning, large language models, jailbreaking resistance<br /><br />Summary:<br /><br />Many machine learning models fine-tuned from large language models (LLMs) suffer from systematic degradation in safety alignment, leading to ethical risks and increased harmful outputs. EnchTable is introduced as a new framework that transfers and maintains safety alignment in downstream LLMs without requiring extensive retraining. The framework uses a Neural Tangent Kernel (NTK)-based safety vector distillation method, which effectively decouples safety constraints from task-specific reasoning, allowing broad compatibility across various architectures and sizes. It also incorporates an interference-aware merging technique to balance safety and utility, reducing performance compromises across different task domains. A fully functional prototype of EnchTable was implemented on three diverse task domains and LLM architectures, evaluated extensively on eleven datasets for utility and safety. EnchTable demonstrated strong generalization across models from multiple vendors. It also showed robust resistance to both static and dynamic jailbreaking attacks, outperforming vendor safety models in mitigating adversarial prompts. Compared to six parameter modification methods and two inference-time alignment baselines, EnchTable achieved lower unsafe rates, higher utility scores, and universal applicability. Finally, EnchTable integrates seamlessly into deployment pipelines without adding significant operational overhead. <div>
arXiv:2511.09880v1 Announce Type: new 
Abstract: Many machine learning models are fine-tuned from large language models (LLMs) to achieve high performance in specialized domains like code generation, biomedical analysis, and mathematical problem solving. However, this fine-tuning process often introduces a critical vulnerability: the systematic degradation of safety alignment, undermining ethical guidelines and increasing the risk of harmful outputs. Addressing this challenge, we introduce EnchTable, a novel framework designed to transfer and maintain safety alignment in downstream LLMs without requiring extensive retraining. EnchTable leverages a Neural Tangent Kernel (NTK)-based safety vector distillation method to decouple safety constraints from task-specific reasoning, ensuring compatibility across diverse model architectures and sizes. Additionally, our interference-aware merging technique effectively balances safety and utility, minimizing performance compromises across various task domains. We implemented a fully functional prototype of EnchTable on three different task domains and three distinct LLM architectures, and evaluated its performance through extensive experiments on eleven diverse datasets, assessing both utility and model safety. Our evaluations include LLMs from different vendors, demonstrating EnchTable's generalization capability. Furthermore, EnchTable exhibits robust resistance to static and dynamic jailbreaking attacks, outperforming vendor-released safety models in mitigating adversarial prompts. Comparative analyses with six parameter modification methods and two inference-time alignment baselines reveal that EnchTable achieves a significantly lower unsafe rate, higher utility score, and universal applicability across different task domains. Additionally, we validate EnchTable can be seamlessly integrated into various deployment pipelines without significant overhead.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HI-TransPA: Hearing Impairments Translation Personal Assistant</title>
<link>https://arxiv.org/abs/2511.09915</link>
<guid>https://arxiv.org/abs/2511.09915</guid>
<content:encoded><![CDATA[
<div> Keywords: Omni-Model, assistive technology, hearing-impaired, audio-visual, curriculum learning<br /><br />Summary:<br /><br />This article introduces HI-TransPA, an instruction-driven audio-visual personal assistant designed to enhance daily communication for hearing-impaired individuals by leveraging the Omni-Model paradigm. The proposed model effectively fuses indistinct speech with high-frame-rate lip dynamics, allowing for both translation and dialogue within a unified multimodal framework. To address the challenges posed by noisy, heterogeneous data and the limited adaptability of current Omni-Models to hearing-impaired speech, a comprehensive preprocessing and curation pipeline is developed. This pipeline detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses the quality of multimodal samples. The derived quality scores inform a curriculum learning strategy that initially trains on clean, high-confidence data and gradually introduces more challenging cases to improve model robustness. Additionally, the study incorporates a SigLIP encoder combined with a Unified 3D-Resampler to efficiently represent high-frame-rate lip motions. Experimental results on the HI-Dialogue dataset demonstrate that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. Overall, this work lays a foundational framework for applying Omni-Models in assistive communication technologies and provides essential tools and methodologies for future research in this domain. <div>
arXiv:2511.09915v1 Announce Type: new 
Abstract: To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection</title>
<link>https://arxiv.org/abs/2511.09918</link>
<guid>https://arxiv.org/abs/2511.09918</guid>
<content:encoded><![CDATA[
<div> Keywords: social norms, multi-turn dialogues, Norm-RAG, multilingual dataset, norm adherence  

<br /><br />Summary:  
This paper addresses the challenge of socially normative reasoning in conversational AI, emphasizing the subjective, context-dependent, and culturally diverse nature of social norms that traditional commonsense models do not capture. It introduces Norm-RAG, a retrieval-augmented agentic framework designed to infer nuanced social norms in multi-turn dialogues by modeling utterance-level attributes such as communicative intent, speaker roles, interpersonal framing, and linguistic cues. Norm-RAG leverages a novel Semantic Chunking approach to retrieve and ground dialogue analysis in structured normative documentation, enabling interpretable and context-aware assessments of normative adherence and violations. Additionally, the work contributes MINDS (Multilingual Interactions with Norm-Driven Speech), a new bilingual dataset containing 31 Mandarin-English and Spanish-English conversations annotated at each turn for norm category and adherence status, reflecting realistic and culturally diverse social interactions. Experimental results show that Norm-RAG enhances norm detection accuracy and generalizes better across cultural contexts, improving the capabilities of dialogue systems to act socially intelligent and culturally adaptive in multilingual conversation settings. This approach moves beyond isolated utterance analysis by handling the fluid and multi-turn nature of conversations with nuanced norm reasoning. <div>
arXiv:2511.09918v1 Announce Type: new 
Abstract: Social norms are implicit, culturally grounded expectations that guide interpersonal communication. Unlike factual commonsense, norm reasoning is subjective, context-dependent, and varies across cultures, posing challenges for computational models. Prior works provide valuable normative annotations but mostly target isolated utterances or synthetic dialogues, limiting their ability to capture the fluid, multi-turn nature of real-world conversations. In this work, we present Norm-RAG, a retrieval-augmented, agentic framework for nuanced social norm inference in multi-turn dialogues. Norm-RAG models utterance-level attributes including communicative intent, speaker roles, interpersonal framing, and linguistic cues and grounds them in structured normative documentation retrieved via a novel Semantic Chunking approach. This enables interpretable and context-aware reasoning about norm adherence and violation across multilingual dialogues. We further introduce MINDS (Multilingual Interactions with Norm-Driven Speech), a bilingual dataset comprising 31 multi-turn Mandarin-English and Spanish-English conversations. Each turn is annotated for norm category and adherence status using multi-annotator consensus, reflecting cross-cultural and realistic norm expression. Our experiments show that Norm-RAG improves norm detection and generalization, demonstrates improved performance for culturally adaptive and socially intelligent dialogue systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Identifying Knowledge Components</title>
<link>https://arxiv.org/abs/2511.09935</link>
<guid>https://arxiv.org/abs/2511.09935</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Components, Large Language Models, KC merging, cosine similarity, adaptive learning systems  

<br /><br />Summary:  
This study investigates the automation of Knowledge Component (KC) identification for adaptive learning systems using Large Language Models (LLMs), specifically GPT-4o-mini. The researchers scaled a "simulated textbook" prompting strategy to analyze a larger dataset of 646 multiple-choice questions. Initial results showed that the LLM-generated KCs performed worse than a domain expert-created KC model, with an RMSE of 0.4285 compared to 0.4206, while also producing an excessive number of KCs (569 versus 101). To address the problem of redundancy and overgeneration, the paper proposes a novel semantic merging approach that clusters semantically similar KCs based on their cosine similarity scores. Applying a cosine similarity threshold of 0.8 to merge KCs reduced their count from 569 to 428 and improved model accuracy, lowering the RMSE to 0.4259. The findings indicate that purely scaled LLM generation is insufficient for effective KC identification, but integrating semantic similarity-based merging offers a promising solution to automate and refine this process in adaptive learning contexts. This work advances the automation of educational content modeling by combining generation and post-processing techniques to reduce noise and redundancy in KC labels. <div>
arXiv:2511.09935v1 Announce Type: new 
Abstract: Knowledge Components (KCs) are foundational to adaptive learning systems, but their manual identification by domain experts is a significant bottleneck. While Large Language Models (LLMs) offer a promising avenue for automating this process, prior research has been limited to small datasets and has been shown to produce superfluous, redundant KC labels. This study addresses these limitations by first scaling a "simulated textbook" LLM prompting strategy (using GPT-4o-mini) to a larger dataset of 646 multiple-choice questions. We found that this initial automated approach performed significantly worse than an expert-designed KC model (RMSE 0.4285 vs. 0.4206) and generated an excessive number of KCs (569 vs. 101). To address the issue of redundancy, we proposed and evaluated a novel method for merging semantically similar KC labels based on their cosine similarity. This merging strategy significantly improved the model's performance; a model using a cosine similarity threshold of 0.8 achieved the best result, reducing the KC count to 428 and improving the RMSE to 0.4259. This demonstrates that while scaled LLM generation alone is insufficient, combining it with a semantic merging technique offers a viable path toward automating and refining KC identification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2511.09966</link>
<guid>https://arxiv.org/abs/2511.09966</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, multi-hop reasoning, global planning, Sub-task Planner, Fact Extractor  

<br /><br />Summary:  
This paper addresses limitations in current retrieval-augmented generation (RAG) approaches for multi-hop reasoning, highlighting their lack of global planning which often leads to local reasoning dead-ends. To resolve this, the authors propose Recursive Evaluation and Adaptive Planning (REAP), featuring two key components: the Sub-task Planner (SP) and the Fact Extractor (FE). SP maintains a global overview, guiding the reasoning trajectory by evaluating intermediate task states, while FE performs detailed analysis of retrieved content to extract reliable facts and clues. Together, these modules incrementally build a coherent global knowledge representation, improving both the reliability and traceability of reasoning processes. Additionally, a unified task paradigm is introduced to facilitate effective multi-task fine-tuning, which notably enhances SP’s capabilities on complex and data-scarce tasks. The effectiveness of REAP is validated through extensive experiments on multiple public multi-hop datasets, where it significantly outperforms existing RAG methods in both in-domain and out-of-domain scenarios. The results demonstrate that REAP provides a more robust and accurate solution for complex multi-hop reasoning challenges, mitigating hallucinations and improving reasoning outcomes in large language models. <div>
arXiv:2511.09966v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has been extensively employed to mitigate hallucinations in large language models (LLMs). However, existing methods for multi-hop reasoning tasks often lack global planning, increasing the risk of falling into local reasoning impasses. Insufficient exploitation of retrieved content and the neglect of latent clues fail to ensure the accuracy of reasoning outcomes. To overcome these limitations, we propose Recursive Evaluation and Adaptive Planning (REAP), whose core idea is to explicitly maintain structured sub-tasks and facts related to the current task through the Sub-task Planner (SP) and Fact Extractor (FE) modules. SP maintains a global perspective, guiding the overall reasoning direction and evaluating the task state based on the outcomes of FE, enabling dynamic optimization of the task-solving trajectory. FE performs fine-grained analysis over retrieved content to extract reliable answers and clues. These two modules incrementally enrich a logically coherent representation of global knowledge, enhancing the reliability and the traceability of the reasoning process. Furthermore, we propose a unified task paradigm design that enables effective multi-task fine-tuning, significantly enhancing SP's performance on complex, data-scarce tasks. We conduct extensive experiments on multiple public multi-hop datasets, and the results demonstrate that our method significantly outperforms existing RAG methods in both in-domain and out-of-domain settings, validating its effectiveness in complex multi-hop reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NumPert: Numerical Perturbations to Probe Language Models for Veracity Prediction</title>
<link>https://arxiv.org/abs/2511.09971</link>
<guid>https://arxiv.org/abs/2511.09971</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical reasoning, veracity prediction, robustness, large language models, fact-checking<br /><br />Summary:<br /><br />This paper evaluates the performance of state-of-the-art large language models on the task of numerical fact-checking, focusing on their ability to assess veracity of numerical claims paired with evidence. The study uses controlled perturbations, including label-flipping probes, to systematically test the robustness of these models. Results reveal that even leading proprietary models suffer significant accuracy drops, up to 62%, under specific perturbations, indicating a lack of consistent robustness across different conditions. Additionally, the research finds that increasing the length of the input context generally leads to a reduction in accuracy. However, when the extended context is enhanced with perturbed demonstration examples, many models show substantial recovery in performance. These findings highlight critical limitations of current language models in handling numerical reasoning tasks within fact-checking applications. The study concludes that improving robustness to numerical perturbations remains a significant open challenge for future development in large language model capabilities. <div>
arXiv:2511.09971v1 Announce Type: new 
Abstract: Large language models show strong performance on knowledge intensive tasks such as fact-checking and question answering, yet they often struggle with numerical reasoning. We present a systematic evaluation of state-of-the-art models for veracity prediction on numerical claims and evidence pairs using controlled perturbations, including label-flipping probes, to test robustness. Our results indicate that even leading proprietary systems experience accuracy drops of up to 62\% under certain perturbations. No model proves to be robust across all conditions. We further find that increasing context length generally reduces accuracy, but when extended context is enriched with perturbed demonstrations, most models substantially recover. These findings highlight critical limitations in numerical fact-checking and suggest that robustness remains an open challenge for current language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG</title>
<link>https://arxiv.org/abs/2511.09980</link>
<guid>https://arxiv.org/abs/2511.09980</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic retrieval, language models, Entropy-Trend Constraint, token-level uncertainty, retrieval timing<br /><br />Summary: Dynamic retrieval-augmented generation (RAG) improves large language models (LLMs) by enabling on-demand fetching of external knowledge, unlike static RAG which is less adaptable. The main challenge addressed is determining the optimal timing for retrieval to avoid delayed interventions after errors propagate. Existing methods rely on low token-level confidence to trigger retrieval, which can be too late. The paper introduces Entropy-Trend Constraint (ETC), a training-free method that models the dynamics of token-level uncertainty using first- and second-order differences of entropy sequences. This approach detects emerging uncertainty trends, facilitating earlier and more precise retrieval. Experiments conducted on six question-answering benchmarks using three different LLM backbones demonstrate ETC’s consistent superiority over strong baselines while also reducing the frequency of retrieval. ETC is especially effective in domain-specific contexts and shows robust generalization capabilities. Further ablation studies and qualitative analyses validate that modeling uncertainty trends leads to more effective retrieval timing. The method is plug-and-play, model-agnostic, and can be easily integrated into existing decoding pipelines. The authors provide implementation code in the supplementary materials to encourage adoption. <div>
arXiv:2511.09980v1 Announce Type: new 
Abstract: Dynamic retrieval-augmented generation (RAG) allows large language models (LLMs) to fetch external knowledge on demand, offering greater adaptability than static RAG. A central challenge in this setting lies in determining the optimal timing for retrieval. Existing methods often trigger retrieval based on low token-level confidence, which may lead to delayed intervention after errors have already propagated. We introduce Entropy-Trend Constraint (ETC), a training-free method that determines optimal retrieval timing by modeling the dynamics of token-level uncertainty. Specifically, ETC utilizes first- and second-order differences of the entropy sequence to detect emerging uncertainty trends, enabling earlier and more precise retrieval. Experiments on six QA benchmarks with three LLM backbones demonstrate that ETC consistently outperforms strong baselines while reducing retrieval frequency. ETC is particularly effective in domain-specific scenarios, exhibiting robust generalization capabilities. Ablation studies and qualitative analyses further confirm that trend-aware uncertainty modeling yields more effective retrieval timing. The method is plug-and-play, model-agnostic, and readily integrable into existing decoding pipelines. Implementation code is included in the supplementary materials.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation</title>
<link>https://arxiv.org/abs/2511.09984</link>
<guid>https://arxiv.org/abs/2511.09984</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual Retrieval-Augmented Generation, Language Drift, Chain-of-Thought, Soft Constrained Decoding, Language Alignment  

<br /><br />Summary:  
This paper investigates the phenomenon of language drift in multilingual Retrieval-Augmented Generation (RAG) systems, where large language models generate responses in unintended languages when the retrieved evidence and input queries differ in language. The issue is most pronounced during reasoning-intensive tasks like Chain-of-Thought (CoT) generation, where the intermediate reasoning steps exacerbate language instability. Through comprehensive experiments across multiple datasets, languages, and various LLM backbones, the authors identify that the drift is due not to comprehension errors but to decoder-level collapse, driven by dominant token distributions and high-frequency English patterns. English emerges as a semantic attractor, acting as the primary interference source and fallback language in cross-lingual situations. To address this, the authors propose Soft Constrained Decoding (SCD), a training-free, lightweight decoding strategy that softly penalizes tokens from non-target languages to steer the generation toward the intended language. SCD is model-agnostic and can be integrated with any generation algorithm without requiring architectural changes or additional data. Experiments on three multilingual datasets covering typologically diverse languages demonstrate that SCD consistently enhances language alignment and overall task performance, offering an effective and generalizable solution for mitigating language drift in multilingual RAG systems. <div>
arXiv:2511.09984v1 Announce Type: new 
Abstract: Multilingual Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to perform knowledge-intensive tasks in multilingual settings by leveraging retrieved documents as external evidence. However, when the retrieved evidence differs in language from the user query and in-context exemplars, the model often exhibits language drift by generating responses in an unintended language. This phenomenon is especially pronounced during reasoning-intensive decoding, such as Chain-of-Thought (CoT) generation, where intermediate steps introduce further language instability. In this paper, we systematically study output language drift in multilingual RAG across multiple datasets, languages, and LLM backbones. Our controlled experiments reveal that the drift results not from comprehension failure but from decoder-level collapse, where dominant token distributions and high-frequency English patterns dominate the intended generation language. We further observe that English serves as a semantic attractor under cross-lingual conditions, emerging as both the strongest interference source and the most frequent fallback language.
  To mitigate this, we propose Soft Constrained Decoding (SCD), a lightweight, training-free decoding strategy that gently steers generation toward the target language by penalizing non-target-language tokens. SCD is model-agnostic and can be applied to any generation algorithm without modifying the architecture or requiring additional data. Experiments across three multilingual datasets and multiple typologically diverse languages show that SCD consistently improves language alignment and task performance, providing an effective and generalizable solution in multilingual RAG.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>