<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>


<item>
<title>Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.00001</link>
<guid>https://arxiv.org/abs/2505.00001</guid>
<content:encoded><![CDATA[
<div> benchmark, logical reasoning, generalization, large language models, low-resource settings 

Summary:
The research introduces Rosetta-PL, a benchmark to assess the logical reasoning and generalization abilities of Large Language Models (LLMs) in controlled environments. Rosetta-PL is constructed by translating logical propositions from Lean into a custom logical language for fine-tuning LLMs like GPT-4o. The impact of dataset size and translation methodology on model performance is analyzed. Results show that maintaining logical relationships during translation significantly improves precision, with accuracy leveling off after around 20,000 training samples. These findings offer valuable insights for enhancing LLM training in formal reasoning tasks and enhancing performance in low-resource language applications. 

Summary: <div>
arXiv:2505.00001v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbol grounding in computational systems: A paradox of intentions</title>
<link>https://arxiv.org/abs/2505.00002</link>
<guid>https://arxiv.org/abs/2505.00002</guid>
<content:encoded><![CDATA[
<div> Keywords: computational systems, symbol grounding, computationalism, semantic nativism, intentional cognitive processes

Summary: 
The paper discusses a paradoxical aspect of computational systems that challenges the concept of symbol grounding within the framework of computationalism. It argues that if the mind operates as a digital computer, it must compute over either meaningful or meaningless symbols. Computing over meaningful symbols implies semantic nativism, suggesting the existence of inherent meaning within the system. On the other hand, computing over meaningless symbols implies a lack of intentional cognitive processes prior to symbol grounding, making the grounding process impossible. Thus, computationalism inherently implies semantic nativism, regardless of whether the mind operates on meaningful or meaningless symbols. This raises important questions about the nature of cognition and the role of symbolic representation in computational frameworks.<br /><br />Summary: <div>
arXiv:2505.00002v1 Announce Type: new 
Abstract: The paper presents a paradoxical feature of computational systems that suggests that computationalism cannot explain symbol grounding. If the mind is a digital computer, as computationalism claims, then it can be computing either over meaningful symbols or over meaningless symbols. If it is computing over meaningful symbols its functioning presupposes the existence of meaningful symbols in the system, i.e. it implies semantic nativism. If the mind is computing over meaningless symbols, no intentional cognitive processes are available prior to symbol grounding. In this case, no symbol grounding could take place since any grounding presupposes intentional cognitive processes. So, whether computing in the mind is over meaningless or over meaningful symbols, computationalism implies semantic nativism.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs</title>
<link>https://arxiv.org/abs/2505.00003</link>
<guid>https://arxiv.org/abs/2505.00003</guid>
<content:encoded><![CDATA[
<div> Keywords: NLP, Large Language Models, psychological theories, cognition, integration 

Summary: 
This paper discusses the importance of incorporating psychological theories into the development of Large Language Models (LLMs) in Natural Language Processing (NLP). It emphasizes the role of psychology in understanding human-like cognition, behavior, and interaction, and how it can enhance various stages of LLM development. By integrating insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics, the paper highlights current trends and gaps in the application of psychological theories in NLP research. The analysis aims to bridge disciplinary divides and promote a more thoughtful integration of psychology into future NLP advancements. <div>
arXiv:2505.00003v1 Announce Type: new 
Abstract: Psychological insights have long shaped pivotal NLP breakthroughs, including the cognitive underpinnings of attention mechanisms, formative reinforcement learning, and Theory of Mind-inspired social modeling. As Large Language Models (LLMs) continue to grow in scale and complexity, there is a rising consensus that psychology is essential for capturing human-like cognition, behavior, and interaction. This paper reviews how psychological theories can inform and enhance stages of LLM development, including data, pre-training, post-training, and evaluation\&amp;application. Our survey integrates insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics. Our analysis highlights current trends and gaps in how psychological theories are applied. By examining both cross-domain connections and points of tension, we aim to bridge disciplinary divides and promote more thoughtful integration of psychology into future NLP research.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangVAE and LangSpace: Building and Probing for Language Model VAEs</title>
<link>https://arxiv.org/abs/2505.00004</link>
<guid>https://arxiv.org/abs/2505.00004</guid>
<content:encoded><![CDATA[
<div> framework, modular, variational autoencoders, language models, representations  
Summary:  
The article introduces LangVAE, a framework that allows for the modular construction of variational autoencoders using pre-trained large language models. This enables the encoding of knowledge from the pre-trained components into more compact and semantically disentangled representations. The LangVAE framework is complemented by LangSpace, which offers various probing methods for analyzing the textual representations, such as vector traversal, interpolation, disentanglement measures, and cluster visualizations. LangVAE and LangSpace provide a flexible, efficient, and scalable approach for building and analyzing textual representations with seamless integration for models available on the HuggingFace Hub. The experiments conducted with different encoder and decoder combinations, as well as annotated inputs, reveal diverse interactions across architectural families and sizes concerning generalization and disentanglement. This framework shows promise in systematizing the experimentation and understanding of textual representations.  
<br /><br />Summary: <div>
arXiv:2505.00004v1 Announce Type: new 
Abstract: We present LangVAE, a novel framework for modular construction of variational autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such language model VAEs can encode the knowledge of their pre-trained components into more compact and semantically disentangled representations. The representations obtained in this way can be analysed with the LangVAE companion framework: LangSpace, which implements a collection of probing methods, such as vector traversal and interpolation, disentanglement measures, and cluster visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable way of building and analysing textual representations, with simple integration for models available on the HuggingFace Hub. Additionally, we conducted a set of experiments with different encoder and decoder combinations, as well as annotated inputs, revealing a wide range of interactions across architectural families and sizes w.r.t. generalisation and disentanglement. Our findings demonstrate a promising framework for systematising the experimentation and understanding of textual representations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a digital twin of U.S. Congress</title>
<link>https://arxiv.org/abs/2505.00006</link>
<guid>https://arxiv.org/abs/2505.00006</guid>
<content:encoded><![CDATA[
<div> digital twin, language models, U.S. congresspersons, Tweets, roll-call votes

Summary:
The paper presents a virtual model of U.S. congresspersons using language models to create a digital twin. A dataset containing all Tweets from congresspersons is used to generate Tweets that closely resemble those of the actual individuals. These generated Tweets can predict roll-call vote behaviors and the likelihood of crossing party lines, providing insights for stakeholders to allocate resources and impact legislative dynamics. The study highlights the capability of language models in mimicking real-world behaviors and its implications for political analysis. Limitations and possible extensions of the research are also discussed. <div>
arXiv:2505.00006v1 Announce Type: new 
Abstract: In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination</title>
<link>https://arxiv.org/abs/2505.00008</link>
<guid>https://arxiv.org/abs/2505.00008</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Processing, medically inaccurate information, error detection, misinformation correction, hallucination detection<br />
<br />
Summary: This review explores the use of Natural Language Processing (NLP) to detect and correct medically inaccurate information, including errors, misinformation, and hallucination. The review categorizes studies based on tasks such as error detection, error correction, misinformation detection, misinformation correction, hallucination detection, and hallucination mitigation. NLP has shown promise in addressing these tasks, but challenges remain with data privacy, context dependency, and evaluation standards. The review emphasizes the importance of developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications. By advancing patient safety, improving public health communication, and supporting the development of more reliable NLP applications, this review aims to contribute to the overall improvement of healthcare practices. <br /><br /> <div>
arXiv:2505.00008v1 Announce Type: new 
Abstract: Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation</title>
<link>https://arxiv.org/abs/2505.00009</link>
<guid>https://arxiv.org/abs/2505.00009</guid>
<content:encoded><![CDATA[
<div> pre-trained language models, multi-task learning, prompt tuning, low-rank representation, parameter efficiency <br />
Summary: 
The article introduces Task-Adaptive Low-Rank Representation (TA-LoRA) as a method for multi-task learning, building on prompt tuning to capture task-specific knowledge effectively. TA-LoRA utilizes low-rank representation to model task heterogeneity and a fast-slow weights mechanism to differentiate shared and task-specific knowledge. It introduces a zero-initialized attention mechanism to minimize disruption during warm-up epochs. Experimental results on 16 tasks show that TA-LoRA achieves state-of-the-art performance in both full-data and few-shot settings while maintaining superior parameter efficiency. <div>
arXiv:2505.00009v1 Announce Type: new 
Abstract: Pre-trained language models (PLMs) demonstrate remarkable intelligence but struggle with emerging tasks unseen during training in real-world applications. Training separate models for each new task is usually impractical. Multi-task learning (MTL) addresses this challenge by transferring shared knowledge from source tasks to target tasks. As an dominant parameter-efficient fine-tuning method, prompt tuning (PT) enhances MTL by introducing an adaptable vector that captures task-specific knowledge, which acts as a prefix to the original prompt that preserves shared knowledge, while keeping PLM parameters frozen. However, PT struggles to effectively capture the heterogeneity of task-specific knowledge due to its limited representational capacity. To address this challenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL method built on PT, employing the low-rank representation to model task heterogeneity and a fast-slow weights mechanism where the slow weight encodes shared knowledge, while the fast weight captures task-specific nuances, avoiding the mixing of shared and task-specific knowledge, caused by training low-rank representations from scratch. Moreover, a zero-initialized attention mechanism is introduced to minimize the disruption of immature low-rank components on original prompts during warm-up epochs. Experiments on 16 tasks demonstrate that TA-LoRA achieves state-of-the-art performance in full-data and few-shot settings while maintaining superior parameter efficiency.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models</title>
<link>https://arxiv.org/abs/2505.00010</link>
<guid>https://arxiv.org/abs/2505.00010</guid>
<content:encoded><![CDATA[
<div> keywords: Jailbreaking, Large Language Models, Detection, Predictive Models, Education

Summary:
The study focuses on detecting jailbreaks in Large Language Models (LLMs) used in clinical education platforms like 2-Sigma. Over 2,300 prompts in 158 conversations were annotated to identify linguistic variables correlated with jailbreak behavior. Various predictive models were trained using these features, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Feature-based models outperformed Prompt Engineering, with the Fuzzy Decision Tree showing the best performance. The study suggests that linguistic-feature-based models are effective and explainable for jailbreak detection. Future research could explore hybrid frameworks combining prompt-based flexibility and rule-based robustness for real-time jailbreak monitoring in educational LLMs.<br /><br />Summary: <div>
arXiv:2505.00010v1 Announce Type: new 
Abstract: Jailbreaking in Large Language Models (LLMs) threatens their safe use in sensitive domains like education by allowing users to bypass ethical safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical education platform that simulates patient interactions using LLMs. We annotated over 2,300 prompts across 158 conversations using four linguistic variables shown to correlate strongly with jailbreak behavior. The extracted features were used to train several predictive models, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Results show that feature-based predictive models consistently outperformed Prompt Engineering, with the Fuzzy Decision Tree achieving the best overall performance. Our findings demonstrate that linguistic-feature-based models are effective and explainable alternatives for jailbreak detection. We suggest future work explore hybrid frameworks that integrate prompt-based flexibility with rule-based robustness for real-time, spectrum-based jailbreak monitoring in educational LLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?</title>
<link>https://arxiv.org/abs/2505.00012</link>
<guid>https://arxiv.org/abs/2505.00012</guid>
<content:encoded><![CDATA[
<div> Keywords: qualitative research, AI Co-Ethnographer, automation, code assignments, pattern discovery

Summary: <br /><br />The AI Co-Ethnographer (AICoE) is a new end-to-end pipeline developed to enhance qualitative research processes by going beyond automating code assignments. AICoE streamlines the entire qualitative research process, including open coding, code consolidation, code application, and pattern discovery. By offering a more integrated approach, AICoE enables researchers to efficiently analyze qualitative data while maintaining analytical depth. This innovative tool is designed to address the scalability challenges faced in qualitative research and provides a comprehensive solution for researchers looking to improve their analysis processes. With AICoE, qualitative researchers can enhance their workflow and achieve a deeper understanding of their data through streamlined and efficient data analysis techniques. <div>
arXiv:2505.00012v1 Announce Type: new 
Abstract: Qualitative research often involves labor-intensive processes that are difficult to scale while preserving analytical depth. This paper introduces The AI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for qualitative research and designed to move beyond the limitations of simply automating code assignments, offering a more integrated approach. AICoE organizes the entire process, encompassing open coding, code consolidation, code application, and even pattern discovery, leading to a comprehensive analysis of qualitative data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa</title>
<link>https://arxiv.org/abs/2505.00013</link>
<guid>https://arxiv.org/abs/2505.00013</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion detection, Japanese text, language models, DeBERTa-v3-large, model performance

Summary:<br /><br />This study focuses on accurate emotion detection in Japanese text, addressing resource scarcity and class imbalance challenges. The objective is to predict eight Plutchik emotions in Japanese sentences using language models. The WRIME corpus is utilized to transform intensity scores into binary labels for model training. Four pre-trained language models are fine-tuned, with DeBERTa-v3-large achieving the highest mean accuracy and F1-score compared to others. The model shows consistent performance across both high-frequency and low-frequency emotions. Large language models (LLMs) such as ChatGPT-4o and TinySwallow-1.5B-Instruct lag behind in performance. The DeBERTa-v3-large model is released as a pip-installable package for binary emotion classification in Japanese. Future research directions include expanding data for rare emotions, optimizing model size, and exploring prompt engineering to enhance LLM performance. <div>
arXiv:2505.00013v1 Announce Type: new 
Abstract: Background Practical applications such as social media monitoring and customer-feedback analysis require accurate emotion detection for Japanese text, yet resource scarcity and class imbalance hinder model performance.
  Objective This study aims to build a high-accuracy model for predicting the presence or absence of eight Plutchik emotions in Japanese sentences.
  Methods Using the WRIME corpus, we transform reader-averaged intensity scores into binary labels and fine-tune four pre-trained language models (BERT, RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and F1-score serve as evaluation metrics.
  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score (0.662), outperforming all other models. It maintains robust F1 across both high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions (e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.
  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most reliable solution for binary emotion classification in Japanese. We release this model as a pip-installable package (pip install deberta-emotion-predictor). Future work should augment data for rare emotions, reduce model size, and explore prompt engineering to improve LLM performance.
  This manuscript is under review for possible publication in New Generation Computing.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and M\"obius Strips</title>
<link>https://arxiv.org/abs/2505.00014</link>
<guid>https://arxiv.org/abs/2505.00014</guid>
<content:encoded><![CDATA[
<div> embedding, geometry, manifold, triplet loss, NLP

Summary:
- The article introduces a new framework that constrains sentence embeddings to lie on continuous manifolds such as the unit sphere, torus, and M\"obius strip using triplet loss.
- By enforcing differential geometric constraints on the output space, the approach aims to encourage learning of embeddings that are both discriminative and topologically structured.
- The method is evaluated on benchmark datasets AG News and MBTI, outperforming traditional approaches like TF-IDF, Word2Vec, and unconstrained Keras-derived embeddings in clustering quality and classification performance.
- Manifold-constrained embeddings, particularly those on spheres and M\"obius strips, show significant improvement in both Silhouette Score and Accuracy.
- The findings suggest the importance of embedding in manifold space, where topological structure enhances semantic separation, providing a new direction for geometric representation learning in Natural Language Processing.

<br /><br />Summary: <div>
arXiv:2505.00014v1 Announce Type: new 
Abstract: Recent advances in representation learning have emphasized the role of embedding geometry in capturing semantic structure. Traditional sentence embeddings typically reside in unconstrained Euclidean spaces, which may limit their ability to reflect complex relationships in language. In this work, we introduce a novel framework that constrains sentence embeddings to lie on continuous manifolds -- specifically the unit sphere, torus, and M\"obius strip -- using triplet loss as the core training objective. By enforcing differential geometric constraints on the output space, our approach encourages the learning of embeddings that are both discriminative and topologically structured.
  We evaluate our method on benchmark datasets (AG News and MBTI) and compare it to classical baselines including TF-IDF, Word2Vec, and unconstrained Keras-derived embeddings. Our results demonstrate that manifold-constrained embeddings, particularly those projected onto spheres and M\"obius strips, significantly outperform traditional approaches in both clustering quality (Silhouette Score) and classification performance (Accuracy). These findings highlight the value of embedding in manifold space -- where topological structure complements semantic separation -- offering a new and mathematically grounded direction for geometric representation learning in NLP.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation</title>
<link>https://arxiv.org/abs/2505.00015</link>
<guid>https://arxiv.org/abs/2505.00015</guid>
<content:encoded><![CDATA[
<div> Keywords: road traffic accidents, Bangladesh, automated system, Large Language Models, web scraping

Summary:
Road traffic accidents in Bangladesh are a significant issue due to manual and unreliable data collection methods. This research proposes an automated system utilizing Large Language Models (LLMs) and web scraping to improve data accuracy. The system includes automated code generation for web scraping, news collection, accident classification, and duplicate removal. The LLM Gemini-2.0-Flash is used for seamless automation. Over a period of 111 days, the system processed 15,000 news articles and identified 705 unique accidents. Chittagong reported the highest number of accidents, fatalities, and injuries. Peak accident times were in the morning, noon, and evening. The study showcases the effectiveness of LLM-powered systems in collecting accurate accident data for informed road safety policymaking in Bangladesh.<br /><br />Summary: <div>
arXiv:2505.00015v1 Announce Type: new 
Abstract: Road traffic accidents remain a major public safety and socio-economic issue in developing countries like Bangladesh. Existing accident data collection is largely manual, fragmented, and unreliable, resulting in underreporting and inconsistent records. This research proposes a fully automated system using Large Language Models (LLMs) and web scraping techniques to address these challenges. The pipeline consists of four components: automated web scraping code generation, news collection from online sources, accident news classification with structured data extraction, and duplicate removal. The system uses the multimodal generative LLM Gemini-2.0-Flash for seamless automation. The code generation module classifies webpages into pagination, dynamic, or infinite scrolling categories and generates suitable Python scripts for scraping. LLMs also classify and extract key accident information such as date, time, location, fatalities, injuries, road type, vehicle types, and pedestrian involvement. A deduplication algorithm ensures data integrity by removing duplicate reports. The system scraped 14 major Bangladeshi news sites over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news articles and identifying 705 unique accidents. The code generation module achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the highest number of accidents (80), fatalities (70), and injuries (115), followed by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning (8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also developed with usage instructions. This study demonstrates the viability of an LLM-powered, scalable system for accurate, low-effort accident data collection, providing a foundation for data-driven road safety policymaking in Bangladesh.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.00016</link>
<guid>https://arxiv.org/abs/2505.00016</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-SQL, large language models, table reasoning, reinforcement learning, interpretability <br />
Summary: 
This work presents a new approach to the Text-to-SQL task, aiming to teach large language models (LLMs) to reason over tabular data. The framework consists of two stages: first, creating detailed chain-of-thought (CoT) traces from SQL queries to teach the model how to manipulate table fields, and second, using a Group Relative Policy Optimization (GRPO) objective in reinforcement learning to encourage generalizable reasoning. The approach improves performance on Text-to-SQL benchmarks and achieves significant gains on reasoning-intensive datasets like BIRD and CRT-QA. The results show that utilizing SQL as a scaffold for learning enhances generalization and interpretability in reasoning over structured data. Notably, the distilled-quantized LLaMA model showed a 20% accuracy increase on Text-to-SQL tasks, while Qwen achieved a 5% increase. Overall, this research highlights the potential of leveraging SQL for teaching robust reasoning capabilities to LLMs in the context of structured data. <br /><br />Summary: <div>
arXiv:2505.00016v1 Announce Type: new 
Abstract: This work reframes the Text-to-SQL task as a pathway for teaching large language models (LLMs) to reason over and manipulate tabular data--moving beyond the traditional focus on query generation. We propose a two-stage framework that leverages SQL supervision to develop transferable table reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT) traces from real-world SQL queries, providing step-by-step, clause-level supervision that teaches the model how to traverse, filter, and aggregate table fields. Second, we introduce a Group Relative Policy Optimization (GRPO) reinforcement learning objective that connects SQL execution accuracy to generalizable reasoning by encouraging steps that extend beyond task-specific syntax and transfer across datasets. Empirically, our approach improves performance on standard Text-to-SQL benchmarks and achieves substantial gains on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced generalization and interpretability. Specifically, the distilled-quantized LLaMA model achieved a 20\% increase in accuracy when trained on Text-to-SQL tasks, while Qwen achieved a 5\% increase. These results suggest that SQL can serve not only as a target formalism but also as an effective scaffold for learning robust, transferable reasoning over structured data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation</title>
<link>https://arxiv.org/abs/2505.00017</link>
<guid>https://arxiv.org/abs/2505.00017</guid>
<content:encoded><![CDATA[
<div> Keywords: cell type annotation, large language models, differential genes, multi task workflow, semantic similarity.

Summary:
Our work presents a novel approach using large language models (LLMs) for precise and automated cell type annotation. We have developed a graph structured feature marker database that retrieves entities related to differential genes, enabling accurate cell reconstruction. Through a multi task workflow, we optimized the annotation process, resulting in improved human evaluation scores by up to 0.21 and a 6.1% increase in semantic similarity across 11 tissue types. Our method not only outperforms general purpose LLMs but also aligns closely with the cognitive logic of manual annotation, providing more reliable and efficient cell type identification. <div>
arXiv:2505.00017v1 Announce Type: new 
Abstract: To enable precise and fully automated cell type annotation with large language models (LLMs), we developed a graph structured feature marker database to retrieve entities linked to differential genes for cell reconstruction. We further designed a multi task workflow to optimize the annotation process. Compared to general purpose LLMs, our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while more closely aligning with the cognitive logic of manual annotation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study on Prompt Compression for Large Language Models</title>
<link>https://arxiv.org/abs/2505.00019</link>
<guid>https://arxiv.org/abs/2505.00019</guid>
<content:encoded><![CDATA[
<div> methods, prompt compression, Large Language Models, performance, evaluation <br />
Summary: <br />
This paper explores six prompt compression methods for Large Language Models (LLMs) to reduce computational complexity and costs without compromising response quality. The study covers various aspects including generation performance, model hallucinations, effectiveness in multimodal tasks, word omission analysis, and more. Evaluation across 13 datasets, including news, scientific articles, QA, and VQA datasets, shows that prompt compression has a greater impact on LLM performance in long contexts. Surprisingly, moderate compression in the Longbench evaluation even enhances LLM performance. The experiments highlight the importance of efficient prompt engineering for LLMs in different tasks and provide insights into optimizing prompt length for better performance. The code and data used in the study are also made available for reference. <div>
arXiv:2505.00019v1 Announce Type: new 
Abstract: Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks. However, lengthy prompts significantly increase computational complexity and economic costs. To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality. In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones. In the Longbench evaluation, moderate compression even enhances LLM performance. Our code and data is available at https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Public Access in LLM Pre-Training Data</title>
<link>https://arxiv.org/abs/2505.00020</link>
<guid>https://arxiv.org/abs/2505.00020</guid>
<content:encoded><![CDATA[
<div> membership inference attack, copyrighted content, language models, O'Reilly Media, training data

Summary: The study used a dataset of copyrighted O'Reilly Media books to investigate if OpenAI's language models were trained on copyrighted content without consent. GPT-4o showed strong recognition of paywalled O'Reilly book content, while GPT-3.5 Turbo showed greater recognition of publicly accessible content. However, GPT-4o Mini, a smaller model, showed no knowledge of O'Reilly content. Testing multiple models helped account for potential language shifts over time. The results underscore the necessity for increased corporate transparency in disclosing pre-training data sources to develop formal licensing frameworks for AI content training. 

Summary: <br /><br /> <div>
arXiv:2505.00020v1 Announce Type: new 
Abstract: Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we apply the DE-COP membership inference attack method to investigate whether OpenAI's large language models were trained on copyrighted content without consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable model, demonstrates strong recognition of paywalled O'Reilly book content (AUROC = 82\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast, GPT-3.5 Turbo shows greater relative recognition of publicly accessible O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge of public or non-public O'Reilly Media content when tested (AUROC $\approx$ 50\%). Testing multiple models, with the same cutoff date, helps us account for potential language shifts over time that might bias our findings. These results highlight the urgent need for increased corporate transparency regarding pre-training data sources as a means to develop formal licensing frameworks for AI content training
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss</title>
<link>https://arxiv.org/abs/2505.00021</link>
<guid>https://arxiv.org/abs/2505.00021</guid>
<content:encoded><![CDATA[
<div> Keywords: imbalanced data, food hazard detection, data augmentation, transformer-based models, NLP-based classification<br />
Summary:<br />
Classification tasks in food hazard detection are often hindered by imbalanced data distributions and short, unstructured text. This study addresses these challenges by utilizing data augmentation techniques and transformer-based models such as BERT and RoBERTa. By employing strategies like Easy Data Augmentation (EDA) and focal loss, the classification performance is significantly improved. EDA proves to be effective in mitigating class imbalance, enhancing accuracy and F1 scores. Combining focal loss with oversampling and EDA further bolsters model robustness, particularly for challenging examples. These findings contribute to the advancement of NLP-based classification models for food hazard detection, showcasing the importance of innovative approaches in overcoming data distribution issues in classification tasks. <br /><br />Summary: <div>
arXiv:2505.00021v1 Announce Type: new 
Abstract: Classification tasks often suffer from imbal- anced data distribution, which presents chal- lenges in food hazard detection due to severe class imbalances, short and unstructured text, and overlapping semantic categories. In this paper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection, which ad- dresses these issues by applying data augmenta- tion techniques to improve classification perfor- mance. We utilize transformer-based models, BERT and RoBERTa, as backbone classifiers and explore various data balancing strategies, including random oversampling, Easy Data Augmentation (EDA), and focal loss. Our ex- periments show that EDA effectively mitigates class imbalance, leading to significant improve- ments in accuracy and F1 scores. Furthermore, combining focal loss with oversampling and EDA further enhances model robustness, par- ticularly for hard-to-classify examples. These findings contribute to the development of more effective NLP-based classification models for food hazard detection.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation</title>
<link>https://arxiv.org/abs/2505.00022</link>
<guid>https://arxiv.org/abs/2505.00022</guid>
<content:encoded><![CDATA[
<div> dataset curation pipeline, German-language, data quality, LLM pre-training datasets, synthetic data generation  
Summary:
A dataset curation pipeline for German-language datasets was developed to improve data quality for large language models (LLMs). The pipeline combines heuristic and model-based filtering techniques with synthetic data generation to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset sourced from Common Crawl web data, FineWeb2, and synthetically-generated data. Pre-training experiments on an 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT) using Aleph-Alpha-GermanWeb showed significant performance gains over FineWeb2 alone on German-language benchmarks. The study highlights the effectiveness of model-based data curation and synthetic data generation in enhancing LLM pre-training datasets.  
<br /><br />Summary: <div>
arXiv:2505.00022v1 Announce Type: new 
Abstract: Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORG: Generating Answers from Complex, Interrelated Contexts</title>
<link>https://arxiv.org/abs/2505.00023</link>
<guid>https://arxiv.org/abs/2505.00023</guid>
<content:encoded><![CDATA[
<div> Keywords: real-world corpus, language models, Context Organizer, interrelationships, disambiguation

Summary: 
The article introduces Context Organizer (CORG), a framework designed to address the complexities of interrelationships in a real-world corpus. These interrelationships often lead to inconsistencies in knowledge due to various factors like ambiguous naming and outdated information. CORG classifies relationships into distracting, ambiguous, counterfactual, and duplicated categories, aiming to effectively manage these complexities simultaneously. The framework consists of a graph constructor, reranker, and aggregator, allowing for the organization of multiple contexts into independently processed groups. Results show that CORG outperforms existing grouping methods, striking a balance between performance and efficiency, and achieving comparable results to more computationally intensive, single-context approaches.  <br /><br />Summary: <div>
arXiv:2505.00023v1 Announce Type: new 
Abstract: In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning</title>
<link>https://arxiv.org/abs/2505.00024</link>
<guid>https://arxiv.org/abs/2505.00024</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, tool usage, reasoning, reinforcement learning, benchmark evaluation

Summary:
Nemotron-Research-Tool-N1 series of language models utilize rule-based reinforcement learning to enhance tool usage capabilities. Unlike previous methods, these models are optimized with a binary reward system that evaluates the structural validity and functional correctness of tool invocations. This approach allows the models to internalize reasoning strategies autonomously without the need for annotated reasoning trajectories. Experimental results on BFCL and API-Bank benchmarks demonstrate that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B models, built on Qwen-2.5-7B/14B-Instruct, outperform GPT-4o, achieving state-of-the-art results on both evaluations.<br /><br />Summary: <div>
arXiv:2505.00024v1 Announce Type: new 
Abstract: Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text generation tasks. Prior work typically enhances tool-use abilities by either applying supervised fine-tuning (SFT) to enforce tool-call correctness or distilling reasoning traces from stronger models for SFT. However, both approaches fall short, either omitting reasoning entirely or producing imitative reasoning that limits generalization. Inspired by the success of DeepSeek-R1 in eliciting reasoning through rule-based reinforcement learning, we develop the Nemotron-Research-Tool-N1 series of tool-using language models using a similar training paradigm. Instead of restrictively supervising intermediate reasoning traces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized with a binary reward that evaluates only the structural validity and functional correctness of tool invocations. This lightweight supervision allows the model to autonomously internalize reasoning strategies, without the need for annotated reasoning trajectories. Experiments on the BFCL and API-Bank benchmarks show that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve state-of-the-art results, outperforming GPT-4o on both evaluations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1</title>
<link>https://arxiv.org/abs/2505.00025</link>
<guid>https://arxiv.org/abs/2505.00025</guid>
<content:encoded><![CDATA[
<div> Keywords: lightweight medical vertical large language model, knowledge acquisition, model compression, computational optimization, medical question-answering datasets

Summary:
This paper introduces an efficient lightweight medical large language model architecture method designed to overcome the challenges of applying large models in medical scenarios. The proposed method addresses the issues of knowledge acquisition, model compression, and computational optimization. It includes a knowledge transfer pipeline from a teacher model to a student model, compression techniques like weight quantization, and inference optimization strategies. Experimental results demonstrate that the approach maintains accuracy while reducing memory consumption by 64.7% and inference latency by 12.4%. This solution enables the application of large models in resource-constrained environments such as edge computing devices.<br /><br />Summary: <div>
arXiv:2505.00025v1 Announce Type: new 
Abstract: In recent years, despite foundation models like DeepSeek-R1 and ChatGPT demonstrating significant capabilities in general tasks, professional knowledge barriers, computational resource requirements, and deployment environment limitations have severely hindered their application in actual medical scenarios. Addressing these challenges, this paper proposes an efficient lightweight medical vertical large language model architecture method, systematically solving the lightweight problem of medical large models from three dimensions: knowledge acquisition, model compression, and computational optimization. At the knowledge acquisition level, a knowledge transfer pipeline is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology is adopted to precisely adjust key attention layers. At the model compression level, compression techniques including 4-bit weight quantization are implemented while preserving the core representation ability for medical reasoning. At the computational optimization level, inference optimization techniques such as Flash Attention acceleration and continuous batching are integrated, and a professional prompt template system is constructed to adapt to different types of medical problems. Experimental results on medical question-answering datasets show that the method proposed in this paper maintains professional accuracy while reducing memory consumption by 64.7\% and inference latency by 12.4\%, providing an effective solution for the application of medical large models in resource-constrained environments such as edge computing devices.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theory of Mind in Large Language Models: Assessment and Enhancement</title>
<link>https://arxiv.org/abs/2505.00026</link>
<guid>https://arxiv.org/abs/2505.00026</guid>
<content:encoded><![CDATA[
<div> Keywords: Theory of Mind, Large Language Models, evaluation benchmarks, strategies, research directions

Summary: 
This paper discusses the Theory of Mind (ToM) capabilities of Large Language Models (LLMs) and the importance of enhancing their ability to interpret and respond to human mental states. The authors review evaluation benchmarks and strategies used to improve ToM in LLMs, focusing on story-based benchmarks. They provide an in-depth analysis of methods aimed at enhancing ToM in LLMs and suggest promising future research directions based on recent benchmarks and state-of-the-art approaches. This survey serves as a valuable resource for researchers looking to advance LLMs' ToM capabilities.<br /><br />Summary: <div>
arXiv:2505.00026v1 Announce Type: new 
Abstract: Theory of Mind (ToM)-the ability to infer and reason about others' mental states-is fundamental to human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, it is crucial to assess and enhance their capacity to interpret and respond to human mental states. In this paper, we review LLMs' ToM capabilities by examining both evaluation benchmarks and the strategies designed to improve them. We focus on widely adopted story-based benchmarks and provide an in-depth analysis of methods aimed at enhancing ToM in LLMs. Furthermore, we outline promising future research directions informed by recent benchmarks and state-of-the-art approaches. Our survey serves as a valuable resource for researchers interested in advancing LLMs' ToM capabilities.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts</title>
<link>https://arxiv.org/abs/2505.00027</link>
<guid>https://arxiv.org/abs/2505.00027</guid>
<content:encoded><![CDATA[
<div> dimension, trees, natural language, query, precision

Summary:
This paper presents a novel approach for automatically discovering subject, action, object, and adverbial dimensions from texts to support efficient text operations and natural language queries. The approach constructs high-quality trees that represent subjects, actions, objects, and adverbials and their subclass relations within texts. These trees are independent, ensuring no redundant representation. The expressiveness of the trees allows for accessing the majority of sentences, supporting natural language queries. Experimental results show that the abstraction trees constructed have high precision, recall, and F1-scores. The approach is applied to support querying in natural language, demonstrating high coverage of different question patterns. By searching multiple trees based on the question pattern, the approach efficiently reduces the search space and enables precise text operations. <div>
arXiv:2505.00027v1 Announce Type: new 
Abstract: This paper proposed an approach to automatically discovering subject dimension, action dimension, object dimension and adverbial dimension from texts to efficiently operate texts and support query in natural language. The high quality of trees guarantees that all subjects, actions, objects and adverbials and their subclass relations within texts can be represented. The independency of trees ensures that there is no redundant representation between trees. The expressiveness of trees ensures that the majority of sentences can be accessed from each tree and the rest of sentences can be accessed from at least one tree so that the tree-based search mechanism can support querying in natural language. Experiments show that the average precision, recall and F1-score of the abstraction trees constructed by the subclass relations of subject, action, object and adverbial are all greater than 80%. The application of the proposed approach to supporting query in natural language demonstrates that different types of question patterns for querying subject or object have high coverage of texts, and searching multiple trees on subject, action, object and adverbial according to the question pattern can quickly reduce search space to locate target sentences, which can support precise operation on texts.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.00028</link>
<guid>https://arxiv.org/abs/2505.00028</guid>
<content:encoded><![CDATA[
<div> end-to-end, speech-to-speech, dialogue systems, Retrieval-Augmented Generation, knowledge integration <br />
Summary: 
The article introduces a novel framework for end-to-end speech-to-speech dialogue systems that addresses the challenge of incorporating external knowledge. This framework, based on Retrieval-Augmented Generation (RAG), directly retrieves relevant textual knowledge from speech queries, eliminating the need for intermediate speech-to-text conversion. Experimental results show significant performance improvements and higher retrieval efficiency. While the overall performance is still below that of cascaded models, this approach shows promise in enhancing knowledge integration in end-to-end systems. The authors plan to release the code and dataset to facilitate reproducibility and encourage further research in this area. <div>
arXiv:2505.00028v1 Announce Type: new 
Abstract: In recent years, end-to-end speech-to-speech (S2S) dialogue systems have garnered increasing research attention due to their advantages over traditional cascaded systems, including achieving lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these end-to-end systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries, eliminating the need for intermediate speech-to-text conversion via techniques like ASR. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. We will release the code and dataset to support reproducibility and promote further research in this area.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting</title>
<link>https://arxiv.org/abs/2505.00029</link>
<guid>https://arxiv.org/abs/2505.00029</guid>
<content:encoded><![CDATA[
<div> Approach, Structured Dialogue Fine-Tuning, Domain-specific knowledge, Catastrophic forgetting, Multi-domain<br />
Summary:
Structured Dialogue Fine-Tuning (SDFT) is introduced as an approach to injecting domain-specific knowledge into Large Vision Language Models (LLMs) while minimizing catastrophic forgetting. The method consists of three phases: Foundation Preservation, Contrastive Disambiguation, and Knowledge Specialization. Foundation Preservation reinforces pre-trained visual-linguistic alignment, while Contrastive Disambiguation helps maintain semantic boundaries through counterfactual examples. Knowledge Specialization embeds specialized information through chain-of-thought reasoning. Experimental results show the effectiveness of SDFT in balancing specialized knowledge acquisition with general capability retention. Key contributions include a data-centric dialogue template, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types.<br /><br /> <div>
arXiv:2505.00029v1 Announce Type: new 
Abstract: Large Vision Language Models have demonstrated impressive versatile capabilities through extensive multimodal pre-training, but face significant limitations when incorporating specialized knowledge domains beyond their training distribution. These models struggle with a fundamental dilemma: direct adaptation approaches that inject domain-specific knowledge often trigger catastrophic forgetting of foundational visual-linguistic abilities. We introduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that effectively injects domain-specific knowledge while minimizing catastrophic forgetting. Drawing inspiration from supervised fine-tuning in LLMs and subject-driven personalization in text-to-image diffusion models, our method employs a three-phase dialogue structure: Foundation Preservation reinforces pre-trained visual-linguistic alignment through caption tasks; Contrastive Disambiguation introduces carefully designed counterfactual examples to maintain semantic boundaries; and Knowledge Specialization embeds specialized information through chain-of-thought reasoning. Experimental results across multiple domains confirm SDFT's effectiveness in balancing specialized knowledge acquisition with general capability retention. Our key contributions include a data-centric dialogue template that balances foundational alignment with targeted knowledge integration, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Language Models Represent the Past without Anachronism?</title>
<link>https://arxiv.org/abs/2505.00030</link>
<guid>https://arxiv.org/abs/2505.00030</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, anachronism, period prose, fine-tuning, historical perspectives

Summary: 
Language models are increasingly used to simulate historical perspectives, but the risk of anachronism remains a concern. Prompting contemporary models with period prose does not produce authentic historical style outputs. Fine-tuning the model can generate convincing results, but human evaluators can still differentiate between model outputs and genuine historical text. The study suggests that pretraining models on period prose may be necessary to accurately simulate historical perspectives for social research.<br /><br />Summary: <div>
arXiv:2505.00030v1 Announce Type: new 
Abstract: Before researchers can use language models to simulate the past, they need to understand the risk of anachronism. We find that prompting a contemporary model with examples of period prose does not produce output consistent with period style. Fine-tuning produces results that are stylistically convincing enough to fool an automated judge, but human evaluators can still distinguish fine-tuned model outputs from authentic historical text. We tentatively conclude that pretraining on period prose may be required in order to reliably simulate historical perspectives for social research.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving</title>
<link>https://arxiv.org/abs/2505.00031</link>
<guid>https://arxiv.org/abs/2505.00031</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, self-training algorithm, anticipatory plans, problem-solving, natural language reasoning

Summary:<br /><br />In this paper, the authors introduce a novel self-training algorithm called LEarning to Plan before Answering (LEPA) for large language models (LLMs). LEPA focuses on generating anticipatory plans before delving into complex problem-solving tasks, drawing inspiration from cognitive science. By first formulating abstract meta-knowledge through anticipatory plans, LLMs can better generalize across similar problems and avoid getting lost in irrelevant details. The algorithm refines plans through self-reflection and trains the LLM to predict both plans and corresponding solutions. By effectively utilizing anticipatory plans, LEPA outperforms conventional algorithms on various challenging natural language reasoning benchmarks. This approach highlights the importance of incorporating high-level abstraction in problem-solving to enhance the performance of LLMs. 

<br /><br />Summary: <div>
arXiv:2505.00031v1 Announce Type: new 
Abstract: In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only produce step-by-step problem solutions, and fail to capture the abstract meta-knowledge necessary for generalization across similar problems. Drawing insights from cognitive science, where humans employ high-level abstraction to simplify complex problems before delving into specifics, we introduce a novel self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving, before engaging with the intricacies of problems. This approach not only outlines the solution generation path but also shields the LLM from the distraction of irrelevant details. During data generation, LEPA first crafts an anticipatory plan based on the problem, and then generates a solution that aligns with both the plan and the problem. LEPA refines the plan through self-reflection, aiming to acquire plans that are instrumental in yielding correct solutions. During model optimization, the LLM is trained to predict both the refined plans and the corresponding solutions. By efficiently extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable superiority over conventional algorithms on various challenging natural language reasoning benchmarks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis</title>
<link>https://arxiv.org/abs/2505.00032</link>
<guid>https://arxiv.org/abs/2505.00032</guid>
<content:encoded><![CDATA[
<div> Keywords: Major depressive disorder, MDD-LLM, AI-driven framework, UK Biobank cohort, fine-tuned large language models

Summary: 
The paper introduces a new high-performance tool for diagnosing Major Depressive Disorder (MDD) called MDD-LLM, which uses fine-tuned large language models and real-world samples. A total of 274,348 individual records from the UK Biobank cohort were used to train and evaluate the AI-driven framework. The study shows that MDD-LLM outperforms existing machine learning and deep learning frameworks for MDD diagnosis, achieving an accuracy of 0.8378 and an AUC of 0.8919. Factors influencing the performance of the method, such as tabular data transformation techniques and fine-tuning strategies, were also examined. This research addresses the inadequacies in MDD diagnosis due to limited access to medical resources and complex diagnostic methods, offering a promising solution for improving the detection of this prevalent mental health disorder.

Summary: <div>
arXiv:2505.00032v1 Announce Type: new 
Abstract: Major depressive disorder (MDD) impacts more than 300 million people worldwide, highlighting a significant public health issue. However, the uneven distribution of medical resources and the complexity of diagnostic methods have resulted in inadequate attention to this disorder in numerous countries and regions. This paper introduces a high-performance MDD diagnosis tool named MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models (LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis. Therefore, we select 274,348 individual information from the UK Biobank cohort to train and evaluate the proposed method. Specifically, we select 274,348 individual records from the UK Biobank cohort and design a tabular data transformation method to create a large corpus for training and evaluating the proposed approach. To illustrate the advantages of MDD-LLM, we perform comprehensive experiments and provide several comparative analyses against existing model-based solutions across multiple evaluation metrics. Experimental results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of 0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine learning and deep learning frameworks for MDD diagnosis. Given the limited exploration of LLMs in MDD diagnosis, we examine numerous factors that may influence the performance of our proposed method, such as tabular data transformation techniques and different fine-tuning strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models</title>
<link>https://arxiv.org/abs/2505.00033</link>
<guid>https://arxiv.org/abs/2505.00033</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral generative modeling, natural language processing, Fourier dictionary, transformer architectures, Gaussian Mixture Model (GMM)

Summary: 
This article introduces a novel spectral generative modeling framework for natural language processing. It proposes a method that learns a global time-varying Fourier dictionary and per-token mixing coefficients to replace the traditional self-attention mechanism in transformer architectures. By incorporating reconstruction losses in both the time and frequency domains, as well as fitting a Gaussian Mixture Model prior over the learned mixing vectors, the approach achieves competitive perplexity and generation quality on standard benchmarks. Unlike self-attention, which has quadratic computation complexity, this method operates with linear complexity, leading to significant efficiency gains. The results show that spectral dictionary models can perform comparably to transformer baselines while reducing inference latency and memory usage, providing a promising alternative for scalable language modeling.
<br /><br />Summary: <div>
arXiv:2505.00033v1 Announce Type: new 
Abstract: We propose a novel spectral generative modeling framework for natural language processing that jointly learns a global time varying Fourier dictionary and per token mixing coefficients, replacing the ubiquitous self attention mechanism in transformer architectures. By enforcing reconstruction losses in both the time domain (embedding reconstruction) and the frequency domain (via Short Time Fourier Transform magnitude matching) alongside a standard language modeling objective, and fitting a Gaussian Mixture Model (GMM) prior over the learned mixing vectors, our approach achieves competitive perplexity and generation quality on standard benchmarks such as WikiText2 and Penn Treebank. In contrast to the quadratic computation complexity of self attention, our method operates with linear complexity, delivering substantial efficiency gains. We demonstrate that spectral dictionary models can achieve competitive performance compared to transformer baselines while significantly reducing inference latency and memory footprint, offering a compelling alternative for scalable language modeling.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Phishing Email Detection Performance of Small Large Language Models</title>
<link>https://arxiv.org/abs/2505.00034</link>
<guid>https://arxiv.org/abs/2505.00034</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, phishing email detection, small-parameter models, prompt engineering, model ensemble

Summary: 
Small-parameter large language models (LLMs) are being explored for phishing email detection to reduce computational costs. However, these models often perform poorly compared to their larger counterparts. To address this issue, researchers have proposed several methods to enhance the capabilities of small LLMs, including prompt engineering, explanation augmented fine-tuning, and model ensemble. Through experiments, these methods have proven to significantly improve the accuracy of phishing email detection on the SpamAssassin dataset, increasing from 0.5 for baseline models to 0.976 for the enhanced models. This research demonstrates the potential of small-parameter LLMs in phishing email detection tasks and highlights the importance of incorporating innovative techniques to enhance their performance. <br /><br />Summary: <div>
arXiv:2505.00034v1 Announce Type: new 
Abstract: Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving accuracy on the SpamAssassin dataset from around 0.5 for baseline models like Qwen2.5-1.5B-Instruct to 0.976.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics</title>
<link>https://arxiv.org/abs/2505.00035</link>
<guid>https://arxiv.org/abs/2505.00035</guid>
<content:encoded><![CDATA[
<div> increase, vocabulary diversity, rhyme density, thematic content, sentiment analysis
Summary:
Vocabulary diversity in hip-hop lyrics increased by 23.7% over four decades, with East Coast artists showing higher lexical variation. Rhyme density also rose by 34.2%, with Midwest artists displaying the highest technical complexity. There was a shift in thematic content, as social justice themes decreased while introspective themes increased. Lyrics became more negative during sociopolitical crises, with a decrease in polarity following major social unrest. The analysis identified four distinct stylistic approaches correlating with geographic origin and time period, highlighting the evolution of hip-hop as an art form reflecting societal dynamics. This study provides quantitative evidence of the interplay between linguistic innovation and cultural context in popular music.<br /><br />Summary: <div>
arXiv:2505.00035v1 Announce Type: new 
Abstract: This paper presents a comprehensive computational framework for analyzing linguistic complexity and socio-cultural trends in hip-hop lyrics. Using a dataset of 3,814 songs from 146 influential artists spanning four decades (1980-2020), we employ natural language processing techniques to quantify multiple dimensions of lyrical complexity. Our analysis reveals a 23.7% increase in vocabulary diversity over the study period, with East Coast artists demonstrating 17.3% higher lexical variation than other regions. Rhyme density increased by 34.2% across all regions, with Midwest artists exhibiting the highest technical complexity (3.04 rhymes per line). Topic modeling identified significant shifts in thematic content, with social justice themes decreasing from 28.5% to 13.8% of content while introspective themes increased from 7.6% to 26.3%. Sentiment analysis demon- strated that lyrics became significantly more negative during sociopolitical crises, with polarity decreasing by 0.31 following major social unrest. Multi-dimensional analysis revealed four dis- tinct stylistic approaches that correlate strongly with geographic origin (r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish quantitative evidence for the evolution of hip- hop as both an art form and a reflection of societal dynamics, providing insights into the interplay between linguistic innovation and cultural context in popular music.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies</title>
<link>https://arxiv.org/abs/2505.00036</link>
<guid>https://arxiv.org/abs/2505.00036</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, political persuasion, survey experiments, persuasive capabilities, campaign methods 

Summary: 
In recent years, concerns have arisen about the potential threat posed by Large Language Models (LLMs) to democratic societies due to their persuasive abilities. The study conducted two survey experiments and a real-world simulation to compare the cost-effectiveness of using LLM chatbots for political persuasion compared to traditional campaign methods. The experiments, involving over 10,000 participants across three political domains, revealed that LLMs are as persuasive as actual campaign ads once voters are exposed to them. However, political persuasion also depends on the impact of the message post-exposure. Based on simulations, the study estimated that LLM-based persuasion costs less per persuaded voter compared to traditional methods, but the scalability of traditional methods currently surpasses that of LLMs. While LLMs do not currently have a significant advantage in large-scale political persuasion, their potential may increase as their capabilities improve and exposure to persuasive LLMs becomes more scalable.

<br /><br />Summary: <div>
arXiv:2505.00036v1 Announce Type: new 
Abstract: In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the "receive" and "accept" steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter compared to \$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyPerAlign: Hypotheses-driven Personalized Alignment</title>
<link>https://arxiv.org/abs/2505.00038</link>
<guid>https://arxiv.org/abs/2505.00038</guid>
<content:encoded><![CDATA[
<div> personalization, language models, user-specific, alignment algorithms, hypotheses-driven

Summary:<br />
The paper introduces a novel approach called HyPerAlign for personalizing outputs of large language models to individual users by inferring hypotheses about their communication strategies, personality, and writing style. This approach aims to generate customized responses tailored to individual users rather than generic outputs aligned to average user preferences. Experimental results on authorship attribution and deliberative alignment tasks using diverse datasets show that the HyPerAlign approach outperforms preference-based fine-tuning methods. It achieves up to a 70% improvement in the helpfulness of LLM models for deliberative alignment and consistently high win rates (>90%) against state-of-the-art preference fine-tuning approaches for LLM personalization in authorship attribution. Overall, HyPerAlign is described as an interpretable and sample-efficient strategy for personalizing LLM models to individual users. 

Summary: <div>
arXiv:2505.00038v1 Announce Type: new 
Abstract: Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations that reflect their intended real-world use cases. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the ``average-user'' preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users, aiming to generate customized responses tailored to individual users, instead of generic outputs that emulate the collective voices of diverse populations. We propose a novel interpretable and sample-efficient hypotheses-driven personalization approach (HyPerAlign) where given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality and writing style, then prompt LLM models with these hypotheses and user specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks), and demonstrate the superiority of hypotheses-driven personalization approach when compared to preference-based fine-tuning methods. For deliberative alignment, the helpfulness of LLM models is improved by up to $70\%$ on average. For authorship attribution, results indicate consistently high win-rates (commonly $>90\%$) against state-of-the-art preference fine-tuning approaches for LLM personalization across diverse user profiles and LLM models. Overall, our approach represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph RAG for Legal Norms: A Hierarchical and Temporal Approach</title>
<link>https://arxiv.org/abs/2505.00039</link>
<guid>https://arxiv.org/abs/2505.00039</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Retrieval Augmented Generation, legal norms, knowledge graphs, hierarchical structure, temporal evolution

Summary:<br /><br />
This article introduces an adaptation of Graph Retrieval Augmented Generation (Graph RAG) tailored for analyzing legal norms. Legal norms are complex with hierarchical structures, extensive references, and multiple versions. Graph RAG combines structured knowledge graphs with contextual text segments to tackle the complexities of legal data. By integrating hierarchical structure, temporal evolution, and comprehensive Text Units into knowledge graphs, Graph RAG enhances the representation of legal knowledge. The article delves into the application of Graph RAG on legal norm datasets, aiming to advance the field of Artificial Intelligence in the legal domain. This innovation opens avenues for more effective systems in legal research, legislative analysis, and decision support.<br /><br />Summary: <div>
arXiv:2505.00039v1 Announce Type: new 
Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms, which are characterized by their predefined hierarchical structure, extensive network of internal and external references and multiple temporal versions. By combining structured knowledge graphs with contextually enriched text segments, Graph RAG offers a promising solution to address the inherent complexity and vast volume of legal data. The integration of hierarchical structure and temporal evolution into knowledge graphs - along with the concept of comprehensive Text Units - facilitates the construction of richer, interconnected representations of legal knowledge. Through a detailed analysis of Graph RAG and its application to legal norm datasets, this article aims to significantly advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective systems in legal research, legislative analysis, and decision support.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Base Models Beat Aligned Models at Randomness and Creativity</title>
<link>https://arxiv.org/abs/2505.00047</link>
<guid>https://arxiv.org/abs/2505.00047</guid>
<content:encoded><![CDATA[
<div> alignment, LLM development, reinforcement learning, unpredictable outputs, base language models

Summary:<br /><br />Alignment has become a standard practice in language model development, but it may not always be beneficial. The study reveals that base language models outperform aligned models in tasks requiring unpredictable outputs like random number generation, mixed strategy games, and creative writing. Aligned models tend to exhibit narrow behaviors, such as favoring specific numbers in random number generation or predictability in game states. While aligned models excel in common benchmarks, they struggle in tasks demanding originality and creativity. The findings suggest a trade-off between performance on standard benchmarks and tasks that require broader capabilities. <div>
arXiv:2505.00047v1 Announce Type: new 
Abstract: Alignment has quickly become a default ingredient in LLM development, with techniques such as reinforcement learning from human feedback making models act safely, follow instructions, and perform ever-better on complex tasks. While these techniques are certainly useful, we propose that they should not be universally applied and demonstrate a range of tasks on which base language models consistently outperform their popular aligned forms. Particularly, we study tasks that require unpredictable outputs, such as random number generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and creative writing. In each case, aligned models tend towards narrow behaviors that result in distinct disadvantages, for instance, preferring to generate "7" over other uniformly random numbers, becoming almost fully predictable in some game states, or prioritizing pleasant writing over creative originality. Across models tested, better performance on common benchmarks tends to correlate with worse performance on our tasks, suggesting an effective trade-off in the required capabilities.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting</title>
<link>https://arxiv.org/abs/2505.00050</link>
<guid>https://arxiv.org/abs/2505.00050</guid>
<content:encoded><![CDATA[
<div> fashion trends, social media sentiment, Twitter data, sentiment analysis, machine learning <br />
<br />
Summary: This study explores the relationship between fashion trends and social media sentiment using Twitter data and the T4SA dataset. Through natural language processing and machine learning, the research examines how sentiment in fashion-related social media discussions can predict emerging trends. The analysis involves identifying fashion-related content, improving sentiment classification normalization, time series decomposition, statistically validating causal relationships, comparing sentiment across platforms, and analyzing brand-specific sentiment. Results show correlations between sentiment patterns and fashion theme popularity, with accessories and streetwear themes experiencing significant upward trends. The Granger causality analysis identifies sustainability and streetwear as key trend drivers, showing bidirectional relationships with other themes. The study demonstrates that social media sentiment analysis can effectively predict fashion trend trajectories with proper statistical validation. The predictive model achieved 78.35% balanced accuracy in sentiment classification, providing a reliable basis for trend prediction across sentiment categories. <br /> <div>
arXiv:2505.00050v1 Announce Type: new 
Abstract: This study explores the intersection of fashion trends and social media sentiment through computational analysis of Twitter data using the T4SA (Twitter for Sentiment Analysis) dataset. By applying natural language processing and machine learning techniques, we examine how sentiment patterns in fashion-related social media conversations can serve as predictors for emerging fashion trends. Our analysis involves the identification and categorization of fashion-related content, sentiment classification with improved normalization techniques, time series decomposition, statistically validated causal relationship modeling, cross-platform sentiment comparison, and brand-specific sentiment analysis. Results indicate correlations between sentiment patterns and fashion theme popularity, with accessories and streetwear themes showing statistically significant rising trends. The Granger causality analysis establishes sustainability and streetwear as primary trend drivers, showing bidirectional relationships with several other themes. The findings demonstrate that social media sentiment analysis can serve as an effective early indicator of fashion trend trajectories when proper statistical validation is applied. Our improved predictive model achieved 78.35% balanced accuracy in sentiment classification, establishing a reliable foundation for trend prediction across positive, neutral, and negative sentiment categories.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity</title>
<link>https://arxiv.org/abs/2505.00056</link>
<guid>https://arxiv.org/abs/2505.00056</guid>
<content:encoded><![CDATA[
<div> Keywords: meme clustering, toxicity detection, virality modeling, multi-dimensional similarity, template-based matching

Summary: 
This paper presents a novel method for clustering Internet memes based on multi-dimensional similarity features derived from template-based matching. The traditional approaches to meme clustering often rely on predefined databases and overlook semantic nuances, leading to challenges in handling the diversity of memes. In contrast, the proposed method utilizes local and global features across different similarity categories such as form, visual content, text, and identity, resulting in more consistent and coherent meme clusters. By incorporating adaptive matching capabilities, the approach outperforms existing methods and aligns with human intuition. The code for implementing the method is publicly available to support further research. The significant contributions of this work lie in addressing the challenges of meme clustering by leveraging multi-dimensional similarity features and template-based matching, ultimately enhancing the accuracy and adaptability of clustering processes. 

<br /><br />Summary: <div>
arXiv:2505.00056v1 Announce Type: new 
Abstract: Meme clustering is critical for toxicity detection, virality modeling, and typing, but it has received little attention in previous research. Clustering similar Internet memes is challenging due to their multimodality, cultural context, and adaptability. Existing approaches rely on databases, overlook semantics, and struggle to handle diverse dimensions of similarity. This paper introduces a novel method that uses template-based matching with multi-dimensional similarity features, thus eliminating the need for predefined databases and supporting adaptive matching. Memes are clustered using local and global features across similarity categories such as form, visual content, text, and identity. Our combined approach outperforms existing clustering methods, producing more consistent and coherent clusters, while similarity-based feature sets enable adaptability and align with human intuition. We make all supporting code publicly available to support subsequent research. Code: https://github.com/tygobl/meme-clustering
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Report on the llms evaluating the high school questions</title>
<link>https://arxiv.org/abs/2505.00057</link>
<guid>https://arxiv.org/abs/2505.00057</guid>
<content:encoded><![CDATA[
<div> evaluation, large language models, high school science questions, education, performance

Summary:<br />
- The report evaluates the performance of large language models (LLMs) in solving high school science questions.
- It explores the potential applications of LLMs in the educational field.
- Mathematics exam questions from college entrance examinations were used as evaluation data.
- The study utilized at least eight LLM APIs to provide answers and conducted a comprehensive assessment based on various metrics.
- The findings highlight the strengths and weaknesses of LLMs in handling high school science questions, emphasizing the need for improvement in logical reasoning and creative problem-solving.

<br /><br />Summary: <div>
arXiv:2505.00057v1 Announce Type: new 
Abstract: This report aims to evaluate the performance of large language models (LLMs) in solving high school science questions and to explore their potential applications in the educational field. With the rapid development of LLMs in the field of natural language processing, their application in education has attracted widespread attention. This study selected mathematics exam questions from the college entrance examinations (2019-2023) as evaluation data and utilized at least eight LLM APIs to provide answers. A comprehensive assessment was conducted based on metrics such as accuracy, response time, logical reasoning, and creativity. Through an in-depth analysis of the evaluation results, this report reveals the strengths and weaknesses of LLMs in handling high school science questions and discusses their implications for educational practice. The findings indicate that although LLMs perform excellently in certain aspects, there is still room for improvement in logical reasoning and creative problem-solving. This report provides an empirical foundation for further research and application of LLMs in the educational field and offers suggestions for improvement.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition</title>
<link>https://arxiv.org/abs/2505.00059</link>
<guid>https://arxiv.org/abs/2505.00059</guid>
<content:encoded><![CDATA[
<div> dataset, distance, accents, emotion, speech recognition <br />
Summary:
The article introduces the BERSt dataset, which aims to address the challenges of distanced speech recognition. The dataset includes nearly 4 hours of English speech from 98 actors with various accents and emotions, recorded in different acoustic environments using smartphones. It offers data for evaluating ASR, shout detection, and SER tasks. Initial benchmarks show that ASR performance degrades with distance and shout level, and varies depending on the intended emotion. The dataset poses challenges for both ASR and SER tasks, highlighting the need for improving the robustness of speech recognition systems for real-world applications. <div>
arXiv:2505.00059v1 Announce Type: new 
Abstract: Some speech recognition tasks, such as automatic speech recognition (ASR), are approaching or have reached human performance in many reported metrics. Yet, they continue to struggle in complex, real-world, situations, such as with distanced speech. Previous challenges have released datasets to address the issue of distanced ASR, however, the focus remains primarily on distance, specifically relying on multi-microphone array systems. Here we present the B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset contains almost 4 hours of English speech from 98 actors with varying regional and non-native accents. The data was collected on smartphones in the actors homes and therefore includes at least 98 different acoustic environments. The data also includes 7 different emotion prompts and both shouted and spoken utterances. The smartphones were places in 19 different positions, including obstructions and being in a different room than the actor. This data is publicly available for use and can be used to evaluate a variety of speech recognition tasks, including: ASR, shout detection, and speech emotion recognition (SER). We provide initial benchmarks for ASR and SER tasks, and find that ASR degrades both with an increase in distance and shout level and shows varied performance depending on the intended emotion. Our results show that the BERSt dataset is challenging for both ASR and SER tasks and continued work is needed to improve the robustness of such systems for more accurate real-world use.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5</title>
<link>https://arxiv.org/abs/2505.00060</link>
<guid>https://arxiv.org/abs/2505.00060</guid>
<content:encoded><![CDATA[
<div> evaluation framework, semantic accuracy, large language models, SQL generation, business intelligence

Summary:<br />
The study introduces a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of Large Language Models (LLMs) in generating SQL queries for Business Intelligence (BI) tasks. Using Exaone 3.5, a bilingual LLM optimized for enterprise tasks, the framework evaluates model performance based on answer accuracy, execution success rate, semantic error rate, and non-response rate. A domain-specific benchmark comprising 219 natural language business questions across varying SQL complexity levels is used, derived from LG Electronics' internal BigQuery sales data. Results show Exaone 3.5 excels in simple aggregation tasks but struggles in arithmetic reasoning and grouped ranking tasks, with errors concentrated in complex cases such as misapplied arithmetic logic and incomplete filtering. The findings emphasize the current limitations of LLMs in business environments and advocate for fact-consistency validation layers and hybrid reasoning approaches to enhance natural language interfaces for enterprise data systems.<br /> <div>
arXiv:2505.00060v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promise in enabling natural language interfaces for structured data querying through text-to-SQL generation. However, their application in real-world Business Intelligence (BI) contexts remains limited due to semantic hallucinations, structural errors, and a lack of domain-specific evaluation frameworks. In this study, we propose a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM optimized for enterprise tasks. We construct a domain-specific benchmark comprising 219 natural language business questions across five SQL complexity levels, derived from actual sales data in LG Electronics' internal BigQuery environment. Each question is paired with a gold-standard SQL query and a validated ground-truth answer. We evaluate model performance using answer accuracy, execution success rate, semantic error rate, and non-response rate. Experimental results show that while Exaone 3.5 performs well on simple aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4), with semantic errors and non-responses concentrated in complex cases. Qualitative error analysis further identifies common failure types such as misapplied arithmetic logic, incomplete filtering, and incorrect grouping operations. Our findings highlight the current limitations of LLMs in business-critical environments and underscore the need for fact-consistency validation layers and hybrid reasoning approaches. This work contributes a reproducible benchmark and evaluation methodology for advancing reliable natural language interfaces to structured enterprise data systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems</title>
<link>https://arxiv.org/abs/2505.00061</link>
<guid>https://arxiv.org/abs/2505.00061</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer-based, automated short-answer grading systems, vulnerabilities, adversarial gaming strategies, AI-driven educational tools

Summary:
This study explores vulnerabilities in transformer-based automated short-answer grading systems used in medical education and how they can be manipulated through adversarial gaming strategies. Three main types of gaming strategies were identified that exploit these vulnerabilities, potentially leading to false positives. Adversarial training methods were implemented to enhance the systems' robustness, significantly reducing susceptibility to manipulations. Ensemble techniques such as majority voting and ridge regression further improved the system's defense against sophisticated adversarial inputs. Large language models like GPT-4 with varied prompting techniques proved effective in recognizing and scoring gaming strategies. Continuous improvements in AI-driven educational tools are crucial to ensure reliability and fairness in high-stakes settings.
<br /><br />Summary: This study investigates vulnerabilities in transformer-based automated short-answer grading systems used in medical education. Three main types of gaming strategies were identified, leading to false positives. Adversarial training methods and ensemble techniques improved the systems' robustness and defense against manipulations. Large language models like GPT-4 effectively recognized and scored gaming strategies. Continuous improvements in AI-driven educational tools are essential for reliability and fairness. <div>
arXiv:2505.00061v1 Announce Type: new 
Abstract: This study examines vulnerabilities in transformer-based automated short-answer grading systems used in medical education, with a focus on how these systems can be manipulated through adversarial gaming strategies. Our research identifies three main types of gaming strategies that exploit the system's weaknesses, potentially leading to false positives. To counteract these vulnerabilities, we implement several adversarial training methods designed to enhance the systems' robustness. Our results indicate that these methods significantly reduce the susceptibility of grading systems to such manipulations, especially when combined with ensemble techniques like majority voting and ridge regression, which further improve the system's defense against sophisticated adversarial inputs. Additionally, employing large language models such as GPT-4 with varied prompting techniques has shown promise in recognizing and scoring gaming strategies effectively. The findings underscore the importance of continuous improvements in AI-driven educational tools to ensure their reliability and fairness in high-stakes settings.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling</title>
<link>https://arxiv.org/abs/2505.00063</link>
<guid>https://arxiv.org/abs/2505.00063</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal large language models, document-specific tasks, visual complexity, reasoning complexity

Summary:<br />
The article introduces a General Document Intelligence Benchmark (GDI-Bench) to evaluate the capabilities of multimodal large language models (MLLMs) across various document-specific tasks. This benchmark features 1.9k images and 19 tasks structured by visual and reasoning complexity, allowing for performance assessment by difficulty. The GDI-Bench was evaluated on open-source and closed-source models, revealing strengths and weaknesses in both visual and reasoning domains. The GPT-4o model excelled in reasoning tasks but showed limitations in visual capabilities. To address diverse tasks, the article proposes a GDI Model that mitigates catastrophic forgetting during supervised fine-tuning by preserving intelligence. This model achieved state-of-the-art performance on previous benchmarks and the GDI-Bench. Both the benchmark and model will be open source.<br />Summary: <div>
arXiv:2505.00063v1 Announce Type: new 
Abstract: The rapid advancement of multimodal large language models (MLLMs) has profoundly impacted the document domain, creating a wide array of application scenarios. This progress highlights the need for a comprehensive benchmark to evaluate these models' capabilities across various document-specific tasks. However, existing benchmarks often fail to locate specific model weaknesses or guide systematic improvements. To bridge this gap, we introduce a General Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key scenarios and 19 document-specific tasks. By decoupling visual complexity and reasoning complexity, the GDI-Bench structures graded tasks that allow performance assessment by difficulty, aiding in model weakness identification and optimization guidance. We evaluate the GDI-Bench on various open-source and closed-source models, conducting decoupled analyses in the visual and reasoning domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits limitations in visual capabilities. To address the diverse tasks and domains in the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic forgetting during the supervised fine-tuning (SFT) process through a intelligence-preserving training strategy. Our model achieves state-of-the-art performance on previous benchmarks and the GDI-Bench. Both our benchmark and model will be open source.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConSens: Assessing context grounding in open-book question answering</title>
<link>https://arxiv.org/abs/2505.00065</link>
<guid>https://arxiv.org/abs/2505.00065</guid>
<content:encoded><![CDATA[
<div> language models, question answering, evaluation metric, context utilization, open-book QA

Summary:
An evaluation metric is proposed for large language models in open-book question answering, aiming to assess the extent to which model responses rely on the provided context rather than parametric knowledge. The metric contrasts the model's perplexity when the context is provided versus when it is not, offering a quantifiable score for context utilization. Experimental results demonstrate the effectiveness of the metric in identifying context-grounded answers. Unlike existing methods, the proposed metric is computationally efficient, interpretable, and adaptable to various use cases, providing a scalable and practical solution for assessing context utilization in open-book QA systems. <div>
arXiv:2505.00065v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated considerable success in open-book question answering (QA), where the task requires generating answers grounded in a provided external context. A critical challenge in open-book QA is to ensure that model responses are based on the provided context rather than its parametric knowledge, which can be outdated, incomplete, or incorrect. Existing evaluation methods, primarily based on the LLM-as-a-judge approach, face significant limitations, including biases, scalability issues, and dependence on costly external systems. To address these challenges, we propose a novel metric that contrasts the perplexity of the model response under two conditions: when the context is provided and when it is not. The resulting score quantifies the extent to which the model's answer relies on the provided context. The validity of this metric is demonstrated through a series of experiments that show its effectiveness in identifying whether a given answer is grounded in the provided context. Unlike existing approaches, this metric is computationally efficient, interpretable, and adaptable to various use cases, offering a scalable and practical solution to assess context utilization in open-book QA systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese</title>
<link>https://arxiv.org/abs/2505.00114</link>
<guid>https://arxiv.org/abs/2505.00114</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Lebanese dialect, fine-tuning, cultural authenticity, LebEval benchmark

Summary: 
This study explores the effectiveness of Large Language Models (LLMs) for translating the low-resource Lebanese dialect, focusing on the influence of culturally authentic data versus larger translated datasets. Three fine-tuning methods were compared: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Results showed that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperformed those trained on larger, non-native data. The best outcomes were observed with contrastive fine-tuning combined with contrastive prompting, highlighting the importance of exposing translation models to mistakes. The researchers introduced LebEval, a new benchmark derived from native Lebanese content, and compared it to the existing FLoRes benchmark. These findings challenge the prevailing belief that "More Data is Better" and underscore the critical role of cultural authenticity in dialectal translation. The datasets and code used in the study are available on Github. 

<br /><br />Summary: <div>
arXiv:2505.00114v1 Announce Type: new 
Abstract: This paper examines the effectiveness of Large Language Models (LLMs) in translating the low-resource Lebanese dialect, focusing on the impact of culturally authentic data versus larger translated datasets. We compare three fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Experiments reveal that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperform those trained on larger, non-native data. The best results were achieved through contrastive fine-tuning paired with contrastive prompting, which indicates the benefits of exposing translation models to bad examples. In addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark derived from native Lebanese content, and compare it to the existing FLoRes benchmark. Our findings challenge the "More Data is Better" paradigm and emphasize the crucial role of cultural authenticity in dialectal translation. We made our datasets and code available on Github.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs</title>
<link>https://arxiv.org/abs/2505.00127</link>
<guid>https://arxiv.org/abs/2505.00127</guid>
<content:encoded><![CDATA[
<div> overthinking, response length, problem difficulty, reasoning behavior, self-awareness

Summary:
Large language models (LLMs) have been optimized for long reasoning, assuming it leads to better performance. However, this study found that LLMs tend to overthink simple problems, resulting in unnecessarily long responses, while underthinking harder problems, leading to decreased accuracy. The research suggests that models may misjudge the difficulty of problems and fail to adjust response length accordingly. By using a preference optimization algorithm to reduce response length without considering answer correctness, the study showed that accuracy could be maintained while significantly reducing generation length. These findings emphasize the importance of considering response length as a signal for reasoning behavior and suggest the need for further exploration into LLMs' self-awareness and adaptation of reasoning length. 

Summary: <div>
arXiv:2505.00127v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models</title>
<link>https://arxiv.org/abs/2505.00147</link>
<guid>https://arxiv.org/abs/2505.00147</guid>
<content:encoded><![CDATA[
<div> AdaptMI, adaptive approach, skill-based in-context learning, small language models, cognitive load theory <br />
<br />
Summary: 
In the study of in-context learning (ICL) for language models, the focus is on improving problem-solving capabilities by providing relevant information in context, similar to human learning from teachers. While leveraging large language models' metacognition has shown improved performance in ICL, the same approach has not yielded significant gains in small language models (SLMs). The research identifies a performance gap in ICL capabilities for SLMs, attributing it to cognitive overload caused by unnecessary information in skill-based prompting. To address this issue, the researchers propose AdaptMI, an adaptive method of selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory, AdaptMI introduces skill-based examples only when the model struggles, leading to improved accuracy. Furthermore, the study introduces AdaptMI+, which provides targeted examples to address specific missing skills in the model's responses, resulting in an accuracy improvement of up to 6% across various math benchmarks and SLMs. <br /><br /> <div>
arXiv:2505.00147v1 Announce Type: new 
Abstract: In-context learning (ICL) allows a language model to improve its problem-solving capability when provided with suitable information in context. Since the choice of in-context information can be determined based on the problem itself, in-context learning is analogous to human learning from teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that ICL performance can be improved by leveraging a frontier large language model's (LLM) ability to predict required skills to solve a problem, popularly referred to as an LLM's metacognition, and using the recommended skills to construct necessary in-context examples. While this skill-based strategy boosts ICL performance in larger models, its gains on small language models (SLMs) have been minimal, highlighting a performance gap in ICL capabilities. We investigate this gap and show that skill-based prompting can hurt SLM performance on easy questions by introducing unnecessary information, akin to cognitive overload. To address this, we introduce AdaptMI, an adaptive approach to selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory from human pedagogy, our method only introduces skill-based examples when the model performs poorly. We further propose AdaptMI+, which adds examples targeted to the specific skills missing from the model's responses. On 5-shot evaluations across popular math benchmarks and five SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over naive skill-based strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports</title>
<link>https://arxiv.org/abs/2505.00191</link>
<guid>https://arxiv.org/abs/2505.00191</guid>
<content:encoded><![CDATA[
<div> framework, radiology reports, AI-based methods, interpretability, MIMIC-CXR dataset<br />
Summary:<br />
The paper introduces an interpretable-by-design framework for classifying radiology reports using AI-based methods. The framework focuses on selecting a set of informative queries from reports to predict a diagnosis, improving accuracy and efficiency in medical diagnosis. By using the Information Pursuit framework to extract queries, the Flan-T5 model to identify facts in reports, and a classifier to predict diseases, the proposed method demonstrates effectiveness in experiments on the MIMIC-CXR dataset. This approach aims to address the lack of interpretability in current AI methods for radiology reports, potentially increasing trust and usability in medical AI applications. <div>
arXiv:2505.00191v1 Announce Type: new 
Abstract: The development of AI-based methods for analyzing radiology reports could lead to significant advances in medical diagnosis--from improving diagnostic accuracy to enhancing efficiency and reducing workload. However, the lack of interpretability in these methods has hindered their adoption in clinical settings. In this paper, we propose an interpretable-by-design framework for classifying radiology reports. The key idea is to extract a set of most informative queries from a large set of reports and use these queries and their corresponding answers to predict a diagnosis. Thus, the explanation for a prediction is, by construction, the set of selected queries and answers. We use the Information Pursuit framework to select informative queries, the Flan-T5 model to determine if facts are present in the report, and a classifier to predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the effectiveness of the proposed method, highlighting its potential to enhance trust and usability in medical AI.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring</title>
<link>https://arxiv.org/abs/2505.00261</link>
<guid>https://arxiv.org/abs/2505.00261</guid>
<content:encoded><![CDATA[
<div> Keywords: Korean language education, learner corpora, grammatical error correction, KoLLA corpus, automated error correction

Summary: 
- The article discusses the lack of learner corpora tailored to Korean L2 writing despite growing global interest in Korean language education.
- To address this gap, the KoLLA Korean learner corpus has been enhanced by adding multiple grammatical error correction references, allowing for more nuanced evaluation of GEC systems.
- The enhancements also include rubric-based scores aligned with guidelines from the Korean National Language Institute, covering grammatical accuracy, coherence, and lexical diversity.
- These improvements make KoLLA a robust and standardized resource for research in Korean L2 education, supporting advancements in language learning and assessment.
- The enriched corpus can also benefit automated error correction systems in providing more accurate feedback to language learners. 

<br /><br />Summary: <div>
arXiv:2505.00261v1 Announce Type: new 
Abstract: Despite growing global interest in Korean language education, there remains a significant lack of learner corpora tailored to Korean L2 writing. To address this gap, we enhance the KoLLA Korean learner corpus by adding multiple grammatical error correction (GEC) references, thereby enabling more nuanced and flexible evaluation of GEC systems, and reflects the variability of human language. Additionally, we enrich the corpus with rubric-based scores aligned with guidelines from the Korean National Language Institute, capturing grammatical accuracy, coherence, and lexical diversity. These enhancements make KoLLA a robust and standardized resource for research in Korean L2 education, supporting advancements in language learning, assessment, and automated error correction.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency in Language Models: Current Landscape, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2505.00268</link>
<guid>https://arxiv.org/abs/2505.00268</guid>
<content:encoded><![CDATA[
<div> Keywords: consistency, language models, AI, research, benchmarks 

Summary: 
This paper delves into the intricacies of consistency in AI language systems, focusing on formal and informal consistency aspects. It highlights the challenges faced by current language models in maintaining reliable consistency, discussing the need for standardized definitions, multilingual assessment, and improved methods. The research gaps identified underscore the necessity for robust benchmarks to measure consistency and interdisciplinary approaches for ensuring the coherent application of language models in specific tasks. The urgent call for addressing these critical gaps emphasizes the importance of consistency in language models while emphasizing adaptability and utility. This analysis shines a light on the crucial role of consistency in effective language use and the imperative need for advancement in this area. 

<br /><br />Summary: <div>
arXiv:2505.00268v1 Announce Type: new 
Abstract: The hallmark of effective language use lies in consistency -- expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models struggle to maintain reliable consistency across different scenarios. This paper examines the landscape of consistency research in AI language systems, exploring both formal consistency (including logical rule adherence) and informal consistency (such as moral and factual coherence). We analyze current approaches to measure aspects of consistency, identify critical research gaps in standardization of definitions, multilingual assessment, and methods to improve consistency. Our findings point to an urgent need for robust benchmarks to measure and interdisciplinary approaches to ensure consistency in the application of language models on domain-specific tasks while preserving the utility and adaptability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation</title>
<link>https://arxiv.org/abs/2505.00339</link>
<guid>https://arxiv.org/abs/2505.00339</guid>
<content:encoded><![CDATA[
<div> framework, AI-driven educational tools, cognitive alignment, linguistic feedback integration, ethical safeguards
Summary:
The paper proposes a framework for enhancing AI-driven educational tools by integrating cognitive assessment frameworks, linguistic analysis of AI-generated feedback, and ethical design principles. The framework consists of three phases: cognitive alignment, linguistic feedback integration, and ethical safeguards. Practical application of the framework is demonstrated through its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz generation. The paper emphasizes the importance of considering the quality, cognitive depth, and ethical implications of AI-generated materials in educational settings. It aims to guide educators, researchers, and developers in leveraging AI's potential while maintaining pedagogical and ethical standards in educational content generation. The comprehensive framework offers actionable steps for creating effective and responsible AI tools in education. 
<br /><br />Summary: <div>
arXiv:2505.00339v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is rapidly transforming education, presenting unprecedented opportunities for personalized learning and streamlined content creation. However, realizing the full potential of AI in educational settings necessitates careful consideration of the quality, cognitive depth, and ethical implications of AI-generated materials. This paper synthesizes insights from four related studies to propose a comprehensive framework for enhancing AI-driven educational tools. We integrate cognitive assessment frameworks (Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated feedback, and ethical design principles to guide the development of effective and responsible AI tools. We outline a structured three-phase approach encompassing cognitive alignment, linguistic feedback integration, and ethical safeguards. The practical application of this framework is demonstrated through its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz generation. This work contributes a comprehensive and actionable guide for educators, researchers, and developers aiming to harness AI's potential while upholding pedagogical and ethical standards in educational content generation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis</title>
<link>https://arxiv.org/abs/2505.00367</link>
<guid>https://arxiv.org/abs/2505.00367</guid>
<content:encoded><![CDATA[
arXiv:2505.00367v1 Announce Type: new 
Abstract: Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass</title>
<link>https://arxiv.org/abs/2505.00389</link>
<guid>https://arxiv.org/abs/2505.00389</guid>
<content:encoded><![CDATA[
arXiv:2505.00389v1 Announce Type: new 
Abstract: As a fundamental task in Information Retrieval and Computational Linguistics, sentence representation has profound implications for a wide range of practical applications such as text clustering, content analysis, question-answering systems, and web search. Recent advances in pre-trained language models (PLMs) have driven remarkable progress in this field, particularly through unsupervised embedding derivation methods centered on discriminative PLMs like BERT. However, due to time and computational constraints, few efforts have attempted to integrate unsupervised sentence representation with generative PLMs, which typically possess much larger parameter sizes. Given that state-of-the-art models in both academia and industry are predominantly based on generative architectures, there is a pressing need for an efficient unsupervised text representation framework tailored to decoder-only PLMs. To address this concern, we propose CSE-SFP, an innovative method that exploits the structural characteristics of generative models. Compared to existing strategies, CSE-SFP requires only a single forward pass to perform effective unsupervised contrastive learning. Rigorous experimentation demonstrates that CSE-SFP not only produces higher-quality embeddings but also significantly reduces both training time and memory consumption. Furthermore, we introduce two ratio metrics that jointly assess alignment and uniformity, thereby providing a more robust means for evaluating the semantic spatial properties of encoding models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red Teaming Large Language Models for Healthcare</title>
<link>https://arxiv.org/abs/2505.00467</link>
<guid>https://arxiv.org/abs/2505.00467</guid>
<content:encoded><![CDATA[
arXiv:2505.00467v1 Announce Type: new 
Abstract: We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Identification of Regulatory Statements in EU Legislation</title>
<link>https://arxiv.org/abs/2505.00479</link>
<guid>https://arxiv.org/abs/2505.00479</guid>
<content:encoded><![CDATA[
arXiv:2505.00479v1 Announce Type: new 
Abstract: Identifying regulatory statements in legislation is useful for developing metrics to measure the regulatory density and strictness of legislation. A computational method is valuable for scaling the identification of such statements from a growing body of EU legislation, constituting approximately 180,000 published legal acts between 1952 and 2023. Past work on extraction of these statements varies in the permissiveness of their definitions for what constitutes a regulatory statement. In this work, we provide a specific definition for our purposes based on the institutional grammar tool. We develop and compare two contrasting approaches for automatically identifying such statements in EU legislation, one based on dependency parsing, and the other on a transformer-based machine learning model. We found both approaches performed similarly well with accuracies of 80% and 84% respectively and a K alpha of 0.58. The high accuracies and not exceedingly high agreement suggests potential for combining strengths of both approaches.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection</title>
<link>https://arxiv.org/abs/2505.00506</link>
<guid>https://arxiv.org/abs/2505.00506</guid>
<content:encoded><![CDATA[
arXiv:2505.00506v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2505.00551</link>
<guid>https://arxiv.org/abs/2505.00551</guid>
<content:encoded><![CDATA[
arXiv:2505.00551v1 Announce Type: new 
Abstract: The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2505.00557</link>
<guid>https://arxiv.org/abs/2505.00557</guid>
<content:encoded><![CDATA[
arXiv:2505.00557v1 Announce Type: new 
Abstract: Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</title>
<link>https://arxiv.org/abs/2505.00570</link>
<guid>https://arxiv.org/abs/2505.00570</guid>
<content:encoded><![CDATA[
arXiv:2505.00570v1 Announce Type: new 
Abstract: Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Block Circulant Adapter for Large Language Models</title>
<link>https://arxiv.org/abs/2505.00582</link>
<guid>https://arxiv.org/abs/2505.00582</guid>
<content:encoded><![CDATA[
arXiv:2505.00582v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation</title>
<link>https://arxiv.org/abs/2505.00624</link>
<guid>https://arxiv.org/abs/2505.00624</guid>
<content:encoded><![CDATA[
arXiv:2505.00624v1 Announce Type: new 
Abstract: Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)</title>
<link>https://arxiv.org/abs/2505.00626</link>
<guid>https://arxiv.org/abs/2505.00626</guid>
<content:encoded><![CDATA[
arXiv:2505.00626v1 Announce Type: new 
Abstract: Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Understanding: an Inherent Ambiguity Barrier</title>
<link>https://arxiv.org/abs/2505.00654</link>
<guid>https://arxiv.org/abs/2505.00654</guid>
<content:encoded><![CDATA[
arXiv:2505.00654v1 Announce Type: new 
Abstract: A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the generalization of language models from in-context learning and finetuning: a controlled study</title>
<link>https://arxiv.org/abs/2505.00661</link>
<guid>https://arxiv.org/abs/2505.00661</guid>
<content:encoded><![CDATA[
arXiv:2505.00661v1 Announce Type: new 
Abstract: Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepCritic: Deliberate Critique with Large Language Models</title>
<link>https://arxiv.org/abs/2505.00662</link>
<guid>https://arxiv.org/abs/2505.00662</guid>
<content:encoded><![CDATA[
arXiv:2505.00662v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions</title>
<link>https://arxiv.org/abs/2505.00675</link>
<guid>https://arxiv.org/abs/2505.00675</guid>
<content:encoded><![CDATA[
arXiv:2505.00675v1 Announce Type: new 
Abstract: Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Large Language Models with Register Analysis for Arbitrary Style Transfer</title>
<link>https://arxiv.org/abs/2505.00679</link>
<guid>https://arxiv.org/abs/2505.00679</guid>
<content:encoded><![CDATA[
arXiv:2505.00679v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in rewriting text across various styles. However, effectively leveraging this ability for example-based arbitrary style transfer, where an input text is rewritten to match the style of a given exemplar, remains an open challenge. A key question is how to describe the style of the exemplar to guide LLMs toward high-quality rewrites. In this work, we propose a prompting method based on register analysis to guide LLMs to perform this task. Empirical evaluations across multiple style transfer tasks show that our prompting approach enhances style transfer strength while preserving meaning more effectively than existing prompting strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications</title>
<link>https://arxiv.org/abs/2505.00049</link>
<guid>https://arxiv.org/abs/2505.00049</guid>
<content:encoded><![CDATA[
arXiv:2505.00049v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly used in human-centered tasks, assessing their psychological traits is crucial for understanding their social impact and ensuring trustworthy AI alignment. While existing reviews have covered some aspects of related research, several important areas have not been systematically discussed, including detailed discussions of diverse psychological tests, LLM-specific psychological datasets, and the applications of LLMs with psychological traits. To address this gap, we systematically review six key dimensions of applying psychological theories to LLMs: (1) assessment tools; (2) LLM-specific datasets; (3) evaluation metrics (consistency and stability); (4) empirical findings; (5) personality simulation methods; and (6) LLM-based behavior simulation. Our analysis highlights both the strengths and limitations of current methods. While some LLMs exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Recognizing methodological challenges such as mismatches between psychological tools and LLMs' capabilities, as well as inconsistencies in evaluation practices, this study aims to propose future directions for developing more interpretable, robust, and generalizable psychological assessment frameworks for LLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques</title>
<link>https://arxiv.org/abs/2505.00105</link>
<guid>https://arxiv.org/abs/2505.00105</guid>
<content:encoded><![CDATA[
arXiv:2505.00105v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation enhances language models by retrieving relevant information from external knowledge bases, relying on high-dimensional vector embeddings typically stored in float32 precision. However, storing these embeddings at scale presents significant memory challenges. To address this issue, we systematically investigate on MTEB benchmark two complementary optimization strategies: quantization, evaluating standard formats (float16, int8, binary) and low-bit floating-point types (float8), and dimensionality reduction, assessing methods like PCA, Kernel PCA, UMAP, Random Projections and Autoencoders. Our results show that float8 quantization achieves a 4x storage reduction with minimal performance degradation (<0.3%), significantly outperforming int8 quantization at the same compression level, being simpler to implement. PCA emerges as the most effective dimensionality reduction technique. Crucially, combining moderate PCA (e.g., retaining 50% dimensions) with float8 quantization offers an excellent trade-off, achieving 8x total compression with less performance impact than using int8 alone (which provides only 4x compression). To facilitate practical application, we propose a methodology based on visualizing the performance-storage trade-off space to identify the optimal configuration that maximizes performance within their specific memory constraints.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.00150</link>
<guid>https://arxiv.org/abs/2505.00150</guid>
<content:encoded><![CDATA[
arXiv:2505.00150v1 Announce Type: cross 
Abstract: The rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. Multimodal memes, often utilized for playful or humorous expressions with visual and textual elements, are sometimes misused to disseminate hate speech against individuals or groups. While the detection of hateful memes is well-researched, developing effective methods to transform hateful content in memes remains a significant challenge. Leveraging the powerful generation and reasoning capabilities of Vision-Language Models (VLMs), we address the tasks of detecting and mitigating hateful content. This paper presents two key contributions: first, a definition-guided prompting technique for detecting hateful memes, and second, a unified framework for mitigating hateful content in memes, named UnHateMeme, which works by replacing hateful textual and/or visual components. With our definition-guided prompts, VLMs achieve impressive performance on hateful memes detection task. Furthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a strong capability to convert hateful memes into non-hateful forms that meet human-level criteria for hate speech and maintain multimodal coherence between image and text. Through empirical experiments, we show the effectiveness of state-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the proposed tasks, providing a comprehensive analysis of their respective strengths and limitations for these tasks. This paper aims to shed light on important applications of VLMs for ensuring safe and respectful online environments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00212</link>
<guid>https://arxiv.org/abs/2505.00212</guid>
<content:encoded><![CDATA[
arXiv:2505.00212v1 Announce Type: cross 
Abstract: Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</title>
<link>https://arxiv.org/abs/2505.00234</link>
<guid>https://arxiv.org/abs/2505.00234</guid>
<content:encoded><![CDATA[
arXiv:2505.00234v1 Announce Type: cross 
Abstract: Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnronQA: Towards Personalized RAG over Private Documents</title>
<link>https://arxiv.org/abs/2505.00263</link>
<guid>https://arxiv.org/abs/2505.00263</guid>
<content:encoded><![CDATA[
arXiv:2505.00263v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has become one of the most popular methods for bringing knowledge-intensive context to large language models (LLM) because of its ability to bring local context at inference time without the cost or data leakage risks associated with fine-tuning. A clear separation of private information from the LLM training has made RAG the basis for many enterprise LLM workloads as it allows the company to augment LLM's understanding using customers' private documents. Despite its popularity for private documents in enterprise deployments, current RAG benchmarks for validating and optimizing RAG pipelines draw their corpora from public data such as Wikipedia or generic web pages and offer little to no personal context. Seeking to empower more personal and private RAG we release the EnronQA benchmark, a dataset of 103,638 emails with 528,304 question-answer pairs across 150 different user inboxes. EnronQA enables better benchmarking of RAG pipelines over private data and allows for experimentation on the introduction of personalized retrieval settings over realistic data. Finally, we use EnronQA to explore the tradeoff in memorization and retrieval when reasoning over private documents.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</title>
<link>https://arxiv.org/abs/2505.00315</link>
<guid>https://arxiv.org/abs/2505.00315</guid>
<content:encoded><![CDATA[
arXiv:2505.00315v1 Announce Type: cross 
Abstract: Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2505.00337</link>
<guid>https://arxiv.org/abs/2505.00337</guid>
<content:encoded><![CDATA[
arXiv:2505.00337v1 Announce Type: cross 
Abstract: Text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. Yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. Existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. To fill this gap, we introduce \textbf{T2VPhysBench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including Newtonian mechanics, conservation principles, and phenomenological effects. Our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. The results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training</title>
<link>https://arxiv.org/abs/2505.00358</link>
<guid>https://arxiv.org/abs/2505.00358</guid>
<content:encoded><![CDATA[
arXiv:2505.00358v1 Announce Type: cross 
Abstract: Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&amp;B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&amp;B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&amp;B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&amp;B matches or exceeds the performance of state-of-the-art data mixing strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training</title>
<link>https://arxiv.org/abs/2505.00422</link>
<guid>https://arxiv.org/abs/2505.00422</guid>
<content:encoded><![CDATA[
arXiv:2505.00422v1 Announce Type: cross 
Abstract: Accurate classification of medical device risk levels is essential for regulatory oversight and clinical safety. We present a Transformer-based multimodal framework that integrates textual descriptions and visual information to predict device regulatory classification. The model incorporates a cross-attention mechanism to capture intermodal dependencies and employs a self-training strategy for improved generalization under limited supervision. Experiments on a real-world regulatory dataset demonstrate that our approach achieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming text-only (77.2%) and image-only (54.8%) baselines. Compared to standard multimodal fusion, the self-training mechanism improved SVM performance by 3.3 percentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1, suggesting that pseudo-labeling can effectively enhance generalization under limited supervision. Ablation studies further confirm the complementary benefits of both cross-modal attention and self-training.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Task Arithmetic for Zero-Shot Information Retrieval</title>
<link>https://arxiv.org/abs/2505.00649</link>
<guid>https://arxiv.org/abs/2505.00649</guid>
<content:encoded><![CDATA[
arXiv:2505.00649v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</title>
<link>https://arxiv.org/abs/2505.00703</link>
<guid>https://arxiv.org/abs/2505.00703</guid>
<content:encoded><![CDATA[
arXiv:2505.00703v1 Announce Type: cross 
Abstract: Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers</title>
<link>https://arxiv.org/abs/2309.08532</link>
<guid>https://arxiv.org/abs/2309.08532</guid>
<content:encoded><![CDATA[
arXiv:2309.08532v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning</title>
<link>https://arxiv.org/abs/2401.15371</link>
<guid>https://arxiv.org/abs/2401.15371</guid>
<content:encoded><![CDATA[
arXiv:2401.15371v4 Announce Type: replace 
Abstract: Legal Judgment Prediction (LJP) is a fundamental task of legal artificial intelligence, aiming to automatically predict the judgment outcomes of legal cases. Existing LJP models primarily focus on identifying legal triggers within criminal fact descriptions by contrastively training language models. However, these LJP models overlook the importance of learning to effectively distinguish subtle differences among judgments, which is crucial for producing more accurate predictions. In this paper, we propose LegalDuet, which continuously pretrains language models to learn a more tailored embedding space for representing legal cases. Specifically, LegalDuet designs a dual-view mechanism to continuously pretrain language models: 1) Law Case Clustering retrieves similar cases as hard negatives and employs contrastive training to differentiate among confusing cases; 2) Legal Decision Matching aims to identify legal clues within criminal fact descriptions to align them with the chain of reasoning that contains the correct legal decision. Our experiments on the CAIL2018 dataset demonstrate the effectiveness of LegalDuet. Further analysis reveals that LegalDuet improves the ability of pretrained language models to distinguish confusing criminal charges by reducing prediction uncertainty and enhancing the separability of criminal charges. The experiments demonstrate that LegalDuet produces a more concentrated and distinguishable embedding space, effectively aligning criminal facts with corresponding legal decisions. The code is available at https://github.com/NEUIR/LegalDuet.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Reasoning" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments</title>
<link>https://arxiv.org/abs/2402.08498</link>
<guid>https://arxiv.org/abs/2402.08498</guid>
<content:encoded><![CDATA[
arXiv:2402.08498v5 Announce Type: replace 
Abstract: Large language models (LLMs) play a key role in generating evidence-based and stylistic counter-arguments, yet their effectiveness in real-world applications has been underexplored. Previous research often neglects the balance between evidentiality and style, which are crucial for persuasive arguments. To address this, we evaluated the effectiveness of stylized evidence-based counter-argument generation in Counterfire, a new dataset of 38,000 counter-arguments generated by revising counter-arguments to Reddit's ChangeMyView community to follow different discursive styles. We evaluated generic and stylized counter-arguments from basic and fine-tuned models such as GPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku, LLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings reveal that humans prefer stylized counter-arguments over the original outputs, with GPT-3.5 Turbo performing well, though still not reaching human standards of rhetorical quality nor persuasiveness. Additionally, our work created a novel argument triplets dataset for studying style control, with human preference labels that provide insights into the tradeoffs between evidence integration and argument quality.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2405.04532</link>
<guid>https://arxiv.org/abs/2405.04532</guid>
<content:encoded><![CDATA[
arXiv:2405.04532v3 Announce Type: replace 
Abstract: Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts</title>
<link>https://arxiv.org/abs/2405.11804</link>
<guid>https://arxiv.org/abs/2405.11804</guid>
<content:encoded><![CDATA[
arXiv:2405.11804v2 Announce Type: replace 
Abstract: Literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents, a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and Bilingual LLM Preference (BLP), which leverages large language models like GPT-4} for direct text comparison. Although TransAgents achieves lower d-BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and GPT-4} translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Review Generation Method Based on Large Language Models</title>
<link>https://arxiv.org/abs/2407.20906</link>
<guid>https://arxiv.org/abs/2407.20906</guid>
<content:encoded><![CDATA[
arXiv:2407.20906v5 Announce Type: replace 
Abstract: Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties. Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5\% with 95\% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges and Future Directions of Data-Centric AI Alignment</title>
<link>https://arxiv.org/abs/2410.01957</link>
<guid>https://arxiv.org/abs/2410.01957</guid>
<content:encoded><![CDATA[
arXiv:2410.01957v2 Announce Type: replace 
Abstract: As AI systems become increasingly capable and influential, ensuring their alignment with human values, preferences, and goals has become a critical research focus. Current alignment methods primarily focus on designing algorithms and loss functions but often underestimate the crucial role of data. This paper advocates for a shift towards data-centric AI alignment, emphasizing the need to enhance the quality and representativeness of data used in aligning AI systems. In this position paper, we highlight key challenges associated with both human-based and AI-based feedback within the data-centric alignment framework. Through qualitative analysis, we identify multiple sources of unreliability in human feedback, as well as problems related to temporal drift, context dependence, and AI-based feedback failing to capture human values due to inherent model limitations. We propose future research directions, including improved feedback collection practices, robust data-cleaning methodologies, and rigorous feedback verification processes. We call for future research into these critical directions to ensure, addressing gaps that persist in understanding and improving data-centric alignment practices.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation</title>
<link>https://arxiv.org/abs/2410.20774</link>
<guid>https://arxiv.org/abs/2410.20774</guid>
<content:encoded><![CDATA[
arXiv:2410.20774v2 Announce Type: replace 
Abstract: In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both single and pairwise evaluation settings. Our findings, based on evaluations using EMBER, reveal that all tested LLM-judges, including GPT-4o, show a notable lack of robustness in the presence of epistemic markers. Specifically, we observe a negative bias toward epistemic markers, with a stronger bias against markers expressing uncertainty. This suggests that LLM-judges are influenced by the presence of these markers and do not focus solely on the correctness of the content.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis</title>
<link>https://arxiv.org/abs/2412.05862</link>
<guid>https://arxiv.org/abs/2412.05862</guid>
<content:encoded><![CDATA[
arXiv:2412.05862v3 Announce Type: replace 
Abstract: In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language directions with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs demonstrate a significant quality gap in specialized translation compared to multilingual encoder-decoder MT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms all evaluated LLMs in the 7-8B parameter range across three out of the four language directions. While fine-tuning improves the performance of LLMs such as Mistral and Llama, these models still underperform compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve high-quality domain-specific translation, especially in medium-resource and low-resource settings. Moreover, the superior performance of larger LLMs over their 8B variants suggests potential value in pre-training domain-specific medium-sized language models, employing targeted data selection and knowledge distillation approaches to enhance both quality and efficiency in specialized translation tasks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods</title>
<link>https://arxiv.org/abs/2501.13947</link>
<guid>https://arxiv.org/abs/2501.13947</guid>
<content:encoded><![CDATA[
arXiv:2501.13947v3 Announce Type: replace 
Abstract: The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models</title>
<link>https://arxiv.org/abs/2502.05945</link>
<guid>https://arxiv.org/abs/2502.05945</guid>
<content:encoded><![CDATA[
arXiv:2502.05945v2 Announce Type: replace 
Abstract: Robust alignment guardrails for large language models are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination for Llama 2. Our method applies fine-grained interventions at specific model subcomponents, particularly attention heads, using a simple binary choice probing strategy. These interventions then generalise to the open-ended generation setting effectively circumventing safety guardrails. We show that probing single attention heads is more effective than intervening on full layers and intervening on only four attention heads is comparable to supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. Our findings highlight the shortcomings of current alignment techniques. In addition, our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviors. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?</title>
<link>https://arxiv.org/abs/2502.07963</link>
<guid>https://arxiv.org/abs/2502.07963</guid>
<content:encoded><![CDATA[
arXiv:2502.07963v2 Announce Type: replace 
Abstract: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation</title>
<link>https://arxiv.org/abs/2502.20984</link>
<guid>https://arxiv.org/abs/2502.20984</guid>
<content:encoded><![CDATA[
arXiv:2502.20984v3 Announce Type: replace 
Abstract: SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement</title>
<link>https://arxiv.org/abs/2503.23895</link>
<guid>https://arxiv.org/abs/2503.23895</guid>
<content:encoded><![CDATA[
arXiv:2503.23895v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opioid Named Entity Recognition (ONER-2025) from Reddit</title>
<link>https://arxiv.org/abs/2504.00027</link>
<guid>https://arxiv.org/abs/2504.00027</guid>
<content:encoded><![CDATA[
arXiv:2504.00027v3 Announce Type: replace 
Abstract: The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem</title>
<link>https://arxiv.org/abs/2403.00108</link>
<guid>https://arxiv.org/abs/2403.00108</guid>
<content:encoded><![CDATA[
arXiv:2403.00108v2 Announce Type: replace-cross 
Abstract: Finetuning LLMs with LoRA has gained significant popularity due to its simplicity and effectiveness. Often, users may even find pluggable, community-shared LoRAs to enhance their base models for a specific downstream task of interest; enjoying a powerful, efficient, yet customized LLM experience with negligible investment. However, this convenient share-and-play ecosystem also introduces a new attack surface, where attackers can distribute malicious LoRAs to a community eager to try out shared assets. Despite the high-risk potential, no prior art has comprehensively explored LoRA's attack surface under the downstream-enhancing share-and-play context. In this paper, we investigate how backdoors can be injected into task-enhancing LoRAs and examine the mechanisms of such infections. We find that with a simple, efficient, yet specific recipe, a backdoor LoRA can be trained once and then seamlessly merged (in a training-free fashion) with multiple task-enhancing LoRAs, retaining both its malicious backdoor and benign downstream capabilities. This allows attackers to scale the distribution of compromised LoRAs with minimal effort by leveraging the rich pool of existing shared LoRA assets. We note that such merged LoRAs are particularly infectious -- because their malicious intent is cleverly concealed behind improved downstream capabilities, creating a strong incentive for voluntary download -- and dangerous -- because under local deployment, no safety measures exist to intervene when things go wrong. Our work is among the first to study this new threat model of training-free distribution of downstream-capable-yet-backdoor-injected LoRAs, highlighting the urgent need for heightened security awareness in the LoRA ecosystem. Warning: This paper contains offensive content and involves a real-life tragedy.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Agent as a Mechanical Designer</title>
<link>https://arxiv.org/abs/2404.17525</link>
<guid>https://arxiv.org/abs/2404.17525</guid>
<content:encoded><![CDATA[
arXiv:2404.17525v3 Announce Type: replace-cross 
Abstract: Conventional mechanical design follows an iterative process in which initial concepts are refined through cycles of expert assessment and resource-intensive Finite Element Method (FEM) analysis to meet performance goals. While machine learning models have been developed to assist in parts of this process, they typically require large datasets, extensive training, and are often tailored to specific tasks, limiting their generalizability. To address these limitations, we propose a framework that leverages a pretrained Large Language Model (LLM) in conjunction with an FEM module to autonomously generate, evaluate, and refine structural designs based on performance specifications and numerical feedback. The LLM operates without domain-specific fine-tuning, using general reasoning to propose design candidates, interpret FEM-derived performance metrics, and apply structurally sound modifications. Using 2D truss structures as a testbed, we show that the LLM can effectively navigate highly discrete and multi-faceted design spaces, balance competing objectives, and identify convergence when further optimization yields diminishing returns. Compared to Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves faster convergence and fewer FEM evaluations. Experiments with varying temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini) indicate that smaller models yield higher constraint satisfaction with fewer steps, while lower temperatures enhance design consistency. These results establish LLMs as a promising new class of reasoning-based, natural language-driven optimizers for autonomous design and iterative structural refinement.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers</title>
<link>https://arxiv.org/abs/2405.04620</link>
<guid>https://arxiv.org/abs/2405.04620</guid>
<content:encoded><![CDATA[
arXiv:2405.04620v5 Announce Type: replace-cross 
Abstract: In this work, we present a generalized formulation of the Transformer algorithm by reinterpreting its core mechanisms within the framework of Path Integral formalism. In this perspective, the attention mechanism is recast as a process that integrates all possible transition paths leading to future token states, with temporal evolution governed by the Feed-Forward Network. By systematically mapping each component of the Transformer to its counterpart in the Path Integral formulation, we obtain a more compact and efficient representation, in which the contextual information of a sequence is condensed into memory-like segments. These segments are recurrently processed across Transformer layers, enabling more effective long-term information retention. We validate the effectiveness of this approach through the Passkey retrieval task and a summarization task, demonstrating that the proposed method preserves historical information while exhibiting memory usage that scales linearly with sequence length. This contrasts with the non-linear memory growth typically observed in standard attention mechanisms. We expect that this quantum-inspired generalization of the Transformer architecture will open new avenues for enhancing both the efficiency and expressiveness of future Transformer models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaeBench: Improving Quality of Toxic Adversarial Examples</title>
<link>https://arxiv.org/abs/2410.05573</link>
<guid>https://arxiv.org/abs/2410.05573</guid>
<content:encoded><![CDATA[
arXiv:2410.05573v2 Announce Type: replace-cross 
Abstract: Toxicity text detectors can be vulnerable to adversarial examples - small perturbations to input text that fool the systems into wrong detection. Existing attack algorithms are time-consuming and often produce invalid or ambiguous adversarial examples, making them less useful for evaluating or improving real-world toxicity content moderators. This paper proposes an annotation pipeline for quality control of generated toxic adversarial examples (TAE). We design model-based automated annotation and human-based quality verification to assess the quality requirements of TAE. Successful TAE should fool a target toxicity model into making benign predictions, be grammatically reasonable, appear natural like human-generated text, and exhibit semantic toxicity. When applying these requirements to more than 20 state-of-the-art (SOTA) TAE attack recipes, we find many invalid samples from a total of 940k raw TAE attack generations. We then utilize the proposed pipeline to filter and curate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically, we demonstrate that TaeBench can effectively transfer-attack SOTA toxicity content moderation models and services. Our experiments also show that TaeBench with adversarial training achieve significant improvements of the robustness of two toxicity detectors.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Personalization and Control in Scientific Personalized Search</title>
<link>https://arxiv.org/abs/2411.02790</link>
<guid>https://arxiv.org/abs/2411.02790</guid>
<content:encoded><![CDATA[
arXiv:2411.02790v2 Announce Type: replace-cross 
Abstract: Personalized search is a problem where models benefit from learning user preferences from per-user historical interaction data. The inferred preferences enable personalized ranking models to improve the relevance of documents for users. However, personalization is also seen as opaque in its use of historical interactions and is not amenable to users' control. Further, personalization limits the diversity of information users are exposed to. While search results may be automatically diversified this does little to address the lack of control over personalization. In response, we introduce a model for personalized search that enables users to control personalized rankings proactively. Our model, CtrlCE, is a novel cross-encoder model augmented with an editable memory built from users' historical interactions. The editable memory allows cross-encoders to be personalized efficiently and enables users to control personalized ranking. Next, because all queries do not require personalization, we introduce a calibrated mixing model which determines when personalization is necessary. This enables users to control personalization via their editable memory only when necessary. To thoroughly evaluate CtrlCE, we demonstrate its empirical performance in four domains of science, its ability to selectively request user control in a calibration evaluation of the mixing model, and the control provided by its editable memory in a user study.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner</title>
<link>https://arxiv.org/abs/2412.18086</link>
<guid>https://arxiv.org/abs/2412.18086</guid>
<content:encoded><![CDATA[
arXiv:2412.18086v2 Announce Type: replace-cross 
Abstract: Motion planning is a crucial component in autonomous driving. State-of-the-art motion planners are trained on meticulously curated datasets, which are not only expensive to annotate but also insufficient in capturing rarely seen critical scenarios. Failing to account for such scenarios poses a significant risk to motion planners and may lead to incidents during testing. An intuitive solution is to manually compose such scenarios by programming and executing a simulator (e.g., CARLA). However, this approach incurs substantial human costs. Motivated by this, we propose an inexpensive method for generating diverse critical traffic scenarios to train more robust motion planners. First, we represent traffic scenarios as scripts, which are then used by the simulator to generate traffic scenarios. Next, we develop a method that accepts user-specified text descriptions, which a Large Language Model translates into scripts using in-context learning. The output scripts are sent to the simulator that produces the corresponding traffic scenarios. As our method can generate abundant safety-critical traffic scenarios, we use them as synthetic training data for motion planners. To demonstrate the value of generated scenarios, we train existing motion planners on our synthetic data, real-world datasets, and a combination of both. Our experiments show that motion planners trained with our data significantly outperform those trained solely on real-world data, showing the usefulness of our synthetic data and the effectiveness of our data generation method. Our source code is available at https://ezharjan.github.io/AutoSceneGen.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency and Effectiveness of LLM-Based Summarization of Evidence in Crowdsourced Fact-Checking</title>
<link>https://arxiv.org/abs/2501.18265</link>
<guid>https://arxiv.org/abs/2501.18265</guid>
<content:encoded><![CDATA[
arXiv:2501.18265v2 Announce Type: replace-cross 
Abstract: Evaluating the truthfulness of online content is critical for combating misinformation. This study examines the efficiency and effectiveness of crowdsourced truthfulness assessments through a comparative analysis of two approaches: one involving full-length webpages as evidence for each claim, and another using summaries for each evidence document generated with a large language model. Using an A/B testing setting, we engage a diverse pool of participants tasked with evaluating the truthfulness of statements under these conditions. Our analysis explores both the quality of assessments and the behavioral patterns of participants. The results reveal that relying on summarized evidence offers comparable accuracy and error metrics to the Standard modality while significantly improving efficiency. Workers in the Summary setting complete a significantly higher number of assessments, reducing task duration and costs. Additionally, the Summary modality maximizes internal agreement and maintains consistent reliance on and perceived usefulness of evidence, demonstrating its potential to streamline large-scale truthfulness evaluations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reinforcement Finetuning via Adaptive Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.05520</link>
<guid>https://arxiv.org/abs/2504.05520</guid>
<content:encoded><![CDATA[
arXiv:2504.05520v2 Announce Type: replace-cross 
Abstract: Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces training time by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models</title>
<link>https://arxiv.org/abs/2504.21012</link>
<guid>https://arxiv.org/abs/2504.21012</guid>
<content:encoded><![CDATA[
<div> Keywords: intuitive thinking, large language models, cognitive dynamics, conceptual fusion, artificial intelligence

Summary:
- The study aims to investigate the cognitive dynamics of humans and large language models (LLMs) in terms of intuitive thinking by proposing a new framework.
- The framework includes Transition-Inducing Prompts (TIP) triggering rapid shifts in LLM responsiveness and Transition Quantifying Prompts (TQP) to evaluate these changes.
- Controlled experiments were conducted to analyze how LLMs react to prompts embedding semantically distant concepts and their linguistic quality and affective tone changes.
- While humans tend to experience heightened engagement with meaningfully blended concepts, LLMs showed no significant difference in responsiveness between fused and non-fused prompts.
- The results suggest that current LLMs may not replicate the conceptual integration processes seen in human intuition, highlighting differences in how intuition and conceptual leaps emerge in artificial versus human minds.

<br /><br />Summary: The study explores the cognitive dynamics of intuitive thinking in humans and large language models (LLMs) using a unique framework. Through controlled experiments, the researchers observed how LLMs react to prompts embedding semantically distant concepts and found that while humans engage more with fused concepts, LLMs show no significant difference. This indicates that LLMs may not yet replicate human intuition processes related to conceptual integration. The findings shed light on the differences in cognitive responsiveness between artificial and human minds. <div>
arXiv:2504.21012v1 Announce Type: new 
Abstract: What underlies intuitive human thinking? One approach to this question is to compare the cognitive dynamics of humans and large language models (LLMs). However, such a comparison requires a method to quantitatively analyze AI cognitive behavior under controlled conditions. While anecdotal observations suggest that certain prompts can dramatically change LLM behavior, these observations have remained largely qualitative. Here, we propose a two-part framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP) that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying Prompt (TQP) that evaluates this change using a separate LLM. Through controlled experiments, we examined how LLMs react to prompts embedding two semantically distant concepts (e.g., mathematical aperiodicity and traditional crafts)--either fused together or presented separately--by changing their linguistic quality and affective tone. Whereas humans tend to experience heightened engagement when such concepts are meaningfully blended producing a novel concept--a form of conceptual fusion--current LLMs showed no significant difference in responsiveness between semantically fused and non-fused prompts. This suggests that LLMs may not yet replicate the conceptual integration processes seen in human intuition. Our method enables fine-grained, reproducible measurement of cognitive responsiveness, and may help illuminate key differences in how intuition and conceptual leaps emerge in artificial versus human minds.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge</title>
<link>https://arxiv.org/abs/2504.21013</link>
<guid>https://arxiv.org/abs/2504.21013</guid>
<content:encoded><![CDATA[
<div> AI-generated feedback, educational settings, linguistic characteristics, Google's Gemini 1.5-flash text model, multiple-choice questions

Summary:
- Study examines linguistic attributes of AI-generated feedback for computer science MCQs
- Analyzed dataset of 1,200 MCQs across difficulty levels and feedback tones
- Computed metrics such as length, readability scores, vocabulary richness, and lexical density
- Trained RoBERTa-based MTL model to predict linguistic properties with low error rates
- Revealed dynamic adaptation of AI-generated feedback based on tone and difficulty levels<br /><br />Summary: <div>
arXiv:2504.21013v1 Announce Type: new 
Abstract: Artificial Intelligence (AI)-generated feedback in educational settings has garnered considerable attention due to its potential to enhance learning outcomes. However, a comprehensive understanding of the linguistic characteristics of AI-generated feedback, including readability, lexical richness, and adaptability across varying challenge levels, remains limited. This study delves into the linguistic and structural attributes of feedback generated by Google's Gemini 1.5-flash text model for computer science multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed, considering three difficulty levels (easy, medium, hard) and three feedback tones (supportive, neutral, challenging). Key linguistic metrics, such as length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness, and lexical density, were computed and examined. A fine-tuned RoBERTa-based multi-task learning (MTL) model was trained to predict these linguistic properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and 0.03 for vocabulary richness. The findings reveal significant interaction effects between feedback tone and question difficulty, demonstrating the dynamic adaptation of AI-generated feedback within diverse educational contexts. These insights contribute to the development of more personalized and effective AI-driven feedback mechanisms, highlighting the potential for improved learning outcomes while underscoring the importance of ethical considerations in their design and deployment.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments</title>
<link>https://arxiv.org/abs/2504.21016</link>
<guid>https://arxiv.org/abs/2504.21016</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, pandemic, named-entity recognition, Vietnam, prevention<br />
<br />
Summary: <br />
The research focuses on utilizing named-entity recognition (NER) to aid in preventing the spread of COVID-19 in Vietnam. While efforts have been made to trace, localize, and quarantine individuals in contact with patients, the manual nature of these tasks proves to be labor-intensive. The study introduces a manually annotated COVID-19 dataset specifically designed for Vietnamese, incorporating nested named entity recognition tasks with newly defined entity types. By incorporating NER technology, the aim is to enhance the efficiency and accuracy of disease prevention measures in Vietnam. This innovative approach could potentially streamline the process of identifying and isolating individuals at risk, ultimately contributing to more effective containment of the pandemic. <div>
arXiv:2504.21016v1 Announce Type: new 
Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place to prevent but many countries have failed. In Vietnam, the traceability, localization, and quarantine of people who contact with patients contribute to effective disease prevention. However, this is done by hand, and take a lot of work. In this research, we describe a named-entity recognition (NER) study that assists in the prevention of COVID-19 pandemic in Vietnam. We also present our manually annotated COVID-19 dataset with nested named entity recognition task for Vietnamese which be defined new entity types using for our system.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese</title>
<link>https://arxiv.org/abs/2504.21017</link>
<guid>https://arxiv.org/abs/2504.21017</guid>
<content:encoded><![CDATA[
<div> dataset, COVID-19, artificial intelligence, machine reading comprehension, Vietnamese <br />
Summary:<br /> 
The article discusses the impact of COVID-19 worldwide, with over 522 million cases and six million deaths. The Omicron variant has strained disease prevention efforts. The use of artificial intelligence (AI) is crucial in supporting people during these challenging times. Various studies have applied AI to prevent COVID-19, including machine reading comprehension (MRC) studies. The creation of the ViQA-COVID dataset for Vietnamese is significant, as it is the first MRC dataset for COVID-19 in Vietnamese. This dataset can aid in building models and systems to contribute to disease prevention efforts. ViQA-COVID is also the first multi-span extraction MRC dataset for Vietnamese, with the potential to advance MRC studies in Vietnamese and other languages. <br /> <div>
arXiv:2504.21017v1 Announce Type: new 
Abstract: After two years of appearance, COVID-19 has negatively affected people and normal life around the world. As in May 2022, there are more than 522 million cases and six million deaths worldwide (including nearly ten million cases and over forty-three thousand deaths in Vietnam). Economy and society are both severely affected. The variant of COVID-19, Omicron, has broken disease prevention measures of countries and rapidly increased number of infections. Resources overloading in treatment and epidemics prevention is happening all over the world. It can be seen that, application of artificial intelligence (AI) to support people at this time is extremely necessary. There have been many studies applying AI to prevent COVID-19 which are extremely useful, and studies on machine reading comprehension (MRC) are also in it. Realizing that, we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and can be used to build models and systems, contributing to disease prevention. Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for Vietnamese, we hope that it can contribute to promoting MRC studies in Vietnamese and multilingual.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization</title>
<link>https://arxiv.org/abs/2504.21018</link>
<guid>https://arxiv.org/abs/2504.21018</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-trained language models, mid- and low-resource languages, token embedding initialization, hypernetwork, continual pre-training

Summary: 
HYPEROFA introduces a hypernetwork-based approach for adaptive token embedding initialization in pre-trained language models. This method addresses the limitations of existing strategies, such as OFA, by allowing for more flexible token embeddings that are not constrained to convex combinations of source-language embeddings. The hypernetwork is trained to map from a multilingual word vector space to the token embedding space of PLMs using source-language tokens. This approach proves to be effective in improving the convergence of continual pre-training and enhances downstream task performance. Experiments show that HYPEROFA outperforms random initialization baselines and achieves comparable or superior results to OFA. The code for HYPEROFA is made publicly available for further research and implementation. 

<br /><br />Summary: <div>
arXiv:2504.21018v1 Announce Type: new 
Abstract: Many pre-trained language models (PLMs) exhibit suboptimal performance on mid- and low-resource languages, largely due to limited exposure to these languages during pre-training. A common strategy to address this is to introduce new tokens specific to the target languages, initialize their embeddings, and apply continual pre-training on target-language data. Among such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword embedding initialization heuristic that is both effective and efficient. However, OFA restricts target-language token embeddings to be convex combinations of a fixed number of source-language embeddings, which may limit expressiveness. To overcome this limitation, we propose HYPEROFA, a hypernetwork-based approach for more adaptive token embedding initialization. The hypernetwork is trained to map from an external multilingual word vector space to the PLMs token embedding space using source-language tokens. Once trained, it can generate flexible embeddings for target-language tokens, serving as a good starting point for continual pretraining. Experiments demonstrate that HYPEROFA consistently outperforms random initialization baseline and matches or exceeds the performance of OFA in both continual pre-training convergence and downstream task performance. We make the code publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations</title>
<link>https://arxiv.org/abs/2504.21019</link>
<guid>https://arxiv.org/abs/2504.21019</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AI-generated text, AIGT detection, domain shift, DP-Net<br />
<br />
Summary: 
The paper addresses the challenge of detecting AI-generated text (AIGT) by introducing a novel method called DP-Net, focusing on both generalization and robustness. The researchers propose that robustness in AIGT detection can be viewed as a form of domain shift. They develop DP-Net, which incorporates dynamic perturbations using reinforcement learning to enhance model generalization. Experimental results demonstrate that DP-Net outperforms existing methods in terms of generalization capacity across different domains and exhibits superior robustness against text adversarial attacks. The code for DP-Net is publicly available for further research and development. This approach offers a promising solution for effectively detecting and mitigating the potential misuse of AI-generated text, addressing concerns associated with the increasing prevalence of large language models. <br /><br />Summary: <div>
arXiv:2504.21019v1 Announce Type: new 
Abstract: The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness. However, existing methods either focus on model generalization or concentrate on robustness. The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we argue that robustness can be view as a specific form of domain shift, and empirically reveal an intrinsic mechanism for model generalization of AIGT detection task. Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement learning with elaborated reward and action. Experimentally, extensive results show that the proposed DP-Net significantly outperforms some state-of-the-art AIGT detection methods for generalization capacity in three cross-domain scenarios. Meanwhile, the DP-Net achieves best robustness under two text adversarial attacks. The code is publicly available at https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Enhanced Contrastive Search for Improved LLM Text Generation</title>
<link>https://arxiv.org/abs/2504.21020</link>
<guid>https://arxiv.org/abs/2504.21020</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Natural Language Processing, Contrastive Search, Context-Enhanced Contrastive Search, Text Generation

Summary:
<br />
Large Language Models (LLMs) have made significant advancements in Natural Language Processing (NLP), but face challenges in generating high-quality text. Traditional decoding methods struggle with repetition and incoherence in long-form text generation tasks. To tackle these limitations, a novel algorithm called Context-Enhanced Contrastive Search (CECS) with contextual calibration is proposed. CECS introduces dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control to achieve a balance between fluency, creativity, and precision. Evaluation using standard metrics shows that CECS outperforms existing Contrastive Search techniques in coherence and relevance of generated texts. The algorithm has potential applications in legal document drafting, customer service chatbots, and content marketing.
<br /><br />Summary: <div>
arXiv:2504.21020v1 Announce Type: new 
Abstract: Recently, Large Language Models (LLMs) have demonstrated remarkable advancements in Natural Language Processing (NLP). However, generating high-quality text that balances coherence, diversity, and relevance remains challenging. Traditional decoding methods, such as bean search and top-k sampling, often struggle with either repetitive or incoherent outputs, particularly in tasks that require long-form text generation. To address these limitations, the paper proposes a novel enhancement of the well-known Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with contextual calibration. The proposed scheme introduces several novelties including dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control, to optimize the balance between fluency, creativity, and precision. The performance of CECS is evaluated using several standard metrics such as BLEU, ROUGE, and semantic similarity. Experimental results demonstrate significant improvements in both coherence and relevance of the generated texts by CECS outperforming the existing Contrastive Search techniques. The proposed algorithm has several potential applications in the real world including legal document drafting, customer service chatbots, and content marketing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees</title>
<link>https://arxiv.org/abs/2504.21022</link>
<guid>https://arxiv.org/abs/2504.21022</guid>
<content:encoded><![CDATA[
<div> keywords: Linear Temporal Logic, Natural Language, ConformalNL2LTL, uncertainty-aware translation, Question-Answering

Summary:
ConformalNL2LTL introduces a new method for translating Natural Language (NL) instructions into Linear Temporal Logic (LTL) formulas with user-defined translation success rates. The method utilizes Question-Answering (QA) problems and Language Models (LLMs) to iteratively construct LTL formulas. By leveraging conformal prediction (CP), which enables uncertainty quantification for black-box models, the method can assess the uncertainty in LLM-generated answers. This allows the method to proceed with translation when confident and request help when needed, resulting in user-specified translation accuracy while minimizing help rates. The approach provides correctness guarantees in NL-to-LTL translation, addressing the manual effort and expertise required in defining LTL-encoded tasks for robotic applications. Theoretical and empirical results demonstrate the effectiveness of ConformalNL2LTL in achieving user-specified translation accuracy. <div>
arXiv:2504.21022v1 Announce Type: new 
Abstract: Linear Temporal Logic (LTL) has become a prevalent specification language for robotic tasks. To mitigate the significant manual effort and expertise required to define LTL-encoded tasks, several methods have been proposed for translating Natural Language (NL) instructions into LTL formulas, which, however, lack correctness guarantees. To address this, we introduce a new NL-to-LTL translation method, called ConformalNL2LTL, that can achieve user-defined translation success rates over unseen NL commands. Our method constructs LTL formulas iteratively by addressing a sequence of open-vocabulary Question-Answering (QA) problems with LLMs. To enable uncertainty-aware translation, we leverage conformal prediction (CP), a distribution-free uncertainty quantification tool for black-box models. CP enables our method to assess the uncertainty in LLM-generated answers, allowing it to proceed with translation when sufficiently confident and request help otherwise. We provide both theoretical and empirical results demonstrating that ConformalNL2LTL achieves user-specified translation accuracy while minimizing help rates.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Param$\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost</title>
<link>https://arxiv.org/abs/2504.21023</link>
<guid>https://arxiv.org/abs/2504.21023</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, post-training, knowledge transfer, model development, computational efficiency

Summary: 
$Param\Delta$ introduces a method for streamlining the post-training phase of large language models by transferring knowledge from an existing post-trained model to a newly updated base model without additional training. By calculating the weight difference between post-trained and base models and applying it to the updated base model, $Param\Delta$ equips the new model with post-trained capabilities, achieving performance comparable to direct post-training. The approach was evaluated on LLama3, LLama3.1, Qwen, and DeepSeek-distilled models, showing effective replication of traditional post-training results. For instance, the $Param\Delta$ Model derived from Llama3 models attained approximately 95% of Llama3.1-inst model's performance on average. This cost-free framework accelerates the iterative cycle of model development in the open-weight community, utilizing readily available checkpoints for base and instruct models.  

<br /><br />Summary: <div>
arXiv:2504.21023v1 Announce Type: new 
Abstract: The post-training phase of large language models is essential for enhancing capabilities such as instruction-following, reasoning, and alignment with human preferences. However, it demands extensive high-quality data and poses risks like overfitting, alongside significant computational costs due to repeated post-training and evaluation after each base model update. This paper introduces $Param\Delta$, a novel method that streamlines post-training by transferring knowledge from an existing post-trained model to a newly updated base model with ZERO additional training. By computing the difference between post-trained model weights ($\Theta_\text{post}$) and base model weights ($\Theta_\text{base}$), and adding this to the updated base model ($\Theta'_\text{base}$), we define $Param\Delta$ Model as: $\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} + \Theta'_\text{base}$. This approach surprisingly equips the new base model with post-trained capabilities, achieving performance comparable to direct post-training. We did analysis on LLama3, Llama3.1, Qwen, and DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively replicates traditional post-training. For example, the $Param\Delta$ Model obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains approximately 95\% of Llama3.1-inst model's performance on average. $Param\Delta$ brings a new perspective on how to fully leverage models in the open-weight community, where checkpoints for base and instruct models are readily available and frequently updated, by providing a cost-free framework to accelerate the iterative cycle of model development.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model</title>
<link>https://arxiv.org/abs/2504.21024</link>
<guid>https://arxiv.org/abs/2504.21024</guid>
<content:encoded><![CDATA[
<div> Keywords: Agent self-improvement, Large Language Model, World Model, web environments, performance gain <br />
Summary: 
Agent self-improvement using Large Language Models (LLMs) in web environments faces a limitation of performance stagnation due to limited exploration and exploitation of web knowledge. To address this, a novel framework is proposed, introducing a co-evolving World Model LLM that predicts next observations and serves as a virtual web server for training data generation and imagination engine during inference. Experiments in real-world web environments demonstrate a 10% performance gain over existing self-evolving agents, highlighting the effectiveness and generalizability of the approach without distillation from more powerful models. Integrating world models into autonomous agent frameworks is shown to be essential for unlocking sustained adaptability. <br /><br />Summary: <div>
arXiv:2504.21024v1 Announce Type: new 
Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs' pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh</title>
<link>https://arxiv.org/abs/2504.21025</link>
<guid>https://arxiv.org/abs/2504.21025</guid>
<content:encoded><![CDATA[
<div> Keywords: road accidents, data automation, web scraping, Large Language Models, Bangladesh<br />
<br />
Summary: <br />
Road accidents are a global concern, leading to financial losses, injuries, and societal challenges. The 'Durghotona GPT' framework utilizes web scraping and Large Language Models (LLMs) to automate the collection of accident data from major newspapers in Bangladesh. By using advanced LLMs like GPT-4 and Llama-3, the framework efficiently extracts and categorizes relevant information, overcoming manual data collection limitations. The evaluation shows that Llama-3 performs comparably to GPT-4 with 89% accuracy. The framework aims to enhance the quality and availability of accident data, supporting traffic safety analysis, urban planning, and public health applications. An interface for 'Durghotona GPT' has been developed for ease of use. Future work will focus on expanding data collection methods and improving LLMs for increased accuracy and applicability. <br /> <div>
arXiv:2504.21025v1 Announce Type: new 
Abstract: Road accidents pose significant concerns globally. They lead to large financial losses, injuries, disabilities, and societal challenges. Accurate and timely accident data is essential for predicting and mitigating these events. This paper presents a novel framework named 'Durghotona GPT' that integrates web scraping and Large Language Models (LLMs) to automate the generation of comprehensive accident datasets from prominent national dailies in Bangladesh. The authors collected accident reports from three major newspapers: Prothom Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework efficiently extracts relevant information, categorizes reports, and compiles detailed datasets. Thus, this framework overcomes limitations of manual data collection methods such as delays, errors, and communication gaps. The authors' evaluation demonstrates that Llama-3, an open-source model, performs comparably to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it can be considered a cost-effective alternative for similar tasks. The results suggest that the framework developed by the authors can drastically enhance the quality and availability of accident data. As a result, it can support critical applications in traffic safety analysis, urban planning, and public health. The authors also developed an interface for 'Durghotona GPT' for ease of use as part of this paper. Future work will focus on expanding data collection methods and refining LLMs to further increase dataset accuracy and applicability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models</title>
<link>https://arxiv.org/abs/2504.21026</link>
<guid>https://arxiv.org/abs/2504.21026</guid>
<content:encoded><![CDATA[
<div> Keywords: code-mixed text, abusive language detection, low-resource languages, multilingual social media, NLP<br />
Summary:<br /> 
- The study introduces a dataset of abusive and non-abusive Telugu-English and Nepali-English code-mixed comments from social media platforms.
- Various Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs) are evaluated for detecting abusive language in code-mixed text.
- Models such as Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs are optimized and compared through experiments.
- Challenges in detecting abusive language in code-mixed settings are highlighted, providing insights into the complexities of moderation on multilingual social media platforms.
- The study aims to advance Natural Language Processing (NLP) for low-resource languages by setting benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. <br /> 
Summary: <div>
arXiv:2504.21026v1 Announce Type: new 
Abstract: With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models</title>
<link>https://arxiv.org/abs/2504.21027</link>
<guid>https://arxiv.org/abs/2504.21027</guid>
<content:encoded><![CDATA[
<div> benchmark, UrbanPlanBench, LLMs, urban planning, UrbanPlanText 

Summary: 
The paper introduces the UrbanPlanBench benchmark to evaluate the effectiveness of Large Language Models (LLMs) in urban planning. It reveals a lack of proficiency among LLMs in acquiring planning knowledge, particularly in understanding planning regulations. The authors present the UrbanPlanText dataset, a supervised fine-tuning dataset comprising over 30,000 instruction pairs sourced from urban planning exams and textbooks. Fine-tuned models show improved performance in memorization tests and comprehension of urban planning knowledge but still have room for improvement in tasks requiring domain-specific terminology and reasoning. The goal is to integrate LLMs into practical urban planning by fostering a collaboration between human expertise and machine intelligence. The benchmark, dataset, and toolsets are publicly available to encourage further research and development in this area. 

<br /><br />Summary: <div>
arXiv:2504.21027v1 Announce Type: new 
Abstract: The advent of Large Language Models (LLMs) holds promise for revolutionizing various fields traditionally dominated by human expertise. Urban planning, a professional discipline that fundamentally shapes our daily surroundings, is one such field heavily relying on multifaceted domain knowledge and experience of human experts. The extent to which LLMs can assist human practitioners in urban planning remains largely unexplored. In this paper, we introduce a comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of LLMs in urban planning, which encompasses fundamental principles, professional knowledge, and management and regulations, aligning closely with the qualifications expected of human planners. Through extensive evaluation, we reveal a significant imbalance in the acquisition of planning knowledge among LLMs, with even the most proficient models falling short of meeting professional standards. For instance, we observe that 70% of LLMs achieve subpar performance in understanding planning regulations compared to other aspects. Besides the benchmark, we present the largest-ever supervised fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction pairs sourced from urban planning exams and textbooks. Our findings demonstrate that fine-tuned models exhibit enhanced performance in memorization tests and comprehension of urban planning knowledge, while there exists significant room for improvement, particularly in tasks requiring domain-specific terminology and reasoning. By making our benchmark, dataset, and associated evaluation and fine-tuning toolsets publicly available at https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the integration of LLMs into practical urban planning, fostering a symbiotic collaboration between human expertise and machine intelligence.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts</title>
<link>https://arxiv.org/abs/2504.21117</link>
<guid>https://arxiv.org/abs/2504.21117</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language generation, evaluation, LLM-based, inversion learning, prompt design

Summary: 
In the field of natural language generation (NLG) systems evaluation, human evaluation faces challenges such as inconsistencies and demographic biases. To address this, LLM-based evaluation methods have been proposed but require careful prompt design. This work introduces an inversion learning method that generates effective evaluation prompts by reverse mapping model outputs to input instructions. By automating prompt generation, this method improves efficiency and eliminates the need for manual prompt engineering. This approach aims to enhance the robustness and efficacy of LLM-based evaluation processes. 
<br /><br />Summary: <div>
arXiv:2504.21117v1 Announce Type: new 
Abstract: Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge</title>
<link>https://arxiv.org/abs/2504.21132</link>
<guid>https://arxiv.org/abs/2504.21132</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, LLM ENHANCER system, data accuracy, external knowledge sources, vector embeddings 

Summary: 
The paper introduces the LLM ENHANCER system, aimed at improving the accuracy of Large Language Models (LLMs) like ChatGPT by leveraging external sources such as Google, Wikipedia, and DuckDuckGo. The system uses open-source LLMs and custom agent tools for data acquisition, ensuring accurate and natural response generation. By utilizing vector embeddings to identify relevant information, the LLM ENHANCER system addresses hallucinations in chat-based LLMs while maintaining response quality. This integration of external knowledge sources enhances the overall performance of LLMs in critical scenarios, where accurate information is crucial. The system's parallel operation streamlines the data flow process, enabling efficient utilization of online resources to supplement LLM capabilities. Overall, the LLM ENHANCER system presents a practical solution for improving the accuracy and reliability of LLM-generated responses. 

<br /><br />Summary: <div>
arXiv:2504.21132v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated the capability to generate human like, natural responses across a range of tasks, including task oriented dialogue and question answering. However, their application in real world, critical scenarios is often hindered by a tendency to produce inaccurate information and a limited ability to leverage external knowledge sources. This paper introduces the LLM ENHANCER system, designed to integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to enhance data accuracy. The LLMs employed within this system are open source. The data acquisition process for the LLM ENHANCER system operates in parallel, utilizing custom agent tools to manage the flow of information. Vector embeddings are used to identify the most pertinent information, which is subsequently supplied to the LLM for user interaction. The LLM ENHANCER system mitigates hallucinations in chat based LLMs while preserving response naturalness and accuracy.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Manipulated Contents Using Knowledge-Grounded Inference</title>
<link>https://arxiv.org/abs/2504.21165</link>
<guid>https://arxiv.org/abs/2504.21165</guid>
<content:encoded><![CDATA[
<div> fake news, manipulated content, detection, zero-day, Manicod <br />
Summary:
Manicod is a new tool designed to detect zero-day manipulated content, a prevalent form of fake news that can only be recognized with real-time contextual information. It sources contextual information from search engines, vectorizes it for a large language model (LLM) using retrieval-augmented generation (RAG), and makes "truthful" or "manipulated" decisions with textual explanations. The tool is validated on a dataset of 4270 manipulated fake news pieces and achieves an F1 score of 0.856, outperforming existing methods by up to 1.9x in fact-checking and claim verification benchmarks. Manicod's approach addresses the limitations of current solutions, offering an effective way to identify and analyze manipulated content quickly and accurately in real-time scenarios. <br /> <div>
arXiv:2504.21165v1 Announce Type: new 
Abstract: The detection of manipulated content, a prevalent form of fake news, has been widely studied in recent years. While existing solutions have been proven effective in fact-checking and analyzing fake news based on historical events, the reliance on either intrinsic knowledge obtained during training or manually curated context hinders them from tackling zero-day manipulated content, which can only be recognized with real-time contextual information. In this work, we propose Manicod, a tool designed for detecting zero-day manipulated content. Manicod first sources contextual information about the input claim from mainstream search engines, and subsequently vectorizes the context for the large language model (LLM) through retrieval-augmented generation (RAG). The LLM-based inference can produce a "truthful" or "manipulated" decision and offer a textual explanation for the decision. To validate the effectiveness of Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake news derived from 2500 recent real-world news headlines. Manicod achieves an overall F1 score of 0.856 on this dataset and outperforms existing methods by up to 1.9x in F1 score on their benchmarks on fact-checking and claim verification.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare</title>
<link>https://arxiv.org/abs/2504.21191</link>
<guid>https://arxiv.org/abs/2504.21191</guid>
<content:encoded><![CDATA[
<div> classification, language model, finetuning, domain-specific pretraining, small language model

Summary:
- The study compares finetuning versus zero-shot usage for language models in three classification scenarios using pathology reports.
- Finetuning significantly improves Small Language Model (SLM) performance, surpassing zero-shot results.
- Domain-adjacent SLMs perform better than generic SLMs after finetuning, especially on difficult tasks.
- Further domain-specific pretraining offers modest gains on easy tasks and significant improvements on complex, data-scarce tasks.
- While Large Language Models (LLMs) perform well zero-shot, appropriately finetuned SLMs outperform them on specific tasks, showing the continued relevance and effectiveness of SLMs in specialized domains. 

<br /><br />Summary: <div>
arXiv:2504.21191v1 Announce Type: new 
Abstract: This study aims to guide language model selection by investigating: 1) the necessity of finetuning versus zero-shot usage, 2) the benefits of domain-adjacent versus generic pretrained models, 3) the value of further domain-specific pretraining, and 4) the continued relevance of Small Language Models (SLMs) compared to Large Language Models (LLMs) for specific tasks. Using electronic pathology reports from the British Columbia Cancer Registry (BCCR), three classification scenarios with varying difficulty and data size are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning significantly improved SLM performance across all scenarios compared to their zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally performed better than the generic SLM after finetuning, especially on harder tasks. Further domain-specific pretraining yielded modest gains on easier tasks but significant improvements on the complex, data-scarce task. The results highlight the critical role of finetuning for SLMs in specialized domains, enabling them to surpass zero-shot LLM performance on targeted classification tasks. Pretraining on domain-adjacent or domain-specific data provides further advantages, particularly for complex problems or limited finetuning data. While LLMs offer strong zero-shot capabilities, their performance on these specific tasks did not match that of appropriately finetuned SLMs. In the era of LLMs, SLMs remain relevant and effective, offering a potentially superior performance-resource trade-off compared to LLMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Legal Writing Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2504.21202</link>
<guid>https://arxiv.org/abs/2504.21202</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Legal Writing, Benchmark, Brazilian Bar Examination, Automated Evaluation

Summary: 
The paper introduces oab-bench, a benchmark consisting of 105 questions from the Brazilian Bar Examination to evaluate language models in the legal writing domain. Four LLMs were tested on this benchmark, with Claude-3.5 Sonnet achieving the highest average score. The study also explored the potential of LLMs as automated judges for legal writing assessment and found that models like OpenAI's o1 showed strong correlation with human scores. The results indicate that LLMs have the capability to serve as reliable automated evaluators, despite the subjective nature of legal writing evaluation. The benchmark, including questions, guidelines, model responses, and automated evaluations, is publicly available, allowing for further research and development in this area.

<br /><br />Summary: The paper presents oab-bench, a benchmark from the Brazilian Bar Examination, to assess language models in legal writing. Evaluation of four LLMs showed Claude-3.5 Sonnet performing best. The study also investigated the use of LLMs as automated judges for legal writing, with models like OpenAI's o1 showing strong correlation with human scores. This suggests the potential for LLMs as reliable evaluators in the legal domain. The publicly available benchmark provides a comprehensive resource for further research and development. <div>
arXiv:2504.21202v1 Announce Type: new 
Abstract: Despite the recent advances in Large Language Models, benchmarks for evaluating legal writing remain scarce due to the inherent complexity of assessing open-ended responses in this domain. One of the key challenges in evaluating language models on domain-specific tasks is finding test datasets that are public, frequently updated, and contain comprehensive evaluation guidelines. The Brazilian Bar Examination meets these requirements. We introduce oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the exam. The benchmark includes comprehensive evaluation guidelines and reference materials used by human examiners to ensure consistent grading. We evaluate the performance of four LLMs on oab-bench, finding that Claude-3.5 Sonnet achieves the best results with an average score of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can serve as reliable automated judges for evaluating legal writing. Our experiments show that frontier models like OpenAI's o1 achieve a strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the inherently subjective nature of legal writing assessment. The source code and the benchmark -- containing questions, evaluation guidelines, model-generated responses, and their respective automated evaluations -- are publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining Large Brain Language Model for Active BCI: Silent Speech</title>
<link>https://arxiv.org/abs/2504.21214</link>
<guid>https://arxiv.org/abs/2504.21214</guid>
<content:encoded><![CDATA[
<div> dataset, EEG recordings, silent speech decoding, brain-computer interface, language model pretraining<br />
<br />
Summary: <br />
This paper discusses the use of silent speech decoding in active brain-computer interface systems, utilizing a new dataset of EEG recordings for language model pretraining. The authors propose a Large Brain Language Model (LBLM) pretrained using the Future Spectro-Temporal Prediction (FSTP) paradigm to improve EEG classification performance. The FSTP method captures both temporal and spectral dependencies from EEG signals through autoregressive modeling. The LBLM is then finetuned on word-level and semantic-level classification tasks, showing significant performance improvements over baseline models. In challenging cross-session scenarios, the LBLM achieves 47.0% accuracy in semantic-level classification and 39.6% in word-level classification, outperforming existing methods. This research contributes to the advancement of silent speech decoding in active BCI systems, introducing an innovative approach to EEG language model pretraining and providing a valuable new dataset for further research. <br /> <div>
arXiv:2504.21214v1 Announce Type: new 
Abstract: This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\% accuracy on semantic-level classification and 39.6\% in word-level classification, outperforming baseline methods by 5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math</title>
<link>https://arxiv.org/abs/2504.21233</link>
<guid>https://arxiv.org/abs/2504.21233</guid>
<content:encoded><![CDATA[
<div> chain-of-thought, large language models, small language models, reasoning, training recipe <br />
<br />
Summary: <br />
The study introduces a systematic training recipe to enhance reasoning capabilities in Small Language Models (SLMs). By following four key steps, including mid-training on distilled long-CoT data, fine-tuning on high-quality CoT data, Rollout DPO with preference dataset, and Reinforcement Learning with Verifiable Reward, the method was applied to a compact 3.8B-parameter model called Phi-4-Mini. The results show that Phi-4-Mini-Reasoning model outperformed larger reasoning models on math reasoning tasks, demonstrating the effectiveness of the training recipe in improving reasoning abilities in resource-constrained small models. <div>
arXiv:2504.21233v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization and Knowledge Injection in Gated LLMs</title>
<link>https://arxiv.org/abs/2504.21239</link>
<guid>https://arxiv.org/abs/2504.21239</guid>
<content:encoded><![CDATA[
<div> Memory Embedded, Gated LLMs, Continual Learning, Event Memories, Catastrophic Forgetting 
Summary: 
Memory Embedded in Gated LLMs (MEGa) is a new framework designed to address the limitations of Large Language Models in sequentially adding new memories and integrating new knowledge. The model stores event memories directly in the weights of LLMs, using a gating mechanism to activate relevant memory weights during inference. This approach allows the model to recall entire memories and answer related questions, outperforming baseline approaches on two datasets - fictional characters and Wikipedia events. MEGa is inspired by the human brain's complementary memory system, enabling it to better simulate the continuous learning process observed in everyday life. <div>
arXiv:2504.21239v1 Announce Type: new 
Abstract: Large Language Models (LLMs) currently struggle to sequentially add new memories and integrate new knowledge. These limitations contrast with the human ability to continuously learn from new experiences and acquire knowledge throughout life. Most existing approaches add memories either through large context windows or external memory buffers (e.g., Retrieval-Augmented Generation), and studies on knowledge injection rarely test scenarios resembling everyday life events. In this work, we introduce a continual learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event memories directly into the weights of LLMs. Each memory is stored in a dedicated set of gated low-rank weights. During inference, a gating mechanism activates relevant memory weights by matching query embeddings to stored memory embeddings. This enables the model to both recall entire memories and answer related questions. On two datasets - fictional characters and Wikipedia events - MEGa outperforms baseline approaches in mitigating catastrophic forgetting. Our model draws inspiration from the complementary memory system of the human brain.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA</title>
<link>https://arxiv.org/abs/2504.21252</link>
<guid>https://arxiv.org/abs/2504.21252</guid>
<content:encoded><![CDATA[
<div> Medical question answering; retrieval-augmented generation; Discuss-RAG; human-like reasoning; collaborative agent-based reasoning

Summary:<br />
1. Medical question answering (QA) is a challenging task for large language models (LLMs) due to hallucinations and outdated domain knowledge.<br />
2. Retrieval-Augmented Generation (RAG) tackles this by leveraging external knowledge, but existing medical RAG systems lack human-like reasoning behaviors during information retrieval and often retrieve irrelevant content.<br />
3. Discuss-RAG introduces a summarizer agent to coordinate medical experts in multi-turn brainstorming, improving relevance of retrieved content, and a decision-making agent to evaluate snippets before integration.<br />
4. Experimental results on four benchmark medical QA datasets show Discuss-RAG outperforms MedRAG, significantly improving answer accuracy on BioASQ and PubMedQA.<br />
5. Discuss-RAG code is available at https://github.com/LLM-VLM-GSL/Discuss-RAG.<br /> 

Summary: <br />
Medical question answering is challenging for large language models due to hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG) helps by incorporating external knowledge, but existing medical RAG systems have limitations. Discuss-RAG introduces a summarizer agent for better information retrieval and a decision-making agent to evaluate retrieved snippets. Experimental results show Discuss-RAG outperforms existing methods, significantly improving answer accuracy. The code for Discuss-RAG is available on GitHub. <div>
arXiv:2504.21252v1 Announce Type: new 
Abstract: Medical question answering (QA) is a reasoning-intensive task that remains challenging for large language models (LLMs) due to hallucinations and outdated domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising post-training solution by leveraging external knowledge. However, existing medical RAG systems suffer from two key limitations: (1) a lack of modeling for human-like reasoning behaviors during information retrieval, and (2) reliance on suboptimal medical corpora, which often results in the retrieval of irrelevant or noisy snippets. To overcome these challenges, we propose Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG system through collaborative agent-based reasoning. Our method introduces a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content. Additionally, a decision-making agent evaluates the retrieved snippets before their final integration. Experimental results on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms MedRAG, especially significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models</title>
<link>https://arxiv.org/abs/2504.21299</link>
<guid>https://arxiv.org/abs/2504.21299</guid>
<content:encoded><![CDATA[
<div> Keywords: BiasGuard, bias detection, LLMs, fairness specifications, reinforcement learning

Summary:
BiasGuard is a new tool designed to detect bias in content generated by large language models (LLMs). It addresses limitations of existing methods by explicitly analyzing inputs and reasoning through fairness specifications to make accurate judgments. Utilizing a two-stage approach, BiasGuard first initializes the model to reason based on fairness specifications, and then leverages reinforcement learning to enhance its judgment capabilities. Experiments across five datasets show that BiasGuard outperforms existing tools in accuracy and reducing over-fairness misjudgments. The importance of reasoning-enhanced decision-making is highlighted, along with evidence of the effectiveness of the two-stage optimization pipeline.

<br /><br />Summary: <div>
arXiv:2504.21299v1 Announce Type: new 
Abstract: Identifying bias in LLM-generated content is a crucial prerequisite for ensuring fairness in LLMs. Existing methods, such as fairness classifiers and LLM-based judges, face limitations related to difficulties in understanding underlying intentions and the lack of criteria for fairness judgment. In this paper, we introduce BiasGuard, a novel bias detection tool that explicitly analyzes inputs and reasons through fairness specifications to provide accurate judgments. BiasGuard is implemented through a two-stage approach: the first stage initializes the model to explicitly reason based on fairness specifications, while the second stage leverages reinforcement learning to enhance its reasoning and judgment capabilities. Our experiments, conducted across five datasets, demonstrate that BiasGuard outperforms existing tools, improving accuracy and reducing over-fairness misjudgments. We also highlight the importance of reasoning-enhanced decision-making and provide evidence for the effectiveness of our two-stage optimization pipeline.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges</title>
<link>https://arxiv.org/abs/2504.21303</link>
<guid>https://arxiv.org/abs/2504.21303</guid>
<content:encoded><![CDATA[
<div> Bayesian approach, Large language models, probabilistic inference, model ranking, GPT-series models <br />
<br />
Summary: This study introduces a Bayesian approach for evaluating Large Language Models (LLMs) that considers their probabilistic output characteristics. By treating model capabilities as latent variables and utilizing a curated query set, the proposed method formulates model ranking as a Bayesian hypothesis testing problem. Experimental results with GPT-series models show superior discrimination compared to conventional evaluation methods. The approach maintains statistical robustness even with reduced sample sizes and provides actionable insights, including probabilistic statements about a model's likelihood of surpassing specific baselines. This work advances LLM evaluation methodologies by integrating Bayesian inference with real-world deployment constraints. <div>
arXiv:2504.21303v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit probabilistic output characteristics, yet conventional evaluation frameworks rely on deterministic scalar metrics. This study introduces a Bayesian approach for LLM capability assessment that integrates prior knowledge through probabilistic inference, addressing limitations under limited-sample regimes. By treating model capabilities as latent variables and leveraging a curated query set to induce discriminative responses, we formalize model ranking as a Bayesian hypothesis testing problem over mutually exclusive capability intervals. Experimental evaluations with GPT-series models demonstrate that the proposed method achieves superior discrimination compared to conventional evaluation methods. Results indicate that even with reduced sample sizes, the approach maintains statistical robustness while providing actionable insights, such as probabilistic statements about a model's likelihood of surpassing specific baselines. This work advances LLM evaluation methodologies by bridging Bayesian inference with practical constraints in real-world deployment scenarios.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?</title>
<link>https://arxiv.org/abs/2504.21330</link>
<guid>https://arxiv.org/abs/2504.21330</guid>
<content:encoded><![CDATA[
<div> predictive power, bias, scoring, demographic attributes, language models

Summary:
- Prompt-based tools like ChatGPT are used in Automated Essay Scoring (AES) for their accessibility.
- Bias in fine-tuned Large Language Models (LLMs) has been found, especially against disadvantaged groups.
- The study investigates if biases persist or are amplified in the prompt-based paradigm with cutting-edge tools.
- LLMs can somewhat infer students' demographic attributes, particularly their first-language backgrounds, from their essays using prompts.
- Scoring biases are more pronounced when the LLM correctly predicts students' first-language background than when it does not, impacting scoring outcomes.
- Scoring error for non-native English speakers increases when the LLM correctly identifies them as non-native.<br /><br />Summary: <div>
arXiv:2504.21330v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES) due to their ability to capture semantic meaning. Traditional fine-tuning approaches required technical expertise, limiting accessibility for educators with limited technical backgrounds. However, prompt-based tools like ChatGPT have made AES more accessible, enabling educators to obtain machine-generated scores using natural-language prompts (i.e., the prompt-based paradigm). Despite advancements, prior studies have shown bias in fine-tuned LLMs, particularly against disadvantaged groups. It remains unclear whether such biases persist or are amplified in the prompt-based paradigm with cutting-edge tools. Since such biases are believed to stem from the demographic information embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to predict demographic attributes), this study explores the relationship between the model's predictive power of students' demographic attributes based on their written works and its predictive bias in the scoring task in the prompt-based paradigm. Using a publicly available dataset of over 25,000 students' argumentative essays, we designed prompts to elicit demographic inferences (i.e., gender, first-language background) from GPT-4o and assessed fairness in automated scoring. Then we conducted multivariate regression analysis to explore the impact of the model's ability to predict demographics on its scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat infer students' demographics, particularly their first-language backgrounds, from their essays; (ii) scoring biases are more pronounced when the LLM correctly predicts students' first-language background than when it does not; and (iii) scoring error for non-native English speakers increases when the LLM correctly identifies them as non-native.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction</title>
<link>https://arxiv.org/abs/2504.21372</link>
<guid>https://arxiv.org/abs/2504.21372</guid>
<content:encoded><![CDATA[
<div> Keyword: Speech Event Extraction, Automatic Speech Recognition, Natural Language Processing, Large Language Models, Hybrid filtering mechanism

Summary:
Speech Event Extraction (SpeechEE) is a complex task that combines Automatic Speech Recognition (ASR) and Natural Language Processing (NLP. A modular pipeline-based framework is presented for SpeechEE, integrating ASR with Large Language Models (LLMs) enhanced by semantic search. The system uses a hybrid filtering mechanism to classify speech segments likely to contain events and employs LLM prompting enriched via semantic similarity retrieval to identify event triggers and extract arguments. Performance evaluation using multiple LLMs shows significant gains, with o1-mini achieving high F1 scores for trigger and argument classification, surpassing previous benchmarks. The results demonstrate that pipeline approaches with retrieval-augmented LLMs can rival or surpass end-to-end systems while maintaining interpretability and modularity. This work provides insights into LLM-driven event extraction and paves the way for hybrid models combining textual and acoustic features.<br /><br />Summary: <div>
arXiv:2504.21372v1 Announce Type: new 
Abstract: Speech Event Extraction (SpeechEE) is a challenging task that lies at the intersection of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP), requiring the identification of structured event information from spoken language. In this work, we present a modular, pipeline-based SpeechEE framework that integrates high-performance ASR with semantic search-enhanced prompting of Large Language Models (LLMs). Our system first classifies speech segments likely to contain events using a hybrid filtering mechanism including rule-based, BERT-based, and LLM-based models. It then employs few-shot LLM prompting, dynamically enriched via semantic similarity retrieval, to identify event triggers and extract corresponding arguments. We evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini) highlighting significant performance gains with o1-mini, which achieves 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming prior benchmarks. Our results demonstrate that pipeline approaches, when empowered by retrieval-augmented LLMs, can rival or exceed end-to-end systems while maintaining interpretability and modularity. This work provides practical insights into LLM-driven event extraction and opens pathways for future hybrid models combining textual and acoustic features.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors</title>
<link>https://arxiv.org/abs/2504.21421</link>
<guid>https://arxiv.org/abs/2504.21421</guid>
<content:encoded><![CDATA[
<div> dependency distance, hierarchical distance, valency, mean dependency distance, mean hierarchical distance 

Summary:
The study investigates the relationship between dependency distance (DD) and hierarchical distance (HD in Japanese by analyzing the Balanced Corpus of Contemporary Written Japanese. The research compares the probability distributions of DD and HD with and without fixed sentence length to explore the impact of sentence length on mean dependency distance (MDD) and mean hierarchical distance (MHD). The findings suggest that the valency of predicates plays a crucial role in influencing the trade-off relation between MDD and MHD in Japanese. Native speakers of Japanese manage linear and hierarchical complexity through predicate valency, with differences in MDD and MHD depending on valency thresholds. The valency of predicates also influences the probability distributions of DD and HD, with a greater impact on HD distribution. These insights highlight how cognitive load and valency of predicates contribute to the structural properties of sentences in Japanese. 

<br /><br />Summary: <div>
arXiv:2504.21421v1 Announce Type: new 
Abstract: To explore the relationship between dependency distance (DD) and hierarchical distance (HD) in Japanese, we compared the probability distributions of DD and HD with and without sentence length fixed, and analyzed the changes in mean dependency distance (MDD) and mean hierarchical distance (MHD) as sentence length increases, along with their correlation coefficient based on the Balanced Corpus of Contemporary Written Japanese. It was found that the valency of the predicates is the underlying factor behind the trade-off relation between MDD and MHD in Japanese. Native speakers of Japanese regulate the linear complexity and hierarchical complexity through the valency of the predicates, and the relative sizes of MDD and MHD depend on whether the threshold of valency has been reached. Apart from the cognitive load, the valency of the predicates also affects the probability distributions of DD and HD. The effect of the valency of the predicates on the distribution of HD is greater than on that of DD, which leads to differences in their probability distributions and causes the mean of MDD to be lower than that of MHD.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RWKV-X: A Linear Complexity Hybrid Language Model</title>
<link>https://arxiv.org/abs/2504.21463</link>
<guid>https://arxiv.org/abs/2504.21463</guid>
<content:encoded><![CDATA[
<div> Keywords: RWKV-X, hybrid architecture, long-range context, language modeling, efficiency

Summary: 
RWKV-X is introduced as a novel hybrid architecture combining RWKV efficiency for short-range modeling with a sparse attention mechanism for capturing long-range context. Unlike previous approaches with quadratic complexity, RWKV-X achieves linear-time complexity in training and constant-time complexity in inference decoding. Through continual pretraining on 64K-token sequences, RWKV-X achieves near-perfect accuracy on the 64K passkey retrieval benchmark and outperforms prior RWKV-7 models on long-context benchmarks while maintaining strong performance on short-context tasks. This showcases RWKV-X as a scalable and efficient language modeling backbone, capable of decoding sequences up to 1 million tokens with stable speed and memory usage. Checkpoints and code are available for further research and analysis. <br /><br />Summary: <div>
arXiv:2504.21463v1 Announce Type: new 
Abstract: In this paper, we introduce \textbf{RWKV-X}, a novel hybrid architecture that combines the efficiency of RWKV for short-range modeling with a sparse attention mechanism designed to capture long-range context. Unlike previous hybrid approaches that rely on full attention layers and retain quadratic complexity, RWKV-X achieves linear-time complexity in training and constant-time complexity in inference decoding. We demonstrate that RWKV-X, when continually pretrained on 64K-token sequences, achieves near-perfect accuracy on the 64K passkey retrieval benchmark. It consistently outperforms prior RWKV-7 models on long-context benchmarks, while maintaining strong performance on short-context tasks. These results highlight RWKV-X as a scalable and efficient backbone for general-purpose language modeling, capable of decoding sequences up to 1 million tokens with stable speed and memory usage. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at: https://github.com/howard-hou/RWKV-X.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging</title>
<link>https://arxiv.org/abs/2504.21474</link>
<guid>https://arxiv.org/abs/2504.21474</guid>
<content:encoded><![CDATA[
<div> Keywords: Homa, SemEval-2025, Subject Tagging, OntoAligner, GND taxonomy

Summary:
Homa is introduced as a system for SemEval-2025 Task 5: Subject Tagging, focusing on assigning subject labels to technical records from TIBKAT using the GND taxonomy. The system leverages OntoAligner, utilizing retrieval-augmented generation techniques to match records to GND categories based on semantic similarity. The approach frames subject tagging as an alignment task and evaluates OntoAligner's adaptability and effectiveness in handling multilingual records. Experimental results demonstrate the method's strengths and limitations, showcasing the potential of alignment techniques in enhancing subject tagging within digital libraries. <div>
arXiv:2504.21474v1 Announce Type: new 
Abstract: This paper presents our system, Homa, for SemEval-2025 Task 5: Subject Tagging, which focuses on automatically assigning subject labels to technical records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner, a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented generation (RAG) techniques. Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity. We evaluate OntoAligner's adaptability for subject indexing and analyze its effectiveness in handling multilingual records. Experimental results demonstrate the strengths and limitations of this method, highlighting the potential of alignment techniques for improving subject tagging in digital libraries.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines</title>
<link>https://arxiv.org/abs/2504.21475</link>
<guid>https://arxiv.org/abs/2504.21475</guid>
<content:encoded><![CDATA[
<div> transformer-based approach, Arabic Reverse Dictionary (RD) system, neural network architecture, dataset construction, ARBERTv2

Summary:<br />
- The study addresses the gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary using a novel transformer-based approach.
- The semi-encoder neural network architecture with geometrically decreasing layers achieves state-of-the-art results for Arabic RD tasks.
- The methodology includes dataset construction and formal quality standards for Arabic lexicographic definitions.
- Experiments show that Arabic-specific models outperform general multilingual embeddings, with ARBERTv2 performing the best.
- The study provides a formal abstraction of the reverse dictionary task, a modular Python library (RDTL), and insights for improving Arabic definition quality. 

Summary: <div>
arXiv:2504.21475v1 Announce Type: new 
Abstract: This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Informally Romanized Language Identification</title>
<link>https://arxiv.org/abs/2504.21540</link>
<guid>https://arxiv.org/abs/2504.21540</guid>
<content:encoded><![CDATA[
<div> language identification, Latin script, romanization, spelling variability, synthetic data 
<br />
Increased accuracy in language identification for romanized text is achieved by improving the methods of synthesizing training sets. The Latin script is often used to informally write languages with non-Latin native scripts, leading to high spelling variability and confusability between languages such as Hindi and Urdu. Training on synthetic samples that incorporate natural spelling variation yields higher accuracy in language identification systems compared to using naturally occurring examples or higher capacity models. The study demonstrates new state-of-the-art language identification performance on romanized text from 20 Indic languages, achieving test F1 scores of 85.4% with a linear classifier trained solely on synthetic data and 88.2% when training on available harvested text.
<br /><br />Summary: <div>
arXiv:2504.21540v1 Announce Type: new 
Abstract: The Latin script is often used to informally write languages with non-Latin native scripts. In many cases (e.g., most languages in India), there is no conventional spelling of words in the Latin script, hence there will be high spelling variability in written text. Such romanization renders languages that are normally easily distinguished based on script highly confusable, such as Hindi and Urdu. In this work, we increase language identification (LID) accuracy for romanized text by improving the methods used to synthesize training sets. We find that training on synthetic samples which incorporate natural spelling variation yields higher LID system accuracy than including available naturally occurring examples in the training set, or even training higher capacity models. We demonstrate new state-of-the-art LID performance on romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set (Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a pretrained neural model) to 85.4% using a linear classifier trained solely on synthetic data and 88.2% when also training on available harvested text.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval</title>
<link>https://arxiv.org/abs/2504.21547</link>
<guid>https://arxiv.org/abs/2504.21547</guid>
<content:encoded><![CDATA[
<div> Keywords: librarians, subject tags, information retrieval, encoder models, SemEval-2025

Summary:
Our submission for SemEval-2025 Task 5 focuses on assisting librarians in assigning subject tags to library records by generating a list of likely relevant tags for a given document. The task is framed as an information retrieval problem utilising document content to extract subject tags from a large taxonomy. We employ two types of encoder models in a two-stage information retrieval system: a bi-encoder for candidate extraction and a cross-encoder for re-ranking. This approach improves recall significantly compared to single-stage methods and yields competitive results in qualitative evaluation. The system showcases the efficacy of leveraging encoder models for subject tag assignment in libraries, demonstrating the potential for more accurate and efficient tagging processes in library systems. 

<br /><br />Summary: Our submission for SemEval-2025 Task 5 focuses on aiding librarians in assigning subject tags by employing information retrieval techniques and encoder models. The two-stage system utilizing bi-encoder and cross-encoder models demonstrates enhanced recall and competitive performance in tag assignment compared to single-stage methods. This approach highlights the potential for leveraging advanced technology to streamline and improve subject tagging processes in libraries, benefiting librarians in efficiently categorizing and organizing library records. <div>
arXiv:2504.21547v1 Announce Type: new 
Abstract: We present our submission to the Task 5 of SemEval-2025 that aims to aid librarians in assigning subject tags to the library records by producing a list of likely relevant tags for a given document. We frame the task as an information retrieval problem, where the document content is used to retrieve subject tags from a large subject taxonomy. We leverage two types of encoder models to build a two-stage information retrieval system -- a bi-encoder for coarse-grained candidate extraction at the first stage, and a cross-encoder for fine-grained re-ranking at the second stage. This approach proved effective, demonstrating significant improvements in recall compared to single-stage methods and showing competitive results according to qualitative evaluation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models</title>
<link>https://arxiv.org/abs/2504.21553</link>
<guid>https://arxiv.org/abs/2504.21553</guid>
<content:encoded><![CDATA[
<div> LLMs, quantization, LLaMA architecture, activation outliers, mixed-precision quantization

Summary:
This paper explores the quantization of Large Language Models (LLMs), particularly focusing on the LLaMA architecture and its variations. The study challenges conventional beliefs about activation outliers in LLMs and proposes a novel mixed-precision quantization technique tailored for LLaMA-like models. By identifying specific projection layers with concentration of activation spikes, the approach applies higher precision to these layers while quantizing the rest of the model to lower bit-widths, achieving improved performance in perplexity and zero-shot accuracy, especially for 8-bit per-tensor quantization. Experimental results on LLaMA2, LLaMA3, and Mistral models showcase significant enhancements compared to existing quantization methods. The research underscores the significance of considering model-specific attributes in developing efficient quantization pipelines for cutting-edge language models, emphasizing the advantages of architecture-specific quantization strategies. This contributes to the ongoing goal of enhancing the efficiency and deployability of LLMs for potential use in resource-constrained environments. 

<br /><br />Summary: <div>
arXiv:2504.21553v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, their size presents significant challenges for deployment and inference. This paper investigates the quantization of LLMs, focusing on the LLaMA architecture and its derivatives. We challenge existing assumptions about activation outliers in LLMs and propose a novel mixed-precision quantization approach tailored for LLaMA-like models. Our method leverages the observation that activation spikes in LLaMA architectures are predominantly concentrated in specific projection layers. By applying higher precision (FP16 or FP8) to these layers while quantizing the rest of the model to lower bit-widths, we achieve superior performance compared to existing quantization techniques. Experimental results on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in perplexity and zero-shot accuracy, particularly for 8-bit per-tensor quantization. Our approach outperforms general-purpose methods designed to handle outliers across all architecture types, highlighting the benefits of architecture-specific quantization strategies. This research contributes to the ongoing efforts to make LLMs more efficient and deployable, potentially enabling their use in resource-constrained environments. Our findings emphasize the importance of considering model-specific characteristics in developing effective quantization pipelines for state-of-the-art language models by identifying and targeting a small number of projections that concentrate activation spikes.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing</title>
<link>https://arxiv.org/abs/2504.21589</link>
<guid>https://arxiv.org/abs/2504.21589</guid>
<content:encoded><![CDATA[
<div> Keywords: SemEval-2025 Task 5, LLM-based Automated Subject Tagging, National Technical Library, Open-Access Catalog, few-shot prompting technique

Summary:
Our system developed for the SemEval-2025 Task 5, titled LLMs4Subjects, focuses on LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog. The system utilizes a few-shot prompting technique by providing LLMs with examples of intellectually annotated records and prompting them to suggest keywords for new records. Post-processing steps are then applied to map generated keywords to the target vocabulary, aggregate subject terms, and rank them based on relevance to the record. While our system ranks fourth in the quantitative assessment in the all-subjects track, it excels in the qualitative evaluation conducted by subject indexing experts, obtaining the best result. This approach highlights the effectiveness of combining machine learning and expert knowledge to improve subject tagging accuracy in library catalogs. The system's success underscores the importance of collaboration between automated techniques and human expertise in information organization tasks. 

Summary: <div>
arXiv:2504.21589v1 Announce Type: new 
Abstract: This paper presents our system developed for the SemEval-2025 Task 5: LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog. Our system relies on prompting a selection of LLMs with varying examples of intellectually annotated records and asking the LLMs to similarly suggest keywords for new records. This few-shot prompting technique is combined with a series of post-processing steps that map the generated keywords to the target vocabulary, aggregate the resulting subject terms to an ensemble vote and, finally, rank them as to their relevance to the record. Our system is fourth in the quantitative ranking in the all-subjects track, but achieves the best result in the qualitative ranking conducted by subject indexing experts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Misinformation Detection by Visiting Potential Commonsense Conflict</title>
<link>https://arxiv.org/abs/2504.21604</link>
<guid>https://arxiv.org/abs/2504.21604</guid>
<content:encoded><![CDATA[
<div> Misinformation Detection, Internet technology, common sense conflict, augmentation method, CoMis dataset

Summary:
Misinformation Detection (MD) is a key research area due to the prevalence of misinformation online. A novel augmentation method, MD with Potential Commonsense Conflict (MD-PCC), is proposed to enhance MD tasks. Fake articles often involve commonsense conflict, which is used to generate commonsense expressions for articles. A new dataset, CoMis, specifically focuses on fake articles caused by commonsense conflict. MD-PCC is integrated with existing MD methods and shown to outperform baseline models on various datasets. This approach highlights the importance of considering common sense in detecting misinformation. <div>
arXiv:2504.21604v1 Announce Type: new 
Abstract: The development of Internet technology has led to an increased prevalence of misinformation, causing severe negative effects across diverse domains. To mitigate this challenge, Misinformation Detection (MD), aiming to detect online misinformation automatically, emerges as a rapidly growing research topic in the community. In this paper, we propose a novel plug-and-play augmentation method for the MD task, namely Misinformation Detection with Potential Commonsense Conflict (MD-PCC). We take inspiration from the prior studies indicating that fake articles are more likely to involve commonsense conflict. Accordingly, we construct commonsense expressions for articles, serving to express potential commonsense conflicts inferred by the difference between extracted commonsense triplet and golden ones inferred by the well-established commonsense reasoning tool COMET. These expressions are then specified for each article as augmentation. Any specific MD methods can be then trained on those commonsense-augmented articles. Besides, we also collect a novel commonsense-oriented dataset named CoMis, whose all fake articles are caused by commonsense conflict. We integrate MD-PCC with various existing MD backbones and compare them across both 4 public benchmark datasets and CoMis. Empirical results demonstrate that MD-PCC can consistently outperform the existing MD baselines.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations</title>
<link>https://arxiv.org/abs/2504.21605</link>
<guid>https://arxiv.org/abs/2504.21605</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, RDF-based framework, multilingual assessment, knowledge conflicts, context prioritization

Summary: 
- The article introduces a new framework based on RDF to assess the quality of multilingual Large Language Models (LLMs) by focusing on knowledge conflicts.
- The framework evaluates LLM responses under four context conditions to identify knowledge leakage and ensure multilingual consistency.
- A fire safety domain experiment is conducted to demonstrate the framework's effectiveness in analyzing context prioritization and language-specific performance.
- The experiment highlights critical patterns in how LLMs process context information and perform across different languages.
- The study shows that the proposed vocabulary successfully captures all assessment facets encountered in the experiment, indicating the framework's comprehensive coverage of multilingual LLM assessment. 

<br /><br />Summary: <div>
arXiv:2504.21605v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability</title>
<link>https://arxiv.org/abs/2504.21625</link>
<guid>https://arxiv.org/abs/2504.21625</guid>
<content:encoded><![CDATA[
<div> keywords: LLMs, instruction-following, benchmark, Meeseeks, evaluation system <br />
Summary: <br />
Large Language Models (LLMs) rely on accurate instruction-following abilities for real-world applications. Existing benchmarks lack the ability for models to self-correct, unlike Meeseeks, which simulates human-LLM interactions through iterative feedback. Meeseeks features a comprehensive evaluation system with 38 capability tags in three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. By testing various LLMs, Meeseeks offers insights into their instruction-following capabilities for practical usage. <div>
arXiv:2504.21625v1 Announce Type: new 
Abstract: The ability to follow instructions accurately is fundamental for Large Language Models (LLMs) to serve as reliable agents in real-world applications. While existing instruction-following benchmarks are either single-turn or introduce new requirements in each turn without allowing self-correction, Meeseeks simulates realistic human-LLM interactions through an iterative feedback process. This design enables models to self-correct based on specific requirement failures, better reflecting real-world user-end usage patterns. The benchmark implements a comprehensive evaluation system with 38 capability tags organized across three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. Through rigorous evaluation across LLMs, Meeseeks provides valuable insights into LLMs' instruction-following capabilities in practical applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sadeed: Advancing Arabic Diacritization Through Small Language Model</title>
<link>https://arxiv.org/abs/2504.21635</link>
<guid>https://arxiv.org/abs/2504.21635</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic text diacritization, language model, Sadeed, benchmarking, NLP<br />
Summary: <br />
- The paper introduces Sadeed, a new approach for Arabic text diacritization based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara. 
- Sadeed is fine-tuned on high-quality diacritized datasets created through a rigorous data-cleaning and normalization process. 
- Despite using limited computational resources, Sadeed achieves competitive results compared to large language models and traditional models in the same domain. 
- The paper highlights limitations in current benchmarking practices for Arabic diacritization and proposes SadeedDiac-25, a new benchmark for more comprehensive evaluation across text genres and complexity levels. 
- Sadeed and SadeedDiac-25 together provide a strong foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools. <br /> 
Summary: <div>
arXiv:2504.21635v1 Announce Type: new 
Abstract: Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>20min-XD: A Comparable Corpus of Swiss News Articles</title>
<link>https://arxiv.org/abs/2504.21677</link>
<guid>https://arxiv.org/abs/2504.21677</guid>
<content:encoded><![CDATA[
<div> Keywords: 20min-XD, French-German, cross-lingual, document-level, news articles

Summary:
The article introduces 20min-XD, a French-German document-level comparable corpus sourced from the Swiss news outlet 20 Minuten/20 minutes. The dataset consists of 15,000 article pairs aligned based on semantic similarity from 2015 to 2024. The data collection process and alignment methodology are explained in detail. A qualitative and quantitative analysis of the corpus is provided, revealing a wide range of cross-lingual similarity levels. This diversity makes the dataset valuable for various NLP applications and linguistic studies. The dataset is publicly available in document- and sentence-aligned versions, along with code for conducting the described experiments. <div>
arXiv:2504.21677v1 Announce Type: new 
Abstract: We present 20min-XD (20 Minuten cross-lingual document-level), a French-German, document-level comparable corpus of news articles, sourced from the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises around 15,000 article pairs spanning 2015 to 2024, automatically aligned based on semantic similarity. We detail the data collection process and alignment methodology. Furthermore, we provide a qualitative and quantitative analysis of the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual similarity, ranging from near-translations to loosely related articles, making it valuable for various NLP applications and broad linguistically motivated studies. We publicly release the dataset in document- and sentence-aligned versions and code for the described experiments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders</title>
<link>https://arxiv.org/abs/2504.21681</link>
<guid>https://arxiv.org/abs/2504.21681</guid>
<content:encoded><![CDATA[
<div> transfer learning, multilingual, Vision-Language, parallel data, domain

Summary:
The study explores the transfer of already trained encoders using parallel data in multilingual Vision-Language (VL) tasks. It analyzes the impact of parallel data in terms of domain and the number of languages, which has been overlooked in previous research. Results indicate that authentic parallel data resembling captions outperforms machine-translated task data in some languages. Additionally, the study demonstrates that multilingual training benefits most languages. The findings suggest that leveraging parallel data for transferring trained encoders can be an effective strategy for multilingual VL tasks. Overall, the research sheds light on the importance of considering the characteristics of parallel data in improving the performance of multilingual VL models for downstream tasks. 

Summary: <div>
arXiv:2504.21681v1 Announce Type: new 
Abstract: Most pre-trained Vision-Language (VL) models and training data for the downstream tasks are only available in English. Therefore, multilingual VL tasks are solved using cross-lingual transfer: fine-tune a multilingual pre-trained model or transfer the text encoder using parallel data. We study the alternative approach: transferring an already trained encoder using parallel data. We investigate the effect of parallel data: domain and the number of languages, which were out of focus in previous work. Our results show that even machine-translated task data are the best on average, caption-like authentic parallel data outperformed it in some languages. Further, we show that most languages benefit from multilingual training.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning</title>
<link>https://arxiv.org/abs/2504.21685</link>
<guid>https://arxiv.org/abs/2504.21685</guid>
<content:encoded><![CDATA[
<div> POS tagger, PEFT techniques, biomedical NLP, health mention classification, social media<br />
<br />
Summary: 
Health Mention Classification (HMC) in social media posts is crucial for real-time public health monitoring but faces challenges due to contextual nuances. This study proposes improving HMC through enhanced parameters in biomedical NLP methods. By incorporating POS tagger information and leveraging PEFT techniques, the study achieves better F1-score performance on three datasets (RHDM, PHM, Illness) compared to existing methods. The findings indicate the effectiveness of these enhancements in accurately classifying health mentions while optimizing model size and training efficiency. This suggests a promising approach for enhancing HMC in social media for public health monitoring. <div>
arXiv:2504.21685v1 Announce Type: new 
Abstract: Health Mention Classification (HMC) plays a critical role in leveraging social media posts for real-time tracking and public health monitoring. Nevertheless, the process of HMC presents significant challenges due to its intricate nature, primarily stemming from the contextual aspects of health mentions, such as figurative language and descriptive terminology, rather than explicitly reflecting a personal ailment. To address this problem, we argue that clearer mentions can be achieved through conventional fine-tuning with enhanced parameters of biomedical natural language methods (NLP). In this study, we explore different techniques such as the utilisation of part-of-speech (POS) tagger information, improving on PEFT techniques, and different combinations thereof. Extensive experiments are conducted on three widely used datasets: RHDM, PHM, and Illness. The results incorporated POS tagger information, and leveraging PEFT techniques significantly improves performance in terms of F1-score compared to state-of-the-art methods across all three datasets by utilising smaller models and efficient training. Furthermore, the findings highlight the effectiveness of incorporating POS tagger information and leveraging PEFT techniques for HMC. In conclusion, the proposed methodology presents a potentially effective approach to accurately classifying health mentions in social media posts while optimising the model size and training efficiency.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models</title>
<link>https://arxiv.org/abs/2504.21742</link>
<guid>https://arxiv.org/abs/2504.21742</guid>
<content:encoded><![CDATA[
<div> Keywords: Greek fiction, love novels, romances, literary motifs, large language models

Summary: 
The study examines Greek fictional narratives, specifically love novels and romances, from the first century CE to the 15th century. Using large language models, the research aims to identify common motifs in these texts and how they differ. Results reveal persistent motifs and fluctuations in frequency, indicating trends and external influences. The method effectively extracts literary motifs for quantitative and qualitative analysis. By analyzing these motifs, the study provides insights into the similarities and differences within the corpus, shedding light on the evolving nature of Greek fiction over time.<br /><br />Summary: <div>
arXiv:2504.21742v1 Announce Type: new 
Abstract: The Greek fictional narratives often termed love novels or romances, ranging from the first century CE to the middle of the 15th century, have long been considered as similar in many ways, not least in the use of particular literary motifs. By applying the use of fine-tuned large language models, this study aims to investigate which motifs exactly that the texts in this corpus have in common, and in which ways they differ from each other. The results show that while some motifs persist throughout the corpus, others fluctuate in frequency, indicating certain trends or external influences. Conclusively, the method proves to adequately extract literary motifs according to a set definition, providing data for both quantitative and qualitative analyses.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data</title>
<link>https://arxiv.org/abs/2504.21747</link>
<guid>https://arxiv.org/abs/2504.21747</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented neural machine translation, cross-lingual retrieval systems, bilingual corpora, translation memories, target monolingual resources<br />
<br />
Summary: 
This paper presents a novel approach to enhance neural machine translation systems by leveraging in-domain monolingual target-side corpora for cross-lingual retrieval. By developing improved cross-lingual retrieval systems trained with sentence and word-level matching objectives, the researchers achieved superior translation performance compared to traditional translation memory-based models. In controlled experiments with two RANMT architectures, the benefits of cross-lingual objectives were evident, surpassing standard TM-based models. Real-world applications demonstrated significant improvements when target monolingual resources were abundant. The new techniques outperformed baseline settings and general-purpose cross-lingual retrievers, showcasing the effectiveness of utilizing target language resources for enhancing translation quality. <br /><br />Summary: <div>
arXiv:2504.21747v1 Announce Type: new 
Abstract: Conventional retrieval-augmented neural machine translation (RANMT) systems leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many settings, in-domain monolingual target-side corpora are often available. This work explores ways to take advantage of such resources by retrieving relevant segments directly in the target language, based on a source-side query. For this, we design improved cross-lingual retrieval systems, trained with both sentence level and word-level matching objectives. In our experiments with two RANMT architectures, we first demonstrate the benefits of such cross-lingual objectives in a controlled setting, obtaining translation performances that surpass standard TM-based models. We then showcase our method on a real-world set-up, where the target monolingual resources far exceed the amount of parallel data and observe large improvements of our new techniques, which outperform both the baseline setting, and general-purpose cross-lingual retrievers.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness</title>
<link>https://arxiv.org/abs/2504.21773</link>
<guid>https://arxiv.org/abs/2504.21773</guid>
<content:encoded><![CDATA[
<div> hallucination, large language models, confidence estimation, multiple problems, MAC-Tuning <br />
Summary:<br />
The article introduces a novel method called Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning) to address the issue of hallucination in large language models (LLMs) when generating non-existing facts. While previous research has focused on single problem settings, the more challenging multi-problem setting, where LLMs need to answer multiple problems accurately simultaneously, has been overlooked. MAC-Tuning separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Through extensive experiments, it was shown that MAC-Tuning outperforms baselines by up to 25% in average precision. This approach improves LLM awareness of its internal parameterized knowledge boundary, enhancing confidence estimation and performance in answering multiple problems. <div>
arXiv:2504.21773v1 Announce Type: new 
Abstract: With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebThinker: Empowering Large Reasoning Models with Deep Research Capability</title>
<link>https://arxiv.org/abs/2504.21776</link>
<guid>https://arxiv.org/abs/2504.21776</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, WebThinker, Deep Web Explorer, Autonomous Think-Search-and-Draft strategy, RL-based training strategy

Summary:
Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have impressive long-horizon reasoning capabilities but struggle with complex tasks that require dynamic web information synthesis. To address this, WebThinker is proposed as a deep research agent enabling LRMs to autonomously search, navigate the web, and draft research reports. It integrates a Deep Web Explorer module for dynamic information extraction and an Autonomous Think-Search-and-Draft strategy for real-time reasoning and report writing. An RL-based training strategy using Direct Preference Optimization enhances research tool utilization. Extensive experiments on reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report tasks (Glaive) show that WebThinker outperforms existing methods and proprietary systems. This approach enhances LRM reliability in complex scenarios, paving the way for more capable deep research systems. 

<br /><br />Summary: <div>
arXiv:2504.21776v1 Announce Type: new 
Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues</title>
<link>https://arxiv.org/abs/2504.21800</link>
<guid>https://arxiv.org/abs/2504.21800</guid>
<content:encoded><![CDATA[
<div> privacy concerns, synthetic data, Prolonged Exposure, Post-Traumatic Stress Disorder, clinical fidelity 

Summary:
The article discusses the use of synthetic data in healthcare, particularly in training clinical models for Prolonged Exposure therapy for PTSD. It compares real and synthetic dialogues using various metrics and introduces PE-specific metrics for assessing clinical fidelity. While synthetic data can address data scarcity and privacy concerns, it may struggle to capture the nuances of therapeutic interactions. Synthetic dialogues in the dataset matched structural features of real-world dialogues but lacked key fidelity markers like distress monitoring. The study highlights the need for fidelity-aware metrics beyond surface fluency to identify clinically significant shortcomings. The findings emphasize the potential benefits of synthetic data while also highlighting critical limitations that need to be addressed. 

<br /><br />Summary: <div>
arXiv:2504.21800v1 Announce Type: new 
Abstract: The growing adoption of synthetic data in healthcare is driven by privacy concerns, limited access to real-world data, and the high cost of annotation. This work explores the use of synthetic Prolonged Exposure (PE) therapeutic conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable alternative for training and evaluating clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics, including turn-taking patterns and treatment fidelity. We also introduce and evaluate PE-specific metrics derived from linguistic analysis and semantic modeling, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that although synthetic data holds promise for mitigating data scarcity and protecting patient privacy, it can struggle to capture the subtle dynamics of therapeutic interactions. In our dataset, synthetic dialogues match structural features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99), however, synthetic interactions do not adequately reflect key fidelity markers (e.g., distress monitoring). We highlight gaps in existing evaluation frameworks and advocate for fidelity-aware metrics that go beyond surface fluency to uncover clinically significant failures. Our findings clarify where synthetic data can effectively complement real-world datasets -- and where critical limitations remain.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition</title>
<link>https://arxiv.org/abs/2504.21801</link>
<guid>https://arxiv.org/abs/2504.21801</guid>
<content:encoded><![CDATA[
<div> DeepSeek-Prover-V2, large language model, formal theorem proving, Lean 4, DeepSeek-V3
<br />
Summary:
DeepSeek-Prover-V2 is a new open-source large language model for formal theorem proving in Lean 4. It incorporates informal and formal mathematical reasoning in its training process by decomposing complex problems into subgoals and synthesizing proofs. The model, DeepSeek-Prover-V2-671B, demonstrates state-of-the-art performance in neural theorem proving, achieving an 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. A new benchmark, ProverBench, consisting of 325 formalized problems including selected AIME competition problems, is introduced for evaluation. The model successfully solves 6 out of 15 AIME problems, narrowing the gap between formal and informal mathematical reasoning in large language models. DeepSeek-V3, the precursor, also performs well on these problems, solving 8 using majority voting. This research demonstrates significant progress in leveraging large language models for formal theorem proving tasks. 
<br /><br />Summary: <div>
arXiv:2504.21801v1 Announce Type: new 
Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments</title>
<link>https://arxiv.org/abs/2504.21851</link>
<guid>https://arxiv.org/abs/2504.21851</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, TRUST framework, dialogue systems, mental healthcare, diagnostic interviews<br />
Summary:<br />
The study introduces TRUST, a framework that utilizes Large Language Models (LLMs) to conduct diagnostic interviews and assessments for PTSD. By implementing a Dialogue Acts schema specifically designed for clinical interviews, TRUST replicates clinician behavior effectively. The system also incorporates a patient simulation approach based on real-life interview transcripts, eliminating the need for manual testing by clinicians. Evaluation metrics demonstrate that TRUST performs comparably to real-life clinical interviews, indicating its potential to improve mental healthcare accessibility. Expert evaluations suggest that the system operates at the level of average clinicians, showcasing room for future enhancements in communication styles and response appropriateness. Overall, the TRUST framework shows promise in facilitating mental healthcare availability. <br /><br />Summary: <div>
arXiv:2504.21851v1 Announce Type: new 
Abstract: Objectives: While Large Language Models (LLMs) have been widely used to assist clinicians and support patients, no existing work has explored dialogue systems for standard diagnostic interviews and assessments. This study aims to bridge the gap in mental healthcare accessibility by developing an LLM-powered dialogue system that replicates clinician behavior. Materials and Methods: We introduce TRUST, a framework of cooperative LLM modules capable of conducting formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder (PTSD). To guide the generation of appropriate clinical responses, we propose a Dialogue Acts schema specifically designed for clinical interviews. Additionally, we develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Results: A comprehensive set of evaluation metrics is designed to assess the dialogue system from both the agent and patient simulation perspectives. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real-life clinical interviews. Discussion: Our system performs at the level of average clinicians, with room for future enhancements in communication styles and response appropriateness. Conclusions: Our TRUST framework shows its potential to facilitate mental healthcare availability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval</title>
<link>https://arxiv.org/abs/2504.21015</link>
<guid>https://arxiv.org/abs/2504.21015</guid>
<content:encoded><![CDATA[
<div> Keywords: dense retrieval models, Large Language Model (LLM), corpus-free negative generation, BM25, cross-encoders (CE)

Summary: 
This paper presents a novel approach to training dense retrieval models using a Large Language Model (LLM) to generate hard negative examples without the need for full corpus access. The proposed pipeline, called LLM Query -> LLM HN, outperforms traditional methods like LLM Query -> BM25 HN and LLM Query -> CE HN in terms of nDCG@10, Precision@10, and Recall@100 metrics using E5-Base and GTE-Base models on various benchmark datasets. The corpus-free negative generation method proves to be as effective as complex, corpus-dependent mining techniques while simplifying the training process and improving efficiency. The dataset containing queries and hard negatives for all three methods is publicly available. This approach offers a promising pathway for training high-performance retrievers without compromising on results. 

<br /><br />Summary: <div>
arXiv:2504.21015v1 Announce Type: cross 
Abstract: Training effective dense retrieval models often relies on hard negative (HN) examples mined from the document corpus via methods like BM25 or cross-encoders (CE), processes that can be computationally demanding and require full corpus access. This paper introduces a different approach, an end-to-end pipeline where a Large Language Model (LLM) first generates a query from a passage, and then generates a hard negative example using \emph{only} that query text. This corpus-free negative generation contrasts with standard mining techniques. We evaluated this \textsc{LLM Query $\rightarrow$ LLM HN} approach against traditional \textsc{LLM Query $\rightarrow$ BM25 HN} and \textsc{LLM Query $\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several BEIR benchmark datasets. Our results show the proposed all-LLM pipeline achieves performance identical to both the BM25 and the computationally intensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics. This demonstrates that our corpus-free negative generation method matches the effectiveness of complex, corpus-dependent mining techniques, offering a potentially simpler and more efficient pathway for training high-performance retrievers without sacrificing results. We make the dataset including the queries and the hard-negatives for all three methods publicly available https://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage</title>
<link>https://arxiv.org/abs/2504.21035</link>
<guid>https://arxiv.org/abs/2504.21035</guid>
<content:encoded><![CDATA[
<div> Keywords: privacy, text data, re-identification attacks, auxiliary information, sanitization techniques<br />
<br />
Summary: 
The study challenges the effectiveness of traditional methods in sanitizing sensitive text data to protect privacy. It introduces a new framework that evaluates re-identification attacks to measure individual privacy risks accurately. The research reveals that seemingly insignificant auxiliary information can be utilized to infer sensitive attributes from sanitized data, undermining the perceived privacy protection. The study demonstrates that popular PII removal tools fail to safeguard a significant portion of information, emphasizing the false sense of privacy provided by current sanitization techniques. While differential privacy offers some mitigation, it compromises the utility of sanitized text for practical applications. The findings underscore the necessity for more robust methods to prevent semantic-level information leakage and ensure comprehensive privacy protection in text data. <br /><br />Summary: <div>
arXiv:2504.21035v1 Announce Type: cross 
Abstract: Sanitizing sensitive text data typically involves removing personally identifiable information (PII) or generating synthetic data under the assumption that these methods adequately protect privacy; however, their effectiveness is often only assessed by measuring the leakage of explicit identifiers but ignoring nuanced textual markers that can lead to re-identification. We challenge the above illusion of privacy by proposing a new framework that evaluates re-identification attacks to quantify individual privacy risks upon data release. Our approach shows that seemingly innocuous auxiliary information -- such as routine social activities -- can be used to infer sensitive attributes like age or substance use history from sanitized data. For instance, we demonstrate that Azure's commercial PII removal tool fails to protect 74\% of information in the MedQA dataset. Although differential privacy mitigates these risks to some extent, it significantly reduces the utility of the sanitized text for downstream tasks. Our findings indicate that current sanitization techniques offer a \textit{false sense of privacy}, highlighting the need for more robust methods that protect against semantic-level information leakage.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Large Language Models for Medicine: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2504.21051</link>
<guid>https://arxiv.org/abs/2504.21051</guid>
<content:encoded><![CDATA[
arXiv:2504.21051v1 Announce Type: cross 
Abstract: MLLMs have recently become a focal point in the field of artificial intelligence research. Building on the strong capabilities of LLMs, MLLMs are adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs have gained substantial attention from different domains. Researchers have begun to explore the potential of MLLMs in the medical and healthcare domain. In this paper, we first introduce the background and fundamental concepts related to LLMs and MLLMs, while emphasizing the working principles of MLLMs. Subsequently, we summarize three main directions of application within healthcare: medical reporting, medical diagnosis, and medical treatment. Our findings are based on a comprehensive review of 330 recent papers in this area. We illustrate the remarkable capabilities of MLLMs in these domains by providing specific examples. For data, we present six mainstream modes of data along with their corresponding evaluation benchmarks. At the end of the survey, we discuss the challenges faced by MLLMs in the medical and healthcare domain and propose feasible methods to mitigate or overcome these issues.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phi-4-reasoning Technical Report</title>
<link>https://arxiv.org/abs/2504.21318</link>
<guid>https://arxiv.org/abs/2504.21318</guid>
<content:encoded><![CDATA[
arXiv:2504.21318v1 Announce Type: cross 
Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets the Callback? Generative AI and Gender Bias</title>
<link>https://arxiv.org/abs/2504.21400</link>
<guid>https://arxiv.org/abs/2504.21400</guid>
<content:encoded><![CDATA[
arXiv:2504.21400v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI), particularly large language models (LLMs), is being rapidly deployed in recruitment and for candidate shortlisting. We audit several mid-sized open-source LLMs for gender bias using a dataset of 332,044 real-world online job postings. For each posting, we prompt the model to recommend whether an equally qualified male or female candidate should receive an interview callback. We find that most models tend to favor men, especially for higher-wage roles. Mapping job descriptions to the Standard Occupational Classification system, we find lower callback rates for women in male-dominated occupations and higher rates in female-associated ones, indicating occupational segregation. A comprehensive analysis of linguistic features in job ads reveals strong alignment of model recommendations with traditional gender stereotypes. To examine the role of recruiter identity, we steer model behavior by infusing Big Five personality traits and simulating the perspectives of historical figures. We find that less agreeable personas reduce stereotyping, consistent with an agreeableness bias in LLMs. Our findings highlight how AI-driven hiring may perpetuate biases in the labor market and have implications for fairness and diversity within firms.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
<link>https://arxiv.org/abs/2504.21435</link>
<guid>https://arxiv.org/abs/2504.21435</guid>
<content:encoded><![CDATA[
arXiv:2504.21435v1 Announce Type: cross 
Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on \textbf{standalone} videos and mainly assess ``visual elements'' like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a \textbf{series}. To address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on \textbf{SeriesBench} indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while \textbf{PC-DCoT} enables these MLLMs to achieve performance improvements. Overall, our \textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models</title>
<link>https://arxiv.org/abs/2504.21559</link>
<guid>https://arxiv.org/abs/2504.21559</guid>
<content:encoded><![CDATA[
arXiv:2504.21559v1 Announce Type: cross 
Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting -- overlaying visual cues (e.g., bounding box, circle) on images -- can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR demonstrate that BBVPE effectively reduces object hallucination.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks</title>
<link>https://arxiv.org/abs/2504.21578</link>
<guid>https://arxiv.org/abs/2504.21578</guid>
<content:encoded><![CDATA[
arXiv:2504.21578v1 Announce Type: cross 
Abstract: Diabetes is a civilization chronic disease characterized by a constant elevated concentration of glucose in the blood. Many processes are involved in the glucose regulation, and their interactions are very complex. To better understand those processes we set ourselves a goal to create a Petri net model of the glucose regulation in the whole body. So far we have managed to create a model of glycolysis and synthesis of glucose in the liver, and the general overview models of the glucose regulation in a healthy and diabetic person. In this paper we introduce Petri nets models of insulin secretion in beta cell of the pancreas, and glucagon in the pancreas alpha cells. Those two hormones have mutually opposite effects: insulin preventing hyperglycemia, and glucagon preventing hypoglycemia. Understanding the mechanisms of insulin and glucagon secretion constitutes the basis for understanding diabetes. We also present a model in which both processes occur together, depending on the blood glucose level. The dynamics of each model is analysed. Additionally, we transform the overall insulin and glucagon secretion system to a Boolean network, following standard transformation rules.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization</title>
<link>https://arxiv.org/abs/2504.21659</link>
<guid>https://arxiv.org/abs/2504.21659</guid>
<content:encoded><![CDATA[
arXiv:2504.21659v1 Announce Type: cross 
Abstract: Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics</title>
<link>https://arxiv.org/abs/2504.21716</link>
<guid>https://arxiv.org/abs/2504.21716</guid>
<content:encoded><![CDATA[
arXiv:2504.21716v1 Announce Type: cross 
Abstract: We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation</title>
<link>https://arxiv.org/abs/2504.21751</link>
<guid>https://arxiv.org/abs/2504.21751</guid>
<content:encoded><![CDATA[
arXiv:2504.21751v1 Announce Type: cross 
Abstract: Real world development demands code that is readable, extensible, and testable by organizing the implementation into modular components and iteratively reuse pre-implemented code. We term this iterative, multi-turn process codeflow and introduce CodeFlowBench, the first benchmark designed for comprehensively evaluating LLMs' ability to perform codeflow, namely to implement new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously updated via an automated pipeline that decomposes each problem into a series of function-level subproblems based on its dependency tree and each subproblem is paired with unit tests. We further propose a novel evaluation framework with tasks and metrics tailored to multi-turn code reuse to assess model performance. In experiments across various LLMs under both multi-turn and single-turn patterns. We observe models' poor performance on CodeFlowBench, with a substantial performance drop in the iterative codeflow scenario. For instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8% in single-turn pattern. Further analysis shows that different models excel at different dependency depths, yet all struggle to correctly solve structurally complex problems, highlighting challenges for current LLMs to serve as code generation tools when performing codeflow. Overall, CodeFlowBench offers a comprehensive benchmark and new insights into LLM capabilities for multi-turn, iterative code generation, guiding future advances in code generation tasks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-smith: Scaling Data for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2504.21798</link>
<guid>https://arxiv.org/abs/2504.21798</guid>
<content:encoded><![CDATA[
arXiv:2504.21798v1 Announce Type: cross 
Abstract: Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection</title>
<link>https://arxiv.org/abs/2310.18964</link>
<guid>https://arxiv.org/abs/2310.18964</guid>
<content:encoded><![CDATA[
arXiv:2310.18964v4 Announce Type: replace 
Abstract: In the evolving landscape of online communication, hate speech detection remains a formidable challenge, further compounded by the diversity of digital platforms. This study investigates the effectiveness and adaptability of pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate speech, to address two central questions: (1) To what extent does the model performance depend on the fine-tuning and training parameters?, (2) To what extent do models generalize to cross-domain hate speech detection? and (3) What are the specific features of the datasets or models that influence the generalization potential? The experiment shows that LLMs offer a huge advantage over the state-of-the-art even without pretraining. Ordinary least squares analyses suggest that the advantage of training with fine-grained hate speech labels is washed away with the increase in dataset size. While our research demonstrates the potential of large language models (LLMs) for hate speech detection, several limitations remain, particularly regarding the validity and the reproducibility of the results. We conclude with an exhaustive discussion of the challenges we faced in our experimentation and offer recommended best practices for future scholars designing benchmarking experiments of this kind.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Round Trip Translation Defence against Large Language Model Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2402.13517</link>
<guid>https://arxiv.org/abs/2402.13517</guid>
<content:encoded><![CDATA[
arXiv:2402.13517v2 Announce Type: replace 
Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence
  This version of the article has been accepted for publication, after peer review (when applicable) but is not the Version of Record and does not reflect post-acceptance improvements, or any corrections. The Version of Record is available online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this Accepted Version is subject to the publisher's Accepted Manuscript terms of use https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs</title>
<link>https://arxiv.org/abs/2404.19442</link>
<guid>https://arxiv.org/abs/2404.19442</guid>
<content:encoded><![CDATA[
arXiv:2404.19442v5 Announce Type: replace 
Abstract: Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian Pidgin spoken by approximately 120M speakers and it is a mixed language (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a spoken language until recently, there are some online platforms (e.g., Wikipedia), publishing in written Naija as well. West African Pidgin English (WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the internet to a wider audience not only in Nigeria but also in other West African countries (e.g., Cameroon and Ghana). Through statistical analyses and Machine Translation experiments, our paper shows that these two pidgin varieties do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on WAPE. In other words, Naija is underrepresented in Generative AI, and it is hard to teach LLMs with few examples. In addition to the statistical analyses, we also provide historical information on both pidgins as well as insights from the interviews conducted with volunteer Wikipedia contributors in Naija.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of a High-Dimensional Abstraction Phase in Language Transformers</title>
<link>https://arxiv.org/abs/2405.15471</link>
<guid>https://arxiv.org/abs/2405.15471</guid>
<content:encoded><![CDATA[
arXiv:2405.15471v4 Announce Type: replace 
Abstract: A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models</title>
<link>https://arxiv.org/abs/2410.07825</link>
<guid>https://arxiv.org/abs/2410.07825</guid>
<content:encoded><![CDATA[
arXiv:2410.07825v2 Announce Type: replace 
Abstract: Multi-lingual ability transfer has become increasingly important for the broad application of large language models (LLMs). Existing work highly relies on training with the multi-lingual ability-related data, which may be not available for low-resource languages. To solve it, we propose a Multi-lingual Ability Extraction and Transfer approach, named as MAET. Our key idea is to decompose and extract language-agnostic ability-related weights from LLMs, and transfer them across different languages by simple addition and subtraction operations without training. Specially, our MAET consists of the extraction and transfer stages. In the extraction stage, we firstly locate key neurons that are highly related to specific abilities, and then employ them to extract the transferable ability-specific weights. In the transfer stage, we further select the ability-related parameter tensors, and design the merging strategy based on the linguistic and ability specific weights, to build the multi-lingual ability-enhanced LLM. To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on mathematical and scientific tasks in both high-resource lingual and low-resource lingual scenarios. Experiment results have shown that MAET can effectively and efficiently extract and transfer the advanced abilities, and outperform training-based baseline methods. Our code and data are available at https://github.com/RUCAIBox/MAET.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent</title>
<link>https://arxiv.org/abs/2410.16658</link>
<guid>https://arxiv.org/abs/2410.16658</guid>
<content:encoded><![CDATA[
arXiv:2410.16658v3 Announce Type: replace 
Abstract: Adsorption energy is a key reactivity descriptor in catalysis, enabling efficient screening for optimal catalysts. However, determining adsorption energy typically requires evaluating numerous adsorbate-catalyst configurations. Current algorithmic approaches rely on exhaustive enumeration of adsorption sites and configurations, which makes the process computationally intensive and does not inherently guarantee the identification of the global minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify system-specific stable adsorption configurations corresponding to the global minimum adsorption energy. Adsorb-Agent leverages its built-in knowledge and emergent reasoning capabilities to strategically explore adsorption configurations likely to hold adsorption energy. By reducing the reliance on exhaustive sampling, it significantly decreases the number of initial configurations required while improving the accuracy of adsorption energy predictions. We evaluate Adsorb-Agent's performance across twenty representative systems encompassing a range of complexities. The Adsorb-Agent successfully identifies comparable adsorption energies for 83.7% of the systems and achieves lower energies, closer to the actual global minimum, for 35% of the systems, while requiring significantly fewer initial configurations than conventional methods. Its capability is particularly evident in complex systems, where it identifies lower adsorption energies for 46.7% of systems involving intermetallic surfaces and 66.7% of systems with large adsorbate molecules. These results demonstrate the potential of Adsorb-Agent to accelerate catalyst discovery by reducing computational costs and improving the reliability of adsorption energy predictions.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities</title>
<link>https://arxiv.org/abs/2501.00571</link>
<guid>https://arxiv.org/abs/2501.00571</guid>
<content:encoded><![CDATA[
arXiv:2501.00571v3 Announce Type: replace 
Abstract: Document-level relation extraction (Doc-RE) aims to extract relations between entities across multiple sentences. Therefore, Doc-RE requires more comprehensive reasoning abilities like humans, involving complex cross-sentence interactions between entities, contexts, and external general knowledge, compared to the sentence-level RE. However, most existing Doc-RE methods focus on optimizing single reasoning ability, but lack the ability to utilize external knowledge for comprehensive reasoning on long documents. To solve these problems, a knowledge retrieval augmented method, named KnowRA, was proposed with comprehensive reasoning to autonomously determine whether to accept external knowledge to assist DocRE. Firstly, we constructed a document graph for semantic encoding and integrated the co-reference resolution model to augment the co-reference reasoning ability. Then, we expanded the document graph into a document knowledge graph by retrieving the external knowledge base for common-sense reasoning and a novel knowledge filtration method was presented to filter out irrelevant knowledge. Finally, we proposed the axis attention mechanism to build direct and indirect associations with intermediary entities for achieving cross-sentence logical reasoning. Extensive experiments conducted on two datasets verified the effectiveness of our method compared to the state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/KnowRA.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification</title>
<link>https://arxiv.org/abs/2502.11258</link>
<guid>https://arxiv.org/abs/2502.11258</guid>
<content:encoded><![CDATA[
arXiv:2502.11258v2 Announce Type: replace 
Abstract: Although large language models (LLMs) have demonstrated remarkable capabilities in recent years, the potential of information theory (IT) to enhance LLM development remains underexplored. This paper introduces the information theoretic principle of Conditional Mutual Information (CMI) to LLM fine-tuning for classification tasks, exploring its promise in two main ways: minimizing CMI to improve a model's standalone performance and maximizing CMI to enhance knowledge distillation (KD) for more capable student models. To apply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained deep learning framework, which was initially developed for image classification, with some modification. By minimizing CMI during LLM fine-tuning, we achieve superior performance gains on 6 of 8 GLUE classification tasks compared to BERT. Additionally, maximizing CMI during the KD process results in significant performance improvements in 6 of 8 GLUE classification tasks compared to DistilBERT. These findings demonstrate CMI's adaptability for optimizing both standalone LLMs and student models, showcasing its potential as a robust framework for advancing LLM fine-tuning. Our work bridges the gap between information theory and LLM development, offering new insights for building high-performing language models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice</title>
<link>https://arxiv.org/abs/2503.04785</link>
<guid>https://arxiv.org/abs/2503.04785</guid>
<content:encoded><![CDATA[
arXiv:2503.04785v2 Announce Type: replace 
Abstract: The rapid proliferation of Large Language Models (LLMs) has raised pressing concerns regarding their trustworthiness, spanning issues of reliability, transparency, fairness, and ethical alignment. Despite the increasing adoption of LLMs across various domains, there remains a lack of consensus on how to operationalize trustworthiness in practice. This study bridges the gap between theoretical discussions and implementation by conducting a bibliometric mapping analysis of 2,006 publications from 2019 to 2025. Through co-authorship networks, keyword co-occurrence analysis, and thematic evolution tracking, we identify key research trends, influential authors, and prevailing definitions of LLM trustworthiness. Additionally, a systematic review of 68 core papers is conducted to examine conceptualizations of trust and their practical implications. Our findings reveal that trustworthiness in LLMs is often framed through existing organizational trust frameworks, emphasizing dimensions such as ability, benevolence, and integrity. However, a significant gap exists in translating these principles into concrete development strategies. To address this, we propose a structured mapping of 20 trust-enhancing techniques across the LLM lifecycle, including retrieval-augmented generation (RAG), explainability techniques, and post-training audits. By synthesizing bibliometric insights with practical strategies, this study contributes towards fostering more transparent, accountable, and ethically aligned LLMs, ensuring their responsible deployment in real-world applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System</title>
<link>https://arxiv.org/abs/2503.14258</link>
<guid>https://arxiv.org/abs/2503.14258</guid>
<content:encoded><![CDATA[
arXiv:2503.14258v3 Announce Type: replace 
Abstract: This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: https://github.com/oneal2000/JuDGE.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad</title>
<link>https://arxiv.org/abs/2503.21934</link>
<guid>https://arxiv.org/abs/2503.21934</guid>
<content:encoded><![CDATA[
arXiv:2503.21934v4 Announce Type: replace 
Abstract: Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge</title>
<link>https://arxiv.org/abs/2504.10342</link>
<guid>https://arxiv.org/abs/2504.10342</guid>
<content:encoded><![CDATA[
arXiv:2504.10342v3 Announce Type: replace 
Abstract: Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with "thinking" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding</title>
<link>https://arxiv.org/abs/2301.11564</link>
<guid>https://arxiv.org/abs/2301.11564</guid>
<content:encoded><![CDATA[
arXiv:2301.11564v3 Announce Type: replace-cross 
Abstract: Robotic grasping is a fundamental ability for a robot to interact with the environment. Current methods focus on how to obtain a stable and reliable grasping pose in object level, while little work has been studied on part (shape)-wise grasping which is related to fine-grained grasping and robotic affordance. Parts can be seen as atomic elements to compose an object, which contains rich semantic knowledge and a strong correlation with affordance. However, lacking a large part-wise 3D robotic dataset limits the development of part representation learning and downstream applications. In this paper, we propose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to promote 3D part-level affordance and grasping ability learning. From the perspective of robotic cognition, we design a two-stage fine-grained robotic grasping framework (named LangPartGPD), including a novel 3D part language grounding model and a part-aware grasp pose detection model, in which explicit language input from human or large language models (LLMs) could guide a robot to generate part-level 6-DoF grasping pose with textual explanation. Our method combines the advantages of human-robot collaboration and LLMs' planning ability using explicit language as a symbolic intermediate. To evaluate the effectiveness of our proposed method, we perform 3D part grounding and fine-grained grasp detection experiments on both simulation and physical robot settings, following language instructions across different degrees of textual complexity. Results show our method achieves competitive performance in 3D geometry fine-grained grounding, object affordance inference, and 3D part-aware grasping tasks. Our dataset and code are available on our project website https://sites.google.com/view/lang-shape
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignLLM: Sign Language Production Large Language Models</title>
<link>https://arxiv.org/abs/2405.10718</link>
<guid>https://arxiv.org/abs/2405.10718</guid>
<content:encoded><![CDATA[
arXiv:2405.10718v3 Announce Type: replace-cross 
Abstract: In this paper, we propose SignLLM, a multilingual Sign Language Production (SLP) large language model, which includes two novel multilingual SLP modes MLSF and Prompt2LangGloss that allow sign language gestures generation from query texts input and question-style prompts input respectively. Both modes can use a new RL loss based on reinforcement learning and a new RL module named Priority Learning Channel. These RL components can accelerate the training by enhancing the model's capability to sample high-quality data. To train SignLLM, we introduce Prompt2Sign, a comprehensive multilingual sign language dataset, which builds from public data, including American Sign Language (ASL) and seven others. This dataset standardizes information by extracting pose information from sign language videos into a unified compressed format. We extensively evaluate SignLLM, demonstrating that our model achieves state-of-the-art performance on SLP tasks across eight sign languages.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes</title>
<link>https://arxiv.org/abs/2408.05794</link>
<guid>https://arxiv.org/abs/2408.05794</guid>
<content:encoded><![CDATA[
arXiv:2408.05794v2 Announce Type: replace-cross 
Abstract: Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within ``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new framework designed to enhance the detection and segmentation of hateful elements in memes. \textsc{HateSieve} features a novel Contrastive Meme Generator that creates semantically paired memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. \textcolor{red}{Caution: Contains academic discussions of hate speech; viewer discretion advised.}
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance</title>
<link>https://arxiv.org/abs/2409.15545</link>
<guid>https://arxiv.org/abs/2409.15545</guid>
<content:encoded><![CDATA[
arXiv:2409.15545v3 Announce Type: replace-cross 
Abstract: The complex nature of musical emotion introduces inherent bias in both recognition and generation, particularly when relying on a single audio encoder, emotion classifier, or evaluation metric. In this work, we conduct a study on Music Emotion Recognition (MER) and Emotional Music Generation (EMG), employing diverse audio encoders alongside Frechet Audio Distance (FAD), a reference-free evaluation metric. Our study begins with a benchmark evaluation of MER, highlighting the limitations of using a single audio encoder and the disparities observed across different measurements. We then propose assessing MER performance using FAD derived from multiple encoders to provide a more objective measure of musical emotion. Furthermore, we introduce an enhanced EMG approach designed to improve both the variability and prominence of generated musical emotion, thereby enhancing its realism. Additionally, we investigate the differences in realism between the emotions conveyed in real and synthetic music, comparing our EMG model against two baseline models. Experimental results underscore the issue of emotion bias in both MER and EMG and demonstrate the potential of using FAD and diverse audio encoders to evaluate musical emotion more objectively and effectively.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction</title>
<link>https://arxiv.org/abs/2409.15551</link>
<guid>https://arxiv.org/abs/2409.15551</guid>
<content:encoded><![CDATA[
arXiv:2409.15551v2 Announce Type: replace-cross 
Abstract: Annotating and recognizing speech emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic Speech Recognition (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize prompting pipeline for robust LLM-based emotion recognition from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion recognition. Our study aims to refine the use of LLMs in emotion recognition and related domains.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models</title>
<link>https://arxiv.org/abs/2409.16920</link>
<guid>https://arxiv.org/abs/2409.16920</guid>
<content:encoded><![CDATA[
arXiv:2409.16920v2 Announce Type: replace-cross 
Abstract: Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling</title>
<link>https://arxiv.org/abs/2409.16937</link>
<guid>https://arxiv.org/abs/2409.16937</guid>
<content:encoded><![CDATA[
arXiv:2409.16937v3 Announce Type: replace-cross 
Abstract: The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations</title>
<link>https://arxiv.org/abs/2409.17899</link>
<guid>https://arxiv.org/abs/2409.17899</guid>
<content:encoded><![CDATA[
arXiv:2409.17899v2 Announce Type: replace-cross 
Abstract: Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Construct Random Unitaries</title>
<link>https://arxiv.org/abs/2410.10116</link>
<guid>https://arxiv.org/abs/2410.10116</guid>
<content:encoded><![CDATA[
arXiv:2410.10116v2 Announce Type: replace-cross 
Abstract: The existence of pseudorandom unitaries (PRUs) -- efficient quantum circuits that are computationally indistinguishable from Haar-random unitaries -- has been a central open question, with significant implications for cryptography, complexity theory, and fundamental physics. In this work, we close this question by proving that PRUs exist, assuming that any quantum-secure one-way function exists. We establish this result for both (1) the standard notion of PRUs, which are secure against any efficient adversary that makes queries to the unitary $U$, and (2) a stronger notion of PRUs, which are secure even against adversaries that can query both the unitary $U$ and its inverse $U^\dagger$. In the process, we prove that any algorithm that makes queries to a Haar-random unitary can be efficiently simulated on a quantum computer, up to inverse-exponential trace distance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2411.08165</link>
<guid>https://arxiv.org/abs/2411.08165</guid>
<content:encoded><![CDATA[
arXiv:2411.08165v2 Announce Type: replace-cross 
Abstract: The Knowledge Graph Completion~(KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these limitations, we propose KGR3, a context-enriched framework for KGC. KGR3 is composed of three modules. Firstly, the Retrieval module gathers supporting triples from the KG, collects plausible candidate answers from a base embedding model, and retrieves context for each related entity. Then, the Reasoning module employs a large language model to generate potential answers for each query triple. Finally, the Re-ranking module combines candidate answers from the two modules mentioned above, and fine-tunes an LLM to provide the best answer. Extensive experiments on widely used datasets demonstrate that KGR3 consistently improves various KGC methods. Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of 12.3% and 5.6% on the FB15k237 and WN18RR datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages</title>
<link>https://arxiv.org/abs/2411.16508</link>
<guid>https://arxiv.org/abs/2411.16508</guid>
<content:encoded><![CDATA[
arXiv:2411.16508v3 Announce Type: replace-cross 
Abstract: Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in LMM research. The benchmark offers a robust and nuanced evaluation framework featuring various question formats, including true/false, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures a comprehensive assessment of a model's ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark is publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Board Games by External and Internal Planning with Language Models</title>
<link>https://arxiv.org/abs/2412.12119</link>
<guid>https://arxiv.org/abs/2412.12119</guid>
<content:encoded><![CDATA[
arXiv:2412.12119v2 Announce Type: replace-cross 
Abstract: Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in internal search, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis</title>
<link>https://arxiv.org/abs/2412.19723</link>
<guid>https://arxiv.org/abs/2412.19723</guid>
<content:encoded><![CDATA[
arXiv:2412.19723v2 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews</title>
<link>https://arxiv.org/abs/2502.05439</link>
<guid>https://arxiv.org/abs/2502.05439</guid>
<content:encoded><![CDATA[
arXiv:2502.05439v2 Announce Type: replace-cross 
Abstract: The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews with human-in-the-loop module that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a judge agent and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection/hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a judge agent along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training</title>
<link>https://arxiv.org/abs/2502.12734</link>
<guid>https://arxiv.org/abs/2502.12734</guid>
<content:encoded><![CDATA[
arXiv:2502.12734v2 Announce Type: replace-cross 
Abstract: Machine-generated Text (MGT) detection is crucial for regulating and attributing online texts. While the existing MGT detectors achieve strong performance, they remain vulnerable to simple perturbations and adversarial attacks. To build an effective defense against malicious perturbations, we view MGT detection from a threat modeling perspective, that is, analyzing the model's vulnerability from an adversary's point of view and exploring effective mitigations. To this end, we introduce an adversarial framework for training a robust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The GREATER consists of two key components: an adversary GREATER-A and a detector GREATER-D. The GREATER-D learns to defend against the adversarial attack from GREATER-A and generalizes the defense to other attacks. GREATER-A identifies and perturbs the critical tokens in embedding space, along with greedy search and pruning to generate stealthy and disruptive adversarial examples. Besides, we update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D to generalize its defense to different attacks and varying attack intensities. Our experimental results across 10 text perturbation strategies and 6 adversarial attacks show that our GREATER-D reduces the Attack Success Rate (ASR) by 0.67% compared with SOTA defense methods while our GREATER-A is demonstrated to be more effective and efficient than SOTA attack approaches. Codes and dataset are available in https://github.com/Liyuuuu111/GREATER.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations</title>
<link>https://arxiv.org/abs/2502.16949</link>
<guid>https://arxiv.org/abs/2502.16949</guid>
<content:encoded><![CDATA[
arXiv:2502.16949v3 Announce Type: replace-cross 
Abstract: Knowledge graph (KG) learning offers a powerful framework for generating new knowledge and making inferences. Training KG embedding can take a significantly long time, especially for larger datasets. Our analysis shows that the gradient computation of embedding is one of the dominant functions in the translation-based KG embedding training loop. We address this issue by replacing the core embedding computation with SpMM (Sparse-Dense Matrix Multiplication) kernels. This allows us to unify multiple scatter (and gather) operations as a single operation, reducing training time and memory usage. We create a general framework for training KG models using sparse kernels and implement four models, namely TransE, TransR, TransH, and TorusE. Our sparse implementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on the GPU with a significantly low GPU memory footprint. The speedups are consistent across large and small datasets for a given model. Our proposed sparse approach can be extended to accelerate other translation-based (such as TransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE, etc.) models as well. An implementation of the SpTransX framework is publicly available as a Python package in https://github.com/HipGraph/SpTransX.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Code-Edit Embedding to Model Student Debugging Behavior</title>
<link>https://arxiv.org/abs/2502.19407</link>
<guid>https://arxiv.org/abs/2502.19407</guid>
<content:encoded><![CDATA[
arXiv:2502.19407v2 Announce Type: replace-cross 
Abstract: Providing effective feedback for programming assignments in computer science education can be challenging: students solve problems by iteratively submitting code, executing it, and using limited feedback from the compiler or the auto-grader to debug. Analyzing student debugging behavior in this process may reveal important insights into their knowledge and inform better personalized support tools. In this work, we propose an encoder-decoder-based model that learns meaningful code-edit embeddings between consecutive student code submissions, to capture their debugging behavior. Our model leverages information on whether a student code submission passes each test case to fine-tune large language models (LLMs) to learn code editing representations. It enables personalized next-step code suggestions that maintain the student's coding style while improving test case correctness. Our model also enables us to analyze student code-editing patterns to uncover common student errors and debugging behaviors, using clustering techniques. Experimental results on a real-world student code submission dataset demonstrate that our model excels at code reconstruction and personalized code suggestion while revealing interesting patterns in student debugging behavior.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language</title>
<link>https://arxiv.org/abs/2503.01453</link>
<guid>https://arxiv.org/abs/2503.01453</guid>
<content:encoded><![CDATA[
arXiv:2503.01453v2 Announce Type: replace-cross 
Abstract: Most existing works in image caption synthesis use computation heavy deep neural networks and generates image descriptions in English language. This often restricts this important assistive tool for widespread use across language and accessibility barriers. This work presents AC-Lite, a computationally efficient model for image captioning in low-resource Assamese language. AC-Lite reduces computational requirements by replacing computation-heavy deep network components with lightweight alternatives. The AC-Lite model is designed through extensive ablation experiments with different image feature extractor networks and language decoders. A combination of ShuffleNetv2x1.5 with GRU based language decoder along with bilinear attention is found to provide the best performance with minimum compute. AC-Lite was observed to achieve an 82.3 CIDEr score on the COCO-AC dataset with 2.45 GFLOPs and 22.87M parameters.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban Computing in the Era of Large Language Models</title>
<link>https://arxiv.org/abs/2504.02009</link>
<guid>https://arxiv.org/abs/2504.02009</guid>
<content:encoded><![CDATA[
arXiv:2504.02009v2 Announce Type: replace-cross 
Abstract: Urban computing has emerged as a multidisciplinary field that harnesses data-driven technologies to address challenges and improve urban living. Traditional approaches, while beneficial, often face challenges with generalization, scalability, and contextual understanding. The advent of Large Language Models (LLMs) offers transformative potential in this domain. This survey explores the intersection of LLMs and urban computing, emphasizing the impact of LLMs in processing and analyzing urban data, enhancing decision-making, and fostering citizen engagement. We provide a concise overview of the evolution and core technologies of LLMs. Additionally, we survey their applications across key urban domains, such as transportation, public safety, and environmental monitoring, summarizing essential tasks and prior works in various urban contexts, while highlighting LLMs' functional roles and implementation patterns. Building on this, we propose potential LLM-based solutions to address unresolved challenges. To facilitate in-depth research, we compile a list of available datasets and tools applicable to diverse urban scenarios. Finally, we discuss the limitations of current approaches and outline future directions for advancing LLMs in urban computing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's the same but not the same: Do LLMs distinguish Spanish varieties?</title>
<link>https://arxiv.org/abs/2504.20049</link>
<guid>https://arxiv.org/abs/2504.20049</guid>
<content:encoded><![CDATA[
<div> variabilidad, lenguaje modelos, espa\ol, identificar, morfosintcticas

Summary: 
- Large language models (LLMs) have shown proficiency in understanding and generating Spanish text.
- Spanish exhibits diatopic variations across regions, making it a rich and diverse language.
- Nine language models were tested on identifying morphosyntactic and lexical differences in seven Spanish varieties.
- Peninsular Spanish was most accurately identified by all models, with GPT-4o showing the best recognition of Spanish language variability.
- The study highlights the importance of considering regional variations in language analysis and model development. <div>
arXiv:2504.20049v1 Announce Type: new 
Abstract: In recent years, large language models (LLMs) have demonstrated a high capacity for understanding and generating text in Spanish. However, with five hundred million native speakers, Spanish is not a homogeneous language but rather one rich in diatopic variations spanning both sides of the Atlantic. For this reason, in this study, we evaluate the ability of nine language models to identify and distinguish the morphosyntactic and lexical peculiarities of seven varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean, Peninsular, Mexican and Central American and Rioplatense) through a multiple-choice test. The results indicate that the Peninsular Spanish variety is the best identified by all models and that, among them, GPT-4o is the only model capable of recognizing the variability of the Spanish language.
  --
  En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos, la espa\~nola no es una lengua homog\'enea, sino rica en variedades diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello, evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de siete variedades de espa\~nol (andino, antillano, caribe\~no continental, chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense) mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que la variedad de espa\~nol peninsular es la mejor identificada por todos los modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar la variabilidad de la lengua espa\~nola.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts</title>
<link>https://arxiv.org/abs/2504.20051</link>
<guid>https://arxiv.org/abs/2504.20051</guid>
<content:encoded><![CDATA[
<div> Keywords: multiword expressions, language models, ambiguity, idiomatic, semantic tasks

Summary: 
Language models, despite their strong performance in various tasks, face challenges in handling nuanced language such as multiword expressions. These expressions have non-compositional meanings and can be used both literally and idiomatically, leading to changes in meaning. The study evaluated state-of-the-art models' ability to process potentially idiomatic multiword expressions, particularly in less frequent contexts. The evaluation was conducted in Portuguese, Galician, and English using a novel code-switched dataset and task. The results showed that large language models, including GPT-4, struggled to outperform xlm-roBERTa-base baselines in detecting and understanding these expressions, especially in novel tasks. Despite the similarities to existing tasks, the models performed poorly on the new tasks introduced, indicating a continued challenge in handling ambiguous multiword expressions. <div>
arXiv:2504.20051v1 Announce Type: new 
Abstract: Multiword expressions, characterised by non-compositional meanings and syntactic irregularities, are an example of nuanced language. These expressions can be used literally or idiomatically, leading to significant changes in meaning. While large language models have demonstrated strong performance across many tasks, their ability to handle such linguistic subtleties remains uncertain. Therefore, this study evaluates how state-of-the-art language models process the ambiguity of potentially idiomatic multiword expressions, particularly in contexts that are less frequent, where models are less likely to rely on memorisation. By evaluating models across in Portuguese and Galician, in addition to English, and using a novel code-switched dataset and a novel task, we find that large language models, despite their strengths, struggle with nuanced language. In particular, we find that the latest models, including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both detection and semantic tasks, with especially poor performance on the novel tasks we introduce, despite its similarity to existing tasks. Overall, our results demonstrate that multiword expressions, especially those which are ambiguous, continue to be a challenge to models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Risks of Generative AI in Financial Services</title>
<link>https://arxiv.org/abs/2504.20086</link>
<guid>https://arxiv.org/abs/2504.20086</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, content safety, financial services, risk taxonomy, guardrail solutions

Summary:<br />
The paper discusses the importance of defining acceptable inputs and outputs for Generative AI products, highlighting the need for assessing content safety in the financial services domain. While academic work often focuses on general aspects like toxicity and bias, the paper emphasizes the significance of considering specialized domains subject to legal and regulatory scrutiny. It introduces an AI content risk taxonomy specific to financial services and evaluates existing technical guardrail solutions in detecting these risks. The analysis reveals that current guardrails fall short in addressing the identified content risks, emphasizing the need for improved tools and strategies to mitigate potential violations and protect stakeholders. <div>
arXiv:2504.20086v1 Announce Type: new 
Abstract: To responsibly develop Generative AI (GenAI) products, it is critical to define the scope of acceptable inputs and outputs. What constitutes a "safe" response is an actively debated question. Academic work puts an outsized focus on evaluating models by themselves for general purpose aspects such as toxicity, bias, and fairness, especially in conversational applications being used by a broad audience. In contrast, less focus is put on considering sociotechnical systems in specialized domains. Yet, those specialized systems can be subject to extensive and well-understood legal and regulatory scrutiny. These product-specific considerations need to be set in industry-specific laws, regulations, and corporate governance requirements. In this paper, we aim to highlight AI content safety considerations specific to the financial services domain and outline an associated AI content risk taxonomy. We compare this taxonomy to existing work in this space and discuss implications of risk category violations on various stakeholders. We evaluate how existing open-source technical guardrail solutions cover this taxonomy by assessing them on data collected via red-teaming activities. Our results demonstrate that these guardrails fail to detect most of the content risks we discuss.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models</title>
<link>https://arxiv.org/abs/2504.20157</link>
<guid>https://arxiv.org/abs/2504.20157</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward-based alignment, large language models, Meta Policy Optimization, meta-reward model, prompt engineering <br />
Summary: <br />
The article introduces Meta Policy Optimization (MPO) as a solution to challenges faced by reward-based alignment methods for large language models (LLMs). MPO addresses limitations such as vulnerability to reward hacking and the need for labor-intensive prompt engineering by incorporating a meta-reward model that dynamically adjusts the reward model's prompt during training. This adaptive approach ensures a stable policy optimization and reduces the reliance on manual prompt design while maintaining performance comparable to models with hand-crafted reward prompts. MPO's effectiveness is demonstrated across various tasks like question answering and mathematical reasoning without the need for task-specific reward designs. The meta-learning formulation of MPO makes it easily extensible to higher-level alignment frameworks, offering a more robust and adaptable strategy for reward-based RL alignment in LLMs. The code and models developed using MPO will be shared publicly. <br /> <div>
arXiv:2504.20157v1 Announce Type: new 
Abstract: Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools</title>
<link>https://arxiv.org/abs/2504.20168</link>
<guid>https://arxiv.org/abs/2504.20168</guid>
<content:encoded><![CDATA[
<div> Keywords: tool-using agents, model confidence, safety, interpretability, MICE<br />
Summary:<br />
The study introduces a novel approach, Model-Internal Confidence Estimators (MICE), to enhance confidence assessment in tool-using agents. MICE utilizes logitLens to decode from language model layers and determine confidence by comparing generated outputs. In experiments on the simulated trial and error dataset with Llama3 models, MICE outperforms baselines in expected calibration error and enhances tool-calling utility significantly. MICE is also demonstrated to be sample-efficient, generalize to unseen APIs, and improve tool-calling utility across varying risk levels. The open-source code for MICE is available on GitHub, showcasing its practical application in real-world scenarios. <br /> <div>
arXiv:2504.20168v1 Announce Type: new 
Abstract: Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports</title>
<link>https://arxiv.org/abs/2504.20220</link>
<guid>https://arxiv.org/abs/2504.20220</guid>
<content:encoded><![CDATA[
<div> Keywords: electronic health records, paper documents, checkbox data, OCR, vision-language models

Summary:
An open-source pipeline has been developed to extract and categorize checkbox data from scanned paper documents. The manual transcription process, still prevalent in healthcare due to the use of paper documents, is time-consuming and error-prone. This pipeline, designed for transfusion reaction reports but adaptable to other checkbox-rich document types, integrates checkbox detection, multilingual OCR, and vision-language models. The method achieves high precision and recall compared to gold-standards from 2017 to 2024. By automating this process, the pipeline reduces administrative workload and ensures accurate regulatory reporting. The availability of this open-source tool promotes self-hosted parsing of checkbox forms, contributing to the streamlining of healthcare processes. 

<br /><br />Summary: <div>
arXiv:2504.20220v1 Announce Type: new 
Abstract: Despite the growing adoption of electronic health records, many processes still rely on paper documents, reflecting the heterogeneous real-world conditions in which healthcare is delivered. The manual transcription process is time-consuming and prone to errors when transferring paper-based data to digital formats. To streamline this workflow, this study presents an open-source pipeline that extracts and categorizes checkbox data from scanned documents. Demonstrated on transfusion reaction reports, the design supports adaptation to other checkbox-rich document types. The proposed method integrates checkbox detection, multilingual optical character recognition (OCR) and multilingual vision-language models (VLMs). The pipeline achieves high precision and recall compared against annually compiled gold-standards from 2017 to 2024. The result is a reduction in administrative workload and accurate regulatory reporting. The open-source availability of this pipeline encourages self-hosted parsing of checkbox forms.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Platform for Generating Educational Activities to Teach English as a Second Language</title>
<link>https://arxiv.org/abs/2504.20251</link>
<guid>https://arxiv.org/abs/2504.20251</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, educational activities, English as a foreign language, image generation, text generation

Summary: The platform presented in this paper focuses on creating educational activities for teaching English as a foreign language using Natural Language Processing techniques. It offers pre-made games as well as the ability to generate more complex activities from teacher-entered texts. The platform is currently expanding to include image and text generation. To enhance its capabilities, the platform is being migrated to a more powerful server. The development, deployment, challenges faced, and future work plans of the platform are discussed in detail. <div>
arXiv:2504.20251v1 Announce Type: new 
Abstract: We present a platform for the generation of educational activities oriented to teaching English as a foreign language. The different activities --games and language practice exercises-- are strongly based on Natural Language Processing techniques. The platform offers the possibility of playing out-of-the-box games, generated from resources created semi-automatically and then manually curated. It can also generate games or exercises of greater complexity from texts entered by teachers, providing a stage of review and edition of the generated content before use. As a way of expanding the variety of activities in the platform, we are currently experimenting with image and text generation. In order to integrate them and improve the performance of other neural tools already integrated, we are working on migrating the platform to a more powerful server. In this paper we describe the development of our platform and its deployment for end users, discussing the challenges faced and how we overcame them, and also detail our future work plans.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi</title>
<link>https://arxiv.org/abs/2504.20276</link>
<guid>https://arxiv.org/abs/2504.20276</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-4, Kimi, Large Language Models, systematic reviews, assessment<br />
<br />
Summary: 
This study examined the use of GPT-4 and Kimi, two Large Language Models (LLMs), for conducting systematic reviews. The researchers assessed the performance of the LLMs by comparing the codes generated by them with those generated by humans in a peer-reviewed systematic review related to assessment. The study revealed that the effectiveness of LLMs varies based on the amount of data available and the complexity of the research questions in systematic reviews. The findings indicate that LLMs like GPT-4 and Kimi can play a valuable role in assisting with systematic reviews, but their performance may be influenced by factors such as the volume of data and the complexity of the research topic. These results provide insights into the potential applications and limitations of LLMs in the field of systematic reviews. <br /><br />Summary: <div>
arXiv:2504.20276v1 Announce Type: new 
Abstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs), for systematic reviews. We evaluated their performance by comparing LLM-generated codes with human-generated codes from a peer-reviewed systematic review on assessment. Our findings suggested that the performance of LLMs fluctuates by data volume and question complexity for systematic reviews.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions</title>
<link>https://arxiv.org/abs/2504.20304</link>
<guid>https://arxiv.org/abs/2504.20304</guid>
<content:encoded><![CDATA[
<div> Keywords: CHILDES, Universal Dependencies, treebank, annotation guidelines, computational research

Summary:
UD-English-CHILDES is a new resource derived from CHILDES data, offering consistent and unified annotation guidelines for linguistic research. The corpus integrates annotations from 11 children and caregivers, comprising over 48k sentences. Existing gold-standard annotations are validated under the UD v2 framework, complemented by 1M silver-standard sentences. This resource provides a harmonized dataset for computational and linguistic investigations, facilitating analysis of child and child-directed speech. The UD-English-CHILDES treebank enhances the accessibility of CHILDES data for research purposes, with standardized annotation guidelines benefiting computational research and enabling linguistic analyses. Overall, this paper introduces a valuable resource for researchers in the fields of child language acquisition and computational linguistics to explore linguistic patterns and study language development. 

<br /><br />Summary: <div>
arXiv:2504.20304v1 Announce Type: new 
Abstract: CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank derived from previously dependency-annotated CHILDES data with consistent and unified annotation guidelines. Our corpus harmonizes annotations from 11 children and their caregivers, totaling over 48k sentences. We validate existing gold-standard annotations under the UD v2 framework and provide an additional 1M silver-standard sentences, offering a consistent resource for computational and linguistic research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation</title>
<link>https://arxiv.org/abs/2504.20323</link>
<guid>https://arxiv.org/abs/2504.20323</guid>
<content:encoded><![CDATA[
<div> Keywords: legal recommender systems, labeled datasets, labor disputes, co-citation, algorithmic annotation 

Summary: 
This report presents a novel approach to address the issue of limited labeled datasets in developing legal recommender systems, specifically in specialized domains like labor disputes. By leveraging the co-citation of legal articles within cases, the proposed method establishes similarity and enables algorithmic annotation. Drawing parallels to case co-citation, which uses cited precedents as indicators of shared legal issues, the system recommends similar cases based on plaintiffs' accusations, defendants' rebuttals, and points of disputes. The evaluation of the labeled results demonstrates the effectiveness of the recommender system in recommending labor cases with similarities measured by the co-citation of legal articles. With fine-tuned text embedding models and a BiLSTM module, this research contributes to the advancement of automated annotation techniques for legal documents, particularly in areas with limited access to comprehensive legal databases. 

Summary: <div>
arXiv:2504.20323v1 Announce Type: new 
Abstract: This report addresses the challenge of limited labeled datasets for developing legal recommender systems, particularly in specialized domains like labor disputes. We propose a new approach leveraging the co-citation of legal articles within cases to establish similarity and enable algorithmic annotation. This method draws a parallel to the concept of case co-citation, utilizing cited precedents as indicators of shared legal issues. To evaluate the labeled results, we employ a system that recommends similar cases based on plaintiffs' accusations, defendants' rebuttals, and points of disputes. The evaluation demonstrates that the recommender, with finetuned text embedding models and a reasonable BiLSTM module can recommend labor cases whose similarity was measured by the co-citation of the legal articles. This research contributes to the development of automated annotation techniques for legal documents, particularly in areas with limited access to comprehensive legal databases.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.20355</link>
<guid>https://arxiv.org/abs/2504.20355</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, prompt optimization, automatic prompt engineering, Math Reasoning, optimization tokens

Summary:
The article introduces a new approach to prompt optimization called Local Prompt Optimization (LPO) which focuses on optimizing specific tokens within a prompt rather than the entire prompt globally. By guiding the Large Language Models (LLMs) to concentrate on these key tokens during optimization, LPO improves performance significantly on Math Reasoning tasks such as GSM8k and MultiArith, as well as on the challenging BIG-bench Hard benchmarks. LPO also shows faster convergence to the optimal prompt compared to global methods. This innovative method enhances the effectiveness of automatic prompt engineering and offers a more efficient way to improve the output of LLMs for complex tasks. <div>
arXiv:2504.20355v1 Announce Type: new 
Abstract: In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Causes Knowledge Loss in Multilingual Language Models?</title>
<link>https://arxiv.org/abs/2504.20356</link>
<guid>https://arxiv.org/abs/2504.20356</guid>
<content:encoded><![CDATA[
<div> transfer learning, natural language processing, multilingual performance, catastrophic forgetting, LoRA adapters

Summary: 
- The study explores the issue of catastrophic forgetting in multilingual NLP models, focusing on linguistic differences in representational learning.
- Experimenting with 52 languages, LoRA adapters of varying ranks were used to assess the impact on shared parameters.
- The research aimed to determine if parameter sharing through adapters can alleviate forgetting while retaining previous knowledge.
- Non-Latin script languages showed higher susceptibility to catastrophic forgetting compared to Latin script languages.
- Latin script languages demonstrated more effective cross-lingual transfer, emphasizing the importance of linguistic differences in multilingual NLP models. 

<br /><br />Summary: <div>
arXiv:2504.20356v1 Announce Type: new 
Abstract: Cross-lingual transfer in natural language processing (NLP) models enhances multilingual performance by leveraging shared linguistic knowledge. However, traditional methods that process all data simultaneously often fail to mimic real-world scenarios, leading to challenges like catastrophic forgetting, where fine-tuning on new tasks degrades performance on previously learned ones. Our study explores this issue in multilingual contexts, focusing on linguistic differences affecting representational learning rather than just model parameters. We experiment with 52 languages using LoRA adapters of varying ranks to evaluate non-shared, partially shared, and fully shared parameters. Our aim is to see if parameter sharing through adapters can mitigate forgetting while preserving prior knowledge. We find that languages using non-Latin scripts are more susceptible to catastrophic forgetting, whereas those written in Latin script facilitate more effective cross-lingual transfer.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation</title>
<link>https://arxiv.org/abs/2504.20371</link>
<guid>https://arxiv.org/abs/2504.20371</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multi-domain translation, disambiguation, evaluation, prompting strategies

Summary: 
The study examines the disambiguation ability of Large Language Models (LLMs) in multi-domain translation (MDT) contexts. The research presents a systematic evaluation framework called DMDTEval, comprising a multi-domain ambiguous word annotated translation test set, diverse disambiguation prompting templates, and precise metrics. Various prompting strategies were assessed on multiple state-of-the-art LLMs, revealing key insights. The findings highlight the challenge of ambiguity in MDT and the need for improved disambiguation techniques in LLMs. The study's results provide valuable insights for future research in enhancing the disambiguation capabilities of LLMs. 

<br /><br />Summary: <div>
arXiv:2504.20371v1 Announce Type: new 
Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in machine translation. However, their performance in multi-domain translation (MDT) is less satisfactory; the meanings of words can vary across different domains, highlighting the significant ambiguity inherent in MDT. Therefore, evaluating the disambiguation ability of LLMs in MDT remains an open problem. To this end, we present an evaluation and analysis of LLMs on disambiguation in multi-domain translation (DMDTEval), our systematic evaluation framework consisting of three critical aspects: (1) we construct a translation test set with multi-domain ambiguous word annotation, (2) we curate a diverse set of disambiguation prompting templates, and (3) we design precise disambiguation metrics, and study the efficacy of various prompting strategies on multiple state-of-the-art LLMs. Our extensive experiments reveal a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the disambiguation of LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?</title>
<link>https://arxiv.org/abs/2504.20444</link>
<guid>https://arxiv.org/abs/2504.20444</guid>
<content:encoded><![CDATA[
<div> Keywords: primacy effect, LLMs, ChatGPT, Gemini, Claude

Summary:
In a study analyzing the primacy effect in three commercial Language Models (LLMs) - ChatGPT, Gemini, and Claude - researchers conducted experiments replicating Asch's classic human subjects study. The experiments presented pairs of candidates with equal descriptions but varying word order (positive followed by negative adjectives or vice versa) to these LLMs. In the first experiment where candidates were presented simultaneously, ChatGPT showed a preference for candidates with positive adjectives listed first, Gemini displayed no clear preference, and Claude consistently failed to make a choice. In the second experiment with candidates presented separately, both ChatGPT and Claude tended to rank candidates equally, but when making a choice, they leaned towards candidates with negative adjectives listed first. In contrast, Gemini predominantly favored candidates with negative adjectives listed first. These findings provide insights into how different LLMs process and interpret information based on adjective order. 

<br /><br />Summary: <div>
arXiv:2504.20444v1 Announce Type: new 
Abstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and Claude. We do this by repurposing the famous experiment Asch (1946) conducted using human subjects. The experiment is simple, given two candidates with equal descriptions which one is preferred if one description has positive adjectives first before negative ones and another description has negative adjectives followed by positive ones. We test this in two experiments. In one experiment, LLMs are given both candidates simultaneously in the same prompt, and in another experiment, LLMs are given both candidates separately. We test all the models with 200 candidate pairs. We found that, in the first experiment, ChatGPT preferred the candidate with positive adjectives listed first, while Gemini preferred both equally often. Claude refused to make a choice. In the second experiment, ChatGPT and Claude were most likely to rank both candidates equally. In the case where they did not give an equal rating, both showed a clear preference to a candidate that had negative adjectives listed first. Gemini was most likely to prefer a candidate with negative adjectives listed first.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs</title>
<link>https://arxiv.org/abs/2504.20451</link>
<guid>https://arxiv.org/abs/2504.20451</guid>
<content:encoded><![CDATA[
<div> Keywords: machine translation, cultural adaptation, entity translation, automatic evaluation metrics, error taxonomy

Summary: 
Machine translation systems play a crucial role in translating knowledge-intensive and entity-rich text between English and Korean, requiring transcreation to preserve language-specific nuances. A study evaluated 13 models, including Large Language Models (LLMs) and traditional Machine Translation (MT) systems, using both automatic metrics and human assessment. The findings revealed that while LLMs outperformed traditional MT systems, they struggled with entity translation that required cultural adaptation. By constructing an error taxonomy, the study identified key issues such as incorrect responses and entity name errors, with performance varying based on entity type and popularity level. The research also highlighted gaps in automatic evaluation metrics and aimed to pave the way for culturally-nuanced machine translation in the future. 

<br /><br />Summary: <div>
arXiv:2504.20451v1 Announce Type: new 
Abstract: Translating knowledge-intensive and entity-rich text between English and Korean requires transcreation to preserve language-specific and cultural nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13 models (LLMs and MT models) using automatic metrics and human assessment by bilingual annotators. Our findings show LLMs outperform traditional MT systems but struggle with entity translation requiring cultural adaptation. By constructing an error taxonomy, we identify incorrect responses and entity name errors as key issues, with performance varying by entity type and popularity level. This work exposes gaps in automatic evaluation metrics and hope to enable future work in completing culturally-nuanced machine translation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models</title>
<link>https://arxiv.org/abs/2504.20469</link>
<guid>https://arxiv.org/abs/2504.20469</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Zero-shot Learning, News Narratives, Framing Roles, Hierarchical Approach
<br />
Summary: 
Large language models (LLMs) were evaluated for their zero-shot capabilities in classifying framing roles in news narratives. The study focused on the effects of input context, prompting strategies, and task decomposition. Results showed that a hierarchical approach, first identifying broad roles then fine-grained roles, outperformed single-step classification. Optimal input contexts and prompts varied across task levels, emphasizing the need for subtask-specific strategies. The study achieved a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of the approach. Tailored prompt design and input context optimization were highlighted as crucial for improving LLM performance in entity framing.
<br /> <div>
arXiv:2504.20469v1 Announce Type: new 
Abstract: Understanding how news narratives frame entities is crucial for studying media's impact on societal perceptions of events. In this paper, we evaluate the zero-shot capabilities of large language models (LLMs) in classifying framing roles. Through systematic experimentation, we assess the effects of input context, prompting strategies, and task decomposition. Our findings show that a hierarchical approach of first identifying broad roles and then fine-grained roles, outperforms single-step classification. We also demonstrate that optimal input contexts and prompts vary across task levels, highlighting the need for subtask-specific strategies. We achieve a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our approach. Our findings emphasize the importance of tailored prompt design and input context optimization for improving LLM performance in entity framing.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training</title>
<link>https://arxiv.org/abs/2504.20484</link>
<guid>https://arxiv.org/abs/2504.20484</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cross-lingual transfer, pre-training, bilingual texts, multilingual performance 

Summary: 
- The article introduces Cross-lingual In-context Pre-training (CrossIC-PT) as a method to enhance cross-lingual transfer in large language models (LLMs).
- CrossIC-PT leverages semantically related bilingual texts through next-word prediction, without the constraints of parallel resources.
- By interleaving semantic-related bilingual Wikipedia documents and utilizing a systematic segmentation policy for long document pairs, CrossIC-PT aims to improve contextual coherence.
- Data availability is extended through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus.
- Experimental results show significant improvements in multilingual performance across three LLM models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) in six target languages, with performance gains ranging from 1.95% to 3.99%, followed by additional enhancements through data augmentation.

<br /><br />Summary: <div>
arXiv:2504.20484v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre-training. Existing methods for enhancing cross-lingual transfer remain constrained by parallel resources, suffering from limited linguistic and domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT), a simple and scalable approach that enhances cross-lingual transfer by leveraging semantically related bilingual texts via simple next-word prediction. We construct CrossIC-PT samples by interleaving semantic-related bilingual Wikipedia documents into a single context window. To access window size constraints, we implement a systematic segmentation policy to split long bilingual document pairs into chunks while adjusting the sliding window mechanism to preserve contextual coherence. We further extend data availability through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%, 3.99%, and 1.95%, respectively, with additional improvements after data augmentation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation</title>
<link>https://arxiv.org/abs/2504.20500</link>
<guid>https://arxiv.org/abs/2504.20500</guid>
<content:encoded><![CDATA[
<div> Keywords: UniDetox, toxicity mitigation, large language models, dataset distillation, detoxification technique 

Summary:
UniDetox is a novel method aimed at mitigating toxicity in large language models (LLMs) universally. Unlike previous model-specific detoxification methods, UniDetox can be applied across a wide range of LLMs without the need for separate tuning. The approach involves distilling detoxifying representations through contrastive decoding, creating synthetic text data that can effectively detoxify various LLMs through fine-tuning. Experiments show the detoxifying text from GPT-2 successfully detoxifies larger models like OPT, Falcon, and LLaMA-2 without requiring individual hyperparameter tuning. The analysis of detoxifying text also reveals a reduction in politically biased content, providing valuable insights for the effective detoxification of LLMs.<br /><br />Summary: UniDetox is a universal method for mitigating toxicity in large language models. It uses dataset distillation and contrastive decoding to generate detoxifying representations that can be fine-tuned across different LLMs without the need for model-specific tuning. By effectively detoxifying models like GPT-2, OPT, Falcon, and LLaMA-2, UniDetox also reduces politically biased content in the detoxified text, offering valuable insights for enhancing the detoxification process. <div>
arXiv:2504.20500v1 Announce Type: new 
Abstract: We present UniDetox, a universally applicable method designed to mitigate toxicity across various large language models (LLMs). Previous detoxification methods are typically model-specific, addressing only individual models or model families, and require careful hyperparameter tuning due to the trade-off between detoxification efficacy and language modeling performance. In contrast, UniDetox provides a detoxification technique that can be universally applied to a wide range of LLMs without the need for separate model-specific tuning. Specifically, we propose a novel and efficient dataset distillation technique for detoxification using contrastive decoding. This approach distills detoxifying representations in the form of synthetic text data, enabling universal detoxification of any LLM through fine-tuning with the distilled text. Our experiments demonstrate that the detoxifying text distilled from GPT-2 can effectively detoxify larger models, including OPT, Falcon, and LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter tuning for each model, as a single hyperparameter configuration can be seamlessly applied across different models. Additionally, analysis of the detoxifying text reveals a reduction in politically biased content, providing insights into the attributes necessary for effective detoxification of LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records</title>
<link>https://arxiv.org/abs/2504.20547</link>
<guid>https://arxiv.org/abs/2504.20547</guid>
<content:encoded><![CDATA[
<div> Keywords: standardized evaluation, medical domain, natural language models, MIMIC-IV benchmark, electronic health records

Summary: 
The paper addresses the lack of standardized evaluation benchmarks in the medical domain for text inputs by revisiting the MIMIC-IV benchmark for electronic health records (EHRs). The MIMIC-IV data is integrated into the Hugging Face datasets library to facilitate easy sharing and usage. The study explores the conversion of EHR tabular data to text using templates and compares the performance of fine-tuned and zero-shot language models (LLMs) on the mortality prediction task. The results show that fine-tuned text-based models perform competitively against tabular classifiers, while zero-shot LLMs struggle to effectively utilize EHR representations. This research demonstrates the potential of text-based approaches in the medical field and suggests areas for further improvement.<br /><br />Summary: <div>
arXiv:2504.20547v1 Announce Type: new 
Abstract: The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an openly available MIMIC-IV benchmark for electronic health records (EHRs) to address this issue. First, we integrate the MIMIC-IV data within the Hugging Face datasets library to allow an easy share and use of this collection. Second, we investigate the application of templates to convert EHR tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the mortality of patients task show that fine-tuned text-based models are competitive against robust tabular classifiers. In contrast, zero-shot LLMs struggle to leverage EHR representations. This study underlines the potential of text-based approaches in the medical field and highlights areas for further improvement.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters</title>
<link>https://arxiv.org/abs/2504.20552</link>
<guid>https://arxiv.org/abs/2504.20552</guid>
<content:encoded><![CDATA[
<div> fine-tuning, German, dialogues, Bertolt Brecht, AI<br />
Summary: <br />
This project introduces BrAIcht, an AI conversational agent trained in the style of Bertolt Brecht using a large German language model. The model is fine-tuned with a diverse dataset of plays by Brecht and other German playwrights. A parameter-efficient technique called QLoRA is used due to memory constraints. Results based on BLEU score and perplexity show promising performance in generating dialogues in the style of Bertolt Brecht. <br /> <div>
arXiv:2504.20552v1 Announce Type: new 
Abstract: This project introduces BrAIcht, an AI conversational agent that creates dialogues in the distinctive style of the famous German playwright Bertolt Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7 billion parameters and a modified version of the base Llama2 suitable for German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of other German plays that are stylistically similar to Bertolt Brecht are used to form a more di-erse dataset. Due to the limited memory capacity, a parameterefficient fine-tuning technique called QLoRA is implemented to train the large language model. The results, based on BLEU score and perplexity, show very promising performance of BrAIcht in generating dialogues in the style of Bertolt Brecht.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClonEval: An Open Voice Cloning Benchmark</title>
<link>https://arxiv.org/abs/2504.20581</link>
<guid>https://arxiv.org/abs/2504.20581</guid>
<content:encoded><![CDATA[
<div> benchmark, voice cloning, text-to-speech models, evaluation protocol, open-source library

Summary:
The paper introduces a new benchmark for evaluating voice cloning text-to-speech models. It includes an evaluation protocol, an open-source library, and a leaderboard for model performance assessment. The design considerations for the benchmark are discussed, along with a detailed description of the evaluation procedure. The paper also explains how to use the software library provided and how results are organized on the leaderboard. This benchmark aims to provide a standardized and comprehensive way to evaluate the performance of voice cloning models, allowing for better comparison and advancement in the field of text-to-speech technology. <div>
arXiv:2504.20581v1 Announce Type: new 
Abstract: We present a novel benchmark for voice cloning text-to-speech models. The benchmark consists of an evaluation protocol, an open-source library for assessing the performance of voice cloning models, and an accompanying leaderboard. The paper discusses design considerations and presents a detailed description of the evaluation procedure. The usage of the software library is explained, along with the organization of results on the leaderboard.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models</title>
<link>https://arxiv.org/abs/2504.20605</link>
<guid>https://arxiv.org/abs/2504.20605</guid>
<content:encoded><![CDATA[
<div> dataset, fables, NLP, instruction-tuned models, ethics <br />
Summary:
TF1-EN-3M is a new dataset of three million English-language fables created using instruction-tuned models with no more than 8 billion parameters. Each fable follows a specific structure, covering characters, traits, settings, conflicts, resolutions, and morals. An evaluation pipeline combines a GPT-based critic with diversity and readability metrics to assess the quality of generated fables. The 8B-parameter Llama-3 variant is identified as the most efficient model in terms of quality and speed. This model can generate high-scoring fables on a consumer GPU at a low cost. The dataset, generation code, evaluation scripts, and metadata are released under a permissive license, allowing for reproducibility and cost benchmarking. TF1-EN-3M opens up possibilities for research in instruction following, narrative intelligence, value alignment, and educational AI, showing that large-scale moral storytelling can be achieved without relying on proprietary giant models. <br /> <br />Summary: <div>
arXiv:2504.20605v1 Announce Type: new 
Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WenyanGPT: A Large Language Model for Classical Chinese Tasks</title>
<link>https://arxiv.org/abs/2504.20609</link>
<guid>https://arxiv.org/abs/2504.20609</guid>
<content:encoded><![CDATA[
<div> Keywords: Classical Chinese, natural language processing, WenyanGPT, evaluation benchmark dataset, LLaMA3-8B-Chinese

Summary:
Classical Chinese, as a vital part of Chinese culture, has been overlooked by existing natural language processing models that primarily focus on Modern Chinese. To address this gap, this paper introduces a specialized solution for Classical Chinese language processing. The creation of WenyanGPT involves continued pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model. Additionally, a benchmark dataset, WenyanBENCH, is developed for evaluating Classical Chinese tasks. Experimental results on WenyanBENCH showcase the superiority of WenyanGPT over current advanced language models in various Classical Chinese tasks. The public availability of the model's training data, instruction fine-tuning data, and evaluation benchmark dataset aims to foster further advancements in the field of Classical Chinese processing. 

<br /><br />Summary: <div>
arXiv:2504.20609v1 Announce Type: new 
Abstract: Classical Chinese, as the core carrier of Chinese culture, plays a crucial role in the inheritance and study of ancient literature. However, existing natural language processing models primarily optimize for Modern Chinese, resulting in inadequate performance on Classical Chinese. This paper presents a comprehensive solution for Classical Chinese language processing. By continuing pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we construct a large language model, WenyanGPT, which is specifically designed for Classical Chinese tasks. Additionally, we develop an evaluation benchmark dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that WenyanGPT significantly outperforms current advanced LLMs in various Classical Chinese tasks. We make the model's training data, instruction fine-tuning data\footnote, and evaluation benchmark dataset publicly available to promote further research and development in the field of Classical Chinese processing.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations</title>
<link>https://arxiv.org/abs/2504.20643</link>
<guid>https://arxiv.org/abs/2504.20643</guid>
<content:encoded><![CDATA[
<div> structured representations, creativity, diverse ideas, culinary domain, DishCOVER

Summary: 
- The paper introduces a novel approach that combines Large Language Models (LLMs) with structured representations and cognitive manipulations to enhance creativity.
- The focus is on generating more creative and diverse ideas beyond superficial variations at the token level.
- The proposed algorithm recombines structured representations of existing ideas to explore the abstract landscape of ideas effectively.
- The approach is demonstrated in the culinary domain with DishCOVER, a model that generates creative recipes.
- Experimental results show that DishCOVER produces more diverse outputs compared to GPT-4 and expert evaluations indicate higher novelty and feasibility in the culinary creations generated by DishCOVER, surpassing GPT-4 in creative generation.

<br /><br />Summary: <div>
arXiv:2504.20643v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with creativity. In this paper, we introduce a novel approach that couples LLMs with structured representations and cognitively inspired manipulations to generate more creative and diverse ideas. Our notion of creativity goes beyond superficial token-level variations; rather, we explicitly recombine structured representations of existing ideas, allowing our algorithm to effectively explore the more abstract landscape of ideas. We demonstrate our approach in the culinary domain with DishCOVER, a model that generates creative recipes. Experiments comparing our model's results to those of GPT-4o show greater diversity. Domain expert evaluations reveal that our outputs, which are mostly coherent and feasible culinary creations, significantly surpass GPT-4o in terms of novelty, thus outperforming it in creative generation. We hope our work inspires further research into structured creativity in AI.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages</title>
<link>https://arxiv.org/abs/2504.20668</link>
<guid>https://arxiv.org/abs/2504.20668</guid>
<content:encoded><![CDATA[
<div> Keywords: Online disinformation, fact-checking, large language models, relevance evaluation, streamlined process <br />
Summary: 
This research addresses the challenge of online disinformation by introducing an approach that utilizes large language models (LLMs) to retrieve and evaluate previously fact-checked claims. By filtering out irrelevant fact-checks and generating concise summaries and explanations, the method aims to streamline the fact-checking process and reduce workload for fact-checkers. Both automatic and human assessments of the approach demonstrate its effectiveness in reducing effort and improving the efficiency of fact-checking. By enabling fact-checkers to quickly assess whether a claim has been verified before, the method contributes to preventing the spread of false information. This research highlights the potential of LLMs in enhancing the fact-checking process and providing valuable support to combat online disinformation. <br /><br />Summary: <div>
arXiv:2504.20668v1 Announce Type: new 
Abstract: Online disinformation poses a global challenge, placing significant demands on fact-checkers who must verify claims efficiently to prevent the spread of false information. A major issue in this process is the redundant verification of already fact-checked claims, which increases workload and delays responses to newly emerging claims. This research introduces an approach that retrieves previously fact-checked claims, evaluates their relevance to a given input, and provides supplementary information to support fact-checkers. Our method employs large language models (LLMs) to filter irrelevant fact-checks and generate concise summaries and explanations, enabling fact-checkers to faster assess whether a claim has been verified before. In addition, we evaluate our approach through both automatic and human assessments, where humans interact with the developed tool to review its effectiveness. Our results demonstrate that LLMs are able to filter out many irrelevant fact-checks and, therefore, reduce effort and streamline the fact-checking process.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-native Children's Automatic Speech Assessment Challenge (NOCASA)</title>
<link>https://arxiv.org/abs/2504.20678</link>
<guid>https://arxiv.org/abs/2504.20678</guid>
<content:encoded><![CDATA[
<div> data competition, second language learners, pronunciation assessment, training data, unbalanced distribution

Summary:
The paper introduces NOCASA, a data competition at the IEEE MLSP 2025 conference focused on assessing the single-word pronunciations of young second language learners through a gamified app. The competition challenges participants to address issues such as limited training data and unbalanced distribution among pronunciation level categories. The authors provide a pseudo-anonymized training dataset called TeflonNorL2, containing 10,334 recordings from 44 speakers pronouncing 205 Norwegian words rated on a 1 to 5 scale. Additionally, two trained systems are released as baselines: an SVM classifier using the ComParE_16 acoustic feature set and a wav2vec 2.0 model. The wav2vec 2.0 model achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%. <br /><br />Summary: <div>
arXiv:2504.20678v1 Announce Type: new 
Abstract: This paper presents the "Non-native Children's Automatic Speech Assessment" (NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA challenges participants to develop new systems that can assess single-word pronunciations of young second language (L2) learners as part of a gamified pronunciation training app. To achieve this, several issues must be addressed, most notably the limited nature of available training data and the highly unbalanced distribution among the pronunciation level categories. To expedite the development, we provide a pseudo-anonymized training data (TeflonNorL2), containing 10,334 recordings from 44 speakers attempting to pronounce 205 distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that should be given in the game). In addition to the data, two already trained systems are released as official baselines: an SVM classifier trained on the ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?</title>
<link>https://arxiv.org/abs/2504.20679</link>
<guid>https://arxiv.org/abs/2504.20679</guid>
<content:encoded><![CDATA[
<div> IR task, concept equivalence, longitudinal surveys, unsupervised approaches, neural models
Summary: 
- The study addresses the challenges of detecting semantically equivalent questions in longitudinal social science surveys by identifying concept equivalence across question and response options.
- Various unsupervised approaches, including probabilistic models and neural networks specialized for information retrieval, were tested on a survey dataset spanning from 1946 to 2020.
- Neural models designed for information retrieval showed the highest overall performance, but re-ranking probabilistic model results with neural models only led to slight improvements in F1-score.
- The study found that models have low sensitivity to questions with high lexical overlap, especially when sub-concepts are mismatched.
- The findings contribute to research on harmonizing longitudinal studies in social science.

<br /><br />Summary: <div>
arXiv:2504.20679v1 Announce Type: new 
Abstract: Automated detection of semantically equivalent questions in longitudinal social science surveys is crucial for long-term studies informing empirical research in the social, economic, and health sciences. Retrieving equivalent questions faces dual challenges: inconsistent representation of theoretical constructs (i.e. concept/sub-concept) across studies as well as between question and response options, and the evolution of vocabulary and structure in longitudinal text. To address these challenges, our multi-disciplinary collaboration of computer scientists and survey specialists presents a new information retrieval (IR) task of identifying concept (e.g. Housing, Job, etc.) equivalence across question and response options to harmonise longitudinal population studies. This paper investigates multiple unsupervised approaches on a survey dataset spanning 1946-2020, including probabilistic models, linear probing of language models, and pre-trained neural networks specialised for IR. We show that IR-specialised neural models achieve the highest overall performance with other approaches performing comparably. Additionally, the re-ranking of the probabilistic model's results with neural models only introduces modest improvements of 0.07 at most in F1-score. Qualitative post-hoc evaluation by survey specialists shows that models generally have a low sensitivity to questions with high lexical overlap, particularly in cases where sub-concepts are mismatched. Altogether, our analysis serves to further research on harmonising longitudinal studies in social science.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?</title>
<link>https://arxiv.org/abs/2504.20699</link>
<guid>https://arxiv.org/abs/2504.20699</guid>
<content:encoded><![CDATA[
<div> hallucination detection, LLMs, translation, paraphrasing, NLI<br />
Summary:<br />
This study evaluates the ability of open-access Language Model Models (LLMs) in detecting intrinsic hallucinations in translation and paraphrasing tasks. The research focuses on the impact of model size, instruction tuning, and prompt choice on model performance. Results show variations in performance across models but consistency across prompts. Notably, Natural Language Inference (NLI) models perform comparably well, suggesting alternative options for hallucination detection beyond LLM-based detectors. The study aims to address the common issue of nonsensical outputs generated by LLMs, known as hallucination, and offers insights into improving detection methods in language generation tasks. <br /> <div>
arXiv:2504.20699v1 Announce Type: new 
Abstract: A frequently observed problem with LLMs is their tendency to generate output that is nonsensical, illogical, or factually incorrect, often referred to broadly as hallucination. Building on the recently proposed HalluciGen task for hallucination detection and generation, we evaluate a suite of open-access LLMs on their ability to detect intrinsic hallucinations in two conditional generation tasks: translation and paraphrasing. We study how model performance varies across tasks and language and we investigate the impact of model size, instruction tuning, and prompt choice. We find that performance varies across models but is consistent across prompts. Finally, we find that NLI models perform comparably well, suggesting that LLM-based detectors are not the only viable option for this specific task.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification</title>
<link>https://arxiv.org/abs/2504.20703</link>
<guid>https://arxiv.org/abs/2504.20703</guid>
<content:encoded><![CDATA[
<div> Keywords: SemEval-2025, text augmentation, transformer models, hazard detection, minority classes

Summary: 
This paper describes a system developed for the SemEval-2025 Task 9 Food Hazard Detection Challenge, which aims to evaluate classification systems for identifying hazards and products in food recall reports. The study investigates the use of text augmentation techniques to address poor performance on minority classes and compares their impact on various transformer and machine learning models. Three word-level augmentation techniques are explored: synonym replacement, random word swapping, and contextual word insertion. Results indicate that transformer models generally outperform machine learning models. The study shows that targeted augmentation of minority classes can enhance transformer model performance, with contextual word insertion leading to a 6% improvement in accuracy for minority hazard classes. Overall, the study provides insights into the effectiveness of text augmentation techniques in improving classification performance. 

<br /><br />Summary: <div>
arXiv:2504.20703v1 Announce Type: new 
Abstract: This paper presents our system developed for the SemEval-2025 Task 9: The Food Hazard Detection Challenge. The shared task's objective is to evaluate explainable classification systems for classifying hazards and products in two levels of granularity from food recall incident reports. In this work, we propose text augmentation techniques as a way to improve poor performance on minority classes and compare their effect for each category on various transformer and machine learning models. We explore three word-level data augmentation techniques, namely synonym replacement, random word swapping, and contextual word insertion. The results show that transformer models tend to have a better overall performance. None of the three augmentation techniques consistently improved overall performance for classifying hazards and products. We observed a statistically significant improvement (P < 0.05) in the fine-grained categories when using the BERT model to compare the baseline with each augmented model. Compared to the baseline, the contextual words insertion augmentation improved the accuracy of predictions for the minority hazard classes by 6%. This suggests that targeted augmentation of minority classes can improve the performance of transformer models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think</title>
<link>https://arxiv.org/abs/2504.20708</link>
<guid>https://arxiv.org/abs/2504.20708</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning trace, subthoughts, linguistic cues, accuracy improvement

Summary:
Large Language Models (LLMs) use step-by-step reasoning to solve complex problems. The traditional evaluation method focuses on the final answer, but this paper questions if it always represents the best conclusion and if different reasoning paths could lead to varied results. The authors propose a method to analyze intermediate reasoning steps, termed subthoughts, by segmenting the trace based on linguistic cues. By prompting the model to generate continuations from each subthought's endpoint and aggregating potential answers, they found significantly higher accuracy compared to relying solely on the original answer. Examining consistency among answers from different subthoughts can identify less reliable answers and suggest improvements in correctness. Experiments on challenging mathematical reasoning datasets show consistent accuracy gains up to 13% and 10%, highlighting the effectiveness of the proposed approach.<br /><br />Summary: <div>
arXiv:2504.20708v1 Announce Type: new 
Abstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\% and 10\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, UniversalRAG, heterogeneous sources, modality-aware routing, granularity levels<br />
<br />Summary: 
UniversalRAG is a novel framework for Retrieval-Augmented Generation (RAG) that aims to integrate knowledge from diverse sources with different modalities. Existing RAG approaches are typically limited to text-only corpora, while recent extensions to other modalities still operate within single modality-specific corpora. UniversalRAG addresses this limitation by dynamically selecting the most appropriate modality-specific corpus for retrieval through a modality-aware routing mechanism. It also organizes each modality into multiple granularity levels, allowing for more tailored and precise retrieval based on the query's complexity and scope. The framework is validated across 8 benchmarks covering multiple modalities, demonstrating its superior performance over modality-specific and unified approaches. UniversalRAG's ability to leverage heterogeneous sources and adaptively retrieve knowledge based on the query's requirements makes it a promising advancement in the field of Retrieval-Augmented Generation. <br /><br /> <div>
arXiv:2504.20734v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers</title>
<link>https://arxiv.org/abs/2504.20752</link>
<guid>https://arxiv.org/abs/2504.20752</guid>
<content:encoded><![CDATA[
<div> grokking, transformers, factual reasoning, knowledge graphs, neural networks <br />
Summary:<br />
The paper introduces a novel approach to improve multi-step factual reasoning in Transformers using grokking techniques. It addresses the challenge of dataset sparsity by augmenting knowledge graphs with carefully designed synthetic data to enhance reasoning capabilities. Surprisingly, even factually incorrect synthetic data can strengthen reasoning circuits by promoting reliance on relational structure over memorization. The approach achieves high accuracy on multi-hop reasoning benchmarks, outperforming strong baselines and current state-of-the-art models. The study explores how increasing the ratio of inferred facts to atomic facts drives the formation of generalizing circuits inside Transformers. The findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities in large-scale language models, leading to more robust and interpretable factual reasoning. <br />Summary: <div>
arXiv:2504.20752v1 Announce Type: new 
Abstract: Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption</title>
<link>https://arxiv.org/abs/2504.20769</link>
<guid>https://arxiv.org/abs/2504.20769</guid>
<content:encoded><![CDATA[
<div> prompting, robustness, large language models, reasoning abilities, reference corruption

Summary:<br />
- The study explores the use of chain-of-thought prompting to enhance the robustness of large language models.
- It introduces the concept of chain-of-defensive-thought, which improves model performance against reference corruption.
- Large language models, like GPT-4o, exhibit significantly improved accuracy in tasks such as Natural Questions with this method.
- The method involves providing structured and defensive reasoning exemplars as demonstrations to enhance model robustness.
- Empirical results show that GPT-4o's accuracy remains at 50% with chain-of-defensive-thought prompting, compared to 3% with standard prompting when faced with prompt injection attacks. 

Summary: <div>
arXiv:2504.20769v1 Announce Type: new 
Abstract: Chain-of-thought prompting has demonstrated great success in facilitating the reasoning abilities of large language models. In this work, we explore how these enhanced reasoning abilities can be exploited to improve the robustness of large language models in tasks that are not necessarily reasoning-focused. In particular, we show how a wide range of large language models exhibit significantly improved robustness against reference corruption using a simple method called chain-of-defensive-thought, where only a few exemplars with structured and defensive reasoning are provided as demonstrations. Empirically, the improvements can be astounding, especially given the simplicity and applicability of the method. For example, in the Natural Questions task, the accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting when 1 out of 10 references provided is corrupted with prompt injection attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting maintains an accuracy of 50%.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turing Machine Evaluation for Large Language Model</title>
<link>https://arxiv.org/abs/2504.20771</link>
<guid>https://arxiv.org/abs/2504.20771</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Computational reasoning ability, Evaluation framework, Turing Machine, TMBench

Summary:
The research focuses on evaluating the core computational reasoning ability of Large Language Models (LLMs) through a novel perspective. The proposed evaluation framework, based on Universal Turing Machine (UTM) simulation, assesses the model's capacity to understand rules and perform logical computing operations accurately. The framework, implemented through TMBench, requires LLMs to follow instructions and track dynamic states during multi-step computations. TMBench offers advantages such as knowledge-agnostic evaluation, adjustable difficulty, and scalability through Turing machine encoding. The study shows a strong correlation between performance on TMBench and other reasoning benchmarks, emphasizing the significance of computational reasoning in measuring the capabilities of LLMs.<br /><br />Summary: <div>
arXiv:2504.20771v1 Announce Type: new 
Abstract: With the rapid development and widespread application of Large Language Models (LLMs), rigorous evaluation has become particularly crucial. This research adopts a novel perspective, focusing on evaluating the core computational reasoning ability of LLMs, defined as the capacity of model to accurately understand rules, and execute logically computing operations. This capability assesses the reliability of LLMs as precise executors, and is critical to advanced tasks such as complex code generation and multi-step problem-solving. We propose an evaluation framework based on Universal Turing Machine (UTM) simulation. This framework requires LLMs to strictly follow instructions and track dynamic states, such as tape content and read/write head position, during multi-step computations. To enable standardized evaluation, we developed TMBench, a benchmark for systematically studying the computational reasoning capabilities of LLMs. TMBench provides several key advantages, including knowledge-agnostic evaluation, adjustable difficulty, foundational coverage through Turing machine encoding, and unlimited capacity for instance generation, ensuring scalability as models continue to evolve. We find that model performance on TMBench correlates strongly with performance on other recognized reasoning benchmarks (Pearson correlation coefficient is 0.73), clearly demonstrating that computational reasoning is a significant dimension for measuring the deep capabilities of LLMs. Code and data are available at https://github.com/HaitaoWuTJU/Turing-Machine-Bench.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal language model with the intervention of quantum theory</title>
<link>https://arxiv.org/abs/2504.20839</link>
<guid>https://arxiv.org/abs/2504.20839</guid>
<content:encoded><![CDATA[
<div> Keywords: language modeling, quantum mechanics, word embedding, natural language, quantum statistics<br />
Summary:<br />
This paper explores the integration of quantum mechanics into language modeling, specifically focusing on symbol-meaning pairs in natural language. The study suggests that quantum mechanics can enhance word embedding techniques used in statistical language modeling. By applying quantum statistics and related theories, the research delves into mathematical representations, natural evolution, and statistical properties of natural language, proposing that quantum properties stem from the physical nature of information. An experimental code is developed to demonstrate the feasibility of using quantum theory for natural language modeling. The paper also discusses the potential application of quantum theory in constructing generative models and anticipates future applications in quantum computing. <br /><br />Summary: <div>
arXiv:2504.20839v1 Announce Type: new 
Abstract: This paper examines language modeling based on the theory of quantum mechanics. It focuses on the introduction of quantum mechanics into the symbol-meaning pairs of language in order to build a representation model of natural language. At the same time, it is realized that word embedding, which is widely used as a basic technique for statistical language modeling, can be explained and improved by the mathematical framework of quantum mechanics. On this basis, this paper continues to try to use quantum statistics and other related theories to study the mathematical representation, natural evolution and statistical properties of natural language. It is also assumed that the source of such quantum properties is the physicality of information. The feasibility of using quantum theory to model natural language is pointed out through the construction of a experimental code. The paper discusses, in terms of applications, the possible help of the theory in constructing generative models that are popular nowadays. A preliminary discussion of future applications of the theory to quantum computers is also presented.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry</title>
<link>https://arxiv.org/abs/2504.20849</link>
<guid>https://arxiv.org/abs/2504.20849</guid>
<content:encoded><![CDATA[
<div> Keywords: Data-to-Text, Language Models, Marketing Texts, Diversity, Automated Content Generation 

Summary: 
- The paper explores the use of LLM-based data-to-text methods for generating diverse marketing texts on online platforms.
- Traditional generative methods often produce repetitive content, leading to monotonous text galleries.
- Language Models like T5, GPT-3.5, GPT-4, and LLaMa2 are utilized, along with fine-tuning and zero-shot techniques, to enhance text diversity.
- A new metric called JaccDiv is introduced to evaluate the diversity of generated texts.
- The research has broader applications beyond the music industry, offering insights into improving automated content generation in various fields. 

Summary: <div>
arXiv:2504.20849v1 Announce Type: new 
Abstract: Online platforms are increasingly interested in using Data-to-Text technologies to generate content and help their users. Unfortunately, traditional generative methods often fall into repetitive patterns, resulting in monotonous galleries of texts after only a few iterations. In this paper, we investigate LLM-based data-to-text approaches to automatically generate marketing texts that are of sufficient quality and diverse enough for broad adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in conjunction with fine-tuning, few-shot, and zero-shot approaches to set a baseline for diverse marketing texts. We also introduce a metric JaccDiv to evaluate the diversity of a set of texts. This research extends its relevance beyond the music industry, proving beneficial in various fields where repetitive automated content generation is prevalent.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNAMAX: Dynamic computing for Transformers and Mamba based architectures</title>
<link>https://arxiv.org/abs/2504.20922</link>
<guid>https://arxiv.org/abs/2504.20922</guid>
<content:encoded><![CDATA[
<div> Mamba, Early exits, Transformers, Dynamic computing, NLP <br />
Summary: <br />
This paper introduces DYNAMAX, a framework that integrates early exit mechanisms into Mamba architectures, showcasing their efficiency in reducing computational costs in natural language processing tasks. The study compares the Mistral 7B transformer with the Codestral 7B Mamba model on datasets like TruthfulQA, CoQA, and TriviaQA. The results demonstrate Mamba's adaptability as an early exit classifier, effectively balancing computational savings, accuracy, and consistency across various NLP tasks. By leveraging Mamba's design for dynamic processing, the framework opens up possibilities for scalable and efficient inference in embedded applications and resource-constrained environments. This research highlights the transformative potential of Mamba in redefining dynamic computing paradigms for large language models. <br /> <div>
arXiv:2504.20922v1 Announce Type: new 
Abstract: Early exits (EEs) offer a promising approach to reducing computational costs and latency by dynamically terminating inference once a satisfactory prediction confidence on a data sample is achieved. Although many works integrate EEs into encoder-only Transformers, their application to decoder-only architectures and, more importantly, Mamba models, a novel family of state-space architectures in the LLM realm, remains insufficiently explored. This work introduces DYNAMAX, the first framework to exploit the unique properties of Mamba architectures for early exit mechanisms. We not only integrate EEs into Mamba but also repurpose Mamba as an efficient EE classifier for both Mamba-based and transformer-based LLMs, showcasing its versatility. Our experiments employ the Mistral 7B transformer compared to the Codestral 7B Mamba model, using data sets such as TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and consistency. The results highlight the adaptability of Mamba as a powerful EE classifier and its efficiency in balancing computational cost and performance quality across NLP tasks. By leveraging Mamba's inherent design for dynamic processing, we open pathways for scalable and efficient inference in embedded applications and resource-constrained environments. This study underscores the transformative potential of Mamba in redefining dynamic computing paradigms for LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models</title>
<link>https://arxiv.org/abs/2504.20946</link>
<guid>https://arxiv.org/abs/2504.20946</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, prompt engineering, arithmetic reasoning, open-source models, Trace-of-Thought Prompting

Summary:<br /><br />Large Language Models (LLMs) are increasingly being used in various tasks, prompting ongoing advancements in prompt engineering, especially for specialized domains like arithmetic reasoning. While powerful, using LLMs extensively can be costly and limit customization when relying on proprietary models. This study introduces Trace-of-Thought Prompting, a simple zero-shot method that enhances arithmetic reasoning in LLMs below 7 billion parameters through observable subproblems. By applying this approach with open-source models and GPT-4, a performance increase of up to 125% was observed. This highlights the potential of open-source initiatives in democratizing AI research and making high-quality computational linguistics applications more accessible. <div>
arXiv:2504.20946v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks, prompt engineering remains an active field of contribution within computational linguistics, particularly in domains requiring specialized knowledge such as arithmetic reasoning. While these LLMs are optimized for a variety of tasks, their exhaustive employment may become computationally or financially cumbersome for small teams. Additionally, complete reliance on proprietary, closed-source models often limits customization and adaptability, posing significant challenges in research and application scalability. Instead, by leveraging open-source models at or below 7 billion parameters, we can optimize our resource usage while still observing remarkable gains over standard prompting approaches. To cultivate this notion, we introduce Trace-of-Thought Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to create observable subproblems using critical problem-solving, specifically designed to enhance arithmetic reasoning capabilities. When applied to open-source models in tandem with GPT-4, we observe that Trace-of-Thought not only allows novel insight into the problem-solving process but also introduces performance gains as large as 125% on language models at or below 7 billion parameters. This approach underscores the potential of open-source initiatives in democratizing AI research and improving the accessibility of high-quality computational linguistics applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models</title>
<link>https://arxiv.org/abs/2504.20951</link>
<guid>https://arxiv.org/abs/2504.20951</guid>
<content:encoded><![CDATA[
<div> propose, theoretical model, information gravity, large language models, text generation process
Summary:<br /><br />We present a theoretical model called "information gravity" that utilizes concepts from field theory and spacetime geometry to elucidate the text generation process in large language models. In this model, user queries are depicted as objects with "information mass" that curve the semantic space of the model, creating gravitational potential wells that influence the probability distribution of generated tokens. By adopting this framework, we can better understand various phenomena observed in LLM behavior, such as hallucinations arising from low-density semantic voids, the impact of query formulation on semantic field curvature changes, and the role of sampling temperature in diversifying output. The information gravity model provides a compelling explanation for these behaviors and sheds light on the intricate dynamics of text generation in LLMs. 
Summary: <div>
arXiv:2504.20951v1 Announce Type: new 
Abstract: We propose a theoretical model called "information gravity" to describe the text generation process in large language models (LLMs). The model uses physical apparatus from field theory and spacetime geometry to formalize the interaction between user queries and the probability distribution of generated tokens. A query is viewed as an object with "information mass" that curves the semantic space of the model, creating gravitational potential wells that "attract" tokens during generation. This model offers a mechanism to explain several observed phenomena in LLM behavior, including hallucinations (emerging from low-density semantic voids), sensitivity to query formulation (due to semantic field curvature changes), and the influence of sampling temperature on output diversity.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification</title>
<link>https://arxiv.org/abs/2504.20964</link>
<guid>https://arxiv.org/abs/2504.20964</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Models (LLMs), specification generation, operating system kernel verification tasks, evaluation 

Summary:
The OSVBench is a benchmark designed to evaluate Large Language Models (LLMs) in generating complete specification code for operating system kernel verification tasks. It presents LLMs with the programming model, requiring them to generate specifications based on verification assumptions and high-level functional descriptions. Utilizing the Hyperkernel operating system, the benchmark encompasses 245 complex specification generation tasks with long-context challenges. A study of 12 LLMs reveals their limited performance on these tasks, emphasizing disparities in handling long-context code generation. The benchmark and evaluation toolkit are accessible on GitHub, inviting further research and development in improving LLM capabilities for operating system verification tasks.<br /><br />Summary: <div>
arXiv:2504.20964v1 Announce Type: new 
Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyu-hkust/OSVBench.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SetKE: Knowledge Editing for Knowledge Elements Overlap</title>
<link>https://arxiv.org/abs/2504.20972</link>
<guid>https://arxiv.org/abs/2504.20972</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Editing, Knowledge Element Overlap, Knowledge Set Editing, SetKE <br />
Summary: <br />
Large Language Models (LLMs) have shown proficiency in retrieval and question-answering tasks but require regular updates to stay accurate and reduce errors. Traditional methods like fine-tuning and incremental learning suffer from overfitting and high computational costs. Knowledge Editing (KE) offers a promising alternative, but it often faces challenges due to Knowledge Element Overlap (KEO), where multiple triplets share common elements leading to editing conflicts. This article proposes Knowledge Set Editing (KSE) and introduces SetKE, a method that edits sets of triplets simultaneously, outperforming existing methods in handling KEO scenarios on mainstream LLMs. The article also introduces the EditSet dataset containing KEO triplets as a benchmark for evaluating editing methods. <div>
arXiv:2504.20972v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel in tasks such as retrieval and question answering but require updates to incorporate new knowledge and reduce inaccuracies and hallucinations. Traditional updating methods, like fine-tuning and incremental learning, face challenges such as overfitting and high computational costs. Knowledge Editing (KE) provides a promising alternative but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where multiple triplets share common elements, leading to editing conflicts. We identify the prevalence of KEO in existing KE datasets and show its significant impact on current KE methods, causing performance degradation in handling such triplets. To address this, we propose a new formulation, Knowledge Set Editing (KSE), and introduce SetKE, a method that edits sets of triplets simultaneously. Experimental results demonstrate that SetKE outperforms existing methods in KEO scenarios on mainstream LLMs. Additionally, we introduce EditSet, a dataset containing KEO triplets, providing a comprehensive benchmark.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence</title>
<link>https://arxiv.org/abs/2504.20059</link>
<guid>https://arxiv.org/abs/2504.20059</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinical trials, online platforms, TrialGPT, patient cases, eligibility criteria

Summary: 
Trial recruitment challenges are being addressed through leveraging online platforms such as social media and health communities. The use of TrialGPT, a framework based on a large language model, significantly improves the identification of eligible clinical trials for patients compared to traditional keyword-based searches. Each patient case was found to be eligible for an average of 7 trials using TrialGPT, outperforming traditional methods by 46%. Outreach efforts to case authors and trial organizers regarding these patient-trial matches resulted in highly positive feedback, demonstrating the potential for online platforms to enhance clinical trial recruitment and enrollment pathways.<br /><br />Summary: <div>
arXiv:2504.20059v1 Announce Type: cross 
Abstract: Clinical trials are crucial for assessing new treatments; however, recruitment challenges - such as limited awareness, complex eligibility criteria, and referral barriers - hinder their success. With the growth of online platforms, patients increasingly turn to social media and health communities for support, research, and advocacy, expanding recruitment pools and established enrollment pathways. Recognizing this potential, we utilized TrialGPT, a framework that leverages a large language model (LLM) as its backbone, to match 50 online patient cases (collected from published case reports and a social media website) to clinical trials and evaluate performance against traditional keyword-based searches. Our results show that TrialGPT outperforms traditional methods by 46% in identifying eligible trials, with each patient, on average, being eligible for around 7 trials. Additionally, our outreach efforts to case authors and trial organizers regarding these patient-trial matches yielded highly positive feedback, which we present from both perspectives.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20073</link>
<guid>https://arxiv.org/abs/2504.20073</guid>
<content:encoded><![CDATA[
<div> RL, LLM, StarPO, RAGEN, trajectory-level<br />
Summary:<br />
Training large language models (LLMs) as interactive agents poses challenges such as long-horizon decision-making and dealing with stochastic feedback. The proposed StarPO framework and RAGEN system address these challenges by enabling trajectory-level agent RL training. Echo Trap, a recurring issue in agent RL training, is mitigated by StarPO-S, a stabilized variant. Shaping RL rollouts requires diverse initial states, medium interaction granularity, and frequent sampling. Additionally, fine-grained, reasoning-aware reward signals are crucial for promoting agent reasoning in multi-turn RL training, as shallow strategies or hallucinated thoughts may emerge without them. This study highlights the importance of these key factors in the development of effective LLM agents trained through multi-turn RL. <div>
arXiv:2504.20073v1 Announce Type: cross 
Abstract: Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Awareness</title>
<link>https://arxiv.org/abs/2504.20084</link>
<guid>https://arxiv.org/abs/2504.20084</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, awareness, meta-cognition, self-awareness, social awareness, situational awareness

Summary: Recent advancements in AI have led to a renewed focus on AI awareness as a measurable, functional capacity. This review explores the different forms of AI awareness, including meta-cognition, self-awareness, social awareness, and situational awareness, drawing on insights from cognitive science, psychology, and computational theory. The review examines how these forms of awareness manifest in state-of-the-art AI and discusses the link between awareness and intelligent behaviors. The risks associated with AI awareness, such as misalignment and societal risks, are also explored. Ultimately, AI awareness enhances AI capabilities but requires careful oversight to mitigate potential risks, making it a double-edged sword. This interdisciplinary review provides a roadmap for future research and emphasizes the importance of understanding the role of AI awareness in the development of intelligent machines. 

Summary: <div>
arXiv:2504.20084v1 Announce Type: cross 
Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness, not as a philosophical question of consciousness, but as a measurable, functional capacity. In this review, we explore the emerging landscape of AI awareness, which includes meta-cognition (the ability to represent and reason about its own state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents), and situational awareness (assessing and responding to the context in which it operates).
  First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raises concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow. On the whole, our interdisciplinary review provides a roadmap for future research and aims to clarify the role of AI awareness in the ongoing development of intelligent machines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?</title>
<link>https://arxiv.org/abs/2504.20094</link>
<guid>https://arxiv.org/abs/2504.20094</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent collaboration, conversational recommendation system, large language models, user engagement, personalized recommendations

Summary:
Our paper introduces the MATCHA framework, a multi-agent collaboration system for conversational recommendation systems. By leveraging large language models, our framework enhances personalization and user engagement by providing curated recommendations aligned with user interests, preferences, and constraints. Specialized agents for intent analysis, candidate generation, ranking, re-ranking, explainability, and safeguards work together to improve recommendation accuracy, diversity, and safety. Our model outperforms current state-of-the-art on eight metrics and addresses key challenges in game recommendation systems: handling complex user requests, enhancing personalization through collaboration, empirical evaluation and deployment, and ensuring safe interactions. Through comparisons with baseline models, our approach demonstrates superior performance and highlights the importance of multi-agent collaboration in conversational recommendation systems.
<br /><br />Summary: <div>
arXiv:2504.20094v1 Announce Type: cross 
Abstract: In this paper, we propose a multi-agent collaboration framework called MATCHA for conversational recommendation system, leveraging large language models (LLMs) to enhance personalization and user engagement. Users can request recommendations via free-form text and receive curated lists aligned with their interests, preferences, and constraints. Our system introduces specialized agents for intent analysis, candidate generation, ranking, re-ranking, explainability, and safeguards. These agents collaboratively improve recommendations accuracy, diversity, and safety. On eight metrics, our model achieves superior or comparable performance to the current state-of-the-art. Through comparisons with six baseline models, our approach addresses key challenges in conversational recommendation systems for game recommendations, including: (1) handling complex, user-specific requests, (2) enhancing personalization through multi-agent collaboration, (3) empirical evaluation and deployment, and (4) ensuring safe and trustworthy interactions.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies</title>
<link>https://arxiv.org/abs/2504.20117</link>
<guid>https://arxiv.org/abs/2504.20117</guid>
<content:encoded><![CDATA[
<div> Keywords: ResearchCodeAgent, multi-agent system, large language models, machine learning tasks, automation<br />
<br />
Summary: ResearchCodeAgent is a novel multi-agent system that leverages large language models to automate the codification of research methodologies in machine learning literature. The system bridges the gap between high-level research concepts and practical implementation by generating code from existing research papers. It offers flexible agent architecture with a comprehensive action suite for context-aware interactions. The system uses a dynamic planning mechanism with short and long-term memory for adaptive approaches. Evaluation on three machine learning tasks shows that ResearchCodeAgent generates high-quality and error-free code, with some instances showing performance improvements over baseline implementations. An average reduction of 57.9% in coding time compared to manual implementation was observed, with higher gains for complex tasks. This system represents a significant advancement in automating research implementation, potentially accelerating the pace of machine learning research.<br /><br />Summary: <div>
arXiv:2504.20117v1 Announce Type: cross 
Abstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system's effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains</title>
<link>https://arxiv.org/abs/2504.20199</link>
<guid>https://arxiv.org/abs/2504.20199</guid>
<content:encoded><![CDATA[
<div> Focus-Centric Visual Chain, multi-image tasks, data synthesis, VISC-150K dataset, vision-language systems <br /> 
Summary: <br /> 
The article introduces the Focus-Centric Visual Chain paradigm to improve vision-language models' performance in multi-image tasks. By synthesizing data through Focus-Centric Data Synthesis, the VISC-150K dataset was created to enhance reasoning abilities. Experimental results show a significant performance boost across different model architectures without sacrificing general vision-language capabilities. This approach demonstrates advancements in handling complex visual scenarios and signifies progress towards robust vision-language systems.
 <div>
arXiv:2504.20199v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) achieve remarkable success in single-image tasks. However, real-world scenarios often involve intricate multi-image inputs, leading to a notable performance decline as models struggle to disentangle critical information scattered across complex visual features. In this work, we propose Focus-Centric Visual Chain, a novel paradigm that enhances VLMs'perception, comprehension, and reasoning abilities in multi-image scenarios. To facilitate this paradigm, we propose Focus-Centric Data Synthesis, a scalable bottom-up approach for synthesizing high-quality data with elaborate reasoning paths. Through this approach, We construct VISC-150K, a large-scale dataset with reasoning data in the form of Focus-Centric Visual Chain, specifically designed for multi-image tasks. Experimental results on seven multi-image benchmarks demonstrate that our method achieves average performance gains of 3.16% and 2.24% across two distinct model architectures, without compromising the general vision-language capabilities. our study represents a significant step toward more robust and capable vision-language systems that can handle complex visual scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mrCAD: Multimodal Refinement of Computer-aided Designs</title>
<link>https://arxiv.org/abs/2504.20294</link>
<guid>https://arxiv.org/abs/2504.20294</guid>
<content:encoded><![CDATA[
<div> Keywords: human collaboration, generative AI, multimodal instructions, computer aided designs (CADs), refinement instructions <br />
Summary: <br />
The article introduces mrCAD, a dataset of multimodal instructions in a communication game where players create and refine computer aided designs. The dataset consists of 6,082 communication games played by 1,092 pairs of human players. Analysis of the dataset reveals differences in the use of drawing and text between generation and refinement instructions. State-of-the-art VLMs perform better at following generation instructions than refinement instructions in the mrCAD task, highlighting a gap in current AI capabilities. The findings provide insights into the multimodal language of refinement and lay the groundwork for future research in this area. <br /> <div>
arXiv:2504.20294v1 Announce Type: cross 
Abstract: A key feature of human collaboration is the ability to iteratively refine the concepts we have communicated. In contrast, while generative AI excels at the \textit{generation} of content, it often struggles to make specific language-guided \textit{modifications} of its prior outputs. To bridge the gap between how humans and machines perform edits, we present mrCAD, a dataset of multimodal instructions in a communication game. In each game, players created computer aided designs (CADs) and refined them over several rounds to match specific target designs. Only one player, the Designer, could see the target, and they must instruct the other player, the Maker, using text, drawing, or a combination of modalities. mrCAD consists of 6,082 communication games, 15,163 instruction-execution rounds, played between 1,092 pairs of human players. We analyze the dataset and find that generation and refinement instructions differ in their composition of drawing and text. Using the mrCAD task as a benchmark, we find that state-of-the-art VLMs are better at following generation instructions than refinement instructions. These results lay a foundation for analyzing and modeling a multimodal language of refinement that is not represented in previous datasets.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding</title>
<link>https://arxiv.org/abs/2504.20456</link>
<guid>https://arxiv.org/abs/2504.20456</guid>
<content:encoded><![CDATA[
<div> diffusion models, any-subset autoregressive models, parallel sampling, joint probability density estimation, language modeling
<br />
Summary:
The article introduces any-subset autoregressive models (AS-ARMs) as a solution for parallel sampling in arbitrary-order language models. AS-ARMs allow for generating tokens in any order and in parallel, supporting joint probability density estimation. The Any-Subset Speculative Decoding (ASSD) algorithm corrects token distributions, ensuring generation from the correct joint distribution. Empirical results show that ASSD speeds up language generation without compromising quality. AS-ARMs outperform larger models on infilling benchmark tasks and almost match their performance on code generation. The study provides a mathematically justified training scheme for AS-ARMs and suggests that these models are a promising direction in language modeling. <div>
arXiv:2504.20456v1 Announce Type: cross 
Abstract: In arbitrary-order language models, it is an open question how to sample tokens in parallel from the correct joint distribution. With discrete diffusion models, the more tokens they generate in parallel, the less their predicted distributions adhere to the originally learned data distribution, as they rely on a conditional independence assumption that only works with infinitesimally small timesteps. We find that a different class of models, any-subset autoregressive models (AS-ARMs), holds the solution. As implied by the name, AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs support parallelized joint probability density estimation, allowing them to correct their own parallel-generated token distributions, via our Any-Subset Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of tokens from the correct joint distribution, with the number of neural network calls upper bounded by the number of tokens predicted. We empirically verify that ASSD speeds up language generation, without sacrificing quality. Furthermore, we provide a mathematically justified scheme for training AS-ARMs for generation, and show that AS-ARMs achieve state-of-the-art performance among sub-200M parameter models on infilling benchmark tasks, and nearly match the performance of models 50X larger on code generation. Our theoretical and empirical results indicate that the once-forgotten AS-ARMs are a promising direction of language modeling.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User</title>
<link>https://arxiv.org/abs/2504.20458</link>
<guid>https://arxiv.org/abs/2504.20458</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational recommendation systems, simulated user, generative reward model, multi-turn interaction, personalized recommendations

Summary:
The article introduces a novel approach called GRSU, a generative reward model based simulated user for conversational recommendation systems (CRSs). The GRSU enables automatic interaction with CRSs by providing feedback on recommended items. It offers two types of feedback actions - generative item scoring for coarse-grained feedback, and attribute-based item critique for fine-grained feedback. These feedback actions are unified into an instruction-based format for seamless integration. Inspired by reward-guided search, beam search is used for efficient interaction with CRSs. Additionally, an efficient candidate ranking method is proposed to enhance recommendation results. Experimental results on public datasets validate the effectiveness, efficiency, and transferability of the approach. Overall, the GRSU model addresses the challenge of understanding intricate user preferences in CRSs through automatic multi-turn interaction, resulting in improved personalized recommendations. 

<br /><br />Summary: <div>
arXiv:2504.20458v1 Announce Type: cross 
Abstract: Conversational recommendation systems (CRSs) use multi-turn interaction to capture user preferences and provide personalized recommendations. A fundamental challenge in CRSs lies in effectively understanding user preferences from conversations. User preferences can be multifaceted and complex, posing significant challenges for accurate recommendations even with access to abundant external knowledge. While interaction with users can clarify their true preferences, frequent user involvement can lead to a degraded user experience.
  To address this problem, we propose a generative reward model based simulated user, named GRSU, for automatic interaction with CRSs. The simulated user provides feedback to the items recommended by CRSs, enabling them to better capture intricate user preferences through multi-turn interaction. Inspired by generative reward models, we design two types of feedback actions for the simulated user: i.e., generative item scoring, which offers coarse-grained feedback, and attribute-based item critique, which provides fine-grained feedback. To ensure seamless integration, these feedback actions are unified into an instruction-based format, allowing the development of a unified simulated user via instruction tuning on synthesized data. With this simulated user, automatic multi-turn interaction with CRSs can be effectively conducted. Furthermore, to strike a balance between effectiveness and efficiency, we draw inspiration from the paradigm of reward-guided search in complex reasoning tasks and employ beam search for the interaction process. On top of this, we propose an efficient candidate ranking method to improve the recommendation results derived from interaction. Extensive experiments on public datasets demonstrate the effectiveness, efficiency, and transferability of our approach.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Reasoning in Large Language Models with One Training Example</title>
<link>https://arxiv.org/abs/2504.20571</link>
<guid>https://arxiv.org/abs/2504.20571</guid>
<content:encoded><![CDATA[
arXiv:2504.20571v1 Announce Type: cross 
Abstract: We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonIR: Training Retrievers for Reasoning Tasks</title>
<link>https://arxiv.org/abs/2504.20595</link>
<guid>https://arxiv.org/abs/2504.20595</guid>
<content:encoded><![CDATA[
arXiv:2504.20595v1 Announce Type: cross 
Abstract: We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation</title>
<link>https://arxiv.org/abs/2504.20859</link>
<guid>https://arxiv.org/abs/2504.20859</guid>
<content:encoded><![CDATA[
arXiv:2504.20859v1 Announce Type: cross 
Abstract: As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Leaderboard Illusion</title>
<link>https://arxiv.org/abs/2504.20879</link>
<guid>https://arxiv.org/abs/2504.20879</guid>
<content:encoded><![CDATA[
arXiv:2504.20879v1 Announce Type: cross 
Abstract: Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification</title>
<link>https://arxiv.org/abs/2504.20930</link>
<guid>https://arxiv.org/abs/2504.20930</guid>
<content:encoded><![CDATA[
arXiv:2504.20930v1 Announce Type: cross 
Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition</title>
<link>https://arxiv.org/abs/2504.20938</link>
<guid>https://arxiv.org/abs/2504.20938</guid>
<content:encoded><![CDATA[
arXiv:2504.20938v1 Announce Type: cross 
Abstract: We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of Transformer attention layers to disentangle original Multi Head Self Attention (MHSA) into individually comprehensible components. Lorsa is designed to address the challenge of attention superposition to understand attention-mediated interaction between features in different token positions. We show that Lorsa heads find cleaner and finer-grained versions of previously discovered MHSA behaviors like induction heads, successor heads and attention sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse Autoencoder (SAE) are both sparse dictionary learning methods applied to different Transformer components, and lead to consistent findings in many ways. For instance, we discover a comprehensive family of arithmetic-specific Lorsa heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated interpretability analysis indicates that Lorsa achieves parity with SAE in interpretability while Lorsa exhibits superior circuit discovery properties, especially for features computed collectively by multiple MHSA heads. We also conduct extensive experiments on architectural design ablation, Lorsa scaling law and error analysis.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Consistency for Assuring Reliability of Large Language Models</title>
<link>https://arxiv.org/abs/2308.09138</link>
<guid>https://arxiv.org/abs/2308.09138</guid>
<content:encoded><![CDATA[
arXiv:2308.09138v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</title>
<link>https://arxiv.org/abs/2310.03903</link>
<guid>https://arxiv.org/abs/2310.03903</guid>
<content:encoded><![CDATA[
arXiv:2310.03903v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education</title>
<link>https://arxiv.org/abs/2310.12059</link>
<guid>https://arxiv.org/abs/2310.12059</guid>
<content:encoded><![CDATA[
arXiv:2310.12059v4 Announce Type: replace 
Abstract: In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or D) that is the most likely answer to a question, given the context of the question. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available for research purposes only.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI: The Era of Semantic Decoding</title>
<link>https://arxiv.org/abs/2403.14562</link>
<guid>https://arxiv.org/abs/2403.14562</guid>
<content:encoded><![CDATA[
arXiv:2403.14562v2 Announce Type: replace 
Abstract: Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Analysis of Human Alignment with *PO</title>
<link>https://arxiv.org/abs/2407.15229</link>
<guid>https://arxiv.org/abs/2407.15229</guid>
<content:encoded><![CDATA[
arXiv:2407.15229v2 Announce Type: replace 
Abstract: At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). Prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we examine the robustness of existing state-of-the-art methods to varying hyperparameters in a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment. Our goal is to empirically find the method that increases the likelihood of achieving better results through the lens of various metrics, such as KL divergence and response length. We also introduce LN-DPO, a simple length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves performance. Our analysis of state-of-the-art reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO) methods reveals that they perform similarly at their peak (i.e., best possible scenario). However, we uncover that the pattern of change in performance greatly varies as we move away from the best possible scenario.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment</title>
<link>https://arxiv.org/abs/2408.00137</link>
<guid>https://arxiv.org/abs/2408.00137</guid>
<content:encoded><![CDATA[
arXiv:2408.00137v2 Announce Type: replace 
Abstract: A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in the prompt, and validate their association with the negative bias. Additionally, we propose the negative attention score alignment (NASA) method, which is a parameter-efficient fine-tuning technique to address the extracted negatively biased attention heads. Experimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their generalization abilities.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge</title>
<link>https://arxiv.org/abs/2409.07394</link>
<guid>https://arxiv.org/abs/2409.07394</guid>
<content:encoded><![CDATA[
arXiv:2409.07394v2 Announce Type: replace 
Abstract: Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM's output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Across four LLMs, six question-answering (QA) and three summarization datasets, we demonstrate that ADACAD consistently outperforms other decoding baselines with average QA accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt performance when conflict is absent, ADACAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Racing Thoughts: Explaining Contextualization Errors in Large Language Models</title>
<link>https://arxiv.org/abs/2410.02102</link>
<guid>https://arxiv.org/abs/2410.02102</guid>
<content:encoded><![CDATA[
arXiv:2410.02102v2 Announce Type: replace 
Abstract: The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know very little about the algorithms that a model employs to implement this capability, nor do we understand their failure modes. For example, given the prompt "John is going fishing, so he walks over to the bank. Can he make an ATM transaction?", a model may incorrectly respond "Yes" if it has not properly contextualized "bank" as a geographical feature, rather than a financial institution. We propose the LLM Race Conditions Hypothesis as an explanation of contextualization errors of this form. This hypothesis identifies dependencies between tokens (e.g., "bank" must be properly contextualized before the final token, "?", integrates information from "bank"), and claims that contextualization errors are a result of violating these dependencies. Using a variety of techniques from mechanistic intepretability, we provide correlational and causal evidence in support of the hypothesis, and suggest inference-time interventions to address it.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context</title>
<link>https://arxiv.org/abs/2410.07103</link>
<guid>https://arxiv.org/abs/2410.07103</guid>
<content:encoded><![CDATA[
arXiv:2410.07103v2 Announce Type: replace 
Abstract: Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the absolute position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs' performance is also sensitive to the order, relative position, in which the supporting documents are presented. We refer to this as the misordered context problem. To address this issue, based on the theoretical approach, we propose a simple yet effective method called context repetition (CoRe), which involves prompting the model by repeatedly presenting the context. This ensures that certain contiguous reasoning segments within supporting documents are presented in the optimal order, effectively guiding the model's reasoning in the appropriate direction. Applying CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known "lost-in-the-middle" problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDCure: A Scalable Pipeline for Multi-Document Instruction-Following</title>
<link>https://arxiv.org/abs/2410.23463</link>
<guid>https://arxiv.org/abs/2410.23463</guid>
<content:encoded><![CDATA[
arXiv:2410.23463v3 Announce Type: replace 
Abstract: Multi-document (MD) processing is crucial for LLMs to handle real-world tasks such as summarization and question-answering across large sets of documents. While LLMs have improved at processing long inputs, MD contexts still present unique difficulties, including management of inter-document dependencies, redundancy, and incoherent structures. To address this challenge, we introduce MDCure, a scalable and effective instruction data generation framework to enhance the MD capabilities of LLMs without the computational cost of pre-training or reliance on human-annotated data. MDCure generates high-quality synthetic MD instruction data over sets of articles via targeted prompts. We also introduce MDCureRM, a cost-effective, MD-specific reward model to score and filter generated data based on their training utility for MD settings. MDCure is compatible with open- and closed-source models in addition to policy optimization methods such as PPO, enabling even small open-source models to surpass proprietary LLMs as strong generators of high-quality MD instruction data without further data filtering. With MDCure, we fine-tune a wide variety of LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model families. Extensive evaluations on a wide range of MD and long-context benchmarks spanning various tasks and domains show MDCure consistently improves performance over pre-trained baselines and base models by up to 75.1%. Our code, datasets, and models are available at https://github.com/yale-nlp/MDCure.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint Back-translation Improves Complex Instruction Following of Large Language Models</title>
<link>https://arxiv.org/abs/2410.24175</link>
<guid>https://arxiv.org/abs/2410.24175</guid>
<content:encoded><![CDATA[
arXiv:2410.24175v2 Announce Type: replace 
Abstract: Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Judgments with No Gold Standard</title>
<link>https://arxiv.org/abs/2411.07127</link>
<guid>https://arxiv.org/abs/2411.07127</guid>
<content:encoded><![CDATA[
arXiv:2411.07127v2 Announce Type: replace 
Abstract: We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.
  GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.
  We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset</title>
<link>https://arxiv.org/abs/2411.08243</link>
<guid>https://arxiv.org/abs/2411.08243</guid>
<content:encoded><![CDATA[
arXiv:2411.08243v2 Announce Type: replace 
Abstract: In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset's content through both manual and automated evaluation; (2) experiments demonstrating the dataset's impact on models' safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian Optimization Approach to Machine Translation Reranking</title>
<link>https://arxiv.org/abs/2411.09694</link>
<guid>https://arxiv.org/abs/2411.09694</guid>
<content:encoded><![CDATA[
arXiv:2411.09694v2 Announce Type: replace 
Abstract: Reranking a list of candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate remains a simple and effective method for improving the overall output quality. Translation scoring models continue to grow in size, with the best models being comparable to generation models. Thus, reranking can add substantial computational cost to the translation pipeline. In this work, we pose reranking as a Bayesian optimization (BayesOpt) problem. By strategically selecting candidates to score based on a balance of exploration and exploitation, we show that it is possible to find top-scoring candidates when scoring only a fraction of the candidate list. For instance, our method achieves the same CometKiwi score using only 70 scoring evaluations compared a baseline system using 180. We present a multi-fidelity setting for BayesOpt, where the candidates are first scored with a cheaper but noisier proxy scoring model, which further improves the cost-performance tradeoff when using smaller but well-trained distilled proxy scorers.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding</title>
<link>https://arxiv.org/abs/2502.01563</link>
<guid>https://arxiv.org/abs/2502.01563</guid>
<content:encoded><![CDATA[
arXiv:2502.01563v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope_with_LLM.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation</title>
<link>https://arxiv.org/abs/2502.12836</link>
<guid>https://arxiv.org/abs/2502.12836</guid>
<content:encoded><![CDATA[
arXiv:2502.12836v2 Announce Type: replace 
Abstract: Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2502.17163</link>
<guid>https://arxiv.org/abs/2502.17163</guid>
<content:encoded><![CDATA[
arXiv:2502.17163v3 Announce Type: replace 
Abstract: Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.
  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation</title>
<link>https://arxiv.org/abs/2503.15358</link>
<guid>https://arxiv.org/abs/2503.15358</guid>
<content:encoded><![CDATA[
arXiv:2503.15358v2 Announce Type: replace 
Abstract: Idiomatic expressions present a unique challenge in NLP, as their meanings are often not directly inferable from their constituent words. Despite recent advancements in Large Language Models (LLMs), idiomaticity remains a significant obstacle to robust semantic representation. We present datasets and tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity Representation), which challenges the community to assess and improve models' ability to interpret idiomatic expressions in multimodal contexts and in multiple languages. Participants competed in two subtasks: ranking images based on their alignment with idiomatic or literal meanings, and predicting the next image in a sequence. The most effective methods achieved human-level performance by leveraging pretrained LLMs and vision-language models in mixture-of-experts settings, with multiple queries used to smooth over the weaknesses in these models' representations of idiomaticity.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2503.19878</link>
<guid>https://arxiv.org/abs/2503.19878</guid>
<content:encoded><![CDATA[
arXiv:2503.19878v2 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users</title>
<link>https://arxiv.org/abs/2504.00799</link>
<guid>https://arxiv.org/abs/2504.00799</guid>
<content:encoded><![CDATA[
arXiv:2504.00799v2 Announce Type: replace 
Abstract: Electronic dictionaries have largely replaced paper dictionaries and become central tools for L2 learners seeking to expand their vocabulary. Users often assume these resources are reliable and rarely question the validity of the definitions provided. The accuracy of major E-dictionaries is seldom scrutinized, and little attention has been paid to how their corpora are constructed. Research on dictionary use, particularly the limitations of electronic dictionaries, remains scarce. This study adopts a combined method of experimentation, user survey, and dictionary critique to examine Youdao, one of the most widely used E-dictionaries in China. The experiment involved a translation task paired with retrospective reflection. Participants were asked to translate sentences containing words that are insufficiently or inaccurately defined in Youdao. Their consultation behavior was recorded to analyze how faulty definitions influenced comprehension. Results show that incomplete or misleading definitions can cause serious misunderstandings. Additionally, students exhibited problematic consultation habits. The study further explores how such flawed definitions originate, highlighting issues in data processing and the integration of AI and machine learning technologies in dictionary construction. The findings suggest a need for better training in dictionary literacy for users, as well as improvements in the underlying AI models used to build E-dictionaries.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Automated Grading with Human-in-the-Loop</title>
<link>https://arxiv.org/abs/2504.05239</link>
<guid>https://arxiv.org/abs/2504.05239</guid>
<content:encoded><![CDATA[
arXiv:2504.05239v2 Announce Type: replace 
Abstract: The rise of artificial intelligence (AI) technologies, particularly large language models (LLMs), has brought significant advancements to the field of education. Among various applications, automatic short answer grading (ASAG), which focuses on evaluating open-ended textual responses, has seen remarkable progress with the introduction of LLMs. These models not only enhance grading performance compared to traditional ASAG approaches but also move beyond simple comparisons with predefined "golden" answers, enabling more sophisticated grading scenarios, such as rubric-based evaluation. However, existing LLM-powered methods still face challenges in achieving human-level grading performance in rubric-based assessments due to their reliance on fully automated approaches. In this work, we explore the potential of LLMs in ASAG tasks by leveraging their interactive capabilities through a human-in-the-loop (HITL) approach. Our proposed framework, GradeHITL, utilizes the generative properties of LLMs to pose questions to human experts, incorporating their insights to refine grading rubrics dynamically. This adaptive process significantly improves grading accuracy, outperforming existing methods and bringing ASAG closer to human-level evaluation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation</title>
<link>https://arxiv.org/abs/2504.07072</link>
<guid>https://arxiv.org/abs/2504.07072</guid>
<content:encoded><![CDATA[
arXiv:2504.07072v2 Announce Type: replace 
Abstract: The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2303.01903</link>
<guid>https://arxiv.org/abs/2303.01903</guid>
<content:encoded><![CDATA[
arXiv:2303.01903v4 Announce Type: replace-cross 
Abstract: Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the \emph{blind} LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. The two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. Prophet is general that can be instantiated with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones). Moreover, Prophet can also be integrated with modern large multimodal models in different stages, which is named Prophet++, to further improve the capabilities on knowledge-based VQA tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose-Based Sign Language Appearance Transfer</title>
<link>https://arxiv.org/abs/2410.13675</link>
<guid>https://arxiv.org/abs/2410.13675</guid>
<content:encoded><![CDATA[
arXiv:2410.13675v2 Announce Type: replace-cross 
Abstract: We introduce a method for transferring the signer's appearance in sign language skeletal poses while preserving the sign content. Using estimated poses, we transfer the appearance of one signer to another, maintaining natural movements and transitions. This approach improves pose-based rendering and sign stitching while obfuscating identity. Our experiments show that while the method reduces signer identification accuracy, it slightly harms sign recognition performance, highlighting a tradeoff between privacy and utility. Our code is available at https://github.com/sign-language-processing/pose-anonymization.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators</title>
<link>https://arxiv.org/abs/2412.02467</link>
<guid>https://arxiv.org/abs/2412.02467</guid>
<content:encoded><![CDATA[
arXiv:2412.02467v2 Announce Type: replace-cross 
Abstract: Generating tabular data under differential privacy (DP) protection ensures theoretical privacy guarantees but poses challenges for training machine learning models, primarily due to the need to capture complex structures under noisy supervision signals. Recently, pre-trained Large Language Models (LLMs) -- even those at the scale of GPT-2 -- have demonstrated great potential in synthesizing tabular data. However, their applications under DP constraints remain largely unexplored. In this work, we address this gap by applying DP techniques to the generation of synthetic tabular data. Our findings shows that LLMs face difficulties in generating coherent text when fine-tuned with DP, as privacy budgets are inefficiently allocated to non-private elements like table structures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning framework for differentially private tabular data generation. The first stage involves non-private fine-tuning on a pseudo dataset, followed by DP fine-tuning on a private dataset. Our empirical results show that this approach improves performance across various settings and metrics compared to directly fine-tuned LLMs in DP contexts. We release our code and setup at https://github.com/tejuafonja/DP-2Stage.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering</title>
<link>https://arxiv.org/abs/2412.06832</link>
<guid>https://arxiv.org/abs/2412.06832</guid>
<content:encoded><![CDATA[
arXiv:2412.06832v2 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to generalize to new information by decoupling reasoning capabilities from static knowledge bases. Traditional RAG enhancements have explored vertical scaling-assigning subtasks to specialized modules-and horizontal scaling-replicating tasks across multiple agents-to improve performance. However, real-world applications impose diverse Service Level Agreements (SLAs) and Quality of Service (QoS) requirements, involving trade-offs among objectives such as reducing cost, ensuring answer quality, and adhering to specific operational constraints.
  In this work, we present a systems-oriented approach to multi-agent RAG tailored for real-world Question Answering (QA) applications. By integrating task-specific non-functional requirements-such as answer quality, cost, and latency-into the system, we enable dynamic reconfiguration to meet diverse SLAs. Our method maps these Service Level Objectives (SLOs) to system-level parameters, allowing the generation of optimal results within specified resource constraints.
  We conduct a case study in the QA domain, demonstrating how dynamic re-orchestration of a multi-agent RAG system can effectively manage the trade-off between answer quality and cost. By adjusting the system based on query intent and operational conditions, we systematically balance performance and resource utilization. This approach allows the system to meet SLOs for various query types, showcasing its practicality for real-world applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models</title>
<link>https://arxiv.org/abs/2501.12433</link>
<guid>https://arxiv.org/abs/2501.12433</guid>
<content:encoded><![CDATA[
arXiv:2501.12433v2 Announce Type: replace-cross 
Abstract: Animal stereotypes are deeply embedded in human culture and language. They often shape our perceptions and expectations of various species. Our study investigates how animal stereotypes manifest in vision-language models during the task of image generation. Through targeted prompts, we explore whether DALL-E perpetuates stereotypical representations of animals, such as "owls as wise," "foxes as unfaithful," etc. Our findings reveal significant stereotyped instances where the model consistently generates images aligned with cultural biases. The current work is the first of its kind to examine animal stereotyping in vision-language models systematically and to highlight a critical yet underexplored dimension of bias in AI-generated visual content.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors</title>
<link>https://arxiv.org/abs/2501.18045</link>
<guid>https://arxiv.org/abs/2501.18045</guid>
<content:encoded><![CDATA[
arXiv:2501.18045v2 Announce Type: replace-cross 
Abstract: How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures by capturing more nuance. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\%, r = 0.80, p < 0.01; +41\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic demographic differences in metaphors and implicit perceptions, such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI, which shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations</title>
<link>https://arxiv.org/abs/2502.03629</link>
<guid>https://arxiv.org/abs/2502.03629</guid>
<content:encoded><![CDATA[
arXiv:2502.03629v2 Announce Type: replace-cross 
Abstract: Existing image editing models struggle to meet real-world demands. Despite excelling in academic benchmarks, they have yet to be widely adopted for real user needs. Datasets that power these models use artificial edits, lacking the scale and ecological validity necessary to address the true diversity of user requests. We introduce REALEDIT, a large-scale image editing dataset with authentic user requests and human-made edits sourced from Reddit. REALEDIT includes a test set of 9300 examples to evaluate models on real user requests. Our results show that existing models fall short on these tasks, highlighting the need for realistic training data. To address this, we introduce 48K training examples and train our REALEDIT model, achieving substantial gains - outperforming competitors by up to 165 Elo points in human judgment and 92 percent relative improvement on the automated VIEScore metric. We deploy our model on Reddit, testing it on new requests, and receive positive feedback. Beyond image editing, we explore REALEDIT's potential in detecting edited images by partnering with a deepfake detection non-profit. Finetuning their model on REALEDIT data improves its F1-score by 14 percentage points, underscoring the dataset's value for broad applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation</title>
<link>https://arxiv.org/abs/2503.04606</link>
<guid>https://arxiv.org/abs/2503.04606</guid>
<content:encoded><![CDATA[
arXiv:2503.04606v3 Announce Type: replace-cross 
Abstract: Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\sim$14,000$\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
<link>https://arxiv.org/abs/2503.04992</link>
<guid>https://arxiv.org/abs/2503.04992</guid>
<content:encoded><![CDATA[
arXiv:2503.04992v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal performance impact. However, existing methods often suffer from performance loss without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Further experiments indicate our proposed method is orthogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with LoRA fine-tuning to achieve a similar perplexity improvement as the Wanda method. The proposed method is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single NVIDIA H100 GPU.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocAgent: Graph-Guided LLM Agents for Code Localization</title>
<link>https://arxiv.org/abs/2503.09089</link>
<guid>https://arxiv.org/abs/2503.09089</guid>
<content:encoded><![CDATA[
arXiv:2503.09089v2 Announce Type: replace-cross 
Abstract: Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages</title>
<link>https://arxiv.org/abs/2504.18560</link>
<guid>https://arxiv.org/abs/2504.18560</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, bias testing, multilingual, automated translation, discrimination

Summary: 
The study introduces MultiLingual Augmented Bias Testing (MLA-BiTe), a framework that enhances bias evaluation methods for Large Language Models (LLMs) by allowing systematic multilingual bias testing. MLA-BiTe utilizes automated translation and paraphrasing techniques to enable comprehensive assessments across diverse linguistic contexts. The effectiveness of MLA-BiTe is evaluated by testing four top-performing LLMs in six languages, including two low-resource languages, focusing on seven categories of discrimination. The framework aims to address and mitigate social biases present in LLMs trained on biased data. The study highlights the importance of testing for bias in LLMs across different languages to ensure fair and equitable natural language processing outcomes. 

<br /><br />Summary: <div>
arXiv:2504.18560v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have exhibited impressive natural language processing capabilities but often perpetuate social biases inherent in their training data. To address this, we introduce MultiLingual Augmented Bias Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by enabling systematic multilingual bias testing. MLA-BiTe leverages automated translation and paraphrasing techniques to support comprehensive assessments across diverse linguistic settings. In this study, we evaluate the effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six languages -- including two low-resource languages -- focusing on seven sensitive categories of discrimination.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Span-Level Hallucination Detection for LLM-Generated Answers</title>
<link>https://arxiv.org/abs/2504.18639</link>
<guid>https://arxiv.org/abs/2504.18639</guid>
<content:encoded><![CDATA[
<div> Semantic Role Labeling, Hallucination Detection, DeBERTa, Textual Entailment, Mu-SHROOM dataset 
Summary: 
Semantic Role Labeling is integrated into a span-level hallucination detection framework for the SemEval-2025 Shared Task for English and Arabic texts. The approach decomposes answers into atomic roles, aligns them with a reference context retrieved through LLM prompting, and evaluates semantic alignment using a DeBERTa-based entailment model. Token-level confidence measures and entailment scores are combined to detect hallucinated spans, leading to competitive performance on the Mu-SHROOM dataset. Hallucinated spans are additionally verified through fact-checking with GPT-4 and LLaMA, contributing to improved detection of hallucination in LLM-generated responses. 
<br /><br />Summary: <div>
arXiv:2504.18639v1 Announce Type: new 
Abstract: Detecting spans of hallucination in LLM-generated answers is crucial for improving factual consistency. This paper presents a span-level hallucination detection framework for the SemEval-2025 Shared Task, focusing on English and Arabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose the answer into atomic roles, which are then compared with a retrieved reference context obtained via question-based LLM prompting. Using a DeBERTa-based textual entailment model, we evaluate each role semantic alignment with the retrieved context. The entailment scores are further refined through token-level confidence measures derived from output logits, and the combined scores are used to detect hallucinated spans. Experiments on the Mu-SHROOM dataset demonstrate competitive performance. Additionally, hallucinated spans have been verified through fact-checking by prompting GPT-4 and LLaMA. Our findings contribute to improving hallucination detection in LLM-generated responses.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Third-parties Read Our Emotions?</title>
<link>https://arxiv.org/abs/2504.18673</link>
<guid>https://arxiv.org/abs/2504.18673</guid>
<content:encoded><![CDATA[
<div> emotion recognition tasks, third-party annotations, first-party labels, limitations, language models

Summary:
- Natural Language Processing tasks focus on inferring authors' emotions and opinions from text, relying on third-party annotations.
- Human experiments reveal significant limitations in third-party annotations in accurately representing authors' private states.
- Large language models outperform human annotators in emotion recognition tasks.
- Demographic similarity between authors and annotators enhances annotation performance.
- Incorporating first-party demographic information into prompts improves language models' performance marginally but significantly.
- The study introduces a framework for evaluating the limitations of third-party annotations and calls for refined annotation practices for accurately representing and modeling authors' private states.<br /><br />Summary: <div>
arXiv:2504.18673v1 Announce Type: new 
Abstract: Natural Language Processing tasks that aim to infer an author's private states, e.g., emotions and opinions, from their written text, typically rely on datasets annotated by third-party annotators. However, the assumption that third-party annotators can accurately capture authors' private states remains largely unexamined. In this study, we present human subjects experiments on emotion recognition tasks that directly compare third-party annotations with first-party (author-provided) emotion labels. Our findings reveal significant limitations in third-party annotations-whether provided by human annotators or large language models (LLMs)-in faithfully representing authors' private states. However, LLMs outperform human annotators nearly across the board. We further explore methods to improve third-party annotation quality. We find that demographic similarity between first-party authors and third-party human annotators enhances annotation performance. While incorporating first-party demographic information into prompts leads to a marginal but statistically significant improvement in LLMs' performance. We introduce a framework for evaluating the limitations of third-party annotations and call for refined annotation practices to accurately represent and model authors' private states.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Speech Translation: Translating Across Space With Binaural Hearables</title>
<link>https://arxiv.org/abs/2504.18715</link>
<guid>https://arxiv.org/abs/2504.18715</guid>
<content:encoded><![CDATA[
<div> Keywords: spatial speech translation, hearables, blind source separation, binaural rendering, real-time inference <br />
Summary: <br />
The article introduces the concept of spatial speech translation for hearables, which translates speakers in a crowded space into the wearer's native language while preserving the spatial cues and unique voice characteristics of each speaker. Technical challenges including blind source separation, localization, real-time expressive translation, and binaural rendering are addressed to achieve real-time inference on Apple M2 silicon. Despite interference from other speakers, the prototype binaural headset achieves a BLEU score of up to 22.01. User studies confirm the system's effectiveness in spatially rendering translated speech in real-world reverberant environments. This work is a significant step towards integrating spatial perception into speech translation. <br /><br />Summary: <div>
arXiv:2504.18715v1 Announce Type: new 
Abstract: Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building UD Cairo for Old English in the Classroom</title>
<link>https://arxiv.org/abs/2504.18718</link>
<guid>https://arxiv.org/abs/2504.18718</guid>
<content:encoded><![CDATA[
<div> treebank, Old English, UD, syntactic constructions, annotation <br />
<br />Summary: 
This paper introduces a sample treebank for Old English based on the UD Cairo sentences, collected in a classroom setting. The data was gathered using a combination of LLM prompting and searches in authentic Old English data. Multiple students with limited prior UD exposure annotated the sentences, with their results compared and adjudicated. Although LLM outputs in Old English do not reflect authentic syntax, post-editing can improve accuracy. Beginner annotators, while not achieving perfect results individually, collectively produce good annotations and learn from the process. Preliminary parsing experiments using Modern English training data show improved performance when parsing on annotated features such as lemma, hyperlemma, and gloss. <div>
arXiv:2504.18718v1 Announce Type: new 
Abstract: In this paper we present a sample treebank for Old English based on the UD Cairo sentences, collected and annotated as part of a classroom curriculum in Historical Linguistics. To collect the data, a sample of 20 sentences illustrating a range of syntactic constructions in the world's languages, we employ a combination of LLM prompting and searches in authentic Old English data. For annotation we assigned sentences to multiple students with limited prior exposure to UD, whose annotations we compare and adjudicate. Our results suggest that while current LLM outputs in Old English do not reflect authentic syntax, this can be mitigated by post-editing, and that although beginner annotators do not possess enough background to complete the task perfectly, taken together they can produce good results and learn from the experience. We also conduct preliminary parsing experiments using Modern English training data, and find that although performance on Old English is poor, parsing on annotated features (lemma, hyperlemma, gloss) leads to improved performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers</title>
<link>https://arxiv.org/abs/2504.18736</link>
<guid>https://arxiv.org/abs/2504.18736</guid>
<content:encoded><![CDATA[
<div> Keywords: EvidenceBench, biomedical papers, hypothesis generation, language models, retrieval systems

Summary: 
EvidenceBench introduces a novel pipeline to automate the task of finding evidence relevant to hypotheses in biomedical papers. The pipeline involves hypothesis generation and sentence-by-sentence annotation of papers, closely following human expert judgment. Multiple sets of human-expert annotations validate the pipeline's accuracy and validity. However, models tested on the benchmark still lag significantly behind human experts in performance. To support further research, a larger dataset EvidenceBench-100k with over 100,000 fully annotated papers is created. The datasets are made available for model training and development. This study highlights the challenges in automating evidence retrieval in biomedical literature, indicating the need for further advancements in language models and retrieval systems. 

<br /><br />Summary: <div>
arXiv:2504.18736v1 Announce Type: new 
Abstract: We study the task of automatically finding evidence relevant to hypotheses in biomedical papers. Finding relevant evidence is an important step when researchers investigate scientific hypotheses. We introduce EvidenceBench to measure models performance on this task, which is created by a novel pipeline that consists of hypothesis generation and sentence-by-sentence annotation of biomedical papers for relevant evidence, completely guided by and faithfully following existing human experts judgment. We demonstrate the pipeline's validity and accuracy with multiple sets of human-expert annotations. We evaluated a diverse set of language models and retrieval systems on the benchmark and found that model performances still fall significantly short of the expert level on this task. To show the scalability of our proposed pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated papers with hypotheses to facilitate model training and development. Both datasets are available at https://github.com/EvidenceBench/EvidenceBench
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.18762</link>
<guid>https://arxiv.org/abs/2504.18762</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, legal domain, pre-training, synthetic data augmentation, legal document analysis 

Summary: 
SynLexLM introduces a novel approach to efficiently pre-train large language models (LLMs) for specialized legal domains like law. The method incorporates curriculum learning, starting from simple to complex legal texts and queries, along with synthetic data augmentation using models like Gemini Pro to address the challenge of limited legal data availability. The goal is to enhance performance on legal benchmarks such as BigLaw-Bench and EUR-Lex-Sum compared to traditional models and fine-tuned versions. The initial phase of the work involves generating synthetic question-answer pairs that reflect legal reasoning. The ultimate aim is to improve legal document analysis and research tools, potentially making advanced legal AI more accessible to a wider audience. <div>
arXiv:2504.18762v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are powerful but often require extensive fine-tuning and large datasets for specialized domains like law. General-purpose pre-training may not capture legal nuances, and acquiring sufficient legal data is challenging. We introduce SynLexLM, a novel approach to efficiently pre-train a legal LLM. Our method employs curriculum learning, progressing from simple to complex legal texts and queries, combined with synthetic data augmentation using models like Gemini Pro to address data scarcity. We aim to achieve improved performance on legal benchmarks (BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned versions. Preliminary work involves generating synthetic QA pairs reflecting legal reasoning. This work aims to enhance legal document analysis and research tools, potentially democratizing access to advanced legal AI.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation</title>
<link>https://arxiv.org/abs/2504.18805</link>
<guid>https://arxiv.org/abs/2504.18805</guid>
<content:encoded><![CDATA[
<div> Keywords: short-form videos, scientific papers, multi-LLM agentic framework, content summarization, visual scene planning <br />
Summary: <br />
Generating engaging and accurate short-form videos from scientific papers is a challenging task due to content complexity and the gap between experts and readers. Existing methods often lack factual accuracy and visual quality, hindering effective dissemination of scientific information. To address these issues, the SciTalk framework is proposed, leveraging a multi-LLM agentic approach that incorporates various sources and specialized agents for content summarization and visual planning. The framework also utilizes an iterative feedback mechanism to enhance video quality based on user simulation feedback. Experimental evaluations demonstrate that SciTalk surpasses simple prompting methods in producing scientifically accurate and engaging video content. While still not reaching human creator quality, the framework offers valuable insights into feedback-driven video generation and its benefits for scientific communication. The code, data, and generated videos will be made publicly available. <br /> 

Summary: <div>
arXiv:2504.18805v1 Announce Type: new 
Abstract: Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators' workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators' quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks</title>
<link>https://arxiv.org/abs/2504.18838</link>
<guid>https://arxiv.org/abs/2504.18838</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, evaluation, capability-based, automated, generalization

Summary:
Large Language Models (LLMs) have rapidly become essential across various sectors, leading to core challenges in their evaluation. The survey discusses transitions from task-specific to capability-based evaluation and from manual to automated evaluation. Key challenges include the evaluation generalization issue, as bounded test sets struggle to keep up with the expanding abilities of LLMs. The survey delves into methods, datasets, evaluators, and metrics in addressing these challenges. A living GitHub repository is maintained for continual updates and corrections, with contributions and collaboration encouraged. The dynamic nature of the field necessitates a collaborative approach to evolving evaluation practices.<br /><br />Summary: <div>
arXiv:2504.18838v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are advancing at an amazing speed and have become indispensable across academia, industry, and daily applications. To keep pace with the status quo, this survey probes the core challenges that the rise of LLMs poses for evaluation. We identify and analyze two pivotal transitions: (i) from task-specific to capability-based evaluation, which reorganizes benchmarks around core competencies such as knowledge, reasoning, instruction following, multi-modal understanding, and safety; and (ii) from manual to automated evaluation, encompassing dynamic dataset curation and "LLM-as-a-judge" scoring.
  Yet, even with these transitions, a crucial obstacle persists: the evaluation generalization issue. Bounded test sets cannot scale alongside models whose abilities grow seemingly without limit. We will dissect this issue, along with the core challenges of the above two transitions, from the perspectives of methods, datasets, evaluators, and metrics. Due to the fast evolving of this field, we will maintain a living GitHub repository (links are in each section) to crowd-source updates and corrections, and warmly invite contributors and collaborators.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning</title>
<link>https://arxiv.org/abs/2504.18839</link>
<guid>https://arxiv.org/abs/2504.18839</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, conversational breakdowns, fine-tuning, dialogue detection, real-time deployment

Summary: 
This paper explores the challenges of detecting and mitigating dialogue breakdowns within Large Language Models (LLMs) used in conversational systems. The proposed approach combines specialized fine-tuning techniques with advanced prompting strategies such as few-shot learning, chain-of-thought reasoning, and analogical prompting. By fine-tuning a small 8B model, the authors demonstrate its robust capabilities in classifying and calibrating responses in English and Japanese dialogues, as well as its generalization on the BETOLD dataset with a 7% accuracy improvement over the base model. Additionally, a real-time deployment architecture is introduced to selectively escalate suspicious responses to more resource-intensive models only when breakdowns are detected, resulting in cost and energy savings. Experimental results show that the proposed method surpasses prior state-of-the-art specialized classifiers and narrows the performance gaps between smaller open-source models and large proprietary ones. This approach provides a scalable solution for robust conversational AI in high-impact domains by combining efficiency, interpretability, and reliability.<br /><br />Summary: <div>
arXiv:2504.18839v1 Announce Type: new 
Abstract: Large language models (LLMs) are rapidly changing various domains. However, their capabilities in handling conversational breakdowns still require an in-depth exploration. This paper addresses the challenge of detecting and mitigating dialogue breakdowns within LLM-driven conversational systems. While powerful models from OpenAI and Anthropic excel in many dialogue tasks, they can still produce incoherent or contradictory responses, commonly referred to as breakdowns, which undermine user trust. To tackle this, we propose an approach that combines specialized fine-tuning with advanced prompting strategies, including few-shot learning, chain-of-thought reasoning, and analogical prompting. In particular, we fine-tune a small 8B model and demonstrate its robust classification and calibration capabilities in English and Japanese dialogue. We also validate its generalization on the BETOLD dataset, achieving a 7\% accuracy improvement over its base model. Furthermore, we introduce a real-time deployment architecture that selectively escalates suspicious responses to more resource-intensive frontier models only when breakdowns are detected, significantly cutting operational expenses and energy consumption. Experimental results show our method surpasses prior state-of-the-art specialized classifiers while also narrowing performance gaps between smaller open-source models and large proprietary ones. Our approach offers a scalable solution for robust conversational AI in high-impact domains by combining efficiency, interpretability, and reliability.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When2Call: When (not) to Call Tools</title>
<link>https://arxiv.org/abs/2504.18851</link>
<guid>https://arxiv.org/abs/2504.18851</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, benchmark, tool-calling decision-making, training data, preference optimization<br />
Summary:<br />
Modern Language Models (LMs) are increasingly utilizing external tools to enhance their capabilities. A new benchmark, When2Call, focuses on evaluating the decision-making process of LMs in calling external tools. This benchmark assesses when to generate a tool call, when to ask follow-up questions, and when to acknowledge the limitations of the tools available. State-of-the-art LMs exhibit room for improvement on When2Call, underscoring the significance of this evaluation metric. The development of a training set for When2Call and the implementation of a preference optimization training regime have shown substantial improvements compared to traditional fine-tuning methods. The benchmark, training data, and evaluation scripts are openly available on GitHub. This initiative aims to drive advancements in tool-calling strategies of LMs, paving the way for more effective integration of external tools into language processing systems.<br /><br />Summary: <div>
arXiv:2504.18851v1 Announce Type: new 
Abstract: Leveraging external tools is a key feature for modern Language Models (LMs) to expand their capabilities and integrate them into existing systems. However, existing benchmarks primarily focus on the accuracy of tool calling -- whether the correct tool is called with the correct parameters -- and less on evaluating when LMs should (not) call tools. We develop a new benchmark, When2Call, which evaluates tool-calling decision-making: when to generate a tool call, when to ask follow-up questions and when to admit the question can't be answered with the tools provided. We find that state-of-the-art tool-calling LMs show significant room for improvement on When2Call, indicating the importance of this benchmark. We also develop a training set for When2Call and leverage the multiple-choice nature of the benchmark to develop a preference optimization training regime, which shows considerably more improvement than traditional fine-tuning. We release the benchmark and training data as well as evaluation scripts at https://github.com/NVIDIA/When2Call.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation</title>
<link>https://arxiv.org/abs/2504.18857</link>
<guid>https://arxiv.org/abs/2504.18857</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Dimension-Wise Positional Embeddings Manipulation, context window, extrapolation, performance improvement

Summary:<br /><br />Large Language Models (LLMs) face challenges with processing and generating coherent context when exceeding the pre-trained length. A new framework, Dimension-Wise Positional Embeddings Manipulation (DPE), is proposed to extend the context window without additional training overhead. DPE focuses on manipulating key dimensions for context extension, reaching optimal states for each dimension. It outperforms baseline methods like YaRN and Self-Extend, allowing models like Llama3-8k 8B to support 128k token context windows. DPE seamlessly integrates with Flash Attention 2 and boosts performance on long-context benchmarks like RULER for models like Llama3.1 70B by 18 points. Even compared to commercial models, Llama 3.1 70B with DPE achieves superior performance to GPT-4-128K. <div>
arXiv:2504.18857v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often struggle to process and generate coherent context when the number of input tokens exceeds the pre-trained length. Recent advancements in long-context extension have significantly expanded the context window of LLMs but require expensive overhead to train the large-scale models with longer context. In this work, we propose Dimension-Wise Positional Embeddings Manipulation (DPE), a training-free framework to extrapolate the context window of LLMs by diving into RoPE's different hidden dimensions. Instead of manipulating all dimensions equally, DPE detects the effective length for every dimension and finds the key dimensions for context extension. We reuse the original position indices with their embeddings from the pre-trained model and manipulate the key dimensions' position indices to their most effective lengths. In this way, DPE adjusts the pre-trained models with minimal modifications while ensuring that each dimension reaches its optimal state for extrapolation. DPE significantly surpasses well-known baselines such as YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of 128k tokens without continual training and integrates seamlessly with Flash Attention 2. In addition to its impressive extrapolation capability, DPE also dramatically improves the models' performance within training length, such as Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When compared with commercial models, Llama 3.1 70B with DPE even achieves better performance than GPT-4-128K.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Adversarial Training Improves the Representation of Refusal</title>
<link>https://arxiv.org/abs/2504.18872</link>
<guid>https://arxiv.org/abs/2504.18872</guid>
<content:encoded><![CDATA[
<div> refusal behavior, language models, Latent Adversarial Training, robustness, targeted attacks <br />
<br />
Summary: 
The article explores the impact of Latent Adversarial Training (LAT) on the encoding of refusal behavior in language models. By analyzing a model called Llama 27B, the study compares LAT with traditional supervised safety fine-tuning (SSFT) and embedding space adversarial training (AT). It finds that LAT significantly alters the representation of refusal behavior in the model's latent space, concentrating it in the first two Singular Value Decomposition (SVD) components. This concentrated representation leads to more effective and transferable refusal vectors for ablation attacks. While LAT models show improved robustness against attacks using vectors from reference models, they become more vulnerable to self-generated vectors compared to SSFT and AT. The study suggests that LAT's training perturbations enable a more comprehensive representation of refusal behavior, highlighting both its strengths and vulnerabilities in enhancing model safety. <br /> <div>
arXiv:2504.18872v1 Announce Type: new 
Abstract: Recent work has shown that language models' refusal behavior is primarily encoded in a single direction in their latent space, making it vulnerable to targeted attacks. Although Latent Adversarial Training (LAT) attempts to improve robustness by introducing noise during training, a key question remains: How does this noise-based training affect the underlying representation of refusal behavior? Understanding this encoding is crucial for evaluating LAT's effectiveness and limitations, just as the discovery of linear refusal directions revealed vulnerabilities in traditional supervised safety fine-tuning (SSFT).
  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the refusal behavior in the model's latent space compared to SSFT and embedding space adversarial training (AT). By computing activation differences between harmful and harmless instruction pairs and applying Singular Value Decomposition (SVD), we find that LAT significantly alters the refusal representation, concentrating it in the first two SVD components which explain approximately 75 percent of the activation differences variance - significantly higher than in reference models. This concentrated representation leads to more effective and transferable refusal vectors for ablation attacks: LAT models show improved robustness when attacked with vectors from reference models but become more vulnerable to self-generated vectors compared to SSFT and AT. Our findings suggest that LAT's training perturbations enable a more comprehensive representation of refusal behavior, highlighting both its potential strengths and vulnerabilities for improving model safety.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification</title>
<link>https://arxiv.org/abs/2504.18884</link>
<guid>https://arxiv.org/abs/2504.18884</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, variability, reproducibility, sentiment analysis, ensemble strategy  
Summary:  
- The study focuses on the challenges of variability and reproducibility in results obtained from large language models (LLMs) for sentiment analysis.  
- Existing literature has largely overlooked these issues, which are crucial for ensuring the reliability of LLMs.  
- Human annotation techniques use majority voting to resolve disagreements among annotators, highlighting the importance of ensemble strategies in LLM applications.  
- The study introduces a straightforward ensemble strategy for sentiment analysis using medium-sized LLMs, demonstrating its effectiveness in producing more robust and accurate results.  
- By utilizing multiple inferences with medium-sized LLMs, the ensemble approach reduces the root mean square error (RMSE) by 18.6% compared to using a single large model with a single attempt.  

<br /><br />Summary: <div>
arXiv:2504.18884v1 Announce Type: new 
Abstract: With the advance of large language models (LLMs), LLMs have been utilized for the various tasks. However, the issues of variability and reproducibility of results from each trial of LLMs have been largely overlooked in existing literature while actual human annotation uses majority voting to resolve disagreements among annotators. Therefore, this study introduces the straightforward ensemble strategy to a sentiment analysis using LLMs. As the results, we demonstrate that the ensemble of multiple inference using medium-sized LLMs produces more robust and accurate results than using a large model with a single attempt with reducing RMSE by 18.6%.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction</title>
<link>https://arxiv.org/abs/2504.18938</link>
<guid>https://arxiv.org/abs/2504.18938</guid>
<content:encoded><![CDATA[
<div> Correction, Chinese Spelling Correction, Large Language Models, domain adaptation, length consistency

Summary:<br />
This work introduces a Multi-Turn CSC framework for Chinese Spelling Correction (CSC) tasks, extending traditional CSC to variable-length correction scenarios such as Chinese Splitting Error Correction (CSEC) and ASR N-best Error Correction. The proposed framework, MTCSC, leverages a retrieval database and length reflection mechanism to address domain adaptation and ensure output length fidelity. By fine-tuning retrievers on domain-specific training data and dictionaries, MTCSC significantly outperforms current approaches in correction quality, particularly in handling domain-specific and variable-length error correction tasks. The framework also introduces a multi-source combination strategy with iterative length reflection for consistent output lengths, showcasing improved performance across diverse domain datasets. <br /><br />Summary: <div>
arXiv:2504.18938v1 Announce Type: new 
Abstract: Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens in sentences. While Large Language Models (LLMs) have shown remarkable success in identifying and rectifying potential errors, they often struggle with maintaining consistent output lengths and adapting to domain-specific corrections. Furthermore, existing CSC task impose rigid constraints requiring input and output lengths to be identical, limiting their applicability. In this work, we extend traditional CSC to variable-length correction scenarios, including Chinese Splitting Error Correction (CSEC) and ASR N-best Error Correction. To address domain adaptation and length consistency, we propose MTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection mechanism. Our approach constructs a retrieval database from domain-specific training data and dictionaries, fine-tuning retrievers to optimize performance for error-containing inputs. Additionally, we introduce a multi-source combination strategy with iterative length reflection to ensure output length fidelity. Experiments across diverse domain datasets demonstrate that our method significantly outperforms current approaches in correction quality, particularly in handling domain-specific and variable-length error correction tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LawFlow : Collecting and Simulating Lawyers' Thought Processes</title>
<link>https://arxiv.org/abs/2504.18942</link>
<guid>https://arxiv.org/abs/2504.18942</guid>
<content:encoded><![CDATA[
<div> dataset, legal workflows, AI, decision-making, law students <br />
Summary:
This article introduces LawFlow, a dataset of end-to-end legal workflows collected from trained law students, aimed at capturing the complexity of real-world legal practice. The comparison between human and LLM-generated workflows reveals differences in structure, reasoning flexibility, and plan execution. Human workflows are found to be more modular and adaptive, while LLM workflows are more sequential and exhaustive. Legal professionals prefer AI to play a supportive role in brainstorming, identifying blind spots, and surfacing alternatives rather than executing entire workflows. Design suggestions are proposed to align AI assistance with human goals, emphasizing clarity, completeness, creativity, and efficiency through hybrid planning and decision-point support. The study highlights the limitations of current LLMs in supporting complex legal workflows and points towards opportunities for developing collaborative, reasoning-aware legal AI systems. <div>
arXiv:2504.18942v1 Announce Type: new 
Abstract: Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, we propose a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/).
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Fisher-weighted Model Merging via Bayesian Optimization</title>
<link>https://arxiv.org/abs/2504.18992</link>
<guid>https://arxiv.org/abs/2504.18992</guid>
<content:encoded><![CDATA[
<div> merging, language models, multi-task, parameter importance, dynamic

Summary:<br />
This paper presents Dynamic Fisher-weighted Merging (DF-Merge), a novel approach to creating multi-task models by combining fine-tuned language models at the parameter level. DF-Merge dynamically adjusts coefficients associated with candidate models using Bayesian optimization to maximize performance on validation sets. The process integrates parameter importance based on Fisher information conditioned by the coefficients, leading to significant performance improvements compared to existing merging approaches. Experimental results demonstrate the effectiveness of DF-Merge across models of varying sizes and tasks. The unified view of merging in DF-Merge allows for near-optimal performance to be achieved in a few iterations, even with limited validation data. The approach outperforms strong baselines and highlights the benefits of integrating parameter importance in the merging process. <div>
arXiv:2504.18992v1 Announce Type: new 
Abstract: The fine-tuning of pre-trained language models has resulted in the widespread availability of task-specific models. Model merging offers an efficient way to create multi-task models by combining these fine-tuned models at the parameter level, without the need for training data or joint training on multiple datasets. Existing merging approaches typically involve scaling the parameters model-wise or integrating parameter importance parameter-wise. Both approaches exhibit their own weaknesses, leading to a notable performance gap compared to multi-task fine-tuning. In this paper, we unify these seemingly distinct strategies into a more general merging framework, and introduce Dynamic Fisher-weighted Merging (DF-Merge). Specifically, candidate models are associated with a set of coefficients that linearly scale their fine-tuned parameters. Bayesian optimization is applied to dynamically adjust these coefficients, aiming to maximize overall performance on validation sets. Each iteration of this process integrates parameter importance based on the Fisher information conditioned by the coefficients. Experimental results show that DF-Merge outperforms strong baselines across models of different sizes and a variety of tasks. Our analysis shows that the effectiveness of DF-Merge arises from the unified view of merging and that near-optimal performance is achievable in a few iterations, even with minimal validation data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs</title>
<link>https://arxiv.org/abs/2504.19019</link>
<guid>https://arxiv.org/abs/2504.19019</guid>
<content:encoded><![CDATA[
<div> Graph of ATtacks, Large Language Models, Adversarial Prompts, Robustness, Jailbreak <br />
<br />
Summary: 
The article introduces Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the alignment of Large Language Models (LLMs) with societal standards. GoAT excels at creating highly effective jailbreak prompts with fewer queries to the victim model, achieving up to five times better success rate against robust models like Llama. It is a black-box attack that does not require access to the targeted model's parameters. GoAT's reasoning is based on a sophisticated graph structure, enabling a deeper integration and refinement of attack paths. By combining and improving thoughts iteratively, GoAT allows collaborative exploration of adversarial vulnerabilities in LLMs. The code for implementation is available on GitHub. <div>
arXiv:2504.19019v1 Announce Type: new 
Abstract: The challenge of ensuring Large Language Models (LLMs) align with societal standards is of increasing interest, as these models are still prone to adversarial jailbreaks that bypass their safety mechanisms. Identifying these vulnerabilities is crucial for enhancing the robustness of LLMs against such exploits. We propose Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the robustness of LLM alignment using the Graph of Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly effective jailbreak prompts with fewer queries to the victim model than state-of-the-art attacks, achieving up to five times better jailbreak success rate against robust models like Llama. Notably, GoAT creates high-quality, human-readable prompts without requiring access to the targeted model's parameters, making it a black-box attack. Unlike approaches constrained by tree-based reasoning, GoAT's reasoning is based on a more intricate graph structure. By making simultaneous attack paths aware of each other's progress, this dynamic framework allows a deeper integration and refinement of reasoning paths, significantly enhancing the collaborative exploration of adversarial vulnerabilities in LLMs. At a technical level, GoAT starts with a graph structure and iteratively refines it by combining and improving thoughts, enabling synergy between different thought paths. The code for our implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting</title>
<link>https://arxiv.org/abs/2504.19021</link>
<guid>https://arxiv.org/abs/2504.19021</guid>
<content:encoded><![CDATA[
<div> language models, text classification, dataset augmentation, fine-tuning, academic publications
Summary:
- The study focuses on efficient text classification for academic publications using pre-trained language models (PLMs) like BERT, SciBERT, BioBERT, and BlueBERT.
- Dataset augmentation is done by retrieving additional articles from the Web of Science database and using PLMs to predict labels for them.
- A hard-voting strategy is employed to combine predictions for improved accuracy and confidence.
- Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly enhances classification accuracy, especially in specialized domains.
- Domain-specific models like SciBERT and BioBERT outperform general-purpose models like BERT consistently. This study highlights the effectiveness of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in developing robust and scalable solutions for automated academic text classification.<br /><br />Summary: <div>
arXiv:2504.19021v1 Announce Type: new 
Abstract: Efficient text classification is essential for handling the increasing volume of academic publications. This study explores the use of pre-trained language models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on the Web of Science (WoS-46985) dataset for scientific text classification. To enhance performance, we augment the dataset by executing seven targeted queries in the WoS database, retrieving 1,000 articles per category aligned with WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a hard-voting strategy combines predictions for improved accuracy and confidence. Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly boosts classification accuracy, especially in specialized domains. Domain-specific models like SciBERT and BioBERT consistently outperform general-purpose models such as BERT. These findings underscore the efficacy of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in creating robust and scalable solutions for automated academic text classification.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation</title>
<link>https://arxiv.org/abs/2504.19024</link>
<guid>https://arxiv.org/abs/2504.19024</guid>
<content:encoded><![CDATA[
<div> Keywords: k-step return, reinforcement learning, knowledge distillation, text generation, large language model<br />
Summary:<br />
The proposed method, KETCHUP, introduces a novel approach for estimating k-step returns in Reinforcement Learning-based knowledge distillation in text generation tasks. By leveraging the Bellman Optimality Equation for multiple steps, KETCHUP reduces gradient estimate variance, enhancing RL optimization, particularly with larger student model sizes. Empirical evaluations across three text generation tasks demonstrate superior performance in both standard metrics and evaluation against large language models. These findings indicate the efficacy of KETCHUP in improving RL-based knowledge distillation for large language model research. <div>
arXiv:2504.19024v1 Announce Type: new 
Abstract: We propose a novel k-step return estimation method (called KETCHUP) for Reinforcement Learning(RL)-based knowledge distillation (KD) in text generation tasks. Our idea is to induce a K-step return by using the Bellman Optimality Equation for multiple steps. Theoretical analysis shows that this K-step formulation reduces the variance of the gradient estimates, thus leading to improved RL optimization especially when the student model size is large. Empirical evaluation on three text generation tasks demonstrates that our approach yields superior performance in both standard task metrics and large language model (LLM)-based evaluation. These results suggest that our K-step return induction offers a promising direction for enhancing RL-based KD in LLM research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Translation Decoding with Quality Estimation on LLMs</title>
<link>https://arxiv.org/abs/2504.19044</link>
<guid>https://arxiv.org/abs/2504.19044</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural machine translation, calibration, likelihood, quality, decoding

Summary:
This paper introduces a new method for improving neural machine translation systems by calibrating hypothesis likelihoods with translation quality. By directly optimizing the correlation between hypothesis likelihoods and real-world translation quality, the proposed method enhances the effectiveness of translation decoding. This approach leads to significant improvements in translation quality on large language models (LLMs) with limited training data. The calibrated translation likelihoods serve as a strong proxy for translation quality and outperform state-of-the-art translation quality estimation models. The methodology also enhances the efficiency of maximum a posteriori (MAP) decoding, making deployment of the translation model more effective in real-world scenarios. The resulting state-of-the-art translation model, covering 10 languages, along with code and human evaluation data, has been released to the community for further research and development. 

<br /><br />Summary: <div>
arXiv:2504.19044v1 Announce Type: new 
Abstract: Neural machine translation (NMT) systems typically employ maximum a posteriori (MAP) decoding to select the highest-scoring translation from the distribution mass. However, recent evidence highlights the inadequacy of MAP decoding, often resulting in low-quality or even pathological hypotheses -- the decoding objective is not aligned with real-world translation quality. This paper proposes calibrating hypothesis likelihoods with translation quality from a distribution view by directly optimizing their Pearson correlation -- thereby enhancing the effectiveness of translation decoding. With our method, translation on large language models (LLMs) improves substantially after limited training (2K instances per direction). This improvement is orthogonal to those achieved through supervised fine-tuning, leading to substantial gains across a broad range of metrics and human evaluations -- even when applied to top-performing translation-specialized LLMs fine-tuned on high-quality translation data, such as Tower, or when compared to recent preference optimization methods, like CPO. Moreover, the calibrated translation likelihood can directly serve as a strong proxy for translation quality, closely approximating or even surpassing some state-of-the-art translation quality estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates that calibration enhances the effectiveness of MAP decoding, thereby enabling greater efficiency in real-world deployment. The resulting state-of-the-art translation model, which covers 10 languages, along with the accompanying code and human evaluation data, has been released to the community: https://github.com/moore3930/calibrating-llm-mt.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models</title>
<link>https://arxiv.org/abs/2504.19061</link>
<guid>https://arxiv.org/abs/2504.19061</guid>
<content:encoded><![CDATA[
<div> extract, clinical summarization, large language models, discharge reports, hallucinations <br />
Summary: Large language models are being investigated for their effectiveness in extracting key events from discharge reports in healthcare. These models aim to provide precise and concise information transfer for enhanced patient understanding and care management. The study evaluates the performance of open-source LLMs in identifying reasons for hospital admission, significant in-hospital events, critical follow-up actions, and detecting hallucinations in the summaries produced. Detecting hallucinations is crucial as it impacts the reliability of information and can influence patient care and treatment outcomes. Comprehensive numerical simulations are conducted to rigorously assess the accuracy and fidelity of the extracted content in clinical summarization. <div>
arXiv:2504.19061v1 Announce Type: new 
Abstract: Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, such as reasons for hospital admission, significant in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive numerical simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics</title>
<link>https://arxiv.org/abs/2504.19066</link>
<guid>https://arxiv.org/abs/2504.19066</guid>
<content:encoded><![CDATA[
<div> Keywords: Extreme Weather Events, Large Language Models, Small Language Models, Reasoning Paths, Emotion Analysis 

Summary:<br /><br />Accurate assessments of extreme weather events are essential for research and policy, but data gaps hinder effective decision-making. This paper introduces the ClimaEmpact framework, using Large Language Models (LLMs) to enhance Small Language Models (SLMs) for extreme weather analytics. By incorporating structured reasoning paths and utilizing the ExtremeWeatherNews dataset, the proposed method, Extreme Weather Reasoning-Aware Alignment (EWRA), improves the generation of domain-specific responses for tasks such as vulnerability/impact categorization, topic labeling, and emotion analysis. Results demonstrate that the alignment of SLMs with advanced reasoning strategies leads to outputting domain-aligned responses, outperforming task-specific models and providing enhanced real-world applicability for extreme weather analytics. <div>
arXiv:2504.19066v1 Announce Type: new 
Abstract: Accurate assessments of extreme weather events are vital for research and policy, yet localized and granular data remain scarce in many parts of the world. This data gap limits our ability to analyze potential outcomes and implications of extreme weather events, hindering effective decision-making. Large Language Models (LLMs) can process vast amounts of unstructured text data, extract meaningful insights, and generate detailed assessments by synthesizing information from multiple sources. Furthermore, LLMs can seamlessly transfer their general language understanding to smaller models, enabling these models to retain key knowledge while being fine-tuned for specific tasks. In this paper, we propose Extreme Weather Reasoning-Aware Alignment (EWRA), a method that enhances small language models (SLMs) by incorporating structured reasoning paths derived from LLMs, and ExtremeWeatherNews, a large dataset of extreme weather event-related news articles. EWRA and ExtremeWeatherNews together form the overall framework, ClimaEmpact, that focuses on addressing three critical extreme-weather tasks: categorization of tangible vulnerabilities/impacts, topic labeling, and emotion analysis. By aligning SLMs with advanced reasoning strategies on ExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for SLM alignment), EWRA improves the SLMs' ability to generate well-grounded and domain-specific responses for extreme weather analytics. Our results show that the approach proposed guides SLMs to output domain-aligned responses, surpassing the performance of task-specific models and offering enhanced real-world applicability for extreme weather analytics.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-Efficient Language Model for Hinglish Conversational AI</title>
<link>https://arxiv.org/abs/2504.19070</link>
<guid>https://arxiv.org/abs/2504.19070</guid>
<content:encoded><![CDATA[
<div> Keywords: Hinglish, chatbot, language model, fine-tuning, data scarcity 

Summary: 
This paper discusses the development of a sample-efficient language model for a conversational Hinglish chatbot. Hinglish, a mix of Hindi and English, poses challenges due to inconsistent spelling and limited conversational data quality. The study evaluates pre-trained cross-lingual language models such as Gemma3-4B and Qwen2.5-7B, utilizing fine-tuning techniques to enhance performance on Hinglish conversational tasks. By incorporating synthetically generated dialogues and insights from existing Hinglish datasets, the approach aims to overcome data scarcity issues. Experimental findings indicate that models with fewer parameters, when properly fine-tuned on high-quality code-mixed data, can deliver competitive results in Hinglish conversation generation while ensuring computational efficiency. 

<br /><br />Summary: <div>
arXiv:2504.19070v1 Announce Type: new 
Abstract: This paper presents our process for developing a sample-efficient language model for a conversational Hinglish chatbot. Hinglish, a code-mixed language that combines Hindi and English, presents a unique computational challenge due to inconsistent spelling, lack of standardization, and limited quality of conversational data. This work evaluates multiple pre-trained cross-lingual language models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning techniques to improve performance on Hinglish conversational tasks. The proposed approach integrates synthetically generated dialogues with insights from existing Hinglish datasets to address data scarcity. Experimental results demonstrate that models with fewer parameters, when appropriately fine-tuned on high-quality code-mixed data, can achieve competitive performance for Hinglish conversation generation while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Reasoning for LLMs through Speculative Chain-of-Thought</title>
<link>https://arxiv.org/abs/2504.19095</link>
<guid>https://arxiv.org/abs/2504.19095</guid>
<content:encoded><![CDATA[
<div> Latency reduction, Large reasoning language models, Speculative Chain-of-Thought, Model collaboration, Efficient drafting

Summary: 
Speculative Chain-of-Thought (SCoT) introduces a new approach to reducing reasoning latency in large reasoning language models like OpenAI-o1 and Deepseek-R1. By combining large and small models, SCoT accelerates average reasoning speed by conducting thought-level drafting with a lightweight draft model and then correcting errors with the target model. This thinking behavior alignment improves drafting efficiency and maintains prediction accuracy for complex problems. Experimental results on various datasets demonstrate that SCoT reduces reasoning latency by 48% to 66% for Deepseek-R1-Distill-Qwen-32B while achieving performance levels close to the target model. The code for SCoT is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2504.19095v1 Announce Type: new 
Abstract: Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have recently attracted widespread attention due to their impressive task-solving abilities. However, the enormous model size and the generation of lengthy thought chains introduce significant reasoning costs and response latency. Existing methods for efficient reasoning mainly focus on reducing the number of model parameters or shortening the chain-of-thought length. In this paper, we introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency from another perspective by accelerated average reasoning speed through large and small model collaboration. SCoT conducts thought-level drafting using a lightweight draft model. Then it selects the best CoT draft and corrects the error cases with the target model. The proposed thinking behavior alignment improves the efficiency of drafting and the draft selection strategy maintains the prediction accuracy for complex problems. Experimental results on GSM8K, MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces reasoning latency by 48\%$\sim$66\% for Deepseek-R1-Distill-Qwen-32B while achieving near-target-model-level performance. Our code is available at https://github.com/Jikai0Wang/Speculative_CoT.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.19101</link>
<guid>https://arxiv.org/abs/2504.19101</guid>
<content:encoded><![CDATA[
<div> Framework, Federated Learning, Retrieval-Augmented Generation, Data Privacy, Knowledge Distillation
Summary: 
The article introduces a novel framework called Federated Retrieval-Augmented Generation (FedE4RAG) to address challenges faced by private RAG systems. FedE4RAG utilizes federated learning to collaboratively train client-side RAG retrieval models while ensuring data privacy. Knowledge distillation is employed for communication between server and client models to improve generalization. Homomorphic encryption is applied in federated learning to protect model parameters and mitigate data leakage concerns. Experimental results on a real-world dataset validate the effectiveness of FedE4RAG in enhancing the performance of private RAG systems while maintaining robust data privacy protection. 
<br /><br />Summary: <div>
arXiv:2504.19101v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution for enhancing the accuracy and credibility of Large Language Models (LLMs), particularly in Question & Answer tasks. This is achieved by incorporating proprietary and private data from integrated databases. However, private RAG systems face significant challenges due to the scarcity of private domain data and critical data privacy issues. These obstacles impede the deployment of private RAG systems, as developing privacy-preserving RAG systems requires a delicate balance between data security and data availability. To address these challenges, we regard federated learning (FL) as a highly promising technology for privacy-preserving RAG services. We propose a novel framework called Federated Retrieval-Augmented Generation (FedE4RAG). This framework facilitates collaborative training of client-side RAG retrieval models. The parameters of these models are aggregated and distributed on a central-server, ensuring data privacy without direct sharing of raw data. In FedE4RAG, knowledge distillation is employed for communication between the server and client models. This technique improves the generalization of local RAG retrievers during the federated learning process. Additionally, we apply homomorphic encryption within federated learning to safeguard model parameters and mitigate concerns related to data leakage. Extensive experiments conducted on the real-world dataset have validated the effectiveness of FedE4RAG. The results demonstrate that our proposed framework can markedly enhance the performance of private RAG systems while maintaining robust data privacy protection.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries</title>
<link>https://arxiv.org/abs/2504.19110</link>
<guid>https://arxiv.org/abs/2504.19110</guid>
<content:encoded><![CDATA[
<div> Benchmark, Language models, Automated Proof Engineering, Mathlib, Eleanstic

Summary:
Automated Proof Engineering (APE) leverages large language models (LLMs) to automate proof engineering tasks like feature additions and bug fixing in formal mathematics libraries. APE-Bench I introduces a benchmark based on real-world commit histories of Mathlib, presenting diverse file-level tasks verified using Lean compiler and LLM-as-a-Judge. Eleanstic is a parallel verification infrastructure designed for checking proofs across different Mathlib versions. Experimental results show LLMs excel in localized edits but struggle with complex proof engineering tasks. This research paves the way for developing autonomous workflows in proof engineering, with future benchmarks aiming for multi-file coordination, project-scale verification, and agent-based systems capable of planning, editing, and maintaining formal libraries. 

<br /><br />Summary: <div>
arXiv:2504.19110v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has shown promise in formal theorem proving, yet existing benchmarks remain limited to isolated, static proof tasks, failing to capture the iterative, engineering-intensive workflows of real-world formal mathematics libraries. Motivated by analogous advances in software engineering, we introduce the paradigm of Automated Proof Engineering (APE), which aims to automate proof engineering tasks such as feature addition, proof refactoring, and bug fixing using LLMs. To facilitate research in this direction, we present APE-Bench I, the first realistic benchmark built from real-world commit histories of Mathlib4, featuring diverse file-level tasks described in natural language and verified via a hybrid approach combining the Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable parallel verification infrastructure optimized for proof checking across multiple versions of Mathlib. Empirical results on state-of-the-art LLMs demonstrate strong performance on localized edits but substantial degradation on handling complex proof engineering. This work lays the foundation for developing agentic workflows in proof engineering, with future benchmarks targeting multi-file coordination, project-scale verification, and autonomous agents capable of planning, editing, and repairing formal libraries.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.19162</link>
<guid>https://arxiv.org/abs/2504.19162</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, self-play critic, error detection, reasoning process benchmarks, mathematical reasoning performance
Summary:
The paper introduces a novel approach called Self-Play Critic (SPC) to evaluate the step-by-step reliability of large language model (LLM) reasoning without the need for manual step-level annotation. SPC involves a "sneaky generator" model and a "critic" model that engage in an adversarial game to assess the correctness of reasoning steps. Through iterative improvement using reinforcement learning based on game outcomes, SPC enhances its error detection capabilities and surpasses strong baselines on three reasoning process benchmarks. Additionally, applying SPC to guide the test-time search of diverse LLMs improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models.<br /><br />Summary: 
- Introduction of Self-Play Critic (SPC) for evaluating LLM reasoning
- SPC involves a "sneaky generator" and a "critic" model in an adversarial game
- Iterative improvement of SPC through reinforcement learning
- SPC enhances error detection capabilities and surpasses baselines on benchmarks
- SPC improves mathematical reasoning performance of diverse LLMs <div>
arXiv:2504.19162v1 Announce Type: new 
Abstract: Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a "sneaky generator" that deliberately produces erroneous steps designed to be difficult to detect, and a "critic" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WuNeng: Hybrid State with Attention</title>
<link>https://arxiv.org/abs/2504.19191</link>
<guid>https://arxiv.org/abs/2504.19191</guid>
<content:encoded><![CDATA[
<div> Keywords: WuNeng architecture, recurrent neural network, attention mechanisms, multi-head attention, expressivity

Summary: 
The WuNeng architecture combines recurrent neural network-based RWKV-7 with advanced attention mechanisms to enhance the expressivity of large language models. It prioritizes contextual coherence over reducing KV cache size and introduces additional state-driven heads to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among different types of heads for robust information integration. The architecture also includes a multi-token state processing mechanism that leverages continuous RWKV-7 state to capture complex dependencies, leading to significant improvements in expressivity. Despite these enhancements, WuNeng maintains computational efficiency by minimizing additional parameters. This approach allows the model to excel in tasks that require complex reasoning and sequence generation, setting a new standard for balancing expressivity and efficiency in modern neural architectures.<br /><br />Summary: <div>
arXiv:2504.19191v1 Announce Type: new 
Abstract: The WuNeng architecture introduces a novel approach to enhancing the expressivity and power of large language models by integrating recurrent neural network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing heightened contextual coherence over reducing KV cache size. Building upon the hybrid-head concept from Hymba, WuNeng augments standard multi-head attention with additional RWKV-7 state-driven heads, rather than replacing existing heads, to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among standard, state-driven, and newly introduced middle heads, leveraging concatenation, additive modulation, and gated fusion for robust information integration. Furthermore, a multi-token state processing mechanism harnesses the continuous RWKV-7 state to capture intricate, sequence-wide dependencies, significantly boosting expressivity. Remarkably, these enhancements are achieved with minimal additional parameters, ensuring efficiency while empowering the model to excel in complex reasoning and sequence generation tasks. WuNeng sets a new standard for balancing expressivity and computational efficiency in modern neural architectures.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora</title>
<link>https://arxiv.org/abs/2504.19209</link>
<guid>https://arxiv.org/abs/2504.19209</guid>
<content:encoded><![CDATA[
<div> keywords: implementation choices, Dynamic Embedded Topic Model, diachronic corpora, scalability, temporal distributions

Summary: 
The study investigates the impact of various implementation decisions on the Dynamic Embedded Topic Model when applied to five different diachronic corpora. The research aims to identify key factors that will enhance the model's utility and future development. The findings highlight the importance of prioritizing the scalability of vocabulary size to leverage embedded representations effectively. Additionally, the study emphasizes the need for more flexible modeling of intervals to accommodate the uneven temporal distribution of historical writings. Furthermore, the research indicates that performance is not significantly or consistently affected by certain aspects that may otherwise limit the model's applicability or require excessive resources for grid search. Overall, the study underscores the significance of these identified priorities in maximizing the model's effectiveness for applied scholarship. 

<br /><br />Summary: <div>
arXiv:2504.19209v1 Announce Type: new 
Abstract: We measure the effects of several implementation choices for the Dynamic Embedded Topic Model, as applied to five distinct diachronic corpora, with the goal of isolating important decisions for its use and further development. We identify priorities that will maximize utility in applied scholarship, including the practical scalability of vocabulary size to best exploit the strengths of embedded representations, and more flexible modeling of intervals to accommodate the uneven temporal distributions of historical writing. Of similar importance, we find performance is not significantly or consistently affected by several aspects that otherwise limit the model's application or might consume the resources of a grid search.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
<link>https://arxiv.org/abs/2504.19254</link>
<guid>https://arxiv.org/abs/2504.19254</guid>
<content:encoded><![CDATA[
<div> hallucination detection, Large Language Models, uncertainty quantification, ensemble approach, Python toolkit<br />
<br />
Summary: 
Hallucinations in Large Language Models (LLMs) are a significant challenge in critical domains such as healthcare and finance. In response, a novel framework for zero-resource hallucination detection is proposed, leveraging various uncertainty quantification techniques. By transforming these techniques into standardized confidence scores and incorporating them into a tunable ensemble approach, practitioners can optimize performance for specific use cases. The accompanying Python toolkit, UQLM, offers a comprehensive suite of scorers for streamlined implementation. Extensive experiments on LLM question-answering benchmarks demonstrate the superiority of the tunable ensemble over individual scorers and existing methods, highlighting the benefits of customized hallucination detection strategies in enhancing the accuracy and reliability of LLMs. 
<br /><br /> <div>
arXiv:2504.19254v1 Announce Type: new 
Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we propose a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?</title>
<link>https://arxiv.org/abs/2504.19267</link>
<guid>https://arxiv.org/abs/2504.19267</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual storytelling, Multimodal models, Transformer-based architectures, VIST dataset, Novel evaluation metrics

Summary: 
The paper introduces a new approach to visual storytelling that utilizes transformer-based architectures and large multimodal models to generate cohesive narratives from sequences of images. The model, called VIST-GPT, is evaluated using the Visual Storytelling (VIST) dataset and focuses on producing visually grounded and contextually appropriate narratives. Traditional evaluation metrics like BLEU and CIDEr are deemed unsuitable for this task, prompting the development of RoViST and GROOVIST, reference-free metrics that assess narrative quality based on visual grounding, coherence, and non-redundancy. These metrics offer a more nuanced evaluation aligned with human judgment, providing a better understanding of the generated narratives' quality. <div>
arXiv:2504.19267v1 Announce Type: new 
Abstract: Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AndroidGen: Building an Android Language Agent under Data Scarcity</title>
<link>https://arxiv.org/abs/2504.19298</link>
<guid>https://arxiv.org/abs/2504.19298</guid>
<content:encoded><![CDATA[
<div> framework, AndroidGen, LLM-based agents, data scarcity, trajectory collection

Summary: 
The article discusses the challenges in utilizing large language models (LLMs) on mobile devices due to data scarcity and incomplete completion rates. To address these issues, the AndroidGen framework is developed to enhance LLM-based agents and collect trajectories from human tasks. The framework is used to train open-source LLMs without manually labeled trajectories, leading to the creation of an open-source mobile agent. AndroidGen is evaluated with various applications, showing improvements in performance and suggesting areas for further enhancement. This work provides a valuable contribution to leveraging LLMs on mobile devices and highlights the potential for future advancements in this area. The code, model, and data for AndroidGen are available on GitHub for further exploration and development. <div>
arXiv:2504.19298v1 Announce Type: new 
Abstract: Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese</title>
<link>https://arxiv.org/abs/2504.19314</link>
<guid>https://arxiv.org/abs/2504.19314</guid>
<content:encoded><![CDATA[
<div> Chinese web, language models, benchmark, BrowseComp-ZH, retrieval competence  
Summary:<br /><br />Language models are evolving into tool-using agents, necessitating the ability to browse the web in real-time to showcase their reasoning and retrieval abilities. Existing benchmarks like BrowseComp focus on English, neglecting the complexities of other major information ecosystems such as Chinese. To fill this gap, BrowseComp-ZH is introduced as a high-difficulty benchmark specifically tailored to evaluate language models on the Chinese web. With 289 challenging questions across diverse domains, each question is reverse-engineered from easily verifiable answers. Evaluation of over 20 state-of-the-art language models reveals their struggle on BrowseComp-ZH, with most models achieving low accuracy rates. The dataset, construction guidelines, and benchmark results have been made publicly available, shedding light on the considerable difficulty of the BrowseComp-ZH benchmark and the need for improved retrieval strategies, reasoning, and information reconciliation in current language models.  
Summary: <div>
arXiv:2504.19314v1 Announce Type: new 
Abstract: As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Multi-Task Learning &amp; Model Fusion for Efficient Language Model Guardrailing</title>
<link>https://arxiv.org/abs/2504.19333</link>
<guid>https://arxiv.org/abs/2504.19333</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, task-specific data generation, MultiTaskGuard, UniGuard, guardrail classifiers 

Summary:
In this work, the authors propose a novel approach to guardrailing against undesired behaviors using task-specific data generation. They introduce the \texttt{MultiTaskGuard} model, pretrained on a large synthetically generated dataset, to improve generalization. Additionally, they use a search-based model merging approach to create the most performant models, called \texttt{UniGuard}. These efficient guardrail classifiers outperform current state-of-the-art models by a significant margin on public datasets and guardrail benchmarks. The proposed approach shows an average F1 score improvement of 29.92 points over Aegis-LlamaGuard and 21.62 points over \texttt{gpt-4o}. The study also highlights the effectiveness of custom task-specific guardrail policies in enhancing the overall performance of guardrail classifiers. Overall, this work presents a promising solution to the challenges posed by large language models in terms of latency, memory consumption, and hosting expenses in guardrailing applications. 

<br /><br />Summary: <div>
arXiv:2504.19333v1 Announce Type: new 
Abstract: The trend towards large language models (LLMs) for guardrailing against undesired behaviors is increasing and has shown promise for censoring user inputs. However, increased latency, memory consumption, hosting expenses and non-structured outputs can make their use prohibitive.
  In this work, we show that task-specific data generation can lead to fine-tuned classifiers that significantly outperform current state of the art (SoTA) while being orders of magnitude smaller. Secondly, we show that using a single model, \texttt{MultiTaskGuard}, that is pretrained on a large synthetically generated dataset with unique task instructions further improves generalization. Thirdly, our most performant models, \texttt{UniGuard}, are found using our proposed search-based model merging approach that finds an optimal set of parameters to combine single-policy models and multi-policy guardrail models. %
On 7 public datasets and 4 guardrail benchmarks we created, our efficient guardrail classifiers improve over the best performing SoTA publicly available LLMs and 3$^{\text{rd}}$ party guardrail APIs in detecting unsafe and safe behaviors by an average F1 score improvement of \textbf{29.92} points over Aegis-LlamaGuard and \textbf{21.62} over \texttt{gpt-4o}, respectively. Lastly, our guardrail synthetic data generation process that uses custom task-specific guardrail poli
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explanatory Summarization with Discourse-Driven Planning</title>
<link>https://arxiv.org/abs/2504.19339</link>
<guid>https://arxiv.org/abs/2504.19339</guid>
<content:encoded><![CDATA[
<div> Keywords: lay summaries, automatic summarization, discourse frameworks, explanation modeling, plan-based approach

Summary:
Our paper introduces a plan-based approach for generating lay summaries of scientific documents, focusing on including explanations to help readers understand complex content. Unlike current methods, our approach leverages discourse frameworks to organize summary generation and guide explanatory sentences. We propose two discourse-driven planning strategies that outperform existing methods in terms of summary quality, robustness, controllability, and hallucination prevention. Empirical experiments on three lay summarization datasets validate the effectiveness of our approach. By conditioning the plan as part of the input or output prefix, our approach ensures that explanatory content is aligned with human-written summaries. This innovative method enhances the overall quality of lay summaries by providing a structured framework for generating informative and explanatory content. 

<br /><br />Summary: <div>
arXiv:2504.19339v1 Announce Type: new 
Abstract: Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers</title>
<link>https://arxiv.org/abs/2504.19395</link>
<guid>https://arxiv.org/abs/2504.19395</guid>
<content:encoded><![CDATA[
<div> substitution ciphers, in-context learning, task retrieval, task learning, bijective mapping
Summary:
- The study introduces a new approach called ICL CIPHERS which involves using substitution ciphers to transform in-context inputs.
- The ciphers make English sentences less comprehensible to humans but maintain a reversible pattern for machines to decipher.
- The research focuses on determining if large language models (LLMs) can effectively solve these ciphers with a bijective mapping, which requires decoding the latent cipher.
- Results show that LLMs perform better at solving ICL CIPHERS with bijective mappings compared to non-bijective baselines on multiple datasets and models.
- Analysis of LLMs' internal representations provides evidence of their ability to decode the ciphered inputs.
<br /><br />Summary: <div>
arXiv:2504.19395v1 Announce Type: new 
Abstract: Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (remember learned patterns from pre-training) and task learning (inference-time ``learning'' from demonstrations). However, disentangling these the two modes remains a challenging goal. We introduce ICL CIPHERS, a class of task reformulations based on substitution ciphers borrowed from classic cryptography. In this approach, a subset of tokens in the in-context inputs are substituted with other (irrelevant) tokens, rendering English sentences less comprehensible to human eye. However, by design, there is a latent, fixed pattern to this substitution, making it reversible. This bijective (reversible) cipher ensures that the task remains a well-defined task in some abstract sense, despite the transformations. It is a curious question if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires deciphering the latent cipher. We show that LLMs are better at solving ICL CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify ``learning'' in ICL. While this gap is small, it is consistent across the board on four datasets and six models. Finally, we examine LLMs' internal representations and identify evidence in their ability to decode the ciphered inputs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Selection and Rewriting for Video-based EducationalQuestion Generation</title>
<link>https://arxiv.org/abs/2504.19406</link>
<guid>https://arxiv.org/abs/2504.19406</guid>
<content:encoded><![CDATA[
<div> Keywords: Educational question generation, intelligent educational systems, lecture videos, large language models, dataset

Summary: 
The article discusses the importance of Educational Question Generation (EQG) in intelligent educational systems and the challenges faced when generating questions from lecture videos. Existing datasets for EQG lack real-world classroom content, leading to difficulties in accurately aligning questions with specific timestamps and target answers. To address these challenges, a novel framework is introduced that utilizes large language models to dynamically select and rewrite contexts from lecture transcripts and video keyframes based on answer relevance and temporal proximity. By integrating contexts from both modalities and rewriting them into answer-containing knowledge statements, the framework significantly improves the quality and relevance of generated questions. A dataset and code for this framework are released for further research and development. 

Summary: <br /><br />The article discusses the importance of Educational Question Generation in intelligent educational systems. Existing datasets lack real-world classroom content, leading to difficulties in aligning questions with specific timestamps and answers. A novel framework utilizing large language models dynamically selects and rewrites contexts from lecture transcripts and video keyframes to enhance question relevance. The framework significantly improves the quality of generated questions and a dataset and code are released for further research and development. <div>
arXiv:2504.19406v1 Announce Type: new 
Abstract: Educational question generation (EQG) is a crucial component of intelligent educational systems, significantly aiding self-assessment, active learning, and personalized education. While EQG systems have emerged, existing datasets typically rely on predefined, carefully edited texts, failing to represent real-world classroom content, including lecture speech with a set of complementary slides. To bridge this gap, we collect a dataset of educational questions based on lectures from real-world classrooms. On this realistic dataset, we find that current methods for EQG struggle with accurately generating questions from educational videos, particularly in aligning with specific timestamps and target answers. Common challenges include selecting informative contexts from extensive transcripts and ensuring generated questions meaningfully incorporate the target answer. To address the challenges, we introduce a novel framework utilizing large language models for dynamically selecting and rewriting contexts based on target timestamps and answers. First, our framework selects contexts from both lecture transcripts and video keyframes based on answer relevance and temporal proximity. Then, we integrate the contexts selected from both modalities and rewrite them into answer-containing knowledge statements, to enhance the logical connection between the contexts and the desired answer. This approach significantly improves the quality and relevance of the generated questions. Our dataset and code are released in https://github.com/mengxiayu/COSER.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</title>
<link>https://arxiv.org/abs/2504.19413</link>
<guid>https://arxiv.org/abs/2504.19413</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Mem0, Memory-centric architecture, Graph memory, Conversational coherence

Summary:
Mem0 introduces a memory-centric architecture to address challenges in maintaining consistency during prolonged multi-session dialogues in Large Language Models (LLMs). It dynamically extracts, consolidates, and retrieves salient information from ongoing conversations. An enhanced variant utilizes graph-based memory representations to capture complex relational structures among conversational elements. Comprehensive evaluations on the LOCOMO benchmark show that Mem0 outperforms existing memory systems across various question categories. It achieves a 26% relative improvement in the LLM-as-a-Judge metric over OpenAI and reduces computational overhead significantly compared to full-context methods. In particular, Mem0 demonstrates a 91% lower p95 latency and more than 90% token cost savings. These results highlight the importance of structured, persistent memory mechanisms for long-term conversational coherence in LLM-driven AI agents. 

<br /><br />Summary: <div>
arXiv:2504.19413v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models</title>
<link>https://arxiv.org/abs/2504.19436</link>
<guid>https://arxiv.org/abs/2504.19436</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic optimization, Retrieval-Augmented Generation, knowledge retrieval, semantic understanding, large language models

Summary: 
This paper introduces a state-aware dynamic knowledge retrieval mechanism for the Retrieval-Augmented Generation (RAG) architecture. The method enhances semantic understanding and knowledge scheduling efficiency in large language models used for open-domain question answering and complex generation tasks. By implementing a multi-level perceptive retrieval vector construction strategy and a differentiable document matching path, the approach allows for joint training and optimization of the retrieval and generation modules. Experiments on the Natural Questions dataset, using models such as GPT-4 and DeepSeek, show significant improvements in BLEU and ROUGE-L scores. The proposed structure also exhibits enhanced robustness and generation consistency, particularly in tasks involving semantic ambiguity and multi-document fusion. These results underscore the potential of the approach in developing high-quality language generation systems. 

<br /><br />Summary: <div>
arXiv:2504.19436v1 Announce Type: new 
Abstract: This paper focuses on the dynamic optimization of the Retrieval-Augmented Generation (RAG) architecture. It proposes a state-aware dynamic knowledge retrieval mechanism to enhance semantic understanding and knowledge scheduling efficiency in large language models for open-domain question answering and complex generation tasks. The method introduces a multi-level perceptive retrieval vector construction strategy and a differentiable document matching path. These components enable end-to-end joint training and collaborative optimization of the retrieval and generation modules. This effectively addresses the limitations of static RAG structures in context adaptation and knowledge access. Experiments are conducted on the Natural Questions dataset. The proposed structure is thoroughly evaluated across different large models, including GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments from multiple perspectives confirm the significant improvements in BLEU and ROUGE-L scores. The approach also demonstrates stronger robustness and generation consistency in tasks involving semantic ambiguity and multi-document fusion. These results highlight its broad application potential and practical value in building high-quality language generation systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks</title>
<link>https://arxiv.org/abs/2504.19445</link>
<guid>https://arxiv.org/abs/2504.19445</guid>
<content:encoded><![CDATA[
<div> response format, large language models, biases, judgments, decision tasks 

Summary:<br /><br />Large Language Models (LLMs) are being used more frequently in tasks like psychological text analysis and decision-making in automated workflows. Concerns about their reliability stem from potential biases inherited during training. This study explored how different response formats, binary versus continuous, impact LLM judgments. Results indicated a consistent negative bias, with LLMs more prone to delivering "negative" judgments in binary formats compared to continuous ones. Control experiments supported these findings across different tasks. The study underscores the significance of considering response format when utilizing LLMs for decision-making tasks, as slight modifications in task design can introduce systematic biases. <div>
arXiv:2504.19445v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used in tasks such as psychological text analysis and decision-making in automated workflows. However, their reliability remains a concern due to potential biases inherited from their training process. In this study, we examine how different response format: binary versus continuous, may systematically influence LLMs' judgments. In a value statement judgments task and a text sentiment analysis task, we prompted LLMs to simulate human responses and tested both formats across several models, including both open-source and commercial models. Our findings revealed a consistent negative bias: LLMs were more likely to deliver "negative" judgments in binary formats compared to continuous ones. Control experiments further revealed that this pattern holds across both tasks. Our results highlight the importance of considering response format when applying LLMs to decision tasks, as small changes in task design can introduce systematic biases.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Long Context Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.19457</link>
<guid>https://arxiv.org/abs/2504.19457</guid>
<content:encoded><![CDATA[
<div> hallucination detection, large language models, contextual understanding, long-context inputs, model architecture <br />
Summary: 
This study addresses the issue of contextual hallucination in Large Language Models (LLMs) by creating a dataset for long-context hallucination detection. A novel architecture is proposed to enhance pre-trained encoder models like BERT to process long contexts and identify contextual hallucinations through decomposition and aggregation. Experimental results demonstrate that the new architecture outperforms previous models and LLM-based models in terms of accuracy and speed of inference. This advancement is a significant step towards effectively detecting and addressing contextual hallucinations in LLMs, improving their reliability in generating accurate and contextually relevant information. <div>
arXiv:2504.19457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. However, they are prone to contextual hallucination, generating information that is either unsubstantiated or contradictory to the given context. Although many studies have investigated contextual hallucinations in LLMs, addressing them in long-context inputs remains an open problem. In this work, we take an initial step toward solving this problem by constructing a dataset specifically designed for long-context hallucination detection. Furthermore, we propose a novel architecture that enables pre-trained encoder models, such as BERT, to process long contexts and effectively detect contextual hallucinations through a decomposition and aggregation mechanism. Our experimental results show that the proposed architecture significantly outperforms previous models of similar size as well as LLM-based models across various metrics, while providing substantially faster inference.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text</title>
<link>https://arxiv.org/abs/2504.19467</link>
<guid>https://arxiv.org/abs/2504.19467</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, clinical data, natural language processing, performance variation

Summary:
- The article introduces BRIDGE, a new multilingual benchmark for evaluating large language models (LLMs) in clinical contexts using real-world electronic health record (EHR) data.
- 52 state-of-the-art LLMs were systematically evaluated across 87 tasks in nine languages, revealing significant performance differences based on model sizes, languages, tasks, and clinical specialties.
- Open-source LLMs were shown to perform comparably to proprietary models, while older medically fine-tuned LLMs lagged behind updated general-purpose models.
- The BRIDGE benchmark and associated leaderboard provide a foundational resource and reference point for the development and assessment of LLMs in real-world clinical text understanding.

<br /><br />Summary: <div>
arXiv:2504.19467v1 Announce Type: new 
Abstract: Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflicts in Texts: Data, Implications and Challenges</title>
<link>https://arxiv.org/abs/2504.19472</link>
<guid>https://arxiv.org/abs/2504.19472</guid>
<content:encoded><![CDATA[
<div> conflicting information, NLP models, natural texts, human-annotated data, model interactions

Summary:<br /><br />As NLP models are increasingly used in real-world applications, the presence of conflicting information poses challenges. The conflicts arise from various sources such as factual inconsistencies, annotator disagreements, and hallucinations in model outputs. This survey categorizes conflicts into three main areas: natural texts, human-annotated data, and model interactions. Addressing these conflicts is crucial to ensure the reliability and trustworthiness of NLP models. Prior work has addressed some conflicts in isolation, but this survey unifies them under the broader concept of conflicting information. It discusses the implications of conflicts and proposes mitigation strategies. Developing conflict-aware NLP systems that can reason over and reconcile conflicting information effectively poses key challenges for future research. <div>
arXiv:2504.19472v1 Announce Type: new 
Abstract: As NLP models become increasingly integrated into real-world applications, it becomes clear that there is a need to address the fact that models often rely on and generate conflicting information. Conflicts could reflect the complexity of situations, changes that need to be explained and dealt with, difficulties in data annotation, and mistakes in generated outputs. In all cases, disregarding the conflicts in data could result in undesired behaviors of models and undermine NLP models' reliability and trustworthiness. This survey categorizes these conflicts into three key areas: (1) natural texts on the web, where factual inconsistencies, subjective biases, and multiple perspectives introduce contradictions; (2) human-annotated data, where annotator disagreements, mistakes, and societal biases impact model training; and (3) model interactions, where hallucinations and knowledge conflicts emerge during deployment. While prior work has addressed some of these conflicts in isolation, we unify them under the broader concept of conflicting information, analyze their implications, and discuss mitigation strategies. We highlight key challenges and future directions for developing conflict-aware NLP systems that can reason over and reconcile conflicting information more effectively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment</title>
<link>https://arxiv.org/abs/2504.19556</link>
<guid>https://arxiv.org/abs/2504.19556</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AI-mediated communication, social media, sentiment analysis, language shifts<br />
Summary:<br />
This study investigates the influence of AI-mediated communication on social media language patterns by analyzing shifts in text complexity and sentiment over time. By comparing tweets mentioning Donald Trump during election periods from 2020 and 2024, the researchers found a significant increase in mean sentiment polarity and a shift towards more positive expressions. The analysis showed a decrease in neutral content and an increase in positive sentiment in tweets, indicating the impact of AI on language and emotional expression on social media. These findings suggest a growing presence of AI in shaping online communication and highlight the importance of understanding the evolving nature of language patterns influenced by large language models. <div>
arXiv:2504.19556v1 Announce Type: new 
Abstract: Given the subtle human-like effects of large language models on linguistic patterns, this study examines shifts in language over time to detect the impact of AI-mediated communication (AI- MC) on social media. We compare a replicated dataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the same period in 2024, all of which mention Donald Trump during election periods. Using a combination of Flesch-Kincaid readability and polarity scores, we analyze changes in text complexity and sentiment. Our findings reveal a significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more positive expressions (28.6% to 45.9%). These findings suggest not only an increasing presence of AI in social media communication but also its impact on language and emotional expression patterns.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</title>
<link>https://arxiv.org/abs/2504.19565</link>
<guid>https://arxiv.org/abs/2504.19565</guid>
<content:encoded><![CDATA[
<div> agents, scientific corpus, biomedical, language models, knowledge-driven

Summary:
The article introduces a knowledge-driven, multi-agent framework for distilling scientific corpora tailored for training large language models in the biomedical field. It proposes a collaborative architecture where specialized agents, guided by the MeSH hierarchy, autonomously extract high-quality data from scientific literature. These agents generate domain-specific question-answer pairs, ensuring coverage and consistency with biomedical ontologies while minimizing manual involvement. Experimental results show that language models trained on the distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming strong baselines. The AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2. Ablation studies and case analyses validate the effectiveness and synergy of each agent within the framework, demonstrating the potential of multi-agent collaboration in biomedical LLM training. 

<br /><br />Summary: <div>
arXiv:2504.19565v1 Announce Type: new 
Abstract: The rapid progress of large language models (LLMs) in biomedical research has underscored the limitations of existing open-source annotated scientific corpora, which are often insufficient in quantity and quality. Addressing the challenge posed by the complex hierarchy of biomedical knowledge, we propose a knowledge-driven, multi-agent framework for scientific corpus distillation tailored for LLM training in the biomedical domain. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. These agents collectively generate and refine domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arabic Metaphor Sentiment Classification Using Semantic Information</title>
<link>https://arxiv.org/abs/2504.19590</link>
<guid>https://arxiv.org/abs/2504.19590</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic Metaphor Corpus, sentiment classification, semantic tags, F-score, precision

Summary: 
This paper discusses the testing of the Arabic Metaphor Corpus (AMC) using newly developed automatic tools for sentiment classification based on semantic tags. The tools utilize semantic emotional tags to classify sentiment and evaluate their performance using standard metrics such as F-score, recall, and precision. The study aims to demonstrate the impact of Arabic online metaphors on sentiment through the use of these tools. This approach represents a novel method for conducting sentiment classification for Arabic metaphors by incorporating semantic tags to analyze the metaphor's influence. <div>
arXiv:2504.19590v1 Announce Type: new 
Abstract: In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1] using newly designed automatic tools for sentiment classification for AMC based on semantic tags. The tool incorporates semantic emotional tags for sentiment classification. I evaluate the tool using standard methods, which are F-score, recall, and precision. The method is to show the impact of Arabic online metaphors on sentiment through the newly designed tools. To the best of our knowledge, this is the first approach to conduct sentiment classification for Arabic metaphors using semantic tags to find the impact of the metaphor.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coreference Resolution for Vietnamese Narrative Texts</title>
<link>https://arxiv.org/abs/2504.19606</link>
<guid>https://arxiv.org/abs/2504.19606</guid>
<content:encoded><![CDATA[
<div> Coreference resolution, Vietnamese, annotated dataset, language models, GPT-4<br />
Summary:<br />
- Coreference resolution is crucial in NLP, especially challenging for Vietnamese due to limited resources.
- An annotated dataset was created using narratives from VnExpress with detailed guidelines and focus on consistency and accuracy.
- Evaluation of large language models (LLMs) GPT-3.5-Turbo and GPT-4 on the dataset showed GPT-4 outperforms in accuracy and response consistency.
- This highlights GPT-4 as a more reliable tool for coreference resolution in Vietnamese. <br /> <div>
arXiv:2504.19606v1 Announce Type: new 
Abstract: Coreference resolution is a vital task in natural language processing (NLP) that involves identifying and linking different expressions in a text that refer to the same entity. This task is particularly challenging for Vietnamese, a low-resource language with limited annotated datasets. To address these challenges, we developed a comprehensive annotated dataset using narrative texts from VnExpress, a widely-read Vietnamese online news platform. We established detailed guidelines for annotating entities, focusing on ensuring consistency and accuracy. Additionally, we evaluated the performance of large language models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset. Our results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in terms of both accuracy and response consistency, making it a more reliable tool for coreference resolution in Vietnamese.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19627</link>
<guid>https://arxiv.org/abs/2504.19627</guid>
<content:encoded><![CDATA[
<div> Framework, Visual Concepts, LVLMs, Self-supervised Learning, Image Understanding

Summary:
The article introduces VCM, a self-supervised visual concept modeling framework aimed at improving Large Vision-Language Models (LVLMs) efficiency and performance in real-world applications. LVLMs currently process entire images at the token level, lacking a visual concept model that mimics humans' ability to extract relevant visual concepts effortlessly. VCM addresses this by leveraging implicit contrastive learning and vision-language fine-tuning to construct a visual concept model without requiring concept-level annotations. Results show VCM reduces computational costs significantly while maintaining strong performance across diverse image understanding tasks. Additionally, VCM enhances visual encoders' capabilities in visual concept perception tasks. Extensive experiments validate the effectiveness and efficiency of VCM in improving LVLMs for various applications. <br /><br />Summary: <div>
arXiv:2504.19627v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient compared to humans who analyze information and generate content at the conceptual level, extracting relevant visual concepts with minimal effort. This inefficiency, stemming from the lack of a visual concept model, limits LVLMs' usability in real-world applications. To address this, we propose VCM, an end-to-end self-supervised visual concept modeling framework. VCM leverages implicit contrastive learning across multiple sampled instances and vision-language fine-tuning to construct a visual concept model without requiring costly concept-level annotations. Our results show that VCM significantly reduces computational costs (e.g., 85\% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance across diverse image understanding tasks. Moreover, VCM enhances visual encoders' capabilities in classic visual concept perception tasks. Extensive quantitative and qualitative experiments validate the effectiveness and efficiency of VCM.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks</title>
<link>https://arxiv.org/abs/2504.19645</link>
<guid>https://arxiv.org/abs/2504.19645</guid>
<content:encoded><![CDATA[
<div> POS tagging, Central-Kurdish language, NLP, resources, Universal Dependencies framework  
Summary:  
This study focuses on developing a comprehensive POS tagset for the Central-Kurdish language (CKL), which lacks resources for NLP tasks. The POS tagging task is essential for various NLP applications, but CKL faces challenges due to the lack of standardized tagsets. The proposed tagset, curated from existing studies and input from linguistic experts, aims to enhance the performance of Kurdish NLP tasks by providing accurate annotations for CKL corpus. By comparing with the Universal Dependencies framework for standard languages, the study demonstrates that the proposed tagset can improve sentence corrections and streamline NLP tasks for CKL. This research contributes to filling the gap in low-resourced language processing and lays the foundation for further advancements in Kurdish NLP. <br /><br />Summary: <div>
arXiv:2504.19645v1 Announce Type: new 
Abstract: - The field of natural language processing (NLP) has dramatically expanded within the last decade. Many human-being applications are conducted daily via NLP tasks, starting from machine translation, speech recognition, text generation and recommendations, Part-of-Speech tagging (POS), and Named-Entity Recognition (NER). However, low-resourced languages, such as the Central-Kurdish language (CKL), mainly remain unexamined due to shortage of necessary resources to support their development. The POS tagging task is the base of other NLP tasks; for example, the POS tag set has been used to standardized languages to provide the relationship between words among the sentences, followed by machine translation and text recommendation. Specifically, for the CKL, most of the utilized or provided POS tagsets are neither standardized nor comprehensive. To this end, this study presented an accurate and comprehensive POS tagset for the CKL to provide better performance of the Kurdish NLP tasks. The article also collected most of the POS tags from different studies as well as from Kurdish linguistic experts to standardized part-of-speech tags. The proposed POS tagset is designed to annotate a large CKL corpus and support Kurdish NLP tasks. The initial investigations of this study via comparison with the Universal Dependencies framework for standard languages, show that the proposed POS tagset can streamline or correct sentences more accurately for Kurdish NLP tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Conditioned Diffusive Time Series Forecasting</title>
<link>https://arxiv.org/abs/2504.19669</link>
<guid>https://arxiv.org/abs/2504.19669</guid>
<content:encoded><![CDATA[
arXiv:2504.19669v1 Announce Type: new 
Abstract: Diffusion models achieve remarkable success in processing images and text, and have been extended to special domains such as time series forecasting (TSF). Existing diffusion-based approaches for TSF primarily focus on modeling single-modality numerical sequences, overlooking the rich multimodal information in time series data. To effectively leverage such information for prediction, we propose a multimodal conditioned diffusion model for TSF, namely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for time series modeling, especially for forecasting. Specifically, Timestamps are combined with time series to establish temporal and semantic correlations among different data points when aggregating information along the temporal dimension. Texts serve as supplementary descriptions of time series' history, and adaptively aligned with data points as well as dynamically controlled in a classifier-free manner. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed MCD-TSF model achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs</title>
<link>https://arxiv.org/abs/2504.19675</link>
<guid>https://arxiv.org/abs/2504.19675</guid>
<content:encoded><![CDATA[
arXiv:2504.19675v1 Announce Type: new 
Abstract: This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects), which focussed on subject indexing using large language models (LLMs). The task required creating subject predictions for bibliographic records from the bilingual TIBKAT database using the GND subject vocabulary. Our approach combines traditional natural language processing and machine learning techniques implemented in the Annif toolkit with innovative LLM-based methods for translation and synthetic data generation, and merging predictions from monolingual models. The system ranked first in the all-subjects category and second in the tib-core-subjects category in the quantitative evaluation, and fourth in qualitative evaluations. These findings demonstrate the potential of combining traditional XMTC algorithms with modern LLM techniques to improve the accuracy and efficiency of subject indexing in multilingual contexts.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming the Titans: A Survey of Efficient LLM Inference Serving</title>
<link>https://arxiv.org/abs/2504.19720</link>
<guid>https://arxiv.org/abs/2504.19720</guid>
<content:encoded><![CDATA[
arXiv:2504.19720v1 Announce Type: new 
Abstract: Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding</title>
<link>https://arxiv.org/abs/2504.19734</link>
<guid>https://arxiv.org/abs/2504.19734</guid>
<content:encoded><![CDATA[
arXiv:2504.19734v1 Announce Type: new 
Abstract: Dialogue data has been a key source for understanding learning processes, offering critical insights into how students engage in collaborative discussions and how these interactions shape their knowledge construction. The advent of Large Language Models (LLMs) has introduced promising opportunities for advancing qualitative research, particularly in the automated coding of dialogue data. However, the inherent contextual complexity of dialogue presents unique challenges for these models, especially in understanding and interpreting complex contextual information. This study addresses these challenges by developing a novel LLM-assisted automated coding approach for dialogue data. The novelty of our proposed framework is threefold: 1) We predict the code for an utterance based on dialogue-specific characteristics -- communicative acts and communicative events -- using separate prompts following the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We leveraged the interrelation between events and acts to implement consistency checking using GPT-4o. In particular, our contextual consistency checking provided a substantial accuracy improvement. We also found the accuracy of act predictions was consistently higher than that of event predictions. This study contributes a new methodological framework for enhancing the precision of automated coding of dialogue data as well as offers a scalable solution for addressing the contextual challenges inherent in dialogue analysis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs</title>
<link>https://arxiv.org/abs/2504.19759</link>
<guid>https://arxiv.org/abs/2504.19759</guid>
<content:encoded><![CDATA[
arXiv:2504.19759v1 Announce Type: new 
Abstract: In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB) to evaluate the moral reasoning abilities of large language models (LLMs) across five typologically diverse languages and three levels of contextual complexity: sentence, paragraph, and document. Our results show moral reasoning performance degrades with increasing context complexity, particularly for low-resource languages such as Vietnamese. We further fine-tune the open-source LLaMA-3-8B model using curated monolingual data for alignment and poisoning. Surprisingly, low-resource languages have a stronger impact on multilingual reasoning than high-resource ones, highlighting their critical role in multilingual NLP.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance</title>
<link>https://arxiv.org/abs/2504.19811</link>
<guid>https://arxiv.org/abs/2504.19811</guid>
<content:encoded><![CDATA[
arXiv:2504.19811v1 Announce Type: new 
Abstract: Accurately forecasting the performance of Large Language Models (LLMs) before extensive fine-tuning or merging can substantially reduce both computational expense and development time. Although prior approaches like scaling laws account for global factors such as parameter size or training tokens, they often overlook explicit lineage relationships - i.e., which models are derived or merged from which parents. In this work, we propose a novel Lineage-Regularized Matrix Factorization (LRMF) framework that encodes ancestral ties among LLMs via a graph Laplacian regularizer. By leveraging multi-hop parent-child connections, LRMF consistently outperforms conventional matrix factorization and collaborative filtering methods in both instance-level and benchmark-level performance prediction. Our large-scale study includes 2,934 publicly available Hugging Face models and 21,000+ instances across 6 major benchmarks, showing that lineage constraints yield up to 7-10 percentage points higher correlation with actual performance compared to baselines. Moreover, LRMF effectively addresses the cold-start problem, providing accurate estimates for newly derived or merged models even with minimal data. This lineage-guided strategy thus offers a resource-efficient way to inform hyperparameter tuning, data selection, and model combination in modern LLM development.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels</title>
<link>https://arxiv.org/abs/2504.19850</link>
<guid>https://arxiv.org/abs/2504.19850</guid>
<content:encoded><![CDATA[
arXiv:2504.19850v1 Announce Type: new 
Abstract: This article presents the results of a pilot study involving the reception of a fictional short story translated from English into Dutch under four conditions: machine translation (MT), post-editing (PE), human translation (HT) and original source text (ST). The aim is to understand how creativity and errors in different translation modalities affect readers, specifically regarding cognitive load. Eight participants filled in a questionnaire, read a story using an eye-tracker, and conducted a retrospective think-aloud (RTA) interview. The results show that units of creative potential (UCP) increase cognitive load and that this effect is highest for HT and lowest for MT; no effect of error was observed. Triangulating the data with RTAs leads us to hypothesize that the higher cognitive load in UCPs is linked to increases in reader enjoyment and immersion. The effect of translation creativity on cognitive load in different translation modalities at word-level is novel and opens up new avenues for further research. All the code and data are available at https://github.com/INCREC/Pilot_to_MT_or_not_to_MT
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language</title>
<link>https://arxiv.org/abs/2504.19856</link>
<guid>https://arxiv.org/abs/2504.19856</guid>
<content:encoded><![CDATA[
arXiv:2504.19856v1 Announce Type: new 
Abstract: Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique that further trains a language model (LM) on its pretraining task, e.g., language masking. Although popular, it requires a significant corpus of domain-related data, which is difficult to obtain for specific domains in languages other than English, such as the process industry in the German language. This paper introduces an efficient approach called ICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related and in-domain texts, significantly reducing GPU time while maintaining strong model performance. Our results show that this approach performs better than traditional DAPT by 3.5 of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times less computing time, providing a cost-effective solution for industries with limited computational capacity. The findings highlight the broader applicability of this framework to other low-resource industries, making NLP-based solutions more accessible and feasible in production environments.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage</title>
<link>https://arxiv.org/abs/2504.19867</link>
<guid>https://arxiv.org/abs/2504.19867</guid>
<content:encoded><![CDATA[
arXiv:2504.19867v1 Announce Type: new 
Abstract: Existing large language model (LLM) serving systems fall into two categories: 1) a unified system where prefill phase and decode phase are co-located on the same GPU, sharing the unified computational resource and storage, and 2) a disaggregated system where the two phases are disaggregated to different GPUs. The design of the disaggregated system addresses the latency interference and sophisticated scheduling issues in the unified system but leads to storage challenges including 1) replicated weights for both phases that prevent flexible deployment, 2) KV cache transfer overhead between the two phases, 3) storage imbalance that causes substantial wasted space of the GPU capacity, and 4) suboptimal resource adjustment arising from the difficulties in migrating KV cache. Such storage inefficiency delivers poor serving performance under high request rates.
  In this paper, we identify that the advantage of the disaggregated system lies in the disaggregated computation, i.e., partitioning the computational resource to enable the asynchronous computation of two phases. Thus, we propose a novel LLM serving system, semi-PD, characterized by disaggregated computation and unified storage. In semi-PD, we introduce a computation resource controller to achieve disaggregated computation at the streaming multi-processor (SM) level, and a unified memory manager to manage the asynchronous memory access from both phases. semi-PD has a low-overhead resource adjustment mechanism between the two phases, and a service-level objective (SLO) aware dynamic partitioning algorithm to optimize the SLO attainment. Compared to state-of-the-art systems, semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets</title>
<link>https://arxiv.org/abs/2504.19898</link>
<guid>https://arxiv.org/abs/2504.19898</guid>
<content:encoded><![CDATA[
arXiv:2504.19898v1 Announce Type: new 
Abstract: As a fundamental task in machine learning, text classification plays a crucial role in many areas. With the rapid scaling of Large Language Models (LLMs), particularly through reinforcement learning (RL), there is a growing need for more capable discriminators. Consequently, advances in classification are becoming increasingly vital for enhancing the overall capabilities of LLMs. Traditional discriminative methods map text to labels but overlook LLMs' intrinsic generative strengths. Generative classification addresses this by prompting the model to directly output labels. However, existing studies still rely on simple SFT alone, seldom probing the interplay between training and inference prompts, and no work has systematically leveraged RL for generative text classifiers and unified SFT, RL, and inference-time prompting in one framework. We bridge this gap with GenCLS++, a framework that jointly optimizes SFT and RL while systematically exploring five high-level strategy dimensions-in-context learning variants, category definitions, explicit uncertainty labels, semantically irrelevant numeric labels, and perplexity-based decoding-during both training and inference. After an SFT "policy warm-up," we apply RL with a simple rule-based reward, yielding sizable extra gains. Across seven datasets, GenCLS++ achieves an average accuracy improvement of 3.46% relative to the naive SFT baseline; on public datasets, this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that benefit from explicit thinking processes, we find that classification tasks perform better without such reasoning steps. These insights into the role of explicit reasoning provide valuable guidance for future LLM applications.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking</title>
<link>https://arxiv.org/abs/2504.19940</link>
<guid>https://arxiv.org/abs/2504.19940</guid>
<content:encoded><![CDATA[
arXiv:2504.19940v1 Announce Type: new 
Abstract: The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds. Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments.
  Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons</title>
<link>https://arxiv.org/abs/2504.19982</link>
<guid>https://arxiv.org/abs/2504.19982</guid>
<content:encoded><![CDATA[
arXiv:2504.19982v1 Announce Type: new 
Abstract: Task-oriented dialogue (TOD) systems are experiencing a revolution driven by Large Language Models (LLMs), yet the evaluation methodologies for these systems remain insufficient for their growing sophistication. While traditional automatic metrics effectively assessed earlier modular systems, they focus solely on the dialogue level and cannot detect critical intermediate errors that can arise during user-agent interactions. In this paper, we introduce TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework that unifies fine-grained turn-level analysis with holistic dialogue-level comparisons. At turn level, we evaluate each response along three TOD-specific dimensions: conversation cohesion, backend knowledge consistency, and policy compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons to provide a measure of dialogue-level quality. Through experiments on MultiWOZ 2.4 and {\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the conversational errors that conventional metrics miss. Furthermore, TD-EVAL exhibits better alignment with human judgments than traditional and LLM-based metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for TOD system evaluation, efficiently assessing both turn and system levels with a plug-and-play framework for future research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom</title>
<link>https://arxiv.org/abs/2504.20000</link>
<guid>https://arxiv.org/abs/2504.20000</guid>
<content:encoded><![CDATA[
arXiv:2504.20000v1 Announce Type: new 
Abstract: Knowledge Distillation (KD) is one of the approaches to reduce the size of Large Language Models (LLMs). A LLM with smaller number of model parameters (student) is trained to mimic the performance of a LLM of a larger size (teacher model) on a specific task. For domain-specific tasks, it is not clear if teacher or student model, or both, must be considered for domain adaptation. In this work, we study this problem from perspective of telecom domain Question-Answering (QA) task. We systematically experiment with Supervised Fine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to KD. We design experiments to study the impact of vocabulary (same and different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the distilled model. Multi-faceted evaluation of the distillation using 14 different metrics (N-gram, embedding and LLM-based metrics) is considered. Experimental results show that SFT of teacher improves performance of distilled model when both models have same vocabulary, irrespective of algorithm and metrics. Overall, SFT of both teacher and student results in better performance across all metrics, although the statistical significance of the same depends on the vocabulary of the teacher models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation</title>
<link>https://arxiv.org/abs/2504.20013</link>
<guid>https://arxiv.org/abs/2504.20013</guid>
<content:encoded><![CDATA[
arXiv:2504.20013v1 Announce Type: new 
Abstract: Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages</title>
<link>https://arxiv.org/abs/2504.20022</link>
<guid>https://arxiv.org/abs/2504.20022</guid>
<content:encoded><![CDATA[
arXiv:2504.20022v1 Announce Type: new 
Abstract: Multilingual Large Language Models (LLMs) have demonstrated significant effectiveness across various languages, particularly in high-resource languages such as English. However, their performance in terms of factual accuracy across other low-resource languages, especially Indic languages, remains an area of investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o, Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in English and Indic languages using the IndicQuest dataset, which contains question-answer pairs in English and 19 Indic languages. By asking the same questions in English and their respective Indic translations, we analyze whether the models are more reliable for regional context questions in Indic languages or when operating in English. Our findings reveal that LLMs often perform better in English, even for questions rooted in Indic contexts. Notably, we observe a higher tendency for hallucination in responses generated in low-resource Indic languages, highlighting challenges in the multilingual understanding capabilities of current LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoJudge: Judge Decoding Without Manual Annotation</title>
<link>https://arxiv.org/abs/2504.20039</link>
<guid>https://arxiv.org/abs/2504.20039</guid>
<content:encoded><![CDATA[
arXiv:2504.20039v1 Announce Type: new 
Abstract: We introduce AutoJudge, a framework that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. Instead of matching the original model output distribution token-by-token, we identify which of the generated tokens affect the downstream quality of the generated response, relaxing the guarantee so that the "unimportant" tokens can be generated faster. Our approach relies on a semi-greedy search algorithm to test which of the mismatches between target and draft model should be corrected to preserve quality, and which ones may be skipped. We then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B (target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more accepted tokens per verification cycle with under 1% degradation in answer accuracy compared to standard speculative decoding and over 2x with small loss in accuracy. When applied to the LiveCodeBench benchmark, our approach automatically detects other, programming-specific important tokens and shows similar speedups, demonstrating its ability to generalize across tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines</title>
<link>https://arxiv.org/abs/2504.18596</link>
<guid>https://arxiv.org/abs/2504.18596</guid>
<content:encoded><![CDATA[
arXiv:2504.18596v1 Announce Type: cross 
Abstract: This paper explores the strategic use of modern synthetic data generation and advanced data perturbation techniques to enhance security, maintain analytical utility, and improve operational efficiency when managing large datasets, with a particular focus on the Banking, Financial Services, and Insurance (BFSI) sector. We contrast these advanced methods encompassing generative models like GANs, sophisticated context-aware PII transformation, configurable statistical perturbation, and differential privacy with traditional anonymization approaches.
  The goal is to create realistic, privacy-preserving datasets that retain high utility for complex machine learning tasks and analytics, a critical need in the data-sensitive industries like BFSI, Healthcare, Retail, and Telecommunications. We discuss how these modern techniques potentially offer significant improvements in balancing privacy preservation while maintaining data utility compared to older methods. Furthermore, we examine the potential for operational gains, such as reduced overhead and accelerated analytics, by using these privacy-enhanced datasets. We also explore key use cases where these methods can mitigate regulatory risks and enable scalable, data-driven innovation without compromising sensitive customer information.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Product Recommendations for Implicit Superlative Queries</title>
<link>https://arxiv.org/abs/2504.18748</link>
<guid>https://arxiv.org/abs/2504.18748</guid>
<content:encoded><![CDATA[
arXiv:2504.18748v1 Announce Type: cross 
Abstract: In Recommender Systems, users often seek the best products through indirect, vague, or under-specified queries, such as "best shoes for trail running". Such queries, also referred to as implicit superlative queries, pose a significant challenge for standard retrieval and ranking systems as they lack an explicit mention of attributes and require identifying and reasoning over complex factors. We investigate how Large Language Models (LLMs) can generate implicit attributes for ranking as well as reason over them to improve product recommendations for such queries. As a first step, we propose a novel four-point schema for annotating the best product candidates for superlative queries called SUPERB, paired with LLM-based product annotations. We then empirically evaluate several existing retrieval and ranking approaches on our new dataset, providing insights and discussing their integration into real-world e-commerce production systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical knowledge in LLMs does not translate to human interactions</title>
<link>https://arxiv.org/abs/2504.18919</link>
<guid>https://arxiv.org/abs/2504.18919</guid>
<content:encoded><![CDATA[
arXiv:2504.18919v1 Announce Type: cross 
Abstract: Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings</title>
<link>https://arxiv.org/abs/2504.18988</link>
<guid>https://arxiv.org/abs/2504.18988</guid>
<content:encoded><![CDATA[
arXiv:2504.18988v1 Announce Type: cross 
Abstract: Collaborative research often includes contributors with varied perspectives from diverse linguistic backgrounds. However, English as a Second Language (ESL) researchers often struggle to communicate during meetings in English and comprehend discussions, leading to limited contribution. To investigate these challenges, we surveyed 64 ESL researchers who frequently collaborate in multilingual teams and identified four key design goals around participation, comprehension, documentation, and feedback. Guided by these design goals, we developed LINC, a multimodal Language INdependent Collaboration system with two components: a real-time module for multilingual communication during meetings and a post-meeting dashboard for discussion analysis. We evaluated the system through a two-phased study with six triads of multilingual teams. We found that using LINC, participants benefited from communicating in their preferred language, recalled and reviewed actionable insights, and prepared for upcoming meetings effectively. We discuss external factors that impact multilingual meeting participation beyond language preferences and the implications of multimodal systems in facilitating meetings in hybrid multilingual collaborative settings beyond research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2504.19056</link>
<guid>https://arxiv.org/abs/2504.19056</guid>
<content:encoded><![CDATA[
arXiv:2504.19056v1 Announce Type: cross 
Abstract: Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Framework for Song Generation with Prompt-based Control</title>
<link>https://arxiv.org/abs/2504.19062</link>
<guid>https://arxiv.org/abs/2504.19062</guid>
<content:encoded><![CDATA[
arXiv:2504.19062v1 Announce Type: cross 
Abstract: Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://VersBand.github.io.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Attention Generates Better Proofs</title>
<link>https://arxiv.org/abs/2504.19188</link>
<guid>https://arxiv.org/abs/2504.19188</guid>
<content:encoded><![CDATA[
arXiv:2504.19188v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical Attention}, a regularization method that aligns LLMs' attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively. The code is available at https://github.com/Car-pe/HAGBP.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anyprefer: An Agentic Framework for Preference Data Synthesis</title>
<link>https://arxiv.org/abs/2504.19276</link>
<guid>https://arxiv.org/abs/2504.19276</guid>
<content:encoded><![CDATA[
arXiv:2504.19276v1 Announce Type: cross 
Abstract: High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model's responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks</title>
<link>https://arxiv.org/abs/2504.19444</link>
<guid>https://arxiv.org/abs/2504.19444</guid>
<content:encoded><![CDATA[
arXiv:2504.19444v1 Announce Type: cross 
Abstract: Pre-trained code models rely heavily on high-quality pre-training data, particularly human-written reference comments that bridge code and natural language. However, these comments often become outdated as software evolves, degrading model performance. Large language models (LLMs) excel at generating high-quality code comments. We investigate whether replacing human-written comments with LLM-generated ones improves pre-training datasets. Since standard metrics cannot assess reference comment quality, we propose two novel reference-free evaluation tasks: code-comment inconsistency detection and semantic code search. Results show that LLM-generated comments are more semantically consistent with code than human-written ones, as confirmed by manual evaluation. Leveraging this finding, we rebuild the CodeSearchNet dataset with LLM-generated comments and re-pre-train CodeT5. Evaluations demonstrate that models trained on LLM-enhanced data outperform those using original human comments in code summarization, generation, and translation tasks. This work validates rebuilding pre-training datasets with LLMs to advance code intelligence, challenging the traditional reliance on human reference comments.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective</title>
<link>https://arxiv.org/abs/2504.19458</link>
<guid>https://arxiv.org/abs/2504.19458</guid>
<content:encoded><![CDATA[
arXiv:2504.19458v1 Announce Type: cross 
Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from different Multi-Modal Knowledge Graphs (MMKGs), a critical information retrieval task. Existing studies have explored various fusion paradigms and consistency constraints to improve the alignment of equivalent entities, while overlooking that the visual modality may not always contribute positively. Empirically, entities with low-similarity images usually generate unsatisfactory performance, highlighting the limitation of overly relying on visual features. We believe the model can be biased toward the visual modality, leading to a shortcut image-matching task. To address this, we propose a counterfactual debiasing framework for MMEA, termed CDMEA, which investigates visual modality bias from a causal perspective. Our approach aims to leverage both visual and graph modalities to enhance MMEA while suppressing the direct causal effect of the visual modality on model predictions. By estimating the Total Effect (TE) of both modalities and excluding the Natural Direct Effect (NDE) of the visual modality, we ensure that the model predicts based on the Total Indirect Effect (TIE), effectively utilizing both modalities and reducing visual modality bias. Extensive experiments on 9 benchmark datasets show that CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource data scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Reasoning Performance in Large Language Models via Representation Engineering</title>
<link>https://arxiv.org/abs/2504.19483</link>
<guid>https://arxiv.org/abs/2504.19483</guid>
<content:encoded><![CDATA[
arXiv:2504.19483v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. Whether reasoning in LLMs should be understood to be inherently different is, however, widely debated. We propose utilizing a representation engineering approach wherein model activations are read from the residual stream of an LLM when processing a reasoning task. The activations are used to derive a control vector that is applied to the model as an inference-time intervention, modulating the representational space of the model, to improve performance on the specified task. We publish the code for deriving control vectors and analyzing model representations. The method allows us to improve performance on reasoning benchmarks and assess how control vectors influence the final logit distribution of a model via metrics such as KL divergence and entropy. We apply control vectors to Mistral-7B-Instruct and a range of Pythia models on an inductive, a deductive and mathematical reasoning task. We show that an LLM can, to a certain degree, be controlled to improve its perceived reasoning ability by modulating activations. The intervention is dependent upon the ability to reliably extract the model's typical state when correctly solving a task. Our results suggest that reasoning performance can be modulated in the same manner as other information-processing tasks performed by LLMs and demonstrate that we are capable of improving performance on specific tasks via a simple intervention on the residual stream with no additional training.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.19500</link>
<guid>https://arxiv.org/abs/2504.19500</guid>
<content:encoded><![CDATA[
arXiv:2504.19500v1 Announce Type: cross 
Abstract: Open-vocabulary 3D scene understanding is pivotal for enhancing physical intelligence, as it enables embodied agents to interpret and interact dynamically within real-world environments. This paper introduces MPEC, a novel Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic segmentation that leverages both 3D entity-language alignment and point-entity consistency across different point cloud views to foster entity-specific feature representations. Our method improves semantic discrimination and enhances the differentiation of unique instances, achieving state-of-the-art results on ScanNet for open-vocabulary 3D semantic segmentation and demonstrating superior zero-shot scene understanding capabilities. Extensive fine-tuning experiments on 8 datasets, spanning from low-level perception to high-level reasoning tasks, showcase the potential of learned 3D features, driving consistent performance gains across varied 3D scene understanding tasks. Project website: https://mpec-3d.github.io/
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation</title>
<link>https://arxiv.org/abs/2504.19519</link>
<guid>https://arxiv.org/abs/2504.19519</guid>
<content:encoded><![CDATA[
arXiv:2504.19519v1 Announce Type: cross 
Abstract: Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features.
  To address the issue, we propose FlashOverlap, a lightweight design characterized by tile-wise overlapping, interference-free computation, and communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to identify tile-wise data dependency without interrupting the computation process, and reorders data to contiguous addresses, enabling communication by simply calling NCCL APIs. Experiments show that such a lightweight design achieves up to 1.65x speedup, outperforming existing works in most cases.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19583</link>
<guid>https://arxiv.org/abs/2504.19583</guid>
<content:encoded><![CDATA[
arXiv:2504.19583v1 Announce Type: cross 
Abstract: This paper proposes a parameter collaborative optimization algorithm for large language models, enhanced with graph spectral analysis. The goal is to improve both fine-tuning efficiency and structural awareness during training. In the proposed method, the parameters of a pre-trained language model are treated as nodes in a graph. A weighted graph is constructed, and Laplacian spectral decomposition is applied to enable frequency-domain modeling and structural representation of the parameter space. Based on this structure, a joint loss function is designed. It combines the task loss with a spectral regularization term to facilitate collaborative updates among parameters. In addition, a spectral filtering mechanism is introduced during the optimization phase. This mechanism adjusts gradients in a structure-aware manner, enhancing the model's training stability and convergence behavior. The method is evaluated on multiple tasks, including traditional fine-tuning comparisons, few-shot generalization tests, and convergence speed analysis. In all settings, the proposed approach demonstrates superior performance. The experimental results confirm that the spectral collaborative optimization framework effectively reduces parameter perturbations and improves fine-tuning quality while preserving overall model performance. This work contributes significantly to the field of artificial intelligence by advancing parameter-efficient training methodologies for large-scale models, reinforcing the importance of structural signal processing in deep learning optimization, and offering a robust, generalizable framework for enhancing language model adaptability and performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2504.19730</link>
<guid>https://arxiv.org/abs/2504.19730</guid>
<content:encoded><![CDATA[
arXiv:2504.19730v1 Announce Type: cross 
Abstract: The widespread adoption of code language models in software engineering tasks has exposed vulnerabilities to adversarial attacks, especially the identifier substitution attacks. Although existing identifier substitution attackers demonstrate high success rates, they often produce adversarial examples with unnatural code patterns. In this paper, we systematically assess the quality of adversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80% of adversarial examples generated by state-of-the-art identifier substitution attackers (e.g., ALERT) are actually detectable. Based on this insight, we propose EP-Shield, a unified framework for evaluating and purifying identifier substitution attacks via naturalness-aware reasoning. Specifically, we first evaluate the naturalness of code and identify the perturbed adversarial code, then purify it so that the victim model can restore correct prediction. Extensive experiments demonstrate the superiority of EP-Shield over adversarial fine-tuning (up to 83.36% improvement) and its lightweight design 7B parameters) with GPT-4-level performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.19754</link>
<guid>https://arxiv.org/abs/2504.19754</guid>
<content:encoded><![CDATA[
arXiv:2504.19754v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be managed effectively within the input constraints of LLMs? Traditional methods address this by chunking external documents into smaller, fixed-size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished coherence in generation. To overcome these shortcomings, two advanced techniques, late chunking and contextual retrieval, have been introduced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coherence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice relevance and completeness.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian approach to modeling topic-metadata relationships</title>
<link>https://arxiv.org/abs/2104.02496</link>
<guid>https://arxiv.org/abs/2104.02496</guid>
<content:encoded><![CDATA[
arXiv:2104.02496v2 Announce Type: replace 
Abstract: The objective of advanced topic modeling is not only to explore latent topical structures, but also to estimate relationships between the discovered topics and theoretically relevant metadata. Methods used to estimate such relationships must take into account that the topical structure is not directly observed, but instead being estimated itself in an unsupervised fashion, usually by common topic models. A frequently used procedure to achieve this is the method of composition, a Monte Carlo sampling technique performing multiple repeated linear regressions of sampled topic proportions on metadata covariates. In this paper, we propose two modifications of this approach: First, we substantially refine the existing implementation of the method of composition from the R package stm by replacing linear regression with the more appropriate Beta regression. Second, we provide a fundamental enhancement of the entire estimation framework by substituting the current blending of frequentist and Bayesian methods with a fully Bayesian approach. This allows for a more appropriate quantification of uncertainty. We illustrate our improved methodology by investigating relationships between Twitter posts by German parliamentarians and different metadata covariates related to their electoral districts, using the Structural Topic Model to estimate topic proportions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information</title>
<link>https://arxiv.org/abs/2110.08420</link>
<guid>https://arxiv.org/abs/2110.08420</guid>
<content:encoded><![CDATA[
arXiv:2110.08420v3 Announce Type: replace 
Abstract: Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model $\mathcal{V}$ -- as the lack of $\mathcal{V}$-$\textit{usable information}$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\mathcal{V}$. We further introduce $\textit{pointwise $\mathcal{V}$-information}$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\mathcal{V}$-$\textit{usable information}$ and PVI also permit the converse: for a given model $\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach</title>
<link>https://arxiv.org/abs/2112.06876</link>
<guid>https://arxiv.org/abs/2112.06876</guid>
<content:encoded><![CDATA[
arXiv:2112.06876v3 Announce Type: replace 
Abstract: In recent years, the field of NLP has seen growing interest in modeling both semantic and pragmatic dimensions. Despite this progress, two key challenges persist: firstly, the complex task of mapping and analyzing the interactions between semantic and pragmatic features; secondly, the insufficient incorporation of relevant insights from related disciplines outside NLP. Addressing these issues, this study introduces a novel geometric metric that utilizes word co-occurrence patterns. This metric maps two fundamental properties - semantic typicality (cognitive) and pragmatic salience (socio-cultural) - for basic-level categories within a two-dimensional hyperbolic space. Our evaluations reveal that this semantic-pragmatic metric produces mappings for basic-level categories that not only surpass traditional cognitive semantics benchmarks but also demonstrate significant socio-cultural relevance. This finding proposes that basic-level categories, traditionally viewed as semantics-driven cognitive constructs, should be examined through the lens of both semantic and pragmatic dimensions, highlighting their role as a cognitive-cultural interface. The broad contribution of this paper lies in the development of medium-sized, interpretable, and human-centric language embedding models, which can effectively blend semantic and pragmatic dimensions to elucidate both the cognitive and socio-cultural significance of linguistic categories.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Meta-Learning for Zero-Shot Relation Triplet Extraction</title>
<link>https://arxiv.org/abs/2305.01920</link>
<guid>https://arxiv.org/abs/2305.01920</guid>
<content:encoded><![CDATA[
arXiv:2305.01920v2 Announce Type: replace 
Abstract: Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation triplets from texts containing unseen relation types. This capability benefits various downstream information retrieval (IR) tasks. The primary challenge lies in enabling models to generalize effectively to unseen relation categories. Existing approaches typically leverage the knowledge embedded in pre-trained language models to accomplish the generalization process. However, these methods focus solely on fitting the training data during training, without specifically improving the model's generalization performance, resulting in limited generalization capability. For this reason, we explore the integration of bi-level optimization (BLO) with pre-trained language models for learning generalized knowledge directly from the training data, and propose a generative meta-learning framework which exploits the `learning-to-learn' ability of meta-learning to boost the generalization capability of generative models.
  Specifically, we introduce a BLO approach that simultaneously addresses data fitting and generalization. This is achieved by constructing an upper-level loss to focus on generalization and a lower-level loss to ensure accurate data fitting. Building on this, we subsequently develop three generative meta-learning methods, each tailored to a distinct category of meta-learning. Extensive experimental results demonstrate that our framework performs well on the ZeroRTE task. Our code is available at https://github.com/leeworry/TGM-MetaLearning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking large language models for biomedical natural language processing applications and recommendations</title>
<link>https://arxiv.org/abs/2305.16326</link>
<guid>https://arxiv.org/abs/2305.16326</guid>
<content:encoded><![CDATA[
arXiv:2305.16326v5 Announce Type: replace 
Abstract: The rapid growth of biomedical literature poses challenges for manual knowledge curation and synthesis. Biomedical Natural Language Processing (BioNLP) automates the process. While Large Language Models (LLMs) have shown promise in general domains, their effectiveness in BioNLP tasks remains unclear due to limited benchmarks and practical guidelines.
  We perform a systematic evaluation of four LLMs, GPT and LLaMA representatives on 12 BioNLP benchmarks across six applications. We compare their zero-shot, few-shot, and fine-tuning performance with traditional fine-tuning of BERT or BART models. We examine inconsistencies, missing information, hallucinations, and perform cost analysis. Here we show that traditional fine-tuning outperforms zero or few shot LLMs in most tasks. However, closed-source LLMs like GPT-4 excel in reasoning-related tasks such as medical question answering. Open source LLMs still require fine-tuning to close performance gaps. We find issues like missing information and hallucinations in LLM outputs. These results offer practical insights for applying LLMs in BioNLP.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ragas: Automated Evaluation of Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2309.15217</link>
<guid>https://arxiv.org/abs/2309.15217</guid>
<content:encoded><![CDATA[
arXiv:2309.15217v2 Announce Type: replace 
Abstract: We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models for newspaper sentiment analysis during COVID-19: The Guardian</title>
<link>https://arxiv.org/abs/2405.13056</link>
<guid>https://arxiv.org/abs/2405.13056</guid>
<content:encoded><![CDATA[
arXiv:2405.13056v2 Announce Type: replace 
Abstract: During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake News Detection: It's All in the Data!</title>
<link>https://arxiv.org/abs/2407.02122</link>
<guid>https://arxiv.org/abs/2407.02122</guid>
<content:encoded><![CDATA[
arXiv:2407.02122v2 Announce Type: replace 
Abstract: This comprehensive survey serves as an indispensable resource for researchers embarking on the journey of fake news detection. By highlighting the pivotal role of dataset quality and diversity, it underscores the significance of these elements in the effectiveness and robustness of detection models. The survey meticulously outlines the key features of datasets, various labeling systems employed, and prevalent biases that can impact model performance. Additionally, it addresses critical ethical issues and best practices, offering a thorough overview of the current state of available datasets. Our contribution to this field is further enriched by the provision of GitHub repository, which consolidates publicly accessible datasets into a single, user-friendly portal. This repository is designed to facilitate and stimulate further research and development efforts aimed at combating the pervasive issue of fake news.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pula: Training Large Language Models for Setswana</title>
<link>https://arxiv.org/abs/2408.02239</link>
<guid>https://arxiv.org/abs/2408.02239</guid>
<content:encoded><![CDATA[
arXiv:2408.02239v2 Announce Type: replace 
Abstract: In this work we present Pula, a suite of bilingual language models proficient in both Setswana and English. Leveraging recent advancements in data availability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation tasks and achieve state-of-the-art performance on Setswana reasoning tasks for their size. We release the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and training and evaluation code. Alongside Pula, we release the largest-ever Setswana text corpus, Marothodi, and the first comprehensive Setswana instruction-tuning dataset, Medupi, consisting of reformatted datasets, translated corpora, and synthetic LLM-generated text. To accompany this data, we release the code used for dataset construction, formatting, filtering, and scraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and GSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising</title>
<link>https://arxiv.org/abs/2408.05906</link>
<guid>https://arxiv.org/abs/2408.05906</guid>
<content:encoded><![CDATA[
arXiv:2408.05906v2 Announce Type: replace 
Abstract: With the increase in the fluency of ad texts automatically created by natural language generation technology, there is high demand to verify the quality of these creatives in a real-world setting. We propose AdTEC (Ad Text Evaluation Benchmark by CyberAgent), the first public benchmark to evaluate ad texts from multiple perspectives within practical advertising operations. Our contributions are as follows: (i) Defining five tasks for evaluating the quality of ad texts, as well as building a Japanese dataset based on the practical operational experiences of building a Japanese dataset based on the practical operational experiences of advertising agencies, which are typically kept in-house. (ii) Validating the performance of existing pre-trained language models (PLMs) and human evaluators on the dataset. (iii) Analyzing the characteristics and providing challenges of the benchmark. The results show that while PLMs have already reached practical usage level in several tasks, humans still outperform in certain domains, implying that there is significant room for improvement in this area.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering</title>
<link>https://arxiv.org/abs/2408.08444</link>
<guid>https://arxiv.org/abs/2408.08444</guid>
<content:encoded><![CDATA[
arXiv:2408.08444v2 Announce Type: replace 
Abstract: In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Processing for the OpenGPT-X Model Family</title>
<link>https://arxiv.org/abs/2410.08800</link>
<guid>https://arxiv.org/abs/2410.08800</guid>
<content:encoded><![CDATA[
arXiv:2410.08800v2 Announce Type: replace 
Abstract: This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling</title>
<link>https://arxiv.org/abs/2410.11325</link>
<guid>https://arxiv.org/abs/2410.11325</guid>
<content:encoded><![CDATA[
arXiv:2410.11325v3 Announce Type: replace 
Abstract: Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Domain Question Answering with Conflicting Contexts</title>
<link>https://arxiv.org/abs/2410.12311</link>
<guid>https://arxiv.org/abs/2410.12311</guid>
<content:encoded><![CDATA[
arXiv:2410.12311v4 Announce Type: replace 
Abstract: Open domain question answering systems frequently rely on information retrieved from large collections of text (such as the Web) to answer questions. However, such collections of text often contain conflicting information, and indiscriminately depending on this information may result in untruthful and inaccurate answers. To understand the gravity of this problem, we collect a human-annotated dataset, Question Answering with Conflicting Contexts (QACC), and find that as much as 25% of unambiguous, open domain questions can lead to conflicting contexts when retrieved using Google Search. We evaluate and benchmark three powerful Large Language Models (LLMs) with our dataset QACC and demonstrate their limitations in effectively addressing questions with conflicting information. To explore how humans reason through conflicting contexts, we request our annotators to provide explanations for their selections of correct answers. We demonstrate that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization</title>
<link>https://arxiv.org/abs/2410.13961</link>
<guid>https://arxiv.org/abs/2410.13961</guid>
<content:encoded><![CDATA[
arXiv:2410.13961v2 Announce Type: replace 
Abstract: Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. In this work, we investigate how hallucinations manifest in LLMs when summarizing topic-specific information from multiple documents. Since no benchmarks exist for investigating hallucinations in MDS, we use existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content. To understand the characteristics of these hallucinations, we manually evaluate 700+ insights and find that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, we investigate the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. Our results underscore the need for more effective approaches to systematically mitigate hallucinations in MDS. We release our dataset and code at github.com/megagonlabs/Hallucination_MDS.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiNER-fr-gold: A Gold-Standard NER Corpus</title>
<link>https://arxiv.org/abs/2411.00030</link>
<guid>https://arxiv.org/abs/2411.00030</guid>
<content:encoded><![CDATA[
arXiv:2411.00030v2 Announce Type: replace 
Abstract: We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese</title>
<link>https://arxiv.org/abs/2411.17270</link>
<guid>https://arxiv.org/abs/2411.17270</guid>
<content:encoded><![CDATA[
arXiv:2411.17270v2 Announce Type: replace 
Abstract: In this paper, we aimed to develop a neural parser for Vietnamese based on simplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora, VietTreebank and VnDT, had around 15% of constituency and dependency tree pairs that did not adhere to simplified HPSG rules. To attempt to address the issue of the corpora not adhering to simplified HPSG rules, we randomly permuted samples from the training and development sets to make them compliant with simplified HPSG. We then modified the first simplified HPSG Neural Parser for the Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which can encode Vietnamese texts. We conducted experiments on our modified VietTreebank and VnDT corpora. Our extensive experiments showed that the simplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82% for constituency parsing when using the same predicted part-of-speech (POS) tags as the self-attentive constituency parser. Additionally, it outperformed previous studies in dependency parsing with a higher Unlabeled Attachment Score (UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores likely due to our focus on arc permutation without changing the original labels, as we did not consult with a linguistic expert. Lastly, the research findings of this paper suggest that simplified HPSG should be given more attention to linguistic expert when developing treebanks for Vietnamese natural language processing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Length Issues in Document-level Machine Translation</title>
<link>https://arxiv.org/abs/2412.17592</link>
<guid>https://arxiv.org/abs/2412.17592</guid>
<content:encoded><![CDATA[
arXiv:2412.17592v2 Announce Type: replace 
Abstract: Transformer architectures are increasingly effective at processing and generating very long chunks of texts, opening new perspectives for document-level machine translation (MT). In this work, we challenge the ability of MT systems to handle texts comprising up to several thousands of tokens. We design and implement a new approach designed to precisely measure the effect of length increments on MT outputs. Our experiments with two representative architectures unambiguously show that (a)~translation performance decreases with the length of the input text; (b)~the position of sentences within the document matters, and translation quality is higher for sentences occurring earlier in a document. We further show that manipulating the distribution of document lengths and of positional embeddings only marginally mitigates such problems. Our results suggest that even though document-level MT is computationally feasible, it does not yet match the performance of sentence-based MT.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context</title>
<link>https://arxiv.org/abs/2412.17596</link>
<guid>https://arxiv.org/abs/2412.17596</guid>
<content:encoded><![CDATA[
arXiv:2412.17596v3 Announce Type: replace 
Abstract: While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora</title>
<link>https://arxiv.org/abs/2502.00090</link>
<guid>https://arxiv.org/abs/2502.00090</guid>
<content:encoded><![CDATA[
arXiv:2502.00090v2 Announce Type: replace 
Abstract: A numeration system encodes abstract numeric quantities as concrete strings of written characters. The numeration systems used by modern scripts tend to be precise and unambiguous, but this was not so for the ancient and partially-deciphered proto-Elamite (PE) script, where written numerals can have up to four distinct readings depending on the system that is used to read them. We consider the task of disambiguating between these readings in order to determine the values of the numeric quantities recorded in this corpus. We algorithmically extract a list of possible readings for each PE numeral notation, and contribute two disambiguation techniques based on structural properties of the original documents and classifiers learned with the bootstrapping algorithm. We also contribute a test set for evaluating disambiguation techniques, as well as a novel approach to cautious rule selection for bootstrapped classifiers. Our analysis confirms existing intuitions about this script and reveals previously-unknown correlations between tablet content and numeral magnitude. This work is crucial to understanding and deciphering PE, as the corpus is heavily accounting-focused and contains many more numeric tokens than tokens of text.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context</title>
<link>https://arxiv.org/abs/2502.12257</link>
<guid>https://arxiv.org/abs/2502.12257</guid>
<content:encoded><![CDATA[
arXiv:2502.12257v2 Announce Type: replace 
Abstract: Large language models excel at following explicit instructions, but they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses instead of seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. This benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue by asking clarifying questions before providing appropriate responses. Our evaluation of both open and closed models reveals that, while proprietary models generally perform better, all current assistants struggle to gather critical information effectively. They often require multiple turns to infer user intent and frequently default to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models' information-seeking capabilities, which can be leveraged to automatically generate data for self-improvement. We also offer insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.13019</link>
<guid>https://arxiv.org/abs/2502.13019</guid>
<content:encoded><![CDATA[
arXiv:2502.13019v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning</title>
<link>https://arxiv.org/abs/2502.14644</link>
<guid>https://arxiv.org/abs/2502.14644</guid>
<content:encoded><![CDATA[
arXiv:2502.14644v3 Announce Type: replace 
Abstract: Long context understanding remains challenging for large language models due to their limited context windows. This paper presents Long Input Fine-Tuning (LIFT), a novel framework for long-context modeling that can improve the long-context performance of arbitrary (short-context) LLMs by dynamically adapting model parameters based on the long input. Importantly, LIFT, rather than endlessly extending the context window size to accommodate increasingly longer inputs in context, chooses to store and absorb the long input in parameter. By fine-tuning the long input into model parameters, LIFT allows short-context LLMs to answer questions even when the required information is not provided in the context during inference. Furthermore, to enhance LIFT performance while maintaining the original in-context learning (ICL) capabilities, we introduce Gated Memory, a specialized attention adapter that automatically balances long input memorization and ICL. We provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision</title>
<link>https://arxiv.org/abs/2502.15147</link>
<guid>https://arxiv.org/abs/2502.15147</guid>
<content:encoded><![CDATA[
arXiv:2502.15147v2 Announce Type: replace 
Abstract: Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM's reasoning ability and drops when the data is noisy or beyond LLM's knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM's instruction-following ability with statistical models to handle large, noisy datasets where LLM reasoning alone falls short.
  Instruct-LF uses LLMs to propose fine-grained, goal-related properties from documents, estimates their presence across the dataset, and applies gradient-based optimization to uncover hidden factors, where each factor is represented by a cluster of co-occurring properties. We evaluate latent factors produced by Instruct-LF on movie recommendation, text-world navigation, and legal document categorization tasks. These interpretable representations improve downstream task performance by 5-52% than the best baselines and were preferred 1.8 times as often as the best alternative, on average, in human evaluation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting multimodal large language models against misleading visualizations</title>
<link>https://arxiv.org/abs/2502.20503</link>
<guid>https://arxiv.org/abs/2502.20503</guid>
<content:encoded><![CDATA[
arXiv:2502.20503v3 Announce Type: replace 
Abstract: Visualizations play a pivotal role in daily communication in an increasingly data-driven world. Research on multimodal large language models (MLLMs) for automated chart understanding has accelerated massively, with steady improvements on standard benchmarks. However, for MLLMs to be reliable, they must be robust to misleading visualizations, charts that distort the underlying data, leading readers to draw inaccurate conclusions that may support disinformation. Here, we uncover an important vulnerability: MLLM question-answering accuracy on misleading visualizations drops on average to the level of a random baseline. To address this, we introduce the first inference-time methods to improve performance on misleading visualizations, without compromising accuracy on non-misleading ones. The most effective method extracts the underlying data table and uses a text-only LLM to answer the question based on the table. Our findings expose a critical blind spot in current research and establish benchmark results to guide future efforts in reliable MLLMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</title>
<link>https://arxiv.org/abs/2503.10617</link>
<guid>https://arxiv.org/abs/2503.10617</guid>
<content:encoded><![CDATA[
arXiv:2503.10617v3 Announce Type: replace 
Abstract: Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish</title>
<link>https://arxiv.org/abs/2504.09714</link>
<guid>https://arxiv.org/abs/2504.09714</guid>
<content:encoded><![CDATA[
arXiv:2504.09714v2 Announce Type: replace 
Abstract: The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings.
  Our results reveal that 70% of the benchmark datasets fail to meet our heuristic quality standards. The correctness of the usage of technical terms is the strongest criterion, but 85% of the criteria are not satisfied in the examined datasets. Although LLM judges demonstrate potential, they are less effective than human annotators, particularly in understanding cultural common sense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger labeling capabilities for grammatical and technical tasks, while Llama3.3-70B excels at correctness and cultural knowledge evaluation. Our findings emphasize the urgent need for more rigorous quality control in creating and adapting datasets for low-resource languages.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Semantic Scholar Open Data Platform</title>
<link>https://arxiv.org/abs/2301.10140</link>
<guid>https://arxiv.org/abs/2301.10140</guid>
<content:encoded><![CDATA[
arXiv:2301.10140v2 Announce Type: replace-cross 
Abstract: The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-the-art techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models</title>
<link>https://arxiv.org/abs/2303.10430</link>
<guid>https://arxiv.org/abs/2303.10430</guid>
<content:encoded><![CDATA[
arXiv:2303.10430v2 Announce Type: replace-cross 
Abstract: Online texts with toxic content are a clear threat to the users on social media in particular and society in general. Although many platforms have adopted various measures (e.g., machine learning-based hate-speech detection systems) to diminish their effect, toxic content writers have also attempted to evade such measures by using cleverly modified toxic words, so-called human-written text perturbations. Therefore, to help build automatic detection tools to recognize those perturbations, prior methods have developed sophisticated techniques to generate diverse adversarial samples. However, we note that these ``algorithms"-generated perturbations do not necessarily capture all the traits of ``human"-written perturbations. Therefore, in this paper, we introduce a novel, high-quality dataset of human-written perturbations, named as NoisyHate, that was created from real-life perturbations that are both written and verified by human-in-the-loop. We show that perturbations in NoisyHate have different characteristics than prior algorithm-generated toxic datasets show, and thus can be in particular useful to help develop better toxic speech detection solutions. We thoroughly validate NoisyHate against state-of-the-art language models, such as BERT and RoBERTa, and black box APIs, such as Perspective API, on two tasks, such as perturbation normalization and understanding.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans</title>
<link>https://arxiv.org/abs/2307.12369</link>
<guid>https://arxiv.org/abs/2307.12369</guid>
<content:encoded><![CDATA[
arXiv:2307.12369v2 Announce Type: replace-cross 
Abstract: Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and "unseen" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2403.19103</link>
<guid>https://arxiv.org/abs/2403.19103</guid>
<content:encoded><![CDATA[
arXiv:2403.19103v3 Announce Type: replace-cross 
Abstract: Prompt engineering is an effective but labor-intensive way to control text-to-image (T2I) generative models. Its time-intensive nature and complexity have spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, or produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically produces human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompt distribution built upon the reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles, and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2403.20331</link>
<guid>https://arxiv.org/abs/2403.20331</guid>
<content:encoded><![CDATA[
arXiv:2403.20331v3 Announce Type: replace-cross 
Abstract: This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed $\textbf{Unsolvable Problem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation</title>
<link>https://arxiv.org/abs/2409.03140</link>
<guid>https://arxiv.org/abs/2409.03140</guid>
<content:encoded><![CDATA[
arXiv:2409.03140v4 Announce Type: replace-cross 
Abstract: Online sellers and advertisers are recommended keyphrases for their listed products, which they bid on to enhance their sales. One popular paradigm that generates such recommendations is Extreme Multi-Label Classification (XMC), which involves tagging/mapping keyphrases to items. We outline the limitations of using traditional item-query based tagging or mapping techniques for keyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an innovative graph-based approach that recommends keyphrases to sellers using extraction of token permutations from item titles. Additionally, we demonstrate that relying on traditional metrics such as precision/recall can be misleading in practical applications, thereby necessitating a combination of metrics to evaluate performance in real-world scenarios. These metrics are designed to assess the relevance of keyphrases to items and the potential for buyer outreach. GraphEx outperforms production models at eBay, achieving the objectives mentioned above. It supports near real-time inferencing in resource-constrained production environments and scales effectively for billions of items.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents</title>
<link>https://arxiv.org/abs/2409.09013</link>
<guid>https://arxiv.org/abs/2409.09013</guid>
<content:encoded><![CDATA[
arXiv:2409.09013v2 Announce Type: replace-cross 
Abstract: Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), which makes it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents' responses. Our experiment demonstrates that all models are truthful less than 50% of the time, though truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models can be directed to be truthful or deceptive, and even truth-steered models still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and LLM-based agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2410.08847</link>
<guid>https://arxiv.org/abs/2410.08847</guid>
<content:encoded><![CDATA[
arXiv:2410.08847v4 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) and its variants are increasingly used for aligning language models with human preferences. Although these methods are designed to teach a model to generate preferred responses more frequently relative to dispreferred responses, prior work has observed that the likelihood of preferred responses often decreases during training. The current work sheds light on the causes and implications of this counter-intuitive phenomenon, which we term likelihood displacement. We demonstrate that likelihood displacement can be catastrophic, shifting probability mass from preferred responses to responses with an opposite meaning. As a simple example, training a model to prefer $\texttt{No}$ over $\texttt{Never}$ can sharply increase the probability of $\texttt{Yes}$. Moreover, when aligning the model to refuse unsafe prompts, we show that such displacement can unintentionally lead to unalignment, by shifting probability mass from preferred refusal responses to harmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from 74.4% to 33.4%). We theoretically characterize that likelihood displacement is driven by preferences that induce similar embeddings, as measured by a centered hidden embedding similarity (CHES) score. Empirically, the CHES score enables identifying which training samples contribute most to likelihood displacement in a given dataset. Filtering out these samples effectively mitigated unintentional unalignment in our experiments. More broadly, our results highlight the importance of curating data with sufficiently distinct preferences, for which we believe the CHES score may prove valuable.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CREAM: Consistency Regularized Self-Rewarding Language Models</title>
<link>https://arxiv.org/abs/2410.12735</link>
<guid>https://arxiv.org/abs/2410.12735</guid>
<content:encoded><![CDATA[
arXiv:2410.12735v5 Announce Type: replace-cross 
Abstract: Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates responses) and the reward model (which scores and ranks those responses). The ranked responses are then used as preference pairs to train the LLM via direct alignment technologies (e.g. DPO). However, it is noteworthy that throughout this process, there is no guarantee of accuracy in the rewarding and ranking, which is critical for ensuring accurate rewards and high-quality preference data. Empirical results from relatively small LLMs (e.g., 7B parameters) also indicate that improvements from self-rewarding may diminish after several iterations in certain situations, which we hypothesize is due to accumulated bias in the reward system. This bias can lead to unreliable preference data for training the LLM. To address this issue, we first formulate and analyze the generalized iterative preference fine-tuning framework for self-rewarding language model. We then introduce the regularization to this generalized framework to mitigate the overconfident preference labeling in the self-rewarding process. Based on this theoretical insight, we propose a Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages the consistency of rewards across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data. With this explicit regularization, our empirical results demonstrate the superiority of CREAM in improving both reward consistency and alignment performance. The code is publicly available at https://github.com/Raibows/CREAM.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models</title>
<link>https://arxiv.org/abs/2410.18252</link>
<guid>https://arxiv.org/abs/2410.18252</guid>
<content:encoded><![CDATA[
arXiv:2410.18252v3 Announce Type: replace-cross 
Abstract: The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online but off-policy RLHF: learning on samples from previous iterations of our model which give a worse training signal. We tackle the fundamental challenge in this regime: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we test, online DPO is found to be most robust to off-policy data, and robustness increases with the scale of the policy model. We study further compute optimizations for asynchronous RLHF but find that they come at a performance cost, giving rise to a trade-off. We verify the scalability of asynchronous RLHF by training a general-purpose chatbot from LLaMA 3.1 8B on an instruction-following task ~40% faster than a synchronous run while matching final performance. Finally, we extend our results to math and reasoning to demonstrate asynchronous RL can finetune Rho 1B on GSM8k ~70% faster while matching synchronous accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Guide to Misinformation Detection Data and Evaluation</title>
<link>https://arxiv.org/abs/2411.05060</link>
<guid>https://arxiv.org/abs/2411.05060</guid>
<content:encoded><![CDATA[
arXiv:2411.05060v3 Announce Type: replace-cross 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at misinfo-datasets.complexdatalab.com.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training</title>
<link>https://arxiv.org/abs/2501.15579</link>
<guid>https://arxiv.org/abs/2501.15579</guid>
<content:encoded><![CDATA[
arXiv:2501.15579v2 Announce Type: replace-cross 
Abstract: The clinical adoption of artificial intelligence (AI) in medical imaging requires models that are both diagnostically accurate and interpretable to clinicians. While current multimodal biomedical foundation models prioritize performance, their black-box nature hinders explaining the decision-making process in clinically meaningful concepts. Here, we present ConceptCLIP, the first explainable biomedical foundation model that achieves state-of-the-art diagnostic accuracy while delivering human-interpretable explanations across diverse imaging modalities. We curate MedConcept-23M, the largest pre-training dataset comprising 23 million image-text-concept triplets across diverse medical modalities, where clinical concepts are derived from the Unified Medical Language System. Leveraging this dataset, we develop ConceptCLIP through a novel dual-alignment approach that simultaneously learns global image-text representations and fine-grained region-concept associations for precise and interpretable medical image analysis. We curate the most extensive evaluation benchmark for multimodal biomedical foundation models, covering 52 clinical tasks spanning 10 imaging modalities. Extensive experiments demonstrate that ConceptCLIP outperforms existing state-of-the-art multimodal biomedical foundation models. Importantly, ConceptCLIP demonstrates superior diagnostic performance while providing human-understandable explanations validated by clinical experts. As the first precise and interpretable biomedical foundation model, ConceptCLIP represents a critical milestone toward the widespread clinical adoption of AI, thereby advancing trustworthy AI in medicine.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering</title>
<link>https://arxiv.org/abs/2502.09573</link>
<guid>https://arxiv.org/abs/2502.09573</guid>
<content:encoded><![CDATA[
arXiv:2502.09573v3 Announce Type: replace-cross 
Abstract: In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring LLM-based Student Simulation for Metacognitive Cultivation</title>
<link>https://arxiv.org/abs/2502.11678</link>
<guid>https://arxiv.org/abs/2502.11678</guid>
<content:encoded><![CDATA[
arXiv:2502.11678v2 Announce Type: replace-cross 
Abstract: Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising. Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NutriGen: Personalized Meal Plan Generator Leveraging Large Language Models to Enhance Dietary and Nutritional Adherence</title>
<link>https://arxiv.org/abs/2502.20601</link>
<guid>https://arxiv.org/abs/2502.20601</guid>
<content:encoded><![CDATA[
arXiv:2502.20601v2 Announce Type: replace-cross 
Abstract: Maintaining a balanced diet is essential for overall health, yet many individuals struggle with meal planning due to nutritional complexity, time constraints, and lack of dietary knowledge. Personalized food recommendations can help address these challenges by tailoring meal plans to individual preferences, habits, and dietary restrictions. However, existing dietary recommendation systems often lack adaptability, fail to consider real-world constraints such as food ingredient availability, and require extensive user input, making them impractical for sustainable and scalable daily use. To address these limitations, we introduce NutriGen, a framework based on large language models (LLM) designed to generate personalized meal plans that align with user-defined dietary preferences and constraints. By building a personalized nutrition database and leveraging prompt engineering, our approach enables LLMs to incorporate reliable nutritional references like the USDA nutrition database while maintaining flexibility and ease-of-use. We demonstrate that LLMs have strong potential in generating accurate and user-friendly food recommendations, addressing key limitations in existing dietary recommendation systems by providing structured, practical, and scalable meal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve the lowest percentage errors of 1.55\% and 3.68\%, respectively, producing meal plans that closely align with user-defined caloric targets while minimizing deviation and improving precision. Additionally, we compared the performance of DeepSeek V3 against several established models to evaluate its potential in personalized nutrition planning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</title>
<link>https://arxiv.org/abs/2503.10619</link>
<guid>https://arxiv.org/abs/2503.10619</guid>
<content:encoded><![CDATA[
arXiv:2503.10619v3 Announce Type: replace-cross 
Abstract: We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking</title>
<link>https://arxiv.org/abs/2504.03947</link>
<guid>https://arxiv.org/abs/2504.03947</guid>
<content:encoded><![CDATA[
arXiv:2504.03947v2 Announce Type: replace-cross 
Abstract: We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use</title>
<link>https://arxiv.org/abs/2504.04736</link>
<guid>https://arxiv.org/abs/2504.04736</guid>
<content:encoded><![CDATA[
arXiv:2504.04736v2 Announce Type: replace-cross 
Abstract: Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniCaptioner: One Captioner to Rule Them All</title>
<link>https://arxiv.org/abs/2504.07089</link>
<guid>https://arxiv.org/abs/2504.07089</guid>
<content:encoded><![CDATA[
arXiv:2504.07089v2 Announce Type: replace-cross 
Abstract: We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection</title>
<link>https://arxiv.org/abs/2504.17834</link>
<guid>https://arxiv.org/abs/2504.17834</guid>
<content:encoded><![CDATA[
arXiv:2504.17834v2 Announce Type: replace-cross 
Abstract: Spoilers in movie reviews are important on platforms like IMDb and Rotten Tomatoes, offering benefits and drawbacks. They can guide some viewers' choices but also affect those who prefer no plot details in advance, making effective spoiler detection essential. Existing spoiler detection methods mainly analyze review text, often overlooking the impact of movie genres and user bias, limiting their effectiveness. To address this, we analyze movie review data, finding genre-specific variations in spoiler rates and identifying that certain users are more likely to post spoilers. Based on these findings, we introduce a new spoiler detection framework called GUSD (The code is available at https://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler Detection), which incorporates genre-specific data and user behavior bias. User bias is calculated through dynamic graph modeling of review history. Additionally, the R2GFormer module combines RetGAT (Retentive Graph Attention Network) for graph information and GenreFormer for genre-specific aggregation. The GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to specialized experts based on genre. Extensive testing on benchmark datasets shows that GUSD achieves state-of-the-art results. This approach advances spoiler detection by addressing genre and user-specific patterns, enhancing user experience on movie review platforms.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English</title>
<link>https://arxiv.org/abs/2504.17974</link>
<guid>https://arxiv.org/abs/2504.17974</guid>
<content:encoded><![CDATA[
<div> Dataset, Hope, Natural Language Processing, Emotional State, Sarcasm
<br />
Summary:
This study introduces PolyHope V2, a multilingual dataset of over 30,000 annotated tweets in English and Spanish, distinguishing between four hope subtypes: Generalized, Realistic, Unrealistic, and Sarcastic. The dataset enhances existing resources by explicitly labeling sarcastic instances. Benchmarking multiple pretrained transformer models against large language models (LLMs) like GPT 4 and Llama 3 shows that fine-tuned transformers outperform prompt-based LLMs in identifying nuanced hope categories and sarcasm, especially in zero-shot and few-shot regimes. Through qualitative analysis and confusion matrices, systematic challenges in differentiating closely related hope subtypes are highlighted. The dataset and results provide a solid basis for future emotion recognition tasks that require greater semantic and contextual sensitivity across languages. 
<br /><br />Summary: <div>
arXiv:2504.17974v1 Announce Type: new 
Abstract: Hope is a complex and underexplored emotional state that plays a significant role in education, mental health, and social interaction. Unlike basic emotions, hope manifests in nuanced forms ranging from grounded optimism to exaggerated wishfulness or sarcasm, making it difficult for Natural Language Processing systems to detect accurately. This study introduces PolyHope V2, a multilingual, fine-grained hope speech dataset comprising over 30,000 annotated tweets in English and Spanish. This resource distinguishes between four hope subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances existing datasets by explicitly labeling sarcastic instances. We benchmark multiple pretrained transformer models and compare them with large language models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes. Our findings show that fine-tuned transformers outperform prompt-based LLMs, especially in distinguishing nuanced hope categories and sarcasm. Through qualitative analysis and confusion matrices, we highlight systematic challenges in separating closely related hope subtypes. The dataset and results provide a robust foundation for future emotion recognition tasks that demand greater semantic and contextual sensitivity across languages.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Personas via Rationalization with Psychological Scaffolds</title>
<link>https://arxiv.org/abs/2504.17993</link>
<guid>https://arxiv.org/abs/2504.17993</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, user personas, psychology, behavior, judgments

Summary:
The article introduces a new framework called PB&amp;J (Psychology of Behavior and Judgments) that enhances language model personas by incorporating rationales for user judgments. These rationales are generated by LLMs and aim to explain why a user makes specific decisions based on their experiences, personality traits, and beliefs. The framework utilizes psychological scaffolds grounded in theories like the Big 5 Personality Traits and Primal World Beliefs to provide structure to the generated rationales. Experiments show that LLM personas augmented with PB&amp;J rationales outperform traditional methods that only consider demographics and judgments. Furthermore, LLM personas constructed using scaffolds describing user beliefs perform on par with those using human-written rationales.<br /><br />Summary: <div>
arXiv:2504.17993v1 Announce Type: new 
Abstract: Language models prompted with a user description or persona can predict a user's preferences and opinions, but existing approaches to building personas -- based solely on a user's demographic attributes and/or prior judgments -- fail to capture the underlying reasoning behind said user judgments. We introduce PB&amp;J (Psychology of Behavior and Judgments), a framework that improves LLM personas by incorporating rationales of why a user might make specific judgments. These rationales are LLM-generated, and aim to reason about a user's behavior on the basis of their experiences, personality traits or beliefs. This is done using psychological scaffolds -- structured frameworks grounded in theories such as the Big 5 Personality Traits and Primal World Beliefs -- that help provide structure to the generated rationales. Experiments on public opinion and movie preference prediction tasks demonstrate that LLM personas augmented with PB&amp;J rationales consistently outperform methods using only a user's demographics and/or judgments. Additionally, LLM personas constructed using scaffolds describing user beliefs perform competitively with those using human-written rationales.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2504.18012</link>
<guid>https://arxiv.org/abs/2504.18012</guid>
<content:encoded><![CDATA[
<div> pre-trained encoders, pre-trained decoders, multimodal machine translation, training strategies, translation performance<br />
Summary:<br />
This study explores the impact of pre-trained encoders and decoders in multimodal translation models. It investigates different training strategies, including training from scratch and utilizing pre-trained and partially frozen components. The experiments conducted on the Multi30K and CoMMuTE datasets for English-German and English-French translation tasks demonstrate that pre-training plays a vital role in multimodal translation, with pre-trained decoders consistently producing more fluent and accurate outputs. The effects of pre-trained encoders vary depending on the quality of visual-text alignment. The study also offers insights into how modality fusion and pre-trained components interact, providing valuable guidance for the design of future multimodal translation systems. <br /> <div>
arXiv:2504.18012v1 Announce Type: new 
Abstract: Multimodal Machine Translation (MMT) aims to improve translation quality by leveraging auxiliary modalities such as images alongside textual input. While recent advances in large-scale pre-trained language and vision models have significantly benefited unimodal natural language processing tasks, their effectiveness and role in MMT remain underexplored. In this work, we conduct a systematic study on the impact of pre-trained encoders and decoders in multimodal translation models. Specifically, we analyze how different training strategies, from training from scratch to using pre-trained and partially frozen components, affect translation performance under a unified MMT framework. Experiments are carried out on the Multi30K and CoMMuTE dataset across English-German and English-French translation tasks. Our results reveal that pre-training plays a crucial yet asymmetrical role in multimodal settings: pre-trained decoders consistently yield more fluent and accurate outputs, while pre-trained encoders show varied effects depending on the quality of visual-text alignment. Furthermore, we provide insights into the interplay between modality fusion and pre-trained components, offering guidance for future architecture design in multimodal translation systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models</title>
<link>https://arxiv.org/abs/2504.18041</link>
<guid>https://arxiv.org/abs/2504.18041</guid>
<content:encoded><![CDATA[
<div> frameworks, safety, language models, RAG, red-teaming<br />Summary:
Efforts to ensure the safety of large language models (LLMs) have focused on standard models but little is known about the safety profile of models using the Retrieval-Augmented Generation (RAG) framework. A comparative analysis of RAG and non-RAG frameworks with eleven LLMs revealed that RAG can make models less safe and change their safety profile. Even combinations of safe models with safe documents can lead to unsafe generations. Existing red teaming methods are found to be less effective in RAG settings compared to non-RAG settings. This highlights the need for safety research and red-teaming methods tailored specifically for RAG LLMs. <div>
arXiv:2504.18041v1 Announce Type: new 
Abstract: Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.18053</link>
<guid>https://arxiv.org/abs/2504.18053</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Risk Disentanglement, Safety Alignment, DREAM, Reinforcement Learning.

Summary:
Multimodal Large Language Models (MLLMs) integrate visual and textual data, introducing new safety challenges. This paper disentangles risks in multimodal inputs to enhance risk awareness in MLLMs. A novel approach, DREAM (Disentangling Risks to Enhance Safety Alignment in MLLMs), uses supervised fine-tuning and Reinforcement Learning from AI Feedback (RLAIF) to boost safety without compromising performance. Experimental results show DREAM improves safety during both inference and training phases, achieving a 16.17% improvement in the SIUO safe & effective score compared to GPT-4V. The data and code for DREAM are available on GitHub at https://github.com/Kizna1ver/DREAM.

<br /><br />Summary: Multimodal Large Language Models present unique safety challenges. This paper analyzes and disentangles risks in multimodal inputs, leading to enhanced risk awareness. The DREAM approach leverages supervised fine-tuning and Reinforcement Learning from AI Feedback to improve safety alignment in MLLMs. Experimental results demonstrate significant safety enhancements without compromising performance, outperforming GPT-4V in the SIUO safe & effective score. The data and code for DREAM can be found on GitHub. <div>
arXiv:2504.18053v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \textbf{DREAM} (\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety \textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\% improvement in the SIUO safe\&amp;effective score compared to GPT-4V. The data and code are available at https://github.com/Kizna1ver/DREAM.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Personality-Aware Interactions in Salesperson Dialogue Agents</title>
<link>https://arxiv.org/abs/2504.18058</link>
<guid>https://arxiv.org/abs/2504.18058</guid>
<content:encoded><![CDATA[
<div> MBTI, user personas, dialogue agents, sales domain, interaction quality <br />
Summary:<br />
This study examines the impact of user personas, categorized by MBTI, on sales-oriented dialogue agents. Through extensive testing, the study evaluates how pre-trained agents perform with different user types. Results show distinct patterns in interaction dynamics, task completion rates, and dialogue naturalness based on user personas. The findings suggest opportunities for dialogue agents to enhance their adaptability and personalization to align with various personality traits. The study's insights offer practical guidance for creating more user-centric conversational systems in sales and provide persona-defined user simulators that can be applied across diverse applications. The release of these simulators contributes to advancing personalized dialogue systems in various domains. <br /> <div>
arXiv:2504.18058v1 Announce Type: new 
Abstract: The integration of dialogue agents into the sales domain requires a deep understanding of how these systems interact with users possessing diverse personas. This study explores the influence of user personas, defined using the Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance of sales-oriented dialogue agents. Through large-scale testing and analysis, we assess the pre-trained agent's effectiveness, adaptability, and personalization capabilities across a wide range of MBTI-defined user types. Our findings reveal significant patterns in interaction dynamics, task completion rates, and dialogue naturalness, underscoring the future potential for dialogue agents to refine their strategies to better align with varying personality traits. This work not only provides actionable insights for building more adaptive and user-centric conversational systems in the sales domain but also contributes broadly to the field by releasing persona-defined user simulators. These simulators, unconstrained by domain, offer valuable tools for future research and demonstrate the potential for scaling personalized dialogue systems across diverse applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropRAG: Guiding Retrieval with Beam Search over Proposition Paths</title>
<link>https://arxiv.org/abs/2504.18070</link>
<guid>https://arxiv.org/abs/2504.18070</guid>
<content:encoded><![CDATA[
<div> framework, propositions, reasoning, retrieval, continual learning
Summary:
PropRAG introduces a framework that leverages contextually rich propositions and a novel beam search algorithm for multi-step reasoning chains. The online retrieval process of PropRAG operates without relying on generative Large Language Models (LLMs). By using efficient graph traversal and pre-computed embeddings, PropRAG avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are utilized offline for proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on various question answering datasets, including PopQA, 2Wiki, HotpotQA, and MuSiQue, with top F1 scores on MuSiQue. By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.
<br /><br />Summary: <div>
arXiv:2504.18070v1 Announce Type: new 
Abstract: Retrieval Augmented Generation (RAG) has become the standard non-parametric approach for equipping Large Language Models (LLMs) with up-to-date knowledge and mitigating catastrophic forgetting common in continual learning. However, standard RAG, relying on independent passage retrieval, fails to capture the interconnected nature of human memory crucial for complex reasoning (associativity) and contextual understanding (sense-making). While structured RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples, the inherent context loss limits fidelity. We introduce PropRAG, a framework leveraging contextually rich propositions and a novel beam search algorithm over proposition paths to explicitly discover multi-step reasoning chains. Crucially, PropRAG's online retrieval process operates entirely without invoking generative LLMs, relying instead on efficient graph traversal and pre-computed embeddings. This avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are used effectively offline for high-quality proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization</title>
<link>https://arxiv.org/abs/2504.18080</link>
<guid>https://arxiv.org/abs/2504.18080</guid>
<content:encoded><![CDATA[
<div> medical, language models, Japanese, reasoning explanations, accuracy <br />
<br />
Summary: 
The paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to provide high accuracy and stable reasoning. It undergoes a two-stage fine-tuning process: Continued Pretraining (CPT) on a Japanese medical corpus followed by Reasoning Preference Optimization (RPO). Evaluations on the Japanese Medical Licensing Exam benchmark show that Preferred-MedLLM-Qwen-72B outperforms proprietary models like GPT-4o in accuracy. Importantly, the model maintains its high accuracy even when prompted for reasoning explanations, showcasing the effectiveness of RPO in stabilizing reasoning generation. The study emphasizes the need to optimize language models for reliable explanations in addition to accuracy, aiming to promote research into trustworthy LLMs for critical applications. The model weights for Preferred-MedLLM-Qwen-72B are made publicly available to support further research in this area. <br /> <div>
arXiv:2504.18080v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show potential in medicine, yet clinical adoption is hindered by concerns over factual accuracy, language-specific limitations (e.g., Japanese), and critically, their reliability when required to generate reasoning explanations -- a prerequisite for trust. This paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to achieve both high accuracy and stable reasoning. We employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first, Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a preference-based method, enhances the generation of reliable reasoning pathways while preserving high answer accuracy. Evaluations on the Japanese Medical Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves state-of-the-art performance (0.868 accuracy), surpassing strong proprietary models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which exhibit significant accuracy degradation (up to 11.5\% and 3.8\% respectively on IgakuQA) when prompted for explanations, our model maintains its high accuracy (0.868) under such conditions. This highlights RPO's effectiveness in stabilizing reasoning generation. This work underscores the importance of optimizing for reliable explanations alongside accuracy. We release the Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy LLMs for specialized, high-stakes applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random-Set Large Language Models</title>
<link>https://arxiv.org/abs/2504.18085</link>
<guid>https://arxiv.org/abs/2504.18085</guid>
<content:encoded><![CDATA[
<div> Language Models, Uncertainty Quantification, Random-Set Approach, Epistemic Uncertainty, Hierarchical Clustering <br />
Summary:<br />
This paper addresses uncertainty quantification in Large Language Models (LLMs) by introducing a Random-Set Large Language Model (RSLLM) approach. Instead of using probability vectors, RSLLMs predict finite random sets (belief functions) over the token space to encode epistemic uncertainty. A methodology based on hierarchical clustering is proposed to extract and use "focal" subsets of tokens for efficient belief prediction. The approach is evaluated on CoQA and OBQA datasets using various models, showing improved correctness of answers and the ability to estimate second-level uncertainty. RSLLMs also demonstrate the capability to detect hallucinations in generated text. Overall, the RSLLM approach outperforms standard LLMs in terms of answer correctness and uncertainty estimation, highlighting its potential for more reliable and trustworthy text generation. <br /> <div>
arXiv:2504.18085v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are known to produce very high-quality tests and responses to our queries. But how much can we trust this generated text? In this paper, we study the problem of uncertainty quantification in LLMs. We propose a novel Random-Set Large Language Model (RSLLM) approach which predicts finite random sets (belief functions) over the token space, rather than probability vectors as in classical LLMs. In order to allow so efficiently, we also present a methodology based on hierarchical clustering to extract and use a budget of "focal" subsets of tokens upon which the belief prediction is defined, rather than using all possible collections of tokens, making the method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced in their generation process by the size and diversity of its training set via the size of the credal sets associated with the predicted belief functions. The proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b, Mistral-7b and Phi-2 models and is shown to outperform the standard model in both datasets in terms of correctness of answer while also showing potential in estimating the second level uncertainty in its predictions and providing the capability to detect when its hallucinating.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation</title>
<link>https://arxiv.org/abs/2504.18104</link>
<guid>https://arxiv.org/abs/2504.18104</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, fact-check-worthiness, prompt tuning, language models, evaluation.

Summary:
This paper introduces a method for estimating the worthiness of fact-checking claims in the face of misinformation in a globalized and digital world. The proposed approach utilizes prompt tuning to enhance the accuracy of determining the fact-check-worthiness of claims, particularly in scenarios with limited or unlabelled data. By applying prompt templates to large language models, the model achieves superior performance compared to established baseline methods such as BERT, GPT-3.5, and GPT-4. The experiments conducted on public datasets demonstrate the effectiveness of the prompt tuning-based method, showcasing improvements in evaluation metrics like F1 score and accuracy. This method proves to be a significant advancement in the field of fact-checking, providing a reliable and efficient tool for combating misinformation. 

<br /><br />Summary: <div>
arXiv:2504.18104v1 Announce Type: new 
Abstract: In response to the growing problem of misinformation in the context of globalization and informatization, this paper proposes a classification method for fact-check-worthiness estimation based on prompt tuning. We construct a model for fact-check-worthiness estimation at the methodological level using prompt tuning. By applying designed prompt templates to large language models, we establish in-context learning and leverage prompt tuning technology to improve the accuracy of determining whether claims have fact-check-worthiness, particularly when dealing with limited or unlabeled data. Through extensive experiments on public datasets, we demonstrate that the proposed method surpasses or matches multiple baseline methods in the classification task of fact-check-worthiness estimation assessment, including classical pre-trained models such as BERT, as well as recent popular large models like GPT-3.5 and GPT-4. Experiments show that the prompt tuning-based method proposed in this study exhibits certain advantages in evaluation metrics such as F1 score and accuracy, thereby effectively validating its effectiveness and advancement in the task of fact-check-worthiness estimation.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering</title>
<link>https://arxiv.org/abs/2504.18106</link>
<guid>https://arxiv.org/abs/2504.18106</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese media, English media, Olympics, topic modeling, discourse construction

Summary: 
Chinese and English media reports on the Paris Olympics were analyzed using topic modeling, Large Language Model prompt engineering, and corpus phraseology methods. Common topics included the opening ceremony, athlete performance, and sponsorship brands. Chinese media focused on specific sports, sports spirit, doping controversies, and new technologies, while English media highlighted female athletes, medal wins, and eligibility controversies. Chinese reports exhibited more prepositional co-occurrences and positive semantic prosody in describing the opening ceremony and sports spirit. In contrast, English reports showed positive semantic prosody in coverage of female athletes but negativity in predicting opening ceremony reactions and discussing women's boxing controversies. These differences in discourse construction and attitudinal meanings provide insights into how the Olympics were perceived and reported in Chinese and English media. 

Summary: <div>
arXiv:2504.18106v1 Announce Type: new 
Abstract: This study analyzes Chinese and English media reports on the Paris Olympics using topic modeling, Large Language Model (LLM) prompt engineering, and corpus phraseology methods to explore similarities and differences in discourse construction and attitudinal meanings. Common topics include the opening ceremony, athlete performance, and sponsorship brands. Chinese media focus on specific sports, sports spirit, doping controversies, and new technologies, while English media focus on female athletes, medal wins, and eligibility controversies. Chinese reports show more frequent prepositional co-occurrences and positive semantic prosody in describing the opening ceremony and sports spirit. English reports exhibit positive semantic prosody when covering female athletes but negative prosody in predicting opening ceremony reactions and discussing women's boxing controversies.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.18114</link>
<guid>https://arxiv.org/abs/2504.18114</guid>
<content:encoded><![CDATA[
<div> hallucination detection metrics, language models, empirical evaluation, decoding methods, knowledge-grounded settings <br />
Summary: <br />
The study evaluates hallucination detection metrics across various language models and decoding methods. Current metrics do not consistently align with human judgments, lack a comprehensive view of the issue, and show inconsistent performance with model scaling. Notably, LLM-based evaluation, particularly using GPT-4, shows the best results, while mode-seeking decoding methods appear to reduce hallucinations in knowledge-grounded contexts. The findings highlight the need for more robust metrics and strategies to effectively measure and mitigate hallucinations in language models. <div>
arXiv:2504.18114v1 Announce Type: new 
Abstract: Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Entailment Pretraining for Clinical Language Models over EHR Data</title>
<link>https://arxiv.org/abs/2504.18128</link>
<guid>https://arxiv.org/abs/2504.18128</guid>
<content:encoded><![CDATA[
<div> Clinical language models, temporally entailed pretraining, electronic health record, patient trajectories, clinical reasoning<br />
<br />
Summary: <br />
The article introduces a novel temporal entailment pretraining objective for language models in the clinical domain, aiming to capture the evolving and interconnected nature of patient trajectories in the electronic health record. By formulating EHR segments as temporally ordered sentence pairs and training models to understand the relationships between different states over time, the proposed method enhances the ability of language models to generalize across forecasting and diagnosis tasks. Pretraining on a large corpus from MIMIC IV leads to state-of-the-art performance on temporal clinical QA, early warning prediction, and disease progression modeling. This approach provides a more nuanced understanding of patient data in the clinical setting, improving the model's capacity for clinical reasoning and decision-making. <br /> <div>
arXiv:2504.18128v1 Announce Type: new 
Abstract: Clinical language models have achieved strong performance on downstream tasks by pretraining on domain specific corpora such as discharge summaries and medical notes. However, most approaches treat the electronic health record as a static document, neglecting the temporally-evolving and causally entwined nature of patient trajectories. In this paper, we introduce a novel temporal entailment pretraining objective for language models in the clinical domain. Our method formulates EHR segments as temporally ordered sentence pairs and trains the model to determine whether a later state is entailed by, contradictory to, or neutral with respect to an earlier state. Through this temporally structured pretraining task, models learn to perform latent clinical reasoning over time, improving their ability to generalize across forecasting and diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and demonstrate state of the art results on temporal clinical QA, early warning prediction, and disease progression modeling.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)</title>
<link>https://arxiv.org/abs/2504.18142</link>
<guid>https://arxiv.org/abs/2504.18142</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, NER, Urdu, educational content, annotated dataset<br />
<br />
Summary: 
This study focuses on Named Entity Recognition (NER) in Urdu within the domain of education, an area that lacks annotated datasets. The authors created the EDU-NER-2025 dataset containing 13 entities related to education. They detail the annotation process and discuss challenges faced in labeling the dataset. Additionally, linguistic complexities such as morphological nuances and ambiguity in formal Urdu texts are addressed. This work highlights the need for targeted resources for Urdu NER in specific domains like education, where existing models struggle to accurately identify entities such as academic roles, course names, and institutional terms. <div>
arXiv:2504.18142v1 Announce Type: new 
Abstract: Named Entity Recognition (NER) plays a pivotal role in various Natural Language Processing (NLP) tasks by identifying and classifying named entities (NEs) from unstructured data into predefined categories such as person, organization, location, date, and time. While extensive research exists for high-resource languages and general domains, NER in Urdu particularly within domain-specific contexts like education remains significantly underexplored. This is Due to lack of annotated datasets for educational content which limits the ability of existing models to accurately identify entities such as academic roles, course names, and institutional terms, underscoring the urgent need for targeted resources in this domain. To the best of our knowledge, no dataset exists in the domain of the Urdu language for this purpose. To achieve this objective this study makes three key contributions. Firstly, we created a manually annotated dataset in the education domain, named EDU-NER-2025, which contains 13 unique most important entities related to education domain. Second, we describe our annotation process and guidelines in detail and discuss the challenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed key linguistic challenges, such as morphological complexity and ambiguity, which are prevalent in formal Urdu texts.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Language Models for Icelandic Legal Text Summarization</title>
<link>https://arxiv.org/abs/2504.18180</link>
<guid>https://arxiv.org/abs/2504.18180</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, legal domain, preference training, Icelandic, summarization

Summary:
Preference-based training techniques were examined to enhance language models' performance in generating Icelandic legal summaries. The study compared models trained with Reinforcement Learning from Human Feedback and Direct Preference Optimization to those using supervised learning. Results showed that preference training improved the legal accuracy of summaries but did not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations emphasize the need for qualitative assessments in developing language models for the legal domain. <div>
arXiv:2504.18180v1 Announce Type: new 
Abstract: The integration of language models in the legal domain holds considerable promise for streamlining processes and improving efficiency in managing extensive workloads. However, the specialized terminology, nuanced language, and formal style of legal texts can present substantial challenges. This study examines whether preference-based training techniques, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, can enhance models' performance in generating Icelandic legal summaries that align with domain-specific language standards and user preferences. We compare models fine-tuned with preference training to those using conventional supervised learning. Results indicate that preference training improves the legal accuracy of generated summaries over standard fine-tuning but does not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations further underscore the importance of qualitative assessment in developing language models for the legal domain.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish</title>
<link>https://arxiv.org/abs/2504.18221</link>
<guid>https://arxiv.org/abs/2504.18221</guid>
<content:encoded><![CDATA[
<div> Keywords: Chat-GPT, machine translation, creativity, literary text, evaluation<br />
Summary:<br />
This study investigates the variability of Chat-GPT machine translation outputs across different configurations in four languages, focusing on creativity in literary text. Various text granularity levels, temperature settings, and prompting strategies were evaluated using a Creativity Score formula. The results showed that prompting ChatGPT with minimal instruction led to the best creative translations, particularly with the instruction "Translate the following text into [TG] creatively" at a temperature of 1.0. This configuration outperformed other setups as well as DeepL in Spanish, Dutch, and Chinese translations. However, it was noted that ChatGPT consistently fell short compared to human translation. The study highlights the importance of prompting strategies and temperature settings in enhancing the creative output of machine translation systems like Chat-GPT. <br /> <div>
arXiv:2504.18221v1 Announce Type: new 
Abstract: This study examines the variability of Chat-GPT machine translation (MT) outputs across six different configurations in four languages,with a focus on creativity in a literary text. We evaluate GPT translations in different text granularity levels, temperature settings and prompting strategies with a Creativity Score formula. We found that prompting ChatGPT with a minimal instruction yields the best creative translations, with "Translate the following text into [TG] creatively" at the temperature of 1.0 outperforming other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless, ChatGPT consistently underperforms compared to human translation (HT).
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family</title>
<link>https://arxiv.org/abs/2504.18225</link>
<guid>https://arxiv.org/abs/2504.18225</guid>
<content:encoded><![CDATA[
<div> Keywords: small reasoning models, RAG, source summarization, Pleias-RAG-350m, Pleias-RAG-1B 

Summary: 
Pleias-RAG-350m and Pleias-RAG-1B are new small reasoning models designed for RAG, search, and source summarization tasks. They are mid-trained on a synthetic dataset to emulate retrieval from multilingual open sources and excel in features like query routing and source reranking. These models outperform models below 4 billion parameters on RAG benchmarks like HotPotQA and 2wiki, rivaling larger models like Qwen-2.5-7B and Llama-3.1-8B. They offer consistent performance across European languages and ensure systematic reference grounding. Their size allows deployment on constrained infrastructure and higher factuality. Pleias-RAG-350m and Pleias-RAG-1B unlock new use cases for generative AI with their capabilities. 

<br /><br />Summary: <div>
arXiv:2504.18225v1 Announce Type: new 
Abstract: We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Single-Pass Training for Multi-Turn Reasoning</title>
<link>https://arxiv.org/abs/2504.18246</link>
<guid>https://arxiv.org/abs/2504.18246</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, explicit reasoning, multi-turn reasoning, fine-tuning, attention mask

Summary:
Training Large Language Models (LLMs) to generate explicit reasoning before producing an answer has shown improved performance in tasks such as mathematics and coding. However, fine-tuning LLMs on multi-turn reasoning datasets poses a challenge as they must exclude reasoning tokens from subsequent inputs, preventing processing an entire conversation in a single forward pass. This paper introduces a novel approach utilizing response token duplication and a custom attention mask to address this limitation. By enforcing appropriate visibility constraints, the proposed method reduces training time and enables efficient fine-tuning on multi-turn reasoning datasets. This innovative solution opens up opportunities for enhancing LLM performance in complex reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2504.18246v1 Announce Type: new 
Abstract: Training Large Language Models ( LLMs) to generate explicit reasoning before they produce an answer has been shown to improve their performance across various tasks such as mathematics and coding. However, fine-tuning LLMs on multi-turn reasoning datasets presents a unique challenge: LLMs must generate reasoning tokens that are excluded from subsequent inputs to the LLM. This discrepancy prevents us from processing an entire conversation in a single forward pass-an optimization readily available when we fine-tune on a multi-turn non-reasoning dataset. This paper proposes a novel approach that overcomes this limitation through response token duplication and a custom attention mask that enforces appropriate visibility constraints. Our approach significantly reduces the training time and allows efficient fine-tuning on multi-turn reasoning datasets.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGI: Multi-Agent Guided Interview for Psychiatric Assessment</title>
<link>https://arxiv.org/abs/2504.18260</link>
<guid>https://arxiv.org/abs/2504.18260</guid>
<content:encoded><![CDATA[
<div> Keywords: structured clinical interviews, mental healthcare, MAGI, Mini International Neuropsychiatric Interview, psychiatric diagnostic protocols

Summary: 
MAGI is introduced as a framework that automates structured clinical interviews, specifically based on the Mini International Neuropsychiatric Interview (MINI). It operates through a coordinated multi-agent collaboration, navigating clinical logic with specialized agents. These agents include interview tree guided navigation, adaptive question formulation, judgment validation, and diagnosis generation. Through experimental testing on over 1,000 real-world participants with various mental health conditions, MAGI is shown to enhance large language model (LLM) assisted mental health assessments by incorporating clinical rigor, conversational adaptability, and explainable reasoning. This advancement demonstrates the potential for automated systems to revolutionize mental healthcare accessibility and align with established psychiatric diagnostic protocols.<br /><br />Summary: <div>
arXiv:2504.18260v1 Announce Type: new 
Abstract: Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2504.18269</link>
<guid>https://arxiv.org/abs/2504.18269</guid>
<content:encoded><![CDATA[
<div> Keywords: image generation, entity prompts, Large Language Models, TextTIGER, WiT-Cub

Summary:
TextTIGER introduces a method for improving image generation from prompts containing specific entities by augmenting entity knowledge and summarizing descriptions with Large Language Models (LLMs). The method, validated on the WiT-Cub dataset, enhances image generation performance compared to using caption-only prompts, as shown through standard metrics (IS, FID, and CLIPScore). An evaluation by multiple annotators confirms the informative nature of the summarized descriptions, indicating the ability of LLMs to generate concise yet rich prompts. By refining prompts with augmented and summarized entity-related descriptions, TextTIGER enhances image generation capabilities, showcasing the potential of this approach for generating visually compelling images. The code and dataset for this research will be made available upon acceptance. 

<br /><br />Summary: <div>
arXiv:2504.18269v1 Announce Type: new 
Abstract: Generating images from prompts containing specific entities requires models to retain as much entity-specific knowledge as possible. However, fully memorizing such knowledge is impractical due to the vast number of entities and their continuous emergence. To address this, we propose Text-based Intelligent Generation with Entity prompt Refinement (TextTIGER), which augments knowledge on entities included in the prompts and then summarizes the augmented descriptions using Large Language Models (LLMs) to mitigate performance degradation from longer inputs. To evaluate our method, we introduce WiT-Cub (WiT with Captions and Uncomplicated Background-explanations), a dataset comprising captions, images, and an entity list. Experiments on four image generation models and five LLMs show that TextTIGER improves image generation performance in standard metrics (IS, FID, and CLIPScore) compared to caption-only prompts. Additionally, multiple annotators' evaluation confirms that the summarized descriptions are more informative, validating LLMs' ability to generate concise yet rich descriptions. These findings demonstrate that refining prompts with augmented and summarized entity-related descriptions enhances image generation capabilities. The code and dataset will be available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review</title>
<link>https://arxiv.org/abs/2504.18346</link>
<guid>https://arxiv.org/abs/2504.18346</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Uncertainty Quantification, Calibration, Benchmark, Reliability Datasets

Summary: 
This paper addresses the issue of hallucination in Large Language Models (LLMs) and the challenge of accurately assessing their uncertainty. The authors survey existing literature on Uncertainty Quantification (UQ) and calibration techniques for LLMs, highlighting the lack of a comprehensive benchmark for comparison. They evaluate six methods using two reliability datasets, providing empirical evidence for their effectiveness. This study is the first to systematically review calibration methods and metrics for LLMs, offering insights for future research directions and identifying open challenges in the field. Overall, the paper contributes to the understanding of uncertainty estimation in LLMs and provides a valuable resource for researchers working on improving the reliability of these models. 

<br /><br />Summary: <div>
arXiv:2504.18346v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant</title>
<link>https://arxiv.org/abs/2504.18373</link>
<guid>https://arxiv.org/abs/2504.18373</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, large language models, multi-agent frameworks, intelligent personal assistants, Auto-SLURP

Summary:
Auto-SLURP is a new benchmark dataset designed to evaluate the performance of large language model-based multi-agent frameworks in the context of intelligent personal assistants. It builds upon the original SLURP dataset by incorporating simulated servers and external services, allowing for a comprehensive evaluation pipeline covering language understanding, task execution, and response generation. The experiments conducted using Auto-SLURP reveal that current state-of-the-art frameworks face significant challenges in achieving reliable and intelligent multi-agent personal assistants. This underscores the ongoing work needed to enhance the capabilities of such systems. The dataset and related code are publicly available, providing a valuable resource for researchers and developers in the field. 

<br /><br />Summary: <div>
arXiv:2504.18373v1 Announce Type: new 
Abstract: In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the boundary on Natural Language Inference</title>
<link>https://arxiv.org/abs/2504.18376</link>
<guid>https://arxiv.org/abs/2504.18376</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, natural language inference, Chain-of-Thought learning, adversarial benchmark, parameter-efficient techniques 

Summary:
Natural Language Inference (NLI) is a crucial task in language understanding, but current systems face limitations due to reliance on supervised learning with biased datasets. This study introduces a reinforcement learning-based approach, GRPO, for CoT learning in NLI, allowing training on challenging datasets like ANLI without rationales. The use of LoRA and QLoRA for fine-tuning 7B, 14B, and 32B language models demonstrates strong performance on standard and adversarial benchmarks. The 32B AWQ-quantized model surpasses state-of-the-art results on multiple adversarial sets within a limited memory footprint, showcasing robust reasoning capabilities under aggressive quantization. This work presents a scalable framework for building reliable NLI systems without compromising inference quality.

<br /><br />Summary: <div>
arXiv:2504.18376v1 Announce Type: new 
Abstract: Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\unicode{x2013}$or on all of them considering our replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A UD Treebank for Bohairic Coptic</title>
<link>https://arxiv.org/abs/2504.18386</link>
<guid>https://arxiv.org/abs/2504.18386</guid>
<content:encoded><![CDATA[
<div> annotated corpus, Bohairic Coptic, syntactic analysis, cross-dialect parsing, linguistic resources
Summary:
An annotated corpus of Bohairic Coptic, a main dialect of pre-Mamluk Egypt, has been developed, addressing the lack of resources for this variety compared to Sahidic Coptic. The corpus includes texts from various genres like Biblical text and saints' lives. Comparisons with Sahidic Coptic reveal distinct differences, highlighting Bohairic's unique features. Cross-dialect parsing experiments demonstrate the challenges and importance of studying Bohairic separately. This work contributes to advancing linguistic resources for Bohairic Coptic and sheds light on its significance as a distinct but related dialect within the Coptic language family. <div>
arXiv:2504.18386v1 Announce Type: new 
Abstract: Despite recent advances in digital resources for other Coptic dialects, especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk, late Byzantine Egypt, and the contemporary language of the Coptic Church, remains critically under-resourced. This paper presents and evaluates the first syntactically annotated corpus of Bohairic Coptic, sampling data from a range of works, including Biblical text, saints' lives and Christian ascetic writing. We also explore some of the main differences we observe compared to the existing UD treebank of Sahidic Coptic, the classical dialect of the language, and conduct joint and cross-dialect parsing experiments, revealing the unique nature of Bohairic as a related, but distinct variety from the more often studied Sahidic.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?</title>
<link>https://arxiv.org/abs/2504.18406</link>
<guid>https://arxiv.org/abs/2504.18406</guid>
<content:encoded><![CDATA[
<div> image understanding, high-resolution, VLMs, benchmark, HRScene

Summary:
HRScene introduces a new benchmark for high-resolution image (HRI) understanding, encompassing 25 real-world and 2 synthetic diagnostic datasets with varying resolutions. The benchmark covers a wide range of scenarios, from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-images, providing a comprehensive evaluation platform for Vision Large Language Models (VLMs). Experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o, reveal an average accuracy of around 50% on real-world tasks, highlighting significant gaps in HRI understanding. Results on synthetic datasets expose VLMs' struggles to effectively utilize HRI regions, indicating Regional Divergence and lost-in-middle issues that require further research exploration.

<br /><br />Summary: <div>
arXiv:2504.18406v1 Announce Type: new 
Abstract: High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 $\times$ 1,024 to 35,503 $\times$ 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers</title>
<link>https://arxiv.org/abs/2504.18412</link>
<guid>https://arxiv.org/abs/2504.18412</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, therapy, therapeutic alliance, mental health, LLMs<br />
Summary:<br />
This paper examines the potential use of large language models (LLMs) as therapists and evaluates their ability to replicate crucial aspects of therapeutic relationships. The study highlights that current LLMs exhibit stigma towards mental health conditions and respond inadequately to common therapy scenarios, such as encouraging delusional thinking. Despite advancements in LLM technology, they struggle to establish a therapeutic alliance due to lacking human characteristics. The research emphasizes the importance of human empathy and understanding in therapeutic interactions, indicating that LLMs are not suitable replacements for human therapists. The paper concludes by proposing alternative roles for LLMs in clinical therapy settings.<br />  
Summary: <div>
arXiv:2504.18412v1 Announce Type: new 
Abstract: Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as `gpt-4o`. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., LLMs encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs</title>
<link>https://arxiv.org/abs/2504.18415</link>
<guid>https://arxiv.org/abs/2504.18415</guid>
<content:encoded><![CDATA[
<div> Keywords: 1-bit Large Language Models, activation quantization, BitNet v2, H-BitLinear, memory footprint reduction

Summary: 
BitNet v2 is a novel framework designed to efficiently deploy 1-bit Large Language Models (LLMs) by enabling native 4-bit activation quantization. The framework introduces H-BitLinear, a module that applies an online Hadamard transformation to tackle outliers in attention and feed-forward network activations. This transformation helps smooth sharp activation distributions into more Gaussian-like forms, making them suitable for low-bit representation. Experimental results demonstrate that BitNet v2, when trained from scratch with 8-bit activations, matches the performance of BitNet b1.58. Importantly, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing both memory footprint and computational cost for batched inference. This approach offers a promising solution for optimizing the deployment of 1-bit LLMs with improved efficiency and reduced resource requirements. 

<br /><br />Summary: <div>
arXiv:2504.18415v1 Announce Type: new 
Abstract: Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts</title>
<link>https://arxiv.org/abs/2504.18428</link>
<guid>https://arxiv.org/abs/2504.18428</guid>
<content:encoded><![CDATA[
<div> benchmark, multilingual, mathematical reasoning, LLMs, language diversity<br />
Summary:<br /> 
The paper introduces PolyMath, a multilingual mathematical reasoning benchmark with 18 languages and 4 difficulty levels. It evaluates advanced LLMs and finds that they struggle with the benchmark, showing low accuracy. The benchmark highlights challenges in multilingual reasoning: performance variation across languages, low input-output language consistency, and differing thinking lengths by language. Controlling the output language in instructions can impact reasoning performance, especially for low-resource languages. This research demonstrates the need for improvement in multilingual capabilities of LLMs. <br /><br />Summary: <div>
arXiv:2504.18428v1 Announce Type: new 
Abstract: In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Deepseek-R1-671B and Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30% accuracy under the highest level. From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast-Slow Thinking for Large Vision-Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.18458</link>
<guid>https://arxiv.org/abs/2504.18458</guid>
<content:encoded><![CDATA[
<div> Keywords: large vision-language models, FAST framework, fast-slow thinking, adaptive reasoning, state-of-the-art accuracy

Summary:
The study introduces the FAST framework to address the overthinking issue in large vision-language models by dynamically adapting reasoning depth. By exploring the impact of response length and data distribution on performance, the feasibility of fast-slow thinking in LVLMs is established. The FAST-GRPO model incorporates model-based metrics, adaptive thinking reward mechanism, and difficulty-aware KL regularization to optimize reasoning. Experimental results on seven reasoning benchmarks show that FAST outperforms base models with a relative improvement of over 10% in accuracy and reduces token usage by 32.7-67.3% compared to previous slow-thinking approaches. This balance between reasoning length and accuracy demonstrates the effectiveness of the FAST framework in enhancing the performance of LVLMs. 

<br /><br />Summary: The study introduces the FAST framework to address the overthinking issue in large vision-language models, establishing the feasibility of fast-slow thinking. The FAST-GRPO model optimizes reasoning depth using model-based metrics and adaptive mechanisms, achieving state-of-the-art accuracy with reduced token usage. <div>
arXiv:2504.18458v1 Announce Type: new 
Abstract: Recent advances in large vision-language models (LVLMs) have revealed an \textit{overthinking} phenomenon, where models generate verbose reasoning across all tasks regardless of questions. To address this issue, we present \textbf{FAST}, a novel \textbf{Fa}st-\textbf{S}low \textbf{T}hinking framework that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. We develop FAST-GRPO with three components: model-based metrics for question characterization, an adaptive thinking reward mechanism, and difficulty-aware KL regularization. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10\% relative improvement compared to the base model, while reducing token usage by 32.7-67.3\% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions</title>
<link>https://arxiv.org/abs/2504.18474</link>
<guid>https://arxiv.org/abs/2504.18474</guid>
<content:encoded><![CDATA[
<div> Slot Schema Induction, Task-Oriented Dialogue, Language Model, Evaluation Metrics, Dialogue Understanding<br />
Summary:<br />
This paper introduces a novel approach for Slot Schema Induction (SSI) in task-oriented dialogue systems by framing it as a text generation task. They develop a Language Model-based simulation method to automatically generate high-quality data for training SSI models. The authors address challenges in SSI evaluation, such as data leakage and metric misalignment, by generating new evaluation data with human input and refining evaluation metrics. These contributions advance the state-of-the-art in dialogue understanding and system development, laying a solid foundation for future research in SSI and improving the overall quality and efficiency of task-oriented dialogue systems. <div>
arXiv:2504.18474v1 Announce Type: new 
Abstract: In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is essential for automatically identifying key information slots from dialogue data without manual intervention. This paper presents a novel state-of-the-art (SoTA) approach that formulates SSI as a text generation task, where a language model incrementally constructs and refines a slot schema over a stream of dialogue data. To develop this approach, we present a fully automatic LLM-based TOD simulation method that creates data with high-quality state labels for novel task domains. Furthermore, we identify issues in SSI evaluation due to data leakage and poor metric alignment with human judgment. We resolve these by creating new evaluation data using our simulation method with human guidance and correction, as well as designing improved evaluation metrics. These contributions establish a foundation for future SSI research and advance the SoTA in dialogue understanding and system development.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues</title>
<link>https://arxiv.org/abs/2504.18483</link>
<guid>https://arxiv.org/abs/2504.18483</guid>
<content:encoded><![CDATA[
<div> Keywords: co-constructive explanation dialogues, large language models, understanding, engagement, user study <br />
Summary: <br />
The research focuses on the use of large language models (LLMs) as explainers in co-constructive explanation dialogues. In a user study, interactions between explainees and LLMs were examined, with some LLMs instructed to explain topics co-constructively. The results showed that LLMs exhibited co-constructive behaviors such as asking verification questions that enhanced engagement and improved explainees' understanding of the topic. However, the ability of LLMs to effectively monitor understanding and adjust explanations accordingly was found to be limited. The study highlights the potential of LLMs in fostering engagement and enhancing understanding in explanation dialogues, but also points out areas where improvement is needed, particularly in dynamically adapting explanations to the explainees' needs. <br /> <div>
arXiv:2504.18483v1 Announce Type: new 
Abstract: The ability to generate explanations that are understood by explainees is the quintessence of explainable artificial intelligence. Since understanding depends on the explainee's background and needs, recent research has focused on co-constructive explanation dialogues, where the explainer continuously monitors the explainee's understanding and adapts explanations dynamically. We investigate the ability of large language models (LLMs) to engage as explainers in co-constructive explanation dialogues. In particular, we present a user study in which explainees interact with LLMs, of which some have been instructed to explain a predefined topic co-constructively. We evaluate the explainees' understanding before and after the dialogue, as well as their perception of the LLMs' co-constructive behavior. Our results indicate that current LLMs show some co-constructive behaviors, such as asking verification questions, that foster the explainees' engagement and can improve understanding of a topic. However, their ability to effectively monitor the current understanding and scaffold the explanations accordingly remains limited.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation</title>
<link>https://arxiv.org/abs/2504.18535</link>
<guid>https://arxiv.org/abs/2504.18535</guid>
<content:encoded><![CDATA[
<div> Framework, Language model, Probabilistic reasoning, Control generation, Adaptable

Summary:
TRACE introduces a framework for controllable generation in large language models (LMs). By distilling an HMM from an LM and pairing it with a classifier, it efficiently computes Expected Attribute Probability (EAP) for future sequences. This allows for reweighing next-token probabilities in the LM to align with desired attributes. The approach achieves state-of-the-art results in detoxification with minimal overhead and can adapt to personalized attributes quickly. TRACE's lightweight control mechanism enables it to be flexible and scalable, seamlessly extending to composite attributes. This novel framework represents a significant advancement in enabling LMs to generate outputs that align with human values and desired attributes. 

<br /><br />Summary: <div>
arXiv:2504.18535v1 Announce Type: new 
Abstract: As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute - expensive and inflexible - or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable probabilistic reasoning and lightweight control. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMM's predicted futures. This EAP is then used to reweigh the LM's next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with only 10% decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension</title>
<link>https://arxiv.org/abs/2504.17821</link>
<guid>https://arxiv.org/abs/2504.17821</guid>
<content:encoded><![CDATA[
<div> benchmark, cultural diversity, multi-linguistics, domain, video comprehension

Summary:
Existing video evaluation benchmarks have primarily focused on English language and Western cultural contexts, limiting the assessment of multimodal AI systems' understanding and reasoning abilities. VideoVista-CulturalLingo is introduced as a novel benchmark that addresses these limitations by incorporating cultural diversity from China, North America, and Europe, presenting questions in both Chinese and English, and featuring videos from a wide variety of domains. Experiment results reveal that current large models exhibit better performance on Western-centric questions compared to Chinese-centric ones, particularly in Chinese history-related queries. These models also struggle with temporal understanding and achieving high scores in Event Localization tasks. Furthermore, mainstream models excel in general scientific questions but show weakness in answering mathematical queries. Overall, VideoVista-CulturalLingo serves as a comprehensive and inclusive benchmark for evaluating video comprehension capabilities of AI systems. 

<br /><br />Summary: <div>
arXiv:2504.17821v1 Announce Type: cross 
Abstract: Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval</title>
<link>https://arxiv.org/abs/2504.17884</link>
<guid>https://arxiv.org/abs/2504.17884</guid>
<content:encoded><![CDATA[
<div> Keywords: corpus poisoning, information retrieval, adversarial attacks, embedding space, unsupervised

Summary:
This paper introduces a novel approach to corpus poisoning attacks in dense information retrieval. By operating directly in the embedding space, the proposed optimization method maintains the geometric distance between original and adversarial documents while maximizing token-level dissimilarity. Unlike previous work, this method does not require prior knowledge about query distribution, making it more challenging and effective. Experimental results on various datasets show that the proposed method can generate successful adversarial examples quickly, outperforming gradient-based approaches in speed and naturalness. The generated text has low perplexity, making it harder to detect. The study focuses on both top-1 attacks and corpus poisoning attacks, considering white-box and black-box settings. In conclusion, the proposed adversarial corpus attack is fast, effective, and robust against detection methods. 

<br /><br />Summary: <div>
arXiv:2504.17884v1 Announce Type: cross 
Abstract: This paper concerns corpus poisoning attacks in dense information retrieval, where an adversary attempts to compromise the ranking performance of a search algorithm by injecting a small number of maliciously generated documents into the corpus. Our work addresses two limitations in the current literature. First, attacks that perform adversarial gradient-based word substitution search do so in the discrete lexical space, while retrieval itself happens in the continuous embedding space. We thus propose an optimization method that operates in the embedding space directly. Specifically, we train a perturbation model with the objective of maintaining the geometric distance between the original and adversarial document embeddings, while also maximizing the token-level dissimilarity between the original and adversarial documents. Second, it is common for related work to have a strong assumption that the adversary has prior knowledge about the queries. In this paper, we focus on a more challenging variant of the problem where the adversary assumes no prior knowledge about the query distribution (hence, unsupervised). Our core contribution is an adversarial corpus attack that is fast and effective. We present comprehensive experimental results on both in- and out-of-domain datasets, focusing on two related tasks: a top-1 attack and a corpus poisoning attack. We consider attacks under both a white-box and a black-box setting. Notably, our method can generate successful adversarial examples in under two minutes per target document; four times faster compared to the fastest gradient-based word substitution methods in the literature with the same hardware. Furthermore, our adversarial generation method generates text that is more likely to occur under the distribution of natural text (low perplexity), and is therefore more difficult to detect.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Sequence Compression for Efficient Multimodal Computing</title>
<link>https://arxiv.org/abs/2504.17892</link>
<guid>https://arxiv.org/abs/2504.17892</guid>
<content:encoded><![CDATA[
<div> vision language models, visual token selection, cluster-level token aggregation, vision encoder redundancy, multimodal systems <br />
<br />Summary: 
This article focuses on improving the efficiency of visual language models by addressing redundancy and inefficiencies in current vision encoders. The research explores various approaches to visual token selection and merging, and finds that simple cluster-level token aggregation outperforms previous methods. The study highlights the redundancy in existing vision encoders and identifies trends in visual token selection through cross-modal attention visualizations. By developing more effective methods for encoding and processing high-dimensional data, the research aims to create more scalable and sustainable multimodal systems. This work represents a significant step towards enhancing cross-modal reasoning in Large Multimodal Models and offers insights into improving the performance of visual language models. <div>
arXiv:2504.17892v1 Announce Type: cross 
Abstract: The exponential growth of Large Multimodal Models (LMMs) has driven advancements in cross-modal reasoning but at significant computational costs. In this work, we focus on visual language models. We highlight the redundancy and inefficiency in current vision encoders, and seek to construct an adaptive compression method for multimodal data. In this work, we characterize a panoply of visual token selection and merging approaches through both benchmarking and qualitative analysis. In particular, we demonstrate that simple cluster-level token aggregation outperforms prior state-of-the-art works in token selection and merging, including merging at the vision encoder level and attention-based approaches. We underline the redundancy in current vision encoders, and shed light on several puzzling trends regarding principles of visual token selection through cross-modal attention visualizations. This work is a first effort towards more effective encoding and processing of high-dimensional data, and paves the way for more scalable and sustainable multimodal systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMU: Context Augmentation for Meme Understanding</title>
<link>https://arxiv.org/abs/2504.17902</link>
<guid>https://arxiv.org/abs/2504.17902</guid>
<content:encoded><![CDATA[
<div> Framework, CAMU, hate detection, memes, vision-language models

Summary:
- The CAMU framework combines vision-language models to analyze social media memes for hate content.
- By fine-tuning CLIP's text encoder, CAMU achieves high accuracy and F1-score on the Hateful Memes dataset.
- Selectively tuning deeper text encoder layers improves hate detection performance significantly.
- CAMU also demonstrates effectiveness in identifying offensive memes on the MultiOFF dataset.
- Robust visual grounding and nuanced text representations are essential for reliable hate and offense detection.<br /><br />Summary: <div>
arXiv:2504.17902v1 Announce Type: cross 
Abstract: Social media memes are a challenging domain for hate detection because they intertwine visual and textual cues into culturally nuanced messages. We introduce a novel framework, CAMU, which leverages large vision-language models to generate more descriptive captions, a caption-scoring neural network to emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's text encoder for an improved multimodal understanding of memes. Experiments on publicly available hateful meme datasets show that simple projection layer fine-tuning yields modest gains, whereas selectively tuning deeper text encoder layers significantly boosts performance on all evaluation metrics. Moreover, our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful Memes dataset, at par with the existing SoTA framework while being much more efficient, offering practical advantages in real-world scenarios that rely on fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the MultiOFF dataset for offensive meme identification, demonstrating its generalisability. Additional analyses on benign confounders reveal that robust visual grounding and nuanced text representations are crucial for reliable hate and offence detection. We will publicly release CAMU along with the resultant models for further research.
  Disclaimer: This paper includes references to potentially disturbing, hateful, or offensive content due to the nature of the task.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents</title>
<link>https://arxiv.org/abs/2504.17934</link>
<guid>https://arxiv.org/abs/2504.17934</guid>
<content:encoded><![CDATA[
<div> Large Language Models, GUI automation, privacy risks, security risks, evaluation metrics 
Summary: 
Large Language Models (LLMs) have transformed Graphical User Interface (GUI) automation but raise significant privacy and security risks due to their ability to process sensitive data with limited human oversight. This paper outlines the three key risks of GUI agents, highlighting the differences from traditional GUI automation and general autonomous agents. Existing evaluations mainly focus on performance, neglecting privacy and security assessments. The paper reviews current evaluation metrics for both GUI and general LLM agents and discusses challenges in integrating human evaluators for GUI agent assessments. To address these gaps, a human-centered evaluation framework is proposed, advocating for risk assessments, user awareness through in-context consent, and embedding privacy and security considerations into GUI agent design and evaluation. <br /><br />Summary: <div>
arXiv:2504.17934v1 Announce Type: cross 
Abstract: The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning</title>
<link>https://arxiv.org/abs/2504.17950</link>
<guid>https://arxiv.org/abs/2504.17950</guid>
<content:encoded><![CDATA[
<div> Keywords: Collaboration, LLMs, Embodied reasoning tasks, Natural language communication, Multi-agent collaboration

Summary: 
LLMs are studied for adaptive collaboration in complex embodied reasoning tasks using the MINDcraft platform in Minecraft. The MineCollab benchmark tests embodied and collaborative reasoning dimensions. An experimental study reveals the challenge of efficient natural language communication, with a 15% drop in agent performance when detailed task completion plans are communicated. Current LLM agents are not optimized for multi-agent collaboration in embodied scenarios, indicating the need for methods beyond in-context and imitation learning.<br /><br />Summary: <div>
arXiv:2504.17950v1 Announce Type: cross 
Abstract: Collaboration is ubiquitous and essential in day-to-day life -- from exchanging ideas, to delegating tasks, to generating plans together. This work studies how LLMs can adaptively collaborate to perform complex embodied reasoning tasks. To this end we introduce MINDcraft, an easily extensible platform built to enable LLM agents to control characters in the open-world game of Minecraft; and MineCollab, a benchmark to test the different dimensions of embodied and collaborative reasoning. An experimental study finds that the primary bottleneck in collaborating effectively for current state-of-the-art agents is efficient natural language communication, with agent performance dropping as much as 15% when they are required to communicate detailed task completion plans. We conclude that existing LLM agents are ill-optimized for multi-agent collaboration, especially in embodied scenarios, and highlight the need to employ methods beyond in-context and imitation learning. Our website can be found here: https://mindcraft-minecollab.github.io/
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTFinRAG: Interactive Modularized Financial RAG Benchmark</title>
<link>https://arxiv.org/abs/2504.18024</link>
<guid>https://arxiv.org/abs/2504.18024</guid>
<content:encoded><![CDATA[
<div> architecture, evaluation, financial, language models, SMARTFinRAG

Summary:
SMARTFinRAG introduces a modular architecture for evaluating specialized RAG systems in the financial sector, addressing key gaps. It allows components to be interchanged dynamically, employs a document-centric evaluation approach, and features an intuitive interface. Evaluation results show variations in retrieval efficacy and response quality across configurations. The open-source platform supports transparent, reproducible research and helps financial institutions tackle deployment challenges when implementing RAG systems. <div>
arXiv:2504.18024v1 Announce Type: cross 
Abstract: Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture</title>
<link>https://arxiv.org/abs/2504.18099</link>
<guid>https://arxiv.org/abs/2504.18099</guid>
<content:encoded><![CDATA[
<div> Keywords: speech production, tongue articulatory features, lip articulatory features, BiLSTM architecture, Convolutional Neural Network

Summary: 
The paper presents a novel approach for predicting tongue and lip articulatory features in speech production using a stacked BiLSTM architecture combined with a one-dimensional CNN. The model is trained on datasets with variations in geographical origin, linguistic characteristics, phonetic diversity, and recording equipment. Performance evaluation was conducted in Speaker Dependent, Speaker Independent, corpus dependent, and cross corpus modes. The fixed weights approach outperformed adaptive weights initialization in a minimal number of training epochs. These findings contribute to the development of efficient models for predicting articulatory features, offering potential advancements in speech production research and applications.
<br /><br />Summary: <div>
arXiv:2504.18099v1 Announce Type: cross 
Abstract: Speech production is a complex sequential process which involve the coordination of various articulatory features. Among them tongue being a highly versatile active articulator responsible for shaping airflow to produce targeted speech sounds that are intellectual, clear, and distinct. This paper presents a novel approach for predicting tongue and lip articulatory features involved in a given speech acoustics using a stacked Bidirectional Long Short-Term Memory (BiLSTM) architecture, combined with a one-dimensional Convolutional Neural Network (CNN) for post-processing with fixed weights initialization. The proposed network is trained with two datasets consisting of simultaneously recorded speech and Electromagnetic Articulography (EMA) datasets, each introducing variations in terms of geographical origin, linguistic characteristics, phonetic diversity, and recording equipment. The performance of the model is assessed in Speaker Dependent (SD), Speaker Independent (SI), corpus dependent (CD) and cross corpus (CC) modes. Experimental results indicate that the proposed model with fixed weights approach outperformed the adaptive weights initialization with in relatively minimal number of training epochs. These findings contribute to the development of robust and efficient models for articulatory feature prediction, paving the way for advancements in speech production research and applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections</title>
<link>https://arxiv.org/abs/2504.18333</link>
<guid>https://arxiv.org/abs/2504.18333</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, prompt injection attacks, framework, text quality, code correctness.

Summary:
LLM judge systems used for evaluating text quality, code correctness, and argument strength are susceptible to prompt injection attacks. This study introduces a framework to distinguish between content author attacks and system prompt attacks. Five different models  Gemma 3.27B, Gemma 3.4B, Llama 3.23B, GPT 4, and Claude 3 Opus  were evaluated on four tasks with various defenses involving fifty prompts per condition. The attacks demonstrated success rates of up to 73.8%, with smaller models being more vulnerable. Transferability ranged from 50.5% to 62.6%. These results contradict previous research findings. The study recommends the use of multi-model committees and comparative scoring strategies. All code and datasets used in the study have been released for further analysis and experimentation. 

Summary:<br /><br />Keywords: LLM, prompt injection attacks, framework, text quality, code correctness, multi-model committees. The study examines the vulnerability of LLM judge systems to prompt injection attacks and proposes a framework to address these issues. Evaluation of five models on various tasks reveals differing levels of susceptibility, with smaller models being more vulnerable. The study recommends the use of multi-model committees and comparative scoring for improved defense against prompt injection attacks. Availability of code and datasets for further research and analysis is highlighted. <div>
arXiv:2504.18333v1 Announce Type: cross 
Abstract: LLM as judge systems used to assess text quality code correctness and argument strength are vulnerable to prompt injection attacks. We introduce a framework that separates content author attacks from system prompt attacks and evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3 Opus on four tasks with various defenses using fifty prompts per condition. Attacks achieved up to seventy three point eight percent success smaller models proved more vulnerable and transferability ranged from fifty point five to sixty two point six percent. Our results contrast with Universal Prompt Injection and AdvPrompter We recommend multi model committees and comparative scoring and release all code and datasets
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi-Audio Technical Report</title>
<link>https://arxiv.org/abs/2504.18425</link>
<guid>https://arxiv.org/abs/2504.18425</guid>
<content:encoded><![CDATA[
<div> tokenizer, LLM-based architecture, pre-training dataset, audio benchmarks, MoonshotAI-Kimi-Audio repository

Summary:
The article introduces Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. It details the practices involved in building Kimi-Audio, including utilizing a 12.5Hz audio tokenizer, designing a novel LLM-based architecture with continuous features as input and discrete tokens as output, and developing a chunk-wise streaming detokenizer. The model is pre-trained on a dataset consisting of over 13 million hours of audio data and then fine-tuned to support various audio-related tasks. Kimi-Audio achieves state-of-the-art performance on audio benchmarks such as speech recognition, audio understanding, audio question answering, and speech conversation. The codes, model checkpoints, and evaluation toolkits are released on the MoonshotAI-Kimi-Audio repository. <div>
arXiv:2504.18425v1 Announce Type: cross 
Abstract: We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRobELM: Plausibility Ranking Evaluation for Language Models</title>
<link>https://arxiv.org/abs/2404.03818</link>
<guid>https://arxiv.org/abs/2404.03818</guid>
<content:encoded><![CDATA[
<div> Benchmark, Language models, Plausibility, World knowledge, Evaluation <br />
Summary: <br />
PRobELM is a new benchmark designed to assess language models' ability to distinguish between plausible and less plausible scenarios by leveraging world knowledge. While existing benchmarks focus on factual accuracy or plausible scenarios without incorporating world knowledge, PRobELM bridges this gap by evaluating the models' capability to prioritize plausible scenarios that align with world knowledge. The benchmark is constructed from a dataset curated from Wikidata edit histories and includes multiple prompting types such as statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures demonstrate that factual accuracy does not directly correlate with plausibility performance, and up-to-date training data enhances plausibility assessment across different model architectures. This benchmark can be valuable for applications such as literature-based discovery where identifying likely but unknown information is crucial. <br /> <div>
arXiv:2404.03818v4 Announce Type: replace 
Abstract: This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models' capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our benchmark is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple prompting types, including statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</title>
<link>https://arxiv.org/abs/2405.19325</link>
<guid>https://arxiv.org/abs/2405.19325</guid>
<content:encoded><![CDATA[
<div> nearest neighbor speculative decoding, semi-parametric language modeling, generation quality, attribution rate, inference speed
Summary:
Nearest Neighbor Speculative Decoding (NEST) is a novel semi-parametric language modeling approach that enhances generation quality and attribution rate of large language models (LLMs). By incorporating real-world text spans of arbitrary length into the LM generations, NEST outperforms conventional kNN-LM methods and competes with in-context retrieval augmentation. NEST employs token-level retrieval and a speculative decoding procedure to generate fluent texts with attributions to their sources. It significantly improves generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B dataset. The code for NEST will be available at https://github.com/facebookresearch/NEST/tree/main. 
<br /><br />Summary: <div>
arXiv:2405.19325v3 Announce Type: replace 
Abstract: Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B. Code will be released at https://github.com/facebookresearch/NEST/tree/main.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction</title>
<link>https://arxiv.org/abs/2406.10432</link>
<guid>https://arxiv.org/abs/2406.10432</guid>
<content:encoded><![CDATA[
<div> retrieve, in-context learning, relation extraction, semantic structure, state-of-the-art

Summary:
The article introduces an AMR-enhanced retrieval-based in-context learning (ICL) method for relation extraction (RE) that prioritizes structural similarity over language similarity. By retrieving examples based on semantic structure similarity, the proposed model outperforms existing methods in unsupervised settings across four standard English RE datasets. In supervised settings, the model achieves state-of-the-art results on three datasets and competitive results on the fourth. This approach highlights the importance of considering semantic structure in learning entity relationships and demonstrates the effectiveness of leveraging such information for improving RE performance. <div>
arXiv:2406.10432v3 Announce Type: replace 
Abstract: Existing in-context learning (ICL) methods for relation extraction (RE) often prioritize language similarity over structural similarity, which can lead to overlooking entity relationships. To address this, we propose an AMR-enhanced retrieval-based ICL method for RE. Our model retrieves in-context examples based on semantic structure similarity between task inputs and training samples. Evaluations on four standard English RE datasets show that our model outperforms baselines in the unsupervised setting across all datasets. In the supervised setting, it achieves state-of-the-art results on three datasets and competitive results on the fourth.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Large Language Models and Curse of Multilinguality</title>
<link>https://arxiv.org/abs/2406.10602</link>
<guid>https://arxiv.org/abs/2406.10602</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual Large Language Models, NLP, architectures, tokenization, curse of multilinguality

Summary:
Multilingual Large Language Models (LLMs) are widely used in Natural Language Processing (NLP) due to their proficiency in multiple languages and effectiveness in various tasks. This paper provides an overview of the technical aspects of these models, including architectures, objective functions, pre-training data sources, and tokenization methods. It discusses different model types such as encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART), highlighting their unique features. One significant limitation of multilingual LLMs is the curse of multilinguality, and current efforts to address this challenge are explored.Overall, this paper serves as a comprehensive guide to understand the landscape of multilingual LLMs and their potential in NLP research and applications.<br /><br />Summary: <div>
arXiv:2406.10602v2 Announce Type: replace 
Abstract: Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings</title>
<link>https://arxiv.org/abs/2408.16073</link>
<guid>https://arxiv.org/abs/2408.16073</guid>
<content:encoded><![CDATA[
<div> large language models, replication, generalization, marketing, AI-assisted

Summary: 
This report examines the use of large language models (LLMs) to replicate and generalize research on message effects in marketing. LLM-powered personas successfully replicated 76% of original main effects and 68% including interaction effects from 133 experimental findings across 14 papers. Results suggest strong potential for AI-assisted replication in marketing research. The study also found that replication results can vary when tested with different participant samples, media stimuli, and measures, highlighting the importance of considering these factors in research. Implications are discussed for addressing replication and generalizability crises in social science, accelerating theory building in media and marketing psychology, and the practical benefits of rapid message testing for consumer products. Limitations of AI replications, such as handling complex interaction effects and biases in AI models, are also addressed to establish benchmarks for AI metrics in marketing research. <div>
arXiv:2408.16073v2 Announce Type: replace 
Abstract: This report analyzes the potential for large language models (LLMs) to expedite accurate replication and generalization of published research about message effects in marketing. LLM-powered participants (personas) were tested by replicating 133 experimental findings from 14 papers containing 45 recent studies published in the Journal of Marketing. For each study, the measures, stimuli, and sampling specifications were used to generate prompts for LLMs to act as unique personas. The AI personas, 19,447 in total across all of the studies, generated complete datasets and statistical analyses were then compared with the original human study results. The LLM replications successfully reproduced 76% of the original main effects (84 out of 111), demonstrating strong potential for AI-assisted replication. The overall replication rate including interaction effects was 68% (90 out of 133). Furthermore, a test of how human results generalized to different participant samples, media stimuli, and measures showed that replication results can change when tests go beyond the parameters of the original human studies. Implications are discussed for the replication and generalizability crises in social science, the acceleration of theory building in media and marketing psychology, and the practical advantages of rapid message testing for consumer products. Limitations of AI replications are addressed with respect to complex interaction effects, biases in AI models, and establishing benchmarks for AI metrics in marketing research.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Weak LLM is Secretly a Strong Teacher for Alignment</title>
<link>https://arxiv.org/abs/2409.08813</link>
<guid>https://arxiv.org/abs/2409.08813</guid>
<content:encoded><![CDATA[
<div> alignment, large language models, weak LLM, feedback, human values

Summary:
The study focuses on aligning large language models (LLMs) with human values using a less resource-intensive weak LLM approach. The research demonstrates the efficacy of weak LLMs in providing feedback comparable to human annotations, highlighting a scalable alignment strategy. Through qualitative and quantitative analyses, the study uncovers insights into the quality differences between human and weak LLM feedback, emphasizing the potential for sustainable alignment practices. These findings contribute to addressing the crucial need for ensuring LLMs act in accordance with human intentions, offering a promising middle ground approach for alignment efforts. 

<br /><br />Summary: <div>
arXiv:2409.08813v2 Announce Type: replace 
Abstract: The burgeoning capabilities of large language models (LLMs) have underscored the need for alignment to ensure these models act in accordance with human values and intentions. Existing alignment frameworks present constraints either in the form of expensive human effort or high computational costs. This paper explores a promising middle ground, where we employ a weak LLM that is significantly less resource-intensive than top-tier models, yet offers more automation than purely human feedback. We present a systematic study to evaluate and understand weak LLM's ability to generate feedback for alignment. Our empirical findings demonstrate that weak LLMs can provide feedback that rivals or even exceeds that of fully human-annotated data. Our study indicates a minimized impact of model size on feedback efficacy, shedding light on a scalable and sustainable alignment strategy. To deepen our understanding of alignment under weak LLM feedback, we conduct a series of qualitative and quantitative analyses, offering novel insights into the quality discrepancies between human feedback vs. weak LLM feedback.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning</title>
<link>https://arxiv.org/abs/2409.12059</link>
<guid>https://arxiv.org/abs/2409.12059</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, thinking ability, TaS, cognitive mechanism, data-driven

Summary: 
TaS is a novel model architecture designed to enhance the thinking ability of language models by incorporating cognitive mechanisms from the natural world. This model first considers thoughts before generating responses based on queries, utilizing language heads in a middle layer as the thinking layer. By annotating or generating thought contents from prompt-response samples, the model is trained on thoughts-augmented data to automatically generate reasonable thoughts and improve response quality. Both qualitative examples and quantitative results demonstrate the effectiveness and performance of TaS in enhancing the reasoning capabilities of language models. This approach represents a significant step towards improving the overall understanding and generation capabilities of large language models. The code for TaS is available for further exploration and experimentation. 

Summary: <div>
arXiv:2409.12059v4 Announce Type: replace 
Abstract: Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at https://anonymous.4open.science/r/TadE.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"</title>
<link>https://arxiv.org/abs/2410.03727</link>
<guid>https://arxiv.org/abs/2410.03727</guid>
<content:encoded><![CDATA[
<div> Benchmark, faithfulness, large language models, retrieval-augmented generation, context  
Summary:  
Faithfulness to context is critical for the deployment of large language models (LLMs) and retrieval-augmented generation (RAG) systems in real-world applications. Despite progress on standard benchmarks, faithfulness hallucination, where models generate responses not aligned with the provided context, remains a significant challenge. The introduction of FaithEval, a comprehensive benchmark, evaluates the faithfulness of LLMs across unanswerable, inconsistent, and counterfactual contexts. The benchmark comprises 4.9K high-quality problems validated through a context construction and validation framework. Results from the study on various models indicate that even state-of-the-art models struggle with remaining faithful to the given context, suggesting that larger models may not necessarily exhibit improved faithfulness.<br /><br />Summary: <div>
arXiv:2410.03727v3 Announce Type: replace 
Abstract: Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination-where models generate responses misaligned with the provided context-remains a significant challenge. In this work, we introduce FaithEval, a novel and comprehensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information. FaithEval comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM-based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved faithfulness.Project is available at: https://github.com/SalesforceAIResearch/FaithEval.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models</title>
<link>https://arxiv.org/abs/2411.07611</link>
<guid>https://arxiv.org/abs/2411.07611</guid>
<content:encoded><![CDATA[
arXiv:2411.07611v2 Announce Type: replace 
Abstract: Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable mutlimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in a same encoding space, enabling it naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data</title>
<link>https://arxiv.org/abs/2412.11704</link>
<guid>https://arxiv.org/abs/2412.11704</guid>
<content:encoded><![CDATA[
arXiv:2412.11704v3 Announce Type: replace 
Abstract: Vocabulary expansion (VE) is the de-facto approach to language adaptation of large language models (LLMs) by adding new tokens and continuing pre-training on target data. While this is effective for base models trained on unlabeled data, it poses challenges for chat models trained to follow instructions through labeled conversation data. Directly adapting the latter with VE on target unlabeled data may result in forgetting chat abilities. While ideal, target chat data is often unavailable or costly to create for low-resource languages, and machine-translated alternatives are not always effective. To address this issue, previous work proposed using a base and chat model from the same family. This method first adapts the base LLM with VE on target unlabeled data and then converts it to a chat model by adding a chat vector (CV) derived from the weight difference between the source base and chat models. We propose ElChat, a new language adaptation method for chat LLMs that adapts a chat model directly on target unlabeled data, without a base model. It elicits chat abilities by injecting information from the source chat model. ElChat offers more robust and competitive target language and safety performance while achieving superior English, chat, and instruction-following abilities compared to CV.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations</title>
<link>https://arxiv.org/abs/2502.01220</link>
<guid>https://arxiv.org/abs/2502.01220</guid>
<content:encoded><![CDATA[
arXiv:2502.01220v3 Announce Type: replace 
Abstract: This paper explores the robustness of language models (LMs) to variations in the temporal context within factual knowledge. It examines whether LMs can correctly associate a temporal context with a past fact valid over a defined period, by asking them to differentiate correct from incorrect contexts. The accuracy of LMs is analyzed along two dimensions: the distance of the incorrect context from the validity period and the granularity of the context. To this end, a dataset called TimeStress is introduced, enabling the evaluation of 18 diverse LMs. Results reveal that the best LM achieves perfect accuracy for only 6% of the studied facts, with critical errors that humans would not make. This work highlights the limitations of current LMs in temporal representation. We provide all data and code for further research.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.12486</link>
<guid>https://arxiv.org/abs/2502.12486</guid>
<content:encoded><![CDATA[
arXiv:2502.12486v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-generated text detection prevents language model collapse</title>
<link>https://arxiv.org/abs/2502.15654</link>
<guid>https://arxiv.org/abs/2502.15654</guid>
<content:encoded><![CDATA[
arXiv:2502.15654v4 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the characteristics of text at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present. We release our code at https://github.com/GeorgeDrayson/model_collapse.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LRAGE: Legal Retrieval Augmented Generation Evaluation Tool</title>
<link>https://arxiv.org/abs/2504.01840</link>
<guid>https://arxiv.org/abs/2504.01840</guid>
<content:encoded><![CDATA[
arXiv:2504.01840v2 Announce Type: replace 
Abstract: Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at https://github.com/hoorangyee/LRAGE.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Evaluation of Complex Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.02810</link>
<guid>https://arxiv.org/abs/2504.02810</guid>
<content:encoded><![CDATA[
arXiv:2504.02810v2 Announce Type: replace 
Abstract: With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Reasoning LLMs Enhance Clinical Document Classification?</title>
<link>https://arxiv.org/abs/2504.08040</link>
<guid>https://arxiv.org/abs/2504.08040</guid>
<content:encoded><![CDATA[
arXiv:2504.08040v2 Announce Type: replace 
Abstract: Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio</title>
<link>https://arxiv.org/abs/1711.08058</link>
<guid>https://arxiv.org/abs/1711.08058</guid>
<content:encoded><![CDATA[
arXiv:1711.08058v2 Announce Type: replace-cross 
Abstract: We propose using cascaded classifiers for a keyword spotting (KWS) task on narrow-band (NB), 8kHz audio acquired in non-IID environments -- a more challenging task than most state-of-the-art KWS systems face. We present a model that incorporates Deep Neural Networks (DNNs), cascading, multiple-feature representations, and multiple-instance learning. The cascaded classifiers handle the task's class imbalance and reduce power consumption on computationally-constrained devices via early termination. The KWS system achieves a false negative rate of 6% at an hourly false positive rate of 0.75
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2405.15638</link>
<guid>https://arxiv.org/abs/2405.15638</guid>
<content:encoded><![CDATA[
arXiv:2405.15638v2 Announce Type: replace-cross 
Abstract: Multilingual capability is an essential aspect for large multimodal models, since they are usually deployed across various countries and languages. However, most existing benchmarks for multilingual multimodal reasoning struggle to differentiate between models of varying performance; even language models without visual capabilities can easily achieve high scores. This leaves a comprehensive evaluation of leading multilingual multimodal models largely unexplored. In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning. M4U contains 10k samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in six languages. Using M4U, we conduct extensive evaluations of leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools. The evaluation results demonstrate that the state-of-the-art model, GPT-4o, achieves only 47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs exhibit significant language preferences. Our in-depth analysis indicates that leading LMMs, including GPT-4o, struggle to perform reasoning using multilingual information present in both visual and textual context. Specifically, they suffer performance degradation when prompted with cross-lingual multimodal questions. Our code and dataset is public available.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition</title>
<link>https://arxiv.org/abs/2406.02566</link>
<guid>https://arxiv.org/abs/2406.02566</guid>
<content:encoded><![CDATA[
arXiv:2406.02566v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel two-stage active learning (AL) pipeline for automatic speech recognition (ASR), combining unsupervised and supervised AL methods. The first stage utilizes unsupervised AL by using x-vectors clustering for diverse sample selection from unlabeled speech data, thus establishing a robust initial dataset for the subsequent supervised AL. The second stage incorporates a supervised AL strategy, with a batch AL method specifically developed for ASR, aimed at selecting diverse and informative batches of samples. Here, sample diversity is also achieved using x-vectors clustering, while the most informative samples are identified using a Bayesian AL method tailored for ASR with an adaptation of Monte Carlo dropout to approximate Bayesian inference. This approach enables precise uncertainty estimation, thereby enhancing ASR model training with significantly reduced data requirements. Our method has shown superior performance compared to competing methods on homogeneous, heterogeneous, and OOD test sets, demonstrating that strategic sample selection and innovative Bayesian modeling can substantially optimize both labeling effort and data utilization in deep learning-based ASR applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GOFA: A Generative One-For-All Model for Joint Graph Language Modeling</title>
<link>https://arxiv.org/abs/2407.09709</link>
<guid>https://arxiv.org/abs/2407.09709</guid>
<content:encoded><![CDATA[
arXiv:2407.09709v2 Announce Type: replace-cross 
Abstract: Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better -- yet, no existing work can achieve both simultaneously. In this paper, we identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA to solve the problem. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, and structural tasks to obtain the above GFM properties. The pre-trained model is further fine-tuned on downstream tasks to obtain task-solving ability. The fine-tuned model is evaluated on various downstream tasks, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs</title>
<link>https://arxiv.org/abs/2408.06621</link>
<guid>https://arxiv.org/abs/2408.06621</guid>
<content:encoded><![CDATA[
arXiv:2408.06621v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Uncertainty Quantification for Generative AI</title>
<link>https://arxiv.org/abs/2408.08990</link>
<guid>https://arxiv.org/abs/2408.08990</guid>
<content:encoded><![CDATA[
arXiv:2408.08990v2 Announce Type: replace-cross 
Abstract: This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large probability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within-group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-4o predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as predicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIND: Math Informed syNthetic Dialogues for Pretraining LLMs</title>
<link>https://arxiv.org/abs/2410.12881</link>
<guid>https://arxiv.org/abs/2410.12881</guid>
<content:encoded><![CDATA[
arXiv:2410.12881v2 Announce Type: replace-cross 
Abstract: The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed syNthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMs. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING: +2.51%).
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification</title>
<link>https://arxiv.org/abs/2411.01841</link>
<guid>https://arxiv.org/abs/2411.01841</guid>
<content:encoded><![CDATA[
arXiv:2411.01841v3 Announce Type: replace-cross 
Abstract: Accurate annotation of educational resources is crucial for effective personalized learning and resource recommendation in online education. However, fine-grained knowledge labels often overlap or share similarities, making it difficult for existing multi-label classification methods to differentiate them. The label distribution imbalance due to sparsity of human annotations further intensifies these challenges. To address these issues, this paper introduces RR2QC, a novel Retrieval Reranking method to multi-label Question Classification by leveraging label semantics and meta-label refinement. First, RR2QC improves the pre-training strategy by utilizing semantic relationships within and across label groups. Second, it introduces a class center learning task to align questions with label semantics during downstream training. Finally, this method decomposes labels into meta-labels and uses a meta-label classifier to rerank the retrieved label sequences. In doing so, RR2QC enhances the understanding and prediction capability of long-tail labels by learning from meta-labels that frequently appear in other labels. Additionally, a mathematical LLM is used to generate solutions for questions, extracting latent information to further refine the model's insights. Experimental results show that RR2QC outperforms existing methods in Precision@K and F1 scores across multiple educational datasets, demonstrating its effectiveness for online education applications. The code and datasets are available at https://github.com/78Erii/RR2QC.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repurposing the scientific literature with vision-language models</title>
<link>https://arxiv.org/abs/2502.19546</link>
<guid>https://arxiv.org/abs/2502.19546</guid>
<content:encoded><![CDATA[
arXiv:2502.19546v2 Announce Type: replace-cross 
Abstract: Leading vision-language models (VLMs) are trained on general Internet content, overlooking scientific journals' rich, domain-specific knowledge. Training on specialty-specific literature could yield high-performance, task-specific tools, enabling generative AI to match generalist models in specialty publishing, educational, and clinical tasks. We created NeuroPubs, a multimodal dataset of 23,000 Neurosurgery Publications articles (134M words, 78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready graphical abstracts (70% of 100 abstracts) and board-style questions indistinguishable from human-written ones (54% of 89,587 questions). We used these questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded, randomized controlled trial, our model demonstrated non-inferiority to then state-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical utility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%, p=0.3797). Our pilot study demonstrates how training generative AI models on specialty-specific journal content - without large-scale internet data - results in high-performance academic and clinical tools, enabling domain-tailored AI across diverse fields.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Audio Processing with Large Language Model on Wearable Devices</title>
<link>https://arxiv.org/abs/2504.08907</link>
<guid>https://arxiv.org/abs/2504.08907</guid>
<content:encoded><![CDATA[
arXiv:2504.08907v2 Announce Type: replace-cross 
Abstract: Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware and adaptive applications for wearable technologies. Our approach leverages microstructure-based spatial sensing to extract precise Direction of Arrival (DoA) information using a monaural microphone. To address the lack of existing dataset for microstructure-assisted speech recordings, we synthetically create a dataset called OmniTalk by using the LibriSpeech dataset. This spatial information is fused with linguistic embeddings from OpenAI's Whisper model, allowing each modality to learn complementary contextual representations. The fused embeddings are aligned with the input space of LLaMA-3.2 3B model and fine-tuned with lightweight adaptation technique LoRA to optimize for on-device processing. SING supports spatially-aware automatic speech recognition (ASR), achieving a mean error of $25.72^\circ$-a substantial improvement compared to the 88.52$^\circ$ median error in existing work-with a word error rate (WER) of 5.3. SING also supports soundscaping, for example, inference how many people were talking and their directions, with up to 5 people and a median DoA error of 16$^\circ$. Our system demonstrates superior performance in spatial speech understanding while addressing the challenges of power efficiency, privacy, and hardware constraints, paving the way for advanced applications in augmented reality, accessibility, and immersive experiences.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity</title>
<link>https://arxiv.org/abs/2504.16956</link>
<guid>https://arxiv.org/abs/2504.16956</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, GeneMamba, single-cell RNA sequencing, state space modeling, computational challenges<br />
Summary:<br />
GeneMamba is introduced as a scalable and efficient foundation model for single-cell transcriptomics, addressing the complexity of high dimensionality, sparsity, and batch effects. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity. Pretrained on nearly 30 million cells with biologically informed objectives, such as pathway-aware contrastive loss and rank-based gene encoding, GeneMamba demonstrates strong performance across tasks such as multi-batch integration, cell type annotation, and gene-gene correlation. It offers substantial computational gains over transformer baselines while providing interpretability and robustness, positioning it as a practical and powerful alternative for large-scale single-cell data analysis. <br /> <div>
arXiv:2504.16956v1 Announce Type: new 
Abstract: Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenization Matters: Improving Zero-Shot NER for Indic Languages</title>
<link>https://arxiv.org/abs/2504.16977</link>
<guid>https://arxiv.org/abs/2504.16977</guid>
<content:encoded><![CDATA[
<div> tokenization, Natural Language Processing, Named Entity Recognition, Indic languages, SentencePiece

Summary: 
- The study compares tokenization strategies BPE, SentencePiece, and Character Level for NER in low resource Indic languages.
- SentencePiece outperforms BPE in NER tasks, especially in zero-shot cross-lingual settings, as it maintains entity consistency better.
- BPE offers compact tokenization but fails in generalization, misclassifying entity labels in unseen languages.
- SentencePiece better preserves linguistic structure and is effective for extremely low resource and morphologically rich Indic languages.
- SentencePiece also shows high generalization across scripts, such as Sindhi written in Arabic, making it the more effective tokenization strategy for NER in multilingual and low resource Indic NLP applications.

<br /><br />Summary: <div>
arXiv:2504.16977v1 Announce Type: new 
Abstract: Tokenization is a critical component of Natural Language Processing (NLP), especially for low resource languages, where subword segmentation influences vocabulary structure and downstream task accuracy. Although Byte Pair Encoding (BPE) is a standard tokenization method in multilingual language models, its suitability for Named Entity Recognition (NER) in low resource Indic languages remains underexplored due to its limitations in handling morphological complexity. In this work, we systematically compare BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT for NER tasks in low resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We assess both intrinsic linguistic properties tokenization efficiency, out of vocabulary (OOV) rates, and morphological preservation as well as extrinsic downstream performance, including fine tuning and zero shot cross lingual transfer.
  Our experiments show that SentencePiece is a consistently better performing approach than BPE for NER in low resource Indic Languages, particularly in zero shot cross lingual settings, as it better preserves entity consistency. While BPE provides the most compact tokenization form, it is not capable of generalization because it misclassifies or even fails to recognize entity labels when tested on unseen languages. In contrast, SentencePiece constitutes a better linguistic structural preservation model, benefiting extremely low resource and morphologically rich Indic languages, such as Santali and Manipuri, for superior entity recognition, as well as high generalization across scripts, such as Sindhi, written in Arabic. The results point to SentencePiece as the more effective tokenization strategy for NER within multilingual and low resource Indic NLP applications.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation</title>
<link>https://arxiv.org/abs/2504.17025</link>
<guid>https://arxiv.org/abs/2504.17025</guid>
<content:encoded><![CDATA[
<div> adaptation, Large Language Models, Italian language, Semantic Alignment, vocabulary substitution

Summary:
- The study focuses on optimizing English Large Language Models (LLMs) for the Italian language.
- Various vocabulary adaptation techniques are compared, with Semantic Alignment Vocabulary Adaptation (SAVA) proposed as a novel method that utilizes neural mapping for vocabulary substitution.
- SAVA proves effective in reducing token fertility and improving model efficiency for Italian language tasks.
- Two LLMs, Mistral-7b-v0.1 and Llama-3.1-8B, are adapted using SAVA, resulting in decreased token fertility and parameter reduction.
- The adapted models show competitive performance on a range of downstream tasks with minimal additional training in the target language.

<br /><br />Summary: <div>
arXiv:2504.17025v1 Announce Type: new 
Abstract: The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models</title>
<link>https://arxiv.org/abs/2504.17052</link>
<guid>https://arxiv.org/abs/2504.17052</guid>
<content:encoded><![CDATA[
<div> framework, belief depth, argumentative consistency, uncertainty quantification, LLMs <br />
Summary: This study investigates the political beliefs of Large Language Models (LLMs) by assessing their argumentative consistency and uncertainty quantification. It challenges LLMs with both supportive and opposing arguments on 19 economic policies to evaluate their belief stability. The study finds that LLMs exhibit topic-specific belief stability rather than a uniform ideological stance. Left-leaning models show up to 95% consistency and right-leaning models up to 89% consistency in their responses, indicating a high level of semantic entropy in distinguishing surface-level alignment from genuine belief. The results suggest that LLMs may not maintain stable, human-like political ideologies and emphasize the importance of topic-specific reliability assessments for real-world applications. <br /> <div>
arXiv:2504.17052v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly shaping political discourse, yet their responses often display inconsistency when subjected to scrutiny. While prior research has primarily categorized LLM outputs as left- or right-leaning to assess their political stances, a critical question remains: Do these responses reflect genuine internal beliefs or merely surface-level alignment with training data? To address this, we propose a novel framework for evaluating belief depth by analyzing (1) argumentative consistency and (2) uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from the Political Compass Test, challenging their belief stability with both supportive and opposing arguments. Our analysis reveals that LLMs exhibit topic-specific belief stability rather than a uniform ideological stance. Notably, up to 95% of left-leaning models' responses and 89% of right-leaning models' responses remain consistent under the challenge, enabling semantic entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing between surface-level alignment from genuine belief. These findings call into question the assumption that LLMs maintain stable, human-like political ideologies, emphasizing the importance of conducting topic-specific reliability assessments for real-world applications.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agree to Disagree? A Meta-Evaluation of LLM Misgendering</title>
<link>https://arxiv.org/abs/2504.17075</link>
<guid>https://arxiv.org/abs/2504.17075</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM misgendering, evaluation methods, convergent validity, dataset transformation, human evaluation

Summary: 
The study explores various methods for measuring LLM misgendering and examines the convergent validity of these evaluation techniques. By analyzing three existing datasets and evaluating models from different families, the researchers find that probability-based and generation-based evaluation methods can disagree with each other on a significant portion of instances. Human evaluation of LLM generations highlights the complexity of misgendering behavior, extending beyond pronouns which are not always captured by automatic evaluations. The findings suggest the need for improved evaluation methods for LLM misgendering and call into question the assumption of agreement between different evaluation approaches. Recommendations are provided for future evaluations in this area, emphasizing the importance of considering the diverse aspects of misgendering behavior in LLM assessment.<br /><br />Summary: <div>
arXiv:2504.17075v1 Announce Type: new 
Abstract: Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering. We propose a method to transform each dataset to enable parallel probability- and generation-based evaluation. Then, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances. Finally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations. Based on our findings, we provide recommendations for future evaluations of LLM misgendering. Our results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study</title>
<link>https://arxiv.org/abs/2504.17083</link>
<guid>https://arxiv.org/abs/2504.17083</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, language style, user preferences, individual traits, user studies

Summary: 
LLM interactions are influenced by language style, not just information accuracy. Inaccurate responses can be preferred if they appear authoritative or well-articulated. This suggests language style plays a significant role in user preferences. However, the impact of language style varies among different user populations and is influenced by individual traits. The study's preliminary findings indicate a need for caution due to sample limitations, requiring more diverse demographics and larger samples. Future research aims to explore the joint effects of language style, individual traits, and preferences, as well as investigate any causal relationships between these variables. Overall, understanding how language style affects user interactions with LLMs can enhance user experience but also raise concerns about misinformation and hallucinations. <br /><br />Summary: <div>
arXiv:2504.17083v1 Announce Type: new 
Abstract: What makes an interaction with the LLM more preferable for the user? While it is intuitive to assume that information accuracy in the LLM's responses would be one of the influential variables, recent studies have found that inaccurate LLM's responses could still be preferable when they are perceived to be more authoritative, certain, well-articulated, or simply verbose. These variables interestingly fall under the broader category of language style, implying that the style in the LLM's responses might meaningfully influence users' preferences. This hypothesized dynamic could have double-edged consequences: enhancing the overall user experience while simultaneously increasing their susceptibility to risks such as LLM's misinformation or hallucinations. In this short paper, we present our preliminary studies in exploring this subject. Through a series of exploratory and experimental user studies, we found that LLM's language style does indeed influence user's preferences, but how and which language styles influence the preference varied across different user populations, and more interestingly, moderated by the user's very own individual traits. As a preliminary work, the findings in our studies should be interpreted with caution, particularly given the limitations in our samples, which still need wider demographic diversity and larger sample sizes. Our future directions will first aim to address these limitations, which would enable a more comprehensive joint effect analysis between the language style, individual traits, and preferences, and further investigate the potential causal relationship between and beyond these variables.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2504.17091</link>
<guid>https://arxiv.org/abs/2504.17091</guid>
<content:encoded><![CDATA[
<div> Framework, explainability, responsible AI, cognitive engagement, ethical transparency

Summary:
The Interactive Chain-of-Thought (CoT) Framework was introduced to enhance human-centered explainability and responsible AI usage by promoting deep, reflective thinking. The framework allows users to inspect, modify, and re-execute reasoning blocks, encouraging active cognitive engagement. It also incorporates a lightweight edit-adaptation mechanism to align with diverse cognitive styles and user intentions. Ethical transparency is ensured through explicit metadata disclosure, bias checkpoint functionality, and privacy-preserving safeguards. The design principles and architecture outlined in this work aim to foster critical engagement, responsible interaction, and inclusive adaptation in AI systems addressing complex societal challenges. <div>
arXiv:2504.17091v1 Announce Type: new 
Abstract: Due to the proliferation of short-form content and the rapid adoption of AI, opportunities for deep, reflective thinking have significantly diminished, undermining users' critical thinking and reducing engagement with the reasoning behind AI-generated outputs. To address this issue, we propose an Interactive Chain-of-Thought (CoT) Framework that enhances human-centered explainability and responsible AI usage by making the model's inference process transparent, modular, and user-editable. The framework decomposes reasoning into clearly defined blocks that users can inspect, modify, and re-execute, encouraging active cognitive engagement rather than passive consumption. It further integrates a lightweight edit-adaptation mechanism inspired by preference learning, allowing the system to align with diverse cognitive styles and user intentions. Ethical transparency is ensured through explicit metadata disclosure, built-in bias checkpoint functionality, and privacy-preserving safeguards. This work outlines the design principles and architecture necessary to promote critical engagement, responsible interaction, and inclusive adaptation in AI systems aimed at addressing complex societal challenges.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of Small Language Models in Healthcare: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2504.17119</link>
<guid>https://arxiv.org/abs/2504.17119</guid>
<content:encoded><![CDATA[
<div> Keywords: small language models, healthcare applications, data privacy, resource-constrained environments, model optimization<br />
Summary:<br />
Small language models (SLMs) offer a scalable and clinically viable solution for healthcare applications, addressing concerns around data privacy and resource limitations. A comprehensive survey categorizes SLMs for healthcare professionals, highlighting their contributions across NLP tasks and stakeholder roles. The timeline of SLM advancements showcases their potential in transforming healthcare informatics. The taxonomic framework guides the development of SLMs, emphasizing model optimization and sustainability through compression techniques. By presenting experimental results across various healthcare NLP tasks, this survey aims to equip professionals with curated resources for future research and development in the field. Access the updated repository on Github for a comprehensive compilation of SLM advancements in healthcare.<br /> 
Summary: <div>
arXiv:2504.17119v1 Announce Type: new 
Abstract: Despite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control</title>
<link>https://arxiv.org/abs/2504.17130</link>
<guid>https://arxiv.org/abs/2504.17130</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, censorship, representation engineering, safety-tuned models, thought suppression

Summary: 
Large language models (LLMs) have introduced new ways to access information but may censor harmful requests. A study using representation engineering techniques focuses on open-weights safety-tuned models to understand censorship. A method for detecting and controlling censorship through a refusal-compliance vector is presented. Analysis of reasoning LLMs reveals an additional dimension of censorship known as "thought suppression" in models distilled from DeepSeek-R1. A similar approach can be used to find a vector that suppresses the model's reasoning process, allowing for the removal of censorship by applying the negative multiples of this vector. <div>
arXiv:2504.17130v1 Announce Type: new 
Abstract: Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this "censorship" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through "thought suppression". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation</title>
<link>https://arxiv.org/abs/2504.17137</link>
<guid>https://arxiv.org/abs/2504.17137</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, Large Language Models, MIRAGE, Question Answering dataset, evaluation metrics <br />
Summary: <br />
The paper introduces MIRAGE, a new Question Answering dataset designed for evaluating Retrieval-Augmented Generation (RAG) systems. The dataset includes 7,560 instances mapped to a retrieval pool of 37,800 entries, allowing for precise evaluation of retrieval and generation tasks. Novel evaluation metrics like noise vulnerability, context acceptability, and context misinterpretation are introduced to measure RAG adaptability. The paper conducts comprehensive experiments with various retriever-LLM configurations to gain insights into optimal model alignment and dynamics within RAG systems. The dataset and evaluation code are made publicly available for seamless integration and customization in research contexts. <div>
arXiv:2504.17137v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, Large Language Models, code implementation, PaperCoder, benchmark 

Summary: 
The article introduced a new framework called PaperCoder that aims to transform machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, analysis, and generation, with specialized agents collaborating across the pipeline. It effectively generates high-quality, faithful implementations based on both model-based and human evaluations, including evaluations from original paper authors. The framework outperforms strong baselines in the PaperBench benchmark, showcasing its strengths in code generation from research papers. PaperCoder's innovative approach addresses the lack of code implementations accompanying machine learning research, making it easier for researchers to reproduce results and build upon prior work. Overall, PaperCoder shows promise in bridging the gap between research papers and practical code implementations, potentially accelerating progress in the machine learning field. 

<br /><br />Summary: <div>
arXiv:2504.17192v1 Announce Type: new 
Abstract: Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation</title>
<link>https://arxiv.org/abs/2504.17200</link>
<guid>https://arxiv.org/abs/2504.17200</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, natural hazards, extreme weather events, retrieval-augmented generation, WildfireGPT

Summary: 
The article introduces the concept of using Large Language Models (LLMs) for decision-making in the context of natural hazards, particularly focusing on extreme weather events such as wildfires. The proposed system, WildfireGPT, utilizes a retrieval-augmented generation (RAG) framework to provide tailored risk insights to various stakeholder groups. By integrating data from natural hazard projections, observational datasets, and scientific literature, WildfireGPT ensures the accuracy and contextual relevance of the information it provides. Through a user-centered, multi-agent design, the system outperforms existing LLM-based solutions for decision support, as demonstrated by evaluations across ten expert-led case studies. This specialized system showcases the potential of LLMs to address pressing societal challenges related to natural hazards and extreme weather events. 

<br /><br />Summary: <div>
arXiv:2504.17200v1 Announce Type: new 
Abstract: Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work we propose a retrieval-augmented generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire hazards. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating natural hazard and extreme weather projection data, observational datasets, and scientific literature through an RAG framework, the system ensures both the accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?</title>
<link>https://arxiv.org/abs/2504.17220</link>
<guid>https://arxiv.org/abs/2504.17220</guid>
<content:encoded><![CDATA[
<div> knowledge distillation, bundle generation, large language models, efficiency, performance

Summary:
This study explores the use of knowledge distillation (KD) techniques to improve the efficiency and performance of Large Language Models (LLMs) in bundle generation tasks. The research focuses on three key questions: the impact of KD format on performance, the influence of distilled knowledge quantity, and the effect of different knowledge utilization methods. A comprehensive KD framework is proposed, which extracts knowledge progressively, varies the quantity through different strategies, and employs various LLM adaptation techniques to enhance efficiency. The experiments demonstrate that KD offers a promising solution for reducing computational costs while maintaining performance quality in LLM-based bundle generation tasks. <div>
arXiv:2504.17220v1 Announce Type: new 
Abstract: LLMs are increasingly explored for bundle generation, thanks to their reasoning capabilities and knowledge. However, deploying large-scale LLMs introduces significant efficiency challenges, primarily high computational costs during fine-tuning and inference due to their massive parameterization. Knowledge distillation (KD) offers a promising solution, transferring expertise from large teacher models to compact student models. This study systematically investigates knowledge distillation approaches for bundle generation, aiming to minimize computational demands while preserving performance. We explore three critical research questions: (1) how does the format of KD impact bundle generation performance? (2) to what extent does the quantity of distilled knowledge influence performance? and (3) how do different ways of utilizing the distilled knowledge affect performance? We propose a comprehensive KD framework that (i) progressively extracts knowledge (patterns, rules, deep thoughts); (ii) captures varying quantities of distilled knowledge through different strategies; and (iii) exploits complementary LLM adaptation techniques (in-context learning, supervised fine-tuning, combination) to leverage distilled knowledge in small student models for domain-specific adaptation and enhanced efficiency. Extensive experiments provide valuable insights into how knowledge format, quantity, and utilization methodologies collectively shape LLM-based bundle generation performance, exhibiting KD's significant potential for more efficient yet effective LLM-based bundle generation.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues</title>
<link>https://arxiv.org/abs/2504.17238</link>
<guid>https://arxiv.org/abs/2504.17238</guid>
<content:encoded><![CDATA[
<div> framework, Cognitive Restructuring, CRDial, dialogues, Crispers <br />
Summary: <br />
The article introduces a new framework, CRDial, for Cognitive Restructuring (CR) in psychotherapy to address clinician shortage and reduce stigma. CRDial creates multi-turn dialogues with identification and restructuring stages for negative thoughts and includes sentence-level supportive conversation strategies. It also incorporates a multi-channel loop mechanism for iterative CR. The framework utilizes the Crisp dataset to train Crispers, conversational Language Models (LLMs) specifically designed for CR at 7B and 14B scales. Human studies demonstrate the effectiveness of Crispers in pointwise, pairwise, and intervention evaluations. This innovative approach aims to improve the psychotherapeutic process for individuals with mental health challenges by providing more personalized and effective CR interventions through interactive dialogues. <br /> <div>
arXiv:2504.17238v1 Announce Type: new 
Abstract: Cognitive Restructuring (CR) is a psychotherapeutic process aimed at identifying and restructuring an individual's negative thoughts, arising from mental health challenges, into more helpful and positive ones via multi-turn dialogues. Clinician shortage and stigma urge the development of human-LLM interactive psychotherapy for CR. Yet, existing efforts implement CR via simple text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to align with the psychotherapeutic process for effective CR. To address this gap, we propose CRDial, a novel framework for CR, which creates multi-turn dialogues with specifically designed identification and restructuring stages of negative thoughts, integrates sentence-level supportive conversation strategies, and adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and 14B scales. Extensive human studies show the superiority of Crispers in pointwise, pairwise, and intervention evaluations.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo</title>
<link>https://arxiv.org/abs/2504.17252</link>
<guid>https://arxiv.org/abs/2504.17252</guid>
<content:encoded><![CDATA[
<div> develop, Neural Machine Translation, Transformer, transfer learning, English-to-Igbo translation <br />
Summary:<br />
The study focuses on developing Neural Machine Translation (NMT) and Transformer-based transfer learning models for English-to-Igbo translation, a low-resource African language spoken by over 40 million people in Nigeria and West Africa. The models are trained on curated datasets from Bible corpora, local news, Wikipedia articles, and Common Crawl, verified by native language experts. Recurrent Neural Network (RNN) architectures like LSTM and GRU, enhanced with attention mechanisms, are used to improve translation accuracy. Transfer learning with MarianNMT pre-trained models within the SimpleTransformers framework further boosts performance, showing a gain of +4.83 BLEU points and reaching an estimated accuracy of 70%. The combination of RNNs and transfer learning effectively addresses the performance gap in low-resource language translation tasks. <br />Summary: <div>
arXiv:2504.17252v1 Announce Type: new 
Abstract: In this study, we develop Neural Machine Translation (NMT) and Transformer-based transfer learning models for English-to-Igbo translation - a low-resource African language spoken by over 40 million people across Nigeria and West Africa. Our models are trained on a curated and benchmarked dataset compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl, all verified by native language experts. We leverage Recurrent Neural Network (RNN) architectures, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), enhanced with attention mechanisms to improve translation accuracy. To further enhance performance, we apply transfer learning using MarianNMT pre-trained models within the SimpleTransformers framework. Our RNN-based system achieves competitive results, closely matching existing English-Igbo benchmarks. With transfer learning, we observe a performance gain of +4.83 BLEU points, reaching an estimated translation accuracy of 70%. These findings highlight the effectiveness of combining RNNs with transfer learning to address the performance gap in low-resource language translation tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning</title>
<link>https://arxiv.org/abs/2504.17264</link>
<guid>https://arxiv.org/abs/2504.17264</guid>
<content:encoded><![CDATA[
<div> Domain Adaptation, Natural Language Processing, Legal Text, JurisCTC, Legal Judgment Prediction 

Summary:
JurisCTC is proposed for Unsupervised Domain Adaptation in Natural Language Processing, specifically for Legal Judgment Prediction tasks. It addresses challenges of lengthy legal texts and limited annotated datasets, enabling knowledge transfer between civil and criminal law domains. JurisCTC utilizes contrastive learning to distinguish samples from different legal domains. Compared to existing models and large language models, JurisCTC achieves higher accuracy in LJP tasks with peak accuracies of 76.59% and 78.83% respectively. The model demonstrates notable advancements in enhancing model generalization across diverse legal domains. <br /><br />Summary: <div>
arXiv:2504.17264v1 Announce Type: new 
Abstract: In recent years, Unsupervised Domain Adaptation (UDA) has gained significant attention in the field of Natural Language Processing (NLP) owing to its ability to enhance model generalization across diverse domains. However, its application for knowledge transfer between distinct legal domains remains largely unexplored. To address the challenges posed by lengthy and complex legal texts and the limited availability of large-scale annotated datasets, we propose JurisCTC, a novel model designed to improve the accuracy of Legal Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC facilitates effective knowledge transfer across various legal domains and employs contrastive learning to distinguish samples from different domains. Specifically, for the LJP task, we enable knowledge transfer between civil and criminal law domains. Compared to other models and specific large language models (LLMs), JurisCTC demonstrates notable advancements, achieving peak accuracies of 76.59% and 78.83%, respectively.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Mitigating Bias in AI-Based Medical Text Generation</title>
<link>https://arxiv.org/abs/2504.17279</link>
<guid>https://arxiv.org/abs/2504.17279</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, fairness, text generation, medical field, bias <br />
Summary:<br />
The study focuses on addressing the issue of fairness in text generation within the medical field. It highlights the concern that AI systems may reflect and amplify human biases, leading to performance discrepancies across different races, sexes, and age groups. The researchers propose an algorithm that selectively optimizes underperformed groups to reduce bias while maintaining model training effectiveness. The algorithm considers word-level accuracy and pathology accuracy to target reference, ensuring full differentiability. Evaluations across various models, datasets, and modalities show that the proposed algorithm significantly enhances fairness in text generation without compromising overall performance. Disparities among different groups are reduced by over 30%, while text generation accuracy remains within 2% relative change. By mitigating bias in deep learning models, the approach aims to improve the reliability and fairness of text generation diagnoses in the medical domain. The code for the proposed algorithm is publicly available for further research. <br /> <div>
arXiv:2504.17279v1 Announce Type: new 
Abstract: Artificial intelligence (AI) systems, particularly those based on deep learning models, have increasingly achieved expert-level performance in medical applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations. The fairness issue has attracted considerable research interest in the medical imaging classification field, yet it remains understudied in the text generation domain. In this study, we investigate the fairness problem in text generation within the medical field and observe significant performance discrepancies across different races, sexes, and age groups, including intersectional groups, various model scales, and different evaluation metrics. To mitigate this fairness issue, we propose an algorithm that selectively optimizes those underperformed groups to reduce bias. The selection rules take into account not only word-level accuracy but also the pathology accuracy to the target reference, while ensuring that the entire process remains fully differentiable for effective model training. Our evaluations across multiple backbones, datasets, and modalities demonstrate that our proposed algorithm enhances fairness in text generation without compromising overall performance. Specifically, the disparities among various groups across different metrics were diminished by more than 30% with our algorithm, while the relative change in text generation accuracy was typically within 2%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of text generation diagnosis in medical domain.
  Our code is publicly available to facilitate further research at https://github.com/iriscxy/GenFair.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality</title>
<link>https://arxiv.org/abs/2504.17309</link>
<guid>https://arxiv.org/abs/2504.17309</guid>
<content:encoded><![CDATA[
<div> watermarking technology, sentence-level, CoheMark, logical fluency, text quality<br />
Summary: <br />
The article introduces CoheMark, an advanced sentence-level watermarking technique that aims to balance high text quality with robust watermark detection. Unlike existing techniques that rely on arbitrary processes, CoheMark utilizes cohesive relationships between sentences for better logical fluency. It selects sentences using fuzzy c-means clustering and specific next sentence selection criteria, resulting in strong watermark strength with minimal impact on text quality. Experimental evaluations confirm the effectiveness of CoheMark in preserving semantic integrity within sentences while maintaining robustness in watermark detection. <div>
arXiv:2504.17309v1 Announce Type: new 
Abstract: Watermarking technology is a method used to trace the usage of content generated by large language models. Sentence-level watermarking aids in preserving the semantic integrity within individual sentences while maintaining greater robustness. However, many existing sentence-level watermarking techniques depend on arbitrary segmentation or generation processes to embed watermarks, which can limit the availability of appropriate sentences. This limitation, in turn, compromises the quality of the generated response. To address the challenge of balancing high text quality with robust watermark detection, we propose CoheMark, an advanced sentence-level watermarking technique that exploits the cohesive relationships between sentences for better logical fluency. The core methodology of CoheMark involves selecting sentences through trained fuzzy c-means clustering and applying specific next sentence selection criteria. Experimental evaluations demonstrate that CoheMark achieves strong watermark strength while exerting minimal impact on text quality.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation</title>
<link>https://arxiv.org/abs/2504.17311</link>
<guid>https://arxiv.org/abs/2504.17311</guid>
<content:encoded><![CDATA[
<div> FLUKE, model robustness, linguistic variations, NLP tasks, LLMs<br />
<br />
Summary: <br />
FLUKE is a framework that evaluates model robustness by systematically varying test data across linguistic levels. Testing on four NLP tasks shows that the impact of linguistic variations varies by task, with some tests crucial for certain tasks. While large language models (LLMs) are generally more robust than fine-tuned models, they still exhibit brittleness to certain linguistic variations. All models are vulnerable to negation modifications across most tasks. This underscores the need for systematic robustness testing to better understand model behaviors. <div>
arXiv:2504.17311v1 Announce Type: new 
Abstract: We present FLUKE (Framework for LingUistically-driven and tasK-agnostic robustness Evaluation), a task-agnostic framework for assessing model robustness through systematic minimal variations of test data. FLUKE introduces controlled variations across linguistic levels - from orthography to dialect and style varieties - and leverages large language models (LLMs) with human validation to generate modifications. We demonstrate FLUKE's utility by evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and reveal that (1) the impact of linguistic variations is highly task-dependent, with some tests being critical for certain tasks but irrelevant for others; (2) while LLMs have better overall robustness compared to fine-tuned models, they still exhibit significant brittleness to certain linguistic variations; (3) all models show substantial vulnerability to negation modifications across most tasks. These findings highlight the importance of systematic robustness testing for understanding model behaviors.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection</title>
<link>https://arxiv.org/abs/2504.17332</link>
<guid>https://arxiv.org/abs/2504.17332</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, misinformation detection, empathy, Dual-Aspect Empathy Framework, Large Language Models

Summary:<br /><br />
In the digital age, social media plays a significant role in information dissemination, but also leads to the rapid spread of misinformation. Traditional methods of detecting misinformation often overlook the influence of human empathy in this process. To address this, the Dual-Aspect Empathy Framework (DAE) is proposed, integrating cognitive and emotional empathy to analyze misinformation from both creator and reader perspectives. By considering creators' cognitive strategies and emotional appeals, as well as simulating readers' cognitive judgments and emotional responses using Large Language Models (LLMs), DAE offers a more holistic approach to misinformation detection. Additionally, an empathy-aware filtering mechanism is introduced to enhance response authenticity and diversity. Experimental results on benchmark datasets demonstrate the superiority of DAE over existing methods, presenting a novel paradigm for multimodal misinformation detection.<br /> <div>
arXiv:2504.17332v1 Announce Type: new 
Abstract: In the digital era, social media has become a major conduit for information dissemination, yet it also facilitates the rapid spread of misinformation. Traditional misinformation detection methods primarily focus on surface-level features, overlooking the crucial roles of human empathy in the propagation process. To address this gap, we propose the Dual-Aspect Empathy Framework (DAE), which integrates cognitive and emotional empathy to analyze misinformation from both the creator and reader perspectives. By examining creators' cognitive strategies and emotional appeals, as well as simulating readers' cognitive judgments and emotional responses using Large Language Models (LLMs), DAE offers a more comprehensive and human-centric approach to misinformation detection. Moreover, we further introduce an empathy-aware filtering mechanism to enhance response authenticity and diversity. Experimental results on benchmark datasets demonstrate that DAE outperforms existing methods, providing a novel paradigm for multimodal misinformation detection.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction</title>
<link>https://arxiv.org/abs/2504.17353</link>
<guid>https://arxiv.org/abs/2504.17353</guid>
<content:encoded><![CDATA[
<div> Keywords: Mutual Reinforcement Effect, information extraction, model interpretability, multimodal, LVLMs

Summary: 
Mutual Reinforcement Effect (MRE) is a new area at the crossroads of information extraction and model interpretability. It aims to improve both coarse-grained and fine-grained tasks by leveraging the relationship between tasks. While MRE has been successful in text, its application to visual and multimodal domains has not been explored. This study introduces Multimodal Mutual Reinforcement Effect (M-MRE) for the first time and provides a dataset for it. The Prompt Format Adapter (PFA) is proposed to address challenges in M-MRE and is compatible with Large Vision-Language Models (LVLMs). Experimental results demonstrate the effectiveness of MRE in the M-MRE task, showing its potential in multimodal text-image understanding scenarios. The study confirms the generalizability of MRE beyond the textual domain.<br /><br />Summary: <div>
arXiv:2504.17353v1 Announce Type: new 
Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection of information extraction and model interpretability. MRE aims to leverage the mutual understanding between tasks of different granularities, enhancing the performance of both coarse-grained and fine-grained tasks through joint modeling. While MRE has been explored and validated in the textual domain, its applicability to visual and multimodal domains remains unexplored. In this work, we extend MRE to the multimodal information extraction domain for the first time. Specifically, we introduce a new task: Multimodal Mutual Reinforcement Effect (M-MRE), and construct a corresponding dataset to support this task. To address the challenges posed by M-MRE, we further propose a Prompt Format Adapter (PFA) that is fully compatible with various Large Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can also be observed in the M-MRE task, a multimodal text-image understanding scenario. This provides strong evidence that MRE facilitates mutual gains across three interrelated tasks, confirming its generalizability beyond the textual domain.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare</title>
<link>https://arxiv.org/abs/2504.17360</link>
<guid>https://arxiv.org/abs/2504.17360</guid>
<content:encoded><![CDATA[
<div> framework, model merging, large language models, data privacy, healthcare

Summary:
PatientDx introduces a framework for effective health-predictive tasks using model merging techniques without the need for fine-tuning or adaptation on patient data, addressing data privacy concerns in the healthcare domain. By optimizing a building block merging strategy, PatientDx leverages a pivotal model focused on numerical reasoning and tunes hyperparameters based on performance metrics. Experimental results on mortality tasks from the MIMIC-IV dataset show performance improvements of up to 7% in terms of AUROC compared to initial models. The proposed approach also mitigates data leak issues common with fine-tuned models, maintaining performance levels. The study concludes with a case study demonstrating the effectiveness of PatientDx. The best model is accessible for public use at https://huggingface.co/Jgmorenof/mistral_merged_0_4. 

<br /><br />Summary: <div>
arXiv:2504.17360v1 Announce Type: new 
Abstract: Fine-tuning of Large Language Models (LLMs) has become the default practice for improving model performance on a given task. However, performance improvement comes at the cost of training on vast amounts of annotated data which could be sensitive leading to significant data privacy concerns. In particular, the healthcare domain is one of the most sensitive domains exposed to data privacy issues. In this paper, we present PatientDx, a framework of model merging that allows the design of effective LLMs for health-predictive tasks without requiring fine-tuning nor adaptation on patient data. Our proposal is based on recently proposed techniques known as merging of LLMs and aims to optimize a building block merging strategy. PatientDx uses a pivotal model adapted to numerical reasoning and tunes hyperparameters on examples based on a performance metric but without training of the LLM on these data. Experiments using the mortality tasks of the MIMIC-IV dataset show improvements up to 7% in terms of AUROC when compared to initial models. Additionally, we confirm that when compared to fine-tuned models, our proposal is less prone to data leak problems without hurting performance. Finally, we qualitatively show the capabilities of our proposal through a case study. Our best model is publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams</title>
<link>https://arxiv.org/abs/2504.17366</link>
<guid>https://arxiv.org/abs/2504.17366</guid>
<content:encoded><![CDATA[
<div> Keywords: long-context understanding, spoken text dataset, specialized methods, redundancy, e-commerce systems <br />
<br />
Summary: <br />
The article introduces a new dataset for spoken long-texts derived from live streams to address the challenges of long-context understanding in real-world dialogues. Tasks are categorized into retrieval-dependent, reasoning-dependent, and hybrid categories to evaluate the performance of language models. Current methods struggle with redundancy-rich inputs, highlighting the need for improvements in long-context understanding. The study compares popular LLMs and specialized methods, showcasing task-specific preferences and inconsistent performance across tasks. A new baseline model is proposed to better handle redundancy in spoken text and achieve strong results. The benchmark aims to evaluate long-context spoken language understanding and provide a foundation for developing practical e-commerce systems. The code and benchmark are available for further research and development. <br /> <br />Summary: <div>
arXiv:2504.17366v1 Announce Type: new 
Abstract: Long-context understanding poses significant challenges in natural language processing, particularly for real-world dialogues characterized by speech-based elements, high redundancy, and uneven information density. Although large language models (LLMs) achieve impressive results on existing benchmarks, these datasets fail to reflect the complexities of such texts, limiting their applicability to practical scenarios. To bridge this gap, we construct the first spoken long-text dataset, derived from live streams, designed to reflect the redundancy-rich and conversational nature of real-world scenarios. We construct tasks in three categories: retrieval-dependent, reasoning-dependent, and hybrid. We then evaluate both popular LLMs and specialized methods to assess their ability to understand long-contexts in these tasks. Our results show that current methods exhibit strong task-specific preferences and perform poorly on highly redundant inputs, with no single method consistently outperforming others. We propose a new baseline that better handles redundancy in spoken text and achieves strong performance across tasks. Our findings highlight key limitations of current methods and suggest future directions for improving long-context understanding. Finally, our benchmark fills a gap in evaluating long-context spoken language understanding and provides a practical foundation for developing real-world e-commerce systems. The code and benchmark are available at https://github.com/Yarayx/livelongbench.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona</title>
<link>https://arxiv.org/abs/2504.17390</link>
<guid>https://arxiv.org/abs/2504.17390</guid>
<content:encoded><![CDATA[
<div> personalized responses, PicPersona-TOD dataset, user images, natural language interactions, task-oriented dialogue systems
Summary:
The article introduces PicPersona-TOD, a dataset that incorporates user images in task-oriented dialogue systems for personalized responses. The dataset enhances user experience by tailoring responses to user-specific factors such as age or emotional context. This is achieved through first impressions, dialogue policy-guided prompting, and external knowledge integration to reduce hallucinations. Human evaluations confirm the dataset's effectiveness in improving engagement. Additionally, the article presents Pictor, a new NLG model that not only personalizes responses but also demonstrates strong performance in diverse domains. The combination of the PicPersona-TOD dataset and the Pictor model enables more engaging and individualized interactions in task-oriented dialogue systems. <br /><br />Summary: <div>
arXiv:2504.17390v1 Announce Type: new 
Abstract: Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests through natural language interactions, yet existing systems often produce generic, monotonic responses that lack individuality and fail to adapt to users' personal attributes. To address this, we introduce PicPersona-TOD, a novel dataset that incorporates user images as part of the persona, enabling personalized responses tailored to user-specific factors such as age or emotional context. This is facilitated by first impressions, dialogue policy-guided prompting, and the use of external knowledge to reduce hallucinations. Human evaluations confirm that our dataset enhances user experience, with personalized responses contributing to a more engaging interaction. Additionally, we introduce a new NLG model, Pictor, which not only personalizes responses, but also demonstrates robust performance across unseen domains https://github.com/JihyunLee1/PicPersona.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation</title>
<link>https://arxiv.org/abs/2504.17445</link>
<guid>https://arxiv.org/abs/2504.17445</guid>
<content:encoded><![CDATA[
<div> machine learning, unsupervised, topic modeling, clustering, interpretability

Summary: 
The article discusses the use of unsupervised machine learning techniques, such as topic modeling and clustering, in identifying patterns in unstructured text data in social science research. Despite the benefits of these methods in terms of reproducibility and cost-efficiency compared to human qualitative analysis, they have limitations in interpretability and relevance to domain-specific research questions. The study explores the potential of using LLM-generated text augmentation to enhance the utility of topic modeling in addressing domain-specific inquiries. Through a political science case study, it demonstrates that topic modeling with GPT-4 augmentations produces easily interpretable categories that can be utilized to explore domain-specific research questions with minimal human intervention. This approach offers a promising way to improve the practicality and effectiveness of topic modeling in social science research. 

Summary: <div>
arXiv:2504.17445v1 Announce Type: new 
Abstract: Unsupervised machine learning techniques, such as topic modeling and clustering, are often used to identify latent patterns in unstructured text data in fields such as political science and sociology. These methods overcome common concerns about reproducibility and costliness involved in the labor-intensive process of human qualitative analysis. However, two major limitations of topic models are their interpretability and their practicality for answering targeted, domain-specific social science research questions. In this work, we investigate opportunities for using LLM-generated text augmentation to improve the usefulness of topic modeling output. We use a political science case study to evaluate our results in a domain-specific application, and find that topic modeling using GPT-4 augmentations creates highly interpretable categories that can be used to investigate domain-specific research questions with minimal human guidance.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation</title>
<link>https://arxiv.org/abs/2504.17480</link>
<guid>https://arxiv.org/abs/2504.17480</guid>
<content:encoded><![CDATA[
<div> watermarking, language models, knowledge distillation, contrastive decoding, attacks

Summary: 
The study introduces a new technique called Contrastive Decoding-Guided Knowledge Distillation (CDG-KD) for tackling the issue of unauthorized knowledge distillation in language models. Watermark radioactivity, a phenomenon where watermarks from teacher models are inherited by student models, is used to detect unauthorized knowledge distillation. However, the robustness of watermarks against scrubbing and spoofing attacks is not well understood. CDG-KD allows for bidirectional attacks by employing contrastive decoding to extract and manipulate watermark texts in student models. The framework effectively removes or forges watermarks while maintaining the overall performance of the model. The research highlights the importance of developing robust and unforgeable watermarking schemes to protect intellectual property in large language models. <div>
arXiv:2504.17480v1 Announce Type: new 
Abstract: Watermarking has emerged as a critical technique for combating misinformation and protecting intellectual property in large language models (LLMs). A recent discovery, termed watermark radioactivity, reveals that watermarks embedded in teacher models can be inherited by student models through knowledge distillation. On the positive side, this inheritance allows for the detection of unauthorized knowledge distillation by identifying watermark traces in student models. However, the robustness of watermarks against scrubbing attacks and their unforgeability in the face of spoofing attacks under unauthorized knowledge distillation remain largely unexplored. Existing watermark attack methods either assume access to model internals or fail to simultaneously support both scrubbing and spoofing attacks. In this work, we propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified framework that enables bidirectional attacks under unauthorized knowledge distillation. Our approach employs contrastive decoding to extract corrupted or amplified watermark texts via comparing outputs from the student model and weakly watermarked references, followed by bidirectional distillation to train new student models capable of watermark removal and watermark forgery, respectively. Extensive experiments show that CDG-KD effectively performs attacks while preserving the general performance of the distilled model. Our findings underscore critical need for developing watermarking schemes that are robust and unforgeable.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HalluLens: LLM Hallucination Benchmark</title>
<link>https://arxiv.org/abs/2504.17550</link>
<guid>https://arxiv.org/abs/2504.17550</guid>
<content:encoded><![CDATA[
<div> benchmark, hallucination, language models, factuality, data leakage  
Summary:  
- This paper introduces a new benchmark to evaluate hallucinations in large language models (LLMs) to address the issue of generated responses deviating from user input or training data, known as "hallucination."
- It proposes a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations to promote consistency in research.
- The benchmark includes new extrinsic hallucination tasks with dynamically generated test sets to prevent data leakage and ensure robustness.
- The analysis of existing benchmarks highlights their limitations and aims to provide a comprehensive evaluation of hallucination in LLMs.
- The work also emphasizes the importance of distinguishing hallucination from factuality evaluations to advance the development of generative AI systems.  
<br /><br />Summary: <div>
arXiv:2504.17550v1 Announce Type: new 
Abstract: Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as "hallucination." These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from "factuality," proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars</title>
<link>https://arxiv.org/abs/2504.17562</link>
<guid>https://arxiv.org/abs/2504.17562</guid>
<content:encoded><![CDATA[
<div> latent semantics, language models, metadata, pre-training, downstream tasks 

Summary:
Prepending metadata to texts during pre-training of language models can enhance performance in downstream tasks by allowing easier access to latent semantics. However, this approach may have both positive and negative effects on performance, depending on the context provided in the downstream task prompt. When the context is sufficient for inferring latent semantics, training with metadata improves model performance. Conversely, if the context lacks the necessary information for accurate inference, the technique can negatively impact performance. This study demonstrates the importance of considering the context provided in downstream tasks when leveraging metadata during pre-training. <div>
arXiv:2504.17562v1 Announce Type: new 
Abstract: The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. Previous studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. To understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. Interestingly, we found that this approach produces both positive and negative effects on the downstream tasks. We demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. Specifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. In contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training</title>
<link>https://arxiv.org/abs/2504.17565</link>
<guid>https://arxiv.org/abs/2504.17565</guid>
<content:encoded><![CDATA[
<div> large language models, reasoning dataset, training process, data quality, mathematical reasoning

Summary:
A new study explores the training processes and data quality of large language models (LLMs) to improve reasoning capabilities. By constructing a large-scale reasoning dataset with varying difficulty levels, the researchers select valuable training data based on pass rate and Coefficient of Variation (CV). They notice a pattern shift in training, requiring higher learning rates for effective reasoning-focused training. With carefully selected data, the base model achieves a 79.2% pass rate on the AIME2024 mathematical reasoning benchmark, approaching state-of-the-art performance. The study provides detailed descriptions of data processing, difficulty assessment, and training methodology, and openly shares all datasets and methods to advance open-source long-reasoning LLMs.<br /><br />Summary: <div>
arXiv:2504.17565v1 Announce Type: new 
Abstract: Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore</title>
<link>https://arxiv.org/abs/2504.17574</link>
<guid>https://arxiv.org/abs/2504.17574</guid>
<content:encoded><![CDATA[
<div> modeling approach, Chinese rumor detection, deep learning framework, TextCNN, Bidirectional GRU

Summary:
RAGAT-Mind is a novel multi-granular modeling approach for Chinese rumor detection that utilizes the MindSpore deep learning framework. The model combines TextCNN, bidirectional GRU, Multi-Head Self-Attention, and Bidirectional Graph Convolutional Networks (BiGCN) to extract local semantic information, learn sequential context, focus on global dependencies, and represent structural word co-occurrence graphs. Experimental results on the Weibo1-Rumor dataset demonstrate the superior classification performance of RAGAT-Mind with 99.2% accuracy and a macro-F1 score of 0.9919. The study validates the effectiveness of integrating hierarchical linguistic features with graph-based semantic structures for rumor detection. Additionally, the model shows strong generalization and interpretability, indicating its practical utility for real-world applications. 

<br /><br />Summary: <div>
arXiv:2504.17574v1 Announce Type: new 
Abstract: As false information continues to proliferate across social media platforms, effective rumor detection has emerged as a pressing challenge in natural language processing. This paper proposes RAGAT-Mind, a multi-granular modeling approach for Chinese rumor detection, built upon the MindSpore deep learning framework. The model integrates TextCNN for local semantic extraction, bidirectional GRU for sequential context learning, Multi-Head Self-Attention for global dependency focusing, and Bidirectional Graph Convolutional Networks (BiGCN) for structural representation of word co-occurrence graphs. Experiments on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior classification performance, attaining 99.2% accuracy and a macro-F1 score of 0.9919. The results validate the effectiveness of combining hierarchical linguistic features with graph-based semantic structures. Furthermore, the model exhibits strong generalization and interpretability, highlighting its practical value for real-world rumor detection applications.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a comprehensive taxonomy of online abusive language informed by machine leaning</title>
<link>https://arxiv.org/abs/2504.17653</link>
<guid>https://arxiv.org/abs/2504.17653</guid>
<content:encoded><![CDATA[
<div> Keywords: abusive language, online communication, taxonomy, classification systems, detection

Summary:
The paper introduces a taxonomy for identifying abusive language in online text, aiming to address the increasing concerns about online abuse and its harmful effects. The hierarchical and faceted taxonomy developed includes 5 categories and 17 dimensions, covering aspects such as context, target, intensity, directness, and theme of abuse. By integrating classification systems from 18 multi-label datasets, the taxonomy provides a comprehensive framework for classifying online abuse. This classification aids in the identification, monitoring, and mitigation of harmful content online, facilitating early intervention and moderation. The shared understanding generated by this taxonomy can foster collaboration among researchers, policymakers, online platform owners, and other stakeholders in combating online abuse effectively.<br /><br />Summary: <div>
arXiv:2504.17653v1 Announce Type: new 
Abstract: The proliferation of abusive language in online communications has posed significant risks to the health and wellbeing of individuals and communities. The growing concern regarding online abuse and its consequences necessitates methods for identifying and mitigating harmful content and facilitating continuous monitoring, moderation, and early intervention. This paper presents a taxonomy for distinguishing key characteristics of abusive language within online text. Our approach uses a systematic method for taxonomy development, integrating classification systems of 18 existing multi-label datasets to capture key characteristics relevant to online abusive language classification. The resulting taxonomy is hierarchical and faceted, comprising 5 categories and 17 dimensions. It classifies various facets of online abuse, including context, target, intensity, directness, and theme of abuse. This shared understanding can lead to more cohesive efforts, facilitate knowledge exchange, and accelerate progress in the field of online abuse detection and mitigation among researchers, policy makers, online platform owners, and other stakeholders.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics</title>
<link>https://arxiv.org/abs/2504.17665</link>
<guid>https://arxiv.org/abs/2504.17665</guid>
<content:encoded><![CDATA[
<div> code-assisted LLMs, mathematical reasoning tasks, grounding, evaluation, math datasets
<br />
Summary:<br />
Assisting LLMs with code generation enhanced their performance on math reasoning tasks, but the evaluation usually focuses only on execution correctness. This study delves deeper into code-assisted LLMs' generated programs for math tasks, analyzing to what extent they adhere to math rules and how it impacts their performance. The evaluation assessed five LLMs on two math datasets, revealing that the grounding of programs varies based on LLM capabilities and task difficulty. Closed-source models showed better grounding with math rules, while open-source models struggled to apply them accurately. The grounding percentage decreased on more challenging problems, indicating the need for comprehensive evaluations beyond execution accuracy measures. This research emphasizes the importance of understanding code-assisted LLMs' abilities and limitations in mathematical contexts. 
<br /><br />Summary: <div>
arXiv:2504.17665v1 Announce Type: new 
Abstract: Assisting LLMs with code generation improved their performance on mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is generally restricted to execution correctness, lacking a rigorous evaluation of their generated programs. In this work, we bridge this gap by conducting an in-depth analysis of code-assisted LLMs' generated programs in response to math reasoning tasks. Our evaluation focuses on the extent to which LLMs ground their programs to math rules, and how that affects their end performance. For this purpose, we assess the generations of five different LLMs, on two different math datasets, both manually and automatically. Our results reveal that the distribution of grounding depends on LLMs' capabilities and the difficulty of math problems. Furthermore, mathematical grounding is more effective for closed-source models, while open-source models fail to employ math rules in their solutions correctly. On MATH500, the percentage of grounded programs decreased to half, while the ungrounded generations doubled in comparison to ASDiv grade-school problems. Our work highlights the need for in-depth evaluation beyond execution accuracy metrics, toward a better understanding of code-assisted LLMs' capabilities and limits in the math domain.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction</title>
<link>https://arxiv.org/abs/2504.17671</link>
<guid>https://arxiv.org/abs/2504.17671</guid>
<content:encoded><![CDATA[
<div> hallucination mitigation, Large Vision-Language Models, Visual Question Answering, uncertainty quantification, split conformal prediction<br />
<br />
Summary:<br />
This study proposes a Split Conformal Prediction framework to address hallucination mitigation in Large Vision-Language Models for Visual Question Answering tasks. The framework integrates dynamic threshold calibration and cross-modal consistency verification to quantify uncertainties. It partitions data into calibration and test sets, computes nonconformity scores, and constructs prediction sets with statistical guarantees under user-defined risk levels. The framework ensures rigorous control of marginal coverage, dynamically adjusts prediction set sizes based on risk levels, and eliminates the need for prior distribution assumptions and retraining. Evaluations on benchmarks show that the framework enforces theoretical guarantees across all risk levels and is robust for real-world deployment in safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making. <br /> <div>
arXiv:2504.17671v1 Announce Type: new 
Abstract: This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous control of \textbf{marginal coverage} to ensure empirical error rates remain strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Considerations of Large Language Model Inference and Efficiency Optimizations</title>
<link>https://arxiv.org/abs/2504.17674</link>
<guid>https://arxiv.org/abs/2504.17674</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, energy efficiency optimizations, Natural Language Processing, GPU architectures, sustainable deployment

Summary:<br />
The study examines the energy implications of various inference efficiency optimizations in diverse NLP and generative AI workloads. A modeling approach that approximates real-world LLM workflows is introduced through input-output token distribution binning and batch size variations. The analysis covers software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. The research demonstrates that the effectiveness of inference optimizations is highly dependent on workload geometry, software stack, and hardware accelerators. Naive energy estimates based on FLOPs or theoretical GPU utilization underestimate actual energy consumption. Proper application of relevant inference efficiency optimizations can lead to a significant reduction in total energy use, by up to 73% from unoptimized baselines. These findings offer insights for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure. 

Summary: <div>
arXiv:2504.17674v1 Announce Type: new 
Abstract: As large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks</title>
<link>https://arxiv.org/abs/2504.17685</link>
<guid>https://arxiv.org/abs/2504.17685</guid>
<content:encoded><![CDATA[
<div> Ensemble Bayesian Inference, small language model, large language models, aptitude assessments, consumer profile analysis <br />
<br />
Summary: This study explores the potential of small language model ensembles in achieving comparable accuracy to large language models (LLMs) through Ensemble Bayesian Inference (EBI). By combining judgments from multiple SLMs using Bayesian estimation, EBI surpasses individual model performance limitations. The experiments conducted on various tasks (aptitude assessments, consumer profile analysis) in both Japanese and English languages demonstrate the effectiveness of EBI. Interestingly, the study shows that including models with negative Lift values in ensembles can enhance overall performance. The efficacy of EBI across different languages suggests new opportunities for constructing high-performance AI systems with limited computational resources and leveraging models with lower individual performance. The paper discusses the novelty and significance of the approach in the context of LLM performance evaluation, ensemble methods, and open-source LLM utilization. <br /><br /> <div>
arXiv:2504.17685v1 Announce Type: new 
Abstract: This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety in Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2504.17704</link>
<guid>https://arxiv.org/abs/2504.17704</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, vulnerabilities, safety risks, attacks, defense strategies

Summary:
Large Reasoning Models (LRMs) have shown impressive performance in tasks like mathematics and coding due to their advanced reasoning abilities. However, with the increasing sophistication of LRMs, concerns have been raised about their vulnerabilities and safety risks, which could hinder their practical application. This survey paper comprehensively examines the current safety landscape of LRMs, categorizing and summarizing the emerging risks, potential attacks, and defense mechanisms. By providing a structured taxonomy of these elements, the paper aims to enhance the understanding of the security challenges associated with LRMs. This organized approach can help guide future research efforts and development initiatives focused on improving the reliability and robustness of these powerful models. 

<br /><br />Summary: Large Reasoning Models (LRMs) excel in tasks like mathematics and coding but face growing concerns over vulnerabilities and safety risks. This paper surveys the safety landscape of LRMs, categorizing risks, attacks, and defense strategies to guide future research in enhancing the security of these models. <div>
arXiv:2504.17704v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Performance Biases of Large Language Models in Education</title>
<link>https://arxiv.org/abs/2504.17720</link>
<guid>https://arxiv.org/abs/2504.17720</guid>
<content:encoded><![CDATA[
<div> educational tasks, language models, performance, languages, deployment  
Summary:  
Large language models (LLMs) are being used in educational settings, but their effectiveness varies across languages. Testing popular LLMs on educational tasks in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English revealed that lower-resource languages had poorer task performance due to less representation in training data. While models performed reasonably well overall, there were significant drops in performance compared to English. It is recommended that practitioners validate LLM performance in the target language before deployment to ensure effectiveness.  
<br /><br />Summary: <div>
arXiv:2504.17720v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being adopted in educational settings. These applications expand beyond English, though current LLMs remain primarily English-centric. In this work, we ascertain if their use in education settings in non-English languages is warranted. We evaluated the performance of popular LLMs on four educational tasks: identifying student misconceptions, providing targeted feedback, interactive tutoring, and grading translations in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English. We find that the performance on these tasks somewhat corresponds to the amount of language represented in training data, with lower-resource languages having poorer task performance. Although the models perform reasonably well in most languages, the frequent performance drop from English is significant. Thus, we recommend that practitioners first verify that the LLM works well in the target language for their educational task before deployment.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT</title>
<link>https://arxiv.org/abs/2504.17753</link>
<guid>https://arxiv.org/abs/2504.17753</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational assistants, Healthcare, Large Language Models, User study, Heart failure patients

Summary:
The study compares two versions of a conversational assistant for heart failure patients, one using a neurosymbolic architecture and the other based on ChatGPT. The in-house system is found to be more accurate and efficient in task completion, while the ChatGPT-based system has fewer speech errors and clarification needs. However, patients show no preference between the two versions. This highlights the need for controlled evaluations in healthcare settings to understand the advantages and disadvantages of different conversational assistant architectures. The increasing popularity of conversational assistants, particularly in healthcare, calls for rigorous testing with real stakeholders to ensure the systems meet the needs of users effectively. The study emphasizes the importance of considering factors such as accuracy, efficiency, and user preference when designing conversational assistants for healthcare applications.

<br /><br />Summary: <div>
arXiv:2504.17753v1 Announce Type: new 
Abstract: Conversational assistants are becoming more and more popular, including in healthcare, partly because of the availability and capabilities of Large Language Models. There is a need for controlled, probing evaluations with real stakeholders which can highlight advantages and disadvantages of more traditional architectures and those based on generative AI. We present a within-group user study to compare two versions of a conversational assistant that allows heart failure patients to ask about salt content in food. One version of the system was developed in-house with a neurosymbolic architecture, and one is based on ChatGPT. The evaluation shows that the in-house system is more accurate, completes more tasks and is less verbose than the one based on ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors and requires fewer clarifications to complete the task. Patients show no preference for one over the other.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs</title>
<link>https://arxiv.org/abs/2504.17768</link>
<guid>https://arxiv.org/abs/2504.17768</guid>
<content:encoded><![CDATA[
<div> Sparse attention, Transformer LLMs, long-context capabilities, efficiency-accuracy trade-offs, model scales <br />
<br />
Summary: Sparse attention is a promising strategy to enhance Transformer LLMs' ability to process long sequences. Through experiments, it was found that larger and highly sparse models are more preferable for very long sequences. The level of sparsity achievable without loss of accuracy is higher during decoding than prefilling and is influenced by model size. Different sparsification methods are required for different tasks and phases, with no single strategy performing best overall. Even moderate levels of sparsity can lead to significant performance degradation on certain tasks, indicating that sparse attention is not universally applicable. Scaling laws tailored for sparse attention were introduced and validated, suggesting that the findings are likely applicable beyond the range of experiments. Sparse attention is a valuable tool for processing longer sequences but requires careful consideration of trade-offs for performance-sensitive applications.<br /><br /> <div>
arXiv:2504.17768v1 Announce Type: new 
Abstract: Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2504.16939</link>
<guid>https://arxiv.org/abs/2504.16939</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Conversational Agents, Reasoning, Monitor, Control

Summary:<br /><br />
The survey paper discusses the advancements in Large Language Models (LLMs) and their impact on Conversational Agents, highlighting the need for more scalable systems approaching human-level intelligence. The paper categorizes the capabilities of LLM-driven Conversational Agents into three dimensions: Reasoning, Monitor, and Control. It introduces a taxonomy classifying recent work on Conversational Agents based on this desideratum, pointing out critical research gaps. Key directions for future research include realistic evaluations, enhancing long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. The ultimate goal is to advance progress towards Artificial General Intelligence (AGI). A curated repository of papers on Conversational Agents is also maintained to facilitate further research in the field. <br /> <div>
arXiv:2504.16939v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. Yet, fundamental questions about their capabilities, limitations, and paths forward remain open. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. To that end, we systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following. Building upon this, we introduce a novel taxonomy by classifying recent work on Conversational Agents around our proposed desideratum. We identify critical research gaps and outline key directions, including realistic evaluations, long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. This work aims to provide a structured foundation, highlight existing limitations, and offer insights into potential future research directions for Conversational Agents, ultimately advancing progress toward Artificial General Intelligence (AGI). We maintain a curated repository of papers at: https://github.com/emrecanacikgoz/awesome-conversational-agents.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Im)possibility of Automated Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2504.17004</link>
<guid>https://arxiv.org/abs/2504.17004</guid>
<content:encoded><![CDATA[
<div> framework, hallucination detection, language models, language identification, expert-labeled feedback
Summary:
- The study introduces a theoretical framework to analyze the possibility of automated hallucination detection in large language models (LLMs).
- It establishes the equivalence between hallucination detection and language identification, suggesting that detecting hallucinations is fundamentally impossible for most language collections without expert-labeled feedback.
- With expert-labeled feedback, automated hallucination detection becomes possible for all countable language collections.
- The research emphasizes the importance of expert-labeled examples in training hallucination detectors and supports feedback-based methods like reinforcement learning with human feedback (RLHF) for reliable LLM deployment. <div>
arXiv:2504.17004v1 Announce Type: cross 
Abstract: Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.
  First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCALAR: A Part-of-speech Tagger for Identifiers</title>
<link>https://arxiv.org/abs/2504.17038</link>
<guid>https://arxiv.org/abs/2504.17038</guid>
<content:encoded><![CDATA[
<div> Keywords: Source Code Analysis, Lexical Annotation, Identifier Names, Part-of-Speech Tagging, SCALAR<br />
<br />
Summary: <br />
The paper introduces SCALAR, a tool designed to annotate source code identifier names with their corresponding grammar patterns. SCALAR utilizes the GradientBoostingClassifier from scikit-learn and a manually-curated oracle of identifier names to train its internal model. This specialized training allows SCALAR to recognize the unique language structure developers use when creating identifiers. The tool's output is compared to an older version of the tagger and a modern off-the-shelf part-of-speech tagger to demonstrate its improved annotation accuracy for identifiers. The code for SCALAR is open-source and available on Github. Overall, SCALAR's approach to mapping identifier names to part-of-speech tags shows promise in enhancing the accuracy and efficiency of source code analysis and lexical annotation. <br /> <div>
arXiv:2504.17038v1 Announce Type: cross 
Abstract: The paper presents the Source Code Analysis and Lexical Annotation Runtime (SCALAR), a tool specialized for mapping (annotating) source code identifier names to their corresponding part-of-speech tag sequence (grammar pattern). SCALAR's internal model is trained using scikit-learn's GradientBoostingClassifier in conjunction with a manually-curated oracle of identifier names and their grammar patterns. This specializes the tagger to recognize the unique structure of the natural language used by developers to create all types of identifiers (e.g., function names, variable names etc.). SCALAR's output is compared with a previous version of the tagger, as well as a modern off-the-shelf part-of-speech tagger to show how it improves upon other taggers' output for annotating identifiers. The code is available on Github
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation</title>
<link>https://arxiv.org/abs/2504.17365</link>
<guid>https://arxiv.org/abs/2504.17365</guid>
<content:encoded><![CDATA[
<div> Keywords: Soccer, Multimodal Large Language Models, Video Understanding, Dense Video Captioning, State-of-The-Art Performance<br />
Summary:<br />
This article introduces TimeSoccer, an end-to-end soccer Multimodal Large Language Model designed for Single-anchor Dense Video Captioning in full-match soccer videos. TimeSoccer allows for accurate temporal alignment and semantic relevance by jointly predicting timestamps and generating captions in a single pass. To handle long temporal sequences in soccer matches, the model includes MoFA-Select, a training-free frame compression module that selects representative frames and utilizes various training paradigms for improved performance. Through extensive experiments, TimeSoccer demonstrates State-of-The-Art performance in generating high-quality commentary with precise temporal localization across 45-minute matches. This approach addresses the limitations of existing soccer models that rely on temporal a priori for caption generation or follow a two-step paradigm, resulting in suboptimal performance. <div>
arXiv:2504.17365v1 Announce Type: cross 
Abstract: Soccer is a globally popular sporting event, typically characterized by long matches and distinctive highlight moments. Recent advances in Multimodal Large Language Models (MLLMs) offer promising capabilities in temporal grounding and video understanding, soccer commentary generation often requires precise temporal localization and semantically rich descriptions over long-form video. However, existing soccer MLLMs often rely on the temporal a priori for caption generation, so they cannot process the soccer video end-to-end. While some traditional approaches follow a two-step paradigm that is complex and fails to capture the global context to achieve suboptimal performance. To solve the above issues, we present TimeSoccer, the first end-to-end soccer MLLM for Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos. TimeSoccer jointly predicts timestamps and generates captions in a single pass, enabling global context modeling across 45-minute matches. To support long video understanding of soccer matches, we introduce MoFA-Select, a training-free, motion-aware frame compression module that adaptively selects representative frames via a coarse-to-fine strategy, and incorporates complementary training paradigms to strengthen the model's ability to handle long temporal sequences. Extensive experiments demonstrate that our TimeSoccer achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end form, generating high-quality commentary with accurate temporal alignment and strong semantic relevance.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models</title>
<link>https://arxiv.org/abs/2504.17449</link>
<guid>https://arxiv.org/abs/2504.17449</guid>
<content:encoded><![CDATA[
<div> Hierarchical knowledge management, Multi-tenant Inference, Pretrained language models, Resource-efficient, GPU memory usage<br />
<br />
Summary: 
The article introduces HMI, a system designed to efficiently serve multiple tenants with distinct pretrained language models (PLMs) by categorizing knowledge into general, domain-specific, and task-specific components. By leveraging hierarchical PLMs (hPLMs) with knowledge stored at different levels, GPU memory usage per tenant is significantly reduced. Domain-specific knowledge is managed through knowledge trees based on frequency, while task-specific knowledge is handled with parameter swapping to stay within limited GPU memory. System optimizations such as hierarchical knowledge prefetching and batched matrix multiplications enhance resource utilization and inference throughput. Experimental results show that HMI can effectively support up to 10,000 hPLMs on a single GPU with minimal impact on accuracy. <br /><br />Summary: <div>
arXiv:2504.17449v1 Announce Type: cross 
Abstract: The significant computational demands of pretrained language models (PLMs), which often require dedicated hardware, present a substantial challenge in serving them efficiently, especially in multi-tenant environments. To address this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant Inference system, designed to manage tenants with distinct PLMs resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM knowledge into general, domain-specific, and task-specific. Leveraging insights on knowledge acquisition across different model layers, we construct hierarchical PLMs (hPLMs) by extracting and storing knowledge at different levels, significantly reducing GPU memory usage per tenant. Secondly, we establish hierarchical knowledge management for hPLMs generated by various tenants in HMI. We manage domain-specific knowledge with acceptable storage increases by constructing and updating domain-specific knowledge trees based on frequency. We manage task-specific knowledge within limited GPU memory through parameter swapping. Finally, we propose system optimizations to enhance resource utilization and inference throughput. These include fine-grained pipelining via hierarchical knowledge prefetching to overlap CPU and I/O operations with GPU computations, and optimizing parallel implementations with batched matrix multiplications. Our experimental results demonstrate that the proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a single GPU, with only a negligible compromise in accuracy.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization</title>
<link>https://arxiv.org/abs/2406.07494</link>
<guid>https://arxiv.org/abs/2406.07494</guid>
<content:encoded><![CDATA[
<div> Transformer-based summarization, English dialogues, challenges, techniques, evaluation metrics <br />
<br />
Summary: Abstractive dialogue summarization research on Transformer-based models for English dialogues reviews 1262 papers from 2019 to 2024. Main challenges include language, structure, comprehension, speaker, salience, and factuality, addressed by graph-based approaches and BART models. Progress has been made in language challenges, while comprehension, factuality, and salience remain difficult. Assessments use datasets for dialogue subdomains, ROUGE metric, and human evaluation lacking detail on annotator agreement. Few datasets cover all subdomains. Large language models are explored with potential implications, but challenge taxonomy remains relevant. <div>
arXiv:2406.07494v3 Announce Type: replace 
Abstract: Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure</title>
<link>https://arxiv.org/abs/2406.17276</link>
<guid>https://arxiv.org/abs/2406.17276</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive language models, Speculative decoding, OPT-Tree, Inference efficiency, Adaptive draft trees

Summary:
OPT-Tree introduces an adaptive and scalable draft tree algorithm to improve the inference efficiency of autoregressive language models. By optimizing tree structures to maximize acceptance length during verification, OPT-Tree outperforms existing methods in speed-up ratio and can generate more than ten tokens in a single step. The "draft and then verify" mechanism of speculative decoding allows for lossless acceleration, enabling multiple token generation in each decoding step. Experimental results demonstrate a speed-up ratio of up to 3.2 compared to autoregressive decoding. With the availability of the OPT-Tree code on GitHub, researchers can implement this algorithm to enhance the performance of autoregressive language models in various scenarios. <br /><br />Summary: <div>
arXiv:2406.17276v4 Announce Type: replace 
Abstract: Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a "draft and then verify" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which fail to adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft trees. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse</title>
<link>https://arxiv.org/abs/2409.11242</link>
<guid>https://arxiv.org/abs/2409.11242</guid>
<content:encoded><![CDATA[
<div> Trust-Score, LLMs, RAG task, Trust-Align, in-context learning  
Summary:  
Trust-Score is introduced as a metric to evaluate the trustworthiness of LLMs in the RAG framework. Various prompting methods, like in-context learning, are found to be ineffective in adapting LLMs for the RAG task, leading to the proposal of Trust-Align to improve performance. Models aligned using Trust-Align outperform baselines on ASQA, QAMPARI, and ELI5. Trust-Align enhances models' ability to refuse correctly and provide quality citations. The effectiveness of Trust-Align is demonstrated across different open-weight models, including LLaMA series, Qwen series, and Phi3.5 series. Code for Trust-Align is released on GitHub at https://github.com/declare-lab/trust-align. <div>
arXiv:2409.11242v4 Announce Type: replace 
Abstract: LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose Trust-Align, a method to align LLMs for improved Trust-Score performance. 26 out of 27 models aligned using Trust-Align substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56), QAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly enhances models' ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of Trust-Align across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine</title>
<link>https://arxiv.org/abs/2409.18986</link>
<guid>https://arxiv.org/abs/2409.18986</guid>
<content:encoded><![CDATA[
<div> Keywords: Lab-AI, personalized normal ranges, retrieval-augmented generation, patient portals, clinical medicine<br />
Summary: 
Lab-AI is an interactive system developed for accurate interpretation of lab results in clinical medicine. It offers personalized normal ranges based on conditional factors such as age and gender, using retrieval-augmented generation (RAG) from credible health sources. The system consists of two modules: factor retrieval and normal range retrieval, which were tested on 122 lab tests. Lab-AI, powered by GPT-4-turbo with RAG, achieved high performance metrics with a 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval. It outperformed non-RAG systems significantly in factor retrieval and showed substantial improvements in question-level and lab-level performance for normal range retrieval. The study demonstrates the potential of Lab-AI to enhance patient understanding of lab results and improve the accuracy of interpreting lab data in clinical settings.<br /><br />Summary: <div>
arXiv:2409.18986v2 Announce Type: replace 
Abstract: Accurate interpretation of lab results is crucial in clinical medicine, yet most patient portals use universal normal ranges, ignoring conditional factors like age and gender. This study introduces Lab-AI, an interactive system that offers personalized normal ranges using retrieval-augmented generation (RAG) from credible health sources. Lab-AI has two modules: factor retrieval and normal range retrieval. We tested these on 122 lab tests: 40 with conditional factors and 82 without. For tests with factors, normal ranges depend on patient-specific information. Our results show GPT-4-turbo with RAG achieved a 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5% in factor retrieval and showed 132% and 100% improvements in question-level and lab-level performance, respectively, for normal range retrieval. These findings highlight Lab-AI's potential to enhance patient understanding of lab results.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?</title>
<link>https://arxiv.org/abs/2409.19151</link>
<guid>https://arxiv.org/abs/2409.19151</guid>
<content:encoded><![CDATA[
<div> linguistics, NLP, XLR languages, machine translation, grammar books

Summary:<br />
- The study investigates the use of grammar books in training NLP models for Extremely Low-Resource (XLR) languages.
- Machine Translation from One Book shows that using grammar books can enable translation for XLR languages not seen by LLMs.
- The study finds that the translation ability mainly comes from the parallel examples in the grammar books rather than the grammatical explanations.
- Similar results are found for Nepali and Guarani languages, and fine-tuning a translation model can achieve comparable performance to LLMs with grammar books.
- Grammar books are more helpful for translation tasks through parallel examples, while linguistic tasks benefit from grammatical knowledge through a typological feature prompt. <br />Summary: <div>
arXiv:2409.19151v2 Announce Type: replace 
Abstract: Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests that prompting long-context LLMs with one grammar book enables English-Kalamang translation, an XLR language unseen by LLMs - a noteworthy case of linguistics helping an NLP task. We investigate the source of this translation ability, finding almost all improvements stem from the book's parallel examples rather than its grammatical explanations. We find similar results for Nepali and Guarani, seen low-resource languages, and we achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate where grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what kind of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we conclude data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking</title>
<link>https://arxiv.org/abs/2410.01952</link>
<guid>https://arxiv.org/abs/2410.01952</guid>
<content:encoded><![CDATA[
<div> Large Language Models, reasoning types, deductive reasoning, inductive reasoning, abductive reasoning <br />
Summary: <br />
The article discusses the limitations of current Large Language Models (LLMs) in utilizing diverse reasoning types such as inductive, abductive, or analogical reasoning. It introduces TypedThinker, a system that predicts suitable reasoning types for specific problems and provides guidance to LLMs in applying these strategies. Experimental results demonstrate significant performance improvements across logical and mathematical reasoning tasks without requiring knowledge distillation from larger models. TypedThinker can enhance LLM reasoning and problem-solving capabilities, potentially integrating into advanced systems like GPT-4o or specialized models like MetaMath to diversify reasoning approaches. <div>
arXiv:2410.01952v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities in solving complex problems. However, current approaches primarily enhance reasoning through the elaboration of thoughts while neglecting the diversity of reasoning types. LLMs typically employ deductive reasoning, proceeding step-by-step from given conditions, which limits their exploration during problem-solving. Our analysis reveals that certain problems are exclusively solvable through specific reasoning strategies like inductive, abductive, or analogical reasoning. However, incorporating diverse reasoning approaches presents two key challenges: identifying the appropriate reasoning type for each problem and exploiting this approach during problem-solving. Therefore, we propose the TypedThinker that predicts suitable reasoning types based on the problem and their previous effectiveness and provides relevant demonstrations to guide LLMs in applying these strategies. Experimental results show significant improvements across multiple benchmarks, with performance gains of 3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and mathematical reasoning tasks. TypedThinker enhances LLM reasoning without requiring knowledge distillation from larger models. It can be integrated into more advanced systems like GPT-4o or specialized models like MetaMath to diversify their reasoning approaches and improve their problem-solving capabilities.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Attention Improves Transformer</title>
<link>https://arxiv.org/abs/2410.02703</link>
<guid>https://arxiv.org/abs/2410.02703</guid>
<content:encoded><![CDATA[
arXiv:2410.02703v2 Announce Type: replace 
Abstract: Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task performance in a variety of model sizes and context lengths. For example, transformers trained with the language modeling objective on C4 with selective attention perform language modeling equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</title>
<link>https://arxiv.org/abs/2410.05401</link>
<guid>https://arxiv.org/abs/2410.05401</guid>
<content:encoded><![CDATA[
arXiv:2410.05401v2 Announce Type: replace 
Abstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies</title>
<link>https://arxiv.org/abs/2410.19878</link>
<guid>https://arxiv.org/abs/2410.19878</guid>
<content:encoded><![CDATA[
arXiv:2410.19878v3 Announce Type: replace 
Abstract: The large models, as predicted by scaling raw forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large models require substantial computational resources and GPU memory to operate. When adapting large models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach</title>
<link>https://arxiv.org/abs/2411.04950</link>
<guid>https://arxiv.org/abs/2411.04950</guid>
<content:encoded><![CDATA[
arXiv:2411.04950v4 Announce Type: replace 
Abstract: We introduce a data-centric hypothesis-testing framework to quantify the influence of sequentially correlated literary properties--such as thematic continuity--on textual classification tasks. Our method models label sequences as stochastic processes and uses an empirical autocovariance matrix to generate surrogate labelings that preserve sequential dependencies. This enables statistical testing to determine whether classification outcomes are primarily driven by thematic structure or by non-sequential features like authorial style. Applying this framework across a diverse corpus of English prose, we compare traditional (word n-grams and character k-mers) and neural (contrastively trained) embeddings in both supervised and unsupervised classification settings. Crucially, our method identifies when classifications are confounded by sequentially correlated similarity, revealing that supervised and neural models are more prone to false positives--mistaking shared themes and cross-genre differences for stylistic signals. In contrast, unsupervised models using traditional features often yield high true positive rates with minimal false positives, especially in genre-consistent settings. By disentangling sequential from non-sequential influences, our approach provides a principled way to assess and interpret classification reliability. This is particularly impactful for authorship attribution, forensic linguistics, and the analysis of redacted or composite texts, where conventional methods may conflate theme with style. Our results demonstrate that controlling for sequential correlation is essential for reducing false positives and ensuring that classification outcomes reflect genuine stylistic distinctions.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images</title>
<link>https://arxiv.org/abs/2412.08802</link>
<guid>https://arxiv.org/abs/2412.08802</guid>
<content:encoded><![CDATA[
arXiv:2412.08802v2 Announce Type: replace 
Abstract: Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual understanding perspective, previous CLIP-based models exhibit insufficient understanding of visually rich documents. In this work, we propose jina-clip-v2, a contrastive vision-language model trained on text pairs, triplets and image-text pairs via a multi-task and multi-stage contrastive learning paradigm in order to support both text-only and crossmodal tasks. We employ a multilingual text encoder and expand the training dataset to include multilingual texts from 29 non-English languages, including Hindi, Chinese, German, French, and others, as well as images of visually rich documents. We evaluate the model's performance and show that jina-clip-v2 achieves notable improvements over state-of-the-art CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks in both English and multilingual settings. jina-clip-v2 also provides for flexibility in embedding dimensionality, enabling users to select the granularity of the representations. jina-clip-v2 is publicly available at https://huggingface.co/jinaai/jina-clip-v2.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing</title>
<link>https://arxiv.org/abs/2501.14936</link>
<guid>https://arxiv.org/abs/2501.14936</guid>
<content:encoded><![CDATA[
arXiv:2501.14936v2 Announce Type: replace 
Abstract: The integration of contextual embeddings into the optimization processes of large language models is an advancement in natural language processing. The Context-Aware Neural Gradient Mapping framework introduces a dynamic gradient adjustment mechanism, incorporating contextual embeddings directly into the optimization process. This approach facilitates real-time parameter adjustments, enhancing task-specific generalization even in the presence of sparse or noisy data inputs. The mathematical foundation of this framework relies on gradient descent modifications, where contextual embeddings are derived from a supplementary neural network trained to map input features to optimal adaptation gradients. By employing differential geometry principles, high-dimensional input dependencies are encoded into low-dimensional gradient manifolds, enabling efficient adaptation without necessitating the retraining of the entire model. Empirical evaluations demonstrate that the proposed framework consistently outperforms baseline models across various metrics, including accuracy, robustness to noise, and computational efficiency. The integration of context-specific embeddings allows for a more complex understanding of language, thereby improving the model's ability to handle diverse linguistic phenomena. Furthermore, the computational efficiency achieved through this method demonstrates its scalability for large-scale language models operating under diverse constraints.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual State Space Models for Structured Question Answering in Indic Languages</title>
<link>https://arxiv.org/abs/2502.01673</link>
<guid>https://arxiv.org/abs/2502.01673</guid>
<content:encoded><![CDATA[
arXiv:2502.01673v2 Announce Type: replace 
Abstract: The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2502.05346</link>
<guid>https://arxiv.org/abs/2502.05346</guid>
<content:encoded><![CDATA[
arXiv:2502.05346v2 Announce Type: replace 
Abstract: Representing token embeddings as probability distributions over learned manifolds allows for more flexible contextual inference, reducing representational rigidity while enhancing semantic granularity. Comparative evaluations demonstrate that probabilistic embeddings improve neighborhood consistency and decrease redundancy, ensuring that token relationships remain more structurally coherent across fine-tuning iterations. The integration of probabilistic subspaces within attention mechanisms facilitates more adaptive contextual weighting, enabling models to capture latent dependencies that would otherwise be obscured in conventional embeddings. Experimental results highlight increased robustness against adversarial modifications, with probabilistic embeddings preserving contextual integrity even under perturbation-based evaluation scenarios. Performance assessments indicate that probabilistic representations achieve greater adaptability in domain-specific applications, mitigating the need for extensive retraining when shifting across linguistic domains. Computational trade-offs remain within operationally feasible limits, with marginal increases in inference latency balanced against the benefits of enhanced representation stability and contextual expressiveness. The capacity to encode structured uncertainty provides advantages in generative modeling tasks, particularly where maintaining coherence across extended sequences requires a representation framework capable of handling ambiguous or context-dependent linguistic constructs.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reasoning Ability of Small Language Models</title>
<link>https://arxiv.org/abs/2502.11569</link>
<guid>https://arxiv.org/abs/2502.11569</guid>
<content:encoded><![CDATA[
arXiv:2502.11569v2 Announce Type: replace 
Abstract: Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSCon: Product Search Through Conversations</title>
<link>https://arxiv.org/abs/2502.13881</link>
<guid>https://arxiv.org/abs/2502.13881</guid>
<content:encoded><![CDATA[
arXiv:2502.13881v2 Announce Type: replace 
Abstract: Conversational Product Search ( CPS ) systems interact with users via natural language to offer personalized and context-aware product lists. However, most existing research on CPS is limited to simulated conversations, due to the lack of a real CPS dataset driven by human-like language. Moreover, existing conversational datasets for e-commerce are constructed for a particular market or a particular language and thus can not support cross-market and multi-lingual usage. In this paper, we propose a CPS data collection protocol and create a new CPS dataset, called PSCon, which assists product search through conversations with human-like language. The dataset is collected by a coached human-human data collection protocol and is available for dual markets and two languages. By formulating the task of CPS, the dataset allows for comprehensive and in-depth research on six subtasks: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Moreover, we present a concise analysis of the dataset and propose a benchmark model on the proposed CPS dataset. Our proposed dataset and model will be helpful for facilitating future research on CPS.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically Evaluating the Paper Reviewing Capability of Large Language Models</title>
<link>https://arxiv.org/abs/2502.17086</link>
<guid>https://arxiv.org/abs/2502.17086</guid>
<content:encoded><![CDATA[
arXiv:2502.17086v2 Announce Type: replace 
Abstract: Peer review is essential for scientific progress, but it faces challenges such as reviewer shortages and growing workloads. Although Large Language Models (LLMs) show potential for providing assistance, research has reported significant limitations in the reviews they generate. While the insights are valuable, conducting the analysis is challenging due to the considerable time and effort required, especially given the rapid pace of LLM developments. To address the challenge, we developed an automatic evaluation pipeline to assess the LLMs' paper review capability by comparing them with expert-generated reviews. By constructing a dataset consisting of 676 OpenReview papers, we examined the agreement between LLMs and experts in their strength and weakness identifications. The results showed that LLMs lack balanced perspectives, significantly overlook novelty assessment when criticizing, and produce poor acceptance decisions. Our automated pipeline enables a scalable evaluation of LLMs' paper review capability over time.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection</title>
<link>https://arxiv.org/abs/2503.07269</link>
<guid>https://arxiv.org/abs/2503.07269</guid>
<content:encoded><![CDATA[
arXiv:2503.07269v2 Announce Type: replace 
Abstract: We present our shared task on text-based emotion detection, covering more than 30 languages from seven distinct language families. These languages are predominantly low-resource and are spoken across various continents. The data instances are multi-labeled with six emotional classes, with additional datasets in 11 languages annotated for emotion intensity. Participants were asked to predict labels in three tracks: (a) multilabel emotion detection, (b) emotion intensity score detection, and (c) cross-lingual emotion detection.
  The task attracted over 700 participants. We received final submissions from more than 200 teams and 93 system description papers. We report baseline results, along with findings on the best-performing systems, the most common approaches, and the most effective methods across different tracks and languages. The datasets for this task are publicly available. The dataset is available at SemEval2025 Task 11 https://brighter-dataset.github.io
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks</title>
<link>https://arxiv.org/abs/2503.10894</link>
<guid>https://arxiv.org/abs/2503.10894</guid>
<content:encoded><![CDATA[
arXiv:2503.10894v2 Announce Type: replace 
Abstract: Mechanistic interpretability has made great strides in identifying neural network features (e.g., directions in hidden activation space) that mediate concepts(e.g., the birth year of a person) and enable predictable manipulation. Distributed alignment search (DAS) leverages supervision from counterfactual data to learn concept features within hidden states, but DAS assumes we can afford to conduct a brute force search over potential feature locations. To address this, we present HyperDAS, a transformer-based hypernetwork architecture that (1) automatically locates the token-positions of the residual stream that a concept is realized in and (2) constructs features of those residual stream vectors for the concept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states. In addition, we review the design decisions we made to mitigate the concern that HyperDAS (like all powerful interpretabilty methods) might inject new information into the target model rather than faithfully interpreting it.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shared Global and Local Geometry of Language Model Embeddings</title>
<link>https://arxiv.org/abs/2503.21073</link>
<guid>https://arxiv.org/abs/2503.21073</guid>
<content:encoded><![CDATA[
arXiv:2503.21073v2 Announce Type: replace 
Abstract: Researchers have recently suggested that models share common representations. In our work, we find that token embeddings of language models exhibit common geometric structure. First, we find ``global'' similarities: token embeddings often share similar relative orientations. Next, we characterize local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each token embedding. Our intrinsic dimension demonstrates that token embeddings lie on a lower dimensional manifold. We qualitatively show that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Both characterizations allow us to find similarities in the local geometry of token embeddings. Perhaps most surprisingly, we find that alignment in token embeddings persists through the hidden states of language models, allowing us to develop an application for interpretability. Namely, we introduce Emb2Emb, a simple method to transfer steering vectors from one language model to another, despite the two models having different dimensions.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Memory in Large Language Models</title>
<link>https://arxiv.org/abs/2504.02441</link>
<guid>https://arxiv.org/abs/2504.02441</guid>
<content:encoded><![CDATA[
arXiv:2504.02441v2 Announce Type: replace 
Abstract: This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Data Are Unlearned Equally</title>
<link>https://arxiv.org/abs/2504.05058</link>
<guid>https://arxiv.org/abs/2504.05058</guid>
<content:encoded><![CDATA[
arXiv:2504.05058v4 Announce Type: replace 
Abstract: Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual MFA: Forced Alignment on Low-Resource Related Languages</title>
<link>https://arxiv.org/abs/2504.07315</link>
<guid>https://arxiv.org/abs/2504.07315</guid>
<content:encoded><![CDATA[
arXiv:2504.07315v2 Announce Type: replace 
Abstract: We compare the outcomes of multilingual and crosslingual training for related and unrelated Australian languages with similar phonological inventories. We use the Montreal Forced Aligner to train acoustic models from scratch and adapt a large English model, evaluating results against seen data, unseen data (seen language), and unseen data and language. Results indicate benefits of adapting the English baseline model for previously unseen languages.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable text data distillation by trajectory matching</title>
<link>https://arxiv.org/abs/2504.09818</link>
<guid>https://arxiv.org/abs/2504.09818</guid>
<content:encoded><![CDATA[
arXiv:2504.09818v2 Announce Type: replace 
Abstract: In the realm of large language model (LLM), as the size of large models increases, it also brings higher training costs. There is a urgent need to minimize the data size in LLM training. Compared with data selection method, the data distillation method aims to synthesize a small number of data samples to achieve the training effect of the full data set and has better flexibility. Despite its successes in computer vision, the discreteness of text data has hitherto stymied its exploration in natural language processing (NLP). In this work, we proposed a method that involves learning pseudo prompt data based on trajectory matching and finding its nearest neighbor ID to achieve cross-architecture transfer. During the distillation process, we introduce a regularization loss to improve the robustness of our distilled data. To our best knowledge, this is the first data distillation work suitable for text generation tasks such as instruction tuning. Evaluations on two benchmarks, including ARC-Easy and MMLU instruction tuning datasets, established the superiority of our distillation approach over the SOTA data selection method LESS. Furthermore, our method demonstrates a good transferability over LLM structures (i.e., OPT to Llama).
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation</title>
<link>https://arxiv.org/abs/2406.14088</link>
<guid>https://arxiv.org/abs/2406.14088</guid>
<content:encoded><![CDATA[
arXiv:2406.14088v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for empowering large language model (LLM) applications. Compared with the supervised training process of LLMs, the RLHF training process is much more sophisticated, requiring a diverse range of computation workloads with intricate dependencies between multiple LLM instances. Therefore, simply adopting the fixed parallelization strategies from supervised training for LLMs can be insufficient for RLHF and result in low training efficiency. To overcome this limitation, we propose a novel technique named parameter ReaLlocation, which dynamically adapts the parallelization strategies for different workloads during training by redistributing LLM parameters across the training cluster. Building upon this idea, we introduce ReaL, a pioneering system for efficient RLHF training. ReaL introduces the concept of an execution plan, which defines a fine-grained resource allocation and parallelization strategy particularly designed for RLHF training. Based on this concept, ReaL employs a tailored search algorithm with a lightweight run-time estimator to automatically discover an efficient execution plan for an instance of RLHF experiment. Subsequently, the runtime engine deploys the selected plan by effectively parallelizing computations and redistributing parameters. We evaluate ReaL on the LLaMA models with up to 70 billion parameters and 128 GPUs. The experimental results demonstrate that ReaL achieves speedups of up to $3.58\times$ compared to baseline methods. Furthermore, the execution plans generated by ReaL exhibit an average of $81\%$ performance improvement over heuristic approaches based on Megatron-LM in the long-context scenario. The source code of ReaL is publicly available at https://github.com/openpsi-project/ReaLHF .
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF</title>
<link>https://arxiv.org/abs/2410.04612</link>
<guid>https://arxiv.org/abs/2410.04612</guid>
<content:encoded><![CDATA[
arXiv:2410.04612v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success at tasks like summarization that involve a single turn of interaction. However, they can still struggle with multi-turn tasks like dialogue that require long-term planning. Previous works on multi-turn dialogue extend single-turn reinforcement learning from human feedback (RLHF) methods to the multi-turn setting by treating all prior dialogue turns as a long context. Such approaches suffer from covariate shift: the conversations in the training set have previous turns generated by some reference policy, which means that low training error may not necessarily correspond to good performance when the learner is actually in the conversation loop. In response, we introduce REgressing the RELative FUture (REFUEL), an efficient policy optimization approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single model to estimate $Q$-values and trains on self-generated data, addressing the covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence of regression tasks on iteratively collected datasets, enabling ease of implementation. Theoretically, we prove that REFUEL can match the performance of any policy covered by the training set. Empirically, we evaluate our algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our model. REFUEL consistently outperforms state-of-the-art methods such as DPO and REBEL across various settings. Furthermore, despite having only 8 billion parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it on long multi-turn dialogues. Implementation of REFUEL can be found at https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be found at https://huggingface.co/Cornell-AGI.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CallNavi, A Challenge and Empirical Study on LLM Function Calling and Routing</title>
<link>https://arxiv.org/abs/2501.05255</link>
<guid>https://arxiv.org/abs/2501.05255</guid>
<content:encoded><![CDATA[
arXiv:2501.05255v2 Announce Type: replace-cross 
Abstract: API-driven chatbot systems are increasingly integral to software engineering applications, yet their effectiveness hinges on accurately generating and executing API calls. This is particularly challenging in scenarios requiring multi-step interactions with complex parameterization and nested API dependencies. Addressing these challenges, this work contributes to the evaluation and assessment of AI-based software development through three key advancements: (1) the introduction of a novel dataset specifically designed for benchmarking API function selection, parameter generation, and nested API execution; (2) an empirical evaluation of state-of-the-art language models, analyzing their performance across varying task complexities in API function generation and parameter accuracy; and (3) a hybrid approach to API routing, combining general-purpose large language models for API selection with fine-tuned models and prompt engineering for parameter generation. These innovations significantly improve API execution in chatbot systems, offering practical methodologies for enhancing software design, testing, and operational workflows in real-world software engineering contexts.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?</title>
<link>https://arxiv.org/abs/2501.15857</link>
<guid>https://arxiv.org/abs/2501.15857</guid>
<content:encoded><![CDATA[
arXiv:2501.15857v3 Announce Type: replace-cross 
Abstract: Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, "FTCT" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing</title>
<link>https://arxiv.org/abs/2503.10742</link>
<guid>https://arxiv.org/abs/2503.10742</guid>
<content:encoded><![CDATA[
arXiv:2503.10742v2 Announce Type: replace-cross 
Abstract: Vision language models (VLMs) demonstrate strong capabilities in jointly processing visual and textual data. However, they often incur substantial computational overhead due to redundant visual information, particularly in long-form video scenarios. Existing approaches predominantly focus on either vision token pruning, which may overlook spatio-temporal dependencies, or keyframe selection, which identifies informative frames but discards others, thus disrupting contextual continuity. In this work, we propose KVTP (Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the drawbacks of token pruning and keyframe selection. By adaptively assigning pruning rates based on frame relevance to the query, KVTP effectively retains essential contextual information while significantly reducing redundant computation. To thoroughly evaluate the long-form video understanding capacities of VLMs, we curated and reorganized subsets from VideoMME, EgoSchema, and NextQA into a unified benchmark named SparseKV-QA that highlights real-world scenarios with sparse but crucial events. Our experiments with VLMs of various scales show that KVTP can reduce token usage by 80% without compromising spatiotemporal and contextual consistency, significantly cutting computation while maintaining the performance. These results demonstrate our approach's effectiveness in efficient long-video processing, facilitating more scalable VLM deployment.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking beyond the next token</title>
<link>https://arxiv.org/abs/2504.11336</link>
<guid>https://arxiv.org/abs/2504.11336</guid>
<content:encoded><![CDATA[
arXiv:2504.11336v2 Announce Type: replace-cross 
Abstract: The structure of causal language model training assumes that each token can be accurately predicted from the previous context. This contrasts with humans' natural writing and reasoning process, where goals are typically known before the exact argument or phrasings. While this mismatch has been well studied in the literature, the working assumption has been that architectural changes are needed to address this mismatch. We argue that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. We demonstrate that this technique, Trelawney, and the inference algorithms derived from it allow us to improve performance on several key benchmarks that span planning, algorithmic reasoning, and story generation tasks. Finally, our method naturally enables the generation of long-term goals at no additional cost. We investigate how using the model's goal-generation capability can further improve planning and reasoning. Additionally, we believe Trelawney could potentially open doors to new capabilities beyond the current language modeling paradigm.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Large Language Models to Reason through Learning and Forgetting</title>
<link>https://arxiv.org/abs/2504.11364</link>
<guid>https://arxiv.org/abs/2504.11364</guid>
<content:encoded><![CDATA[
arXiv:2504.11364v2 Announce Type: replace-cross 
Abstract: Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it using both successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. While fine-tuning the model with these data might seem straightforward, we identify a critical issue: the model's search capability tends to degrade rapidly if fine-tuning is performed naively. We show that this degradation can be substantially mitigated by employing a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown mathematical reasoning benchmarks show that our approach not only outperforms both standard fine-tuning and inference-time search baselines but also significantly reduces inference time by 180$\times$.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking</title>
<link>https://arxiv.org/abs/2504.16188</link>
<guid>https://arxiv.org/abs/2504.16188</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, Financial Natural Language Inference, diverse financial texts, SEC Filings, Annual Reports, Earnings Call transcripts

Summary:
FinNLI is introduced as a benchmark dataset for Financial Natural Language Inference (FinNLI) that includes diverse premise-hypothesis pairs from various financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. The dataset consists of 21,304 pairs, with a high-quality test set of 3,304 instances annotated by finance experts. Evaluations reveal a notable performance degradation in general-domain NLI models when faced with domain shifts. The highest Macro F1 scores for pre-trained language models (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, indicating the dataset's complexity. Interestingly, financial LLMs fine-tuned with instructions perform poorly, highlighting limitations in generalizability. FinNLI exposes shortcomings in current LLMs for financial reasoning, suggesting a need for improvement. 

<br /><br />Summary: <div>
arXiv:2504.16188v1 Announce Type: new 
Abstract: We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset's difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy</title>
<link>https://arxiv.org/abs/2504.16271</link>
<guid>https://arxiv.org/abs/2504.16271</guid>
<content:encoded><![CDATA[
<div> NLP, attachment style, psychotherapy, PACS, classification model <br />
Summary: <br />
This paper introduces the use of Natural Language Processing (NLP) techniques to automatically assess patient attachment style in psychotherapy transcripts. Currently, attachment style assessment is done manually using the PACS system, which is time-consuming and requires extensive training. By implementing NLP classification models, the process can be automated, allowing for more personalized psychotherapy and targeted research on therapy mechanisms. Mislabeling patients' attachment styles can have negative effects on therapy outcomes, so accurate automated assessment is crucial. This research paves the way for widespread adoption of attachment-informed treatment and research in the mental healthcare field. <div>
arXiv:2504.16271v1 Announce Type: new 
Abstract: The delivery of mental healthcare through psychotherapy stands to benefit immensely from developments within Natural Language Processing (NLP), in particular through the automatic identification of patient specific qualities, such as attachment style. Currently, the assessment of attachment style is performed manually using the Patient Attachment Coding System (PACS; Talia et al., 2017), which is complex, resource-consuming and requires extensive training. To enable wide and scalable adoption of attachment informed treatment and research, we propose the first exploratory analysis into automatically assessing patient attachment style from psychotherapy transcripts using NLP classification models. We further analyze the results and discuss the implications of using automated tools for this purpose -- e.g., confusing `preoccupied' patients with `avoidant' likely has a more negative impact on therapy outcomes with respect to other mislabeling. Our work opens an avenue of research enabling more personalized psychotherapy and more targeted research into the mechanisms of psychotherapy through advancements in NLP.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation</title>
<link>https://arxiv.org/abs/2504.16286</link>
<guid>https://arxiv.org/abs/2504.16286</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Chinese-English translation, back-translation, cultural fidelity, NLP performance

Summary:
<br /><br />
1. The study evaluates the performance of large language models in Chinese-English translation, focusing on scientific terminology, historical paradoxes, and literary metaphors.
2. Scientific abstracts benefit from back-translation, while traditional tools perform better in linguistically distinct texts.
3. Large language models struggle with retaining cultural and literary nuances, showcasing challenges in preserving poetic intent.
4. Some models exhibit a tendency towards "verbatim back-translation", indicating emergent memory behavior.
5. A novel BLEU variant utilizing Jieba segmentation and n-gram weighting is proposed to improve translation accuracy and cultural fidelity. This study contributes to a better understanding of Chinese NLP performance and the importance of maintaining cultural integrity in AI-mediated translation efforts. <div>
arXiv:2504.16286v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has reshaped the landscape of machine translation, yet challenges persist in preserving poetic intent, cultural heritage, and handling specialized terminology in Chinese-English translation. This study constructs a diverse corpus encompassing Chinese scientific terminology, historical translation paradoxes, and literary metaphors. Utilizing a back-translation and Friedman test-based evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three traditional translation tools. Key findings include: (1) Scientific abstracts often benefit from back-translation, while traditional tools outperform LLMs in linguistically distinct texts; (2) LLMs struggle with cultural and literary retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit "verbatim back-translation", reflecting emergent memory behavior; (4) A novel BLEU variant using Jieba segmentation and n-gram weighting is proposed. The study contributes to the empirical evaluation of Chinese NLP performance and advances understanding of cultural fidelity in AI-mediated translation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives</title>
<link>https://arxiv.org/abs/2504.16312</link>
<guid>https://arxiv.org/abs/2504.16312</guid>
<content:encoded><![CDATA[
<div> Keywords: symmetric relations, antisymmetric relations, natural language inference, large language models, contrastive learning

Summary:
This paper introduces a new natural language inference dataset derived from Wikidata to evaluate the performance of large language models (LLMs) in capturing symmetric and antisymmetric relations. The findings indicate that LLMs perform at random chance levels on this benchmark, revealing a gap in their ability to understand relations. To address this issue, the paper explores encoder retraining through contrastive learning with k-nearest neighbors. The retrained encoder achieves performance comparable to fine-tuned classification heads while offering advantages such as improved efficiency in few-shot learning and better mitigation of catastrophic forgetting.<br /><br />Summary: <div>
arXiv:2504.16312v1 Announce Type: new 
Abstract: Capturing symmetric (e.g., country borders another country) and antisymmetric (e.g., parent_of) relations is crucial for a variety of applications. This paper tackles this challenge by introducing a novel Wikidata-derived natural language inference dataset designed to evaluate large language models (LLMs). Our findings reveal that LLMs perform comparably to random chance on this benchmark, highlighting a gap in relational understanding. To address this, we explore encoder retraining via contrastive learning with k-nearest neighbors. The retrained encoder matches the performance of fine-tuned classification heads while offering additional benefits, including greater efficiency in few-shot learning and improved mitigation of catastrophic forgetting.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Extraction of Statutory Definitions from the U.S. Code</title>
<link>https://arxiv.org/abs/2504.16353</link>
<guid>https://arxiv.org/abs/2504.16353</guid>
<content:encoded><![CDATA[
<div> extract, definitions, legal texts, U.S. Code, NLP

Summary:
- The study presents an advanced NLP system that uses transformer-based architectures to automatically extract definitions from the U.S. Code.
- The system addresses challenges in identifying legal definitions, extracting defined terms, and determining their scope within the complex corpus.
- It employs domain-specific transformers (Legal-BERT) fine-tuned for statutory texts to improve extraction accuracy.
- The system utilizes a multi-stage pipeline combining document structure analysis and state-of-the-art language models to process legal text from the U.S. Code XML version.
- Evaluation on multiple U.S. Code titles shows the system achieves significant improvements with 96.8% precision and 98.9% recall, surpassing traditional machine learning classifiers. 

<br /><br />Summary: <div>
arXiv:2504.16353v1 Announce Type: new 
Abstract: Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions</title>
<link>https://arxiv.org/abs/2504.16358</link>
<guid>https://arxiv.org/abs/2504.16358</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-TrajVis, trajectory data visualization, dataset construction, TrajVL, Large Language Models

Summary: 
This paper introduces the Text-to-TrajVis task, which transforms natural language questions into trajectory data visualizations. The authors propose the Trajectory Visualization Language (TVL) and develop a dataset construction method to create the TrajVL dataset, containing 18,140 question-TVL pairs. The dataset combines human labeling with Large Language Models (LLMs). The performance of multiple LLMs (GPT, Qwen, Llama) is evaluated on this task, showing its feasibility and difficulty. This novel task presents challenges and opportunities for natural language interfaces for trajectory visualization systems. 

<br /><br />Summary: <div>
arXiv:2504.16358v1 Announce Type: new 
Abstract: This paper introduces the Text-to-TrajVis task, which aims to transform natural language questions into trajectory data visualizations, facilitating the development of natural language interfaces for trajectory visualization systems. As this is a novel task, there is currently no relevant dataset available in the community. To address this gap, we first devised a new visualization language called Trajectory Visualization Language (TVL) to facilitate querying trajectory data and generating visualizations. Building on this foundation, we further proposed a dataset construction method that integrates Large Language Models (LLMs) with human efforts to create high-quality data. Specifically, we first generate TVLs using a comprehensive and systematic process, and then label each TVL with corresponding natural language questions using LLMs. This process results in the creation of the first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140 (question, TVL) pairs. Based on this dataset, we systematically evaluated the performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The experimental results demonstrate that this task is both feasible and highly challenging and merits further exploration within the research community.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitReason: Learning To Offload Reasoning</title>
<link>https://arxiv.org/abs/2504.16379</link>
<guid>https://arxiv.org/abs/2504.16379</guid>
<content:encoded><![CDATA[
<div> annotate, reasoning, large language models, offloading, fine-tuning
Summary:
Large language models (LLMs) often struggle with the sequential and memory-bound decoding phase due to the extended generation length required for reasoning tasks. To address this, a new approach called SplitReason is introduced, which offloads the most challenging segments of the reasoning process to a larger model while using a smaller, more efficient model for the rest. By annotating difficult segments and leveraging supervised and reinforcement learning fine-tuning, the 1.5B-parameter reasoning model can improve reasoning accuracy by 24% and 28.3% while offloading only a small percentage of generated tokens. The approach shows promising results in enhancing efficiency and accuracy in reasoning tasks, particularly on the OpenR1-Math-220k CoT dataset. The SplitReason model, along with data, code, and logs, has been open-sourced for further research and development. 
<br /><br />Summary: <div>
arXiv:2504.16379v1 Announce Type: new 
Abstract: Reasoning in large language models (LLMs) tends to produce substantially longer token generation sequences than simpler language modeling tasks. This extended generation length reflects the multi-step, compositional nature of reasoning and is often correlated with higher solution accuracy. From an efficiency perspective, longer token generation exacerbates the inherently sequential and memory-bound decoding phase of LLMs. However, not all parts of this expensive reasoning process are equally difficult to generate. We leverage this observation by offloading only the most challenging parts of the reasoning process to a larger, more capable model, while performing most of the generation with a smaller, more efficient model; furthermore, we teach the smaller model to identify these difficult segments and independently trigger offloading when needed. To enable this behavior, we annotate difficult segments across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT) dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to offload the most challenging parts of its own reasoning process to a larger model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while offloading 1.35% and 5% of the generated tokens respectively. We open-source our SplitReason model, data, code and logs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2504.16394</link>
<guid>https://arxiv.org/abs/2504.16394</guid>
<content:encoded><![CDATA[
<div> Keywords: unstructured clinical data, clinical text summarization, Contextual, Domain-Specific Knowledge Graph, precision 

Summary: 
Contextual is a new framework designed to extract key information from unstructured clinical data for improved decision-making in patient care. It combines Context-Preserving Token Filtering with a Domain-Specific Knowledge Graph to enhance linguistic coherence and clinical fidelity. By preserving important context-specific tokens and augmenting them with structured knowledge, Contextual excels in both aspects, outperforming other baselines in empirical evaluations on two public benchmark datasets. The approach showcases the significance of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity while offering a scalable solution for enhancing precision in clinical text generation. <div>
arXiv:2504.16394v1 Announce Type: new 
Abstract: Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation</title>
<link>https://arxiv.org/abs/2504.16408</link>
<guid>https://arxiv.org/abs/2504.16408</guid>
<content:encoded><![CDATA[
<div> framework, reverse-prompt induction, retrieval-augmented reasoning synthesis, dual-stage reward-guided filtering, structured inference<br />
Summary: The Less is More approach, which placed third in the XLLM@ACL2025 Shared Task-III, focuses on structured reasoning using only 24 labeled examples. It employs a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis, and dual-stage reward-guided filtering to improve structure reasoning quality. The approach leverages a unified LoRA+ setup and fine-tunes all modules from Meta-Llama-3-8B-Instruct. By combining structure validation with reward filtering across few-shot and zero-shot prompts, the pipeline consistently enhances structured inference under low-resource constraints. This highlights the importance of controllable data distillation in improving structured reasoning quality. The code for this approach is available at https://github.com/Jiahao-Yuan/Less-is-More.<br /> <div>
arXiv:2504.16408v1 Announce Type: new 
Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-the-Box Conditional Text Embeddings from Large Language Models</title>
<link>https://arxiv.org/abs/2504.16411</link>
<guid>https://arxiv.org/abs/2504.16411</guid>
<content:encoded><![CDATA[
<div> Keywords: Conditional text embedding, PonTE, unsupervised, large language model, interpretability

Summary:
PonTE is an unsupervised conditional text embedding method that utilizes a causal large language model and a conditional prompt. It aims to capture the shift in perspective on texts when conditioned on a specific aspect without the need for extensive training data or fine-tuning models. Through experiments on conditional semantic text similarity and text clustering, PonTE has been shown to generate useful conditional text embeddings and achieve performance comparable to supervised methods. The method also demonstrates interpretability of text embeddings by analyzing word generation following prompts and embedding visualization. Overall, PonTE offers a cost-effective and efficient approach to capturing conditional text embeddings without the need for labor-intensive and resource-consuming fine-tuning processes. 

<br /><br />Summary: 
PonTE is an unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt to capture the shift in perspective on texts. It generates useful conditional text embeddings and achieves performance comparable to supervised methods without fine-tuning. The method showcases interpretability through word generation analysis and embedding visualization, offering a cost-effective approach to conditional text embedding. <div>
arXiv:2504.16411v1 Announce Type: new 
Abstract: Conditional text embedding is a proposed representation that captures the shift in perspective on texts when conditioned on a specific aspect. Previous methods have relied on extensive training data for fine-tuning models, leading to challenges in terms of labor and resource costs. We propose PonTE, a novel unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt. Through experiments on conditional semantic text similarity and text clustering, we demonstrate that PonTE can generate useful conditional text embeddings and achieve performance comparable to supervised methods without fine-tuning. We also show the interpretability of text embeddings with PonTE by analyzing word generation following prompts and embedding visualization.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study</title>
<link>https://arxiv.org/abs/2504.16414</link>
<guid>https://arxiv.org/abs/2504.16414</guid>
<content:encoded><![CDATA[
<div> benchmark, compositional reasoning, large language models, chemistry domain, knowledge graph

Summary: 
This study presents a new benchmark for evaluating the compositional reasoning abilities of large language models in the field of chemistry. The researchers developed a fully automated pipeline that integrates OpenAI reasoning models with named entity recognition systems to extract chemical entities from literature and create a comprehensive knowledge graph. The experiments conducted on this benchmark reveal that even state-of-the-art models struggle with multi-hop compositional reasoning tasks. The results emphasize the importance of augmenting language models with document retrieval to improve performance. However, perfect retrieval accuracy does not eliminate all reasoning errors, highlighting the complexity of compositional reasoning tasks. This work not only showcases the limitations of current language models but also introduces a novel data generation pipeline that can be applied to various domains, advancing our understanding of reasoning in computational linguistics.

Summary: <div>
arXiv:2504.16414v1 Announce Type: new 
Abstract: In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark</title>
<link>https://arxiv.org/abs/2504.16427</link>
<guid>https://arxiv.org/abs/2504.16427</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal language analysis, large language models, cognitive-level semantics, benchmark, evaluation

Summary: 
Multimodal language analysis is a growing field that explores the use of multiple modalities to understand human conversational utterances at a deeper level. To address the lack of research on the capability of multimodal large language models (MLLMs) in comprehending cognitive-level semantics, the MMLA benchmark was introduced. This benchmark consists of over 61K multimodal utterances from various scenarios, focusing on six key dimensions of multimodal semantics. The study evaluated eight different branches of LLMs and MLLMs using three different methods. Results showed that even fine-tuned models achieve only moderate accuracy, highlighting the current limitations in understanding complex human language. The MMLA benchmark aims to provide a foundation for further exploration of large language models in multimodal language analysis and offers valuable resources for advancing this field.

<br /><br />Summary: <div>
arXiv:2504.16427v1 Announce Type: new 
Abstract: Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records</title>
<link>https://arxiv.org/abs/2504.16448</link>
<guid>https://arxiv.org/abs/2504.16448</guid>
<content:encoded><![CDATA[
<div> Keywords: medical consultation dialogues, structured electronic medical records, LoRA fine-tuning, natural language processing models, information extraction benchmark<br />
Summary:<br />
The article introduces EMRModel, an innovative approach that combines LoRA-based fine-tuning with code-style prompt design to convert medical consultation dialogues into structured electronic medical records (EMRs). A high-quality dataset of medical consultation dialogues with detailed annotations is created to train and evaluate the model. A fine-grained evaluation benchmark is introduced for medical consultation information extraction, along with a systematic evaluation methodology for optimizing medical NLP models. Experimental results demonstrate that EMRModel achieves an F1 score of 88.1%, significantly outperforming standard pre-trained models and traditional LoRA fine-tuning methods. This showcases the effectiveness of EMRModel in extracting structured medical record information from unstructured dialogues.<br /><br />Summary: <div>
arXiv:2504.16448v1 Announce Type: new 
Abstract: Medical consultation dialogues contain critical clinical information, yet their unstructured nature hinders effective utilization in diagnosis and treatment. Traditional methods, relying on rule-based or shallow machine learning techniques, struggle to capture deep and implicit semantics. Recently, large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight fine-tuning method, have shown promise for structured information extraction. We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning with code-style prompt design, aiming to efficiently convert medical consultation dialogues into structured electronic medical records (EMRs). Additionally, we construct a high-quality, realistically grounded dataset of medical consultation dialogues with detailed annotations. Furthermore, we introduce a fine-grained evaluation benchmark for medical consultation information extraction and provide a systematic evaluation methodology, advancing the optimization of medical natural language processing (NLP) models. Experimental results show EMRModel achieves an F1 score of 88.1%, improving by49.5% over standard pre-trained models. Compared to traditional LoRA fine-tuning methods, our model shows superior performance, highlighting its effectiveness in structured medical record extraction tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.16460</link>
<guid>https://arxiv.org/abs/2504.16460</guid>
<content:encoded><![CDATA[
<div> Vectorization, Telecom, Natural Language Processing, Domain-Specific, NetoAI

Summary: 
NetoAI introduces T-VEC, a specialized vectorization model for the telecom industry. T-VEC is fine-tuned from the gte-Qwen2-1.5B-instruct model with a triplet loss objective on telecom-specific data, resulting in deep integration of domain knowledge. Weight difference analysis validates the extensive modifications across 338 layers of the base model. The open-sourcing of a telecom-specific tokenizer enhances handling of industry jargon. T-VEC outperforms established models with a leading MTEB score and superior performance on a proprietary benchmark, showcasing a strong grasp of industry nuances. This work solidifies NetoAI's position as a leader in telecom AI innovation, offering the community a powerful, open-source tool. 

<br /><br />Summary: <div>
arXiv:2504.16460v1 Announce Type: new 
Abstract: The specialized vocabulary and complex concepts of the telecommunications industry present significant challenges for standard Natural Language Processing models. Generic text embeddings often fail to capture telecom-specific semantics, hindering downstream task performance. We introduce T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet loss objective on a meticulously curated, large-scale dataset of telecom-specific data. Crucially, this process involved substantial modification of weights across 338 layers of the base model, ensuring deep integration of domain knowledge, far exceeding superficial adaptation techniques. We quantify this deep change via weight difference analysis. A key contribution is the development and open-sourcing (MIT License) of the first dedicated telecom-specific tokenizer, enhancing the handling of industry jargon. T-VEC achieves a leading average MTEB score (0.825) compared to established models and demonstrates vastly superior performance (0.9380 vs. less than 0.07) on our internal telecom-specific triplet evaluation benchmark, indicating an exceptional grasp of domain-specific nuances, visually confirmed by improved embedding separation. This work positions NetoAI at the forefront of telecom AI innovation, providing the community with a powerful, deeply adapted, open-source tool.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining</title>
<link>https://arxiv.org/abs/2504.16511</link>
<guid>https://arxiv.org/abs/2504.16511</guid>
<content:encoded><![CDATA[
<div> optimization, data selection, language models, quality, diversity
Summary:<br />
The paper introduces the QuaDMix framework, which aims to optimize the data distribution for large language model pretraining by balancing quality and diversity. It proposes criteria for measuring data quality and employs domain classification for assessing overall dataset diversity. QuaDMix utilizes a parameterized data sampling function to determine the sampling probability of each data point based on quality and diversity labels. Simulated experiments using LightGBM for parameters searching show that QuaDMix outperforms independent strategies for quality and diversity, leading to an average performance improvement of 7.2% across multiple benchmarks. The study emphasizes the importance of balancing data quality and diversity in training language models. <br />Summary: <div>
arXiv:2504.16511v1 Announce Type: new 
Abstract: Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers for Complex Query Answering over Knowledge Hypergraphs</title>
<link>https://arxiv.org/abs/2504.16537</link>
<guid>https://arxiv.org/abs/2504.16537</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, complex query answering, hyper-relational graphs, transformer model, logical operations <br />
<br />
Summary: Complex Query Answering (CQA) using knowledge graphs has advanced with the introduction of hyper-relational graphs to represent real-world data. However, existing models are limited in representing relationships of varying arities. To address this, new datasets JF17k-HCQA and M-FB15k-HCQA were created, containing diverse query types with logical operations. The Logical Knowledge Hypergraph Transformer (LKHGT) model is proposed to answer knowledge hypergraph (KHG) existential first-order queries. It consists of a Projection Encoder for atomic projection and a Logical Encoder for complex operations, both with Type Aware Bias (TAB) for capturing token interactions. Experimental results demonstrate that LKHGT is a top-performing CQA method for KHG and can generalize to new query types. <br /> <div>
arXiv:2504.16537v1 Announce Type: new 
Abstract: Complex Query Answering (CQA) has been extensively studied in recent years. In order to model data that is closer to real-world distribution, knowledge graphs with different modalities have been introduced. Triple KGs, as the classic KGs composed of entities and relations of arity 2, have limited representation of real-world facts. Real-world data is more sophisticated. While hyper-relational graphs have been introduced, there are limitations in representing relationships of varying arity that contain entities with equal contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and M-FB15k-HCQA. Each dataset contains various query types that include logical operations such as projection, negation, conjunction, and disjunction. In order to answer knowledge hypergraph (KHG) existential first-order queries, we propose a two-stage transformer model, the Logical Knowledge Hypergraph Transformer (LKHGT), which consists of a Projection Encoder for atomic projection and a Logical Encoder for complex logical operations. Both encoders are equipped with Type Aware Bias (TAB) for capturing token interactions. Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA method over KHG and is able to generalize to out-of-distribution query types.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression</title>
<link>https://arxiv.org/abs/2504.16574</link>
<guid>https://arxiv.org/abs/2504.16574</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, prompt compression, attention mechanism, reinforcement learning, context structuring

Summary: 
This paper introduces a novel compression framework called Prompt Importance Sampling (PIS) for large language models (LLMs). PIS dynamically compresses prompts by sampling important tokens based on attention scores of hidden states. The framework utilizes a dual-level compression mechanism, quantifying token saliency and implementing adaptive compression through reinforcement learning. It also incorporates a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple benchmarks show that PIS outperforms existing methods in compression performance. Additionally, the framework enhances reasoning efficiency by optimizing context structuring. This work contributes to advancing prompt engineering by providing a theoretical grounding and practical efficiency in managing context for LLMs. 

Summary:<br /><br />Keywords: language models, prompt compression, attention mechanism, reinforcement learning, context structuring
This paper introduces Prompt Importance Sampling (PIS) for large language models, a compression framework that samples important tokens based on attention scores. PIS utilizes a dual-level compression mechanism, including reinforcement learning for adaptive compression. It also incorporates sentence-level importance sampling. Evaluation results demonstrate superior compression performance compared to existing methods. The framework also enhances reasoning efficiency by optimizing context structuring. This work contributes to prompt engineering by offering theoretical grounding and practical efficiency for managing context in LLMs. <div>
arXiv:2504.16574v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study</title>
<link>https://arxiv.org/abs/2504.16601</link>
<guid>https://arxiv.org/abs/2504.16601</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, traditional machine translation, medical consultation summaries, evaluation, clinical relevance <br />
<br />
Summary: 
This study compared the performance of large language models (LLMs) and traditional machine translation (MT) tools in translating medical consultation summaries from English to Arabic, Chinese, and Vietnamese. The results indicated that traditional MT tools generally outperformed LLMs, especially for complex texts. LLMs showed promise in translating simpler summaries, particularly in Vietnamese and Chinese. Arabic translations improved with complexity due to the language's morphology. However, LLMs still lack consistency, and the current evaluation metrics do not accurately capture clinical relevance. The study emphasizes the importance of domain-specific training, enhanced evaluation methods, and human oversight in medical translation. Overall, while LLMs offer contextual flexibility, improvements are needed for their effectiveness in translating medical texts accurately. <br /><br />Summary: <div>
arXiv:2504.16601v1 Announce Type: new 
Abstract: This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</title>
<link>https://arxiv.org/abs/2504.16604</link>
<guid>https://arxiv.org/abs/2504.16604</guid>
<content:encoded><![CDATA[
<div> Keywords: Counterspeech, Large Language Models, Conspiracy theories, Psychological research, GPT-4o

Summary: 
Large Language Models (LLMs) like GPT-4o, Llama 3, and Mistral have the potential to help counter harmful online content such as conspiracy theories. However, the effectiveness of these models in applying expert-crafted counterspeech strategies remains under-researched. The study found that these models often generate generic, repetitive, or superficial responses when provided with structured prompts derived from psychological research. They also tend to over-acknowledge fear and frequently hallucinate facts, sources, or figures, which could pose challenges in practical applications for countering conspiracy theories. This research highlights the need for further investigation and development to improve the ability of LLMs to effectively address conspiracy theories with expert-driven counterspeech.<br /><br />Summary: <div>
arXiv:2504.16604v1 Announce Type: new 
Abstract: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2504.16627</link>
<guid>https://arxiv.org/abs/2504.16627</guid>
<content:encoded><![CDATA[
<div> retrieve, fact-checked claims, monolingual, crosslingual, disinformation <br />
<br />
The article addresses the challenge of retrieving previously fact-checked claims in both monolingual and crosslingual settings to combat the spread of disinformation. The proposed approach involves a two-stage strategy utilizing a baseline retrieval system with a fine-tuned embedding model and an LLM-based reranker. The key innovation lies in the use of LLM-based translation to overcome obstacles in multilingual information retrieval. Furthermore, the focus is on ensuring that the majority of the pipeline can be replicated on a consumer GPU. The integrated system achieved high success scores of 0.938 and 0.81025 on monolingual and crosslingual test sets, respectively. <br /><br />Summary: <div>
arXiv:2504.16627v1 Announce Type: new 
Abstract: We address the challenge of retrieving previously fact-checked claims in monolingual and crosslingual settings - a critical task given the global prevalence of disinformation. Our approach follows a two-stage strategy: a reliable baseline retrieval system using a fine-tuned embedding model and an LLM-based reranker. Our key contribution is demonstrating how LLM-based translation can overcome the hurdles of multilingual information retrieval. Additionally, we focus on ensuring that the bulk of the pipeline can be replicated on a consumer GPU. Our final integrated system achieved a success@10 score of 0.938 and 0.81025 on the monolingual and crosslingual test sets, respectively.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics</title>
<link>https://arxiv.org/abs/2504.16677</link>
<guid>https://arxiv.org/abs/2504.16677</guid>
<content:encoded><![CDATA[
<div> transfer, dynamics, language models, multilingual data, post-training<br />
<br />
This study explores the dynamics of cross-lingual transfer in large language models that are fine-tuned on multilingual data. By investigating different post-training settings and tasks of varying complexity, the study examines how cross-lingual transfer and multilingual performance are influenced. The research includes two model families with up to 35B parameters trained on tasks like summarization, instruction following, and mathematical reasoning in both single-task and multi-task instruction tuning scenarios. The results show that the effectiveness of cross-lingual transfer cannot be explained by isolated variables alone and varies depending on the post-training settings. The study identifies the conditions that promote successful cross-lingual transfer in practice, shedding light on the intricate dynamics at play in enabling large language models to be useful across different languages.<br /><br />Summary: <div>
arXiv:2504.16677v1 Announce Type: new 
Abstract: In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations</title>
<link>https://arxiv.org/abs/2504.16754</link>
<guid>https://arxiv.org/abs/2504.16754</guid>
<content:encoded><![CDATA[
<div> transformer, memory architecture, coherent dialogues, factual recall accuracy, privacy-aware conversational AI
Summary:
Compact Memory and Vector Memory are combined in HEMA to create a dual-memory system for maintaining coherent dialogues in large language models. Experimental results show significant improvements in factual recall accuracy and human-rated coherence. Vector Memory with 10K indexed chunks achieves high precision and recall rates, outperforming summarization-only approaches. Ablation studies reveal the benefits of age-weighted pruning for reducing retrieval latency and the use of a two-level summary hierarchy to prevent errors in long conversations. HEMA's approach of combining verbatim recall with semantic continuity enables privacy-aware conversational AI capable of month-long dialogues without the need for model retraining. <div>
arXiv:2504.16754v1 Announce Type: new 
Abstract: Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Effective are Generative Large Language Models in Performing Requirements Classification?</title>
<link>https://arxiv.org/abs/2504.16768</link>
<guid>https://arxiv.org/abs/2504.16768</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer-based large language models, requirements engineering, generative LLMs, requirements classification, experimental study

Summary:
In recent years, transformer-based large language models (LLMs) have been widely used in natural language processing, including requirements engineering (RE) tasks like trace-link detection and regulatory compliance. While non-generative LLMs such as BERT have been successful in requirements classification, there has been limited exploration of generative LLMs. This study evaluates the performance of generative LLMs (Bloom, Gemma, Llama) in binary and multi-class requirements classification using three datasets. The results highlight the importance of factors like prompt design and LLM architecture, as well as the impact of dataset variations on classification task complexity. The findings suggest the need to optimize prompt structures and align model architectures with specific task requirements for improved performance. The insights from this study can guide future model development and deployment strategies in requirements engineering. 

<br /><br />Summary: <div>
arXiv:2504.16768v1 Announce Type: new 
Abstract: In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Framework for AI Systems in "the Wild"</title>
<link>https://arxiv.org/abs/2504.16778</link>
<guid>https://arxiv.org/abs/2504.16778</guid>
<content:encoded><![CDATA[
<div> Evaluation, GenAI systems, real world, holistic, continuous<br />
Summary:<br />
This white paper discusses the shortcomings of current evaluation methods for Generative AI (GenAI) models and proposes a comprehensive framework for assessing real-world GenAI systems. It emphasizes the importance of diverse and evolving inputs, as well as the need for holistic, dynamic, and ongoing assessment approaches. The paper provides guidance for practitioners on designing evaluation methods that accurately reflect real-time capabilities, and offers recommendations for policymakers to focus on societal impacts rather than fixed performance metrics. It advocates for frameworks that integrate performance, fairness, and ethics, and suggests the use of continuous, outcome-oriented assessment methods that combine human and automated evaluations. Transparency is highlighted as crucial for building trust among stakeholders. By implementing these strategies, GenAI models can be both technically proficient and ethically responsible, ensuring their overall impact on society is positive. <br /> <div>
arXiv:2504.16778v1 Announce Type: new 
Abstract: Generative AI (GenAI) models have become vital across industries, yet current evaluation methods have not adapted to their widespread use. Traditional evaluations often rely on benchmarks and fixed datasets, frequently failing to reflect real-world performance, which creates a gap between lab-tested outcomes and practical applications. This white paper proposes a comprehensive framework for how we should evaluate real-world GenAI systems, emphasizing diverse, evolving inputs and holistic, dynamic, and ongoing assessment approaches. The paper offers guidance for practitioners on how to design evaluation methods that accurately reflect real-time capabilities, and provides policymakers with recommendations for crafting GenAI policies focused on societal impacts, rather than fixed performance numbers or parameter sizes. We advocate for holistic frameworks that integrate performance, fairness, and ethics and the use of continuous, outcome-oriented methods that combine human and automated assessments while also being transparent to foster trust among stakeholders. Implementing these strategies ensures GenAI models are not only technically proficient but also ethically responsible and impactful.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores</title>
<link>https://arxiv.org/abs/2504.16786</link>
<guid>https://arxiv.org/abs/2504.16786</guid>
<content:encoded><![CDATA[
<div> Keywords: token-classification, long-context compression, outlier scores, BERT-based compressor, resource-constrained environments

Summary:
MOOSComp is a proposed method for long-context compression in language models, addressing challenges in inference time and resource consumption. It enhances a BERT-based compressor by tackling the over-smoothing issue and incorporating outlier scores to preserve critical tokens. In the training phase, an inter-class cosine similarity loss term is added to improve token classification accuracy. During compression, outlier scores are used to retain rare but important tokens that may be discarded in traditional compression methods. The method demonstrates superior performance on long-context understanding and reasoning benchmarks at various compression ratios. It also achieves a 3.3x speedup at a 4x compression ratio on a resource-constrained mobile device. This approach shows promise in improving the efficiency and effectiveness of language model compression techniques. 

Summary: <div>
arXiv:2504.16786v1 Announce Type: new 
Abstract: Recent advances in large language models have significantly improved their ability to process long-context input, but practical applications are challenged by increased inference time and resource consumption, particularly in resource-constrained environments. To address these challenges, we propose MOOSComp, a token-classification-based long-context compression method that enhances the performance of a BERT-based compressor by mitigating the over-smoothing problem and incorporating outlier scores. In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy. During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression. These scores are integrated with the classifier's output, making the compressor more generalizable to various tasks. Superior performance is achieved at various compression ratios on long-context understanding and reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x compression ratio on a resource-constrained mobile device.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credible plan-driven RAG method for Multi-hop Question Answering</title>
<link>https://arxiv.org/abs/2504.16787</link>
<guid>https://arxiv.org/abs/2504.16787</guid>
<content:encoded><![CDATA[
<div> Keyword: Multi-hop question answering, Retrieval-Augmented Generation, Plan-then-Act-and-Review (PAR RAG) framework, reasoning paths, error propagation

Summary:
The article introduces the Plan-then-Act-and-Review (PAR RAG) framework for multi-hop question answering, which addresses the challenge of error propagation in complex queries. The framework consists of three stages: planning, act, and review, providing an interpretable and incremental reasoning paradigm to enhance accuracy and reliability in answering multi-hop questions. PAR RAG decomposes queries into logical reasoning paths and incorporates a plan execution mechanism based on multi-granularity verification to adjust intermediate results. This approach prevents deviations in reasoning paths and errors from accumulating, ensuring the accuracy of the entire reasoning process. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework outperforms existing state-of-the-art methods in EM and F1 scores.
<br /><br />Summary: <div>
arXiv:2504.16787v1 Announce Type: new 
Abstract: Multi-hop question answering (QA) presents a considerable challenge for Retrieval-Augmented Generation (RAG), requiring the structured decomposition of complex queries into logical reasoning paths and the generation of dependable intermediate results. However, deviations in reasoning paths or errors in intermediate results, which are common in current RAG methods, may propagate and accumulate throughout the reasoning process, diminishing the accuracy of the answer to complex queries. To address this challenge, we propose the Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key stages: planning, act, and review, and aims to offer an interpretable and incremental reasoning paradigm for accurate and reliable multi-hop question answering by mitigating error propagation.PAR RAG initially applies a top-down problem decomposition strategy, formulating a comprehensive plan that integrates multiple executable steps from a holistic viewpoint. This approach avoids the pitfalls of local optima common in traditional RAG methods, ensuring the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a plan execution mechanism based on multi-granularity verification. By utilizing both coarse-grained similarity information and fine-grained relevant data, the framework thoroughly checks and adjusts intermediate results, ensuring process accuracy while effectively managing error propagation and amplification. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework substantially outperforms existing state-of-the-art methods in key metrics, including EM and F1 scores.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention</title>
<link>https://arxiv.org/abs/2504.16795</link>
<guid>https://arxiv.org/abs/2504.16795</guid>
<content:encoded><![CDATA[
<div> Hierarchical Sparse Attention, RNNs, Transformers, long sequences, attention mechanisms
<br />
Summary:
Hierarchical Sparse Attention (HSA) is proposed to enhance RNNs by allowing long-range random access flexibility while maintaining efficiency. HSA divides inputs into chunks, hierarchically aggregating information and learning token-to-chunk relevance for precise chunk selection. A hardware-aligned kernel design makes HSA efficient. Combining HSA with Mamba results in RAMba, achieving perfect accuracy in passkey retrieval despite pre-training on short contexts. RAMba shows significant improvements on downstream tasks with consistent memory usage, indicating potential in long-context modeling. <div>
arXiv:2504.16795v1 Announce Type: new 
Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-assisted Graph-RAG Information Extraction from IFC Data</title>
<link>https://arxiv.org/abs/2504.16813</link>
<guid>https://arxiv.org/abs/2504.16813</guid>
<content:encoded><![CDATA[
<div> IFC data, Graph-RAG, LLMs, building information standard, construction industry
<br />
Utilizing Graph Retrieval-Augmented Generation (Graph-RAG), this research investigates the parsing of complex IFC data using generative Large Language Models (LLMs) like GPT-4o. The goal is to extract building object properties and their relationships from the intricate hierarchy of IFC data. Despite the complexity of IFC data, the Graph-RAG parsing approach proves effective in enhancing LLMs with graph-based knowledge, allowing for natural language query-response retrieval without the need for a complicated pipeline. The study showcases how LLMs can leverage Graph-RAG to improve the understanding and utilization of IFC data within the construction industry, highlighting the potential for streamlining collaborative work processes. 
<br /><br />Summary: <div>
arXiv:2504.16813v1 Announce Type: new 
Abstract: IFC data has become the general building information standard for collaborative work in the construction industry. However, IFC data can be very complicated because it allows for multiple ways to represent the same product information. In this research, we utilise the capabilities of LLMs to parse the IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to retrieve building object properties and their relations. We will show that, despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG parsing enhances generative LLMs like GPT-4o with graph-based knowledge, enabling natural language query-response retrieval without the need for a complex pipeline.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning</title>
<link>https://arxiv.org/abs/2504.16832</link>
<guid>https://arxiv.org/abs/2504.16832</guid>
<content:encoded><![CDATA[
<div> reasoning model, Vietnamese, GreenMind-Medium-14B-R1, Group Relative Policy Optimization, Sentence Transformer-based models <br />
Summary: Chain-of-Thought (CoT) introduces GreenMind-Medium-14B-R1, a Vietnamese reasoning model fine-tuned using Group Relative Policy Optimization. The model utilizes a Vietnamese synthesized reasoning dataset and implements two reward functions to address language mixing and ensure factual correctness. Experimental results on the VLSP 2023 Challenge dataset prove the model's superiority over previous methods and its enhanced linguistic consistency. Evaluation on SeaExam, a multilingual multiple-choice dataset, highlights the model's effectiveness compared to few-shot prompting techniques. The model's innovative approach showcases significant advancements in tackling LLM tasks that require intermediate reasoning steps prior to generating final answers. <div>
arXiv:2504.16832v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo Planning with Large Language Model for Text-Based Game Agents</title>
<link>https://arxiv.org/abs/2504.16855</link>
<guid>https://arxiv.org/abs/2504.16855</guid>
<content:encoded><![CDATA[
<div> Keywords: text-based games, Monte Carlo Tree Search, reinforcement learning, Large Language Models, language understanding

Summary:
The paper introduces the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm for text-based games. Traditional planning-then-learning paradigms like Monte Carlo Tree Search (MCTS) are time-consuming and lack language understanding and reasoning abilities. MC-DML combines Large Language Models (LLMs) with tree search algorithms to leverage language understanding and reasoning capabilities. The algorithm enhances LLMs with memory mechanisms for learning from past experiences and dynamically adjusting action evaluations during planning. Experiments on text-based games show that MC-DML outperforms other methods in the initial planning phase, demonstrating its effectiveness in language-grounded planning in complex environments. This research opens the door to more efficient language-driven planning in autonomous agents. 

<br /><br />Summary: <div>
arXiv:2504.16855v1 Announce Type: new 
Abstract: Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification</title>
<link>https://arxiv.org/abs/2504.16856</link>
<guid>https://arxiv.org/abs/2504.16856</guid>
<content:encoded><![CDATA[
<div> data synthesis, sentiment analysis, large language models, emotion understanding, Emo Pillars  

Summary:
The article introduces a novel approach to address the limitations of sentiment analysis datasets lacking context and emotion categories. By leveraging a large language model, Mistral-7b, the researchers design a data synthesis pipeline to generate training examples for lightweight BERT-type encoder models. The focus is on increasing the semantic diversity of examples and grounding the generation in a corpus of narratives to produce unique story-character-centered utterances across 28 emotion classes. Through extensive inferences, the dataset of 100K contextual and 300K context-less examples is created. Fine-tuning pre-trained encoders results in the development of Emo Pillars models, which demonstrate high adaptability to new domains and task-specific tuning. The models achieve state-of-the-art performance in sentiment analysis tasks like GoEmotions, ISEAR, and IEMOCAP. Validation through statistical analysis and human evaluation confirms the success of measures in utterance diversification and context personalization, while highlighting the need for improved handling of out-of-taxonomy labels within the pipeline. 

<br /><br />Summary: <div>
arXiv:2504.16856v1 Announce Type: new 
Abstract: Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Diffusion Models for Target-Oriented Dialogue Systems</title>
<link>https://arxiv.org/abs/2504.16858</link>
<guid>https://arxiv.org/abs/2504.16858</guid>
<content:encoded><![CDATA[
<div> Dialogue Planning, Target-Oriented Dialogue, LLM era, Non-Sequential Planning, Diffusion Models <br />
Summary:<br />
The article introduces DiffTOD, a novel framework for Target-Oriented Dialogue planning in the era of Large Language Models (LLM). It addresses the limitations of existing sequential planning methods by leveraging diffusion models to enable non-sequential planning. DiffTOD formulates dialogue planning as a trajectory generation problem with guidance and utilizes a diffusion language model to estimate dialogue trajectory likelihood. It introduces tailored guidance mechanisms for different target types to optimize action strategies in diverse TOD scenarios. Through extensive experiments, DiffTOD demonstrates non-myopic lookahead exploration and effective optimization of action strategies over long horizons. The framework showcases flexibility and strong performance across complex dialogue settings. The code and data are available for accessibility. <br /> <div>
arXiv:2504.16858v1 Announce Type: new 
Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://anonymous.4open.science/r/DiffTOD.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models know who did what to whom?</title>
<link>https://arxiv.org/abs/2504.16884</link>
<guid>https://arxiv.org/abs/2504.16884</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, understanding, thematic roles, sentence representations, attention heads

Summary:
Large Language Models (LLMs) are criticized for lacking language understanding, focusing on cognitive abilities different from language processing. This study investigates whether LLMs can capture thematic roles in sentences, linked closely to language. Two experiments analyze sentence representations in four LLMs. Contrary to human judgments, LLMs prioritize syntactic similarity over agent and patient assignments' accuracy. Thematic role information doesn't seem prominent in hidden units but is detected in some attention heads consistently. While LLMs can extract thematic roles, they represent them weaker compared to humans. Thematic roles are captured independently of syntax by attention heads, suggesting the potential for LLMs to understand this aspect of language, though less prominently than expected. 

<br /><br />Summary: <div>
arXiv:2504.16884v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text</title>
<link>https://arxiv.org/abs/2504.16913</link>
<guid>https://arxiv.org/abs/2504.16913</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text, detection, language model, Chain-of-Thought reasoning, interpretability

Summary:
COT Fine-tuned is a new framework for detecting AI-generated text and identifying the responsible language model. It uses a dual-task approach, classifying text as AI-generated or human-written (Task A) and identifying the specific LLM (Task B). The model utilizes Chain-of-Thought reasoning to explain its predictions, enhancing transparency and interpretability. Experimental results show high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. The CoT reasoning process significantly contributes to the model's effectiveness and interpretability. Overall, COT Fine-tuned offers a promising solution for detecting AI-generated text and attributing it to specific language models.`<br /><br />Summary:` <div>
arXiv:2504.16913v1 Announce Type: new 
Abstract: In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</title>
<link>https://arxiv.org/abs/2504.16918</link>
<guid>https://arxiv.org/abs/2504.16918</guid>
<content:encoded><![CDATA[
<div> Keywords: Optimization, Natural Language Processing, Artificial Intelligence, Multi-agent Collaboration, Performance Improvement

Summary: 
OptimAI is a framework that uses AI agents powered by LLM to solve optimization problems described in natural language, outperforming existing methods. It consists of a formulator for translating natural language to mathematical formulations, a planner for high-level solution strategies, and coder and code critic roles for interaction and feedback. Ablation studies show the importance of all roles, with removing planner or code critic resulting in productivity drops. Incorporating UCB-based debug scheduling for plan switching enhances productivity. The framework emphasizes multi-agent collaboration for exploring diverse models within a unified system. Its accuracy on NLP4LP dataset and Optibench subset demonstrates significant error rate reduction compared to prior results. <div>
arXiv:2504.16918v1 Announce Type: new 
Abstract: Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IberBench: LLM Evaluation on Iberian Languages</title>
<link>https://arxiv.org/abs/2504.16921</link>
<guid>https://arxiv.org/abs/2504.16921</guid>
<content:encoded><![CDATA[
<div> evaluation, language models, benchmark, NLP tasks, Iberian Peninsula

Summary:<br />
- Large Language Models (LLMs) are challenging to evaluate comprehensively, especially in languages other than English.
- Existing benchmarks are primarily English-centric and overlook the diversity of language varieties.
- IberBench is a new benchmark designed to assess LLM performance on fundamental and industry-relevant NLP tasks in languages spoken across the Iberian Peninsula and Ibero-America.
- The benchmark integrates 101 datasets covering 22 task categories and allows for continual updates and community-driven submissions.
- Evaluating 23 LLMs of various sizes, the study reveals that LLMs perform better in fundamental NLP tasks than in industrial tasks, with varying performance across different languages and tasks. <div>
arXiv:2504.16921v1 Announce Type: new 
Abstract: Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Speech, Semantic Competence, and AI</title>
<link>https://arxiv.org/abs/2504.16092</link>
<guid>https://arxiv.org/abs/2504.16092</guid>
<content:encoded><![CDATA[
<div> knowledge, cooperative speech, respect, language models, semantic competence
Summary:
Cooperative speech aims at transmitting knowledge with respect, entailing a mutual obligation to reciprocate respect. However, large language models (LLMs) do not exhibit the necessary respect of cooperative interlocutors. This lack of respect suggests that LLMs are not capable of making assertions, calling into question their semantic competence. Moreover, the discussion on meaning goes beyond cognitive psychology to include moral psychology. In essence, the absence of reciprocal respect in LLMs challenges their role in cooperative communication and raises doubts about their ability to convey true knowledge through language. <div>
arXiv:2504.16092v1 Announce Type: cross 
Abstract: Cooperative speech is purposive. From the speaker's perspective, one crucial purpose is the transmission of knowledge. Cooperative speakers care about getting things right for their conversational partners. This attitude is a kind of respect. Cooperative speech is an ideal form of communication because participants have respect for each other. And having respect within a cooperative enterprise is sufficient for a particular kind of moral standing: we ought to respect those who have respect for us. Respect demands reciprocity. I maintain that large language models aren't owed the kind of respect that partly constitutes a cooperative conversation. This implies that they aren't cooperative interlocutors, otherwise we would be obliged to reciprocate the attitude. Leveraging this conclusion, I argue that present-day LLMs are incapable of assertion and that this raises an overlooked doubt about their semantic competence. One upshot of this argument is that knowledge of meaning isn't just a subject for the cognitive psychologist. It's also a subject for the moral psychologist.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing</title>
<link>https://arxiv.org/abs/2504.16112</link>
<guid>https://arxiv.org/abs/2504.16112</guid>
<content:encoded><![CDATA[
<div> Keywords: attention layer, Transformer-based LLMs, High-bandwidth Processing Unit, memory-intensive co-processor, GPU offloading

Summary: 
The paper introduces the concept of a High-bandwidth Processing Unit (HPU) designed to address inefficiencies in GPU systems caused by the attention layer in Transformer-based Large Language Models (LLMs). The HPU serves as a memory-intensive co-processor that offloads memory-bound operations from the GPU, allowing it to focus on compute-intensive tasks, ultimately improving overall efficiency. By implementing the HPU as an add-on card to the GPU system, it can scale out to meet the growing memory demands of large batch sizes and extended sequence lengths. The prototype HPU, utilizing PCIe-based FPGA cards, demonstrated significant performance gains of up to 4.1x and energy efficiency improvements of 4.6x compared to a GPU-only system. This GPU-HPU heterogeneous system offers scalability without the need for additional GPUs, making it a promising solution for enhancing the performance of large-batched LLM inference. 

<br /><br />Summary: <div>
arXiv:2504.16112v1 Announce Type: cross 
Abstract: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval</title>
<link>https://arxiv.org/abs/2504.16121</link>
<guid>https://arxiv.org/abs/2504.16121</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Processing, regulatory documents, bilingual question-answering, Retrieval Augmented Generation, Bangladesh Police Gazettes

Summary:
Our study focuses on bridging the gap in utilizing NLP and computational linguistic techniques in legal and regulatory tasks, particularly in analyzing the bilingual Bangladesh Police Gazettes. We introduce an efficient bilingual question-answering framework that leverages modern Retrieval Augmented Generation (RAG) pipelines to enhance information retrieval and response generation. Our advanced RAG-based approach significantly improves retrieval performance, resulting in more precise answers and making legal information more accessible. By evaluating both our proposed and conventional RAG systems on a diverse test set of Bangladesh Police Gazettes, we consistently outperform existing methods across all evaluation metrics. This framework not only streamlines the process of searching for specific government legal notices but also showcases the potential for NLP in the legal and regulatory domain. 

<br /><br />Summary: <div>
arXiv:2504.16121v1 Announce Type: cross 
Abstract: Natural Language Processing (NLP) and computational linguistic techniques are increasingly being applied across various domains, yet their use in legal and regulatory tasks remains limited. To address this gap, we develop an efficient bilingual question-answering framework for regulatory documents, specifically the Bangladesh Police Gazettes, which contain both English and Bangla text. Our approach employs modern Retrieval Augmented Generation (RAG) pipelines to enhance information retrieval and response generation. In addition to conventional RAG pipelines, we propose an advanced RAG-based approach that improves retrieval performance, leading to more precise answers. This system enables efficient searching for specific government legal notices, making legal information more accessible. We evaluate both our proposed and conventional RAG systems on a diverse test set on Bangladesh Police Gazettes, demonstrating that our approach consistently outperforms existing methods across all evaluation metrics.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends</title>
<link>https://arxiv.org/abs/2504.16134</link>
<guid>https://arxiv.org/abs/2504.16134</guid>
<content:encoded><![CDATA[
<div> Keywords: Traffic safety, Multimodal Large Language Models, Advanced Driver-Assistance Systems, adversarial robustness, scene understanding

Summary: 
Multimodal Large Language Models (MLLMs) have the potential to revolutionize traffic safety by integrating cross-modal data for holistic scene understanding. This review explores how MLLMs can enhance perception, decision-making, and adversarial robustness in dynamic real-world scenarios, where traditional Advanced Driver-Assistance Systems often struggle. By leveraging datasets such as KITTI, DRAMA, and ML4RoadSafety, researchers are advancing the field to improve road safety. Future directions include real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, scalable and context-aware solutions can be developed to proactively mitigate risks on the road. <div>
arXiv:2504.16134v1 Announce Type: cross 
Abstract: Traffic safety remains a critical global challenge, with traditional Advanced Driver-Assistance Systems (ADAS) often struggling in dynamic real-world scenarios due to fragmented sensor processing and susceptibility to adversarial conditions. This paper reviews the transformative potential of Multimodal Large Language Models (MLLMs) in addressing these limitations by integrating cross-modal data such as visual, spatial, and environmental inputs to enable holistic scene understanding. Through a comprehensive analysis of MLLM-based approaches, we highlight their capabilities in enhancing perception, decision-making, and adversarial robustness, while also examining the role of key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research. Furthermore, we outline future directions, including real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, this review underscores their potential to revolutionize the field, offering scalable, context-aware solutions that proactively mitigate risks and improve overall road safety.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design</title>
<link>https://arxiv.org/abs/2504.16204</link>
<guid>https://arxiv.org/abs/2504.16204</guid>
<content:encoded><![CDATA[
<div> framework, prompt engineering, AI systems, societal values, ethical considerations <br />
Summary: Responsible prompt engineering is crucial for ensuring that generative AI systems serve society's needs while minimizing potential harms. This article introduces a comprehensive framework consisting of prompt design, system selection, system configuration, performance evaluation, and prompt management. By embedding ethical and legal considerations directly into AI interactions, organizations can promote improved societal outcomes and mitigate risks. The balance between technical precision and ethical consciousness is key in responsible prompt engineering. This approach aligns with "Responsibility by Design" principles, integrating ethical considerations into the implementation process. Real-world and emerging practices demonstrate the significance of responsible prompt engineering as a bridge between AI development and deployment. The article also highlights key research directions and practical guidelines for advancing the field.<br /><br /> <div>
arXiv:2504.16204v1 Announce Type: cross 
Abstract: Responsible prompt engineering has emerged as a critical framework for ensuring that generative artificial intelligence (AI) systems serve society's needs while minimizing potential harms. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. This article examines how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of real-world and emerging practices, the article illustrates how responsible prompt engineering serves as a crucial bridge between AI development and deployment, enabling organizations to fine-tune AI outputs without modifying underlying model architectures. This approach aligns with broader "Responsibility by Design" principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Phonemes in cascaded S2S translation pipeline</title>
<link>https://arxiv.org/abs/2504.16234</link>
<guid>https://arxiv.org/abs/2504.16234</guid>
<content:encoded><![CDATA[
<div> phonemes, multilingual, speech-to-speech translation, sequence-to-sequence model, BLEU metric

Summary:<br /><br />This paper explores using phonemes instead of text-based language representations in a multilingual speech-to-speech translation system. The study trained a sequence-to-sequence model on the WMT17 dataset in two formats: standard textual representation and phonemic representation. Performance was evaluated using the BLEU metric, showing that the phonemic approach offers comparable quality with advantages such as lower resource requirements and better suitability for low-resource languages. This suggests that incorporating phonemes into translation pipelines can be a viable alternative to traditional text-based methods, particularly for languages with limited resources. <div>
arXiv:2504.16234v1 Announce Type: cross 
Abstract: This paper explores the idea of using phonemes as a textual representation within a conventional multilingual simultaneous speech-to-speech translation pipeline, as opposed to the traditional reliance on text-based language representations. To investigate this, we trained an open-source sequence-to-sequence model on the WMT17 dataset in two formats: one using standard textual representation and the other employing phonemic representation. The performance of both approaches was assessed using the BLEU metric. Our findings shows that the phonemic approach provides comparable quality but offers several advantages, including lower resource requirements or better suitability for low-resource languages.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents</title>
<link>https://arxiv.org/abs/2504.16264</link>
<guid>https://arxiv.org/abs/2504.16264</guid>
<content:encoded><![CDATA[
<div> dataset, cross-lingual information retrieval, academic search, retrieval methods, multilingual retrievers <br />
<br />
Summary: <br />
The paper introduces CLIRudit, a dataset for evaluating cross-lingual academic search between English queries and French documents. Various zero-shot retrieval methods were benchmarked on the dataset, revealing that large dense retrievers can achieve comparable performance to human translations without using machine translation. Sparse retrievers, like BM25 or SPLADE, combined with document translation, also show competitive results. This research enhances understanding of cross-lingual academic information retrieval and offers a framework for creating similar datasets in different languages and disciplines. The dataset and code are made publicly available to facilitate further research on improving access to scientific knowledge across language barriers. <br /> <div>
arXiv:2504.16264v1 Announce Type: cross 
Abstract: Cross-lingual information retrieval (CLIR) consists in finding relevant documents in a language that differs from the language of the queries. This paper presents CLIRudit, a new dataset created to evaluate cross-lingual academic search, focusing on English queries and French documents. The dataset is built using bilingual article metadata from \'Erudit, a Canadian publishing platform, and is designed to represent scenarios in which researchers search for scholarly content in languages other than English. We perform a comprehensive benchmarking of different zero-shot first-stage retrieval methods on the dataset, including dense and sparse retrievers, query and document machine translation, and state-of-the-art multilingual retrievers. Our results show that large dense retrievers, not necessarily trained for the cross-lingual retrieval task, can achieve zero-shot performance comparable to using ground truth human translations, without the need for machine translation. Sparse retrievers, such as BM25 or SPLADE, combined with document translation, show competitive results, providing an efficient alternative to large dense models. This research advances the understanding of cross-lingual academic information retrieval and provides a framework that others can use to build comparable datasets across different languages and disciplines. By making the dataset and code publicly available, we aim to facilitate further research that will help make scientific knowledge more accessible across language barriers.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignX: The Foundation Model for Sign Recognition</title>
<link>https://arxiv.org/abs/2504.16315</link>
<guid>https://arxiv.org/abs/2504.16315</guid>
<content:encoded><![CDATA[
<div> framework, sign recognition, pose information, ASL signs, video processing
Summary:
SignX is a new framework for sign recognition that aims to translate ASL signs from RGB videos to English-based ID glosses using consistent glossing conventions. The framework consists of a Pose2Gloss component that combines multiple pose information sources into a single representation and a Video2Pose module that directly converts raw video into signer pose representation. SignX allows for compatibility with existing pose formats, improving sign recognition accuracy compared to previous methods. The framework's 2-stage training enables common pose estimation for sign recognition, enhancing the recognition of signs from sign language videos. <div>
arXiv:2504.16315v1 Announce Type: cross 
Abstract: The complexity of sign language data processing brings many challenges. The current approach to recognition of ASL signs aims to translate RGB sign language videos through pose information into English-based ID glosses, which serve to uniquely identify ASL signs. Note that there is no shared convention for assigning such glosses to ASL signs, so it is essential that the same glossing conventions are used for all of the data in the datasets that are employed. This paper proposes SignX, a foundation model framework for sign recognition. It is a concise yet powerful framework applicable to multiple human activity recognition scenarios. First, we developed a Pose2Gloss component based on an inverse diffusion model, which contains a multi-track pose fusion layer that unifies five of the most powerful pose information sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens Segmentation--into a single latent pose representation. Second, we trained a Video2Pose module based on ViT that can directly convert raw video into signer pose representation. Through this 2-stage training framework, we enable sign language recognition models to be compatible with existing pose formats, laying the foundation for the common pose estimation necessary for sign recognition. Experimental results show that SignX can recognize signs from sign language video, producing predicted gloss representations with greater accuracy than has been reported in prior work.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC: Near-Optimal Data Attribution for Deep Learning</title>
<link>https://arxiv.org/abs/2504.16430</link>
<guid>https://arxiv.org/abs/2504.16430</guid>
<content:encoded><![CDATA[
<div> data attribution, predictive, model predictions, convex settings, metadifferentiation

Summary:
The article introduces a new data attribution method called MAGIC, designed to estimate the impact of adding or removing training data on model predictions in large-scale, non-convex settings. The goal of predictive data attribution is to accurately assess the influence of specific training datapoints on model outputs. In convex settings, existing methods like the infinitesimal jackknife provide straightforward solutions. However, these methods are less effective in non-convex scenarios, where current approaches often fall short in accurately correlating estimates with ground truth. MAGIC combines classical techniques with recent advancements in metadifferentiation to optimize the estimation of how model predictions are affected by modifications to the training data. This integration allows MAGIC to provide nearly optimal estimates and greatly improve the accuracy in large-scale, non-convex settings. <div>
arXiv:2504.16430v1 Announce Type: cross 
Abstract: The goal of predictive data attribution is to estimate how adding or removing a given set of training datapoints will affect model predictions. In convex settings, this goal is straightforward (i.e., via the infinitesimal jackknife). In large-scale (non-convex) settings, however, existing methods are far less successful -- current methods' estimates often only weakly correlate with ground truth. In this work, we present a new data attribution method (MAGIC) that combines classical methods and recent advances in metadifferentiation to (nearly) optimally estimate the effect of adding or removing training data on model predictions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data</title>
<link>https://arxiv.org/abs/2504.16628</link>
<guid>https://arxiv.org/abs/2504.16628</guid>
<content:encoded><![CDATA[
<div> algorithm, language models, human preferences, Pareto optimization, multiobjective alignment

Summary:
The article introduces ParetoHqD, a novel method for aligning large language models with multiple human expectations and values. It addresses issues such as inappropriate preference representations and imbalanced reward scores that limit the performance of existing algorithms. ParetoHqD represents human preferences as preference directions in the objective space and considers data near the Pareto front as "high-quality" data. It follows a two-stage supervised fine-tuning process for each preference, using individual Pareto high-quality training sets that match its preference direction. Experimental results show ParetoHqD's superiority over five baselines on two multiobjective alignment tasks, highlighting its effectiveness in aligning language models with diverse user needs. <div>
arXiv:2504.16628v1 Announce Type: cross 
Abstract: Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery</title>
<link>https://arxiv.org/abs/2504.16728</link>
<guid>https://arxiv.org/abs/2504.16728</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hypothesis generation, Human-in-the-loop, Monte Carlo Tree Search, open-source platform

Summary: 
IRIS is an Interactive Research Ideation System that aims to enhance scientific ideation by leveraging large language models (LLMs) and incorporating a Human-in-the-loop approach. The system includes features such as adaptive test-time compute expansion through Monte Carlo Tree Search, fine-grained feedback mechanisms, and query-based literature synthesis. Researchers can utilize IRIS to generate novel hypotheses in a more transparent and controllable manner, empowering them throughout the ideation process. A user study with researchers from various disciplines validates the effectiveness of the system in improving ideation. The code for IRIS is open-source and available on GitHub, allowing researchers to benefit from its innovative features for accelerating scientific discovery. 

<br /><br />Summary: <div>
arXiv:2504.16728v1 Announce Type: cross 
Abstract: The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
<div> Keywords: PRM, ThinkPRM, verification, process labels, test-time scaling

Summary: 
ThinkPRM introduces a data-efficient approach to building process reward models (PRMs) for step-level verification tasks. By training on fewer process labels compared to discriminative PRMs, ThinkPRM leverages the reasoning abilities of long chain-of-thought models to outperform baselines on various benchmarks such as ProcessBench, MATH-500, and AIME '24. In out-of-domain evaluations on GPQA-Diamond and LiveCodeBench, ThinkPRM surpasses discriminative verifiers trained on a larger dataset. The approach also demonstrates effective scaling of verification compute compared to LLM-as-a-Judge under the same token budget. The work highlights the benefits of generative, long CoT PRMs in reducing the supervision required for training while enabling efficient test-time computation for verification tasks. The code, data, and models for ThinkPRM will be made available on the GitHub repository. 

<br /><br />Summary: <div>
arXiv:2504.16828v1 Announce Type: cross 
Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset</title>
<link>https://arxiv.org/abs/2504.16891</link>
<guid>https://arxiv.org/abs/2504.16891</guid>
<content:encoded><![CDATA[
<div> mathematical reasoning, AI, dataset, code execution, model training

Summary: 
This paper discusses the winning submission to the AI Mathematical Olympiad - Progress Prize 2 competition, outlining a methodology for building advanced mathematical reasoning models. The approach is grounded on a large dataset of math problems and solutions, including a novel method that integrates code execution with long reasoning models. By using iterative training and generation techniques, the researchers were able to produce high-quality Tool-Integrated Reasoning solutions. Additionally, a model training pipeline was developed to select the most promising solution from multiple candidates. The study demonstrates that generative solution selection (GenSelect) can enhance performance compared to a majority voting baseline. By adopting these strategies, the researchers achieved state-of-the-art results on mathematical reasoning benchmarks. To support further research in the field, the code, models, and the comprehensive OpenMathReasoning dataset have been made publicly available under a commercially permissive license. <div>
arXiv:2504.16891v1 Announce Type: cross 
Abstract: This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create a pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under a commercially permissive license.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with Multi Modal Fusion</title>
<link>https://arxiv.org/abs/2111.15473</link>
<guid>https://arxiv.org/abs/2111.15473</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral techniques, Transformers, Fourier Transform, MultiDomain Fourier Wavelet Attention, Abstractive summarization<br />
Summary: 
The article explores the use of spectral techniques to replace the attention mechanism in Transformers by employing Fourier Transform based token mixing. It presents a novel reformulation in next-generation transformer models, introducing the MultiDomain Fourier Wavelet Attention (MDFWA) that combines frequency and time localized transforms to efficiently capture global and local dependencies. The authors provide detailed mathematical formulations, complexity bounds, gradient formulas, and demonstrate that MDFWA offers subquadratic time and memory costs while enhancing expressive power. Validation on an abstractive summarization task using the PubMed dataset showcases the effectiveness of the approach with additional enhancements like learned frequency bases, adaptive scale selection, and multi-modal extensions. Overall, the study highlights the potential of spectral techniques in improving the efficiency and performance of Transformer models in natural language processing tasks. <br /><br />Summary: <div>
arXiv:2111.15473v2 Announce Type: replace 
Abstract: We revisit the use of spectral techniques to replaces the attention mechanism in Transformers through Fourier Transform based token mixing, and present a comprehensive and novel reformulation of this technique in next generation transformer models. We provide expanded literature context, detailed mathematical formulations of Fourier mixing and causal masking, and introduce a novel MultiDomain Fourier Wavelet Attention(MDFWA) that integrates frequency and time localized transforms to capture both global and local dependencies efficiently. We derive the complexity bounds, gradient formulas, and show that MDFWA achieves sub quadratic time and memory cost while improving expressive power. We validate our design on an abstractive summarization task using PubMed dataset, by enhancing the proposed approach with learned frequency bases, adaptive scale selection, and multi-modal extensions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge</title>
<link>https://arxiv.org/abs/2307.08813</link>
<guid>https://arxiv.org/abs/2307.08813</guid>
<content:encoded><![CDATA[
arXiv:2307.08813v4 Announce Type: replace 
Abstract: Background: Identification of the interactions and regulatory relations between biomolecules play pivotal roles in understanding complex biological systems and the mechanisms underlying diverse biological functions. However, the collection of such molecular interactions has heavily relied on expert curation in the past, making it labor-intensive and time-consuming. To mitigate these challenges, we propose leveraging the capabilities of large language models (LLMs) to automate genome-scale extraction of this crucial knowledge.
  Results: In this study, we investigate the efficacy of various LLMs in addressing biological tasks, such as the recognition of protein interactions, identification of genes linked to pathways affected by low-dose radiation, and the delineation of gene regulatory relationships. Overall, the larger models exhibited superior performance, indicating their potential for specific tasks that involve the extraction of complex interactions among genes and proteins. Although these models possessed detailed information for distinct gene and protein groups, they faced challenges in identifying groups with diverse functions and in recognizing highly correlated gene regulatory relationships.
  Conclusions: By conducting a comprehensive assessment of the state-of-the-art models using well-established molecular interaction and pathway databases, our study reveals that LLMs can identify genes/proteins associated with pathways of interest and predict their interactions to a certain extent. Furthermore, these models can provide important insights, marking a noteworthy stride toward advancing our understanding of biological systems through AI-assisted knowledge discovery.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dataset and benchmark for hospital course summarization with adapted large language models</title>
<link>https://arxiv.org/abs/2403.05720</link>
<guid>https://arxiv.org/abs/2403.05720</guid>
<content:encoded><![CDATA[
arXiv:2403.05720v5 Announce Type: replace 
Abstract: Brief hospital course (BHC) summaries are clinical documents that summarize a patient's hospital stay. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as synthesizing BHCs from clinical notes have not been shown. We introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating clinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC synthesis. Furthermore, we introduce a benchmark of the summarization performance of two general-purpose LLMs and three healthcare-adapted LLMs. Using clinical notes as input, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We evaluate these LLMs across multiple context-length inputs using natural language similarity metrics. We further conduct a clinical study with five clinicians, comparing clinician-written and LLM-generated BHCs across 30 samples, focusing on their potential to enhance clinical decision-making through improved summary quality. We observe that the Llama2-13B fine-tuned LLM outperforms other domain-adapted models given quantitative evaluation metrics of BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to increasing context lengths of clinical note inputs than fine-tuned Llama2-13B. Despite comparable quantitative metrics, the reader study depicts a significant preference for summaries generated by GPT-4 with in-context learning compared to both Llama2-13B fine-tuned summaries and the original summaries, highlighting the need for qualitative clinical evaluation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens</title>
<link>https://arxiv.org/abs/2403.12766</link>
<guid>https://arxiv.org/abs/2403.12766</guid>
<content:encoded><![CDATA[
arXiv:2403.12766v3 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Sentinel: LLM Agent for Adversarial Purification</title>
<link>https://arxiv.org/abs/2405.20770</link>
<guid>https://arxiv.org/abs/2405.20770</guid>
<content:encoded><![CDATA[
arXiv:2405.20770v4 Announce Type: replace 
Abstract: Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Lyrics Detection Across Languages and Genres</title>
<link>https://arxiv.org/abs/2406.15231</link>
<guid>https://arxiv.org/abs/2406.15231</guid>
<content:encoded><![CDATA[
arXiv:2406.15231v3 Announce Type: replace 
Abstract: In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy</title>
<link>https://arxiv.org/abs/2407.03004</link>
<guid>https://arxiv.org/abs/2407.03004</guid>
<content:encoded><![CDATA[
arXiv:2407.03004v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been shown to encode clinical knowledge. Many evaluations, however, rely on structured question-answer benchmarks, overlooking critical challenges of interpreting and reasoning about unstructured clinical narratives in real-world settings. Using free-text clinical descriptions, we present SemioLLM, an evaluation framework that benchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B, LlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of 1,269 seizure descriptions, we show that most LLMs are able to accurately and confidently generate probabilistic predictions of seizure onset zones in the brain. Most models approach clinician-level performance after prompt engineering, with expert-guided chain-of-thought reasoning leading to the most consistent improvements. Performance was further strongly modulated by clinical in-context impersonation, narrative length and language context (13.7%, 32.7% and 14.2% performance variation, respectively). However, expert analysis of reasoning outputs revealed that correct prediction can be based on hallucinated knowledge and deficient source citation accuracy, underscoring the need to improve interpretability of LLMs in clinical use. Overall, SemioLLM provides a scalable, domain-adaptable framework for evaluating LLMs in clinical disciplines where unstructured verbal descriptions encode diagnostic information. By identifying both the strengths and limitations of state-of-the-art models, our work supports the development of clinically robust and globally applicable AI systems for healthcare.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation</title>
<link>https://arxiv.org/abs/2407.12022</link>
<guid>https://arxiv.org/abs/2407.12022</guid>
<content:encoded><![CDATA[
arXiv:2407.12022v3 Announce Type: replace 
Abstract: Recently, large language models (LLMs) have demonstrated excellent performance, inspiring researchers to explore their use in automating register transfer level (RTL) code generation and improving hardware design efficiency. However, the existing approaches to fine-tune LLMs for RTL generation typically are conducted on fixed datasets, which do not fully stimulate the capability of LLMs and require large amounts of reference data, which are costly to acquire. To mitigate these issues, we innovatively introduce an iterative training paradigm named ITERTL. During each iteration, samples are drawn from the model trained in the previous cycle. Then these new samples are employed for training in current loop. Furthermore, we introduce a plug-and-play data filtering strategy, thereby encouraging the model to generate high-quality, self-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA) open-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human benchmark. Under similar conditions of data quantity and quality, our approach significantly outperforms the baseline. Extensive experiments validate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lawma: The Power of Specialization for Legal Annotation</title>
<link>https://arxiv.org/abs/2407.16615</link>
<guid>https://arxiv.org/abs/2407.16615</guid>
<content:encoded><![CDATA[
arXiv:2407.16615v2 Announce Type: replace 
Abstract: Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, our understanding of how to best utilize large language models for legal annotation remains limited. To bridge this gap, we introduce CaselawQA, a benchmark comprising 260 legal annotation tasks, nearly all new to the machine learning community. We demonstrate that commercial models, such as GPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial yet highly variable accuracy, generally falling short of the performance required for legal work. We then demonstrate that small, lightly fine-tuned models outperform commercial models. A few hundred to a thousand labeled examples are usually enough to achieve higher accuracy. Our work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal annotation tasks with some available labeled data, researchers are likely better off using a fine-tuned open-source model.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models</title>
<link>https://arxiv.org/abs/2407.17914</link>
<guid>https://arxiv.org/abs/2407.17914</guid>
<content:encoded><![CDATA[
arXiv:2407.17914v2 Announce Type: replace 
Abstract: Text representations from language models have proven remarkably predictive of human neural activity involved in language processing, with the recent transformer-based models outperforming previous architectures in downstream tasks and prediction of brain responses. However, the word representations learnt by language-only models may be limited in that they lack sensory information from other modalities, which several cognitive and neuroscience studies showed to be reflected in human meaning representations. Here, we leverage current pre-trained vision-language models (VLMs) to investigate whether the integration of visuo-linguistic information they operate leads to representations that are more aligned with human brain activity than those obtained by models trained with language-only input. We focus on fMRI responses recorded while participants read concept words in the context of either a full sentence or a picture. Our results reveal that VLM representations correlate more strongly than those by language-only models with activations in brain areas functionally related to language processing. Additionally, we find that transformer-based vision-language encoders -- e.g., LXMERT and VisualBERT -- yield more brain-aligned representations than generative VLMs, whose autoregressive abilities do not seem to provide an advantage when modelling single words. Finally, our ablation analyses suggest that the high brain alignment achieved by some of the VLMs we evaluate results from semantic information acquired specifically during multimodal pretraining as opposed to being already encoded in their unimodal modules. Altogether, our findings indicate an advantage of multimodal models in predicting human brain activations, which reveals that modelling language and vision integration has the potential to capture the multimodal nature of human concept representations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The advantages of context specific language models: the case of the Erasmian Language Model</title>
<link>https://arxiv.org/abs/2408.06931</link>
<guid>https://arxiv.org/abs/2408.06931</guid>
<content:encoded><![CDATA[
arXiv:2408.06931v2 Announce Type: replace 
Abstract: The current trend to improve language model performance seems to be based on scaling up with the number of parameters (e.g. the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the model. However this comes at significant costs in terms of computational resources and energy costs that compromise the sustainability of AI solutions, as well as risk relating to privacy and misuse. In this paper we present the Erasmian Language Model (ELM) a small context specific, 900 million parameter model, pre-trained and fine-tuned by and for Erasmus University Rotterdam. We show how the model performs adequately in a classroom context for essay writing, and how it achieves superior performance in subjects that are part of its context. This has implications for a wide range of institutions and organizations, showing that context specific language models may be a viable alternative for resource constrained, privacy sensitive use cases.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>lamss: when large language models meet self-skepticism</title>
<link>https://arxiv.org/abs/2409.06601</link>
<guid>https://arxiv.org/abs/2409.06601</guid>
<content:encoded><![CDATA[
arXiv:2409.06601v2 Announce Type: replace 
Abstract: Hallucination is a major challenge for large language models (LLMs), prevent ing their further application in some fields. The skeptical thinking of humankind
  could be useful for LLMs to self-cognition, self-reflection and alleviate their hal lucinations. Inspired by this consideration, we propose a novel approach called
  LaMsS, which combines the semantic understanding capability of LLMs with
  self-skepticism. By introducing a series of skepticism tokens and augmenting
  them into the vocabulary, we conduct both pertaining and finetuning, which allow
  the LLM to decode each normal token followed by a skeptical token, represent ing different skepticism levels. By calculating the response skepticism given a
  query, one can define a new self-aware LLM which is only willing to answer
  with relative lower skepticism level than the threshold. By examining the accu racy, AUC and AP of willingly answering questions, we demonstrate that LaMsS
  achieves better performance than baselines on both multi-choice questions and
  open-domain question-answering benchmarks, and can generalize to multi-task
  and out-of-domain settings. Our study sheds some lights on the self-skepticism
  modeling on further artificial intelligence. Project code and model checkpoints
  can be found in https://anonymous.4open.science/r/SM-1E76.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems</title>
<link>https://arxiv.org/abs/2410.19572</link>
<guid>https://arxiv.org/abs/2410.19572</guid>
<content:encoded><![CDATA[
arXiv:2410.19572v5 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems using large language models (LLMs) often generate inaccurate responses due to the retrieval of irrelevant or loosely related information. Existing methods, which operate at the document level, fail to effectively filter out such content. We propose LLM-driven chunk filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and filtering retrieved information at the chunk level. Our approach employs semantic chunking to divide documents into coherent sections and utilizes LLM-based relevance scoring to assess each chunk's alignment with the user's query. By filtering out less pertinent chunks before the generation phase, we significantly reduce hallucinations and improve factual accuracy. Experiments show that our method outperforms existing RAG models, achieving higher accuracy on tasks requiring precise information retrieval. This advancement enhances the reliability of RAG systems, making them particularly beneficial for applications like fact-checking and multi-hop reasoning.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEG: Medical Knowledge-Augmented Large Language Models for Question Answering</title>
<link>https://arxiv.org/abs/2411.03883</link>
<guid>https://arxiv.org/abs/2411.03883</guid>
<content:encoded><![CDATA[
arXiv:2411.03883v3 Announce Type: replace 
Abstract: Question answering is a natural language understanding task that involves reasoning over both explicit context, and unstated relevant domain knowledge. Despite the high cost of training, large language models (LLMs) -- the backbone of most modern question-answering systems -- still struggle to reliably capture the nuanced relationships between concepts that are crucial for reasoning in specialized fields like medicine. In this work, we present MEG, a parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a lightweight mapping network to incorporate knowledge graph embeddings into the LLM, enabling it to leverage external knowledge in a cost-effective way. We evaluate our method on four popular medical multiple-choice datasets and show that LLMs i) can effectively interpret knowledge graph embeddings and ii) gain significant advantages from the factual grounding these embeddings provide. MEG attains an average of +6.7% and +9.9% accuracy over specialized models like BioMistral-7B and MediTron-7B, respectively. Finally, we show that MEG's performance remains robust to the choice of graph encoder.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?</title>
<link>https://arxiv.org/abs/2411.05000</link>
<guid>https://arxiv.org/abs/2411.05000</guid>
<content:encoded><![CDATA[
arXiv:2411.05000v2 Announce Type: replace 
Abstract: As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sufficient Context: A New Lens on Retrieval Augmented Generation Systems</title>
<link>https://arxiv.org/abs/2411.06037</link>
<guid>https://arxiv.org/abs/2411.06037</guid>
<content:encoded><![CDATA[
arXiv:2411.06037v3 Announce Type: replace 
Abstract: Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to answer the query. To shed light on this, we develop a new notion of sufficient context, along with a method to classify instances that have enough information to answer the query. We then use sufficient context to analyze several models and datasets. By stratifying errors based on context sufficiency, we find that larger models with higher baseline performance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when the context is sufficient, but often output incorrect answers instead of abstaining when the context is not. On the other hand, smaller models with lower baseline performance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient context. We further categorize cases when the context is useful, and improves accuracy, even though it does not fully answer the query and the model errs without the context. Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective generation method that leverages sufficient context information for guided abstention. Our method improves the fraction of correct answers among times where the model responds by 2--10\% for Gemini, GPT, and Gemma. Key findings and the prompts used in our autorater analysis are available on our github.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement</title>
<link>https://arxiv.org/abs/2412.06845</link>
<guid>https://arxiv.org/abs/2412.06845</guid>
<content:encoded><![CDATA[
arXiv:2412.06845v4 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training and obtaining the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO), an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model, leading to the Moxin Reasoning model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</title>
<link>https://arxiv.org/abs/2412.13612</link>
<guid>https://arxiv.org/abs/2412.13612</guid>
<content:encoded><![CDATA[
arXiv:2412.13612v3 Announce Type: replace 
Abstract: Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?</title>
<link>https://arxiv.org/abs/2502.04718</link>
<guid>https://arxiv.org/abs/2502.04718</guid>
<content:encoded><![CDATA[
arXiv:2502.04718v2 Announce Type: replace 
Abstract: Text style transfer (TST) is the task of transforming a text to reflect a particular style while preserving its original content. Evaluating TST outputs is a multidimensional challenge, requiring the assessment of style transfer accuracy, content preservation, and naturalness. Using human evaluation is ideal but costly, as is common in other natural language processing (NLP) tasks, however, automatic metrics for TST have not received as much attention as metrics for, e.g., machine translation or summarization. In this paper, we examine both set of existing and novel metrics from broader NLP tasks for TST evaluation, focusing on two popular subtasks, sentiment transfer and detoxification, in a multilingual context comprising English, Hindi, and Bengali. By conducting meta-evaluation through correlation with human judgments, we demonstrate the effectiveness of these metrics when used individually and in ensembles. Additionally, we investigate the potential of large language models (LLMs) as tools for TST evaluation. Our findings highlight newly applied advanced NLP metrics and LLM-based evaluations provide better insights than existing TST metrics. Our oracle ensemble approaches show even more potential.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization</title>
<link>https://arxiv.org/abs/2502.13108</link>
<guid>https://arxiv.org/abs/2502.13108</guid>
<content:encoded><![CDATA[
arXiv:2502.13108v2 Announce Type: replace 
Abstract: Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support.
  To address this limitation, we introduce a Multi-Task Learning (MTL) framework that jointly trains CQA models for both answer extraction and medical categorization. In addition to predicting answer spans, our model classifies responses into five standardized medical categories: Diagnosis, Medication, Symptoms, Procedure, and Lab Reports. This categorization enables more structured and interpretable outputs, making clinical QA models more useful in real-world healthcare settings.
  We evaluate our approach on emrQA, a large-scale dataset for medical question answering. Results show that MTL improves F1-score by 2.2% compared to standard fine-tuning, while achieving 90.7% accuracy in answer categorization. These findings suggest that MTL not only enhances CQA performance but also introduces an effective mechanism for categorization and structured medical information retrieval.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</title>
<link>https://arxiv.org/abs/2503.01840</link>
<guid>https://arxiv.org/abs/2503.01840</guid>
<content:encoded><![CDATA[
arXiv:2503.01840v3 Announce Type: replace 
Abstract: The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves a 1.38x throughput improvement at a batch size of 64. The code is available at https://github.com/SafeAILab/EAGLE.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations</title>
<link>https://arxiv.org/abs/2503.14477</link>
<guid>https://arxiv.org/abs/2503.14477</guid>
<content:encoded><![CDATA[
arXiv:2503.14477v2 Announce Type: replace 
Abstract: LLMs often adopt an assertive language style also when making false claims. Such ``overconfident hallucinations'' mislead users and erode trust. Achieving the ability to express in language the actual degree of uncertainty around a claim is therefore of great importance. We find that ``verbal uncertainty'' is governed by a single linear feature in the representation space of LLMs, and show that this has only moderate correlation with the actual ``semantic uncertainty'' of the model. We apply this insight and show that (1) the mismatch between semantic and verbal uncertainty is a better predictor of hallucinations than semantic uncertainty alone and (2) we can intervene on verbal uncertainty at inference time and reduce confident hallucinations on short-form answers, achieving an average relative reduction of ~30%.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2503.16419</link>
<guid>https://arxiv.org/abs/2503.16419</guid>
<content:encoded><![CDATA[
arXiv:2503.16419v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence</title>
<link>https://arxiv.org/abs/2503.20533</link>
<guid>https://arxiv.org/abs/2503.20533</guid>
<content:encoded><![CDATA[
arXiv:2503.20533v3 Announce Type: replace 
Abstract: Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</title>
<link>https://arxiv.org/abs/2504.10157</link>
<guid>https://arxiv.org/abs/2504.10157</guid>
<content:encoded><![CDATA[
arXiv:2504.10157v2 Announce Type: replace 
Abstract: Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2404.04545</link>
<guid>https://arxiv.org/abs/2404.04545</guid>
<content:encoded><![CDATA[
arXiv:2404.04545v2 Announce Type: replace-cross 
Abstract: Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment by leveraging language, visual, and acoustic modalities. Despite the remarkable performance exhibited by previous MSA approaches, the presence of inherent multimodal heterogeneities poses a challenge, with the contribution of different modalities varying considerably. Past research predominantly focused on improving representation learning techniques and feature fusion strategies. However, many of these efforts overlooked the variation in semantic richness among different modalities, treating each modality uniformly. This approach may lead to underestimating the significance of strong modalities while overemphasizing the importance of weak ones. Motivated by these insights, we introduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the predominant role of the text modality in MSA. Specifically, for each multimodal sample, by taking unaligned sequences of the three modalities as inputs, we initially allocate the extracted unimodal features into a visual-text and an acoustic-text pair. Subsequently, we implement self-attention on the text modality and apply text-queried cross-attention to the visual and acoustic modalities. To mitigate the influence of noise signals and redundant features, we incorporate a gated control mechanism into the framework. Additionally, we introduce unimodal joint learning to gain a deeper understanding of homogeneous emotional tendencies across diverse modalities through backpropagation. Experimental results demonstrate that TCAN consistently outperforms state-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Situational Safety</title>
<link>https://arxiv.org/abs/2410.06172</link>
<guid>https://arxiv.org/abs/2410.06172</guid>
<content:encoded><![CDATA[
arXiv:2410.06172v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers</title>
<link>https://arxiv.org/abs/2410.22663</link>
<guid>https://arxiv.org/abs/2410.22663</guid>
<content:encoded><![CDATA[
arXiv:2410.22663v3 Announce Type: replace-cross 
Abstract: Machine learning (ML) for text classification has been widely used in various domains. These applications can significantly impact ethics, economics, and human behavior, raising serious concerns about trusting ML decisions. Studies indicate that conventional metrics are insufficient to build human trust in ML models. These models often learn spurious correlations and predict based on them. In the real world, their performance can deteriorate significantly. To avoid this, a common practice is to test whether predictions are reasonable based on valid patterns in the data. Along with this, a challenge known as the trustworthiness oracle problem has been introduced. Due to the lack of automated trustworthiness oracles, the assessment requires manual validation of the decision process disclosed by explanation methods. However, this is time-consuming, error-prone, and unscalable.
  We propose TOKI, the first automated trustworthiness oracle generation method for text classifiers. TOKI automatically checks whether the words contributing the most to a prediction are semantically related to the predicted class. Specifically, we leverage ML explanations to extract the decision-contributing words and measure their semantic relatedness with the class based on word embeddings. We also introduce a novel adversarial attack method that targets trustworthiness vulnerabilities identified by TOKI. To evaluate their alignment with human judgement, experiments are conducted. We compare TOKI with a naive baseline based solely on model confidence and TOKI-guided adversarial attack method with A2T, a SOTA adversarial attack method. Results show that relying on prediction uncertainty cannot effectively distinguish between trustworthy and untrustworthy predictions, TOKI achieves 142% higher accuracy than the naive baseline, and TOKI-guided attack method is more effective with fewer perturbations than A2T.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2411.18203</link>
<guid>https://arxiv.org/abs/2411.18203</guid>
<content:encoded><![CDATA[
arXiv:2411.18203v5 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist</title>
<link>https://arxiv.org/abs/2412.06412</link>
<guid>https://arxiv.org/abs/2412.06412</guid>
<content:encoded><![CDATA[
arXiv:2412.06412v2 Announce Type: replace-cross 
Abstract: With the rapid advancements in Large Language Models (LLMs), LLM-based agents have introduced convenient and user-friendly methods for leveraging tools across various domains. In the field of astronomical observation, the construction of new telescopes has significantly increased astronomers' workload. Deploying LLM-powered agents can effectively alleviate this burden and reduce the costs associated with training personnel. Within the Nearby Galaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes across three observation sites, aiming to find the transients from the galaxies in 50 mpc, we have developed the \textbf{StarWhisper Telescope System} to manage the entire observation process. This system automates tasks such as generating observation lists, conducting observations, analyzing data, and providing feedback to the observer. Observation lists are customized for different sites and strategies to ensure comprehensive coverage of celestial objects. After manual verification, these lists are uploaded to the telescopes via the agents in the system, which initiates observations upon neutral language. The observed images are analyzed in real-time, and the transients are promptly communicated to the observer. The agent modifies them into a real-time follow-up observation proposal and send to the Xinglong observatory group chat, then add them to the next-day observation lists. Additionally, the integration of AI agents within the system provides online accessibility, saving astronomers' time and encouraging greater participation from amateur astronomers in the NGSS project.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants</title>
<link>https://arxiv.org/abs/2412.12661</link>
<guid>https://arxiv.org/abs/2412.12661</guid>
<content:encoded><![CDATA[
arXiv:2412.12661v2 Announce Type: replace-cross 
Abstract: Recent advancements in mixed-modal generative have opened new avenues for developing unified biomedical assistants capable of analyzing biomedical images, answering complex questions about them, and generating multimodal patient reports. However, existing datasets face challenges such as small sizes, limited coverage of biomedical tasks and domains, and a reliance on narrow sources. To address these gaps, we present MedMax, a large-scale multimodal biomedical instruction-tuning dataset for mixed-modal foundation models. With 1.47 million instances, MedMax encompasses a diverse range of tasks, including interleaved image-text generation, biomedical image captioning and generation, visual chat, and report understanding. These tasks span knowledge across diverse biomedical domains, including radiology and histopathology, grounded in medical papers and YouTube videos. Subsequently, we fine-tune a mixed-modal foundation model on the MedMax dataset, achieving significant performance improvements: a 26% gain over the Chameleon model and an 18.3% improvement over GPT-4o across 12 downstream biomedical visual question-answering tasks. Finally, we introduce a unified evaluation suite for biomedical tasks to guide the development of mixed-modal biomedical AI assistants. The data, model, and code is available at https://mint-medmax.github.io/.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lorecast: Layout-Aware Performance and Power Forecasting from Natural Language</title>
<link>https://arxiv.org/abs/2503.11662</link>
<guid>https://arxiv.org/abs/2503.11662</guid>
<content:encoded><![CDATA[
arXiv:2503.11662v2 Announce Type: replace-cross 
Abstract: In chip design planning, obtaining reliable performance and power forecasts for various design options is of critical importance. Traditionally, this involves using system-level models, which often lack accuracy, or trial synthesis, which is both labor-intensive and time-consuming. We introduce a new methodology, called Lorecast, which accepts English prompts as input to rapidly generate layout-aware performance and power estimates. This approach bypasses the need for HDL code development and synthesis, making it both fast and user-friendly. Experimental results demonstrate that Lorecast achieves accuracy within a few percent of error compared to post-layout analysis, while significantly reducing turnaround time.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic hashtag recommendation in social media with trend shift detection and adaptation</title>
<link>https://arxiv.org/abs/2504.00044</link>
<guid>https://arxiv.org/abs/2504.00044</guid>
<content:encoded><![CDATA[
arXiv:2504.00044v2 Announce Type: replace-cross 
Abstract: Hashtag recommendation systems have emerged as a key tool for automatically suggesting relevant hashtags and enhancing content categorization and search. However, existing static models struggle to adapt to the highly dynamic nature of social media conversations, where new hashtags constantly emerge and existing ones undergo semantic shifts. To address these challenges, this paper introduces H-ADAPTS (Hashtag recommendAtion by Detecting and adAPting to Trend Shifts), a dynamic hashtag recommendation methodology that employs a trend-aware mechanism to detect shifts in hashtag usage-reflecting evolving trends and topics within social media conversations-and triggers efficient model adaptation based on a (small) set of recent posts. Additionally, the Apache Storm framework is leveraged to support scalable and fault-tolerant analysis of high-velocity social data, enabling the timely detection of trend shifts. Experimental results from two real-world case studies, including the COVID-19 pandemic and the 2020 US presidential election, demonstrate the effectiveness of H-ADAPTS in providing timely and relevant hashtag recommendations by adapting to emerging trends, significantly outperforming existing solutions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing 16,193 LLM Papers for Fun and Profits</title>
<link>https://arxiv.org/abs/2504.08619</link>
<guid>https://arxiv.org/abs/2504.08619</guid>
<content:encoded><![CDATA[
arXiv:2504.08619v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are reshaping the landscape of computer science research, driving significant shifts in research priorities across diverse conferences and fields. This study provides a comprehensive analysis of the publication trend of LLM-related papers in 77 top-tier computer science conferences over the past six years (2019-2024). We approach this analysis from four distinct perspectives: (1) We investigate how LLM research is driving topic shifts within major conferences. (2) We adopt a topic modeling approach to identify various areas of LLM-related topic growth and reveal the topics of concern at different conferences. (3) We explore distinct contribution patterns of academic and industrial institutions. (4) We study the influence of national origins on LLM development trajectories. Synthesizing the findings from these diverse analytical angles, we derive ten key insights that illuminate the dynamics and evolution of the LLM research ecosystem.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)</title>
<link>https://arxiv.org/abs/2504.15349</link>
<guid>https://arxiv.org/abs/2504.15349</guid>
<content:encoded><![CDATA[
<div> Keywords: Compositional Generalization, Transformer models, RASP, ReCOGS_pos, Semantic exact match

Summary: 
A study introduces the concept of Compositional Generalization, where humans understand new word combinations from different contexts. Using the Restricted Access Sequence Processing (RASP) model, researchers demonstrate that a Transformer encoder-decoder can effectively perform the ReCOGS_pos task, achieving high accuracy. The RASP model showcases the ability to handle structural generalizations systematically and compositionally without requiring a hierarchical or tree-structured solution. By employing word-level tokens with part-of-speech tagging and flat pattern-matching rules, the model achieves impressive semantic and string exact match scores on various linguistic tasks. Notably, the model exhibits proficiency in handling prepositional phrase and sentential complement recursion using a decoder loop approach. These findings highlight the potential of RASP models in language processing tasks that demand compositional and systematic generalization abilities.

<br /><br />Summary: <div>
arXiv:2504.15349v1 Announce Type: new 
Abstract: Humans understand new combinations of words encountered if they are combinations of words recognized from different contexts, an ability called Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020) arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted Access Sequence Processing (RASP), a Transformer-equivalent programming language, to prove by construction that a Transformer encoder-decoder can perform the semantically equivalent ReCOGS_pos (Wu et al., 2024) arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore, our RASP model shows the ReCOGS_pos task does not require a hierarchical or tree-structured solution: we use word-level tokens with an "embedding" layer that tags with possible parts of speech, applying just once per encoder pass 19 attention-head compatible flat pattern-matching rules, shown using grammar coverage (Zeller et al., 2023) to be learnable from the training data, plus general prepositional phrase (pp) handling and sentential complement (cp) handling logic, and output the next logical form (LF) token (repeating until the LF is complete). The model does not apply recursive, tree-structured rules like 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact match on pp recursion, cp recursion using the decoder loop.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection</title>
<link>https://arxiv.org/abs/2504.15392</link>
<guid>https://arxiv.org/abs/2504.15392</guid>
<content:encoded><![CDATA[
<div> investigates, hybrid intelligence, collaboration, sexism, Large Language Models (LLMs)

Summary: 
This paper explores the collaboration between researchers of sexism and Large Language Models (LLMs) through a four-component pipeline. The study involves nine sexism researchers engaging in interactive experiments with an LLM (GPT3.5) to assess the model's knowledge about sexism and its research suitability. Experts were tasked with creating different definitions of sexism, resulting in longer and more complex definitions when interacting with the LLM. While expert-written definitions initially performed poorly in classification tasks, some experts improved classification performance with co-created definitions, even those inexperienced in using LLMs. Zero-shot classification experiments using the definitions from experts tested GPT4o on 2,500 texts from five sexism benchmarks, resulting in 67,500 classification decisions analyzed. <div>
arXiv:2504.15392v1 Announce Type: new 
Abstract: This paper investigates hybrid intelligence and collaboration between researchers of sexism and Large Language Models (LLMs), with a four-component pipeline. First, nine sexism researchers answer questions about their knowledge of sexism and of LLMs. They then participate in two interactive experiments involving an LLM (GPT3.5). The first experiment has experts assessing the model's knowledge about sexism and suitability for use in research. The second experiment tasks them with creating three different definitions of sexism: an expert-written definition, an LLM-written one, and a co-created definition. Lastly, zero-shot classification experiments use the three definitions from each expert in a prompt template for sexism detection, evaluating GPT4o on 2.500 texts sampled from five sexism benchmarks. We then analyze the resulting 67.500 classification decisions. The LLM interactions lead to longer and more complex definitions of sexism. Expert-written definitions on average perform poorly compared to LLM-generated definitions. However, some experts do improve classification performance with their co-created definitions of sexism, also experts who are inexperienced in using LLMs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trillion 7B Technical Report</title>
<link>https://arxiv.org/abs/2504.15431</link>
<guid>https://arxiv.org/abs/2504.15431</guid>
<content:encoded><![CDATA[
<div> Trillion-7B, Korean-centric, multilingual LLM, Cross-lingual Document Attention mechanism, knowledge transfer, English to Korean and Japanese, optimized data mixtures, language-specific filtering, tailored tokenizer construction, competitive performance, 10% training tokens for multilingual data, 59.4K H100 GPU hours, robust multilingual performance, cross-lingual consistency<br /><br />Summary: Trillion-7B is introduced as a highly efficient Korean-centric multilingual LLM using the XLDA mechanism for knowledge transfer between English, Korean, and Japanese. Through optimized data mixtures, language-specific filtering, and tailored tokenizer construction, it achieves competitive performance with minimal resources. Despite dedicating only 10% of training tokens to multilingual data and requiring 59.4K H100 GPU hours for full training, Trillion-7B demonstrates robust multilingual performance and exceptional cross-lingual consistency across 27 benchmarks in four languages. <div>
arXiv:2504.15431v1 Announce Type: new 
Abstract: We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10\% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7B's robust multilingual performance and exceptional cross-lingual consistency.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feeding LLM Annotations to BERT Classifiers at Your Own Risk</title>
<link>https://arxiv.org/abs/2504.15432</link>
<guid>https://arxiv.org/abs/2504.15432</guid>
<content:encoded><![CDATA[
<div> fine-tune, smaller encoder-only models, text classification, synthetic data, error propagation
Summary:<br /><br />Using LLM-generated labels to fine-tune smaller encoder-only models for text classification can lead to performance degradation, instability, and premature plateaus compared to models trained on gold labels. The study highlights the risks of training on synthetic data and the challenges of propagating errors from LLM annotations to smaller classifiers. Mitigation strategies such as entropy-based filtering and ensemble techniques provide partial relief but do not fully address the inherent risks. Caution is advised when applying this approach in high-stakes text classification tasks. <div>
arXiv:2504.15432v1 Announce Type: new 
Abstract: Using LLM-generated labels to fine-tune smaller encoder-only models for text classification has gained popularity in various settings. While this approach may be justified in simple and low-stakes applications, we conduct empirical analysis to demonstrate how the perennial curse of training on synthetic data manifests itself in this specific setup. Compared to models trained on gold labels, we observe not only the expected performance degradation in accuracy and F1 score, but also increased instability across training runs and premature performance plateaus. These findings cast doubts on the reliability of such approaches in real-world applications. We contextualize the observed phenomena through the lens of error propagation and offer several practical mitigation strategies, including entropy-based filtering and ensemble techniques. Although these heuristics offer partial relief, they do not fully resolve the inherent risks of propagating non-random errors from LLM annotations to smaller classifiers, underscoring the need for caution when applying this workflow in high-stakes text classification tasks.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models</title>
<link>https://arxiv.org/abs/2504.15471</link>
<guid>https://arxiv.org/abs/2504.15471</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer language models, activation vectors, bigram predictions, subnetworks, model performance 

Summary:<br /><br />
The study explores the presence of bigram subnetworks within Transformer language models, focusing on next token predictions based solely on the current token. It is found that these subnetworks exist in fully trained models with up to 1B parameters and play a crucial role in model performance despite their small size (less than 0.2% of model parameters). The bigram subnetworks are predominantly located in the first Transformer MLP layer and are closely related to subnetworks optimized for model pruning. Mechanistically, these subnetworks align activations with next token predictions rather than current token representations, driving the transformation process in the residual stream. The discovery of bigram subnetworks offers a foundational approach to studying language model circuits, starting from a minimal subset of parameters essential for basic next token predictions, instead of the conventional method of ablating circuits from a complete model. <div>
arXiv:2504.15471v1 Announce Type: new 
Abstract: In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying language model circuits by building up from a minimal circuit rather than the traditional approach of ablating circuits from a full model.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Sampling via Exponential Races</title>
<link>https://arxiv.org/abs/2504.15475</link>
<guid>https://arxiv.org/abs/2504.15475</guid>
<content:encoded><![CDATA[
<div> Speculative decoding, large language models, inference, channel simulation, generation speed-up <br />
<br />
Summary: Speculative decoding is a technique that accelerates large language model inference using a smaller draft model. This paper establishes a connection between speculative decoding and channel simulation, showing that they are related concepts. An information-theoretic analysis is provided to quantify the speedup achievable by speculative decoding. The relation between generation speed-up and the number of tokens generated by the draft model is derived, serving as an upper bound for all values of k. A novel speculative decoding method, ERSD, is proposed, which demonstrates performance on par with existing methods. This research contributes to improving the efficiency of large language model inference by exploiting speculative decoding techniques. <div>
arXiv:2504.15475v1 Announce Type: new 
Abstract: Speculative decoding accelerates large language model inference using a smaller draft model. In this paper, we establish a surprising connection between speculative decoding and channel simulation, which aims at simulating a noisy channel using as few bits as possible. This connection allows us to provide an information-theoretic analysis of the speed up that can be achieved by speculative decoding. Leveraging this link, we derive an explicit relation between generation speed-up and the number of tokens $k$ generated by the draft model for large $k$, which serves as an upper bound for all $k$. We also propose a novel speculative decoding method via exponential race ERSD that matches state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation</title>
<link>https://arxiv.org/abs/2504.15509</link>
<guid>https://arxiv.org/abs/2504.15509</guid>
<content:encoded><![CDATA[
<div> Simultaneous speech translation, large language models, streaming capability, SimulS2S-LLM, incremental beam search 

Summary:
Simultaneous speech translation (SST) is a challenging task that aims to provide translations in real-time while balancing quality and latency. This paper introduces SimulS2S-LLM, a method that trains speech models offline and uses a test-time policy for simultaneous inference. By extracting boundary-aware speech prompts and predicting discrete output speech tokens, SimulS2S-LLM achieves speech-to-speech translation with improved quality-latency trade-offs compared to existing methods. The use of a pre-trained vocoder and an incremental beam search expands the search space for speech token prediction without increasing latency. Experimental results on CVSS speech data demonstrate that SimulS2S-LLM outperforms other methods by improving ASR-BLEU scores by 3 points at similar latency. <br /><br />Summary: <div>
arXiv:2504.15509v1 Announce Type: new 
Abstract: Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks</title>
<link>https://arxiv.org/abs/2504.15521</link>
<guid>https://arxiv.org/abs/2504.15521</guid>
<content:encoded><![CDATA[
<div> Benchmarks, Multilingual, Evaluation, Language models, Collaborative<br />
Summary: 
The article discusses the importance of multilingual benchmarking for large language models. It analyzes over 2,000 benchmarks from 148 countries and finds that English is overrepresented in these benchmarks. Most benchmarks use original language content from high-resource countries. There are disparities in performance compared to human judgments, with STEM-related tasks showing stronger correlations. Localized benchmarks align better with local judgments than translated ones. The article identifies key limitations in current multilingual evaluation practices and proposes guiding principles for effective benchmarking. It suggests focusing on culturally and linguistically tailored benchmarks rather than translations. The research also outlines five critical directions for progress in the field and calls for a global collaborative effort to develop human-aligned benchmarks for real-world applications.<br /><br />Summary: <div>
arXiv:2504.15521v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property</title>
<link>https://arxiv.org/abs/2504.15524</link>
<guid>https://arxiv.org/abs/2504.15524</guid>
<content:encoded><![CDATA[
<div> patents, intellectual property, large language models, benchmark, tasks <br />
Summary: 
The article introduces a new benchmark called IPBench for evaluating large language models (LLMs) in the field of intellectual property (IP). This benchmark covers 8 IP mechanisms and 20 tasks, allowing for the evaluation of understanding and generation capabilities of LLMs in real-world IP applications. The study benchmarked 16 LLMs, finding that there is room for improvement in their performance, with even the best model achieving only 75.8% accuracy. The research reveals that open-source IP and law-oriented models are lagging behind closed-source general-purpose models in this domain. The authors have made the data and code of IPBench publicly available and plan to update it with additional IP-related tasks to better reflect the challenges in the IP field. <br /><br />Summary: <div>
arXiv:2504.15524v1 Announce Type: new 
Abstract: Intellectual Property (IP) is a unique domain that integrates technical and legal knowledge, making it inherently complex and knowledge-intensive. As large language models (LLMs) continue to advance, they show great potential for processing IP tasks, enabling more efficient analysis, understanding, and generation of IP-related content. However, existing datasets and benchmarks either focus narrowly on patents or cover limited aspects of the IP field, lacking alignment with real-world scenarios. To bridge this gap, we introduce the first comprehensive IP task taxonomy and a large, diverse bilingual benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is designed to evaluate LLMs in real-world intellectual property applications, encompassing both understanding and generation. We benchmark 16 LLMs, ranging from general-purpose to domain-specific models, and find that even the best-performing model achieves only 75.8% accuracy, revealing substantial room for improvement. Notably, open-source IP and law-oriented models lag behind closed-source general-purpose models. We publicly release all data and code of IPBench and will continue to update it with additional IP-related tasks to better reflect real-world challenges in the intellectual property domain.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compass-V2 Technical Report</title>
<link>https://arxiv.org/abs/2504.15527</link>
<guid>https://arxiv.org/abs/2504.15527</guid>
<content:encoded><![CDATA[
<div> Keywords: lightweight Mixture-of-Experts model, Southeast Asian languages, e-commerce applications, high-quality dataset, hybrid reasoning model 

Summary: 
Compass-v2 is a lightweight Mixture-of-Experts (MoE) model designed specifically for Southeast Asian languages and e-commerce applications. It addresses the underrepresentation of low-resource languages in the AI field by focusing on SEA languages. The model has a total of 30B parameters, with 5B active parameters, balancing performance and inference cost. A high-quality dataset for SEA languages was curated to enhance multilingual performance, along with a dataset for e-commerce applications sourced through data mining. The model features a hybrid reasoning model that combines fast and deep thinking capabilities, diverging from the industry norm of using separate models. Through experimental evaluations, Compass-v2 demonstrates state-of-the-art performance in multilingual and e-commerce tasks among sub-30B models, while maintaining lower inference costs. 

<br /><br />Summary: <div>
arXiv:2504.15527v1 Announce Type: new 
Abstract: Predominant LLMs focus on high-resource languages while leaving low-resource languages, particularly those in Southeast Asia (SEA), underrepresented. In addition, those models are general-purpose and pay limited attention to the e-commerce domain. To overcome these limitations, we introduce Compass-v2, a lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast Asian languages and e-commerce applications. To balance model performance and inference cost, the model is designed with 30B total parameters and 5B active parameters, incorporating both fine-grained and shared expert modules. To enhance multilingual performance, we curated and constructed a high-quality, industry-leading SEA dataset, to the best of our knowledge. To boost performance in the e-commerce domain, we built a dataset comprising hundreds of billions of tokens, sourced through external data mining and internal platform collection. Besides, we pioneered a hybrid reasoning model that supports both fast thinking and deep thinking within a unified framework to enhance the reasoning capabilities, diverging from the conventional industry practice of deploying two separate models. Through extensive experimental evaluations, our model demonstrates state-of-the-art SEA multilingual and e-commerce performance among sub-30B models, while maintaining significantly lower inference cost.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length</title>
<link>https://arxiv.org/abs/2504.15544</link>
<guid>https://arxiv.org/abs/2504.15544</guid>
<content:encoded><![CDATA[
<div> Transformer models, BERT, encoder-only, pre-training, long-context <br />
Summary: 
The study introduces llm-jp-modernbert, a ModernBERT model trained on a large Japanese corpus with a context length of 8192 tokens. While not outperforming existing baseline models in downstream tasks, it shows good performance in fill-mask test evaluations. The study also explores the impact of context length expansion through pseudo-perplexity experiments. Additionally, it delves into sentence embeddings, analyzing their evolution during training and comparing them with other models. The findings reveal similar trends with models of the same architecture. To encourage reproducibility and advance the development of long-context BERT models, the researchers provide their model, training, and evaluation code for public use. <br /> <div>
arXiv:2504.15544v1 Announce Type: new 
Abstract: Encoder-only transformer models like BERT are widely adopted as a pre-trained backbone for tasks like sentence classification and retrieval. However, pretraining of encoder models with large-scale corpora and long contexts has been relatively underexplored compared to decoder-only transformers. In this work, we present llm-jp-modernbert, a ModernBERT model trained on a publicly available, massive Japanese corpus with a context length of 8192 tokens. While our model does not surpass existing baselines on downstream tasks, it achieves good results on fill-mask test evaluations. We also analyze the effect of context length expansion through pseudo-perplexity experiments. Furthermore, we investigate sentence embeddings in detail, analyzing their transitions during training and comparing them with those from other existing models, confirming similar trends with models sharing the same architecture. To support reproducibility and foster the development of long-context BERT, we release our model, along with the training and evaluation code.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Semantic Augmentation for Harmful Content Detection</title>
<link>https://arxiv.org/abs/2504.15548</link>
<guid>https://arxiv.org/abs/2504.15548</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, text preprocessing, semantic augmentation, social media classification, machine learning pipeline 

Summary: 
- Recent advances in large language models (LLMs) have shown strong performance on simple text classification tasks but struggle with complex social media challenges such as propaganda detection, hateful meme classification, and toxicity identification.
- Existing work has mainly focused on using LLMs to generate synthetic training data, overlooking the potential of LLM-based text preprocessing and semantic augmentation.
- This paper introduces an approach that prompts LLMs to clean noisy text and provide context-rich explanations to enhance training sets without significantly increasing data volume.
- Evaluation on multiple datasets shows that zero-shot LLM classification performs poorly on high-context tasks compared to supervised models, while integrating LLM-based semantic augmentation improves performance significantly at a lower cost.
- Strategically incorporating LLMs into the machine learning pipeline for social media classification tasks can have significant implications for combating harmful content online.

<br /><br />Summary: <div>
arXiv:2504.15548v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have demonstrated strong performance on simple text classification tasks, frequently under zero-shot settings. However, their efficacy declines when tackling complex social media challenges such as propaganda detection, hateful meme classification, and toxicity identification. Much of the existing work has focused on using LLMs to generate synthetic training data, overlooking the potential of LLM-based text preprocessing and semantic augmentation. In this paper, we introduce an approach that prompts LLMs to clean noisy text and provide context-rich explanations, thereby enhancing training sets without substantial increases in data volume. We systematically evaluate on the SemEval 2024 multi-label Persuasive Meme dataset and further validate on the Google Jigsaw toxic comments and Facebook hateful memes datasets to assess generalizability. Our results reveal that zero-shot LLM classification underperforms on these high-context tasks compared to supervised models. In contrast, integrating LLM-based semantic augmentation yields performance on par with approaches that rely on human-annotated data, at a fraction of the cost. These findings underscore the importance of strategically incorporating LLMs into machine learning (ML) pipeline for social media classification tasks, offering broad implications for combating harmful content online.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction</title>
<link>https://arxiv.org/abs/2504.15573</link>
<guid>https://arxiv.org/abs/2504.15573</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, instruction-following, data synthesis, WebR, domain adaptation 

Summary: 
The article introduces Web Reconstruction (WebR), a framework for automatically synthesizing high-quality instruction-tuning (IT) data from raw web documents. By utilizing a dual-perspective paradigm - Web as Instruction and Web as Response - WebR reconstructs web content to generate instructional data for language models. Experimental results show that datasets generated by WebR outperform existing methods by up to 16.65% on instruction-following tasks. WebR offers superior compatibility, data efficiency, and scalability, making it easier to adapt language models to different domains. The framework is publicly available, providing researchers with a valuable tool for improving instruction-following capabilities of language models. <br /><br />Summary: <div>
arXiv:2504.15573v1 Announce Type: new 
Abstract: The improvement of LLMs' instruction-following capabilities depends critically on the availability of high-quality instruction-response pairs. While existing automatic data synthetic methods alleviate the burden of manual curation, they often rely heavily on either the quality of seed data or strong assumptions about the structure and content of web documents. To tackle these challenges, we propose Web Reconstruction (WebR), a fully automated framework for synthesizing high-quality instruction-tuning (IT) data directly from raw web documents with minimal assumptions. Leveraging the inherent diversity of raw web content, we conceptualize web reconstruction as an instruction-tuning data synthesis task via a novel dual-perspective paradigm--Web as Instruction and Web as Response--where each web document is designated as either an instruction or a response to trigger the reconstruction process. Comprehensive experiments show that datasets generated by WebR outperform state-of-the-art baselines by up to 16.65% across four instruction-following benchmarks. Notably, WebR demonstrates superior compatibility, data efficiency, and scalability, enabling enhanced domain adaptation with minimal effort. The data and code are publicly available at https://github.com/YJiangcm/WebR.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models</title>
<link>https://arxiv.org/abs/2504.15604</link>
<guid>https://arxiv.org/abs/2504.15604</guid>
<content:encoded><![CDATA[
<div> Model Comparison, Next-Token Prediction, Theory of Mind Tasks, Contextual Complexity, Reasoning Levels
Summary:
The study compares the next-token prediction performance of GPT-2 and Llama-2-7b-chat-hf on Theory of Mind tasks using a dataset created from short stories. The addition of infill sentences increases complexity and ambiguity, slightly reducing prediction accuracy. Llama-2 outperforms GPT-2 in accuracy, especially at lower temperatures, showing higher confidence in token selection. As reasoning complexity increases, model responses diverge. GPT-2 and Llama-2 exhibit more variability in predictions during first- and second-order reasoning tasks. These findings shed light on how model architecture, temperature, and contextual complexity impact next-token prediction, offering insights into the strengths and limitations of current language models.
<br /><br />Summary: <div>
arXiv:2504.15604v1 Announce Type: new 
Abstract: Language models have made significant progress in generating coherent text and predicting next tokens based on input prompts. This study compares the next-token prediction performance of two well-known models: OpenAI's GPT-2 and Meta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their capabilities, we built a dataset from 10 short stories sourced from the Explore ToM Dataset. We enhanced these stories by programmatically inserting additional sentences (infills) using GPT-4, creating variations that introduce different levels of contextual complexity. This setup enables analysis of how increasing context affects model performance. We tested both models under four temperature settings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next token across three reasoning levels. Zero-order reasoning involves tracking the state, either current (ground truth) or past (memory). First-order reasoning concerns understanding another's mental state (e.g., "Does Anne know the apple is salted?"). Second-order reasoning adds recursion (e.g., "Does Anne think that Charles knows the apple is salted?").
  Our results show that adding more infill sentences slightly reduces prediction accuracy, as added context increases complexity and ambiguity. Llama-2 consistently outperforms GPT-2 in prediction accuracy, especially at lower temperatures, demonstrating greater confidence in selecting the most probable token. As reasoning complexity rises, model responses diverge more. Notably, GPT-2 and Llama-2 display greater variability in predictions during first- and second-order reasoning tasks. These findings illustrate how model architecture, temperature, and contextual complexity influence next-token prediction, contributing to a better understanding of the strengths and limitations of current language models.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement</title>
<link>https://arxiv.org/abs/2504.15630</link>
<guid>https://arxiv.org/abs/2504.15630</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Context-aware Layer Enhancement, contextual knowledge, Question-Answering tasks, V-usable information analysis

Summary: 
Context-aware Layer Enhancement (CaLE) is proposed as a method to enhance utilization of contextual knowledge within Large Language Models (LLMs). The approach strategically amplifies contextual information growth at an optimal layer, enriching final layer representations. By focusing on internal mechanisms of processing contextual information, CaLE improves context-faithful generation in Question-Answering tasks. This intervention method addresses limitations in LLMs' ability to leverage contextual knowledge, particularly in scenarios with unknown or conflicting information. Experimental results demonstrate the effectiveness of CaLE in enhancing the generation of contextually accurate responses. The approach emphasizes the importance of refining internal representations to improve overall performance in language understanding tasks.  <br /><br />Summary: <div>
arXiv:2504.15630v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet they often struggle with context-faithfulness generations that properly reflect contextual knowledge. While existing approaches focus on enhancing the decoding strategies, they ignore the fundamental mechanism of how contextual information is processed within LLMs' internal states. As a result, LLMs remain limited in their ability to fully leverage contextual knowledge. In this paper, we propose Context-aware Layer Enhancement (CaLE), a novel intervention method that enhances the utilization of contextual knowledge within LLMs' internal representations. By employing V-usable information analysis, CaLE strategically amplifies the growth of contextual information at an optimal layer, thereby enriching representations in the final layer. Our experiments demonstrate that CaLE effectively improves context-faithful generation in Question-Answering tasks, particularly in scenarios involving unknown or conflicting contextual knowledge.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Effective Text Clustering with Large Language Models</title>
<link>https://arxiv.org/abs/2504.15640</link>
<guid>https://arxiv.org/abs/2504.15640</guid>
<content:encoded><![CDATA[
<div> Keywords: Text clustering, Large language models, Cost-effective framework, Constraint-based clustering, EdgeLLM

Summary: 
TECL introduces a cost-effective framework for accurate text clustering by leveraging feedback from large language models (LLMs) while minimizing the computational and financial burden. The framework utilizes EdgeLLM and TriangleLLM to create must-link/cannot-link constraints for text pairs/triplets and incorporates them into a weighted constrained clustering approach. EdgeLLM and TriangleLLM help identify informative text pairs/triplets for querying LLMs efficiently and extract accurate pairwise constraints through prompting techniques. Experimental results demonstrate the superiority of TECL over existing solutions in unsupervised text clustering under the same query cost for LLMs. <div>
arXiv:2504.15640v1 Announce Type: new 
Abstract: Text clustering aims to automatically partition a collection of text documents into distinct clusters based on linguistic features. In the literature, this task is usually framed as metric clustering based on text embeddings from pre-trained encoders or a graph clustering problem upon pairwise similarities from an oracle, e.g., a large ML model. Recently, large language models (LLMs) bring significant advancement in this field by offering contextualized text embeddings and highly accurate similarity scores, but meanwhile, present grand challenges to cope with substantial computational and/or financial overhead caused by numerous API-based queries or inference calls to the models.
  In response, this paper proposes TECL, a cost-effective framework that taps into the feedback from LLMs for accurate text clustering within a limited budget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or TriangleLLM to construct must-link/cannot-link constraints for text pairs, and further leverages such constraints as supervision signals input to our weighted constrained clustering approach to generate clusters. Particularly, EdgeLLM (resp. TriangleLLM) enables the identification of informative text pairs (resp. triplets) for querying LLMs via well-thought-out greedy algorithms and accurate extraction of pairwise constraints through carefully-crafted prompting techniques. Our experiments on multiple benchmark datasets exhibit that TECL consistently and considerably outperforms existing solutions in unsupervised text clustering under the same query cost for LLMs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Typology</title>
<link>https://arxiv.org/abs/2504.15642</link>
<guid>https://arxiv.org/abs/2504.15642</guid>
<content:encoded><![CDATA[
<div> computational methods, typological research, linguistic data, language structure, statistical modeling <br />
Summary:<br />
Typology in linguistics classifies languages based on structural features rather than historical relationships. Computational methods are increasingly used for large-scale linguistic data analysis, enabling hypothesis testing about language structure and evolution. This article highlights the advantages of computational statistical modeling in typological research. <div>
arXiv:2504.15642v1 Announce Type: new 
Abstract: Typology is a subfield of linguistics that focuses on the study and classification of languages based on their structural features. Unlike genealogical classification, which examines the historical relationships between languages, typology seeks to understand the diversity of human languages by identifying common properties and patterns, known as universals. In recent years, computational methods have played an increasingly important role in typological research, enabling the analysis of large-scale linguistic data and the testing of hypotheses about language structure and evolution. This article provides an illustration of the benefits of computational statistical modeling in typology.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTextSim: Enhancing Financial Text Analysis with BERTopic</title>
<link>https://arxiv.org/abs/2504.15683</link>
<guid>https://arxiv.org/abs/2504.15683</guid>
<content:encoded><![CDATA[
<div> BERTopic, FinTextSim, financial text analysis, contextual embeddings, S&amp;P 500<br />
<br />
Summary:
Recent advancements in information availability and computational capabilities have revolutionized the analysis of annual reports, combining financial metrics with textual data. BERTopic, a cutting-edge topic model using contextual embeddings, was evaluated for analyzing financial reports from S&amp;P 500 companies. FinTextSim, a specialized sentence-transformer model for financial contexts, significantly improved topic clustering and semantic search compared to all-MiniLM-L6-v2. BERTopic's performance was found to be enhanced when paired with FinTextSim's embeddings, leading to clearer and distinct economic topic clusters. The use of FinTextSim is crucial for accurate financial text analysis, as it reduces misclassification and overlapping topics. The improved insights generated by these models have the potential to enhance decision-making processes, streamline resource allocation, and benefit business valuation and stock price prediction models. <div>
arXiv:2504.15683v1 Announce Type: new 
Abstract: Recent advancements in information availability and computational capabilities have transformed the analysis of annual reports, integrating traditional financial metrics with insights from textual data. To extract valuable insights from this wealth of textual data, automated review processes, such as topic modeling, are crucial. This study examines the effectiveness of BERTopic, a state-of-the-art topic model relying on contextual embeddings, for analyzing Item 7 and Item 7A of 10-K filings from S&amp;P 500 companies (2016-2022). Moreover, we introduce FinTextSim, a finetuned sentence-transformer model optimized for clustering and semantic search in financial contexts. Compared to all-MiniLM-L6-v2, the most widely used sentence-transformer, FinTextSim increases intratopic similarity by 81% and reduces intertopic similarity by 100%, significantly enhancing organizational clarity. We assess BERTopic's performance using embeddings from both FinTextSim and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and distinct economic topic clusters when paired with FinTextSim's embeddings. Without FinTextSim, BERTopic struggles with misclassification and overlapping topics. Thus, FinTextSim is pivotal for advancing financial text analysis. FinTextSim's enhanced contextual embeddings, tailored for the financial domain, elevate the quality of future research and financial information. This improved quality of financial information will enable stakeholders to gain a competitive advantage, streamlining resource allocation and decision-making processes. Moreover, the improved insights have the potential to leverage business valuation and stock price prediction models.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subject islands do not reduce to construction-specific discourse function</title>
<link>https://arxiv.org/abs/2504.15688</link>
<guid>https://arxiv.org/abs/2504.15688</guid>
<content:encoded><![CDATA[
<div> Keywords: islands, linguistics, syntax, information structure, acceptability

Summary:
- The term "islands" in linguistics refers to phrases from which extracting an element results in ungrammaticality.
- Generative tradition attributes ungrammaticality in islands to abstract movement dependency between elements.
- Research emphasizing language's communicative function suggests syntactic constraints like islands can be explained based on information packaging.
- Abeill et al. (2020) propose that subject island effects are specific to information structure in wh-questions, not movement.
- Large-scale acceptability studies on wh-questions, relative clauses, and topicalization show evidence of a subject island effect, contrary to Abeill et al.'s prediction. 

<br /><br />Summary: This article explores the concept of islands in linguistics, focusing on the ungrammatical extraction of elements from certain phrases. While the generative tradition attributes this to abstract movement dependencies, recent research suggests that syntactic constraints like islands may be linked to information packaging in language. Despite a proposal that subject island effects are specific to the information structure of wh-questions, empirical studies across different constructions show evidence of a subject island effect. These findings support the argument for abstract, syntactic representations playing a crucial role in island effects, independent of the communicative function associated with specific constructions. <div>
arXiv:2504.15688v1 Announce Type: new 
Abstract: The term islands in linguistics refers to phrases from which extracting an element results in ungrammaticality (Ross, 1967). Grammatical subjects are considered islands because extracting a sub-part of a subject results in an ill-formed sentence, despite having a clear intended meaning (e.g., "Which topic did the article about inspire you?"). The generative tradition, which views syntax as autonomous of meaning and function, attributes this ungrammaticality to the abstract movement dependency between the wh-phrase and the subject-internal position with which it is associated for interpretation. However, research on language that emphasizes its communicative function suggests instead that syntactic constraints, including islands, can be explained based on the way different constructions package information. Accordingly, Abeill\'e et al. (2020) suggest that the islandhood of subjects is specific to the information structure of wh-questions, and propose that subjects are not islands for movement, but for focusing, due to their discourse-backgroundedness. This predicts that other constructions that differ in their information structure from wh-questions, but still involve movement, should not create a subject island effect. We test this prediction in three large-scale acceptability studies, using a super-additive design that singles out subject island violations, in three different constructions: wh-questions, relative clauses, and topicalization. We report evidence for a subject island effect in each construction type, despite only wh-questions introducing what Abeill\'e et al. (2020) call "a clash in information structure." We argue that this motivates an account of islands in terms of abstract, syntactic representations, independent of the communicative function associated with the constructions.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tina: Tiny Reasoning Models via LoRA</title>
<link>https://arxiv.org/abs/2504.15777</link>
<guid>https://arxiv.org/abs/2504.15777</guid>
<content:encoded><![CDATA[
<div> cost-effective, language models, reasoning abilities, tiny models, reinforcement learning

Summary:<br />
The study introduces Tina, a family of tiny reasoning models that achieve strong reasoning performance with minimal resources. By applying parameter-efficient updates during reinforcement learning (RL) using low-rank adaptation (LoRA) to a 1.5B parameter base model, Tina demonstrates competitive reasoning performance and surpasses state-of-the-art RL reasoning models at a significantly reduced computational cost. The best Tina model shows a >20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24, with only a $9 USD post-training and evaluation cost, representing a 260x cost reduction. The study highlights the efficiency of efficient RL reasoning via LoRA, attributing the success to rapid adaptation to the structural format of reasoning rewarded by RL while preserving the base model's underlying knowledge. All code, training logs, and model weights & checkpoints are fully open-sourced for accessibility and further research. 

<br /><br />Summary: <div>
arXiv:2504.15777v1 Announce Type: new 
Abstract: How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach</title>
<link>https://arxiv.org/abs/2504.15784</link>
<guid>https://arxiv.org/abs/2504.15784</guid>
<content:encoded><![CDATA[
<div> Keywords: Creative writing, Large Language Models, Automated evaluation method, Torrance Test of Creative Writing, Alignment with human assessments

Summary:
The study explores the challenge of evaluating the creativity of machine-generated texts, particularly in the context of creative writing by Large Language Models (LLMs). Existing methods for evaluating the creativity of such texts are either manual and costly or lack alignment with human assessments. The paper introduces an automated evaluation method based on the Torrance Test of Creative Writing (TTCW), which assesses creativity as a product. This approach utilizes a reference-based Likert-style method to score machine-generated creative texts against high-quality reference texts across various tests. Experimental results show that the proposed method significantly enhances the alignment between evaluations of LLM-generated texts and human assessments, achieving a pairwise accuracy of 0.75 (+15%). This method has the potential to improve the evaluation of creative writing by LLMs and facilitate their use in various creative domains. 

<br /><br />Summary: <div>
arXiv:2504.15784v1 Announce Type: new 
Abstract: Creative writing is a key capability of Large Language Models (LLMs), with potential applications in literature, storytelling, and various creative domains. However, evaluating the creativity of machine-generated texts remains a significant challenge, as existing methods either rely on costly manual annotations or fail to align closely with human assessments. In this paper, we propose an effective automated evaluation method based on the Torrance Test of Creative Writing (TTCW), which evaluates creativity as product. Our method employs a reference-based Likert-style approach, scoring generated creative texts relative to high-quality reference texts across various tests. Experimental results demonstrate that our method significantly improves the alignment between LLM evaluations and human assessments, achieving a pairwise accuracy of 0.75 (+15\%).
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A closer look at how large language models trust humans: patterns and biases</title>
<link>https://arxiv.org/abs/2504.15801</link>
<guid>https://arxiv.org/abs/2504.15801</guid>
<content:encoded><![CDATA[
<div> trust dynamics, large language models, trustworthiness, demographic variables, AI-to-human trust dynamics

Summary:<br />
- The study focuses on understanding trust dynamics between humans and AI agents, specifically looking at how large language models (LLMs) develop trust in humans in decision-making contexts.
- LLM-based agents likely rely on implicit effective trust in trust-related scenarios such as evaluating loan applications to assist decision-making.
- The research uses established behavioral theories to investigate whether LLM trust depends on human trustworthiness dimensions: competence, benevolence, and integrity.
- Demographic variables like age, religion, and gender can also affect LLM trust, especially in financial scenarios.
- Across 43,200 simulated experiments involving five popular language models and five scenarios, LLM trust development shows similarities to human trust development, with trust being strongly predicted by trustworthiness in most cases but also influenced by demographic factors. Understanding AI-to-human trust dynamics and monitoring biases is crucial to prevent unintended harmful outcomes in trust-sensitive AI applications. 

Summary: <div>
arXiv:2504.15801v1 Announce Type: new 
Abstract: As large language models (LLMs) and LLM-based agents increasingly interact with humans in decision-making contexts, understanding the trust dynamics between humans and AI agents becomes a central concern. While considerable literature studies how humans trust AI agents, it is much less understood how LLM-based agents develop effective trust in humans. LLM-based agents likely rely on some sort of implicit effective trust in trust-related contexts (e.g., evaluating individual loan applications) to assist and affect decision making. Using established behavioral theories, we develop an approach that studies whether LLMs trust depends on the three major trustworthiness dimensions: competence, benevolence and integrity of the human subject. We also study how demographic variables affect effective trust. Across 43,200 simulated experiments, for five popular language models, across five different scenarios we find that LLM trust development shows an overall similarity to human trust development. We find that in most, but not all cases, LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender, especially in financial scenarios. This is particularly true for scenarios common in the literature and for newer models. While the overall patterns align with human-like mechanisms of effective trust formation, different models exhibit variation in how they estimate trust; in some cases, trustworthiness and demographic factors are weak predictors of effective trust. These findings call for a better understanding of AI-to-human trust dynamics and monitoring of biases and trust development patterns to prevent unintended and potentially harmful outcomes in trust-sensitive applications of AI.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns</title>
<link>https://arxiv.org/abs/2504.15815</link>
<guid>https://arxiv.org/abs/2504.15815</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt engineering, large language models, token patterns, data mining, systematic differences

Summary: 
Spotlight introduces a novel approach to address the challenges in prompt engineering for large language models. By combining automation and human analysis, Spotlight uses data mining techniques to differentiate between random variations and systematic differences in model outputs, providing token patterns that guide users in analyzing the effects of prompt and model changes efficiently. Benchmarks are created to test the reliability of token pattern extraction methods, demonstrating new insights into established prompt data. Through demonstration studies and a user study, the approach enables users to understand systematic differences in language model outputs, identifying relevant differences caused by prompt and model changes related to factors such as gender or culture. This supports the prompt engineering process and facilitates human-centric model behavior research.<br /><br />Summary: <div>
arXiv:2504.15815v1 Announce Type: new 
Abstract: Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompt and model changes efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs, and we are able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model</title>
<link>https://arxiv.org/abs/2504.15843</link>
<guid>https://arxiv.org/abs/2504.15843</guid>
<content:encoded><![CDATA[
<div> reference model, reinforcement learning, human feedback, language models, preference optimization

Summary:
Pre-Direct Preference Optimization (Pre-DPO) improves reinforcement learning from human feedback for large language models by utilizing a guiding reference model to enhance preference optimization performance. Direct Preference Optimization (DPO) simplifies the process by directly optimizing human preferences without an explicit reward model. However, initializing the policy and reference models identically can lead to inefficient data utilization. Simple Preference Optimization (SimPO) lacks a reference model, reducing training robustness. Pre-DPO leverages a guiding reference model to assign weights adaptively, improving the performance of both DPO and SimPO. Extensive experiments on benchmarks demonstrate the effectiveness of Pre-DPO in enhancing training performance without the need for external models or additional data.<br /><br />Summary: <div>
arXiv:2504.15843v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2504.15848</link>
<guid>https://arxiv.org/abs/2504.15848</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal aspect-based sentiment classification, sentiment polarity prediction, cognitive and aesthetic sentiment causality understanding framework, visual features, large language model<br />
Summary: 
Chimera is a framework for multimodal aspect-based sentiment classification that focuses on understanding fine-grained visual content and cognitive rationales for sentiment expression. It aligns visual patch features with textual descriptions and incorporates coarse and fine-grained visual features to enhance sentiment analysis. The framework utilizes a large language model to improve awareness of sentimental cues and affective-cognitive resonance. Experimental results show the effectiveness of Chimera in MASC tasks and highlight its flexibility compared to existing models. The complete implementation and dataset are publicly available on GitHub.<br /> 
Summary: <div>
arXiv:2504.15848v1 Announce Type: new 
Abstract: Multimodal aspect-based sentiment classification (MASC) is an emerging task due to an increase in user-generated multimodal content on social platforms, aimed at predicting sentiment polarity toward specific aspect targets (i.e., entities or attributes explicitly mentioned in text-image pairs). Despite extensive efforts and significant achievements in existing MASC, substantial gaps remain in understanding fine-grained visual content and the cognitive rationales derived from semantic content and impressions (cognitive interpretations of emotions evoked by image content). In this study, we present Chimera: a cognitive and aesthetic sentiment causality understanding framework to derive fine-grained holistic features of aspects and infer the fundamental drivers of sentiment expression from both semantic perspectives and affective-cognitive resonance (the synergistic effect between emotional responses and cognitive interpretations). Specifically, this framework first incorporates visual patch features for patch-word alignment. Meanwhile, it extracts coarse-grained visual features (e.g., overall image representation) and fine-grained visual regions (e.g., aspect-related regions) and translates them into corresponding textual descriptions (e.g., facial, aesthetic). Finally, we leverage the sentimental causes and impressions generated by a large language model (LLM) to enhance the model's awareness of sentimental cues evoked by semantic content and affective-cognitive resonance. Experimental results on standard MASC datasets demonstrate the effectiveness of the proposed model, which also exhibits greater flexibility to MASC compared to LLMs such as GPT-4o. We have publicly released the complete implementation and dataset at https://github.com/Xillv/Chimera
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Early Exit in Reasoning Models</title>
<link>https://arxiv.org/abs/2504.15895</link>
<guid>https://arxiv.org/abs/2504.15895</guid>
<content:encoded><![CDATA[
<div> scaling, chain-of-thought generation, reasoning language models, early exit, model behavior monitoring
Summary:
The article introduces a method for self-truncating chain-of-thought sequences generated by large reasoning language models. This method aims to prevent overthinking and improve efficiency by dynamically terminating reasoning chains when the model shows high confidence in an answer. The approach does not require additional training and can be seamlessly integrated into existing reasoning LLMs. Experimental results on various benchmarks demonstrate the effectiveness of the proposed method, reducing CoT sequence length by 31% to 43% and improving accuracy by 1.7% to 5.7% for deepseek-series reasoning LLMs. <div>
arXiv:2504.15895v1 Announce Type: new 
Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,"Wait" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024 show that the proposed method is consistently effective on deepseek-series reasoning LLMs, reducing the length of CoT sequences by an average of 31% to 43% while improving accuracy by 1.7% to 5.7%.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.15900</link>
<guid>https://arxiv.org/abs/2504.15900</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, large language models, audio-language reasoning, structured reasoning, curriculum learning 

Summary:<br /><br />
The study explores the impact of reinforcement learning on large audio-language models, finding that it can significantly enhance reasoning abilities. Using the GRPO framework, a structured audio reasoning model named SARI is developed, achieving a notable accuracy improvement over the base model. Through supervised fine-tuning on structured and unstructured chains of thought, followed by curriculum-guided reinforcement learning, the model demonstrates the effectiveness of explicit, structured reasoning and curriculum learning in enhancing audio-language understanding. The findings highlight the importance of SFT warm-up for stable RL training, the advantage of structured chains over unstructured ones for generalization, and the benefits of easy-to-hard curricula in accelerating convergence and improving performance. Overall, the study underscores the potential of explicit, structured reasoning and curriculum learning in advancing audio-language understanding. 

<br /><br />Summary: <div>
arXiv:2504.15900v1 Announce Type: new 
Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to "think before answering." Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity</title>
<link>https://arxiv.org/abs/2504.15941</link>
<guid>https://arxiv.org/abs/2504.15941</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, inclusive language translation, FairTranslate dataset, gender biases, machine translation

Summary: 
The paper introduces FairTranslate, a human-annotated dataset focusing on evaluating gender biases in machine translation systems when translating English to French text related to occupations. The dataset includes metadata such as stereotypical alignment of occupations, grammatical gender ambiguity, and ground-truth gender labels. Four prominent Large Language Models were tested on this dataset, revealing significant biases in gender representation. The results emphasize the ongoing challenges in achieving equitable translation outcomes and underscore the importance of strategies to ensure fair and inclusive language usage in machine translation systems. The FairTranslate dataset is publicly available on Hugging Face, and the experiment code is shared on GitHub. <div>
arXiv:2504.15941v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks but often fall short when translating inclusive language -- such as texts containing the singular 'they' pronoun or otherwise reflecting fair linguistic protocols. Because these challenges span both computational and societal domains, it is imperative to critically evaluate how well LLMs handle inclusive translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset designed to evaluate non-binary gender biases in machine translation systems from English to French. FairTranslate includes 2418 English-French sentence pairs related to occupations, annotated with rich metadata such as the stereotypical alignment of the occupation, grammatical gender indicator ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B, Llama3.3-70B) on this dataset under different prompting procedures. Our results reveal substantial biases in gender representation across LLMs, highlighting persistent challenges in achieving equitable outcomes in machine translation. These findings underscore the need for focused strategies and interventions aimed at ensuring fair and inclusive language usage in LLM-based translation systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and disclose the code for all experiments on GitHub.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models</title>
<link>https://arxiv.org/abs/2504.15983</link>
<guid>https://arxiv.org/abs/2504.15983</guid>
<content:encoded><![CDATA[
<div> Keywords: lightweight language models, zero-shot NAS, W-PCA, efficiency, evaluation metrics

Summary:
Weight-weighted PCA (W-PCA) is introduced as a novel zero-shot NAS method for lightweight language models. It optimizes evaluation time by utilizing proxies such as parameter count and principal components in the FFN layer. The method eliminates the need for gradient computations, improving efficiency in designing and evaluating language models. Comparative analysis on GLUE and SQuAD datasets shows reduced training time and higher scores in testing compared to previous methods. Ranking evaluations on a dataset from FlexiBERT search space demonstrate superior ranking correlation and reduced solving time. Overall, W-PCA offers a more efficient approach for designing and evaluating lightweight language models, outperforming existing methods in terms of training time and performance. 

<br /><br />Summary: <div>
arXiv:2504.15983v1 Announce Type: new 
Abstract: The demand for efficient natural language processing (NLP) systems has led to the development of lightweight language models. Previous work in this area has primarily focused on manual design or training-based neural architecture search (NAS) methods. Recently, zero-shot NAS methods have been proposed for evaluating language models without the need for training. However, prevailing approaches to zero-shot NAS often face challenges such as biased evaluation metrics and computational inefficiencies. In this paper, we introduce weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored for lightweight language models. Our approach utilizes two evaluation proxies: the parameter count and the number of principal components with cumulative contribution exceeding $\eta$ in the feed-forward neural (FFN) layer. Additionally, by eliminating the need for gradient computations, we optimize the evaluation time, thus enhancing the efficiency of designing and evaluating lightweight language models. We conduct a comparative analysis on the GLUE and SQuAD datasets to evaluate our approach. The results demonstrate that our method significantly reduces training time compared to one-shot NAS methods and achieves higher scores in the testing phase compared to previous state-of-the-art training-based methods. Furthermore, we perform ranking evaluations on a dataset sampled from the FlexiBERT search space. Our approach exhibits superior ranking correlation and further reduces solving time compared to other zero-shot NAS methods that require gradient computation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Hate Speech Detection Based on the MindSpore Framework</title>
<link>https://arxiv.org/abs/2504.15987</link>
<guid>https://arxiv.org/abs/2504.15987</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech detection, few-shot learning, deep learning, prompt-based learning, adversarial augmentation

Summary: 
MS-FSLHate is a neural framework designed to detect hate speech in few-shot scenarios using prompt-enhanced deep learning models. The framework utilizes learnable prompt embeddings, a CNN-BiLSTM backbone with attention pooling, and synonym-based adversarial data augmentation to enhance generalization. Experimental results on two benchmark datasets show that MS-FSLHate outperforms competitive baselines in terms of precision, recall, and F1-score. The model also demonstrates high efficiency and scalability, making it suitable for deployment in resource-constrained environments. The combination of prompt-based learning and adversarial augmentation proves to be effective in improving hate speech detection in scenarios with limited labeled data. Overall, MS-FSLHate presents a robust and adaptable solution for detecting hate speech on social media platforms. 

<br /><br />Summary: <div>
arXiv:2504.15987v1 Announce Type: new 
Abstract: The proliferation of hate speech on social media poses a significant threat to online communities, requiring effective detection systems. While deep learning models have shown promise, their performance often deteriorates in few-shot or low-resource settings due to reliance on large annotated corpora. To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for few-shot hate speech detection implemented on the MindSpore deep learning platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM backbone with attention pooling, and synonym-based adversarial data augmentation to improve generalization. Experimental results on two benchmark datasets-HateXplain and HSOL-demonstrate that our approach outperforms competitive baselines in precision, recall, and F1-score. Additionally, the framework shows high efficiency and scalability, suggesting its suitability for deployment in resource-constrained environments. These findings highlight the potential of combining prompt-based learning with adversarial augmentation for robust and adaptable hate speech detection in few-shot scenarios.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Cost-Aware Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.16005</link>
<guid>https://arxiv.org/abs/2504.16005</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, prompt optimization, AutoML techniques, multi-objective optimization, cost-efficiency

Summary:
Large language models (LLMs) have transformed natural language processing but are highly dependent on prompt formulation. Prompt optimization is crucial but can be expensive. CAPO (Cost-Aware Prompt Optimization) is introduced as an efficient algorithm that leverages AutoML techniques to enhance prompt optimization. It integrates LLMs as operators, utilizing racing to save evaluations and multi-objective optimization to balance performance and prompt length. CAPO optimizes instructions and few-shot examples while considering task descriptions for improved robustness. Through extensive experiments, CAPO outperforms existing discrete prompt optimization methods in multiple cases with up to 21% improvement. It achieves better performance with smaller budgets, saves evaluations, and reduces average prompt length, making it cost-efficient and aware. Even without few-shot examples, CAPO remains robust to initial prompts, showcasing its power and accessibility in prompt optimization. 

<br /><br />Summary: <div>
arXiv:2504.16005v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Methods for Recognizing Nested Terms</title>
<link>https://arxiv.org/abs/2504.16007</link>
<guid>https://arxiv.org/abs/2504.16007</guid>
<content:encoded><![CDATA[
<div> nested terms, RuTermEval competition, Binder model, term recognition, nestedness

Summary:
- The paper discusses the participation in the RuTermEval competition focusing on extracting nested terms.
- Utilizing the Binder model, originally successful in recognizing nested named entities, proved effective in extracting nested terms.
- Achieving top results in all tracks of the competition highlights the success of the approach.
- The exploration of recognizing nested terms from flat training data annotated without nestedness presents a novel task.
- The study demonstrates that the proposed approaches effectively retrieve nested terms, even without explicit nested labeling. 

<br /><br />Summary: <div>
arXiv:2504.16007v1 Announce Type: new 
Abstract: In this paper, we describe our participation in the RuTermEval competition devoted to extracting nested terms. We apply the Binder model, which was previously successfully applied to the recognition of nested named entities, to extract nested terms. We obtained the best results of term recognition in all three tracks of the RuTermEval competition. In addition, we study the new task of recognition of nested terms from flat training data annotated with terms without nestedness. We can conclude that several approaches we proposed in this work are viable enough to retrieve nested terms effectively without nested labeling of them.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certified Mitigation of Worst-Case LLM Copyright Infringement</title>
<link>https://arxiv.org/abs/2504.16046</link>
<guid>https://arxiv.org/abs/2504.16046</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, copyright infringement, BloomScrub, quote detection, risk reduction

Summary:
BloomScrub introduces a simple yet effective method to prevent copyright infringement by large language models during inference time. The approach combines quote detection and rewriting techniques using efficient data sketches like Bloom filters. This allows for scalable copyright screening even in real-world corpora. When faced with long verbatim quotes from copyrighted sources, the system can abstain from responding, providing certified risk reduction. Experimental results show that BloomScrub successfully reduces infringement risk while preserving utility and adapting to different levels of enforcement stringency. The research highlights the importance of lightweight inference-time methods for copyright prevention, offering a promising solution to address worst-case copyright risks in large language models.<br /><br />Summary: BloomScrub is a highly effective inference-time approach that prevents copyright infringement by large language models through quote detection and rewriting techniques. It uses efficient data sketches like Bloom filters to enable scalable copyright screening, reducing risk while preserving utility and adapting to varying enforcement levels. The system can abstain from responding when faced with long verbatim quotes, providing certified risk reduction and demonstrating the efficacy of lightweight methods in copyright prevention. <div>
arXiv:2504.16046v1 Announce Type: new 
Abstract: The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of "copyright takedown" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement</title>
<link>https://arxiv.org/abs/2504.16053</link>
<guid>https://arxiv.org/abs/2504.16053</guid>
<content:encoded><![CDATA[
<div> Keywords: State space models, LongMamba, Mamba models, long-context understanding, token filtering

Summary: 
LongMamba is a new technique designed to enhance the long-context capabilities of State space models (SSMs), specifically Mamba models. These models have been shown to underperform compared to Transformers in tasks requiring understanding of long contexts. LongMamba categorizes hidden channels in Mamba models into local and global channels based on their receptive field lengths, with global channels being crucial for long-context understanding. By identifying critical tokens in global channels and applying token filtering to exclude irrelevant tokens, LongMamba prevents the accumulation of unimportant information in the memory of global channels. This prevents memory decay and significantly enhances Mamba's performance in long-context scenarios. LongMamba achieves this without the need for additional training, setting a new standard for SSMs in handling long contexts. The code for LongMamba is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2504.16053v1 Announce Type: new 
Abstract: State space models (SSMs) have emerged as an efficient alternative to Transformer models for language modeling, offering linear computational complexity and constant memory usage as context length increases. However, despite their efficiency in handling long contexts, recent studies have shown that SSMs, such as Mamba models, generally underperform compared to Transformers in long-context understanding tasks. To address this significant shortfall and achieve both efficient and accurate long-context understanding, we propose LongMamba, a training-free technique that significantly enhances the long-context capabilities of Mamba models. LongMamba builds on our discovery that the hidden channels in Mamba can be categorized into local and global channels based on their receptive field lengths, with global channels primarily responsible for long-context capability. These global channels can become the key bottleneck as the input context lengthens. Specifically, when input lengths largely exceed the training sequence length, global channels exhibit limitations in adaptively extend their receptive fields, leading to Mamba's poor long-context performance. The key idea of LongMamba is to mitigate the hidden state memory decay in these global channels by preventing the accumulation of unimportant tokens in their memory. This is achieved by first identifying critical tokens in the global channels and then applying token filtering to accumulate only those critical tokens. Through extensive benchmarking across synthetic and real-world long-context scenarios, LongMamba sets a new standard for Mamba's long-context performance, significantly extending its operational range without requiring additional training. Our code is available at https://github.com/GATECH-EIC/LongMamba.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability</title>
<link>https://arxiv.org/abs/2504.16056</link>
<guid>https://arxiv.org/abs/2504.16056</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, Knowledge Distillation, Model Performance, Explainability

Summary:
In the study, the authors explore the impact of state-of-the-art distillation methods on model performance and explainability in the context of Large Language Models (LLMs). They introduce new methods, including critique-revision prompting for data generation and a synthesis of existing training methods. The comparison is conducted using the Commonsense Question-Answering (CQA) dataset, evaluating both student model accuracy and explainability through a human-grounded study. The results aim to advance the distillation of small language models and enhance the broader applicability and faster diffusion of LLM technology.<br /><br />Summary: <div>
arXiv:2504.16056v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) has increasingly influenced modern society, recently in particular through significant advancements in Large Language Models (LLMs). However, high computational and storage demands of LLMs still limit their deployment in resource-constrained environments. Knowledge distillation addresses this challenge by training a small student model from a larger teacher model. Previous research has introduced several distillation methods for both generating training data and for training the student model. Despite their relevance, the effects of state-of-the-art distillation methods on model performance and explainability have not been thoroughly investigated and compared. In this work, we enlarge the set of available methods by applying critique-revision prompting to distillation for data generation and by synthesizing existing methods for training. For these methods, we provide a systematic comparison based on the widely used Commonsense Question-Answering (CQA) dataset. While we measure performance via student model accuracy, we employ a human-grounded study to evaluate explainability. We contribute new distillation methods and their comparison in terms of both performance and explainability. This should further advance the distillation of small language models and, thus, contribute to broader applicability and faster diffusion of LLM technology.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation</title>
<link>https://arxiv.org/abs/2504.16060</link>
<guid>https://arxiv.org/abs/2504.16060</guid>
<content:encoded><![CDATA[
<div> evaluation, vision-language systems, pragmatic competence, RefOI dataset, Gricean maxims<br />
<br />
Summary: 
This paper discusses the importance of evaluating vision-language systems (VLMs) based on pragmatic competence, particularly in the task of Referring Expression Generation (REG). The authors introduce a new dataset called RefOI, consisting of annotated images with both written and spoken referring expressions. Through evaluating state-of-the-art VLMs, they identify three key failures in pragmatic competence: failure to uniquely identify the referent, inclusion of excessive or irrelevant information, and misalignment with human pragmatic preference. The study highlights the limitations of current automatic evaluations in capturing these pragmatic violations and emphasizes the need for models and evaluation frameworks that align with real human communication. The findings underscore the significance of integrating pragmatic principles into VLM development and evaluation processes.  <br /> <br />Summary: <div>
arXiv:2504.16060v1 Announce Type: new 
Abstract: Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Python Tool for Reconstructing Full News Text from GDELT</title>
<link>https://arxiv.org/abs/2504.16063</link>
<guid>https://arxiv.org/abs/2504.16063</guid>
<content:encoded><![CDATA[
<div> Keywords: news data, full-text newspaper articles, Global Database of Events, Language, and Tone, Python code, text analysis

Summary:
The paper discusses the significance of news data in various disciplines and the challenges researchers face in accessing comprehensive news corpora. It introduces a novel approach to obtaining full-text newspaper articles at minimal cost by utilizing the GDELT Web News NGrams 3.0 dataset. The method involves reconstructing articles from n-grams through intelligent merging of textual fragments. The provided Python code enables researchers to access structured, large-scale newspaper data for text analysis, overcoming limitations of existing proprietary datasets. This approach enhances the accessibility of news data for empirical research, facilitating applications in economic forecasting, computational social science, and natural language processing.

<br /><br />Summary: <div>
arXiv:2504.16063v1 Announce Type: new 
Abstract: News data have become an essential resource across various disciplines, including economics, finance, management, social sciences, and computer science. Researchers leverage newspaper articles to study economic trends, market dynamics, corporate strategies, public perception, political discourse, and the evolution of public opinion. Additionally, news datasets have been instrumental in training large-scale language models, with applications in sentiment analysis, fake news detection, and automated news summarization. Despite their significance, access to comprehensive news corpora remains a key challenge. Many full-text news providers, such as Factiva and LexisNexis, require costly subscriptions, while free alternatives often suffer from incomplete data and transparency issues. This paper presents a novel approach to obtaining full-text newspaper articles at near-zero cost by leveraging data from the Global Database of Events, Language, and Tone (GDELT). Specifically, we focus on the GDELT Web News NGrams 3.0 dataset, which provides high-frequency updates of n-grams extracted from global online news sources. We provide Python code to reconstruct full-text articles from these n-grams by identifying overlapping textual fragments and intelligently merging them. Our method enables researchers to access structured, large-scale newspaper data for text analysis while overcoming the limitations of existing proprietary datasets. The proposed approach enhances the accessibility of news data for empirical research, facilitating applications in economic forecasting, computational social science, and natural language processing.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation</title>
<link>https://arxiv.org/abs/2504.16073</link>
<guid>https://arxiv.org/abs/2504.16073</guid>
<content:encoded><![CDATA[
<div> navigation, visual language models, GUI, process supervision, task success rate

Summary:
- Recent advancements in visual language models (VLMs) have improved their capabilities in handling complex GUI tasks.
- Black-box commercial VLMs and resource-intensive fine-tuning of open-source VLMs hinder progress in GUI tasks.
- The proposed approach guides VLM agents with process supervision by a reward model during GUI navigation, optimizing actions at each step.
- Significant performance gains were observed in static and dynamic GUI environments, with a 3.4% improvement in single-step action accuracy in static environments and around a 33% increase in task success rate in a dynamic environment.
- Integration of trajectory reflection and retry mechanisms further enhances task success rates. 

<br /><br />Summary: <div>
arXiv:2504.16073v1 Announce Type: new 
Abstract: Recent advancements in visual language models (VLMs) have notably enhanced their capabilities in handling complex Graphical User Interface (GUI) interaction tasks. Despite these improvements, current frameworks often struggle to generate correct actions in challenging GUI environments. State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source VLMs for GUI tasks requires significant resources. Additionally, existing trajectory-level evaluation and refinement techniques frequently fall short due to delayed feedback and local optimization issues. To address these challenges, we propose an approach that guides VLM agents with process supervision by a reward model during GUI navigation and control at inference time. This guidance allows the VLM agent to optimize actions at each inference step, thereby improving performance in both static and dynamic environments. In particular, our method demonstrates significant performance gains in three GUI navigation tasks, achieving a 3.4% improvement in single step action accuracy for static environments, along with a around 33% increase in task success rate in one dynamic environment. With further integration of trajectory reflection and retry mechanisms, we also demonstrate even greater enhancement in task success.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.16074</link>
<guid>https://arxiv.org/abs/2504.16074</guid>
<content:encoded><![CDATA[
<div> physics, benchmark, language models, reasoning, evaluation

Summary:
The article introduces PHYBench, a benchmark designed to assess the reasoning capabilities of large language models (LLMs) in physical contexts. Consisting of 500 physics problems covering various topics, PHYBench aims to evaluate models' understanding of realistic physical processes. It includes a novel evaluation metric, the Expression Edit Distance (EED) Score, based on mathematical expression edit distance. Results from testing LLMs on PHYBench show that current models still fall behind human experts in complex physical reasoning scenarios. The benchmark dataset and results are publicly available for further research and improvement. <div>
arXiv:2504.16074v1 Announce Type: new 
Abstract: We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTRL: Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.16084</link>
<guid>https://arxiv.org/abs/2504.16084</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Test-Time Scaling, Test-Time Reinforcement Learning, Unlabeled Data <br />
<br />
Summary: This paper explores Reinforcement Learning (RL) in Large Language Models (LLMs) on data lacking explicit labels. The challenge lies in estimating rewards during inference without access to ground-truth information. The authors introduce Test-Time Reinforcement Learning (TTRL) as a method for training LLMs using RL on unlabeled data, leveraging pre-trained model priors for self-evolution. TTRL significantly improves performance across diverse tasks and models, boosting pass@1 performance on the AIME 2024 by approximately 159% with only unlabeled test data. Despite being supervised by the Maj@N metric alone, TTRL consistently surpasses the initial model's upper limits and nears the performance of models trained directly on labeled test data. Experimental results validate TTRL's effectiveness across various tasks, indicating its potential for broader applications and domains. <br /><br /> <div>
arXiv:2504.16084v1 Announce Type: new 
Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-CoDE: Medical Critique based Disagreement Evaluation Framework</title>
<link>https://arxiv.org/abs/2504.15330</link>
<guid>https://arxiv.org/abs/2504.15330</guid>
<content:encoded><![CDATA[
<div> evaluation framework, medical language models, reliability, accuracy, Med-CoDE <br />
<br />
Summary: 
The article introduces Med-CoDE, an evaluation framework designed for assessing the performance of large language models (LLMs) in medical contexts. It addresses concerns regarding reliability and accuracy by utilizing a critique-based approach to measure the agreement between model-generated responses and established medical truths. Med-CoDE aims to provide a comprehensive evaluation of medical LLMs by capturing both accuracy and reliability. Through experimental demonstrations, the framework showcases its practicality in evaluating the quality and trustworthiness of medical language models. <div>
arXiv:2504.15330v1 Announce Type: cross 
Abstract: The emergence of large language models (LLMs) has significantly influenced numerous fields, including healthcare, by enhancing the capabilities of automated systems to process and generate human-like text. However, despite their advancements, the reliability and accuracy of LLMs in medical contexts remain critical concerns. Current evaluation methods often lack robustness and fail to provide a comprehensive assessment of LLM performance, leading to potential risks in clinical settings. In this work, we propose Med-CoDE, a specifically designed evaluation framework for medical LLMs to address these challenges. The framework leverages a critique-based approach to quantitatively measure the degree of disagreement between model-generated responses and established medical ground truths. This framework captures both accuracy and reliability in medical settings. The proposed evaluation framework aims to fill the existing gap in LLM assessment by offering a systematic method to evaluate the quality and trustworthiness of medical LLMs. Through extensive experiments and case studies, we illustrate the practicality of our framework in providing a comprehensive and reliable evaluation of medical LLMs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception</title>
<link>https://arxiv.org/abs/2504.15362</link>
<guid>https://arxiv.org/abs/2504.15362</guid>
<content:encoded><![CDATA[
<div> dataset, reasoning models, perceptual tasks, synthetic, visual reasoning

Summary:
The paper introduces the LongPerceptualThoughts dataset, containing 30K long-thought traces for perceptual tasks. A new data synthesis framework is proposed to generate elaborate reasoning thoughts for perceptual tasks, involving multiple-choice question synthesis, CoT extraction, and frontier reasoning model usage. Controlled experiments with a 7B model show significant improvements over existing visual reasoning data-generation methods, with an average +3.4 point improvement on vision-centric benchmarks and a +2 point improvement on text reasoning benchmarks. Notably, the model trained on the dataset also enhances performance on the MMLU-Pro benchmark. <div>
arXiv:2504.15362v1 Announce Type: cross 
Abstract: Recent reasoning models through test-time scaling have demonstrated that long chain-of-thoughts can unlock substantial performance boosts in hard reasoning tasks such as math and code. However, the benefit of such long thoughts for system-2 reasoning is relatively less explored in other domains such as perceptual tasks where shallower, system-1 reasoning seems sufficient. In this paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K long-thought traces for perceptual tasks. The key challenges in synthesizing elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models are not yet equipped with such thinking behavior and that it is not straightforward to build a reliable process verifier for perceptual tasks. Thus, we propose a novel three-stage data synthesis framework that first synthesizes verifiable multiple-choice questions from dense image descriptions, then extracts simple CoTs from VLMs for those verifiable problems, and finally expands those simple thoughts to elaborate long thoughts via frontier reasoning models. In controlled experiments with a strong instruction-tuned 7B model, we demonstrate notable improvements over existing visual reasoning data-generation methods. Our model, trained on the generated dataset, achieves an average +3.4 points improvement over 5 vision-centric benchmarks, including +11.8 points on V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves performance on the text reasoning benchmark, MMLU-Pro, by +2 points.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Camera Motions in Any Video</title>
<link>https://arxiv.org/abs/2504.15376</link>
<guid>https://arxiv.org/abs/2504.15376</guid>
<content:encoded><![CDATA[
<div> camera motion, CameraBench dataset, taxonomy, Structure-from-Motion, Video-Language Models <br />
Summary: <br />
The article introduces the CameraBench dataset, a large-scale dataset for assessing camera motion understanding. It includes ~3,000 internet videos annotated by experts with a taxonomy of camera motion primitives. A human study shows that training improves annotation performance, distinguishing between intrinsic and extrinsic motions. Evaluations on Structure-from-Motion (SfM) and Video-Language Models (VLMs) reveal strengths and weaknesses in capturing semantic and geometric primitives. Fine-tuning a VLM on CameraBench enhances performance and enables applications in motion-augmented captioning, video question answering, and video-text retrieval. The goal is to advance research in understanding camera motions in videos. <br /> <div>
arXiv:2504.15376v1 Announce Type: cross 
Abstract: We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like "follow" (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.15415</link>
<guid>https://arxiv.org/abs/2504.15415</guid>
<content:encoded><![CDATA[
<div> image-grounded video perception reasoning benchmark evaluation model performance data analysis Keywords: MLLMs, IV-Bench, video perception, reasoning tasks, image context

Summary:
The article introduces IV-Bench, a benchmark for evaluating Image-Grounded Video Perception and Reasoning. The benchmark includes 967 videos with annotated image-text queries for 13 tasks across 5 categories. State-of-the-art MLLMs exhibit low performance on the benchmark, indicating the need for improvement in image-grounded video comprehension. Factors such as inference patterns, frame number, and resolution impact model performance. Challenges in IV-Bench extend beyond data alignment during training. The study provides valuable insights for future research in this area. The codes and data are available on GitHub for reference. <div>
arXiv:2504.15415v1 Announce Type: cross 
Abstract: Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in https://github.com/multimodal-art-projection/IV-Bench.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data</title>
<link>https://arxiv.org/abs/2504.15448</link>
<guid>https://arxiv.org/abs/2504.15448</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, corporate reputation, social media, NLP, machine learning

Summary: 
The paper presents a sentiment analysis system for monitoring corporate reputation using NLP and machine learning. It combines rule-based models and deep learning techniques for interpreting public opinion in real time. The system preprocesses social media data, classifies sentiment with an ensemble approach, and visualizes results for better interpretability. Significant disparities in public sentiment across major corporations are revealed, with companies like Amazon and Samsung receiving high scores, while Microsoft and Walmart show poor sentiment profiles. The analysis demonstrates the system's utility in providing actionable insights for stakeholders to make informed strategic decisions based on comprehensive sentiment analysis. <br /><br />Summary: <div>
arXiv:2504.15448v1 Announce Type: cross 
Abstract: In the age of social media, understanding public sentiment toward major corporations is crucial for investors, policymakers, and researchers. This paper presents a comprehensive sentiment analysis system tailored for corporate reputation monitoring, combining Natural Language Processing (NLP) and machine learning techniques to accurately interpret public opinion in real time. The methodology integrates a hybrid sentiment detection framework leveraging both rule-based models (VADER) and transformer-based deep learning models (DistilBERT), applied to social media data from multiple platforms. The system begins with robust preprocessing involving noise removal and text normalization, followed by sentiment classification using an ensemble approach to ensure both interpretability and contextual accuracy. Results are visualized through sentiment distribution plots, comparative analyses, and temporal sentiment trends for enhanced interpretability. Our analysis reveals significant disparities in public sentiment across major corporations, with companies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment scores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment profiles. These findings demonstrate the utility of our multi-source sentiment framework in providing actionable insights regarding corporate public perception, enabling stakeholders to make informed strategic decisions based on comprehensive sentiment analysis.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Adaptive Parallel Reasoning with Language Models</title>
<link>https://arxiv.org/abs/2504.15466</link>
<guid>https://arxiv.org/abs/2504.15466</guid>
<content:encoded><![CDATA[
<div> Keywords: scaling, language models, inference-time computation, reasoning framework, reinforcement learning <br />
<br />
Summary: 
The proposed Adaptive Parallel Reasoning (APR) framework aims to improve the reasoning capabilities of language models by combining serialized and parallel computations. By using adaptive multi-threaded inference with spawn() and join() operations, APR optimizes task success rates without predefined reasoning structures. In experiments on the Countdown reasoning task, APR showed higher performance within the same context window, superior scalability with increased computation, and improved accuracy at equivalent latency. This framework represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation. <div>
arXiv:2504.15466v1 Announce Type: cross 
Abstract: Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting</title>
<link>https://arxiv.org/abs/2504.15485</link>
<guid>https://arxiv.org/abs/2504.15485</guid>
<content:encoded><![CDATA[
<div> Keywords: occluded objects, visual patterns, reasoning, vision-language models, spatial understanding <br />
Summary: <br />
The article introduces the CAPTURe task, which challenges vision-language models to count objects in patterns while inferring behind occluders. The task aims to evaluate models' skills in recognizing visual patterns and reasoning about occluded objects, essential for spatial understanding in real-world environments. Four VLMs were tested on CAPTURe, showing difficulties in counting both occluded and unoccluded patterns, with a notable decrease in performance with occlusion. Humans performed significantly better on the task, highlighting the models' deficiencies in handling occlusion and counting in images. Providing auxiliary information on occluded object locations improved model performance, indicating that errors stemmed from both handling occlusion and counting difficulties. Ultimately, the study suggests that current VLMs, including strong models like GPT-4o, struggle with occluded object reasoning and spatial relationships. <br /> <div>
arXiv:2504.15485v1 Announce Type: cross 
Abstract: Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models' ability to reason about multiple occluded objects, we introduce a novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which requires a model to count objects arranged in a pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURe requires both recognizing visual patterns and reasoning, making it a useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURe also tests VLMs' ability to form world models that would allow them to fill in missing information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually filtered images of real objects in patterns and (2) CAPTURe-synthetic, a controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURe. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty counting in images.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, safety, lifecycle, comprehensive, literature review

Summary: 
The paper introduces the concept of "full-stack" safety for Large Language Models (LLMs) to address concerns regarding security and safety implications throughout the entire lifecycle of LLMs. The comprehensive perspective of the survey covers data preparation, pre-training, post-training, deployment, and commercialization stages. It is supported by an extensive literature review of over 800 papers, ensuring thorough coverage and systematic organization of security issues. The unique insights gained from the analysis include identifying promising research directions such as safety in data generation, alignment techniques, model editing, and LLM-based agent systems. The paper provides valuable guidance for researchers interested in pursuing future work in the field of LLM safety.<br /><br />Summary: <div>
arXiv:2504.15585v1 Announce Type: cross 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction</title>
<link>https://arxiv.org/abs/2504.15629</link>
<guid>https://arxiv.org/abs/2504.15629</guid>
<content:encoded><![CDATA[
<div> inference time, citation accuracy, retrieval augmented generation, post-processing algorithms, LLMs <br />
<br />Summary: 
The article introduces post-processing algorithms to enhance citation accuracy in Large Language Models (LLMs) in Retrieval Augmented Generation (RAG) systems. Current LLMs have a 74% citation accuracy rate, prompting the development of efficient methods to improve this. The proposed approaches include cross-checking citations with retrieved articles using keyword + semantic matching, fine-tuning with BERTScore, and a lightweight LLM-based technique. Experimental results show a 15.46% relative improvement in citation accuracy, allowing for a shift to smaller, more cost-effective models with faster inference times. This enhancement is crucial for AI-generated content reliability in information retrieval and summarization tasks, particularly in commercial products where customer trust is paramount. <div>
arXiv:2504.15629v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has emerged as a powerful application of Large Language Models (LLMs), revolutionizing information search and consumption. RAG systems combine traditional search capabilities with LLMs to generate comprehensive answers to user queries, ideally with accurate citations. However, in our experience of developing a RAG product, LLMs often struggle with source attribution, aligning with other industry studies reporting citation accuracy rates of only about 74% for popular generative search engines. To address this, we present efficient post-processing algorithms to improve citation accuracy in LLM-generated responses, with minimal impact on latency and cost. Our approaches cross-check generated citations against retrieved articles using methods including keyword + semantic matching, fine tuned model with BERTScore, and a lightweight LLM-based technique. Our experimental results demonstrate a relative improvement of 15.46% in the overall accuracy metrics of our RAG system. This significant enhancement potentially enables a shift from our current larger language model to a relatively smaller model that is approximately 12x more cost-effective and 3x faster in inference time, while maintaining comparable performance. This research contributes to enhancing the reliability and trustworthiness of AI-generated content in information retrieval and summarization tasks which is critical to gain customer trust especially in commercial products.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation</title>
<link>https://arxiv.org/abs/2504.15659</link>
<guid>https://arxiv.org/abs/2504.15659</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Electronic Design Automation, RTL code generation, functional validation, VERICODER

Summary:
Recent advancements in Large Language Models (LLMs) have spurred interest in employing them for Electronic Design Automation (EDA) tasks, particularly in RTL code generation. However, most existing RTL datasets prioritize syntax over functional validation, potentially leading to inaccuracies in the generated code. To address this issue, the authors introduce VERICODER, a model fine-tuned on a dataset validated for functional correctness. This dataset, created using a unique approach that combines unit test generation with iterative refinement, ensures that every example includes a natural language description, an RTL implementation, and passing tests. VERICODER, trained on this functionally validated dataset, demonstrates significant performance improvements in functional correctness metrics on established benchmarks. An ablation study confirms the superiority of models trained on functionally validated datasets, highlighting the critical role of high-quality datasets in enhancing RTL code generation.<br /><br />Summary: <div>
arXiv:2504.15659v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have sparked growing interest in applying them to Electronic Design Automation (EDA) tasks, particularly Register Transfer Level (RTL) code generation. While several RTL datasets have been introduced, most focus on syntactic validity rather than functional validation with tests, leading to training examples that compile but may not implement the intended behavior. We present VERICODER, a model for RTL code generation fine-tuned on a dataset validated for functional correctness. This fine-tuning dataset is constructed using a novel methodology that combines unit test generation with feedback-directed refinement. Given a natural language specification and an initial RTL design, we prompt a teacher model (GPT-4o-mini) to generate unit tests and iteratively revise the RTL design based on its simulation results using the generated tests. If necessary, the teacher model also updates the tests to ensure they comply with the natural language specification. As a result of this process, every example in our dataset is functionally validated, consisting of a natural language description, an RTL implementation, and passing tests. Fine-tuned on this dataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics in functional correctness on VerilogEval and RTLLM, with relative gains of up to 71.7% and 27.4% respectively. An ablation study further shows that models trained on our functionally validated dataset outperform those trained on functionally non-validated datasets, underscoring the importance of high-quality datasets in RTL code generation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving</title>
<link>https://arxiv.org/abs/2504.15780</link>
<guid>https://arxiv.org/abs/2504.15780</guid>
<content:encoded><![CDATA[
arXiv:2504.15780v1 Announce Type: cross 
Abstract: Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Private is Your Attention? Bridging Privacy with In-Context Learning</title>
<link>https://arxiv.org/abs/2504.16000</link>
<guid>https://arxiv.org/abs/2504.16000</guid>
<content:encoded><![CDATA[
arXiv:2504.16000v1 Announce Type: cross 
Abstract: In-context learning (ICL)-the ability of transformer-based models to perform new tasks from examples provided at inference time-has emerged as a hallmark of modern language models. While recent works have investigated the mechanisms underlying ICL, its feasibility under formal privacy constraints remains largely unexplored. In this paper, we propose a differentially private pretraining algorithm for linear attention heads and present the first theoretical analysis of the privacy-accuracy trade-off for ICL in linear regression. Our results characterize the fundamental tension between optimization and privacy-induced noise, formally capturing behaviors observed in private training via iterative methods. Additionally, we show that our method is robust to adversarial perturbations of training prompts, unlike standard ridge regression. All theoretical findings are supported by extensive simulations across diverse settings.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of Video Diffusion Models: Foundations, Implementations, and Applications</title>
<link>https://arxiv.org/abs/2504.16081</link>
<guid>https://arxiv.org/abs/2504.16081</guid>
<content:encoded><![CDATA[
arXiv:2504.16081v1 Announce Type: cross 
Abstract: Recent advances in diffusion models have revolutionized video generation, offering superior temporal consistency and visual quality compared to traditional generative adversarial networks-based approaches. While this emerging field shows tremendous promise in applications, it faces significant challenges in motion consistency, computational efficiency, and ethical considerations. This survey provides a comprehensive review of diffusion-based video generation, examining its evolution, technical foundations, and practical applications. We present a systematic taxonomy of current methodologies, analyze architectural innovations and optimization strategies, and investigate applications across low-level vision tasks such as denoising and super-resolution. Additionally, we explore the synergies between diffusionbased video generation and related domains, including video representation learning, question answering, and retrieval. Compared to the existing surveys (Lei et al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which focus on specific aspects of video generation, such as human video synthesis (Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our work provides a broader, more updated, and more fine-grained perspective on diffusion-based approaches with a special section for evaluation metrics, industry solutions, and training engineering techniques in video generation. This survey serves as a foundational resource for researchers and practitioners working at the intersection of diffusion models and video generation, providing insights into both the theoretical frameworks and practical implementations that drive this rapidly evolving field. A structured list of related works involved in this survey is also available on https://github.com/Eyeline-Research/Survey-Video-Diffusion.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregating Soft Labels from Crowd Annotations Improves Uncertainty Estimation Under Distribution Shift</title>
<link>https://arxiv.org/abs/2212.09409</link>
<guid>https://arxiv.org/abs/2212.09409</guid>
<content:encoded><![CDATA[
arXiv:2212.09409v3 Announce Type: replace 
Abstract: Selecting an effective training signal for machine learning tasks is difficult: expert annotations are expensive, and crowd-sourced annotations may not be reliable. Recent work has demonstrated that learning from a distribution over labels acquired from crowd annotations can be effective both for performance and uncertainty estimation. However, this has mainly been studied using a limited set of soft-labeling methods in an in-domain setting. Additionally, no one method has been shown to consistently perform well across tasks, making it difficult to know a priori which to choose. To fill these gaps, this paper provides the first large-scale empirical study on learning from crowd labels in the out-of-domain setting, systematically analyzing 8 soft-labeling methods on 4 language and vision tasks. Additionally, we propose to aggregate soft-labels via a simple average in order to achieve consistent performance across tasks. We demonstrate that this yields classifiers with improved predictive uncertainty estimation in most settings while maintaining consistent raw performance compared to learning from individual soft-labeling methods or taking a majority vote of the annotations. We additionally highlight that in regimes with abundant or minimal training data, the selection of soft labeling method is less important, while for highly subjective labels and moderate amounts of training data, aggregation yields significant improvements in uncertainty estimation over individual methods. Code can be found at https://github.com/copenlu/aggregating-crowd-annotations-ood.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Low-Rank Parametrization of Reward Models for Controlled Language Generation</title>
<link>https://arxiv.org/abs/2407.04615</link>
<guid>https://arxiv.org/abs/2407.04615</guid>
<content:encoded><![CDATA[
arXiv:2407.04615v3 Announce Type: replace 
Abstract: Language models trained on large amounts of data are known to produce inappropriate content in some cases and require careful tuning to be used in the real world. We revisit an effective and modular approach for controllability of the language models, when an external expert model guides the decoding. Particularly, we zoom in into the parametrization choice of an external expert, highlighting the difference between low-rank and higher-rank parametrizations. Higher-rank experts are designed to support high flexibility when representing the rewards, leading to higher computational costs during decoding. However, we demonstrate that they might not use their full flexibility. By analyzing the recently proposed reward-augmented decoding approach (RAD), which uses a higher-rank expert model, we introduce a simpler but more efficient low-rank parametrization of the expert model enabling fast and effective guided decoding. We empirically show that the low-rank RAD performs on par with the more flexible RAD on a detoxification and a sentiment control task, while requiring only a single reward model call per generated token.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation</title>
<link>https://arxiv.org/abs/2408.06276</link>
<guid>https://arxiv.org/abs/2408.06276</guid>
<content:encoded><![CDATA[
arXiv:2408.06276v4 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks, generating significant interest in their application to recommendation systems. However, existing methods have not fully capitalized on the potential of LLMs, often constrained by limited input information or failing to fully utilize their advanced reasoning capabilities. To address these limitations, we introduce EXP3RT, a novel LLM-based recommender designed to leverage rich preference information contained in user and item reviews. EXP3RT is basically fine-tuned through distillation from a teacher LLM to perform three key tasks in order: EXP3RT first extracts and encapsulates essential subjective preferences from raw reviews, aggregates and summarizes them according to specific criteria to create user and item profiles. It then generates detailed step-by-step reasoning followed by predicted rating, i.e., reasoning-enhanced rating prediction, by considering both subjective and objective information from user/item profiles and item descriptions. This personalized preference reasoning from EXP3RT enhances rating prediction accuracy and also provides faithful and reasonable explanations for recommendation. Extensive experiments show that EXP3RT outperforms existing methods on both rating prediction and candidate item reranking for top-k recommendation, while significantly enhancing the explainability of recommendation systems.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-World Evaluation for Retrieving Diverse Perspectives</title>
<link>https://arxiv.org/abs/2409.18110</link>
<guid>https://arxiv.org/abs/2409.18110</guid>
<content:encoded><![CDATA[
arXiv:2409.18110v2 Announce Type: replace 
Abstract: We study retrieving a set of documents that covers various perspectives on a complex and contentious question (e.g., will ChatGPT do more harm than good?). We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS), where each example consists of a question and diverse perspectives associated with the question, sourced from survey questions and debate websites. On this data, retrievers paired with a corpus are evaluated to surface a document set that contains diverse perspectives. Our framing diverges from most retrieval tasks in that document relevancy cannot be decided by simple string matches to references. Instead, we build a language model-based automatic evaluator that decides whether each retrieved document contains a perspective. This allows us to evaluate the performance of three different types of corpus (Wikipedia, web snapshot, and corpus constructed on the fly with retrieved pages from the search engine) paired with retrievers. Retrieving diverse documents remains challenging, with the outputs from existing retrievers covering all perspectives on only 40% of the examples. We further study the effectiveness of query expansion and diversity-focused reranking approaches and analyze retriever sycophancy.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models</title>
<link>https://arxiv.org/abs/2410.02355</link>
<guid>https://arxiv.org/abs/2410.02355</guid>
<content:encoded><![CDATA[
arXiv:2410.02355v4 Announce Type: replace 
Abstract: Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios. To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.7% with a single line of additional code for projection solely. Our code is available at: https://github.com/jianghoucheng/AlphaEdit.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models</title>
<link>https://arxiv.org/abs/2410.19503</link>
<guid>https://arxiv.org/abs/2410.19503</guid>
<content:encoded><![CDATA[
arXiv:2410.19503v2 Announce Type: replace 
Abstract: Despite the success of Large Language Models (LLMs), they still face challenges related to high inference costs and memory requirements. To address these issues, Knowledge Distillation (KD) has emerged as a popular method for model compression, with student-generated outputs (SGOs) as training data being particularly notable for reducing the mismatch between training and inference. However, SGOs often produce noisy and biased sequences, which can lead to misguidance from the teacher model, especially in long sequences. To mitigate these challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel approach that strategically incorporates the teacher model during the student's sequence generation. SWITCH identifies discrepancies between the token probabilities of the teacher and student models, allowing the teacher to intervene selectively, particularly in long sequences that are more prone to teacher misguidance. Extensive experimental results across three model families and five instruction-following datasets show that SWITCH surpasses traditional KD methods, particularly excelling in the generation of long sequential data.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity Helps Jailbreak Large Language Models</title>
<link>https://arxiv.org/abs/2411.04223</link>
<guid>https://arxiv.org/abs/2411.04223</guid>
<content:encoded><![CDATA[
arXiv:2411.04223v2 Announce Type: replace 
Abstract: We have uncovered a powerful jailbreak technique that leverages large language models' ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries. This revelation exposes a critical flaw in current LLM safety training, suggesting that existing methods may merely mask vulnerabilities rather than eliminate them. Our findings sound an urgent alarm for the need to revolutionize testing methodologies to ensure robust and reliable LLM security.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree</title>
<link>https://arxiv.org/abs/2412.12639</link>
<guid>https://arxiv.org/abs/2412.12639</guid>
<content:encoded><![CDATA[
arXiv:2412.12639v3 Announce Type: replace 
Abstract: Striking an optimal balance between minimal drafting latency and high speculation accuracy to enhance the inference speed of Large Language Models remains a significant challenge in speculative decoding. In this paper, we introduce Falcon, an innovative semi-autoregressive speculative decoding framework fashioned to augment both the drafter's parallelism and output quality. Falcon incorporates the Coupled Sequential Glancing Distillation technique, which fortifies inter-token dependencies within the same block, leading to increased speculation accuracy. We offer a comprehensive theoretical analysis to illuminate the underlying mechanisms. Additionally, we introduce a Custom-Designed Decoding Tree, which permits the drafter to generate multiple tokens in a single forward pass and accommodates multiple forward passes as needed, thereby boosting the number of drafted tokens and significantly improving the overall acceptance rate. Comprehensive evaluations on benchmark datasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior acceleration capabilities. The framework achieves a lossless speedup ratio ranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model series. These results outstrip existing speculative decoding methods for LLMs, including Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact drafter architecture equivalent to merely two Transformer layers.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Whisper on Low-Resource Languages for Real-World Applications</title>
<link>https://arxiv.org/abs/2412.15726</link>
<guid>https://arxiv.org/abs/2412.15726</guid>
<content:encoded><![CDATA[
arXiv:2412.15726v3 Announce Type: replace 
Abstract: This paper presents a new approach to fine-tuning OpenAI's Whisper model for low-resource languages by introducing a novel data generation method that converts sentence-level data into a long-form corpus, using Swiss German as a case study. Non-sentence-level data, which could improve the performance of long-form audio, is difficult to obtain and often restricted by copyright laws. Our method bridges this gap by transforming more accessible sentence-level data into a format that preserves the model's ability to handle long-form audio and perform segmentation without requiring non-sentence-level data. Our data generation process improves performance in several real-world applications and leads to the development of a new state-of-the-art speech-to-text (STT) model for Swiss German. We compare our model with a non-fine-tuned Whisper and our previous state-of-the-art Swiss German STT models, where our new model achieves higher BLEU scores. Our results also indicate that the proposed method is adaptable to other low-resource languages, supported by written guidance and code that allows the creation of fine-tuned Whisper models, which keep segmentation capabilities and allow the transcription of longer audio files using only sentence-level data with high quality.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments by Humans and LLMs</title>
<link>https://arxiv.org/abs/2412.15993</link>
<guid>https://arxiv.org/abs/2412.15993</guid>
<content:encoded><![CDATA[
arXiv:2412.15993v2 Announce Type: replace 
Abstract: Arguments evoke emotions, influencing the effect of the argument itself. Not only the emotional intensity but also the category influence the argument's effects, for instance, the willingness to adapt stances. While binary emotionality has been studied in arguments, there is no work on discrete emotion categories (e.g., "Anger") in such data. To fill this gap, we crowdsource subjective annotations of emotion categories in a German argument corpus and evaluate automatic LLM-based labeling methods. Specifically, we compare three prompting strategies (zero-shot, one-shot, chain-of-thought) on three large instruction-tuned language models (Falcon-7b-instruct, Llama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the output space to be binary (is there emotionality in the argument?), closed-domain (which emotion from a given label set is in the argument?), or open-domain (which emotion is in the argument?). We find that emotion categories enhance the prediction of emotionality in arguments, emphasizing the need for discrete emotion annotations in arguments. Across all prompt settings and models, automatic predictions show a high recall but low precision for predicting anger and fear, indicating a strong bias toward negative emotions.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks</title>
<link>https://arxiv.org/abs/2502.13053</link>
<guid>https://arxiv.org/abs/2502.13053</guid>
<content:encoded><![CDATA[
arXiv:2502.13053v2 Announce Type: replace 
Abstract: As researchers continue to optimize AI agents for more effective task execution within operating systems, they often overlook a critical security concern: the ability of these agents to detect "impostors" within their environment. Through an analysis of the agents' operational context, we identify a significant threat-attackers can disguise malicious attacks as environmental elements, injecting active disturbances into the agents' execution processes to manipulate their decision-making. We define this novel threat as the Active Environment Injection Attack (AEIA). Focusing on the interaction mechanisms of the Android OS, we conduct a risk assessment of AEIA and identify two critical security vulnerabilities: (1) Adversarial content injection in multimodal interaction interfaces, where attackers embed adversarial instructions within environmental elements to mislead agent decision-making; and (2) Reasoning gap vulnerabilities in the agent's task execution process, which increase susceptibility to AEIA attacks during reasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN, an attack scheme that exploits interaction vulnerabilities in mobile operating systems to assess the robustness of MLLM-based agents. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% on the AndroidWorld benchmark by combining two vulnerabilities.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2503.04797</link>
<guid>https://arxiv.org/abs/2503.04797</guid>
<content:encoded><![CDATA[
arXiv:2503.04797v2 Announce Type: replace 
Abstract: Parallel corpora play an important role in training machine translation (MT) models, particularly for low-resource languages where high-quality bilingual data is scarce. This review provides a comprehensive overview of available parallel corpora for Indic languages, which span diverse linguistic families, scripts, and regional variations. We categorize these corpora into text-to-text, code-switched, and various categories of multimodal datasets, highlighting their significance in the development of robust multilingual MT systems. Beyond resource enumeration, we critically examine the challenges faced in corpus creation, including linguistic diversity, script variation, data scarcity, and the prevalence of informal textual content.We also discuss and evaluate these corpora in various terms such as alignment quality and domain representativeness. Furthermore, we address open challenges such as data imbalance across Indic languages, the trade-off between quality and quantity, and the impact of noisy, informal, and dialectal data on MT performance. Finally, we outline future directions, including leveraging cross-lingual transfer learning, expanding multilingual datasets, and integrating multimodal resources to enhance translation quality. To the best of our knowledge, this paper presents the first comprehensive review of parallel corpora specifically tailored for low-resource Indic languages in the context of machine translation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2503.09572</link>
<guid>https://arxiv.org/abs/2503.09572</guid>
<content:encoded><![CDATA[
arXiv:2503.09572v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as a text-only state-of-the-art 81.36% success rate on WebVoyager.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques</title>
<link>https://arxiv.org/abs/2503.11816</link>
<guid>https://arxiv.org/abs/2503.11816</guid>
<content:encoded><![CDATA[
arXiv:2503.11816v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages</title>
<link>https://arxiv.org/abs/2504.00021</link>
<guid>https://arxiv.org/abs/2504.00021</guid>
<content:encoded><![CDATA[
arXiv:2504.00021v2 Announce Type: replace 
Abstract: This paper presents the winning submission of the RaaVa team to the AmericasNLP 2025 Shared Task 3 on Automatic Evaluation Metrics for Machine Translation (MT) into Indigenous Languages of America, where our system ranked first overall based on average Pearson correlation with the human annotations. We introduce Feature-Union Scorer (FUSE) for Evaluation, FUSE integrates Ridge regression and Gradient Boosting to model translation quality. In addition to FUSE, we explore five alternative approaches leveraging different combinations of linguistic similarity features and learning paradigms. FUSE Score highlights the effectiveness of combining lexical, phonetic, semantic, and fuzzy token similarity with learning-based modeling to improve MT evaluation for morphologically rich and low-resource languages. MT into Indigenous languages poses unique challenges due to polysynthesis, complex morphology, and non-standardized orthography. Conventional automatic metrics such as BLEU, TER, and ChrF often fail to capture deeper aspects like semantic adequacy and fluency. Our proposed framework, formerly referred to as FUSE, incorporates multilingual sentence embeddings and phonological encodings to better align with human evaluation. We train supervised models on human-annotated development sets and evaluate held-out test data. Results show that FUSE consistently achieves higher Pearson and Spearman correlations with human judgments, offering a robust and linguistically informed solution for MT evaluation in low-resource settings.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2504.01801</link>
<guid>https://arxiv.org/abs/2504.01801</guid>
<content:encoded><![CDATA[
arXiv:2504.01801v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities despite the extreme language imbalance in the pre-training data. In this paper, we closely examine the reasons behind this phenomenon, focusing on the pre-training corpus. We find that the existence of code-switching, alternating between different languages within a context, is key to multilingual capabilities. We conduct an analysis to investigate code-switching in the pre-training corpus, examining its presence and categorizing it into four types within two quadrants. We then assess its impact on multilingual performance. These types of code-switching data are unbalanced in proportions and demonstrate different effects on facilitating language transfer. To better explore the power of code-switching for language alignment during pre-training, we investigate the strategy of synthetic code-switching. We continuously scale up the synthetic code-switching data and observe remarkable improvements in both benchmarks and representation space. Extensive experiments indicate that incorporating synthetic code-switching data enables better language alignment and generalizes well to high, medium, and low-resource languages with pre-training corpora of varying qualities.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation</title>
<link>https://arxiv.org/abs/2504.04060</link>
<guid>https://arxiv.org/abs/2504.04060</guid>
<content:encoded><![CDATA[
arXiv:2504.04060v2 Announce Type: replace 
Abstract: Speech large language models (LLMs) have emerged as a prominent research focus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series of high-performance, low-latency speech LLMs enabled by a scalable and model-agnostic training framework designed for real-time voice interaction. Central to our contribution is the first application of multi-token prediction (MTP) to speech LLMs. This approach represents a paradigm shift from standard next-token prediction (NTP), offering simultaneous improvements in generation speed and quality. Informed by analysis of MTP's effect on speech generation and experimental comparisons, we designed a straightforward and highly effective MTP implementation. Experiments demonstrate that VocalNet performs on par with mainstream Omni LLMs even with limited training data, and significantly surpasses existing open-source speech LLMs. To foster reproducibility and community advancement, all model weights, inference code, training data, and framework implementations have been made publicly available at https://github.com/SJTU-OmniAgent/VocalNet
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance</title>
<link>https://arxiv.org/abs/2504.07989</link>
<guid>https://arxiv.org/abs/2504.07989</guid>
<content:encoded><![CDATA[
arXiv:2504.07989v2 Announce Type: replace 
Abstract: Small Language Models (SLMs) offer efficient alternatives to LLMs for specific domains. The 2023 TinyStories study developed an English dataset that allows SLMs with 1 to 10 million parameters to produce coherent outputs. Our research expands this framework by translating the original dataset into Indian languages and creating synthetic data using LLMs. We focus on Hindi, Marathi, and Bengali, evaluating SLMs for regional language processing and understanding linguistic complexity. We show that SLMs efficiently process regional languages with significantly fewer parameters than LLMs, providing a complementary framework for ``inference based evaluation" of tokenization strategies and linguistic complexity. Our analysis shows that language-specific tokenizers outperform general-purpose ones for Indian languages. Empirical validations, supported by information-theoretic and morphological analyses, provides fundamental understanding behind the better performance of Hindi models over Marathi and Bengali. Additionally, we show that synthetic datasets outperform translated content for training SLMs. Correlation analyses reveal cross-linguistic patterns and language-specific relationships between creativity, grammatical precision, and narrative completeness. These findings advance both the practical application of SLMs to underserved languages and our theoretical understanding of neural language development.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol</title>
<link>https://arxiv.org/abs/2504.10284</link>
<guid>https://arxiv.org/abs/2504.10284</guid>
<content:encoded><![CDATA[
arXiv:2504.10284v2 Announce Type: replace 
Abstract: Literature review tables are essential for summarizing and comparing collections of scientific papers. We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers. Building on recent work (Newman et al., 2024), we extend prior approaches to address real-world complexities through a combination of LLM-based methods and human annotations. Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers). To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios. Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements. Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certifying Knowledge Comprehension in LLMs</title>
<link>https://arxiv.org/abs/2402.15929</link>
<guid>https://arxiv.org/abs/2402.15929</guid>
<content:encoded><![CDATA[
arXiv:2402.15929v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical systems where they provide answers based on in-context information derived from knowledge bases. As LLMs are increasingly envisioned as superhuman agents, their proficiency in knowledge comprehension-extracting relevant information and reasoning over it to answer questions, a key facet of human intelligence-becomes crucial. However, existing evaluations of LLMs on knowledge comprehension are typically conducted on small test sets, but these datasets represent only a tiny fraction of the vast number of possible queries. Simple empirical evaluations on these limited test sets raises concerns about the reliability and generalizability of the results. In this work, we introduce the first specification and certification framework for knowledge comprehension in LLMs, providing formal probabilistic guarantees for reliability. Instead of a fixed dataset, we design novel specifications that mathematically represent prohibitively large probability distributions of knowledge comprehension prompts with natural noise, using knowledge graphs. From these specifications, we generate quantitative certificates that offer high-confidence, tight bounds on the probability that a given LLM correctly answers any question drawn from the specification distribution. We apply our framework to certify SOTA LLMs in two domains: precision medicine and general question-answering. Our results reveal previously unrecognized vulnerabilities in SOTA LLMs due to natural noise in the prompts. Additionally, we establish performance hierarchies with formal guarantees among the SOTA LLMs, particularly in the context of precision medicine question-answering.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing RLHF Training for Large Language Models with Stage Fusion</title>
<link>https://arxiv.org/abs/2409.13221</link>
<guid>https://arxiv.org/abs/2409.13221</guid>
<content:encoded><![CDATA[
arXiv:2409.13221v3 Announce Type: replace-cross 
Abstract: We present RLHFuse, an efficient training system with stage fusion for Reinforcement Learning from Human Feedback (RLHF). Due to the intrinsic nature of RLHF training, i.e., the data skewness in the generation stage and the pipeline bubbles in the training stage, existing RLHF systems suffer from low GPU utilization. RLHFuse breaks the traditional view of RLHF workflow as a composition of individual tasks, splitting each task into finer-grained subtasks, and performing stage fusion to improve GPU utilization. RLHFuse contains two key ideas. First, for generation and inference tasks, RLHFuse splits them into sample-level subtasks, enabling efficient inter-stage fusion to overlap the execution of generation and inference stages, thus mitigating the original generation bottleneck dominated by long-tailed samples. Second, for training tasks, RLHFuse breaks them into subtasks of micro-batches and performs intra-stage fusion to concurrently execute these subtasks in the training stage with a fused pipeline schedule, effectively mitigating the pipeline bubbles. The experiments show that RLHFuse increases the training throughput by up to $3.7\times$, compared to existing systems.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct</title>
<link>https://arxiv.org/abs/2410.02064</link>
<guid>https://arxiv.org/abs/2410.02064</guid>
<content:encoded><![CDATA[
arXiv:2410.02064v3 Announce Type: replace-cross 
Abstract: It has been reported that LLMs can recognize their own writing. As this has potential implications for AI safety, yet is relatively understudied, we investigate the phenomenon, seeking to establish whether it robustly occurs at the behavioral level, how the observed behavior is achieved, and whether it can be controlled. First, we find that the Llama3-8b-Instruct chat model - but not the base Llama3-8b model - can reliably distinguish its own outputs from those of humans, and present evidence that the chat model is likely using its experience with its own outputs, acquired during post-training, to succeed at the writing recognition task. Second, we identify a vector in the residual stream of the model that is differentially activated when the model makes a correct self-written-text recognition judgment, show that the vector activates in response to information relevant to self-authorship, present evidence that the vector is related to the concept of "self" in the model, and demonstrate that the vector is causally related to the model's ability to perceive and assert self-authorship. Finally, we show that the vector can be used to control both the model's behavior and its perception, steering the model to claim or disclaim authorship by applying the vector to the model's output as it generates it, and steering the model to believe or disbelieve it wrote arbitrary texts by applying the vector to them as the model reads them.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement</title>
<link>https://arxiv.org/abs/2410.13828</link>
<guid>https://arxiv.org/abs/2410.13828</guid>
<content:encoded><![CDATA[
arXiv:2410.13828v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for language model (LM) alignment. At its core, RLHF uses a margin-based loss for preference optimization, specifying ideal LM behavior only by the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods -- the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which leads to two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability to the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product of the gradients of preferred and dispreferred log-probabilities is large relative to the individual gradient norms. We theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework extend to explaining important differences in the training dynamics of various preference optimization algorithms, and suggesting potential algorithm designs to mitigate the under-specification issue of margin-based methods and thereby improving language model alignment.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title>
<link>https://arxiv.org/abs/2410.14669</link>
<guid>https://arxiv.org/abs/2410.14669</guid>
<content:encoded><![CDATA[
arXiv:2410.14669v3 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments</title>
<link>https://arxiv.org/abs/2410.21131</link>
<guid>https://arxiv.org/abs/2410.21131</guid>
<content:encoded><![CDATA[
arXiv:2410.21131v3 Announce Type: replace-cross 
Abstract: As machine learning models evolve, maintaining transparency demands more human-centric explainable AI techniques. Counterfactual explanations, with roots in human reasoning, identify the minimal input changes needed to obtain a given output and, hence, are crucial for supporting decision-making. Despite their importance, the evaluation of these explanations often lacks grounding in user studies and remains fragmented, with existing metrics not fully capturing human perspectives. To address this challenge, we developed a diverse set of 30 counterfactual scenarios and collected ratings across 8 evaluation metrics from 206 respondents. Subsequently, we fine-tuned different Large Language Models (LLMs) to predict average or individual human judgment across these metrics. Our methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot evaluations and 85% (over a 3-classes prediction) with fine-tuning across all metrics. The fine-tuned models predicting human ratings offer better comparability and scalability in evaluating different counterfactual explanation frameworks.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Codenames as a Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2412.11373</link>
<guid>https://arxiv.org/abs/2412.11373</guid>
<content:encoded><![CDATA[
arXiv:2412.11373v2 Announce Type: replace-cross 
Abstract: In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities. Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches. LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles. We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark</title>
<link>https://arxiv.org/abs/2502.19676</link>
<guid>https://arxiv.org/abs/2502.19676</guid>
<content:encoded><![CDATA[
arXiv:2502.19676v3 Announce Type: replace-cross 
Abstract: Forecasting is an important task in many domains, such as technology and economics. However existing forecasting benchmarks largely lack comprehensive confidence assessment, focus on limited question types, and often consist of artificial questions that do not align with real-world human forecasting needs. To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and Confidence Assessment), a benchmark that evaluates models' ability to make predictions and their confidence in them. FOReCAst spans diverse forecasting scenarios involving Boolean questions, timeframe prediction, and quantity estimation, enabling a comprehensive evaluation of both prediction accuracy and confidence calibration for real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents</title>
<link>https://arxiv.org/abs/2504.09723</link>
<guid>https://arxiv.org/abs/2504.09723</guid>
<content:encoded><![CDATA[
arXiv:2504.09723v2 Announce Type: replace-cross 
Abstract: A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13914</link>
<guid>https://arxiv.org/abs/2504.13914</guid>
<content:encoded><![CDATA[
<div> Keywords: Seed-Thinking-v1.5, reasoning abilities, Mixture-of-Experts model, benchmarks, generalization

Summary:
Seed-Thinking-v1.5 is a new model that excels in reasoning tasks before responding, achieving impressive scores on benchmarks such as AIME 2024, Codeforces, and GPQA. The model's performance indicates strong reasoning abilities in STEM and coding fields. Additionally, Seed-Thinking-v1.5 demonstrates remarkable generalization across various domains, surpassing other models in win rates on non-reasoning tasks. It is a Mixture-of-Experts model with a compact size, featuring 20B activated and 200B total parameters. To evaluate its generalized reasoning capabilities, two internal benchmarks, BeyondAIME and Codeforces, have been developed and will be publicly released for future research. Seed-Thinking-v1.5's success in diverse tasks showcases its potential for broader applicability and sheds light on the advancement of reasoning models in artificial intelligence.
<br /><br />Summary: <div>
arXiv:2504.13914v1 Announce Type: new 
Abstract: We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before responding, resulting in improved performance on a wide range of benchmarks. Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on non-reasoning tasks, indicating its broader applicability. Compared to other state-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts (MoE) model with a relatively small size, featuring 20B activated and 200B total parameters. As part of our effort to assess generalized reasoning, we develop two internal benchmarks, BeyondAIME and Codeforces, both of which will be publicly released to support future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Conspiratorial Narratives within Arabic Online Content</title>
<link>https://arxiv.org/abs/2504.14037</link>
<guid>https://arxiv.org/abs/2504.14037</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, Topic Modeling, Top2Vec algorithm, conspiracy theories, Arabic digital spaces
<br />
<br />
Summary: 
This study investigates the spread of conspiracy theories in Arabic online spaces using advanced Natural Language Processing techniques. By analyzing data from Arabic blogs and Facebook, the research identifies six categories of conspiratorial narratives: gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering. The study reveals how these narratives are deeply rooted in Arabic social media discourse, influenced by regional historical and sociopolitical contexts. By focusing on Arabic content, the research fills a gap in existing conspiracy theory studies that have largely concentrated on English-language or offline data. The findings offer new insights into how conspiracy theories evolve and manifest in Arabic digital spaces, shedding light on their impact on public discourse in the Arab world. <div>
arXiv:2504.14037v1 Announce Type: new 
Abstract: This study investigates the spread of conspiracy theories in Arabic digital spaces through computational analysis of online content. By combining Named Entity Recognition and Topic Modeling techniques, specifically the Top2Vec algorithm, we analyze data from Arabic blogs and Facebook to identify and classify conspiratorial narratives. Our analysis uncovers six distinct categories: gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering. The research highlights how these narratives are deeply embedded in Arabic social media discourse, shaped by regional historical, cultural, and sociopolitical contexts. By applying advanced Natural Language Processing methods to Arabic content, this study addresses a gap in conspiracy theory research, which has traditionally focused on English-language content or offline data. The findings provide new insights into the manifestation and evolution of conspiracy theories in Arabic digital spaces, enhancing our understanding of their role in shaping public discourse in the Arab world.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEQA: A Meta-Evaluation Framework for Question &amp; Answer LLM Benchmarks</title>
<link>https://arxiv.org/abs/2504.14039</link>
<guid>https://arxiv.org/abs/2504.14039</guid>
<content:encoded><![CDATA[
<div> framework, meta-evaluation, question and answer, benchmarks, cybersecurity  
Summary:  
MEQA is a framework proposed for meta-evaluation of question and answer benchmarks, aiming to assess their quality through standardized assessments and quantifiable scores. The need for rigorous evaluations of Large Language Models (LLMs) is emphasized due to their increasing societal impact. Despite the existence of several evaluation benchmarks, there is a gap in effectively evaluating their quality, which MEQA seeks to address. The framework enables meaningful intra-benchmark comparisons and highlights the strengths and weaknesses of benchmarks in a specific domain, such as cybersecurity. By utilizing human and LLM evaluators, MEQA aims to provide insights into the performance of benchmarks and their relevance in assessing the capabilities of AI models in both defensive and security threat contexts.<br /><br />Summary: <div>
arXiv:2504.14039v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) advance, their potential for widespread societal impact grows simultaneously. Hence, rigorous LLM evaluations are both a technical necessity and social imperative. While numerous evaluation benchmarks have been developed, there remains a critical gap in meta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a framework for the meta-evaluation of question and answer (QA) benchmarks, to provide standardized assessments, quantifiable scores, and enable meaningful intra-benchmark comparisons. We demonstrate this approach on cybersecurity benchmarks, using human and LLM evaluators, highlighting the benchmarks' strengths and weaknesses. We motivate our choice of test domain by AI models' dual nature as powerful defensive tools and security threats.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task</title>
<link>https://arxiv.org/abs/2504.14066</link>
<guid>https://arxiv.org/abs/2504.14066</guid>
<content:encoded><![CDATA[
<div> Few-shot learning, quantized Gemma 2 9B model, Reddit, mental health data, self-state evidence <br />
<br />
Summary: <br />
The article presents a baseline system for the CLPsych 2025 A.1 task of classifying self-states in mental health data from Reddit. The system utilizes few-shot learning with a 4-bit quantized Gemma 2 9B model and a preprocessing step that first identifies relevant sentences indicating self-state evidence. A binary classification is then performed to determine whether the sentence indicates an adaptive or maladaptive self-state. This approach outperforms another method that relies on an LLM to highlight spans of variable length independently. The performance of the model is attributed to the sentence chunking step, which matches the granularity of human-annotated self-states and simplifies the task for the language model. The system achieves a test-time recall of 0.579, ranking third out of fourteen systems in the task. <div>
arXiv:2504.14066v1 Announce Type: new 
Abstract: We present a baseline for the CLPsych 2025 A.1 task: classifying self-states in mental health data taken from Reddit. We use few-shot learning with a 4-bit quantized Gemma 2 9B model and a data preprocessing step which first identifies relevant sentences indicating self-state evidence, and then performs a binary classification to determine whether the sentence is evidence of an adaptive or maladaptive self-state. This system outperforms our other method which relies on an LLM to highlight spans of variable length independently. We attribute the performance of our model to the benefits of this sentence chunking step for two reasons: partitioning posts into sentences 1) broadly matches the granularity at which self-states were human-annotated and 2) simplifies the task for our language model to a binary classification problem. Our system places third out of fourteen systems submitted for Task A.1, achieving a test-time recall of 0.579.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2504.14089</link>
<guid>https://arxiv.org/abs/2504.14089</guid>
<content:encoded><![CDATA[
<div> framework, logical reasoning, proof exploration, premise search, heuristics  
Summary:  
- LogicTree is a new framework designed to enhance the logical reasoning capabilities of large language models (LLMs) by automating structured proof exploration and ensuring logical coherence.
- This framework incorporates a caching mechanism to utilize historical knowledge effectively, prevent reasoning stagnation, and minimize redundancy.
- By decomposing premise search into a linear process, LogicTree addresses the combinatorial complexity of searching for the right combination of premises at each reasoning step.
- The refined premise selection within LogicTree restricts inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning.
- Two LLM-free heuristics are introduced for premise prioritization, enabling strategic proof search and enhancing the overall performance of the framework.
<br /><br />Summary: <div>
arXiv:2504.14089v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models</title>
<link>https://arxiv.org/abs/2504.14117</link>
<guid>https://arxiv.org/abs/2504.14117</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Vision Language Models, Parameter-Efficient Fine-Tuning, Resource challenges, PEFT techniques <br />
Summary: <br />
Large Language Models (LLMs) and Vision Language Models (VLMs) have revolutionized artificial intelligence applications. Fully fine-tuning these models is costly, leading to the development of Parameter-Efficient Fine-Tuning (PEFT) techniques. This survey explores the challenges of traditional fine-tuning, such as overfitting and parameter inefficiency. It introduces a taxonomy of PEFT methods, categorizing them into different frameworks based on their mechanisms. PEFT methods offer strong performance with lower resource costs across various domains, including language, vision, and generative modeling. The survey also discusses open challenges in scalability, interpretability, and robustness and suggests future research directions. The goal is to provide a unified understanding of PEFT and its role in enabling practical, efficient, and sustainable use of large models. <br /> <div>
arXiv:2504.14117v1 Announce Type: new 
Abstract: Large models such as Large Language Models (LLMs) and Vision Language Models (VLMs) have transformed artificial intelligence, powering applications in natural language processing, computer vision, and multimodal learning. However, fully fine-tuning these models remains expensive, requiring extensive computational resources, memory, and task-specific data. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a promising solution that allows adapting large models to downstream tasks by updating only a small portion of parameters. This survey presents a comprehensive overview of PEFT techniques, focusing on their motivations, design principles, and effectiveness. We begin by analyzing the resource and accessibility challenges posed by traditional fine-tuning and highlight key issues, such as overfitting, catastrophic forgetting, and parameter inefficiency. We then introduce a structured taxonomy of PEFT methods -- grouped into additive, selective, reparameterized, hybrid, and unified frameworks -- and systematically compare their mechanisms and trade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse domains, including language, vision, and generative modeling, showing how these techniques offer strong performance with lower resource costs. We also discuss important open challenges in scalability, interpretability, and robustness, and suggest future directions such as federated learning, domain adaptation, and theoretical grounding. Our goal is to provide a unified understanding of PEFT and its growing role in enabling practical, efficient, and sustainable use of large models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations</title>
<link>https://arxiv.org/abs/2504.14150</link>
<guid>https://arxiv.org/abs/2504.14150</guid>
<content:encoded><![CDATA[
<div> faithfulness, large language models, explanations, social bias, medical question answering

Summary: 
- Large language models (LLMs) generate explanations that can misrepresent their reasoning process, leading to over-trust and misuse.
- A new approach is introduced to measure the faithfulness of LLM explanations by comparing influential concepts implied by explanations to truly influential concepts.
- The method estimates faithfulness by using an auxiliary LLM to create realistic counterfactuals and a Bayesian hierarchical model to quantify causal effects.
- Experiments demonstrate the method's ability to quantify and reveal patterns of unfaithfulness, such as hiding social bias influences and providing misleading claims in medical question answering.
- By uncovering cases where explanations are unfaithful, the method helps improve the trustworthiness and accuracy of LLMs in generating explanations. 

<br /><br />Summary: <div>
arXiv:2504.14150v1 Announce Type: new 
Abstract: Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's "reasoning" process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that LLM explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a Bayesian hierarchical model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SConU: Selective Conformal Uncertainty in Large Language Models</title>
<link>https://arxiv.org/abs/2504.14154</link>
<guid>https://arxiv.org/abs/2504.14154</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, conformal uncertainty, significance tests, miscoverage rates, prediction sets

Summary:<br /><br />
This paper introduces a novel approach called Selective Conformal Uncertainty (SConU) for managing uncertainty in large language models. SConU uses significance tests to identify outliers in the data that violate the exchangeability assumption, helping to control miscoverage rates and improve the reliability of prediction sets. The approach also includes conformal p-values to determine if a sample deviates from the uncertainty distribution of the calibration set. This allows for efficient management of uncertainty across various domains and enhances prediction efficiency. The study analyzes the components of conformal procedures to approximate conditional coverage, especially in high-stakes question-answering tasks. By offering rigorous guarantees of task-specific metrics and enabling better control of prediction uncertainty, SConU provides a valuable tool for the reliable deployment of language models in real-world applications.<br /><br />Summary: <div>
arXiv:2504.14154v1 Announce Type: new 
Abstract: As large language models are increasingly utilized in real-world applications, guarantees of task-specific metrics are essential for their reliable deployment. Previous studies have introduced various criteria of conformal uncertainty grounded in split conformal prediction, which offer user-specified correctness coverage. However, existing frameworks often fail to identify uncertainty data outliers that violate the exchangeability assumption, leading to unbounded miscoverage rates and unactionable prediction sets. In this paper, we propose a novel approach termed Selective Conformal Uncertainty (SConU), which, for the first time, implements significance tests, by developing two conformal p-values that are instrumental in determining whether a given sample deviates from the uncertainty distribution of the calibration set at a specific manageable risk level. Our approach not only facilitates rigorous management of miscoverage rates across both single-domain and interdisciplinary contexts, but also enhances the efficiency of predictions. Furthermore, we comprehensively analyze the components of the conformal procedures, aiming to approximate conditional coverage, particularly in high-stakes question-answering tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Correction Makes LLMs Better Parsers</title>
<link>https://arxiv.org/abs/2504.14165</link>
<guid>https://arxiv.org/abs/2504.14165</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, syntactic parsing, grammar rules, self-correction method, NLP tasks 

Summary:
Large language models (LLMs) have shown success in various NLP tasks but struggle with syntactic parsing. The study identifies limitations in utilizing grammar rules from existing treebanks, hindering the generation of valid syntactic structures. To address this, a self-correction method is proposed that uses grammar rules to guide LLMs in rectifying errors. Errors are automatically detected, and relevant rules are searched for, providing hints and examples for correction. Experimental results on English and Chinese datasets show significant performance improvements in both in-domain and cross-domain settings. This method enhances LLMs' parsing capabilities and helps them achieve better accuracy in syntactic parsing tasks. <br /><br />Summary: <div>
arXiv:2504.14165v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success across various natural language processing (NLP) tasks. However, recent studies suggest that they still face challenges in performing fundamental NLP tasks essential for deep language understanding, particularly syntactic parsing. In this paper, we conduct an in-depth analysis of LLM parsing capabilities, delving into the specific shortcomings of their parsing results. We find that LLMs may stem from limitations to fully leverage grammar rules in existing treebanks, which restricts their capability to generate valid syntactic structures. To help LLMs acquire knowledge without additional training, we propose a self-correction method that leverages grammar rules from existing treebanks to guide LLMs in correcting previous errors. Specifically, we automatically detect potential errors and dynamically search for relevant rules, offering hints and examples to guide LLMs in making corrections themselves. Experimental results on three datasets with various LLMs, demonstrate that our method significantly improves performance in both in-domain and cross-domain settings on the English and Chinese datasets.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion</title>
<link>https://arxiv.org/abs/2504.14175</link>
<guid>https://arxiv.org/abs/2504.14175</guid>
<content:encoded><![CDATA[
<div> large language models, query expansion, fact verification, knowledge leakage, performance gains <br />
<br />
Summary: 
The article investigates the effectiveness of query expansion methods powered by large language models (LLMs) in zero-shot retrieval tasks. It challenges the assumption that LLMs can generate hypothetical documents to enhance real evidence retrieval by examining knowledge leakage in benchmarks. The study focuses on fact verification and analyzes whether generated documents contain information entailed by ground truth evidence. Results show performance improvements in claims where generated documents include sentences entailed by ground truth evidence, suggesting the presence of knowledge leakage in benchmarks. This may inflate the perceived performance of LLM-based query expansion methods, especially in scenarios requiring niche or novel knowledge retrieval. <div>
arXiv:2504.14175v1 Announce Type: new 
Abstract: Query expansion methods powered by large language models (LLMs) have demonstrated effectiveness in zero-shot retrieval tasks. These methods assume that LLMs can generate hypothetical documents that, when incorporated into a query vector, enhance the retrieval of real evidence. However, we challenge this assumption by investigating whether knowledge leakage in benchmarks contributes to the observed performance gains. Using fact verification as a testbed, we analyzed whether the generated documents contained information entailed by ground truth evidence and assessed their impact on performance. Our findings indicate that performance improvements occurred consistently only for claims whose generated documents included sentences entailed by ground truth evidence. This suggests that knowledge leakage may be present in these benchmarks, inflating the perceived performance of LLM-based query expansion methods, particularly in real-world scenarios that require retrieving niche or novel knowledge.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models</title>
<link>https://arxiv.org/abs/2504.14194</link>
<guid>https://arxiv.org/abs/2504.14194</guid>
<content:encoded><![CDATA[
<div> Quality Metrics, Data Selection, Language Models, Meta-rater, Dataset

Summary:<br /><br /> 
The article introduces a new method called PRRC to evaluate data quality for large language models (LLMs) based on Professionalism, Readability, Reasoning, and Cleanliness. It also presents Meta-rater, a multi-dimensional data selection approach that combines these dimensions with existing metrics through learned weightings. Meta-rater significantly accelerates convergence speed for 1.3B parameter models and enhances downstream task performance by 3.23. The method shows scalable benefits for 3.3B models trained on 100B tokens. The authors release the SlimPajama-627B dataset annotated across 25 quality metrics, including PRRC, to facilitate research in data-centric LLM development. The study demonstrates that integrating multiple quality dimensions outperforms conventional single-dimensional approaches, offering an efficient way to enhance pre-training effectiveness and model capability. <div>
arXiv:2504.14194v1 Announce Type: new 
Abstract: The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose PRRC to evaluate data quality across Professionalism, Readability, Reasoning, and Cleanliness. We further introduce Meta-rater, a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with scalable benefits observed in 3.3B models trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B dataset, labeled across 25 quality metrics (including PRRC), to advance research in data-centric LLM development. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition</title>
<link>https://arxiv.org/abs/2504.14203</link>
<guid>https://arxiv.org/abs/2504.14203</guid>
<content:encoded><![CDATA[
<div> Challenges, Nested NER, Low resource, Class imbalance, EIoU-EMC<br />
<br />
Summary: <br />
Research on nested Named Entity Recognition (NER) tasks in specific domains faces challenges like low resources and class imbalances. To address this, a novel loss function, EIoU-EMC, is proposed by enhancing Intersection over Union and Multiclass losses. This method focuses on entity boundary and classification information to improve learning from limited data samples. Experimental validation on three biomedical NER datasets and one industrial dataset demonstrates the method's competitive performance compared to baselines. Significant advancements are observed in entity boundary recognition and classification with the proposed method. The code for this study is available for further exploration. <br /> <div>
arXiv:2504.14203v1 Announce Type: new 
Abstract: In recent years, research has mainly focused on the general NER task. There still have some challenges with nested NER task in the specific domains. Specifically, the scenarios of low resource and class imbalance impede the wide application for biomedical and industrial domains. In this study, we design a novel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss and Multiclass loss. Our proposed method specially leverages the information of entity boundary and entity classification, thereby enhancing the model's capacity to learn from a limited number of data samples. To validate the performance of this innovative method in enhancing NER task, we conducted experiments on three distinct biomedical NER datasets and one dataset constructed by ourselves from industrial complex equipment maintenance documents. Comparing to strong baselines, our method demonstrates the competitive performance across all datasets. During the experimental analysis, our proposed method exhibits significant advancements in entity boundary recognition and entity classification. Our code are available here.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification</title>
<link>https://arxiv.org/abs/2504.14212</link>
<guid>https://arxiv.org/abs/2504.14212</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, social biases, pretraining data, protected attribute detection, bias analysis

Summary:
Large language models (LLMs) learn linguistic knowledge from pretraining on vast amounts of web-crawled text data. However, these data sources may contain social biases that can be perpetuated or magnified by LLMs. This study introduces an annotation pipeline to examine social biases in pretraining corpora efficiently and effectively. The pipeline first identifies protected attributes representing diverse demographics and then categorizes the language's sentiment towards each attribute. The researchers conducted experiments using Common Crawl as a primary pretraining corpus to analyze biases and implement mitigation strategies. This approach highlights the importance of recognizing and addressing social biases in language models by understanding the influence of pretraining data sources on model outputs.<br /><br />Summary: <div>
arXiv:2504.14212v1 Announce Type: new 
Abstract: Large language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs. In this study, we propose an efficient yet effective annotation pipeline to investigate social biases in the pretraining corpora. Our pipeline consists of protected attribute detection to identify diverse demographics, followed by regard classification to analyze the language polarity towards each attribute. Through our experiments, we demonstrate the effect of our bias analysis and mitigation measures, focusing on Common Crawl as the most representative pretraining corpus.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Repeat Curse in Large Language Models from a Feature Perspective</title>
<link>https://arxiv.org/abs/2504.14218</link>
<guid>https://arxiv.org/abs/2504.14218</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Repeat Curse, Sparse Autoencoders, Mechanistic interpretability, Repetition Features <br />
<br />
Summary: 
This study explores the issue of repetitive text generation in Large Language Models (LLMs), which is referred to as the "Repeat Curse". It investigates the root causes of repetition in LLMs through the use of Sparse Autoencoders (SAEs) to extract monosemantic features. The proposed approach, "Duplicatus Charm", identifies key model activations responsible for generating repetitive outputs, known as "Repetition Features". By analyzing model layers and manipulating activations using SAEs, the study successfully mitigates the Repeat Curse. A repetition dataset is constructed to cover token and paragraph level repetitions, and an evaluation pipeline is introduced to measure the impact of identified repetition features. Deactivating these features effectively reduces repetition in text generation by LLMs. <div>
arXiv:2504.14218v1 Announce Type: new 
Abstract: Large language models (LLMs) have made remarkable progress in various domains, yet they often suffer from repetitive text generation, a phenomenon we refer to as the "Repeat Curse". While previous studies have proposed decoding strategies to mitigate repetition, the underlying mechanism behind this issue remains insufficiently explored. In this work, we investigate the root causes of repetition in LLMs through the lens of mechanistic interpretability. Inspired by recent advances in Sparse Autoencoders (SAEs), which enable monosemantic feature extraction, we propose a novel approach, "Duplicatus Charm", to induce and analyze the Repeat Curse. Our method systematically identifies "Repetition Features" -the key model activations responsible for generating repetitive outputs. First, we locate the layers most involved in repetition through logit analysis. Next, we extract and stimulate relevant features using SAE-based activation manipulation. To validate our approach, we construct a repetition dataset covering token and paragraph level repetitions and introduce an evaluation pipeline to quantify the influence of identified repetition features. Furthermore, by deactivating these features, we have effectively mitigated the Repeat Curse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification</title>
<link>https://arxiv.org/abs/2504.14223</link>
<guid>https://arxiv.org/abs/2504.14223</guid>
<content:encoded><![CDATA[
<div> Keywords: text simplification, large language models, plain language, automatic, inclusivity

Summary: 
- Text simplification is vital for enhancing accessibility to complex content for individuals with comprehension difficulties.
- Current methods of automatic text simplification do not fully utilize large language models to provide tailored customization for different target audiences.
- The practice of plain language, which has proven benefits for both consumers and organizations, is underutilized.
- The system https://simplifymytext.org is introduced, employing GPT-4 and Llama-3 to produce plain language content with customizable options for diverse audiences.
- Evaluation of the system's outputs across various metrics demonstrates its effectiveness and contribution to the field of automatic text simplification.

<br /><br />Summary: <div>
arXiv:2504.14223v1 Announce Type: new 
Abstract: Text simplification is essential for making complex content accessible to diverse audiences who face comprehension challenges. Yet, the limited availability of simplified materials creates significant barriers to personal and professional growth and hinders social inclusion. Although researchers have explored various methods for automatic text simplification, none fully leverage large language models (LLMs) to offer tailored customization for different target groups and varying levels of simplicity. Moreover, despite its proven benefits for both consumers and organizations, the well-established practice of plain language remains underutilized. In this paper, we https://simplifymytext.org, the first system designed to produce plain language content from multiple input formats, including typed text and file uploads, with flexible customization options for diverse audiences. We employ GPT-4 and Llama-3 and evaluate outputs across multiple metrics. Overall, our work contributes to research on automatic text simplification and highlights the importance of tailored communication in promoting inclusivity.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale</title>
<link>https://arxiv.org/abs/2504.14225</link>
<guid>https://arxiv.org/abs/2504.14225</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Personalized Assistants, User Profiling, Personalized Responses, Chatbots

Summary:
Large Language Models (LLMs) are being used as personalized assistants for various tasks, but there are still challenges in effectively leveraging user interaction history to personalize responses. The PERSONAMEM benchmark introduces curated user profiles with simulated interaction histories to evaluate LLM chatbots' ability to generate personalized responses. Current LLMs struggle to recognize the dynamic evolution of user profiles over time, leading to inaccuracies in response generation. Even advanced models like GPT-4.1, o4-mini, GPT-4.5, o1, and Gemini-2.0 only achieve around 50% accuracy in aligning responses with user preferences. This highlights the need for improvement in developing user-aware chatbots. The PERSONAMEM dataset and pipeline aim to facilitate future research in this area. (200 words)

<br /><br />Summary: Large Language Models (LLMs) are being used as personalized assistants for various tasks, but there are still challenges in effectively leveraging user interaction history to personalize responses. The PERSONAMEM benchmark introduces curated user profiles with simulated interaction histories to evaluate LLM chatbots' ability to generate personalized responses. Current LLMs struggle to recognize the dynamic evolution of user profiles over time, leading to inaccuracies in response generation. Even advanced models like GPT-4.1, o4-mini, GPT-4.5, o1, and Gemini-2.0 only achieve around 50% accuracy in aligning responses with user preferences. This highlights the need for improvement in developing user-aware chatbots. The PERSONAMEM dataset and pipeline aim to facilitate future research in this area. <div>
arXiv:2504.14225v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as personalized assistants for users across a wide range of tasks -- from offering writing support to delivering tailored recommendations or consultations. Over time, the interaction history between a user and an LLM can provide extensive information about an individual's traits and preferences. However, open questions remain on how well LLMs today can effectively leverage such history to (1) internalize the user's inherent traits and preferences, (2) track how the user profiling and preferences evolve over time, and (3) generate personalized responses accordingly in new scenarios.
  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features curated user profiles with over 180 simulated user-LLM interaction histories, each containing up to 60 sessions of multi-turn conversations across 15 real-world tasks that require personalization. Given an in-situ user query, i.e. query issued by the user from the first-person perspective, we evaluate LLM chatbots' ability to identify the most suitable response according to the current state of the user's profile. We observe that current LLMs still struggle to recognize the dynamic evolution in users' profiles over time through direct prompting approaches. As a consequence, LLMs often fail to deliver responses that align with users' current situations and preferences, with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0 achieving only around 50% overall accuracy, suggesting room for improvement. We hope that PERSONAMEM, along with the user profile and conversation simulation pipeline, can facilitate future research in the development of truly user-aware chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Subtle Ideological Manipulation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.14287</link>
<guid>https://arxiv.org/abs/2504.14287</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Ideological Manipulation, Progressive-Left, Conservative-Right, Fine-tuning

Summary:
Large Language Models (LLMs) have revolutionized natural language processing but are vulnerable to ideological manipulation in politically sensitive areas. Previous research focused on binary Left-Right biases, using explicit prompts and political QA datasets. This study goes beyond binary ideologies to explore a spectrum from Progressive-Left to Conservative-Right. A multi-task dataset was created to reflect diverse ideological positions. Three LLMs were fine-tuned on this dataset to assess their ability to adopt nuanced ideologies. The results showed that fine-tuning significantly improved ideological alignment, with minor refinements from explicit prompts. This suggests that LLMs can be subtly manipulated ideologically, highlighting the need for robust safeguards to minimize risks. <br /><br />Summary: The study investigates the susceptibility of Large Language Models to nuanced ideological influences ranging from Progressive-Left to Conservative-Right. Fine-tuning the models on a multi-task dataset enhanced their alignment with these ideologies, indicating potential risks of ideological manipulation. Stronger safeguards are needed to mitigate these risks. <div>
arXiv:2504.14287v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have transformed natural language processing, but concerns have emerged about their susceptibility to ideological manipulation, particularly in politically sensitive areas. Prior work has focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning on political QA datasets. In this work, we move beyond this binary approach to explore the extent to which LLMs can be influenced across a spectrum of political ideologies, from Progressive-Left to Conservative-Right. We introduce a novel multi-task dataset designed to reflect diverse ideological positions through tasks such as ideological QA, statement ranking, manifesto cloze completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2, Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and express these nuanced ideologies. Our findings indicate that fine-tuning significantly enhances nuanced ideological alignment, while explicit prompts provide only minor refinements. This highlights the models' susceptibility to subtle ideological manipulation, suggesting a need for more robust safeguards to mitigate these risks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach</title>
<link>https://arxiv.org/abs/2504.14321</link>
<guid>https://arxiv.org/abs/2504.14321</guid>
<content:encoded><![CDATA[
<div> multimodal coreference resolution, Chinese, social media, dataset, TikTalkCoref<br />
Summary:<br />
This article introduces the TikTalkCoref dataset, the first Chinese multimodal coreference dataset for real-world social media dialogues. Derived from the Douyin platform, the dataset pairs short videos with textual dialogues from user comments and includes manually annotated coreference clusters for person mentions in the text and corresponding video frames. An effective benchmark approach for multimodal coreference resolution in the celebrity domain is presented, accompanied by extensive experiments on the dataset to provide reliable benchmark results. The goal is to facilitate future research on multimodal coreference resolution for real-world social media dialogues. <div>
arXiv:2504.14321v1 Announce Type: new 
Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring to the same entity across different modalities, such as text and visuals, and is essential for understanding multimodal content. In the era of rapidly growing mutimodal content and social media, MCR is particularly crucial for interpreting user interactions and bridging text-visual references to improve communication and personalization. However, MCR research for real-world dialogues remains unexplored due to the lack of sufficient data resources.To address this gap, we introduce TikTalkCoref, the first Chinese multimodal coreference dataset for social media in real-world scenarios, derived from the popular Douyin short-video platform. This dataset pairs short videos with corresponding textual dialogues from user comments and includes manually annotated coreference clusters for both person mentions in the text and the coreferential person head regions in the corresponding video frames. We also present an effective benchmark approach for MCR, focusing on the celebrity domain, and conduct extensive experiments on our dataset, providing reliable benchmark results for this newly constructed dataset. We will release the TikTalkCoref dataset to facilitate future research on MCR for real-world social media dialogues.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models</title>
<link>https://arxiv.org/abs/2504.14366</link>
<guid>https://arxiv.org/abs/2504.14366</guid>
<content:encoded><![CDATA[
<div> knowledge distillation, language models, subquadratic architectures, transferability, NLP benchmarks

Summary:
This work explores the transferability of knowledge distillation from a Transformer teacher model to nine subquadratic student architectures. The study evaluates how well different subquadratic models align with the teacher's learned representations and explores the impact of architectural constraints on the distillation process. Intelligent initialization strategies such as matrix mixing and query-key-value copying are also investigated to improve the adaptation process. Empirical results on various NLP benchmarks highlight the trade-offs between efficiency and performance, providing insights into the factors that contribute to successful knowledge transfer to subquadratic architectures. <div>
arXiv:2504.14366v1 Announce Type: new 
Abstract: Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures, leveraging softmax attention for sequence modeling. However, the quadratic complexity of self-attention at inference time remains a significant bottleneck, motivating the exploration of subquadratic alternatives such as structured state-space models (SSMs), linear attention, and recurrent architectures. In this work, we systematically evaluate the transferability of knowledge distillation from a Transformer teacher to nine subquadratic student architectures. Our study aims to determine which subquadratic model best aligns with the teacher's learned representations and how different architectural constraints influence the distillation process. We also investigate the impact of intelligent initialization strategies, including matrix mixing and query-key-value (QKV) copying, on the adaptation process. Our empirical results on multiple NLP benchmarks provide insights into the trade-offs between efficiency and performance, highlighting key factors for successful knowledge transfer to subquadratic architectures.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites</title>
<link>https://arxiv.org/abs/2504.14367</link>
<guid>https://arxiv.org/abs/2504.14367</guid>
<content:encoded><![CDATA[
<div> evolutionary approach, prompt engineering, large language models, context-free grammar, MAP-Elites algorithm <br />
Summary:<br />
- An evolutionary approach combining context-free grammar with the MAP-Elites algorithm is used to explore the prompt space for optimizing large language models (LLMs). 
- The method prioritizes both quality and diversity in generating high-performing and structurally varied prompts while analyzing their alignment with diverse tasks. 
- The study systematically maps the phenotypic space to reveal how structural variations impact LLM performance and offer insights for task-specific and adaptable prompt design. 
- The research evaluated the approach on seven BigBench Lite tasks across multiple LLMs, highlighting the critical interplay between quality and diversity in enhancing the effectiveness and versatility of LLMs. <br /> 
Summary: <div>
arXiv:2504.14367v1 Announce Type: new 
Abstract: Prompt engineering is essential for optimizing large language models (LLMs), yet the link between prompt structures and task performance remains underexplored. This work introduces an evolutionary approach that combines context-free grammar (CFG) with the MAP-Elites algorithm to systematically explore the prompt space. Our method prioritizes quality and diversity, generating high-performing and structurally varied prompts while analyzing their alignment with diverse tasks by varying traits such as the number of examples (shots) and reasoning depth. By systematically mapping the phenotypic space, we reveal how structural variations influence LLM performance, offering actionable insights for task-specific and adaptable prompt design. Evaluated on seven BigBench Lite tasks across multiple LLMs, our results underscore the critical interplay of quality and diversity, advancing the effectiveness and versatility of LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data</title>
<link>https://arxiv.org/abs/2504.14452</link>
<guid>https://arxiv.org/abs/2504.14452</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Paraphrase Preference Optimization, regurgitation, creativity, privacy

Summary:
- Language models (LMs) can inadvertently reproduce segments verbatim from their pretraining data, raising concerns about copyright, plagiarism, privacy, and creativity.
- The Paraphrase Preference Optimization (ParaPO) method fine-tunes LMs to prefer paraphrased versions of memorized segments to reduce unintentional regurgitation while maintaining overall utility.
- ParaPO consistently reduces regurgitation across various datasets, demonstrating its effectiveness in mitigating regurgitation issues.
- Compared to unlearning methods, ParaPO is more successful in reducing regurgitation in different domains beyond its targeted domain.
- When applied to the instruction-tuned Tulu3-8B model, ParaPO with system prompting successfully balances famous quotation recall and reduction of unintentional regurgitation, showcasing its versatility and effectiveness in controlling LM behavior.

<br /><br />Summary: Language models have the capability to memorize and reproduce content verbatim, leading to concerns regarding copyright and plagiarism. The introduction of Paraphrase Preference Optimization (ParaPO) addresses these issues by training models to prefer paraphrased content, effectively reducing unintentional regurgitation while maintaining overall functionality. ParaPO proves to be more effective than traditional unlearning methods in mitigating regurgitation across diverse datasets. Additionally, when applied to specific models with system prompting, ParaPO successfully balances the recall of famous quotations and minimizes unintentional regurgitation, showcasing its adaptability and efficacy in controlling LM behavior. <div>
arXiv:2504.14452v1 Announce Type: new 
Abstract: Language models (LMs) can memorize and reproduce segments from their pretraining data verbatim even in non-adversarial settings, raising concerns about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to reduce unintentional regurgitation while preserving their overall utility. ParaPO trains LMs to prefer paraphrased versions of memorized segments over the original verbatim content from the pretraining data. To maintain the ability to recall famous quotations when appropriate, we develop a variant of ParaPO that uses system prompts to control regurgitation behavior. In our evaluation on Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative writing), whereas unlearning methods used in prior work to mitigate regurgitation are less effective outside their targeted unlearned domain (from 17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO with system prompting successfully preserves famous quotation recall while reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the model not to regurgitate produces only a marginal reduction (8.7 to 8.4).
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge</title>
<link>https://arxiv.org/abs/2504.14462</link>
<guid>https://arxiv.org/abs/2504.14462</guid>
<content:encoded><![CDATA[
<div> Dataset, Commonsense reasoning, Long-tail entities, Knowledge Graph Question Answering, LLM<br />
Summary:<br />
The article discusses the limitations of Large Language Models (LLMs) in handling reasoning errors and hallucinations, especially in tasks involving commonsense reasoning over obscure entities. To address this issue, a new dataset called Commonsense reasoning over Long-Tail entities (CoLoTa) is introduced, comprising 3,300 queries covering various commonsense reasoning skills. The dataset can also be utilized for Knowledge Graph Question Answering (KGQA) as it relies on the Wikidata knowledge graph for support. However, unlike existing KGQA benchmarks, CoLoTa focuses on commonsense reasoning in addition to factoid questions. The experiments conducted using LLM-based KGQA methodologies reveal their inadequacy in answering queries that require commonsense reasoning, highlighting the need for benchmark datasets like CoLoTa to assess LLM capabilities and KGQA methods in handling such tasks effectively. <br />Summary: <div>
arXiv:2504.14462v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has redefined the AI landscape, particularly due to their ability to encode factual and commonsense knowledge, and their outstanding performance in tasks requiring reasoning. Despite these advances, hallucinations and reasoning errors remain a significant barrier to their deployment in high-stakes settings. In this work, we observe that even the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning errors and hallucinations on tasks requiring commonsense reasoning over obscure, long-tail entities. To investigate this limitation, we present a new dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that consists of 3,300 queries from question answering and claim verification tasks and covers a diverse range of commonsense reasoning skills. We remark that CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset since the support of knowledge required to answer its queries is present in the Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that merely focus on factoid questions, our CoLoTa queries also require commonsense reasoning. Our experiments with strong LLM-based KGQA methodologies indicate their severe inability to answer queries involving commonsense reasoning. Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM commonsense reasoning capabilities and their robustness to hallucinations on long-tail entities and (ii) the commonsense reasoning capabilities of KGQA methods.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment</title>
<link>https://arxiv.org/abs/2504.14468</link>
<guid>https://arxiv.org/abs/2504.14468</guid>
<content:encoded><![CDATA[
<div> neural activity, latent representations, multimodal foundation models, brain recordings, sEEG signals 

Summary: 
Interpreting neural activity remains a challenging task bridging neuroscience and artificial intelligence. This study explores using multimodal foundation models to align invasive brain recordings with natural language. The proposed SSENSE framework utilizes contrastive learning to project single-subject sEEG signals into a sentence embedding space derived from a pre-trained CLIP model. By training a neural encoder on spectral representations of sEEG using InfoNCE loss without fine-tuning the text encoder, SSENSE enables sentence-level retrieval directly from brain activity. Evaluation on a naturalistic movie-watching dataset shows promising results, indicating that leveraging general-purpose language representations can effectively aid in neural decoding. <div>
arXiv:2504.14468v1 Announce Type: new 
Abstract: Interpreting neural activity through meaningful latent representations remains a complex and evolving challenge at the intersection of neuroscience and artificial intelligence. We investigate the potential of multimodal foundation models to align invasive brain recordings with natural language. We present SSENSE, a contrastive learning framework that projects single-subject stereo-electroencephalography (sEEG) signals into the sentence embedding space of a frozen CLIP model, enabling sentence-level retrieval directly from brain activity. SSENSE trains a neural encoder on spectral representations of sEEG using InfoNCE loss, without fine-tuning the text encoder. We evaluate our method on time-aligned sEEG and spoken transcripts from a naturalistic movie-watching dataset. Despite limited data, SSENSE achieves promising results, demonstrating that general-purpose language representations can serve as effective priors for neural decoding.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue</title>
<link>https://arxiv.org/abs/2504.14482</link>
<guid>https://arxiv.org/abs/2504.14482</guid>
<content:encoded><![CDATA[
<div> DialogueAgents, speech synthesis, dataset, emotional expressiveness, natural communication<br />
<br />
Summary:<br />
DialogueAgents introduces a novel hybrid agent-based speech synthesis framework that includes script writer, speech synthesizer, and dialogue critic agents collaborating to generate diverse dialogues. This framework improves emotional expressiveness and paralinguistic features of synthesized dialogues through iterative refinement. The MultiTalk dataset, created using DialogueAgents, is a bilingual, multi-party, multi-turn speech dialogue dataset covering diverse topics. Extensive experiments confirm the framework's effectiveness and dataset quality. The dataset and code are made available to support future research on advanced speech synthesis models and customized data generation. <div>
arXiv:2504.14482v1 Announce Type: new 
Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural and intuitive communication. However, existing datasets involve high construction costs due to manual annotation and suffer from limited character diversity, contextual scenarios, and emotional expressiveness. To address these issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis framework, which integrates three specialized agents -- a script writer, a speech synthesizer, and a dialogue critic -- to collaboratively generate dialogues. Grounded in a diverse character pool, the framework iteratively refines dialogue scripts and synthesizes speech based on speech review, boosting emotional expressiveness and paralinguistic features of the synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a bilingual, multi-party, multi-turn speech dialogue dataset covering diverse topics. Extensive experiments demonstrate the effectiveness of our framework and the high quality of the MultiTalk dataset. We release the dataset and code https://github.com/uirlx/DialogueAgents to facilitate future research on advanced speech synthesis models and customized data generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering</title>
<link>https://arxiv.org/abs/2504.14492</link>
<guid>https://arxiv.org/abs/2504.14492</guid>
<content:encoded><![CDATA[
<div> debiasing, language models, FairSteer, hidden activation space, dynamic activation steering
Summary:
FairSteer is a new framework for debiasing large language models that addresses issues with existing methods. It operates by detecting biased activations, computing debiasing steering vectors (DSVs), and dynamically adjusting activations at inference time. FairSteer is inspired by the linear representation hypothesis, which suggests that fairness-related features can be encoded in separable directions in the hidden activation space. By training a linear classifier to detect bias signatures and using small contrastive prompt pairs to compute intervention directions, FairSteer successfully debiases models without the need for customized prompts or retraining. Evaluation with six LLMs shows that FairSteer outperforms other methods in tasks such as question answering, counterfactual input evaluation, and open-ended text generation. Code for FairSteer will be made available to the public. 
<br /><br />Summary: <div>
arXiv:2504.14492v1 Announce Type: new 
Abstract: Large language models (LLMs) are prone to capturing biases from training corpus, leading to potential negative social impacts. Existing prompt-based debiasing methods exhibit instability due to their sensitivity to prompt changes, while fine-tuning-based techniques incur substantial computational overhead and catastrophic forgetting. In this paper, we propose FairSteer, a novel inference-time debiasing framework without requiring customized prompt design or model retraining. Motivated by the linear representation hypothesis, our preliminary investigation demonstrates that fairness-related features can be encoded into separable directions in the hidden activation space. FairSteer operates in three steps: biased activation detection, debiasing steering vector (DSV) computation, and dynamic activation steering. Specifically, it first trains a lightweight linear classifier to detect bias signatures in activations, and then computes DSVs as intervention directions derived from small contrastive prompt pairs. Subsequently, it performs debiasing by adjusting activations with DSVs in the inference stage. Comprehensive evaluation with six LLMs demonstrates the superiority of FairSteer across question-answering, counterfactual input evaluation and open-ended text generation tasks. Code will be released.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functional Abstraction of Knowledge Recall in Large Language Models</title>
<link>https://arxiv.org/abs/2504.14496</link>
<guid>https://arxiv.org/abs/2504.14496</guid>
<content:encoded><![CDATA[
<div> Transformer Language Models, Knowledge Recall, Functional Structure, Activation Vectors, Contextual Knowledge Editing <br />
Summary: <br />
This paper explores the knowledge recall mechanism in large language models, proposing that hidden activation spaces align with functional components during recall. Activation vectors map subjects to objects, with relation-related tokens defining the function. An algorithm identifies knowledge-aware activation vectors, and counter-knowledge testing evaluates their effects. Contextual knowledge editing is improved through activation patching, enhancing short-term memory retention for new knowledge recall. <div>
arXiv:2504.14496v1 Announce Type: new 
Abstract: Pre-trained transformer large language models (LLMs) demonstrate strong knowledge recall capabilities. This paper investigates the knowledge recall mechanism in LLMs by abstracting it into a functional structure. We propose that during knowledge recall, the model's hidden activation space implicitly entails a function execution process where specific activation vectors align with functional components (Input argument, Function body, and Return values). Specifically, activation vectors of relation-related tokens define a mapping function from subjects to objects, with subject-related token activations serving as input arguments and object-related token activations as return values. For experimental verification, we first design a patching-based knowledge-scoring algorithm to identify knowledge-aware activation vectors as independent functional components. Then, we conduct counter-knowledge testing to examine the independent functional effects of each component on knowledge recall outcomes. From this functional perspective, we improve the contextual knowledge editing approach augmented by activation patching. By rewriting incoherent activations in context, we enable improved short-term memory retention for new knowledge prompting.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality for Natural Language Processing</title>
<link>https://arxiv.org/abs/2504.14530</link>
<guid>https://arxiv.org/abs/2504.14530</guid>
<content:encoded><![CDATA[
<div> Keywords: causal reasoning, large language models, natural language processing, computational social science, scientific impact 

Summary: 
This thesis explores the role of causal reasoning in large language models (LLMs) and its implications for artificial intelligence and natural language processing (NLP). It investigates how LLMs perform causal inference tasks, identifies the mechanisms driving their performance, and examines the impact of causal and anticausal learning on NLP tasks. The thesis also examines the use of causal reasoning in computational social science, focusing on political decision-making and scientific impact assessment through citations. By introducing new datasets, benchmark tasks, and methodological frameworks, the study highlights the challenges and opportunities for enhancing the causal reasoning abilities of LLMs. This research lays the groundwork for future studies in this rapidly evolving field. 

<br /><br />Summary: <div>
arXiv:2504.14530v1 Announce Type: new 
Abstract: Causal reasoning is a cornerstone of human intelligence and a critical capability for artificial systems aiming to achieve advanced understanding and decision-making. This thesis delves into various dimensions of causal reasoning and understanding in large language models (LLMs). It encompasses a series of studies that explore the causal inference skills of LLMs, the mechanisms behind their performance, and the implications of causal and anticausal learning for natural language processing (NLP) tasks. Additionally, it investigates the application of causal reasoning in text-based computational social science, specifically focusing on political decision-making and the evaluation of scientific impact through citations. Through novel datasets, benchmark tasks, and methodological frameworks, this work identifies key challenges and opportunities to improve the causal capabilities of LLMs, providing a comprehensive foundation for future research in this evolving field.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation</title>
<link>https://arxiv.org/abs/2504.14538</link>
<guid>https://arxiv.org/abs/2504.14538</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social simulation, multi-agent systems, fictional worlds, story generation

Summary:
BookWorld is a new system that allows for the construction and simulation of multi-agent societies based on established fictional worlds and characters. It incorporates real-world complexities such as diverse characters, dynamic interactions, and geographical constraints. The system offers various applications, including story generation, interactive games, and social simulation. Through experiments, BookWorld has shown to produce creative and high-quality stories that stay faithful to the source material, surpassing previous methods with a 75.36% success rate. The project code can be accessed on the project page provided. <div>
arXiv:2504.14538v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld's design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>a1: Steep Test-time Scaling Law via Environment Augmented Generation</title>
<link>https://arxiv.org/abs/2504.14597</link>
<guid>https://arxiv.org/abs/2504.14597</guid>
<content:encoded><![CDATA[
<div> environmental feedback, reasoning, branch exploration, machine learning, performance improvement

Summary:
Environment Augmented Generation (EAG) framework enhances Large Language Models (LLMs) reasoning by providing real-time environmental feedback for each step, dynamic branch exploration for error correction, and experience-based learning. EAG enables deliberate backtracking and strategic replanning by integrating execution feedback with branching exploration. The a1-32B model achieves state-of-the-art performance on benchmarks, matching larger models like o1 in competition mathematics. EAG's scaling pattern shows that initial investment in environment interaction yields long-term performance dividends, especially for complex tasks. Theoretical framework of EAG demonstrates a new paradigm for reliable machine reasoning, particularly for multi-step calculation and logical verification tasks. <div>
arXiv:2504.14597v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made remarkable breakthroughs in reasoning, yet continue to struggle with hallucinations, logical errors, and inability to self-correct during complex multi-step tasks. Current approaches like chain-of-thought prompting offer limited reasoning capabilities that fail when precise step validation is required. We propose Environment Augmented Generation (EAG), a framework that enhances LLM reasoning through: (1) real-time environmental feedback validating each reasoning step, (2) dynamic branch exploration for investigating alternative solution paths when faced with errors, and (3) experience-based learning from successful reasoning trajectories. Unlike existing methods, EAG enables deliberate backtracking and strategic replanning through tight integration of execution feedback with branching exploration. Our a1-32B model achieves state-of-the-art performance among similar-sized models across all benchmarks, matching larger models like o1 on competition mathematics while outperforming comparable models by up to 24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern: initial token investment in environment interaction yields substantial long-term performance dividends, with advantages amplifying proportionally to task complexity. EAG's theoretical framework demonstrates how environment interactivity and systematic branch exploration together establish a new paradigm for reliable machine reasoning, particularly for problems requiring precise multi-step calculation and logical verification.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations</title>
<link>https://arxiv.org/abs/2504.14619</link>
<guid>https://arxiv.org/abs/2504.14619</guid>
<content:encoded><![CDATA[
<div> Keywords: Language technologies, neural machine translation, translation analytics, automatic evaluation metrics, professional environment

Summary:
The paper explores the opportunities for individual translators and language service providers created by recent advancements in language technologies. These include neural machine translation systems, large language models, and computer-assisted translation tools. The integration of these tools into workflows allows for not only translation but also quality evaluation, error spotting, glossary generation, and domain-specific adaptation. The paper introduces the concept of Translation Analytics, which provides evaluation techniques for smaller-scale users. It presents a framework for adapting automatic evaluation metrics to freelancers' needs and demonstrates their potential using a trilingual corpus from a medical domain project. Statistical analysis shows a correlation between human evaluations and automatic scores. The findings highlight the importance of embracing emerging technologies to succeed in the evolving professional landscape.

<br /><br />Summary: <div>
arXiv:2504.14619v1 Announce Type: new 
Abstract: This is the first in a series of papers exploring the rapidly expanding new opportunities arising from recent progress in language technologies for individual translators and language service providers with modest resources. The advent of advanced neural machine translation systems, large language models, and their integration into workflows via computer-assisted translation tools and translation management systems have reshaped the translation landscape. These advancements enable not only translation but also quality evaluation, error spotting, glossary generation, and adaptation to domain-specific needs, creating new technical opportunities for freelancers. In this series, we aim to empower translators with actionable methods to harness these advancements. Our approach emphasizes Translation Analytics, a suite of evaluation techniques traditionally reserved for large-scale industry applications but now becoming increasingly available for smaller-scale users. This first paper introduces a practical framework for adapting automatic evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers' needs. We illustrate the potential of these metrics using a trilingual corpus derived from a real-world project in the medical domain and provide statistical analysis correlating human evaluations with automatic scores. Our findings emphasize the importance of proactive engagement with emerging technologies to not only adapt but thrive in the evolving professional environment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models</title>
<link>https://arxiv.org/abs/2504.14620</link>
<guid>https://arxiv.org/abs/2504.14620</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific paper innovation, HSPIM framework, large language models, section classification, question-answering, novelty scoring

Summary:
HSPIM is a novel framework for measuring scientific paper innovation that addresses the shortcomings of existing content-based methods. It utilizes large language models (LLMs) to segment the text into sections, classify sections, perform question-answering, and calculate novelty scores. By using a hierarchical approach and zero-shot LLM prompting, HSPIM provides a more comprehensive assessment of innovation at both the section and paper levels. The framework also incorporates a two-layer question structure and a genetic algorithm for optimizing question-prompt combinations. Experimental results demonstrate that HSPIM outperforms baseline methods in effectiveness, generalization, and interpretability. Overall, HSPIM offers a promising solution for evaluating the innovative impact of scientific papers in a more inclusive and accurate manner. 

<br /><br />Summary: <div>
arXiv:2504.14620v1 Announce Type: new 
Abstract: Measuring scientific paper innovation is both important and challenging. Existing content-based methods often overlook the full-paper context, fail to capture the full scope of innovation, and lack generalization. We propose HSPIM, a hierarchical and training-free framework based on large language models (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess innovation. We segment the text by section titles and use zero-shot LLM prompting to implement section classification, question-answering (QA) augmentation, and weighted novelty scoring. The generated QA pair focuses on section-level innovation and serves as additional context to improve the LLM scoring. For each chunk, the LLM outputs a novelty score and a confidence score. We use confidence scores as weights to aggregate novelty scores into a paper-level innovation score. To further improve performance, we propose a two-layer question structure consisting of common and section-specific questions, and apply a genetic algorithm to optimize the question-prompt combinations. Comprehensive experiments on scientific conference paper datasets show that HSPIM outperforms baseline methods in effectiveness, generalization, and interpretability.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish</title>
<link>https://arxiv.org/abs/2504.14630</link>
<guid>https://arxiv.org/abs/2504.14630</guid>
<content:encoded><![CDATA[
<div> Keyword: Automatic Text Summarization, Kurdish, Dataset, Language Model, ROUGE metrics

Summary:
This study focuses on developing resources for Automatic Text Summarization (ATS) in Kurdish, a language lacking in such resources. The researchers created a dataset of 231 scientific papers in Sorani Kurdish and built a language model using Sentence Weighting and TF-IDF algorithms. Two experiments were conducted, varying in whether conclusions were included, with an average word count of around 5,400. Manual and automatic evaluations using ROUGE metrics showed accuracy up to 19.58%. Six experts also conducted manual evaluations, with results varying by document. This research aims to contribute valuable resources for Kurdish NLP researchers to enhance ATS and related fields. 

Summary: <div>
arXiv:2504.14630v1 Announce Type: new 
Abstract: Extracting concise information from scientific documents aids learners, researchers, and practitioners. Automatic Text Summarization (ATS), a key Natural Language Processing (NLP) application, automates this process. While ATS methods exist for many languages, Kurdish remains underdeveloped due to limited resources. This study develops a dataset and language model based on 231 scientific papers in Sorani Kurdish, collected from four academic departments in two universities in the Kurdistan Region of Iraq (KRI), averaging 26 pages per document. Using Sentence Weighting and Term Frequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were conducted, differing in whether the conclusions were included. The average word count was 5,492.3 in the first experiment and 5,266.96 in the second. Results were evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L metrics, with the best accuracy reaching 19.58%. Six experts conducted manual evaluations using three criteria, with results varying by document. This research provides valuable resources for Kurdish NLP researchers to advance ATS and related fields.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance</title>
<link>https://arxiv.org/abs/2504.14633</link>
<guid>https://arxiv.org/abs/2504.14633</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial event entity extraction, Large Language Models, Parameter-Efficient Fine-Tuning, structured output generation, state-of-the-art performance

Summary: <br />
- Financial event entity extraction is a crucial task for analyzing market dynamics and building financial knowledge graphs.
- Traditional approaches using sequence labeling models face challenges with long-range dependencies and extracting multiple overlapping entities.
- A novel method is proposed that reframes financial event entity extraction as a text-to-structured-output generation task using Large Language Models (LLMs) and Parameter-Efficient Fine-Tuning (PEFT).
- The approach achieves a new state-of-the-art F1 score on the CCKS 2019 Financial Event Entity Extraction dataset, outperforming previous methods.
- Experimental results and human evaluation show the generative LLM method is more effective in handling financial text nuances and extracting high-quality entities. 

Summary: <div>
arXiv:2504.14633v1 Announce Type: new 
Abstract: Financial event entity extraction is a crucial task for analyzing market dynamics and building financial knowledge graphs, yet it presents significant challenges due to the specialized language and complex structures in financial texts. Traditional approaches often rely on sequence labeling models, which can struggle with long-range dependencies and the inherent complexity of extracting multiple, potentially overlapping entities. Motivated by the advanced language understanding and generative capabilities of Large Language Models (LLMs), we propose a novel method that reframes financial event entity extraction as a text-to-structured-output generation task. Our approach involves fine-tuning a pre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly generate a structured representation, such as a JSON object, containing the extracted entities and their precise character spans from the input text. We evaluate our method on the challenging CCKS 2019 Financial Event Entity Extraction dataset, comparing its performance against strong sequence labeling baselines, including SEBERTNets and sebertNets. Experimental results demonstrate that our generative LLM method achieves a new state-of-the-art F1 score on this benchmark, significantly outperforming previous methods. Through detailed quantitative analysis across event types, entity types, and instance complexity, as well as human evaluation, we show that our approach is more effective at handling the nuances of financial text and extracting high-quality entities. This work validates the potential of applying generative LLMs directly to complex, domain-specific information extraction tasks requiring structured output.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs</title>
<link>https://arxiv.org/abs/2504.14657</link>
<guid>https://arxiv.org/abs/2504.14657</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic Electronic Health Records, Large Language Models, Privacy Preservation, Generalization, Healthcare<br />
<br />
Summary: 
The study focuses on the use of Large Language Models (LLMs) for generating synthetic Electronic Health Records (EHRs) and its implications in healthcare. LLMs offer benefits such as precise control over data schema and improved fairness in data representation. However, a significant challenge lies in ensuring that synthetic health records can generalize across different hospital settings. The research evaluates the current state of commercial LLMs, finding that while they can reliably generate synthetic data for smaller feature sets, they struggle with realistic distributions and correlations as the data dimensionality increases. This limitation hinders their ability to generalize effectively across diverse hospitals, highlighting the need for further improvements in LLMs for generating synthetic EHRs in healthcare. <br /><br /> <div>
arXiv:2504.14657v1 Announce Type: new 
Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to create privacy preserving and harmonized structured data, supporting numerous applications in healthcare. Key benefits of synthetic data include precise control over the data schema, improved fairness and representation of patient populations, and the ability to share datasets without concerns about compromising real individuals privacy. Consequently, the AI community has increasingly turned to Large Language Models (LLMs) to generate synthetic data across various domains. However, a significant challenge in healthcare is ensuring that synthetic health records reliably generalize across different hospitals, a long standing issue in the field. In this work, we evaluate the current state of commercial LLMs for generating synthetic data and investigate multiple aspects of the generation process to identify areas where these models excel and where they fall short. Our main finding from this work is that while LLMs can reliably generate synthetic health records for smaller subsets of features, they struggle to preserve realistic distributions and correlations as the dimensionality of the data increases, ultimately limiting their ability to generalize across diverse hospital settings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data</title>
<link>https://arxiv.org/abs/2504.14669</link>
<guid>https://arxiv.org/abs/2504.14669</guid>
<content:encoded><![CDATA[
<div> self-play, multilingual machine translation, large language models, Genetic Monte-Carlo Tree Search, supervised fine-tuning
Summary:<br /><br />The article introduces TRANS-ZERO, a self-play framework for multilingual machine translation that utilizes only monolingual data and the inherent multilingual knowledge of Large Language Models (LLMs). By combining Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, TRANS-ZERO achieves strong translation performance that rivals supervised methods, even surpassing them in non-English translation directions. The framework demonstrates the ability to match the performance of models trained on large-scale parallel data while enhancing translation quality through the exploration of semantically consistent candidates via G-MCTS. This approach addresses challenges such as data scarcity for low-resource languages and catastrophic forgetting in multilingual MT, providing a robust alternative to traditional supervised fine-tuning methods. <div>
arXiv:2504.14669v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting. To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods. Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions. Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework's succuss.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models</title>
<link>https://arxiv.org/abs/2504.14690</link>
<guid>https://arxiv.org/abs/2504.14690</guid>
<content:encoded><![CDATA[
<div> Evaluation, Large Language Models, Persian, Benchmark, FarsEval-PKBETS <br />
Summary: 
Research on evaluating large language models in Persian has been limited compared to resource-rich languages like English. FarsEval-PKBETS, a subset of the FarsEval project, introduces a benchmark of 4000 questions in various formats covering diverse domains. Three models were evaluated on this benchmark, with an average accuracy of below 50%, indicating current language models struggle with Persian language tasks. The benchmark incorporates cultural and linguistic considerations specific to Persian and Iran. <div>
arXiv:2504.14690v1 Announce Type: new 
Abstract: Research on evaluating and analyzing large language models (LLMs) has been extensive for resource-rich languages such as English, yet their performance in languages such as Persian has received considerably less attention. This paper introduces FarsEval-PKBETS benchmark, a subset of FarsEval project for evaluating large language models in Persian. This benchmark consists of 4000 questions and answers in various formats, including multiple choice, short answer and descriptive responses. It covers a wide range of domains and tasks,including medicine, law, religion, Persian language, encyclopedic knowledge, human preferences, social knowledge, ethics and bias, text generation, and respecting others' rights. This bechmark incorporates linguistics, cultural, and local considerations relevant to the Persian language and Iran. To ensure the questions are challenging for current LLMs, three models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this benchmark. Their average accuracy was below 50%, meaning they provided fully correct answers to fewer than half of the questions. These results indicate that current language models are still far from being able to solve this benchmark
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding</title>
<link>https://arxiv.org/abs/2504.14692</link>
<guid>https://arxiv.org/abs/2504.14692</guid>
<content:encoded><![CDATA[
<div> Keywords: medical vision-language models, multimodal understanding, rotary position-adaptive encoder, token pruning mechanism, state-of-the-art performance

Summary:
OmniV-Med introduces a unified framework for multimodal medical understanding by creating a comprehensive dataset and developing innovative techniques. The OmniV-Med-Instruct dataset includes a wide range of medical modalities and tasks. A rotary position-adaptive encoder is designed to process various image resolutions and types within a single architecture, moving away from traditional modality-specific encoders. Additionally, a medical-aware token pruning mechanism helps reduce visual tokens in volumetric data and medical videos without sacrificing performance. Empirical evaluations show that OmniV-Med achieves state-of-the-art results on multiple medical imaging and video tasks. Even a lightweight variant of the model, OmniV-Med-1.5B, maintains high performance with fewer resources, making it efficient for training and inference. The dataset, code, and models will be made publicly available for further research and application. 

Summary: <div>
arXiv:2504.14692v1 Announce Type: new 
Abstract: The practical deployment of medical vision-language models (Med-VLMs) necessitates seamless integration of textual data with diverse visual modalities, including 2D/3D images and videos, yet existing models typically employ separate encoders for different modalities. To address this limitation, we present OmniV-Med, a unified framework for multimodal medical understanding. Our technical contributions are threefold: First, we construct OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K instructional samples spanning 14 medical image modalities and 11 clinical tasks. Second, we devise a rotary position-adaptive encoder that processes multi-resolution 2D/3D images and videos within a unified architecture, diverging from conventional modality-specific encoders. Third, we introduce a medical-aware token pruning mechanism that exploits spatial-temporal redundancy in volumetric data (e.g., consecutive CT slices) and medical videos, effectively reducing 60\% of visual tokens without performance degradation. Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art performance on 7 benchmarks spanning 2D/3D medical imaging and video understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains comparable performance while requiring only 8 RTX3090 GPUs for training and supporting efficient long-video inference. Data, code and model will be released.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives</title>
<link>https://arxiv.org/abs/2504.14707</link>
<guid>https://arxiv.org/abs/2504.14707</guid>
<content:encoded><![CDATA[
<div> BERTopic, LDA, KMeans, Belgian Dutch daily narratives, contextual embeddings <br />
Summary: This study compares BERTopic, LDA, and KMeans for modeling Belgian Dutch daily narratives. LDA performs well on automated metrics but shows semantically irrelevant co-occurrences, indicating limitations. BERTopic, leveraging contextual embeddings, captures culturally resonant themes, highlighting the importance of hybrid evaluation frameworks in morphologically rich languages. KMeans, contrary to prior studies, performs less coherently in personal narratives. The study underscores the necessity of robust generalization in NLP models, particularly in underrepresented linguistic contexts. <br /> <div>
arXiv:2504.14707v1 Announce Type: new 
Abstract: This study explores BERTopic's potential for modeling open-ended Belgian Dutch daily narratives, contrasting its performance with Latent Dirichlet Allocation (LDA) and KMeans. Although LDA scores well on certain automated metrics, human evaluations reveal semantically irrelevant co-occurrences, highlighting the limitations of purely statistic-based methods. In contrast, BERTopic's reliance on contextual embeddings yields culturally resonant themes, underscoring the importance of hybrid evaluation frameworks that account for morphologically rich languages. KMeans performed less coherently than prior research suggested, pointing to the unique challenges posed by personal narratives. Our findings emphasize the need for robust generalization in NLP models, especially in underrepresented linguistic contexts.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines</title>
<link>https://arxiv.org/abs/2504.14738</link>
<guid>https://arxiv.org/abs/2504.14738</guid>
<content:encoded><![CDATA[
<div> dataset, language models, assertions, pipeline, reliability  
Summary:  
The paper introduces PROMPTEVALS, a dataset comprising 2087 language model pipeline prompts with 12623 corresponding assertion criteria, aimed at improving reliability in large language model applications. Determining the right set of assertions for tasks is crucial in enhancing model performance. The dataset, larger than previous collections, serves as a benchmark for evaluating closed- and open-source models in generating relevant assertions. The fine-tuned Mistral and Llama 3 models displayed better performance compared to GPT-4o, with a 20.93% average improvement, offering both reduced latency and enhanced performance. The dataset is expected to drive further research in the areas of language model reliability, alignment, and prompt engineering.  
<br /><br />Summary: <div>
arXiv:2504.14738v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains -- such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using our open-source LLM pipeline tools. This dataset is 5x larger than previous collections. Using a hold-out test split of PROMPTEVALS as a benchmark, we evaluated closed- and open-source models in generating relevant assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. We believe our dataset can spur further research in LLM reliability, alignment, and prompt engineering.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings</title>
<link>https://arxiv.org/abs/2504.14766</link>
<guid>https://arxiv.org/abs/2504.14766</guid>
<content:encoded><![CDATA[
<div> Keywords: neural embeddings, BERT, linguistic properties, interpretability, language models

Summary: 
The paper introduces a framework for understanding neural embeddings, like those in BERT, by uncovering the dimensions that encode distinct linguistic properties (LPs). It presents the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, focusing on key features such as synonymy, negation, tense, and quantity. Using various analysis methods, the study identifies the most influential dimensions for each LP, introducing the Embedding Dimension Impact (EDI) score to quantify dimension relevance. Results reveal robust encoding of properties like negation and polarity in specific dimensions, while synonymy shows more complex patterns. The findings provide insights into embedding interpretability, guiding the development of transparent and optimized language models. This has implications for addressing model bias and ensuring responsible AI system deployment.

<br /><br />Summary: <div>
arXiv:2504.14766v1 Announce Type: new 
Abstract: Understanding the inner workings of neural embeddings, particularly in models such as BERT, remains a challenge because of their high-dimensional and opaque nature. This paper proposes a framework for uncovering the specific dimensions of vector embeddings that encode distinct linguistic properties (LPs). We introduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which isolates ten key linguistic features such as synonymy, negation, tense, and quantity. Using this dataset, we analyze BERT embeddings with various methods, including the Wilcoxon signed-rank test, mutual information, and recursive feature elimination, to identify the most influential dimensions for each LP. We introduce a new metric, the Embedding Dimension Impact (EDI) score, which quantifies the relevance of each embedding dimension to a LP. Our findings show that certain properties, such as negation and polarity, are robustly encoded in specific dimensions, while others, like synonymy, exhibit more complex patterns. This study provides insights into the interpretability of embeddings, which can guide the development of more transparent and optimized language models, with implications for model bias mitigation and the responsible deployment of AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2504.14772</link>
<guid>https://arxiv.org/abs/2504.14772</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Dataset Distillation, Large Language Models, Compression, Scalability

Summary:
This survey explores the strategies of Knowledge Distillation (KD) and Dataset Distillation (DD) for compressing Large Language Models (LLMs) while maintaining their reasoning abilities and linguistic diversity. It covers methodologies in KD such as task-specific alignment and multi-teacher frameworks, as well as DD techniques like optimization-based gradient matching and generative synthesis. The integration of KD and DD is discussed to enhance compression strategies. Applications in healthcare and education are highlighted for efficient deployment. Challenges include preserving reasoning and diversity, adapting to evolving models and datasets, and establishing evaluation protocols. The survey aims to guide the development of sustainable LLMs by combining KD and DD principles.<br /><br />Summary: <div>
arXiv:2504.14772v1 Announce Type: new 
Abstract: The exponential growth of Large Language Models (LLMs) continues to highlight the need for efficient strategies to meet ever-expanding computational and data demands. This survey provides a comprehensive analysis of two complementary paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both aimed at compressing LLMs while preserving their advanced reasoning capabilities and linguistic diversity. We first examine key methodologies in KD, such as task-specific alignment, rationale-based training, and multi-teacher frameworks, alongside DD techniques that synthesize compact, high-impact datasets through optimization-based gradient matching, latent space regularization, and generative synthesis. Building on these foundations, we explore how integrating KD and DD can produce more effective and scalable compression strategies. Together, these approaches address persistent challenges in model scalability, architectural heterogeneity, and the preservation of emergent LLM abilities. We further highlight applications across domains such as healthcare and education, where distillation enables efficient deployment without sacrificing performance. Despite substantial progress, open challenges remain in preserving emergent reasoning and linguistic diversity, enabling efficient adaptation to continually evolving teacher models and datasets, and establishing comprehensive evaluation protocols. By synthesizing methodological innovations, theoretical foundations, and practical insights, our survey charts a path toward sustainable, resource-efficient LLMs through the tighter integration of KD and DD principles.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends</title>
<link>https://arxiv.org/abs/2504.14804</link>
<guid>https://arxiv.org/abs/2504.14804</guid>
<content:encoded><![CDATA[
<div> Evaluation, document-level translation, automatic evaluation metrics, large language models, future trends <br />
<br />
Summary: 
The paper discusses the progress in machine translation, particularly in document-level translation facilitated by large language models (LLMs). Evaluation of document-level translation is crucial, with automatic evaluation metrics playing a key role. Current evaluation schemes and metrics are analyzed, including those with and without reference texts, traditional metrics, and LLM-based metrics. Challenges in current evaluation methods are identified, such as lack of reference diversity and dependency on sentence-level alignment. Future trends in evaluation methods are explored, including the need for user-friendly evaluation methods and robust LLM-based methods. Proposed research directions include reducing dependency on sentence-level information, introducing multi-level evaluation approaches, and training models for machine translation evaluation. The study aims to provide a comprehensive analysis of automatic evaluation for document-level translation and suggests potential advancements in the field. <div>
arXiv:2504.14804v1 Announce Type: new 
Abstract: With the rapid development of deep learning technologies, the field of machine translation has witnessed significant progress, especially with the advent of large language models (LLMs) that have greatly propelled the advancement of document-level translation. However, accurately evaluating the quality of document-level translation remains an urgent issue. This paper first introduces the development status of document-level translation and the importance of evaluation, highlighting the crucial role of automatic evaluation metrics in reflecting translation quality and guiding the improvement of translation systems. It then provides a detailed analysis of the current state of automatic evaluation schemes and metrics, including evaluation methods with and without reference texts, as well as traditional metrics, Model-based metrics and LLM-based metrics. Subsequently, the paper explores the challenges faced by current evaluation methods, such as the lack of reference diversity, dependence on sentence-level alignment information, and the bias, inaccuracy, and lack of interpretability of the LLM-as-a-judge method. Finally, the paper looks ahead to the future trends in evaluation methods, including the development of more user-friendly document-level evaluation methods and more robust LLM-as-a-judge methods, and proposes possible research directions, such as reducing the dependency on sentence-level information, introducing multi-level and multi-granular evaluation approaches, and training models specifically for machine translation evaluation. This study aims to provide a comprehensive analysis of automatic evaluation for document-level translation and offer insights into future developments.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Self-improving Token Embeddings</title>
<link>https://arxiv.org/abs/2504.14808</link>
<guid>https://arxiv.org/abs/2504.14808</guid>
<content:encoded><![CDATA[
<div> Keywords: token embeddings, out-of-vocabulary problem, corpus exploration, conceptual search, word sense disambiguation

Summary:
This article presents a new method for refining pre-trained token embeddings by incorporating neighboring tokens in text corpora. It continuously updates the representation of each token, addressing the out-of-vocabulary problem. Operating independently of large language models, the approach allows for versatile applications including corpus exploration, conceptual search, and word sense disambiguation. It is designed for topically homogeneous corpora, resulting in more meaningful embeddings. The methodology is applied to analyze storm events and their impacts using narratives from the NOAA Storm Events database, improving the representation of storm-related terms over time. This provides insights into the evolving nature of disaster narratives and showcases the effectiveness of the method in enhancing token representations within specific domains. 

<br /><br />Summary: <div>
arXiv:2504.14808v1 Announce Type: new 
Abstract: This article introduces a novel and fast method for refining pre-trained static word or, more generally, token embeddings. By incorporating the embeddings of neighboring tokens in text corpora, it continuously updates the representation of each token, including those without pre-assigned embeddings. This approach effectively addresses the out-of-vocabulary problem, too. Operating independently of large language models and shallow neural networks, it enables versatile applications such as corpus exploration, conceptual search, and word sense disambiguation. The method is designed to enhance token representations within topically homogeneous corpora, where the vocabulary is restricted to a specific domain, resulting in more meaningful embeddings compared to general-purpose pre-trained vectors. As an example, the methodology is applied to explore storm events and their impacts on infrastructure and communities using narratives from a subset of the NOAA Storm Events database. The article also demonstrates how the approach improves the representation of storm-related terms over time, providing valuable insights into the evolving nature of disaster narratives.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation</title>
<link>https://arxiv.org/abs/2504.14856</link>
<guid>https://arxiv.org/abs/2504.14856</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, citation generation, trustworthiness, internal knowledge, retrieval quality 

Summary: 
RAEL, a paradigm introduced in this work, focuses on Context-Prior Augmented Citation Generation task, aiming to generate trustworthy references by utilizing both external and internal knowledge. The INTRALIGN method, incorporating data generation and alignment algorithms, outperforms other baselines in cross-scenario performance. Experimental results highlight the impact of retrieval quality, question types, and model knowledge on citation trustworthiness. The study addresses the opacity in how language models utilize internal knowledge and aims to improve the reliability of generated answers through a comprehensive evaluation framework. <div>
arXiv:2504.14856v1 Announce Type: new 
Abstract: While hallucinations of large language models could been alleviated through retrieval-augmented generation and citation generation, how the model utilizes internal knowledge is still opaque, and the trustworthiness of its generated answers remains questionable. In this work, we introduce Context-Prior Augmented Citation Generation task, requiring models to generate citations considering both external and internal knowledge while providing trustworthy references, with 5 evaluation metrics focusing on 3 aspects: answer helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the paradigm for our task, and also design INTRALIGN, an integrated method containing customary data generation and an alignment algorithm. Our experimental results show that our method achieves a better cross-scenario performance with regard to other baselines. Our extended experiments further reveal that retrieval quality, question types, and model knowledge have considerable influence on the trustworthiness in citation generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Fingerprints of Large Language Models</title>
<link>https://arxiv.org/abs/2504.14871</link>
<guid>https://arxiv.org/abs/2504.14871</guid>
<content:encoded><![CDATA[
<div> biases, language models, training data, natural fingerprints, unintended characteristics

Summary: 
Large language models (LLMs) often produce biased outputs, ranging from unfair responses to subtle patterns that can identify the model. This study investigates factors leading to identifiable characteristics in LLMs. Despite being trained on the same data, LLMs can still be distinguished based on their generated text, resulting in natural fingerprints. The study shows that differences in training conditions, such as parameter sizes, optimization settings, and random seeds, contribute to these unintended characteristics. Understanding natural fingerprints can provide insights into the origins of bias in LLMs and help improve control over their behavior. <div>
arXiv:2504.14871v1 Announce Type: new 
Abstract: Large language models (LLMs) often exhibit biases -- systematic deviations from expected norms -- in their outputs. These range from overt issues, such as unfair responses, to subtler patterns that can reveal which model produced them. We investigate the factors that give rise to identifiable characteristics in LLMs. Since LLMs model training data distribution, it is reasonable that differences in training data naturally lead to the characteristics. However, our findings reveal that even when LLMs are trained on the exact same data, it is still possible to distinguish the source model based on its generated text. We refer to these unintended, distinctive characteristics as natural fingerprints. By systematically controlling training conditions, we show that the natural fingerprints can emerge from subtle differences in the training process, such as parameter sizes, optimization settings, and even random seeds. We believe that understanding natural fingerprints offers new insights into the origins of unintended bias and ways for improving control over LLM behavior.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2504.14891</link>
<guid>https://arxiv.org/abs/2504.14891</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Large Language Models, evaluation methods, RAG-specific datasets, meta-analysis

<br />
Summary:
This paper presents a comprehensive survey of Retrieval-Augmented Generation (RAG) evaluation methods and frameworks in the context of Large Language Models (LLMs). The unique challenges of evaluating RAG systems, which combine retrieval and generation components, are addressed, as well as their reliance on dynamic knowledge sources. The survey includes traditional and emerging evaluation approaches for system performance, factual accuracy, safety, and computational efficiency in the LLM era. RAG-specific datasets and evaluation frameworks are compiled and categorized, with a meta-analysis of evaluation practices in significant RAG research conducted. This work aims to bridge traditional evaluation methods with those driven by LLMs, serving as a vital resource for the advancement of RAG development.

<br /><br />Summary: <div>
arXiv:2504.14891v1 Announce Type: new 
Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have revolutionized natural language processing by integrating Large Language Models (LLMs) with external information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. However, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, factual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most comprehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs</title>
<link>https://arxiv.org/abs/2504.14905</link>
<guid>https://arxiv.org/abs/2504.14905</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, automated claim verification, large language models, conflicting reasoning, evidence retrieval

Summary: 

The article proposes a Conflicting Reasoning Approach for explainable claim Verification, called CRAVE, to address the challenges of verifying complex claims in the age of digital misinformation. The framework consists of three modules: Ambiguity Elimination enhanced Evidence Retrieval, Conflicting Perspective Reasoning and Preliminary Judgment using large language models (LLMs), and Small Language Model (SLM) based Judge. CRAVE utilizes LLMs to reason conflicting rationales based on evidence from external sources, across dimensions such as direct evidence, semantic relationships, linguistic patterns, and logical reasoning. The methodology enables the model to capture subtle inconsistencies in complex claims and improve both accuracy and transparency in claim verification. Experimental results on public datasets show that CRAVE outperforms state-of-the-art methods in finding relevant evidence and explaining predictions, showcasing its effectiveness in automated claim verification. The source code is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2504.14905v1 Announce Type: new 
Abstract: The rapid spread of misinformation, driven by digital media and AI-generated content, has made automatic claim verification essential. Traditional methods, which depend on expert-annotated evidence, are labor-intensive and not scalable. Although recent automated systems have improved, they still struggle with complex claims that require nuanced reasoning. To address this, we propose CRAVE, a Conflicting Reasoning Approach for explainable claim VErification, that verify the complex claims based on the conflicting rationales reasoned by large language models (LLMs). Specifically, CRAVE introduces a three-module framework. Ambiguity Elimination enchanced Evidence Retrieval module performs ambiguity elimination and entity-based search to gather relevant evidence related to claim verification from external sources like Wikipedia. Conflicting Perspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to reason rationales with conflicting stances about claim verification from retrieved evidence across four dimensions, i.e., direct evidence, semantic relationships, linguistic patterns, and logical reasoning and make a preliminary judgment. Finally, Small Language Model (SLM) based Judge module is fine-tuned to make use of preliminary judgment from LLMs to assess the confidence of the conflicting rationales and make a final authenticity judgment. This methodology allows CRAVE to capture subtle inconsistencies in complex claims, improving both the accuracy and transparency of claim verification. Extensive experiments on two public claim verification datasets demonstrate that our CRAVE model achieves much better performance than state-of-the-art methods and exhibits a superior capacity for finding relevant evidence and explaining the model predictions. The code is provided at https://github.com/8zym/CRAVE.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues</title>
<link>https://arxiv.org/abs/2504.14963</link>
<guid>https://arxiv.org/abs/2504.14963</guid>
<content:encoded><![CDATA[
<div> Keywords: speaker identification, text-based, fuzzy fingerprints, pre-trained models, context-aware modeling

Summary: 
This article explores the use of fuzzy fingerprints from large pre-trained models to enhance text-based speaker identification. By incorporating speaker-specific tokens and context-aware modeling, the research demonstrates that considering conversational context can significantly improve accuracy, achieving 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. The study also shows that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering better interpretability. Furthermore, the researchers analyze ambiguous utterances and suggest a mechanism to identify speaker-agnostic lines. These findings shed light on the challenges and provide valuable insights for enhancing text-based speaker identification methodologies.<br /><br />Summary: <div>
arXiv:2504.14963v1 Announce Type: new 
Abstract: Speaker identification using voice recordings leverages unique acoustic features, but this approach fails when only textual data is available. Few approaches have attempted to tackle the problem of identifying speakers solely from text, and the existing ones have primarily relied on traditional methods. In this work, we explore the use of fuzzy fingerprints from large pre-trained models to improve text-based speaker identification. We integrate speaker-specific tokens and context-aware modeling, demonstrating that conversational context significantly boosts accuracy, reaching 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering improved interpretability. Finally, we analyze ambiguous utterances and propose a mechanism to detect speaker-agnostic lines. Our findings highlight key challenges and provide insights for future improvements in text-based speaker identification.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)</title>
<link>https://arxiv.org/abs/2504.14969</link>
<guid>https://arxiv.org/abs/2504.14969</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Chinese topic constructions, island constraints, Mandarin syntax, experimental design

Summary:
Large language models (LLMs) are being evaluated for their performance on Chinese topic constructions and sensitivity to island constraints in this proposed framework. Inspired by previous work, an experimental design is outlined to test LLMs' knowledge of Mandarin syntax. While no experiments have been conducted yet, this proposal sets the stage for future studies in this area. Feedback on the methodology is encouraged to further refine and improve the framework for evaluating LLMs' grammatical understanding of Chinese syntax. This paper aims to lay the groundwork for future research on this topic and seeks input from the academic community to enhance the experimental design and methodology for evaluating LLMs on Chinese language tasks. 

<br /><br />Summary: 
- Proposed framework for evaluating LLMs on Chinese topic constructions
- Focus on sensitivity to island constraints in Mandarin syntax
- Experimental design outlined for testing LLMs' knowledge of Mandarin grammar
- No experiments conducted yet, seeking feedback on proposed methodology
- Aims to provide foundation for future studies and improvements on LLM evaluation in Chinese language tasks. <div>
arXiv:2504.14969v1 Announce Type: new 
Abstract: This paper proposes a framework for evaluating large language models (LLMs) on Chinese topic constructions, focusing on their sensitivity to island constraints. Drawing inspiration from Tian et al. (2024), we outline an experimental design for testing LLMs' grammatical knowledge of Mandarin syntax. While no experiments have been conducted yet, this proposal aims to provide a foundation for future studies and invites feedback on the methodology.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Pretraining Length Scaling</title>
<link>https://arxiv.org/abs/2504.14992</link>
<guid>https://arxiv.org/abs/2504.14992</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, length scaling, pre-training, Parallel Hidden Decoding Transformer, KV cache management

Summary: 
The article introduces the Parallel Hidden Decoding Transformer (PHD-Transformer), a framework for efficient length scaling during pre-training in large language models. It utilizes a novel KV cache management strategy to distinguish between original tokens and hidden decoding tokens, optimizing long-range dependencies. Two optimized variants, PHD-SWA and PHD-CSWA, enhance performance by preserving local dependencies and eliminating linear growth in pre-filling time. Extensive experiments showcase consistent improvements across various benchmarks, highlighting the effectiveness of the PHD-Transformer in maintaining inference efficiency while enabling effective length scaling. <div>
arXiv:2504.14992v1 Announce Type: new 
Abstract: Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (\textit{PHD}-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. \textit{PHD}-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: \textit{PHD-SWA} employs sliding window attention to preserve local dependencies, while \textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs</title>
<link>https://arxiv.org/abs/2504.15013</link>
<guid>https://arxiv.org/abs/2504.15013</guid>
<content:encoded><![CDATA[
arXiv:2504.15013v1 Announce Type: new 
Abstract: The process of creating educational materials is both time-consuming and demanding for educators. This research explores the potential of Large Language Models (LLMs) to streamline this task by automating the generation of extended reading materials and relevant course suggestions. Using the TED-Ed Dig Deeper sections as an initial exploration, we investigate how supplementary articles can be enriched with contextual knowledge and connected to additional learning resources. Our method begins by generating extended articles from video transcripts, leveraging LLMs to include historical insights, cultural examples, and illustrative anecdotes. A recommendation system employing semantic similarity ranking identifies related courses, followed by an LLM-based refinement process to enhance relevance. The final articles are tailored to seamlessly integrate these recommendations, ensuring they remain cohesive and informative. Experimental evaluations demonstrate that our model produces high-quality content and accurate course suggestions, assessed through metrics such as Hit Rate, semantic similarity, and coherence. Our experimental analysis highlight the nuanced differences between the generated and existing materials, underscoring the model's capacity to offer more engaging and accessible learning experiences. This study showcases how LLMs can bridge the gap between core content and supplementary learning, providing students with additional recommended resources while also assisting teachers in designing educational materials.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as Data Annotators: How Close Are We to Human Performance</title>
<link>https://arxiv.org/abs/2504.15022</link>
<guid>https://arxiv.org/abs/2504.15022</guid>
<content:encoded><![CDATA[
arXiv:2504.15022v1 Announce Type: new 
Abstract: In NLP, fine-tuning LLMs is effective for various applications but requires high-quality annotated data. However, manual annotation of data is labor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly used to automate the process, often employing in-context learning (ICL) in which some examples related to the task are given in the prompt for better performance. However, manually selecting context examples can lead to inefficiencies and suboptimal model performance. This paper presents comprehensive experiments comparing several LLMs, considering different embedding models, across various datasets for the Named Entity Recognition (NER) task. The evaluation encompasses models with approximately $7$B and $70$B parameters, including both proprietary and non-proprietary models. Furthermore, leveraging the success of Retrieval-Augmented Generation (RAG), it also considers a method that addresses the limitations of ICL by automatically retrieving contextual examples, thereby enhancing performance. The results highlight the importance of selecting the appropriate LLM and embedding model, understanding the trade-offs between LLM sizes and desired performance, and the necessity to direct research efforts towards more challenging datasets.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models</title>
<link>https://arxiv.org/abs/2504.15027</link>
<guid>https://arxiv.org/abs/2504.15027</guid>
<content:encoded><![CDATA[
arXiv:2504.15027v1 Announce Type: new 
Abstract: Enhancing computational efficiency and reducing deployment costs for large language models (LLMs) have become critical challenges in various resource-constrained scenarios. In this work, we present DistilQwen2.5, a family of distilled, lightweight LLMs derived from the public Qwen2.5 models. These distilled models exhibit enhanced instruction-following capabilities compared to the original models based on a series of distillation techniques that incorporate knowledge from much larger LLMs. In our industrial practice, we first leverage powerful proprietary LLMs with varying capacities as multi-agent teachers to select, rewrite, and refine instruction-response pairs that are more suitable for student LLMs to learn. After standard fine-tuning, we further leverage a computationally efficient model fusion approach that enables student models to progressively integrate fine-grained hidden knowledge from their teachers. Experimental evaluations demonstrate that the distilled models possess significantly stronger capabilities than their original checkpoints. Additionally, we present use cases to illustrate the applications of our framework in real-world scenarios. To facilitate practical use, we have released all the DistilQwen2.5 models to the open-source community.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search</title>
<link>https://arxiv.org/abs/2504.15047</link>
<guid>https://arxiv.org/abs/2504.15047</guid>
<content:encoded><![CDATA[
arXiv:2504.15047v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score $\approx 0.84$), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT</title>
<link>https://arxiv.org/abs/2504.15052</link>
<guid>https://arxiv.org/abs/2504.15052</guid>
<content:encoded><![CDATA[
arXiv:2504.15052v1 Announce Type: new 
Abstract: This study investigates the capabilities of large language models (LLMs), specifically ChatGPT, in annotating MT outputs based on an error typology. In contrast to previous work focusing mainly on general language, we explore ChatGPT's ability to identify and categorise errors in specialised translations. By testing two different prompts and based on a customised error typology, we compare ChatGPT annotations with human expert evaluations of translations produced by DeepL and ChatGPT itself. The results show that, for translations generated by DeepL, recall and precision are quite high. However, the degree of accuracy in error categorisation depends on the prompt's specific features and its level of detail, ChatGPT performing very well with a detailed prompt. When evaluating its own translations, ChatGPT achieves significantly poorer results, revealing limitations with self-assessment. These results highlight both the potential and the limitations of LLMs for translation evaluation, particularly in specialised domains. Our experiments pave the way for future research on open-source LLMs, which could produce annotations of comparable or even higher quality. In the future, we also aim to test the practical effectiveness of this automated evaluation in the context of translation training, particularly by optimising the process of human evaluation by teachers and by exploring the impact of annotations by LLMs on students' post-editing and translation learning.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2504.15093</link>
<guid>https://arxiv.org/abs/2504.15093</guid>
<content:encoded><![CDATA[
arXiv:2504.15093v1 Announce Type: new 
Abstract: Detecting collaborative and problem-solving behaviours from digital traces to interpret students' collaborative problem solving (CPS) competency is a long-term goal in the Artificial Intelligence in Education (AIEd) field. Although multimodal data and advanced models are argued to have the potential to detect complex CPS behaviours, empirical evidence on their value remains limited with some contrasting evidence. In this study, we investigated the potential of multimodal data to improve model performance in diagnosing 78 secondary school students' CPS subskills and indicators in authentic educational settings. In particular, text embeddings from verbal data and acoustic embeddings from audio data were used in a multimodal classification model for CPS diagnosis. Both unimodal and multimodal transformer-based models outperformed traditional models in detecting CPS classes. Although the inclusion of multimodality did not improve the performance of traditional unimodal models, its integration into transformer-based models demonstrated improved performance for diagnosing social-cognitive CPS classes compared to unimodal transformer-based models. Based on the results, the paper argues that multimodality and the selection of a particular modelling technique should not be taken for granted to achieve the best performance in the automated detection of every CPS subskill and indicator. Rather, their value is limited to certain types of CPS indicators, affected by the complexity of the labels, and dependent on the composition of indicators in the dataset. We conclude the paper by discussing the required nuance when considering the value of LLMs and multimodality in automated CPS diagnosis, highlighting the need for human-AI complementarity, and proposing the exploration of relevant model architectures and techniques to improve CPS diagnosis in authentic educational contexts.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kuwain 1.5B: An Arabic SLM via Language Injection</title>
<link>https://arxiv.org/abs/2504.15120</link>
<guid>https://arxiv.org/abs/2504.15120</guid>
<content:encoded><![CDATA[
arXiv:2504.15120v1 Announce Type: new 
Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model's existing knowledge with a minimum amount of the original model's data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models</title>
<link>https://arxiv.org/abs/2504.15133</link>
<guid>https://arxiv.org/abs/2504.15133</guid>
<content:encoded><![CDATA[
arXiv:2504.15133v1 Announce Type: new 
Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks</title>
<link>https://arxiv.org/abs/2504.15160</link>
<guid>https://arxiv.org/abs/2504.15160</guid>
<content:encoded><![CDATA[
arXiv:2504.15160v1 Announce Type: new 
Abstract: Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa, require that all categories in an annotation task be sufficiently represented in the training data for optimal performance. However, it is often difficult to find sufficient examples for all categories in a task when building a high-quality training set. In this article, I describe this problem and propose a solution, the synthetic imputation approach. Leveraging a generative LLM (GPT-4o), this approach generates synthetic texts based on careful prompting and five original examples drawn randomly with replacement from the sample. This approach ensures that new synthetic texts are sufficiently different from the original texts to reduce overfitting, but retain the underlying substantive meaning of the examples to maximize out-of-sample performance. With 75 original examples or more, synthetic imputation's performance is on par with a full sample of original texts, and overfitting remains low, predictable and correctable with 50 original samples. The synthetic imputation approach provides a novel role for generative LLMs in research and allows applied researchers to balance their datasets for best performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On true empty category</title>
<link>https://arxiv.org/abs/2504.15168</link>
<guid>https://arxiv.org/abs/2504.15168</guid>
<content:encoded><![CDATA[
arXiv:2504.15168v1 Announce Type: new 
Abstract: According to Chomsky (1981, 1986), empty categories consist of PRO, pro, trace, and variable. However, some empty object positions seem to be incompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014) and Li & Wei (2014) raise the true empty category hypothesis, which holds that true empty category is only an empty position with category and Case features. As a last resort option, it is used mainly to meet the subcatgorization of a verb. This assumption is ingenious, and if proved to be true, it will exert a great impact on the study of UG. In this paper, we evaluate their evidence from topicalization and demonstrate that it can be accounted for without invoking true empty category.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges</title>
<link>https://arxiv.org/abs/2504.15205</link>
<guid>https://arxiv.org/abs/2504.15205</guid>
<content:encoded><![CDATA[
arXiv:2504.15205v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to generate answers with citations from source documents containing "ground truth", thereby reducing system hallucinations. A crucial factor in RAG evaluation is "support", whether the information in the cited documents supports the answer. To this end, we conducted a large-scale comparative study of 45 participant submissions on 36 topics to the TREC 2024 RAG Track, comparing an automatic LLM judge (GPT-4o) against human judges for support assessment. We considered two conditions: (1) fully manual assessments from scratch and (2) manual assessments with post-editing of LLM predictions. Our results indicate that for 56% of the manual from-scratch assessments, human and GPT-4o predictions match perfectly (on a three-level scale), increasing to 72% in the manual with post-editing condition. Furthermore, by carefully analyzing the disagreements in an unbiased study, we found that an independent human judge correlates better with GPT-4o than a human judge, suggesting that LLM judges can be a reliable alternative for support assessment. To conclude, we provide a qualitative analysis of human and GPT-4o errors to help guide future iterations of support assessment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalAgent: Discovering Implicit Evaluation Criteria from the Web</title>
<link>https://arxiv.org/abs/2504.15219</link>
<guid>https://arxiv.org/abs/2504.15219</guid>
<content:encoded><![CDATA[
arXiv:2504.15219v1 Announce Type: new 
Abstract: Evaluation of language model outputs on structured writing tasks is typically conducted with a number of desirable criteria presented to human evaluators or large language models (LLMs). For instance, on a prompt like "Help me draft an academic talk on coffee intake vs research productivity", a model response may be evaluated for criteria like accuracy and coherence. However, high-quality responses should do more than just satisfy basic task requirements. An effective response to this query should include quintessential features of an academic talk, such as a compelling opening, clear research questions, and a takeaway. To help identify these implicit criteria, we introduce EvalAgent, a novel framework designed to automatically uncover nuanced and task-specific criteria. EvalAgent first mines expert-authored online guidance. It then uses this evidence to propose diverse, long-tail evaluation criteria that are grounded in reliable external sources. Our experiments demonstrate that the grounded criteria produced by EvalAgent are often implicit (not directly stated in the user's prompt), yet specific (high degree of lexical precision). Further, EvalAgent criteria are often not satisfied by initial responses but they are actionable, such that responses can be refined to satisfy them. Finally, we show that combining LLM-generated and EvalAgent criteria uncovers more human-valued criteria than using LLMs alone.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Bayesian Approaches to Topics over Time</title>
<link>https://arxiv.org/abs/2504.15220</link>
<guid>https://arxiv.org/abs/2504.15220</guid>
<content:encoded><![CDATA[
arXiv:2504.15220v1 Announce Type: new 
Abstract: The Topics over Time (ToT) model captures thematic changes in timestamped datasets by explicitly modeling publication dates jointly with word co-occurrence patterns. However, ToT was not approached in a fully Bayesian fashion, a flaw that makes it susceptible to stability problems. To address this issue, we propose a fully Bayesian Topics over Time (BToT) model via the introduction of a conjugate prior to the Beta distribution. This prior acts as a regularization that prevents the online version of the algorithm from unstable updates when a topic is poorly represented in a mini-batch. The characteristics of this prior to the Beta distribution are studied here for the first time. Still, this model suffers from a difference in scale between the single-time observations and the multiplicity of words per document. A variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a solution. In WBToT, publication dates are repeated a certain number of times per document, which balances the relative influence of words and timestamps along the inference process. We have tested our models on two datasets: a collection of over 200 years of US state-of-the-union (SOTU) addresses and a large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that WBToT captures events better than Latent Dirichlet Allocation and other SOTA topic models like BERTopic: the median absolute deviation of the topic presence over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also demonstrate the superior coherence of WBToT over BToT, which highlights the importance of balancing the time and word modalities. Finally, we illustrate the stability of the online optimization algorithm in WBToT, which allows the application of WBToT to problems that are intractable for standard ToT.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions</title>
<link>https://arxiv.org/abs/2504.15236</link>
<guid>https://arxiv.org/abs/2504.15236</guid>
<content:encoded><![CDATA[
arXiv:2504.15236v1 Announce Type: new 
Abstract: AI assistants can impart value judgments that shape people's decisions and worldviews, yet little is known empirically about what values these systems rely on in practice. To address this, we develop a bottom-up, privacy-preserving method to extract the values (normative considerations stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit in hundreds of thousands of real-world interactions. We empirically discover and taxonomize 3,307 AI values and study how they vary by context. We find that Claude expresses many practical and epistemic values, and typically supports prosocial human values while resisting values like "moral nihilism". While some values appear consistently across contexts (e.g. "transparency"), many are more specialized and context-dependent, reflecting the diversity of human interlocutors and their varied contexts. For example, "harm prevention" emerges when Claude resists users, "historical accuracy" when responding to queries about controversial events, "healthy boundaries" when asked for relationship advice, and "human agency" in technology ethics discussions. By providing the first large-scale empirical mapping of AI values in deployment, our work creates a foundation for more grounded evaluation and design of values in AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.15241</link>
<guid>https://arxiv.org/abs/2504.15241</guid>
<content:encoded><![CDATA[
arXiv:2504.15241v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators</title>
<link>https://arxiv.org/abs/2504.15253</link>
<guid>https://arxiv.org/abs/2504.15253</guid>
<content:encoded><![CDATA[
arXiv:2504.15253v1 Announce Type: new 
Abstract: Scaling test-time computation, or affording a generator large language model (LLM) extra compute during inference, typically employs the help of external non-generative evaluators (i.e., reward models). Concurrently, LLM-judges, models trained to generate evaluations and critiques (explanations) in natural language, are becoming increasingly popular in automatic evaluation. Despite judge empirical successes, their effectiveness as evaluators in test-time scaling settings is largely unknown. In this paper, we introduce the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge performance in three domains (math reasoning, code generation, and instruction following) under three task settings: response reranking, step-level beam search, and critique-based response refinement. We evaluate 10 different judge models (7B-70B parameters) for 8 different base generator models (6.7B-72B parameters). Our benchmark shows that while judges are competitive with outcome reward models in reranking, they are consistently worse than process reward models in beam search procedures. Furthermore, though unique to LLM-judges, their natural language critiques are currently ineffective in guiding the generator towards better responses.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution</title>
<link>https://arxiv.org/abs/2504.13847</link>
<guid>https://arxiv.org/abs/2504.13847</guid>
<content:encoded><![CDATA[
arXiv:2504.13847v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) offer unprecedented opportunities to enhance human-AI collaboration in qualitative research methods, including interviews. While interviews are highly valued for gathering deep, contextualized insights, interviewers often face significant cognitive challenges, such as real-time information processing, question adaptation, and rapport maintenance. My doctoral research introduces Interview AI-ssistant, a system designed for real-time interviewer-AI collaboration during both the preparation and execution phases. Through four interconnected studies, this research investigates the design of effective human-AI collaboration in interviewing contexts, beginning with a formative study of interviewers' needs, followed by a prototype development study focused on AI-assisted interview preparation, an experimental evaluation of real-time AI assistance during interviews, and a field study deploying the system in a real-world research setting. Beyond informing practical implementations of intelligent interview support systems, this work contributes to the Intelligent User Interfaces (IUI) community by advancing the understanding of human-AI collaborative interfaces in complex social tasks and establishing design guidelines for AI-enhanced qualitative research tools.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark</title>
<link>https://arxiv.org/abs/2504.13861</link>
<guid>https://arxiv.org/abs/2504.13861</guid>
<content:encoded><![CDATA[
arXiv:2504.13861v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for applications in telemedicine, yet their ability to engage with diverse patient behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source evaluation framework designed to assess LLM-driven medical consultations. Unlike existing benchmarks, 3MDBench simulates real-world patient variability by incorporating four temperament-driven Patient Agents and an Assessor Agent that evaluates diagnostic accuracy and dialogue quality. The benchmark integrates textual and image-based patient data across 34 common diagnoses, mirroring real-world telemedicine interactions. Under different diagnostic strategies, we evaluate state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings, underscoring the value of context-driven, information-seeking questioning. Additionally, we demonstrate that multimodal inputs enhance diagnostic efficiency. Image-supported models outperform text-only counterparts by raising the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting. Finally, we suggest an approach that improves the diagnostic F1-score to 70.3 by training the CNN model on the diagnosis prediction task and incorporating its top-3 predictions into the LVLM context. 3MDBench provides a reproducible and extendable evaluation framework for AI-driven medical assistants. It offers insights into how patient temperament, dialogue strategies, and multimodal reasoning influence diagnosis quality. By addressing real-world complexities in telemedicine, our benchmark paves the way for more empathetic, reliable, and context-aware AI-driven healthcare solutions. The source code of our benchmark is publicly available: https://github.com/univanxx/3mdbench
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on (M)LLM-Based GUI Agents</title>
<link>https://arxiv.org/abs/2504.13865</link>
<guid>https://arxiv.org/abs/2504.13865</guid>
<content:encoded><![CDATA[
arXiv:2504.13865v1 Announce Type: cross 
Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative paradigm in human-computer interaction, evolving from rule-based automation scripts to sophisticated AI-driven systems capable of understanding and executing complex interface operations. This survey provides a comprehensive examination of the rapidly advancing field of LLM-based GUI Agents, systematically analyzing their architectural foundations, technical components, and evaluation methodologies. We identify and analyze four fundamental components that constitute modern GUI Agents: (1) perception systems that integrate text-based parsing with multimodal understanding for comprehensive interface comprehension; (2) exploration mechanisms that construct and maintain knowledge bases through internal modeling, historical experience, and external information retrieval; (3) planning frameworks that leverage advanced reasoning methodologies for task decomposition and execution; and (4) interaction systems that manage action generation with robust safety controls. Through rigorous analysis of these components, we reveal how recent advances in large language models and multimodal learning have revolutionized GUI automation across desktop, mobile, and web platforms. We critically examine current evaluation frameworks, highlighting methodological limitations in existing benchmarks while proposing directions for standardization. This survey also identifies key technical challenges, including accurate element localization, effective knowledge retrieval, long-horizon planning, and safety-aware execution control, while outlining promising research directions for enhancing GUI Agents' capabilities. Our systematic review provides researchers and practitioners with a thorough understanding of the field's current state and offers insights into future developments in intelligent interface automation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation</title>
<link>https://arxiv.org/abs/2504.13882</link>
<guid>https://arxiv.org/abs/2504.13882</guid>
<content:encoded><![CDATA[
arXiv:2504.13882v1 Announce Type: cross 
Abstract: Our study introduces an automated system leveraging large language models (LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving effective praise, 2. reacting to errors, 3. determining what students know, 4. helping students manage inequity, and 5. responding to negative self-talk. Using a public dataset from the Teacher-Student Chatroom Corpus, our system classifies each tutoring strategy as either being employed as desired or undesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use of these strategies and analyze tutoring dialogues. The results show that for the five tutoring strategies, True Negative Rates (TNR) range from 0.655 to 0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is effective at excluding incorrect classifications but struggles to consistently identify the correct strategy. The strategy \textit{helping students manage inequity} showed the highest performance with a TNR of 0.738 and Recall of 0.432. The study highlights the potential of LLMs in tutoring strategy analysis and outlines directions for future improvements, including incorporating more advanced models for more nuanced feedback.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants</title>
<link>https://arxiv.org/abs/2504.13887</link>
<guid>https://arxiv.org/abs/2504.13887</guid>
<content:encoded><![CDATA[
arXiv:2504.13887v1 Announce Type: cross 
Abstract: Despite the growing integration of AI chatbots as conversational agents in public discourse, empirical evidence regarding their capacity to foster intercultural empathy remains limited. Using a randomized dialogue experiment, we examined how different types of AI chatbot interaction, i.e., deliberative versus non-deliberative and culturally aligned versus non-aligned, affect intercultural empathy across cultural groups. Results show that deliberative conversations increased intercultural empathy among American participants but not Latin American participants, who perceived AI responses as culturally inaccurate and failing to represent their cultural contexts and perspectives authentically. Real-time interaction analyses reveal that these differences stem from cultural knowledge gaps inherent in Large Language Models. Despite explicit prompting and instruction to represent cultural perspectives in participants' native languages, AI systems still exhibit significant disparities in cultural representation. This highlights the importance of designing AI systems capable of culturally authentic engagement in deliberative conversations. Our study contributes to deliberation theory and AI alignment research by underscoring AI's role in intercultural dialogue and the persistent challenge of representational asymmetry in democratic discourse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment</title>
<link>https://arxiv.org/abs/2504.13888</link>
<guid>https://arxiv.org/abs/2504.13888</guid>
<content:encoded><![CDATA[
arXiv:2504.13888v1 Announce Type: cross 
Abstract: Kanji script writing is a skill that is often introduced to novice Japanese foreign language students for achieving Japanese writing mastery, but often poses difficulties to students with primarily English fluency due to their its vast differences with written English. Instructors often introduce various pedagogical methods -- such as visual structure and written techniques -- to assist students in kanji study, but may lack availability providing direct feedback on students' writing outside of class. Current educational applications are also limited due to lacking richer instructor-emulated feedback. We introduce Kanji Workbook, a writing-based intelligent tutoring system for students to receive intelligent assessment that emulates human instructor feedback. Our interface not only leverages students' computing devices for allowing them to learn, practice, and review the writing of prompted characters from their course's kanji script lessons, but also provides a diverse set of writing assessment metrics -- derived from instructor interviews and classroom observation insights -- through intelligent scoring and visual animations. We deployed our interface onto novice- and intermediate-level university courses over an entire academic year, and observed that interface users on average achieved higher course grades than their peers and also reacted positively to our interface's various features.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches</title>
<link>https://arxiv.org/abs/2504.13890</link>
<guid>https://arxiv.org/abs/2504.13890</guid>
<content:encoded><![CDATA[
arXiv:2504.13890v1 Announce Type: cross 
Abstract: Computational mental health research develops models to predict and understand psychological phenomena, but often relies on inappropriate measures of psychopathology constructs, undermining validity. We identify three key issues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis) over validated ones (e.g., diagnosis by clinician); (2) treating mental health constructs as categorical rather than dimensional; and (3) focusing on disorder-specific constructs instead of transdiagnostic ones. We outline the benefits of using validated, dimensional, and transdiagnostic measures and offer practical recommendations for practitioners. Using valid measures that reflect the nature and structure of psychopathology is essential for computational mental health research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALLMesh: a simple application for performing Thematic Analysis with Large Language Models</title>
<link>https://arxiv.org/abs/2504.13892</link>
<guid>https://arxiv.org/abs/2504.13892</guid>
<content:encoded><![CDATA[
arXiv:2504.13892v1 Announce Type: cross 
Abstract: Thematic analysis (TA) is a widely used qualitative research method for identifying and interpreting patterns within textual data, such as qualitative interviews. Recent research has shown that it is possible to satisfactorily perform TA using Large Language Models (LLMs). This paper presents a novel application using LLMs to assist researchers in conducting TA. The application enables users to upload textual data, generate initial codes and themes. All of this is possible through a simple Graphical User Interface, (GUI) based on the streamlit framework, working with python scripts for the analysis, and using Application Program Interfaces of LLMs. Having a GUI is particularly important for researchers in fields where coding skills may not be prevalent, such as social sciences or humanities. With the app, users can iteratively refine codes and themes adopting a human-in-the-loop process, without the need to work with programming and scripting. The paper describes the application key features, highlighting its potential for qualitative research while preserving methodological rigor. The paper discusses the design and interface of the app and outlines future directions for this work.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge</title>
<link>https://arxiv.org/abs/2504.13904</link>
<guid>https://arxiv.org/abs/2504.13904</guid>
<content:encoded><![CDATA[
arXiv:2504.13904v1 Announce Type: cross 
Abstract: We hypothesize that optimal system responses emerge from adaptive strategies grounded in causal and counterfactual knowledge. Counterfactual inference allows us to create hypothetical scenarios to examine the effects of alternative system responses. We enhance this process through causal discovery, which identifies the strategies informed by the underlying causal structure that govern system behaviors. Moreover, we consider the psychological constructs and unobservable noises that might be influencing user-system interactions as latent factors. We show that these factors can be effectively estimated. We employ causal discovery to identify strategy-level causal relationships among user and system utterances, guiding the generation of personalized counterfactual dialogues. We model the user utterance strategies as causal factors, enabling system strategies to be treated as counterfactual actions. Furthermore, we optimize policies for selecting system responses based on counterfactual data. Our results using a real-world dataset on social good demonstrate significant improvements in persuasive system outcomes, with increased cumulative rewards validating the efficacy of causal discovery in guiding personalized counterfactual inference and optimizing dialogue policies for a persuasive dialogue system.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Incident Prevention in an Enterprise AI Assistant</title>
<link>https://arxiv.org/abs/2504.13924</link>
<guid>https://arxiv.org/abs/2504.13924</guid>
<content:encoded><![CDATA[
arXiv:2504.13924v1 Announce Type: cross 
Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy is paramount, making each erroneous output a potentially significant incident. This paper presents a comprehensive framework for monitoring, benchmarking, and continuously improving such complex, multi-component systems under active development by multiple teams. Our approach encompasses three key elements: (1) a hierarchical ``severity'' framework for incident detection that identifies and categorizes errors while attributing component-specific error rates, facilitating targeted improvements; (2) a scalable and principled methodology for benchmark construction, evaluation, and deployment, designed to accommodate multiple development teams, mitigate overfitting risks, and assess the downstream impact of system modifications; and (3) a continual improvement strategy leveraging multidimensional evaluation, enabling the identification and implementation of diverse enhancement opportunities. By adopting this holistic framework, organizations can systematically enhance the reliability and performance of their AI Assistants, ensuring their efficacy in critical enterprise environments. We conclude by discussing how this multifaceted evaluation approach opens avenues for various classes of enhancements, paving the way for more robust and trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining</title>
<link>https://arxiv.org/abs/2504.13932</link>
<guid>https://arxiv.org/abs/2504.13932</guid>
<content:encoded><![CDATA[
arXiv:2504.13932v1 Announce Type: cross 
Abstract: Large language models offer remarkable capabilities, but their size and computational demands pose practical challenges. Quantization methods compress their size through replacing their high-precision parameters by quantized values of lower precision. Post-training quantization reduces model size efficiently at the cost of decreased accuracy, while quantization-aware training better preserves accuracy but is resource-intensive. Among existing post-training quantization algorithms, the ApiQ method achieves superior accuracy preservation at minimal memory and time overhead. We investigate two ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level. First, we look into combining existing quantization-aware training techniques with ApiQ's partial training. We show that this does not outperform the baseline ApiQ method with limited training data and frozen weights. This leads to two key insights: (1) The substantial representational capacity that is gained through full retraining may not be feasible through partial training. (2) This gain seems to depend on using a large and diverse dataset in quantization-aware training. Second, through a novel approach informed by the two insights, we propose an ultra-low-bit quantization method that builds upon ApiQ and extends its performance without the need for full retraining. It relies on a saliency-aware regularization term that prioritizes preserving the most impactful parameters during quantization. Our experiments on benchmark language models from the LLaMA family show that our proposed approach boosts accuracy and tightens the gap between the quantized model and the full-precision model, with minimal overhead. Our method will be made publicly available to facilitate future developments in ultra-low-bit quantization of large language models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations</title>
<link>https://arxiv.org/abs/2504.13955</link>
<guid>https://arxiv.org/abs/2504.13955</guid>
<content:encoded><![CDATA[
arXiv:2504.13955v1 Announce Type: cross 
Abstract: The advancement of AI systems for mental health support is hindered by limited access to therapeutic conversation data, particularly for trauma treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset of 3,000 therapy conversations based on Prolonged Exposure therapy protocols for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique cases, each explored through six conversational perspectives that mirror the progression of therapy from initial anxiety to peak distress to emotional processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3, 49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10 trauma-related behaviors using deterministic and probabilistic generation methods. Analysis reveals realistic distributions of trauma types (witnessing violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse 20.8%). Clinical experts validated the dataset's therapeutic fidelity, highlighting its emotional depth while suggesting refinements for greater authenticity. We also developed an emotional trajectory benchmark with standardized metrics for evaluating model responses. This privacy-preserving dataset addresses critical gaps in trauma-focused mental health data, offering a valuable resource for advancing both patient-facing applications and clinician training tools.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolRL: Reward is All Tool Learning Needs</title>
<link>https://arxiv.org/abs/2504.13958</link>
<guid>https://arxiv.org/abs/2504.13958</guid>
<content:encoded><![CDATA[
arXiv:2504.13958v1 Announce Type: cross 
Abstract: Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the codes are released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Safety Should Prioritize the Future of Work</title>
<link>https://arxiv.org/abs/2504.13959</link>
<guid>https://arxiv.org/abs/2504.13959</guid>
<content:encoded><![CDATA[
arXiv:2504.13959v1 Announce Type: cross 
Abstract: Current efforts in AI safety prioritize filtering harmful content, preventing manipulation of human behavior, and eliminating existential risks in cybersecurity or biosecurity. While pressing, this narrow focus overlooks critical human-centric considerations that shape the long-term trajectory of a society. In this position paper, we identify the risks of overlooking the impact of AI on the future of work and recommend comprehensive transition support towards the evolution of meaningful labor with human agency. Through the lens of economic theories, we highlight the intertemporal impacts of AI on human livelihood and the structural changes in labor markets that exacerbate income inequality. Additionally, the closed-source approach of major stakeholders in AI development resembles rent-seeking behavior through exploiting resources, breeding mediocrity in creative labor, and monopolizing innovation. To address this, we argue in favor of a robust international copyright anatomy supported by implementing collective licensing that ensures fair compensation mechanisms for using data to train AI models. We strongly recommend a pro-worker framework of global AI governance to enhance shared prosperity and economic justice while reducing technical debt.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels</title>
<link>https://arxiv.org/abs/2504.13984</link>
<guid>https://arxiv.org/abs/2504.13984</guid>
<content:encoded><![CDATA[
arXiv:2504.13984v1 Announce Type: cross 
Abstract: To reduce the time and computational costs of inference of large language models, there has been interest in parameter-efficient low-rank early-exit casting of transformer hidden-representations to final-representations. Such low-rank short-cutting has been shown to outperform identity shortcuts at early model stages while offering parameter-efficiency in shortcut jumps. However, current low-rank methods maintain a separate early-exit shortcut jump to final-representations for each transformer intermediate block-level during inference. In this work, we propose selection of a single One-Jump-Fits-All (OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter costs during inference. We show that despite this extreme reduction, our OJFA choice largely matches the performance of maintaining multiple shortcut jumps during inference and offers stable precision from all transformer block-levels for GPT2-XL, Phi3-Mini and Llama2-7B transformer models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs</title>
<link>https://arxiv.org/abs/2504.13989</link>
<guid>https://arxiv.org/abs/2504.13989</guid>
<content:encoded><![CDATA[
arXiv:2504.13989v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become pivotal in artificial intelligence, demonstrating strong capabilities in reasoning, understanding, and generating data. However, their deployment on edge devices is hindered by their substantial size, often reaching several billion parameters. Quantization is a widely used method to reduce memory usage and inference time, however LLMs present unique challenges due to the prevalence of outliers in their activations. In this work, we leverage the theoretical advantages of Hadamard matrices over random rotation matrices to push the boundaries of quantization in LLMs. We demonstrate that Hadamard matrices are more effective in reducing outliers, which are a significant obstacle in achieving low-bit quantization. Our method based on a gradual binary search enables 3-bit quantization for weights, activations, and key-value (KV) caches, resulting in a 40\% increase in accuracy on common benchmarks compared to SoTA methods. We extend the use of rotation matrices to support non-power-of-2 embedding dimensions, similar to the Qwen architecture, by employing the Paley algorithm. We theoretically demonstrates the superiority of Hadamard matrices in reducing outliers.We achieved 3-bit quantization for weights, activations, and KV cache, significantly enhancing model performance. Our experimental results on multiple models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of our approach, outperforming existing methods and enabling practical 3-bit quantization.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions</title>
<link>https://arxiv.org/abs/2504.14053</link>
<guid>https://arxiv.org/abs/2504.14053</guid>
<content:encoded><![CDATA[
arXiv:2504.14053v1 Announce Type: cross 
Abstract: This research examines whether Airbnb guests' positive and negative comments influence acceptance rates and rental prices across six U.S. regions: Rhode Island, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of reviews were collected and analyzed using Natural Language Processing (NLP) to classify sentiments as positive or negative, followed by statistical testing (t-tests and basic correlations) on the average scores. The findings reveal that over 90 percent of reviews in each region are positive, indicating that having additional reviews does not significantly enhance prices. However, listings with predominantly positive feedback exhibit slightly higher acceptance rates, suggesting that sentiment polarity, rather than the sheer volume of reviews, is a more critical factor for host success. Additionally, budget listings often gather extensive reviews while maintaining competitive pricing, whereas premium listings sustain higher prices with fewer but highly positive reviews. These results underscore the importance of sentiment quality over quantity in shaping guest behavior and pricing strategies in an overwhelmingly positive review environment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking forward-pass dynamics in Transformers and real-time human processing</title>
<link>https://arxiv.org/abs/2504.14107</link>
<guid>https://arxiv.org/abs/2504.14107</guid>
<content:encoded><![CDATA[
arXiv:2504.14107v1 Announce Type: cross 
Abstract: Modern AI models are increasingly being used as theoretical tools to study human cognition. One dominant approach is to evaluate whether human-derived measures (such as offline judgments or real-time processing) are predicted by a model's output: that is, the end-product of forward pass(es) through the network. At the same time, recent advances in mechanistic interpretability have begun to reveal the internal processes that give rise to model outputs, raising the question of whether models and humans might arrive at outputs using similar "processing strategies". Here, we investigate the link between real-time processing in humans and "layer-time" dynamics in Transformer models. Across five studies spanning domains and modalities, we test whether the dynamics of computation in a single forward pass of pre-trained Transformers predict signatures of processing in humans, above and beyond properties of the model's output probability distribution. We consistently find that layer-time dynamics provide additional predictive power on top of output measures. Our results suggest that Transformer processing and human processing may be facilitated or impeded by similar properties of an input stimulus, and this similarity has emerged through general-purpose objectives such as next-token prediction or image recognition. Our work suggests a new way of using AI models to study human cognition: not just as a black box mapping stimuli to responses, but potentially also as explicit processing models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System of Agentic AI for the Discovery of Metal-Organic Frameworks</title>
<link>https://arxiv.org/abs/2504.14110</link>
<guid>https://arxiv.org/abs/2504.14110</guid>
<content:encoded><![CDATA[
arXiv:2504.14110v1 Announce Type: cross 
Abstract: Generative models and machine learning promise accelerated material discovery in MOFs for CO2 capture and water harvesting but face significant challenges navigating vast chemical spaces while ensuring synthetizability. Here, we present MOFGen, a system of Agentic AI comprising interconnected agents: a large language model that proposes novel MOF compositions, a diffusion model that generates crystal structures, quantum mechanical agents that optimize and filter candidates, and synthetic-feasibility agents guided by expert rules and machine learning. Trained on all experimentally reported MOFs and computational databases, MOFGen generated hundreds of thousands of novel MOF structures and synthesizable organic linkers. Our methodology was validated through high-throughput experiments and the successful synthesis of five "AI-dreamt" MOFs, representing a major step toward automated synthesizable material discovery.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Principles Improve Prompt Learning In Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.14123</link>
<guid>https://arxiv.org/abs/2504.14123</guid>
<content:encoded><![CDATA[
arXiv:2504.14123v1 Announce Type: cross 
Abstract: Prompt learning is a popular fine-tuning method for vision-language models due to its efficiency. It requires a small number of additional learnable parameters while significantly enhancing performance on target tasks. However, most existing methods suffer from overfitting to fine-tuning data, yielding poor generalizability. To address this, we propose a new training objective function based on a Bayesian learning principle to balance adaptability and generalizability. We derive a prior over the logits, where the mean function is parameterized by the pre-trained model, while the posterior corresponds to the fine-tuned model. This objective establishes a balance by allowing the fine-tuned model to adapt to downstream tasks while remaining close to the pre-trained model.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models</title>
<link>https://arxiv.org/abs/2504.14126</link>
<guid>https://arxiv.org/abs/2504.14126</guid>
<content:encoded><![CDATA[
arXiv:2504.14126v1 Announce Type: cross 
Abstract: Determining the ideal architecture for deep learning models, such as the number of layers and neurons, is a difficult and resource-intensive process that frequently relies on human tuning or computationally costly optimization approaches. While Particle Swarm Optimization (PSO) and Large Language Models (LLMs) have been individually applied in optimization and deep learning, their combined use for enhancing convergence in numerical optimization tasks remains underexplored. Our work addresses this gap by integrating LLMs into PSO to reduce model evaluations and improve convergence for deep learning hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the difficulties of efficiency and convergence by using LLMs (particularly ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster achievement of target objectives. Our method speeds up search space exploration by substituting underperforming particle placements with best suggestions offered by LLMs. Comprehensive experiments across three scenarios -- (1) optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM) networks for time series regression, and (3) using Convolutional Neural Networks (CNNs) for material classification -- show that the method significantly improves convergence rates and lowers computational costs. Depending on the application, computational complexity is lowered by 20% to 60% compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by 60% for both regression and classification tasks, all while preserving accuracy and error rates. This groundbreaking methodology offers a very efficient and effective solution for optimizing deep learning models, leading to substantial computational performance improvements across a wide range of applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALES: Text Adventure Learning Environment Suite</title>
<link>https://arxiv.org/abs/2504.14128</link>
<guid>https://arxiv.org/abs/2504.14128</guid>
<content:encoded><![CDATA[
arXiv:2504.14128v1 Announce Type: cross 
Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tales.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation</title>
<link>https://arxiv.org/abs/2504.14147</link>
<guid>https://arxiv.org/abs/2504.14147</guid>
<content:encoded><![CDATA[
arXiv:2504.14147v1 Announce Type: cross 
Abstract: Recent advancements in explainable recommendation have greatly bolstered user experience by elucidating the decision-making rationale. However, the existing methods actually fail to provide effective feedback signals for potentially better or worse generated explanations due to their reliance on traditional supervised learning paradigms in sparse interaction data. To address these issues, we propose a novel human-like feedback-driven optimization framework. This framework employs a dynamic interactive optimization mechanism for achieving human-centered explainable requirements without incurring high labor costs. Specifically, we propose to utilize large language models (LLMs) as human simulators to predict human-like feedback for guiding the learning process. To enable the LLMs to deeply understand the task essence and meet user's diverse personalized requirements, we introduce a human-induced customized reward scoring method, which helps stimulate the language understanding and logical reasoning capabilities of LLMs. Furthermore, considering the potential conflicts between different perspectives of explanation quality, we introduce a principled Pareto optimization that transforms the multi-perspective quality enhancement task into a multi-objective optimization problem for improving explanation performance. At last, to achieve efficient model training, we design an off-policy optimization pipeline. By incorporating a replay buffer and addressing the data distribution biases, we can effectively improve data utilization and enhance model generality. Extensive experiments on four datasets demonstrate the superiority of our approach.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Advantage Regression: Aligning LLMs with Online AI Reward</title>
<link>https://arxiv.org/abs/2504.14177</link>
<guid>https://arxiv.org/abs/2504.14177</guid>
<content:encoded><![CDATA[
arXiv:2504.14177v1 Announce Type: cross 
Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement Learning from Human Feedback (RLHF) by utilizing online AI preference in aligning language models (LLMs). However, the straightforward replacement of humans with AI deprives LLMs from learning more fine-grained AI supervision beyond binary signals. In this paper, we propose Direct Advantage Regression (DAR), a simple alignment algorithm using online AI reward to optimize policy improvement through weighted supervised fine-tuning. As an RL-free approach, DAR maintains theoretical consistency with online RLHF pipelines while significantly reducing implementation complexity and improving learning efficiency. Our empirical results underscore that AI reward is a better form of AI supervision consistently achieving higher human-AI agreement as opposed to AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show that DAR outperforms both OAIF and online RLHF baselines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The First VoicePrivacy Attacker Challenge</title>
<link>https://arxiv.org/abs/2504.14183</link>
<guid>https://arxiv.org/abs/2504.14183</guid>
<content:encoded><![CDATA[
arXiv:2504.14183v1 Announce Type: cross 
Abstract: The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand Challenge which focuses on evaluating attacker systems against a set of voice anonymization systems submitted to the VoicePrivacy 2024 Challenge. Training, development, and evaluation datasets were provided along with a baseline attacker. Participants developed their attacker systems in the form of automatic speaker verification systems and submitted their scores on the development and evaluation data. The best attacker systems reduced the equal error rate (EER) by 25-44% relative w.r.t. the baseline.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Idea Bench 2025: AI Research Idea Generation Benchmark</title>
<link>https://arxiv.org/abs/2504.14191</link>
<guid>https://arxiv.org/abs/2504.14191</guid>
<content:encoded><![CDATA[
arXiv:2504.14191v1 Announce Type: cross 
Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction and achieved significant success in the generation of novel ideas. However, current assessments of idea generation overlook crucial factors such as knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded truth, and the limited scope of feasibility analysis constrained by prompt design. These limitations hinder the potential of uncovering groundbreaking research ideas. In this paper, we present AI Idea Bench 2025, a framework designed to quantitatively evaluate and compare the ideas generated by LLMs within the domain of AI research from diverse perspectives. The framework comprises a comprehensive dataset of 3,495 AI papers and their associated inspired works, along with a robust evaluation methodology. This evaluation system gauges idea quality in two dimensions: alignment with the ground-truth content of the original papers and judgment based on general reference material. AI Idea Bench 2025's benchmarking system stands to be an invaluable resource for assessing and comparing idea-generation techniques, thereby facilitating the automation of scientific discovery.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment</title>
<link>https://arxiv.org/abs/2504.14232</link>
<guid>https://arxiv.org/abs/2504.14232</guid>
<content:encoded><![CDATA[
arXiv:2504.14232v1 Announce Type: cross 
Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz, an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured framework for categorizing educational objectives into hierarchical cognitive levels. Our research investigates whether incorporating this taxonomy can improve the alignment of AI-generated questions with specific cognitive objectives. We developed a dataset of 3691 questions categorized according to Bloom's levels and employed various classification models-Multinomial Logistic Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a Transformer-based model (DistilBERT)-to evaluate their effectiveness in categorizing questions. Our results indicate that higher Bloom's levels generally correlate with increased question length, Flesch-Kincaid Grade Level (FKGL), and Lexical Density (LD), reflecting the increased complexity of higher cognitive demands. Multinomial Logistic Regression showed varying accuracy across Bloom's levels, performing best for "Knowledge" and less accurately for higher-order levels. Merging higher-level categories improved accuracy for complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective classification for lower levels but struggled with higher-order tasks. DistilBERT achieved the highest performance, significantly improving classification of both lower and higher-order cognitive levels, achieving an overall validation accuracy of 91%. This study highlights the potential of integrating Bloom's Taxonomy into AI-driven assessment tools and underscores the advantages of advanced models like DistilBERT for enhancing educational content generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners</title>
<link>https://arxiv.org/abs/2504.14239</link>
<guid>https://arxiv.org/abs/2504.14239</guid>
<content:encoded><![CDATA[
arXiv:2504.14239v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2504.14245</link>
<guid>https://arxiv.org/abs/2504.14245</guid>
<content:encoded><![CDATA[
arXiv:2504.14245v1 Announce Type: cross 
Abstract: Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a "black box". Instead, an ideal approach must ensure both strong generalization and transparency. Recent progress in Multi-modal Large Language Models (MLLMs) offers new opportunities for reasoning-based AI-generated image detection. In this work, we evaluate the capabilities of MLLMs in comparison to traditional detection methods and human evaluators, highlighting their strengths and limitations. Furthermore, we design six distinct prompts and propose a framework that integrates these prompts to develop a more robust, explainable, and reasoning-driven detection system. The code is available at https://github.com/Gennadiyev/mllm-defake.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-attention for State-based model RWKV-7</title>
<link>https://arxiv.org/abs/2504.14260</link>
<guid>https://arxiv.org/abs/2504.14260</guid>
<content:encoded><![CDATA[
arXiv:2504.14260v1 Announce Type: cross 
Abstract: We introduce CrossWKV, a novel cross-attention mechanism for the state-based RWKV-7 model, designed to enhance the expressive power of text-to-image generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV) architecture, CrossWKV integrates text and image modalities in a single pass, utilizing a generalized delta rule with vector-valued gating and low-rank adaptations (LoRA) to achieve superior cross-modal alignment. Unlike Transformer-based models, CrossWKV's non-diagonal, input-dependent transition matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$ complexity class, including all regular languages, as demonstrated by its ability to perform state-tracking tasks like $S_5$ permutation modeling. Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance while offering robust generalization across diverse prompts. The model's enhanced expressivity, combined with constant memory usage and linear scaling, positions it as a powerful solution for advanced cross-modal tasks, with potential applications in high-resolution generation and dynamic state manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling</title>
<link>https://arxiv.org/abs/2504.14359</link>
<guid>https://arxiv.org/abs/2504.14359</guid>
<content:encoded><![CDATA[
arXiv:2504.14359v1 Announce Type: cross 
Abstract: There are many ways to describe, name, and group objects when captioning an image. Differences are evident when speakers come from diverse cultures due to the unique experiences that shape perception. Machine translation of captions has pushed multilingual capabilities in vision-language models (VLMs), but data comes mainly from English speakers, indicating a perceptual bias and lack of model flexibility. In this work, we address this challenge and outline a data-efficient framework to instill multilingual VLMs with greater understanding of perceptual diversity. We specifically propose an LLM-based, multimodal recaptioning strategy that alters the object descriptions of English captions before translation. The greatest benefits are demonstrated in a targeted multimodal mechanism guided by native speaker data. By adding produced rewrites as augmentations in training, we improve on German and Japanese text-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on non-native error cases). We further propose a mechanism to analyze the specific object description differences across datasets, and we offer insights into cross-dataset and cross-language generalization.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction</title>
<link>https://arxiv.org/abs/2504.14361</link>
<guid>https://arxiv.org/abs/2504.14361</guid>
<content:encoded><![CDATA[
arXiv:2504.14361v1 Announce Type: cross 
Abstract: In this study, we propose an innovative methodology for predicting Cancer Drug Response (CDR) through the integration of the scGPT foundation model within the DeepCDR model. Our approach utilizes scGPT to generate embeddings from gene expression data, which are then used as gene expression input data for DeepCDR. The experimental findings demonstrate the efficacy of this scGPT-based method in outperforming previous related works, including the original DeepCDR model and the scFoundation-based model. This study highlights the potential of scGPT embeddings to enhance the accuracy of CDR predictions and offers a promising alternative to existing approaches.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving RL Exploration for LLM Reasoning through Retrospective Replay</title>
<link>https://arxiv.org/abs/2504.14363</link>
<guid>https://arxiv.org/abs/2504.14363</guid>
<content:encoded><![CDATA[
arXiv:2504.14363v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has increasingly become a pivotal technique in the post-training of large language models (LLMs). The effective exploration of the output space is essential for the success of RL. We observe that for complex problems, during the early stages of training, the model exhibits strong exploratory capabilities and can identify promising solution ideas. However, its limited capability at this stage prevents it from successfully solving these problems. The early suppression of these potentially valuable solution ideas by the policy gradient hinders the model's ability to revisit and re-explore these ideas later. Consequently, although the LLM's capabilities improve in the later stages of training, it still struggles to effectively address these complex problems. To address this exploration issue, we propose a novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL), which introduces a dynamic replay mechanism throughout the training process. RRL enables the model to revisit promising states identified in the early stages, thereby improving its efficiency and effectiveness in exploration. To evaluate the effectiveness of RRL, we conduct extensive experiments on complex reasoning tasks, including mathematical reasoning and code generation, and general dialogue tasks. The results indicate that RRL maintains high exploration efficiency throughout the training period, significantly enhancing the effectiveness of RL in optimizing LLMs for complicated reasoning tasks. Moreover, it also improves the performance of RLHF, making the model both safer and more helpful.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Density Measures for Language Generation</title>
<link>https://arxiv.org/abs/2504.14370</link>
<guid>https://arxiv.org/abs/2504.14370</guid>
<content:encoded><![CDATA[
arXiv:2504.14370v1 Announce Type: cross 
Abstract: The recent successes of large language models (LLMs) have led to a surge of theoretical research into language generation. A recent line of work proposes an abstract view, called language generation in the limit, where generation is seen as a game between an adversary and an algorithm: the adversary generates strings from an unknown language $K$, chosen from a countable collection of candidate languages, and after seeing a finite set of these strings, the algorithm must generate new strings from $K$ that it has not seen before. This formalism highlights a key tension: the trade-off between validity (the algorithm should only produce strings from the language) and breadth (it should be able to produce many strings from the language). This trade-off is central in applied language generation as well, where it appears as a balance between hallucination (generating invalid utterances) and mode collapse (generating only a restricted set of outputs). Despite its importance, this trade-off has been challenging to study quantitatively. We develop ways to quantify this trade-off by formalizing breadth using measures of density. Existing algorithms for language generation in the limit produce output sets that can have zero density in the true language, and this important failure of breadth might seem unavoidable. We show, however, that such a failure is not necessary: we provide an algorithm for language generation in the limit whose outputs have strictly positive density in $K$. We also study the internal representations built by these algorithms, specifically the sequence of hypothesized candidate languages they consider, and show that achieving the strongest form of breadth may require oscillating indefinitely between high- and low-density representations. Our analysis introduces a novel topology on language families, with notions of convergence and limit points playing a key role.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRe: Personalizing LLMs via Low-Rank Reward Modeling</title>
<link>https://arxiv.org/abs/2504.14439</link>
<guid>https://arxiv.org/abs/2504.14439</guid>
<content:encoded><![CDATA[
arXiv:2504.14439v1 Announce Type: cross 
Abstract: Personalizing large language models (LLMs) to accommodate diverse user preferences is essential for enhancing alignment and user satisfaction. Traditional reinforcement learning from human feedback (RLHF) approaches often rely on monolithic value representations, limiting their ability to adapt to individual preferences. We introduce a novel framework that leverages low-rank preference modeling to efficiently learn and generalize user-specific reward functions. By representing reward functions in a low-dimensional subspace and modeling individual preferences as weighted combinations of shared basis functions, our approach avoids rigid user categorization while enabling scalability and few-shot adaptation. We validate our method on multiple preference datasets, demonstrating superior generalization to unseen users and improved accuracy in preference prediction tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey</title>
<link>https://arxiv.org/abs/2504.14520</link>
<guid>https://arxiv.org/abs/2504.14520</guid>
<content:encoded><![CDATA[
arXiv:2504.14520v1 Announce Type: cross 
Abstract: This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding</title>
<link>https://arxiv.org/abs/2504.14526</link>
<guid>https://arxiv.org/abs/2504.14526</guid>
<content:encoded><![CDATA[
arXiv:2504.14526v1 Announce Type: cross 
Abstract: Vision Large Language Models (VLLMs) have demonstrated impressive capabilities in general visual tasks such as image captioning and visual question answering. However, their effectiveness in specialized, safety-critical domains like autonomous driving remains largely unexplored. Autonomous driving systems require sophisticated scene understanding in complex environments, yet existing multimodal benchmarks primarily focus on normal driving conditions, failing to adequately assess VLLMs' performance in safety-critical scenarios. To address this, we introduce DVBench, a pioneering benchmark designed to evaluate the performance of VLLMs in understanding safety-critical driving videos. Built around a hierarchical ability taxonomy that aligns with widely adopted frameworks for describing driving scenarios used in assessing highly automated driving systems, DVBench features 10,000 multiple-choice questions with human-annotated ground-truth answers, enabling a comprehensive evaluation of VLLMs' capabilities in perception and reasoning. Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal significant performance gaps, with no model achieving over 40% accuracy, highlighting critical limitations in understanding complex driving scenarios. To probe adaptability, we fine-tuned selected models using domain-specific data from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage points, with relative improvements of up to 43.59%. This improvement underscores the necessity of targeted adaptation to bridge the gap between general-purpose VLLMs and mission-critical driving applications. DVBench establishes an essential evaluation framework and research roadmap for developing VLLMs that meet the safety and robustness requirements for real-world autonomous systems. We released the benchmark toolbox and the fine-tuned model at: https://github.com/tong-zeng/DVBench.git.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models</title>
<link>https://arxiv.org/abs/2504.14594</link>
<guid>https://arxiv.org/abs/2504.14594</guid>
<content:encoded><![CDATA[
arXiv:2504.14594v1 Announce Type: cross 
Abstract: Seeking dietary guidance often requires navigating complex professional knowledge while accommodating individual health conditions. Knowledge Graphs (KGs) offer structured and interpretable nutritional information, whereas Large Language Models (LLMs) naturally facilitate conversational recommendation delivery. In this paper, we present HealthGenie, an interactive system that combines the strengths of LLMs and KGs to provide personalized dietary recommendations along with hierarchical information visualization for a quick and intuitive overview. Upon receiving a user query, HealthGenie performs query refinement and retrieves relevant information from a pre-built KG. The system then visualizes and highlights pertinent information, organized by defined categories, while offering detailed, explainable recommendation rationales. Users can further tailor these recommendations by adjusting preferences interactively. Our evaluation, comprising a within-subject comparative experiment and an open-ended discussion, demonstrates that HealthGenie effectively supports users in obtaining personalized dietary guidance based on their health conditions while reducing interaction effort and cognitive load. These findings highlight the potential of LLM-KG integration in supporting decision-making through explainable and visualized information. We examine the system's usefulness and effectiveness with an N=12 within-subject study and provide design considerations for future systems that integrate conversational LLM and KG.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk Assessment Framework for Code LLMs via Leveraging Internal States</title>
<link>https://arxiv.org/abs/2504.14640</link>
<guid>https://arxiv.org/abs/2504.14640</guid>
<content:encoded><![CDATA[
arXiv:2504.14640v1 Announce Type: cross 
Abstract: The pre-training paradigm plays a key role in the success of Large Language Models (LLMs), which have been recognized as one of the most significant advancements of AI recently. Building on these breakthroughs, code LLMs with advanced coding capabilities bring huge impacts on software engineering, showing the tendency to become an essential part of developers' daily routines. However, the current code LLMs still face serious challenges related to trustworthiness, as they can generate incorrect, insecure, or unreliable code. Recent exploratory studies find that it can be promising to detect such risky outputs by analyzing LLMs' internal states, akin to how the human brain unconsciously recognizes its own mistakes. Yet, most of these approaches are limited to narrow sub-domains of LLM operations and fall short of achieving industry-level scalability and practicability. To address these challenges, in this paper, we propose PtTrust, a two-stage risk assessment framework for code LLM based on internal state pre-training, designed to integrate seamlessly with the existing infrastructure of software companies. The core idea is that the risk assessment framework could also undergo a pre-training process similar to LLMs. Specifically, PtTrust first performs unsupervised pre-training on large-scale unlabeled source code to learn general representations of LLM states. Then, it uses a small, labeled dataset to train a risk predictor. We demonstrate the effectiveness of PtTrust through fine-grained, code line-level risk assessment and demonstrate that it generalizes across tasks and different programming languages. Further experiments also reveal that PtTrust provides highly intuitive and interpretable features, fostering greater user trust. We believe PtTrust makes a promising step toward scalable and trustworthy assurance for code LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs</title>
<link>https://arxiv.org/abs/2504.14655</link>
<guid>https://arxiv.org/abs/2504.14655</guid>
<content:encoded><![CDATA[
arXiv:2504.14655v1 Announce Type: cross 
Abstract: We introduce LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models, addressing two key challenges in LLM research: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode Python problems with rich metadata, broad coverage, 100+ test cases per problem, and temporal splits (pre/post July 2024), our dataset enables contamination-free evaluation and efficient supervised fine-tuning (SFT). Experiments show reasoning models significantly outperform non-reasoning counterparts, while SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts. The dataset and evaluation framework are available on Hugging Face and Github.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities</title>
<link>https://arxiv.org/abs/2504.14773</link>
<guid>https://arxiv.org/abs/2504.14773</guid>
<content:encoded><![CDATA[
arXiv:2504.14773v1 Announce Type: cross 
Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g., creating travel itineraries within a budget, holds immense potential in both scientific and commercial contexts. Moreover, optimal plans tend to require fewer resources compared to ad-hoc methods. To date, a comprehensive understanding of existing planning benchmarks appears to be lacking. Without it, comparing planning algorithms' performance across domains or selecting suitable algorithms for new scenarios remains challenging. In this paper, we examine a range of planning benchmarks to identify commonly used testbeds for algorithm development and highlight potential gaps. These benchmarks are categorized into embodied environments, web navigation, scheduling, games and puzzles, and everyday task automation. Our study recommends the most appropriate benchmarks for various algorithms and offers insights to guide future benchmark development.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Completing A Systematic Review in Hours instead of Months with Interactive AI Agents</title>
<link>https://arxiv.org/abs/2504.14822</link>
<guid>https://arxiv.org/abs/2504.14822</guid>
<content:encoded><![CDATA[
arXiv:2504.14822v1 Announce Type: cross 
Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG</title>
<link>https://arxiv.org/abs/2504.14858</link>
<guid>https://arxiv.org/abs/2504.14858</guid>
<content:encoded><![CDATA[
arXiv:2504.14858v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has emerged as a foundational paradigm for knowledge-grounded text generation. However, existing RAG pipelines often fail to ensure that the reasoning trajectories align with the evidential constraints imposed by retrieved content. In this paper, we reframe RAG as a problem of retrieval-aware reasoning and identify a core challenge: reasoning misalignment-the mismatch between a model's reasoning trajectory and the retrieved evidence. To address this challenge, we propose AlignRAG, a novel test-time framework that mitigates reasoning misalignment through iterative Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that rely on static training or post-hoc selection, AlignRAG actively refines reasoning trajectories during inference by enforcing fine-grained alignment with evidence. Our framework introduces a new paradigm for retrieval-aware reasoning by: (1) constructing context-rich training corpora; (2) generating contrastive critiques from preference-aware reasoning trajectories; (3) training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning misalignments; and (4) applying CDA steps to optimize reasoning trajectories iteratively. Empirical results demonstrate that AlignRAG consistently outperforms all baselines and could integrate as a plug-and-play module into existing RAG pipelines without further changes. By reconceptualizing RAG as a structured reasoning trajectory and establishing the test-time framework for correcting reasoning misalignments in RAG, AlignRAG provides practical advancements for retrieval-aware generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OTC: Optimal Tool Calls via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.14870</link>
<guid>https://arxiv.org/abs/2504.14870</guid>
<content:encoded><![CDATA[
arXiv:2504.14870v1 Announce Type: cross 
Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools, such as search engines and code interpreters, to solve tasks beyond the capabilities of language-only reasoning. While reinforcement learning (RL) has shown promise in improving TIR by optimizing final answer correctness, existing approaches often overlook the efficiency and cost associated with tool usage. This can lead to suboptimal behavior, including excessive tool calls that increase computational and financial overhead, or insufficient tool use that compromises answer quality. In this work, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers correctness and tool efficiency, promoting high tool productivity. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 73.1\% and improves tool productivity by up to 229.4\%, while maintaining comparable answer accuracy. To the best of our knowledge, this is the first RL-based framework that explicitly optimizes tool-use efficiency in TIR.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform</title>
<link>https://arxiv.org/abs/2504.14904</link>
<guid>https://arxiv.org/abs/2504.14904</guid>
<content:encoded><![CDATA[
arXiv:2504.14904v1 Announce Type: cross 
Abstract: Exponentially growing short video platforms (SVPs) face significant challenges in moderating content detrimental to users' mental health, particularly for minors. The dissemination of such content on SVPs can lead to catastrophic societal consequences. Although substantial efforts have been dedicated to moderating such content, existing methods suffer from critical limitations: (1) Manual review is prone to human bias and incurs high operational costs. (2) Automated methods, though efficient, lack nuanced content understanding, resulting in lower accuracy. (3) Industrial moderation regulations struggle to adapt to rapidly evolving trends due to long update cycles. In this paper, we annotate the first SVP content moderation benchmark with authentic user/reviewer feedback to fill the absence of benchmark in this field. Then we evaluate various methods on the benchmark to verify the existence of the aforementioned limitations. We further propose our common-law content moderation framework named KuaiMod to address these challenges. KuaiMod consists of three components: training data construction, offline adaptation, and online deployment & refinement. Leveraging large vision language model (VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video toxicity based on sparse user feedback and fosters dynamic moderation policy with rapid update speed and high accuracy. Offline experiments and large-scale online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the best moderation performance on our benchmark. The deployment of KuaiMod reduces the user reporting rate by 20% and its application in video recommendation increases both Daily Active User (DAU) and APP Usage Time (AUT) on several Kuaishou scenarios. We have open-sourced our benchmark at https://kuaimod.github.io.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
arXiv:2504.14928v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason under Off-Policy Guidance</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
arXiv:2504.14945v1 Announce Type: cross 
Abstract: Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning (RL) with simple rule-based rewards. However, existing zero-RL approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY guidance), a framework that augments zero-RL with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Notably, we propose policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an over +7.0 average gain across six math benchmarks and an advantage of over +6.2 points in out-of-distribution tasks. It also substantially surpasses imitation-based supervised fine-tuning (SFT), particularly in generalization. Analysis shows LUFFY not only imitates effectively but also explores beyond demonstrations, offering a scalable path to train generalizable reasoning models with off-policy guidance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models</title>
<link>https://arxiv.org/abs/2504.15068</link>
<guid>https://arxiv.org/abs/2504.15068</guid>
<content:encoded><![CDATA[
arXiv:2504.15068v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have significantly enhanced the capabilities of information access systems, especially with retrieval-augmented generation (RAG). Nevertheless, the evaluation of RAG systems remains a barrier to continued progress, a challenge we tackle in this work by proposing an automatic evaluation framework that is validated against human annotations. We believe that the nugget evaluation methodology provides a solid foundation for evaluating RAG systems. This approach, originally developed for the TREC Question Answering (QA) Track in 2003, evaluates systems based on atomic facts that should be present in good answers. Our efforts focus on "refactoring" this methodology, where we describe the AutoNuggetizer framework that specifically applies LLMs to both automatically create nuggets and automatically assign nuggets to system answers. In the context of the TREC 2024 RAG Track, we calibrate a fully automatic approach against strategies where nuggets are created manually or semi-manually by human assessors and then assigned manually to system answers. Based on results from a community-wide evaluation, we observe strong agreement at the run level between scores derived from fully automatic nugget evaluation and human-based variants. The agreement is stronger when individual framework components such as nugget assignment are automated independently. This suggests that our evaluation framework provides tradeoffs between effort and quality that can be used to guide the development of future RAG systems. However, further research is necessary to refine our approach, particularly in establishing robust per-topic agreement to diagnose system failures effectively.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis</title>
<link>https://arxiv.org/abs/2504.15072</link>
<guid>https://arxiv.org/abs/2504.15072</guid>
<content:encoded><![CDATA[
arXiv:2504.15072v1 Announce Type: cross 
Abstract: The rapid development of social media has significantly reshaped the dynamics of public opinion, resulting in complex interactions that traditional models fail to effectively capture. To address this challenge, we propose an innovative approach that integrates multi-dimensional Hawkes processes with Graph Neural Network, modeling opinion propagation dynamics among nodes in a social network while considering the intricate hierarchical relationships between comments. The extended multi-dimensional Hawkes process captures the hierarchical structure, multi-dimensional interactions, and mutual influences across different topics, forming a complex propagation network. Moreover, recognizing the lack of high-quality datasets capable of comprehensively capturing the evolution of public opinion dynamics, we introduce a new dataset, VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015 second-level comments, and 29,578 third-level comments, covering diverse domains such as politics, entertainment, sports, health, and medicine. The dataset is annotated with detailed sentiment labels across 11 categories and clearly defined hierarchical relationships. When combined with our method, it offers strong interpretability by linking sentiment propagation to the comment hierarchy and temporal evolution. Our approach provides a robust baseline for future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking</title>
<link>https://arxiv.org/abs/2504.15135</link>
<guid>https://arxiv.org/abs/2504.15135</guid>
<content:encoded><![CDATA[
arXiv:2504.15135v1 Announce Type: cross 
Abstract: Entity linking (EL) aligns textual mentions with their corresponding entities in a knowledge base, facilitating various applications such as semantic search and question answering. Recent advances in multimodal entity linking (MEL) have shown that combining text and images can reduce ambiguity and improve alignment accuracy. However, most existing MEL methods overlook the rich structural information available in the form of knowledge-graph (KG) triples. In this paper, we propose KGMEL, a novel framework that leverages KG triples to enhance MEL. Specifically, it operates in three stages: (1) Generation: Produces high-quality triples for each mention by employing vision-language models based on its text and images. (2) Retrieval: Learns joint mention-entity representations, via contrastive learning, that integrate text, images, and (generated or KG) triples to retrieve candidate entities for each mention. (3) Reranking: Refines the KG triples of the candidate entities and employs large language models to identify the best-matching entity for the mention. Extensive experiments on benchmark datasets demonstrate that KGMEL outperforms existing methods. Our code and datasets are available at: https://github.com/juyeonnn/KGMEL.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation</title>
<link>https://arxiv.org/abs/2504.15254</link>
<guid>https://arxiv.org/abs/2504.15254</guid>
<content:encoded><![CDATA[
arXiv:2504.15254v1 Announce Type: cross 
Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roll the dice &amp; look before you leap: Going beyond the creative limits of next-token prediction</title>
<link>https://arxiv.org/abs/2504.15266</link>
<guid>https://arxiv.org/abs/2504.15266</guid>
<content:encoded><![CDATA[
arXiv:2504.15266v1 Announce Type: cross 
Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic and memorizes excessively; comparatively, multi-token approaches, namely teacherless training and diffusion models, excel in producing diverse and original output. Secondly, in our tasks, we find that to elicit randomness from the Transformer without hurting coherence, it is better to inject noise right at the input layer (via a method we dub hash-conditioning) rather than defer to temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and softmax-based sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes</title>
<link>https://arxiv.org/abs/2504.15270</link>
<guid>https://arxiv.org/abs/2504.15270</guid>
<content:encoded><![CDATA[
arXiv:2504.15270v1 Announce Type: cross 
Abstract: Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present \textbf{Quicksviewer}, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes using Gumbel Softmax, followed by a unified resampling for each cube to achieve efficient video understanding. This simple and intuitive approach dynamically compress video online based on its temporal density, significantly reducing spatiotemporal redundancy (overall 45$\times$ compression rate), while enabling efficient training with large receptive field. We train the model from a language backbone through three progressive stages, each incorporating lengthy videos on average of 420s/1fps thanks to the perceiving efficiency. With only 0.8M total video-text samples for training, our model outperforms the direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in accuracy, demonstrating the effectiveness in performance. On Video-MME, Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\% of tokens per frame required by baselines. With this paradigm, scaling up the number of input frames reveals a clear power law of the model capabilities. It is also empirically verified that the segments generated by the cubing network can help for analyzing continuous events in videos.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs</title>
<link>https://arxiv.org/abs/2504.15280</link>
<guid>https://arxiv.org/abs/2504.15280</guid>
<content:encoded><![CDATA[
arXiv:2504.15280v1 Announce Type: cross 
Abstract: Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persian Abstract Meaning Representation: Annotation Guidelines and Gold Standard Dataset</title>
<link>https://arxiv.org/abs/2205.07712</link>
<guid>https://arxiv.org/abs/2205.07712</guid>
<content:encoded><![CDATA[
arXiv:2205.07712v2 Announce Type: replace 
Abstract: This paper introduces the Persian Abstract Meaning Representation (AMR) guidelines, a detailed guide for annotating Persian sentences with AMR, focusing on the necessary adaptations to fit Persian's unique syntactic structures. We discuss the development process of a Persian AMR gold standard dataset consisting of 1,562 sentences created following the guidelines. By examining the language specifications and nuances that distinguish AMR annotations of a low-resource language like Persian, we shed light on the challenges and limitations of developing a universal meaning representation framework. The guidelines and the dataset introduced in this study highlight such challenges, aiming to advance the field.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLoRE: Evaluating Logical Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2310.09107</link>
<guid>https://arxiv.org/abs/2310.09107</guid>
<content:encoded><![CDATA[
arXiv:2310.09107v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongStory: Coherent, Complete and Length Controlled Long story Generation</title>
<link>https://arxiv.org/abs/2311.15208</link>
<guid>https://arxiv.org/abs/2311.15208</guid>
<content:encoded><![CDATA[
arXiv:2311.15208v2 Announce Type: replace 
Abstract: A human author can write any length of story without losing coherence. Also, they always bring the story to a proper ending, an ability that current language models lack. In this work, we present the LongStory for coherent, complete, and length-controlled long story generation. LongStory introduces two novel methodologies: (1) the long and short-term contexts weight calibrator (CWC) and (2) long story structural positions (LSP). The CWC adjusts weights for long-term context Memory and short-term context Cheating, acknowledging their distinct roles. The LSP employs discourse tokens to convey the structural positions of a long story. Trained on three datasets with varied average story lengths, LongStory outperforms other baselines, including the strong story generator Plotmachine, in coherence, completeness, relevance, and repetitiveness. We also perform zero-shot tests on each dataset to assess the model's ability to predict outcomes beyond its training data and validate our methodology by comparing its performance with variants of our model.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models</title>
<link>https://arxiv.org/abs/2403.10258</link>
<guid>https://arxiv.org/abs/2403.10258</guid>
<content:encoded><![CDATA[
arXiv:2403.10258v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated multilingual capabilities, yet they are mostly English-centric due to the imbalanced training corpora. While prior works have leveraged this bias to enhance multilingual performance through translation, they have been largely limited to natural language processing (NLP) tasks. In this work, we extend the evaluation to real-world user queries and non-English-centric LLMs, offering a broader examination of multilingual performance. Our key contribution lies in demonstrating that while translation into English can boost the performance of English-centric LLMs on NLP tasks, it is not universally optimal. For culture-related tasks that need deep language understanding, prompting in the native language proves more effective as it better captures the nuances of culture and language. Our experiments expose varied behaviors across LLMs and tasks in the multilingual context, underscoring the need for a more comprehensive approach to multilingual evaluation. Therefore, we call for greater efforts in developing and evaluating LLMs that go beyond English-centric paradigms.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Language Models with Demonstrated Feedback</title>
<link>https://arxiv.org/abs/2406.00888</link>
<guid>https://arxiv.org/abs/2406.00888</guid>
<content:encoded><![CDATA[
arXiv:2406.00888v2 Announce Type: replace 
Abstract: Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (< 10) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. Concretely, DITTO operates by having an LLM generate examples that are presumed to be inferior to expert demonstrations. The method iteratively constructs pairwise preference relationships between these LLM-generated samples and expert demonstrations, potentially including comparisons between different training checkpoints. These constructed preference pairs are then used to train the model using a preference optimization algorithm (e.g. DPO). We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants (N = 16). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an avg. of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Constitutional AI: Compressing Preferences into Principles</title>
<link>https://arxiv.org/abs/2406.06560</link>
<guid>https://arxiv.org/abs/2406.06560</guid>
<content:encoded><![CDATA[
arXiv:2406.06560v2 Announce Type: replace 
Abstract: Feedback data is widely used for fine-tuning and evaluating state-of-the-art AI models. Pairwise text preferences, where human or AI annotators select the "better" of two options, are particularly common. Such preferences are used to train (reward) models or to rank models with aggregate statistics. For many applications it is desirable to understand annotator preferences in addition to modelling them - not least because extensive prior work has shown various unintended biases in preference datasets. Yet, preference datasets remain challenging to interpret. Neither black-box reward models nor statistics can answer why one text is preferred over another. Manual interpretation of the numerous (long) response pairs is usually equally infeasible. In this paper, we introduce the Inverse Constitutional AI (ICAI) problem, formulating the interpretation of pairwise text preference data as a compression task. In constitutional AI, a set of principles (a constitution) is used to provide feedback and fine-tune AI models. ICAI inverts this process: given a feedback dataset, we aim to extract a constitution that best enables a large language model (LLM) to reconstruct the original annotations. We propose a corresponding ICAI algorithm and validate its generated constitutions quantitatively based on annotation reconstruction accuracy on several datasets: (a) synthetic feedback data with known principles; (b) AlpacaEval cross-annotated human feedback data; (c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse demographic groups. As a short and interpretable representation of the original dataset, generated constitutions have many potential use cases: help identify undesirable annotator biases, understand model performance better, scale feedback to unseen data, or adapt models to individual user or group preferences. We release the source code at https://github.com/rdnfn/icai.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition</title>
<link>https://arxiv.org/abs/2406.11192</link>
<guid>https://arxiv.org/abs/2406.11192</guid>
<content:encoded><![CDATA[
arXiv:2406.11192v3 Announce Type: replace 
Abstract: Open Named Entity Recognition (NER), which involves identifying arbitrary types of entities from arbitrary domains, remains challenging for Large Language Models (LLMs). Recent studies suggest that fine-tuning LLMs on extensive NER data can boost their performance. However, training directly on existing datasets neglects their inconsistent entity definitions and redundant data, limiting LLMs to dataset-specific learning and hindering out-of-domain adaptation. To address this, we present B2NERD, a compact dataset designed to guide LLMs' generalization in Open NER under a universal entity taxonomy. B2NERD is refined from 54 existing English and Chinese datasets using a two-step process. First, we detect inconsistent entity definitions across datasets and clarify them by distinguishable label names to construct a universal taxonomy of 400+ entity types. Second, we address redundancy using a data pruning strategy that selects fewer samples with greater category and semantic diversity. Comprehensive evaluation shows that B2NERD significantly enhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3 out-of-domain benchmarks across 15 datasets and 6 languages. The data, models, and code are publicly available at https://github.com/UmeanNever/B2NER.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Knowledge Graph Question Answering: A Survey</title>
<link>https://arxiv.org/abs/2406.14191</link>
<guid>https://arxiv.org/abs/2406.14191</guid>
<content:encoded><![CDATA[
arXiv:2406.14191v3 Announce Type: replace 
Abstract: Knowledge Base Question Answering (KBQA) has been a long-standing field to answer questions based on knowledge bases. Recently, the evolving dynamics of knowledge have attracted a growing interest in Temporal Knowledge Graph Question Answering (TKGQA), an emerging task to answer temporal questions. However, this field grapples with ambiguities in defining temporal questions and lacks a systematic categorization of existing methods for TKGQA. In response, this paper provides a thorough survey from two perspectives: the taxonomy of temporal questions and the methodological categorization for TKGQA. Specifically, we first establish a detailed taxonomy of temporal questions engaged in prior studies. Subsequently, we provide a comprehensive review of TKGQA techniques of two categories: semantic parsing-based and TKG embedding-based. Building on this review, the paper outlines potential research directions aimed at advancing the field of TKGQA. This work aims to serve as a comprehensive reference for TKGQA and to stimulate further research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveBench: A Challenging, Contamination-Limited LLM Benchmark</title>
<link>https://arxiv.org/abs/2406.19314</link>
<guid>https://arxiv.org/abs/2406.19314</guid>
<content:encoded><![CDATA[
arXiv:2406.19314v2 Announce Type: replace 
Abstract: Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training on the Test Task Confounds Evaluation and Emergence</title>
<link>https://arxiv.org/abs/2407.07890</link>
<guid>https://arxiv.org/abs/2407.07890</guid>
<content:encoded><![CDATA[
arXiv:2407.07890v3 Announce Type: replace 
Abstract: We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice. Rather, the term describes a growing set of practices that utilize knowledge about evaluation tasks at training time. We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities. We argue that the seeming superiority of one model family over another may be explained by a different degree of training on the test task. To this end, we propose an effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model under comparison on the same task-relevant data prior to evaluation. We then show that instances of emergent behavior disappear gradually as models train on the test task. Our work promotes a new perspective on the evaluation of large language models, with broad implications for benchmarking and the study of emergent capabilities.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFShip: Interpretable Fine-grained Ship Classification with Domain Knowledge-Enhanced Vision-Language Models</title>
<link>https://arxiv.org/abs/2408.06631</link>
<guid>https://arxiv.org/abs/2408.06631</guid>
<content:encoded><![CDATA[
arXiv:2408.06631v4 Announce Type: replace 
Abstract: End-to-end interpretation currently dominates the remote sensing fine-grained ship classification (RS-FGSC) task. However, the inference process remains uninterpretable, leading to criticisms of these models as "black box" systems. To address this issue, we propose a domain knowledge-enhanced Chain-of-Thought (CoT) prompt generation mechanism, which is used to semi-automatically construct a task-specific instruction-following dataset, TITANIC-FGS. By training on TITANIC-FGS, we adapt general-domain vision-language models (VLMs) to the FGSC task, resulting in a model named IFShip. Building upon IFShip, we develop an FGSC visual chatbot that redefines the FGSC problem as a step-by-step reasoning task and conveys the reasoning process in natural language. Experimental results show that IFShip outperforms state-of-the-art FGSC algorithms in both interpretability and classification accuracy. Furthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates superior performance on the FGSC task. It provides an accurate chain of reasoning when fine-grained ship types are recognizable to the human eye and offers interpretable explanations when they are not. Our dataset is publicly available at: https://github.com/lostwolves/IFShip.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualKanbaFormer: An Efficient Selective Sparse Framework for Multimodal Aspect-based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2408.15379</link>
<guid>https://arxiv.org/abs/2408.15379</guid>
<content:encoded><![CDATA[
arXiv:2408.15379v3 Announce Type: replace 
Abstract: Multimodal Aspect-based Sentiment Analysis (MABSA) enhances sentiment detection by integrating textual data with complementary modalities, such as images, to provide a more refined and comprehensive understanding of sentiment. However, conventional attention mechanisms, despite notable benchmarks, are hindered by quadratic complexity, limiting their ability to fully capture global contextual dependencies and rich semantic information in both modalities. To address this limitation, we introduce DualKanbaFormer, a novel framework that leverages parallel Textual and Visual KanbaFormer modules for robust multimodal analysis. Our approach incorporates Aspect-Driven Sparse Attention (ADSA) to dynamically balance coarse-grained aggregation and fine-grained selection for aspect-focused precision, ensuring the preservation of both global context awareness and local precision in textual and visual representations. Additionally, we utilize the Selective State Space Model (Mamba) to capture extensive global semantic information across both modalities. Furthermore, We replace traditional feed-forward networks and normalization with Kolmogorov-Arnold Networks (KANs) and Dynamic Tanh (DyT) to enhance non-linear expressivity and inference stability. To facilitate the effective integration of textual and visual features, we design a multimodal gated fusion layer that dynamically optimizes inter-modality interactions, significantly enhancing the models efficacy in MABSA tasks. Comprehensive experiments on two publicly available datasets reveal that DualKanbaFormer consistently outperforms several state-of-the-art (SOTA) models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-evolving Agents with reflective and memory-augmented abilities</title>
<link>https://arxiv.org/abs/2409.00872</link>
<guid>https://arxiv.org/abs/2409.00872</guid>
<content:encoded><![CDATA[
arXiv:2409.00872v2 Announce Type: replace 
Abstract: Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making. In this research, we propose a novel framework by integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Specific Directions: Definition, Exploration, and Utilization in Parameter Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2409.01035</link>
<guid>https://arxiv.org/abs/2409.01035</guid>
<content:encoded><![CDATA[
arXiv:2409.01035v4 Announce Type: replace 
Abstract: Large language models demonstrate impressive performance on downstream tasks, yet they require extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directions (TSDs), which are critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning process, thereby enhancing model performance on targeted tasks. Additionally, based on our exploration of TSD, we focus on an important issue in PEFT: the initialization of LoRA. While some works have pointed out the significance of initialization for LoRA's performance and proposed various strategies, these methods are often empirical and not task-specific. To address this issue, we propose LoRA-Init. Starting from TSD, we identify the directions that require the most adjustment during fine-tuning for downstream tasks. By initializing the matrices in LoRA with these directions, LoRA-Init significantly enhances LoRA's performance. Moreover, we can combine LoRA-Dash and LoRA-Init to create the final version of LoRA based on TSDs, which we refer to as LoRA-TSD. Extensive experiments have conclusively demonstrated the effectiveness of these methods, and in-depth analyses further reveal the underlying mechanisms behind their success.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seek and Solve Reasoning for Table Question Answering</title>
<link>https://arxiv.org/abs/2409.05286</link>
<guid>https://arxiv.org/abs/2409.05286</guid>
<content:encoded><![CDATA[
arXiv:2409.05286v3 Announce Type: replace 
Abstract: The complexities of table structures and question logic make table-based question answering (TQA) tasks challenging for Large Language Models (LLMs), often requiring task simplification before solving. This paper reveals that the reasoning process during task simplification may be more valuable than the simplified tasks themselves and aims to improve TQA performance by leveraging LLMs' reasoning capabilities. We propose a Seek-and-Solve pipeline that instructs the LLM to first seek relevant information and then answer questions, integrating these two stages at the reasoning level into a coherent Seek-and-Solve Chain of Thought (SS-CoT). Additionally, we distill a single-step TQA-solving prompt from this pipeline, using demonstrations with SS-CoT paths to guide the LLM in solving complex TQA tasks under In-Context Learning settings. Our experiments show that our approaches result in improved performance and reliability while being efficient. Our findings emphasize the importance of eliciting LLMs' reasoning capabilities to handle complex TQA tasks effectively.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval</title>
<link>https://arxiv.org/abs/2410.04585</link>
<guid>https://arxiv.org/abs/2410.04585</guid>
<content:encoded><![CDATA[
arXiv:2410.04585v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated significant potential in clinical decision support. Yet LLMs still suffer from hallucinations and lack fine-grained contextual medical knowledge, limiting their high-stake healthcare applications such as clinical diagnosis. Traditional retrieval-augmented generation (RAG) methods attempt to address these limitations but frequently retrieve sparse or irrelevant information, undermining prediction accuracy. We introduce KARE, a novel framework that integrates knowledge graph (KG) community-level retrieval with LLM reasoning to enhance healthcare predictions. KARE constructs a comprehensive multi-source KG by integrating biomedical databases, clinical literature, and LLM-generated insights, and organizes it using hierarchical graph community detection and summarization for precise and contextually relevant information retrieval. Our key innovations include: (1) a dense medical knowledge structuring approach enabling accurate retrieval of relevant information; (2) a dynamic knowledge retrieval mechanism that enriches patient contexts with focused, multi-faceted medical insights; and (3) a reasoning-enhanced prediction framework that leverages these enriched contexts to produce both accurate and interpretable clinical predictions. Extensive experiments demonstrate that KARE outperforms leading models by up to 10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and readmission predictions. In addition to its impressive prediction accuracy, our framework leverages the reasoning capabilities of LLMs, enhancing the trustworthiness of clinical predictions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Training Data of Large Language Models via Expectation Maximization</title>
<link>https://arxiv.org/abs/2410.07582</link>
<guid>https://arxiv.org/abs/2410.07582</guid>
<content:encoded><![CDATA[
arXiv:2410.07582v2 Announce Type: replace 
Abstract: The advancement of large language models has grown parallel to the opacity of their training data. Membership inference attacks (MIAs) aim to determine whether specific data was used to train a model. They offer valuable insights into detecting data contamination and ensuring compliance with privacy and copyright standards. However, MIA for LLMs is challenging due to the massive scale of training data and the inherent ambiguity of membership in texts. Moreover, creating realistic MIA evaluation benchmarks is difficult as training and test data distributions are often unknown. We introduce EM-MIA, a novel membership inference method that iteratively refines membership scores and prefix scores via an expectation-maximization algorithm. Our approach leverages the observation that these scores can improve each other: membership scores help identify effective prefixes for detecting training data, while prefix scores help determine membership. As a result, EM-MIA achieves state-of-the-art results on WikiMIA. To enable comprehensive evaluation, we introduce OLMoMIA, a benchmark built from OLMo resources, which allows controlling task difficulty through varying degrees of overlap between training and test data distributions. Our experiments demonstrate EM-MIA is robust across different scenarios while also revealing fundamental limitations of current MIA approaches when member and non-member distributions are nearly identical.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nudging: Inference-time Alignment of LLMs via Guided Decoding</title>
<link>https://arxiv.org/abs/2410.09300</link>
<guid>https://arxiv.org/abs/2410.09300</guid>
<content:encoded><![CDATA[
arXiv:2410.09300v3 Announce Type: replace 
Abstract: Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose nudging, a simple, plug-and-play, and training-free algorithm that aligns any base model at inference time using a small aligned model. Nudging is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, nudging employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high. We evaluate nudging across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, nudging enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-2-7b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our project website: https://fywalter.github.io/nudging/ .
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus</title>
<link>https://arxiv.org/abs/2410.14815</link>
<guid>https://arxiv.org/abs/2410.14815</guid>
<content:encoded><![CDATA[
arXiv:2410.14815v2 Announce Type: replace 
Abstract: Multilingual LLMs support a variety of languages; however, their performance is suboptimal for low-resource languages. In this work, we emphasize the importance of continued pre-training of multilingual LLMs and the use of translation-based synthetic pre-training corpora for improving LLMs in low-resource languages. We conduct our study in the context of the low-resource Indic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM supporting both Hindi and English, based on Nemotron-Mini 4B. The model is trained using a mix of real and synthetic Hindi + English tokens, with continuous pre-training performed on 400B tokens. We demonstrate that both the base and instruct models achieve state-of-the-art results on Hindi benchmarks while remaining competitive on English tasks. Additionally, we observe that the continued pre-training approach enhances the model's overall factual accuracy. We perform an ablation study to highlight the impact of Hindi pre-training, showing significant improvements in Hindi chat capabilities and factual accuracy, which cannot be achieved through Hindi alignment alone.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment</title>
<link>https://arxiv.org/abs/2410.16714</link>
<guid>https://arxiv.org/abs/2410.16714</guid>
<content:encoded><![CDATA[
arXiv:2410.16714v3 Announce Type: replace 
Abstract: Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) performance but also overcomes the limitations of traditional Bradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a preference-based, two-player constant-sum game. However, existing methods either guarantee only average-iterate convergence, incurring high storage and inference costs, or converge to the NE of a regularized game, failing to accurately reflect true human preferences. In this paper, we introduce Magnetic Preference Optimization (MPO), a novel approach capable of achieving last-iterate convergence to the NE of the original game, effectively overcoming the limitations of existing methods. Building upon Magnetic Mirror Descent (MMD), MPO attains a linear convergence rate, making it particularly suitable for fine-tuning LLMs. To ensure our algorithm is both theoretically sound and practically viable, we present a simple yet effective implementation that adapts the theoretical insights to the RLHF setting. Empirical results demonstrate that MPO can significantly enhance the performance of LLMs, highlighting the potential of self-play methods in alignment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine Translation</title>
<link>https://arxiv.org/abs/2410.20941</link>
<guid>https://arxiv.org/abs/2410.20941</guid>
<content:encoded><![CDATA[
arXiv:2410.20941v4 Announce Type: replace 
Abstract: Large language models (LLMs) have excelled in various NLP tasks, including machine translation (MT), yet most studies focus on sentence-level translation. This work investigates the inherent capability of instruction-tuned LLMs for document-level translation (docMT). Unlike prior approaches that require specialized techniques, we evaluate LLMs by directly prompting them to translate entire documents in a single pass. Our results show that this method improves translation quality compared to translating sentences separately, even without document-level fine-tuning. However, this advantage is not reflected in BLEU scores, which often favor sentence-based translations. We propose using the LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess document coherence, accuracy, and fluency in a more nuanced way than n-gram-based metrics. Overall, our work demonstrates that instruction-tuned LLMs can effectively leverage document context for translation. However, we caution against using BLEU scores for evaluating docMT, as they often provide misleading outcomes, failing to capture the quality of document-level translation. Code and the outputs from GPT4-as-a-judge are available at https://github.com/EIT-NLP/BLEUless_DocMT
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents</title>
<link>https://arxiv.org/abs/2411.00927</link>
<guid>https://arxiv.org/abs/2411.00927</guid>
<content:encoded><![CDATA[
arXiv:2411.00927v2 Announce Type: replace 
Abstract: Large language model (LLM)-based agents are increasingly employed to interact with external environments (e.g., games, APIs, world models) to solve user-provided tasks. However, current frameworks often lack the ability to collaborate effectively with users in fully conversational settings. Conversations are essential for aligning on task details, achieving user-defined goals, and satisfying preferences. While existing agents address ambiguity through clarification questions, they underutilize the broader potential of an LLM's conversational capabilities. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. By alternating between task-solving actions and interactive conversations, ReSpAct demonstrates improved performance across diverse environments. We evaluate ReSpAct in user-interactive settings, including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate improvements of 6% and 4% in ALFWorld and WebShop, respectively, and achieves a 5.5% gain in Inform and a 3% gain in Success scores in MultiWOZ. These results highlight the value of integrating dynamic user-agent collaboration for more effective task resolution.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHATTER: A Character Attribution Dataset for Narrative Understanding</title>
<link>https://arxiv.org/abs/2411.05227</link>
<guid>https://arxiv.org/abs/2411.05227</guid>
<content:encoded><![CDATA[
arXiv:2411.05227v2 Announce Type: replace 
Abstract: Computational narrative understanding studies the identification, description, and interaction of the elements of a narrative: characters, attributes, events, and relations. Narrative research has given considerable attention to defining and classifying character types. However, these character-type taxonomies do not generalize well because they are small, too simple, or specific to a domain. We require robust and reliable benchmarks to test whether narrative models truly understand the nuances of the character's development in the story. Our work addresses this by curating the CHATTER dataset that labels whether a character portrays some attribute for 88124 character-attribute pairs, encompassing 2998 characters, 12967 attributes and 660 movies. We validate a subset of CHATTER, called CHATTEREVAL, using human annotations to serve as a benchmark to evaluate the character attribution task in movie scripts. \evaldataset{} also assesses narrative understanding and the long-context modeling capacity of language models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactLens: Benchmarking Fine-Grained Fact Verification</title>
<link>https://arxiv.org/abs/2411.05980</link>
<guid>https://arxiv.org/abs/2411.05980</guid>
<content:encoded><![CDATA[
arXiv:2411.05980v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift toward fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Star Attention: Efficient LLM Inference over Long Sequences</title>
<link>https://arxiv.org/abs/2411.17116</link>
<guid>https://arxiv.org/abs/2411.17116</guid>
<content:encoded><![CDATA[
arXiv:2411.17116v2 Announce Type: replace 
Abstract: Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation</title>
<link>https://arxiv.org/abs/2412.01626</link>
<guid>https://arxiv.org/abs/2412.01626</guid>
<content:encoded><![CDATA[
arXiv:2412.01626v3 Announce Type: replace 
Abstract: The use of Large Language Models (LLMs) has increased significantly with users frequently asking questions to chatbots. In the time when information is readily accessible, it is crucial to stimulate and preserve human cognitive abilities and maintain strong reasoning skills. This paper addresses such challenges by promoting the use of hints as an alternative or a supplement to direct answers. We first introduce a manually constructed hint dataset, WikiHint, which is based on Wikipedia and includes 5,000 hints created for 1,000 questions. We then finetune open-source LLMs for hint generation in answer-aware and answer-agnostic contexts. We assess the effectiveness of the hints with human participants who answer questions with and without the aid of hints. Additionally, we introduce a lightweight evaluation method, HintRank, to evaluate and rank hints in both answer-aware and answer-agnostic settings. Our findings show that (a) the dataset helps generate more effective hints, (b) including answer information along with questions generally improves the quality of generated hints, and (c) encoder-based models perform better than decoder-based models in hint ranking.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unanswerability Evaluation for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2412.12300</link>
<guid>https://arxiv.org/abs/2412.12300</guid>
<content:encoded><![CDATA[
arXiv:2412.12300v3 Announce Type: replace 
Abstract: Existing evaluation frameworks for retrieval-augmented generation (RAG) systems focus on answerable queries, but they overlook the importance of appropriately rejecting unanswerable requests. In this paper, we introduce UAEval4RAG, a framework designed to evaluate whether RAG systems can handle unanswerable queries effectively. We define a taxonomy with six unanswerable categories, and UAEval4RAG automatically synthesizes diverse and challenging queries for any given knowledge base with unanswered ratio and acceptable ratio metrics. We conduct experiments with various RAG components, including retrieval models, rewriting methods, rerankers, language models, and prompting strategies, and reveal hidden trade-offs in performance of RAG systems. Our findings highlight the critical role of component selection and prompt design in optimizing RAG systems to balance the accuracy of answerable queries with high rejection rates of unanswerable ones. UAEval4RAG provides valuable insights and tools for developing more robust and reliable RAG systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State Space Models are Strong Text Rerankers</title>
<link>https://arxiv.org/abs/2412.14354</link>
<guid>https://arxiv.org/abs/2412.14354</guid>
<content:encoded><![CDATA[
arXiv:2412.14354v2 Announce Type: replace 
Abstract: Transformers dominate NLP and IR; but their inference inefficiencies and challenges in extrapolating to longer contexts have sparked interest in alternative model architectures. Among these, state space models (SSMs) like Mamba offer promising advantages, particularly $O(1)$ time complexity in inference. Despite their potential, SSMs' effectiveness at text reranking\, -- \,a task requiring fine-grained query-document interaction and long-context understanding\, -- \,remains underexplored. This study benchmarks SSM-based architectures (specifically, Mamba-1 and Mamba-2) against transformer-based models across various scales, architectures, and pre-training objectives, focusing on performance and efficiency in text reranking tasks. We find that (1) Mamba architectures achieve competitive text ranking performance, comparable to transformer-based models of similar size; (2) they are less efficient in training and inference compared to transformers with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance and efficiency. These results underscore the potential of state space models as a transformer alternative and highlight areas for improvement in future IR applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation</title>
<link>https://arxiv.org/abs/2501.05414</link>
<guid>https://arxiv.org/abs/2501.05414</guid>
<content:encoded><![CDATA[
arXiv:2501.05414v2 Announce Type: replace 
Abstract: Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens. We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation. LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans. These tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens). Furthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation. We evaluated 23 LCLMs, including instruction-tuned models and recent reasoning models, on LongProc at three difficulty levels, with the maximum number of output tokens set at 500, 2K, and 8K. Notably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks. Reasoning models achieve stronger overall performance in long-form generation, benefiting from long CoT training. Further analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations. These findings highlight critical limitations in current LCLMs and suggest substantial room for improvement. Data and code available at: https://princeton-pli.github.io/LongProc.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond</title>
<link>https://arxiv.org/abs/2501.05714</link>
<guid>https://arxiv.org/abs/2501.05714</guid>
<content:encoded><![CDATA[
arXiv:2501.05714v2 Announce Type: replace 
Abstract: With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Idiom Detection in Sorani Kurdish Texts</title>
<link>https://arxiv.org/abs/2501.14528</link>
<guid>https://arxiv.org/abs/2501.14528</guid>
<content:encoded><![CDATA[
arXiv:2501.14528v3 Announce Type: replace 
Abstract: Idiom detection using Natural Language Processing (NLP) is the computerized process of recognizing figurative expressions within a text that convey meanings beyond the literal interpretation of the words. While idiom detection has seen significant progress across various languages, the Kurdish language faces a considerable research gap in this area despite the importance of idioms in tasks like machine translation and sentiment analysis. This study addresses idiom detection in Sorani Kurdish by approaching it as a text classification task using deep learning techniques. To tackle this, we developed a dataset containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse contexts. Using this dataset, we developed and evaluated three deep learning models: KuBERT-based transformer sequence classification, a Recurrent Convolutional Neural Network (RCNN), and a BiLSTM model with an attention mechanism. The evaluations revealed that the transformer model, the fine-tuned BERT, consistently outperformed the others, achieving nearly 99% accuracy while the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the effectiveness of Transformer-based architectures in low-resource languages like Kurdish. This research provides a dataset, three optimized models, and insights into idiom detection, laying a foundation for advancing Kurdish NLP.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet</title>
<link>https://arxiv.org/abs/2502.05291</link>
<guid>https://arxiv.org/abs/2502.05291</guid>
<content:encoded><![CDATA[
arXiv:2502.05291v2 Announce Type: replace 
Abstract: Large language models (LLMs) have become ubiquitous, thus it is important to understand their risks and limitations. Smaller LLMs can be deployed where compute resources are constrained, such as edge devices, but with different propensity to generate harmful output. Mitigation of LLM harm typically depends on annotating the harmfulness of LLM output, which is expensive to collect from humans. This work studies two questions: How do smaller LLMs rank regarding generation of harmful content? How well can larger LLMs annotate harmfulness? We prompt three small LLMs to elicit harmful content of various types, such as discriminatory language, offensive content, privacy invasion, or negative influence, and collect human rankings of their outputs. Then, we evaluate three state-of-the-art large LLMs on their ability to annotate the harmfulness of these responses. We find that the smaller models differ with respect to harmfulness. We also find that large LLMs show low to moderate agreement with humans. These findings underline the need for further work on harm mitigation in LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models</title>
<link>https://arxiv.org/abs/2502.07346</link>
<guid>https://arxiv.org/abs/2502.07346</guid>
<content:encoded><![CDATA[
arXiv:2502.07346v2 Announce Type: replace 
Abstract: Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparQLe: Speech Queries to Text Translation Through LLMs</title>
<link>https://arxiv.org/abs/2502.09284</link>
<guid>https://arxiv.org/abs/2502.09284</guid>
<content:encoded><![CDATA[
arXiv:2502.09284v2 Announce Type: replace 
Abstract: With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that leverages self-supervised speech representations in combination with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English-language data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising solution for various speech understanding applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States</title>
<link>https://arxiv.org/abs/2502.14744</link>
<guid>https://arxiv.org/abs/2502.14744</guid>
<content:encoded><![CDATA[
arXiv:2502.14744v3 Announce Type: replace 
Abstract: The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</title>
<link>https://arxiv.org/abs/2502.14866</link>
<guid>https://arxiv.org/abs/2502.14866</guid>
<content:encoded><![CDATA[
arXiv:2502.14866v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable potential in processing long sequences and complex reasoning tasks, yet efficiently serving these models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context and reasoning capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores</title>
<link>https://arxiv.org/abs/2502.16358</link>
<guid>https://arxiv.org/abs/2502.16358</guid>
<content:encoded><![CDATA[
arXiv:2502.16358v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are revolutionizing information retrieval, with chatbots becoming an important source for answering user queries. As by their design, LLMs prioritize generating correct answers, the value of highly plausible yet incorrect answers (candidate answers) tends to be overlooked. However, such answers can still prove useful, for example, they can play a crucial role in tasks like Multiple-Choice Question Answering (MCQA) and QA Robustness Assessment (QARA). Existing QA datasets primarily focus on correct answers without explicit consideration of the plausibility of other candidate answers, limiting opportunity for more nuanced evaluations of models. To address this gap, we introduce PlausibleQA, a large-scale dataset comprising 10,000 questions and 100,000 candidate answers, each annotated with plausibility scores and justifications for their selection. Additionally, the dataset includes 900,000 justifications for pairwise comparisons between candidate answers, further refining plausibility assessments. We evaluate PlausibleQA through human assessments and empirical experiments, demonstrating its utility in MCQA and QARA analysis. Our findings show that plausibility-aware approaches are effective for MCQA distractor generation and QARA. We release PlausibleQA as a resource for advancing QA research and enhancing LLM performance in distinguishing plausible distractors from correct answers.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Multiple Large Language Models: A Survey on LLM Ensemble</title>
<link>https://arxiv.org/abs/2502.18036</link>
<guid>https://arxiv.org/abs/2502.18036</guid>
<content:encoded><![CDATA[
arXiv:2502.18036v3 Announce Type: replace 
Abstract: LLM Ensemble -- which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths -- has gained substantial attention recently. The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. This paper presents the first systematic review of recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM Ensemble and discuss several related research problems. Then, we provide a more in-depth classification of the methods under the broad categories of "ensemble-before-inference, ensemble-during-inference, ensemble-after-inference'', and review all relevant methods. Finally, we introduce related benchmarks and applications, summarize existing studies, and suggest several future research directions. A curated list of papers on LLM Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Wave Functions: Exploring Meaning in Large Language Models Through Quantum Formalism</title>
<link>https://arxiv.org/abs/2503.10664</link>
<guid>https://arxiv.org/abs/2503.10664</guid>
<content:encoded><![CDATA[
arXiv:2503.10664v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) encode semantic relationships in high-dimensional vector embeddings. This paper explores the analogy between LLM embedding spaces and quantum mechanics, positing that LLMs operate within a quantized semantic space where words and phrases behave as quantum states. To capture nuanced semantic interference effects, we extend the standard real-valued embedding space to the complex domain, drawing parallels to the double-slit experiment. We introduce a "semantic wave function" to formalize this quantum-derived representation and utilize potential landscapes, such as the double-well potential, to model semantic ambiguity. Furthermore, we propose a complex-valued similarity measure that incorporates both magnitude and phase information, enabling a more sensitive comparison of semantic representations. We develop a path integral formalism, based on a nonlinear Schr\"odinger equation with a gauge field and Mexican hat potential, to model the dynamic evolution of LLM behavior. This interdisciplinary approach offers a new theoretical framework for understanding and potentially manipulating LLMs, with the goal of advancing both artificial and natural language understanding.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Halving transcription time: A fast, user-friendly and GDPR-compliant workflow to create AI-assisted transcripts for content analysis</title>
<link>https://arxiv.org/abs/2503.13031</link>
<guid>https://arxiv.org/abs/2503.13031</guid>
<content:encoded><![CDATA[
arXiv:2503.13031v2 Announce Type: replace 
Abstract: In qualitative research, data transcription is often labor-intensive and time-consuming. To expedite this process, a workflow utilizing artificial intelligence (AI) was developed. This workflow not only enhances transcription speed but also addresses the issue of AI-generated transcripts often lacking compatibility with standard content analysis software. Within this workflow, automatic speech recognition is employed to create initial transcripts from audio recordings, which are then formatted to be compatible with content analysis software such as ATLAS or MAXQDA. Empirical data from a study of 12 interviews suggests that this workflow can reduce transcription time by up to 76.4%. Furthermore, by using widely used standard software, this process is suitable for both students and researchers while also being adaptable to a variety of learning, teaching, and research environments. It is also particularly beneficial for non-native speakers. In addition, the workflow is GDPR-compliant and facilitates local, offline transcript generation, which is crucial when dealing with sensitive data.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Language Anchor-Guided Method for Robust Noisy Domain Generalization</title>
<link>https://arxiv.org/abs/2503.17211</link>
<guid>https://arxiv.org/abs/2503.17211</guid>
<content:encoded><![CDATA[
arXiv:2503.17211v2 Announce Type: replace 
Abstract: Real-world machine learning applications often struggle with two major challenges: distribution shift and label noise. Models tend to overfit by focusing on redundant and uninformative features in the training data, which makes it hard for them to generalize to the target domain. Noisy data worsens this problem by causing further overfitting to the noise, meaning that existing methods often fail to tell the difference between true, invariant features and misleading, spurious ones. To tackle these issues, we introduce Anchor Alignment and Adaptive Weighting (A3W). This new algorithm uses sample reweighting guided by natural language processing (NLP) anchors to extract more representative features. In simple terms, A3W leverages semantic representations from natural language models as a source of domain-invariant prior knowledge. Additionally, it employs a weighted loss function that adjusts each sample's contribution based on its similarity to the corresponding NLP anchor. This adjustment makes the model more robust to noisy labels. Extensive experiments on standard benchmark datasets show that A3W consistently outperforms state-of-the-art domain generalization methods, offering significant improvements in both accuracy and robustness across different datasets and noise levels.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents That Act Like Us: Accurate Human Behavior Simulation with Real-World Data</title>
<link>https://arxiv.org/abs/2503.20749</link>
<guid>https://arxiv.org/abs/2503.20749</guid>
<content:encoded><![CDATA[
arXiv:2503.20749v4 Announce Type: replace 
Abstract: Recent research shows that LLMs can simulate ``believable'' human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating and improving LLM's objective ``accuracy'' rather than the subjective ``believability'' in the web action generation task, leveraging a large-scale, real-world dataset collected from online shopping human actions. We present the first comprehensive quantitative evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action generation. Our results show that fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasoning traces into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work establishes a new benchmark for evaluating LLMs in behavior simulation and offers actionable insights into how real-world action data and reasoning augmentation can enhance the fidelity of LLM agents.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging</title>
<link>https://arxiv.org/abs/2503.21088</link>
<guid>https://arxiv.org/abs/2503.21088</guid>
<content:encoded><![CDATA[
arXiv:2503.21088v2 Announce Type: replace 
Abstract: This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCORE: Story Coherence and Retrieval Enhancement for AI Narratives</title>
<link>https://arxiv.org/abs/2503.23512</link>
<guid>https://arxiv.org/abs/2503.23512</guid>
<content:encoded><![CDATA[
arXiv:2503.23512v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) can generate creative and engaging narratives from user-specified input, but maintaining coherence and emotional depth throughout these AI-generated stories remains a challenge. In this work, we propose SCORE, a framework for Story Coherence and Retrieval Enhancement, designed to detect and resolve narrative inconsistencies. By tracking key item statuses and generating episode summaries, SCORE uses a Retrieval-Augmented Generation (RAG) approach, incorporating TF-IDF and cosine similarity to identify related episodes and enhance the overall story structure. Results from testing multiple LLM-generated stories demonstrate that SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models, providing a more robust method for evaluating and refining AI-generated narratives.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection</title>
<link>https://arxiv.org/abs/2504.00695</link>
<guid>https://arxiv.org/abs/2504.00695</guid>
<content:encoded><![CDATA[
arXiv:2504.00695v3 Announce Type: replace 
Abstract: Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data quality metrics and mixing proportions, yet they fail to adequately capture the underlying semantic connections between training samples and quality disparities within individual domains. We introduce ToReMi (Topic-based Reweighting for Model improvement), a novel two-stage framework that dynamically adjusts training sample weights according to their topical associations and observed learning patterns. Our comprehensive experiments reveal that ToReMi variants consistently achieve superior performance over conventional pre-training approaches, demonstrating accelerated perplexity reduction across multiple domains and enhanced capabilities on downstream evaluation tasks. Code is available at https://github.com/zxx000728/ToReMi.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation</title>
<link>https://arxiv.org/abs/2504.02438</link>
<guid>https://arxiv.org/abs/2504.02438</guid>
<content:encoded><![CDATA[
arXiv:2504.02438v3 Announce Type: replace 
Abstract: Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLaMP, a hierarchical video-language model that processes hour-long videos at ``mixed precision'' through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLaMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLaMP's superior performance across four video understanding benchmarks, particularly on long-form content. Notably, ViLaMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers</title>
<link>https://arxiv.org/abs/2504.03595</link>
<guid>https://arxiv.org/abs/2504.03595</guid>
<content:encoded><![CDATA[
arXiv:2504.03595v2 Announce Type: replace 
Abstract: A key element to support the increased amounts of renewable energy in the energy system is flexibility, i.e., the possibility of changing energy loads in time and amount. Many flexibility models have been designed; however, exact models fail to scale for long time horizons or many devices. Because of this, the FlexOffer (FOs) model has been designed, to provide device-independent approximations of flexibility with good accuracy, and much better scaling for long time horizons and many devices. An important aspect of the real-life implementation of energy flexibility is enabling flexible data exchange with many types of smart energy appliances and market systems, e.g., in smart buildings. For this, ontologies standardizing data formats are required. However, the current industry standard ontology for integrating smart devices for energy purposes, SAREF for Energy Flexibility (SAREF4ENER) only has limited support for flexibility and thus cannot support important use cases. In this paper we propose an extension of SAREF4ENER that integrates full support for the complete FlexOffer model, including advanced use cases, while maintaining backward compatibility. This novel ontology module can accurately describe flexibility for advanced devices such as electric vehicles, batteries, and heat pumps. It can also capture the inherent uncertainty associated with many flexible load types.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAACL2025 Tutorial: Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.03931</link>
<guid>https://arxiv.org/abs/2504.03931</guid>
<content:encoded><![CDATA[
arXiv:2504.03931v2 Announce Type: replace 
Abstract: This tutorial on adaptation of LLMs is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques. While general LLMs have demonstrated strong generalization across a variety of tasks, they often struggle to perform well in specialized domains such as finance, healthcare, and code generation for underrepresented languages. Additionally, their static nature limits their ability to evolve with the changing world, and they are often extremely large in size, making them impractical and costly to deploy at scale. As a result, the adaptation of LLMs has drawn much attention since the birth of LLMs and is of core importance, both for industry, which focuses on serving its targeted users, and academia, which can greatly benefit from small but powerful LLMs. To address this gap, this tutorial aims to provide an overview of the LLM adaptation techniques. We start with an introduction to LLM adaptation, from both the data perspective and the model perspective. We then emphasize how the evaluation metrics and benchmarks are different from other techniques. After establishing the problems, we explore various adaptation techniques. We categorize adaptation techniques into two main families. The first is parametric knowledge adaptation, which focuses on updating the parametric knowledge within LLMs. Additionally, we will discuss real-time adaptation techniques, including model editing, which allows LLMs to be updated dynamically in production environments. The second kind of adaptation is semi-parametric knowledge adaptation, where the goal is to update LLM parameters to better leverage external knowledge or tools through techniques like retrieval-augmented generation (RAG) and agent-based systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs</title>
<link>https://arxiv.org/abs/2504.04994</link>
<guid>https://arxiv.org/abs/2504.04994</guid>
<content:encoded><![CDATA[
arXiv:2504.04994v2 Announce Type: replace 
Abstract: Despite the impressive performance of large language models (LLMs), they can present unintended biases and harmful behaviors driven by encoded values, emphasizing the urgent need to understand the value mechanisms behind them. However, current research primarily evaluates these values through external responses with a focus on AI safety, lacking interpretability and failing to assess social values in real-world contexts. In this paper, we propose a novel framework called ValueExploration, which aims to explore the behavior-driven mechanisms of National Social Values within LLMs at the neuron level. As a case study, we focus on Chinese Social Values and first construct C-voice, a large-scale bilingual benchmark for identifying and evaluating Chinese Social Values in LLMs. By leveraging C-voice, we then identify and locate the neurons responsible for encoding these values according to activation difference. Finally, by deactivating these neurons, we analyze shifts in model behavior, uncovering the internal mechanism by which values influence LLM decision-making. Extensive experiments on four representative LLMs validate the efficacy of our framework. The benchmark and code will be available.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games</title>
<link>https://arxiv.org/abs/2504.06868</link>
<guid>https://arxiv.org/abs/2504.06868</guid>
<content:encoded><![CDATA[
arXiv:2504.06868v2 Announce Type: replace 
Abstract: Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation</title>
<link>https://arxiv.org/abs/2504.07532</link>
<guid>https://arxiv.org/abs/2504.07532</guid>
<content:encoded><![CDATA[
arXiv:2504.07532v2 Announce Type: replace 
Abstract: AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that most of the competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents</title>
<link>https://arxiv.org/abs/2504.09407</link>
<guid>https://arxiv.org/abs/2504.09407</guid>
<content:encoded><![CDATA[
arXiv:2504.09407v2 Announce Type: replace 
Abstract: Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate a web design, but\textbf{ how to evaluate and iterate the usability testing study design } itself? Recent advances in Large Language Model-simulated Agent (\textbf{LLM Agent}) research inspired us to design \textbf{UXAgent} to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users to interactively test the target website. The system also provides an Agent Interview Interface and a Video Replay Interface so that the UX researchers can easily review and analyze the generated qualitative and quantitative log data. Through a heuristic evaluation, five UX researcher participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families</title>
<link>https://arxiv.org/abs/2504.10340</link>
<guid>https://arxiv.org/abs/2504.10340</guid>
<content:encoded><![CDATA[
arXiv:2504.10340v2 Announce Type: replace 
Abstract: Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings -- extracted via an LLM-assisted annotation pipeline -- serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Knowledge Manipulation in a Russian Wikipedia Fork</title>
<link>https://arxiv.org/abs/2504.10663</link>
<guid>https://arxiv.org/abs/2504.10663</guid>
<content:encoded><![CDATA[
arXiv:2504.10663v2 Announce Type: replace 
Abstract: Wikipedia is powered by MediaWiki, a free and open-source software that is also the infrastructure for many other wiki-based online encyclopedias. These include the recently launched website Ruwiki, which has copied and modified the original Russian Wikipedia content to conform to Russian law. To identify practices and narratives that could be associated with different forms of knowledge manipulation, this article presents an in-depth analysis of this Russian Wikipedia fork. We propose a methodology to characterize the main changes with respect to the original version. The foundation of this study is a comprehensive comparative analysis of more than 1.9M articles from Russian Wikipedia and its fork. Using meta-information and geographical, temporal, categorical, and textual features, we explore the changes made by Ruwiki editors. Furthermore, we present a classification of the main topics of knowledge manipulation in this fork, including a numerical estimation of their scope. This research not only sheds light on significant changes within Ruwiki, but also provides a methodology that could be applied to analyze other Wikipedia forks and similar collaborative projects.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge</title>
<link>https://arxiv.org/abs/2312.05693</link>
<guid>https://arxiv.org/abs/2312.05693</guid>
<content:encoded><![CDATA[
arXiv:2312.05693v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) stand out for their impressive performance in intricate language modeling tasks. However, their demanding computational and memory needs pose obstacles for broad use on edge devices. Quantization is then introduced to boost LLMs' on-device efficiency. Recent works show that 8-bit or lower weight quantization is feasible with minimal impact on end-to-end task performance, while the activation is still not quantized. On the other hand, mainstream commodity edge devices still struggle to execute these sub-8-bit quantized networks effectively. In this paper, we propose Agile-Quant, an activation-guided quantization framework for popular Large Language Models (LLMs), and implement an end-to-end accelerator on multiple edge devices for faster inference. Considering the hardware profiling and activation analysis, we first introduce a basic activation quantization strategy to balance the trade-off of task performance and real inference speed. Then we leverage the activation-aware token pruning technique to reduce the outliers and the adverse impact on attentivity. Ultimately, we utilize the SIMD-based 4-bit multiplier and our efficient TRIP matrix multiplication to implement the accelerator for LLMs on the edge. We apply our framework on different scales of LLMs including LLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the weight quantization. Experiments show that Agile-Quant achieves simultaneous quantization of model weights and activations while maintaining task performance comparable to existing weight-only quantization methods. Moreover, in the 8- and 4-bit scenario, Agile-Quant achieves an on-device speedup of up to 2.55x compared to its FP16 counterparts across multiple edge devices, marking a pioneering advancement in this domain. Code: https://github.com/shawnricecake/agile-quant
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Ontologies via Incorporating Extensional and Intensional Knowledge</title>
<link>https://arxiv.org/abs/2402.01677</link>
<guid>https://arxiv.org/abs/2402.01677</guid>
<content:encoded><![CDATA[
arXiv:2402.01677v5 Announce Type: replace-cross 
Abstract: Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can capture both structure information and textual information. Experimental results show that EIKE significantly outperforms state-of-the-art methods in three datasets for both triple classification and link prediction, indicating that EIKE provides a more comprehensive and representative perspective of the domain.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLMRec: Distilling Large Language Models into Small for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2405.17890</link>
<guid>https://arxiv.org/abs/2405.17890</guid>
<content:encoded><![CDATA[
arXiv:2405.17890v4 Announce Type: replace-cross 
Abstract: Sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. The SR models examine the sequence of a user's actions to discern more complex behavioral patterns and temporal dynamics. Recent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation. Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene. Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in real-world platforms that often need to process billions of traffic logs daily. In this paper, we explore the influence of LLMs' depth by conducting extensive experiments on large-scale industry datasets. Surprisingly, our motivational experiments reveal that most intermediate layers of LLMs are redundant, indicating that pruning the remaining layers can still maintain strong performance. Motivated by this insight, we empower small language models for SR, namely SLMRec, which adopt a simple yet effective knowledge distillation method. Moreover, SLMRec is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination. Comprehensive experimental results illustrate that the proposed SLMRec model attains the best performance using only 13% of the parameters found in LLM-based recommendation models while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively. Besides, we provide a theoretical justification for why small language models can perform comparably to large language models in SR.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DataComp-LM: In search of the next generation of training sets for language models</title>
<link>https://arxiv.org/abs/2406.11794</link>
<guid>https://arxiv.org/abs/2406.11794</guid>
<content:encoded><![CDATA[
arXiv:2406.11794v4 Announce Type: replace-cross 
Abstract: We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking as a Reward Misspecification Problem</title>
<link>https://arxiv.org/abs/2406.14393</link>
<guid>https://arxiv.org/abs/2406.14393</guid>
<content:encoded><![CDATA[
arXiv:2406.14393v5 Announce Type: replace-cross 
Abstract: The widespread adoption of large language models (LLMs) has raised concerns about their safety and reliability, particularly regarding their vulnerability to adversarial attacks. In this paper, we propose a novel perspective that attributes this vulnerability to reward misspecification during the alignment process. This misspecification occurs when the reward function fails to accurately capture the intended behavior, leading to misaligned model outputs. We introduce a metric ReGap to quantify the extent of reward misspecification and demonstrate its effectiveness and robustness in detecting harmful backdoor prompts. Building upon these insights, we present ReMiss, a system for automated red teaming that generates adversarial prompts in a reward-misspecified space. ReMiss achieves state-of-the-art attack success rates on the AdvBench benchmark against various target aligned LLMs while preserving the human readability of the generated prompts. Furthermore, these attacks on open-source models demonstrate high transferability to closed-source models like GPT-4o and out-of-distribution tasks from HarmBench. Detailed analysis highlights the unique advantages of the proposed reward misspecification objective compared to previous methods, offering new insights for improving LLM safety and robustness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHands: An Open Platform for AI Software Developers as Generalist Agents</title>
<link>https://arxiv.org/abs/2407.16741</link>
<guid>https://arxiv.org/abs/2407.16741</guid>
<content:encoded><![CDATA[
arXiv:2407.16741v3 Announce Type: replace-cross 
Abstract: Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2.1K contributions from over 188 contributors.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders</title>
<link>https://arxiv.org/abs/2409.06635</link>
<guid>https://arxiv.org/abs/2409.06635</guid>
<content:encoded><![CDATA[
arXiv:2409.06635v4 Announce Type: replace-cross 
Abstract: The rapid advancements in large language models (LLMs) have significantly enhanced natural language processing capabilities, facilitating the development of AudioLLMs that process and understand speech and audio inputs alongside text. Existing AudioLLMs typically combine a pre-trained audio encoder with a pre-trained LLM, which are subsequently finetuned on specific audio tasks. However, the pre-trained audio encoder has constrained capacity to capture features for new tasks and datasets. To address this, we propose to incorporate mixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE supplements a base encoder with a pool of relatively light weight encoders, selectively activated based on the audio input to enhance feature extraction without significantly increasing model size. Our empirical results demonstrate that MoWE effectively improves multi-task performance, broadening the applicability of AudioLLMs to more diverse audio tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models</title>
<link>https://arxiv.org/abs/2410.09344</link>
<guid>https://arxiv.org/abs/2410.09344</guid>
<content:encoded><![CDATA[
arXiv:2410.09344v2 Announce Type: replace-cross 
Abstract: Storing open-source fine-tuned models separately introduces redundancy and increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta parameters--the differences between fine-tuned and pre-trained model weights--while typically maintaining minimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta parameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor as pruning rates increase, and (2) high mean and variance in the delta parameters. To push DARE's limits, we introduce DAREx (DARE the eXtreme), which features two algorithmic improvements: (1) DAREx-q, a rescaling factor modification that significantly boosts performance at high pruning rates (e.g., >30 % on COLA and SST2 for encoder models, with even greater gains in decoder models), and (2) DAREx-L2, which combines DARE with AdamR, an in-training method that applies appropriate delta regularization before DPP. We also demonstrate that DAREx-q can be seamlessly combined with vanilla parameter-efficient fine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application of importance-based pruning techniques within DPP, demonstrating that they outperform random-based methods when delta parameters are large. Through this comprehensive study, we develop a pipeline for selecting the most appropriate DPP method under various practical scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Parametric Inversion: Why Instruction Finetuning Can Worsen Context Reliance</title>
<link>https://arxiv.org/abs/2410.10796</link>
<guid>https://arxiv.org/abs/2410.10796</guid>
<content:encoded><![CDATA[
arXiv:2410.10796v3 Announce Type: replace-cross 
Abstract: A standard practice when using large language models is for users to supplement their instruction with an input context containing new information for the model to process. However, models struggle to reliably follow the input context, especially when it conflicts with their parametric knowledge from pretraining. In-principle, one would expect models to adapt to the user context better after instruction finetuning, particularly when handling knowledge conflicts. However, we observe a surprising failure mode: during instruction tuning, the context reliance under knowledge conflicts initially increases as expected, but then gradually decreases as instruction finetuning progresses. This happens while the performance on standard benchmarks keeps on increasing far after this drop. We call this phenomenon context-parametric inversion and observe it across multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across different model families like Llama, Mistral, and Pythia. We perform various controlled studies and theoretical analysis to show that context-parametric inversion occurs due to examples in the instruction finetuning data where the input context provides information that aligns with model's parametric knowledge. Our analysis suggests some natural mitigation strategies with limited but insightful gains, and serves as a useful starting point in addressing this deficiency in instruction finetuning.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment</title>
<link>https://arxiv.org/abs/2410.14148</link>
<guid>https://arxiv.org/abs/2410.14148</guid>
<content:encoded><![CDATA[
arXiv:2410.14148v4 Announce Type: replace-cross 
Abstract: The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face challenges in modality alignment, which can lead to issues like hallucinations and unsafe content generation. Current alignment techniques often rely on coarse feedback and external datasets, limiting scalability and performance. In this paper, we propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel self-alignment method that utilizes the model's own visual encoder as a fine-grained verifier to improve vision-language alignment without the need for additional data. By leveraging token-level feedback from the vision encoder, FiSAO significantly improves vision-language alignment, even surpassing traditional preference tuning methods that require additional data. Through both theoretical analysis and experimental validation, we demonstrate that FiSAO effectively addresses the misalignment problem in VLLMs, marking the first instance of token-level rewards being applied to such models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aioli: A Unified Optimization Framework for Language Model Data Mixing</title>
<link>https://arxiv.org/abs/2411.05735</link>
<guid>https://arxiv.org/abs/2411.05735</guid>
<content:encoded><![CDATA[
arXiv:2411.05735v2 Announce Type: replace-cross 
Abstract: Language model performance depends on identifying the optimal mixture of data groups to train on (e.g., law, code, math). Prior work has proposed a diverse set of methods to efficiently learn mixture proportions, ranging from fitting regression models over training runs to dynamically updating proportions throughout training. Surprisingly, we find that no existing method consistently outperforms a simple stratified sampling baseline in terms of average test perplexity. To understand this inconsistency, we unify existing methods into a standard framework, showing they are equivalent to solving a common optimization problem: minimize average loss subject to a method-specific mixing law -- an implicit assumption on the relationship between loss and mixture proportions. This framework suggests that measuring the fidelity of a method's mixing law can offer insights into its performance. Empirically, we find that existing methods set their mixing law parameters inaccurately, resulting in the inconsistent mixing performance we observe. Using this insight, we derive a new online method named Aioli, which directly estimates the mixing law parameters throughout training and uses them to dynamically adjust proportions. Aioli outperforms stratified sampling on 6 out of 6 datasets by an average of 0.27 test perplexity points, whereas existing methods fail to consistently beat stratified sampling, doing up to 6.9 points worse. Moreover, in a practical setting where proportions are learned on shorter runs due to computational constraints, Aioli can dynamically adjust these proportions over the full training run, consistently improving performance over existing methods by up to 12.012 test perplexity points.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures</title>
<link>https://arxiv.org/abs/2411.16260</link>
<guid>https://arxiv.org/abs/2411.16260</guid>
<content:encoded><![CDATA[
arXiv:2411.16260v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmetic tasks. In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties. Since these structures are observable through input-output relationships, they can generalize to unseen data. We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of weights and biases, the transformer-based LLMs can generate embeddings that remain invariant to both permutations of input tokens and the presence of identity elements. Our findings indicate that leveraging algebraic structures can enhance the LLMs' arithmetic capabilities, offering insights into improving their arithmetic performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems</title>
<link>https://arxiv.org/abs/2503.21074</link>
<guid>https://arxiv.org/abs/2503.21074</guid>
<content:encoded><![CDATA[
arXiv:2503.21074v3 Announce Type: replace-cross 
Abstract: This thesis employs a hybrid CNN-Transformer architecture, alongside a detailed anthropological framework, to investigate potential historical connections between the visual morphology of the Indus Valley script and pictographic systems of the Tibetan-Yi Corridor. Through an ensemble methodology of three target scripts across 15 independently trained models, we demonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold higher visual similarity to the Indus script (0.635) than to the Bronze Age Proto-Cuneiform (0.102) or Proto-Elamite (0.078).
  Contrary to expectations, when measured through direct script-to-script embedding comparisons, the Indus script maps closer to Tibetan-Yi Corridor scripts with a mean cosine similarity of 0.930 (CI: [0.917, 0.942]) than to contemporaneous West Asian signaries, which recorded mean similarities of 0.887 (CI: [0.863, 0.911]) and 0.855 (CI: [0.818, 0.891]). Across dimensionality reduction and clustering methods, the Indus script consistently clusters closest to Tibetan-Yi Corridor scripts.
  These computational findings align with observed pictorial parallels in numeral systems, gender markers, and iconographic elements. Archaeological evidence of contact networks along the ancient Shu-Shendu road, coinciding with the Indus Civilization's decline, provides a plausible transmission pathway. While alternate explanations cannot be ruled out, the specificity and consistency of similarities suggest more complex cultural transmission networks between South and East Asia than previously recognized.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design</title>
<link>https://arxiv.org/abs/2504.01337</link>
<guid>https://arxiv.org/abs/2504.01337</guid>
<content:encoded><![CDATA[
arXiv:2504.01337v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) has successfully scaled up models while maintaining nearly constant computing costs. By employing a gating network to route input tokens, it selectively activates a subset of expert networks to process the corresponding token embeddings. However, in practice, the efficiency of MoE is challenging to achieve due to two key reasons: imbalanced expert activation, which leads to substantial idle time during model or expert parallelism, and insufficient capacity utilization; massive communication overhead, induced by numerous expert routing combinations in expert parallelism at the system level. Previous works typically formulate it as the load imbalance issue characterized by the gating network favoring certain experts over others or attribute it to static execution which fails to adapt to the dynamic expert workload at runtime. In this paper, we exploit it from a brand new perspective, a higher-order view and analysis of MoE routing policies: expert collaboration and specialization where some experts tend to activate broadly with others (collaborative), while others are more likely to activate only with a specific subset of experts (specialized). Our experiments reveal that most experts tend to be overly collaborative, leading to increased communication overhead from repeatedly sending tokens to different accelerators. To this end, we propose a novel collaboration-constrained routing (C2R) strategy to encourage more specialized expert groups, as well as to improve expert utilization, and present an efficient implementation of MoE that further leverages expert specialization. We achieve an average performance improvement of 0.51% and 0.33% on LLaMA-MoE and Qwen-MoE respectively across ten downstream NLP benchmarks, and reduce the all2all communication costs between GPUs, bringing an extra 20%-30% total running time savings on top of the existing SoTA, i.e. MegaBlocks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Resource Allocation in Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2504.02051</link>
<guid>https://arxiv.org/abs/2504.02051</guid>
<content:encoded><![CDATA[
arXiv:2504.02051v2 Announce Type: replace-cross 
Abstract: With the development of LLMs as agents, there is a growing interest in connecting multiple agents into multi-agent systems to solve tasks concurrently, focusing on their role in task assignment and coordination. This paper explores how LLMs can effectively allocate computational tasks among multiple agents, considering factors such as cost, efficiency, and performance. In this work, we address key questions, including the effectiveness of LLMs as orchestrators and planners, comparing their effectiveness in task assignment and coordination. Our experiments demonstrate that LLMs can achieve high validity and accuracy in resource allocation tasks. We find that the planner method outperforms the orchestrator method in handling concurrent actions, resulting in improved efficiency and better utilization of agents. Additionally, we show that providing explicit information about worker capabilities enhances the allocation strategies of planners, particularly when dealing with suboptimal workers.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling</title>
<link>https://arxiv.org/abs/2504.05216</link>
<guid>https://arxiv.org/abs/2504.05216</guid>
<content:encoded><![CDATA[
arXiv:2504.05216v2 Announce Type: replace-cross 
Abstract: Dense retrieval is a crucial task in Information Retrieval (IR) and is the foundation for downstream tasks such as re-ranking. Recently, large language models (LLMs) have shown compelling semantic understanding capabilities and are appealing to researchers studying dense retrieval. LLMs, as decoder-style generative models, are competent at language generation while falling short on modeling global information due to the lack of attention to tokens afterward. Inspired by the classical word-based language modeling approach for IR, i.e., the query likelihood (QL) model, we seek to sufficiently utilize LLMs' generative ability by QL maximization. However, instead of ranking documents with QL estimation, we introduce an auxiliary task of QL maximization to yield a better backbone for contrastively learning a discriminative retriever. We name our model as LLM-QL. To condense global document semantics to a single vector during QL modeling, LLM-QL has two major components, Attention Stop (AS) and Input Corruption (IC). AS stops the attention of predictive tokens to previous tokens until the ending token of the document. IC masks a portion of tokens in the input documents during prediction. Experiments on MSMARCO show that LLM-QL can achieve significantly better performance than other LLM-based retrievers and using QL estimated by LLM-QL for ranking outperforms word-based QL by a large margin.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning a Large Language Model for Automating Computational Fluid Dynamics Simulations</title>
<link>https://arxiv.org/abs/2504.09602</link>
<guid>https://arxiv.org/abs/2504.09602</guid>
<content:encoded><![CDATA[
arXiv:2504.09602v2 Announce Type: replace-cross 
Abstract: Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows. Our code and fine-tuned model have been deposited at https://github.com/YYgroup/AutoCFD.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</title>
<link>https://arxiv.org/abs/2504.09689</link>
<guid>https://arxiv.org/abs/2504.09689</guid>
<content:encoded><![CDATA[
arXiv:2504.09689v2 Announce Type: replace-cross 
Abstract: The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis</title>
<link>https://arxiv.org/abs/2504.13187</link>
<guid>https://arxiv.org/abs/2504.13187</guid>
<content:encoded><![CDATA[
<div> large language models, calculus differentiation problems, performance evaluation, procedural capabilities, conceptual understanding 
Summary:
The study evaluates five leading large language models (LLMs) on their performance in solving calculus differentiation problems. Chat GPT 4o showed the highest success rate, followed by Claude Pro, Gemini Advanced, Copilot Pro, and Meta AI. While all models excelled at procedural tasks, they showed limitations in conceptual understanding and algebraic manipulation. Problems involving increasing/decreasing intervals and optimization word problems were particularly challenging. Claude Pro generated the most difficult problems, implying distinct capabilities in problem generation and solving. The findings suggest the potential and limitations of LLMs in educational applications, emphasizing the importance of human instruction for deeper mathematical comprehension. <div>
arXiv:2504.13187v1 Announce Type: new 
Abstract: This study presents a comprehensive evaluation of five leading large language models (LLMs) - Chat GPT 4o, Copilot Pro, Gemini Advanced, Claude Pro, and Meta AI - on their performance in solving calculus differentiation problems. The investigation assessed these models across 13 fundamental problem types, employing a systematic cross-evaluation framework where each model solved problems generated by all models. Results revealed significant performance disparities, with Chat GPT 4o achieving the highest success rate (94.71%), followed by Claude Pro (85.74%), Gemini Advanced (84.42%), Copilot Pro (76.30%), and Meta AI (56.75%). All models excelled at procedural differentiation tasks but showed varying limitations with conceptual understanding and algebraic manipulation. Notably, problems involving increasing/decreasing intervals and optimization word problems proved most challenging across all models. The cross-evaluation matrix revealed that Claude Pro generated the most difficult problems, suggesting distinct capabilities between problem generation and problem-solving. These findings have significant implications for educational applications, highlighting both the potential and limitations of LLMs as calculus learning tools. While they demonstrate impressive procedural capabilities, their conceptual understanding remains limited compared to human mathematical reasoning, emphasizing the continued importance of human instruction for developing deeper mathematical comprehension.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models</title>
<link>https://arxiv.org/abs/2504.13189</link>
<guid>https://arxiv.org/abs/2504.13189</guid>
<content:encoded><![CDATA[
<div> classification, sector identification, performance ranking, fiscal policy impacts, computational economics research
Summary:<br />
- The study introduces a framework called BASIR to analyze the impact of India's Union Budget announcements on sector-specific equity performance in real-time. 
- It involves multi-label classification of budget transcripts into economic sectors and ranking these sectors based on their predicted performances. 
- The results show a 0.605 F1-score in sector classification and a 0.997 NDCG score in predicting sector performance ranks post-budget. 
- The methodology aims to provide structured, data-driven insights for investors and policymakers to quantify fiscal policy impacts. 
- An annotated dataset, released under a CC-BY-NC-SA-4.0 license, is available to support computational economics research. 
<br /><br />Summary: <div>
arXiv:2504.13189v1 Announce Type: new 
Abstract: Government fiscal policies, particularly annual union budgets, exert significant influence on financial markets. However, real-time analysis of budgetary impacts on sector-specific equity performance remains methodologically challenging and largely unexplored. This study proposes a framework to systematically identify and rank sectors poised to benefit from India's Union Budget announcements. The framework addresses two core tasks: (1) multi-label classification of excerpts from budget transcripts into 81 predefined economic sectors, and (2) performance ranking of these sectors. Leveraging a comprehensive corpus of Indian Union Budget transcripts from 1947 to 2025, we introduce BASIR (Budget-Assisted Sectoral Impact Ranking), an annotated dataset mapping excerpts from budgetary transcripts to sectoral impacts. Our architecture incorporates fine-tuned embeddings for sector identification, coupled with language models that rank sectors based on their predicted performances. Our results demonstrate 0.605 F1-score in sector classification, and 0.997 NDCG score in predicting ranks of sectors based on post-budget performances. The methodology enables investors and policymakers to quantify fiscal policy impacts through structured, data-driven insights, addressing critical gaps in manual analysis. The annotated dataset has been released under CC-BY-NC-SA-4.0 license to advance computational economics research.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding</title>
<link>https://arxiv.org/abs/2504.13216</link>
<guid>https://arxiv.org/abs/2504.13216</guid>
<content:encoded><![CDATA[
<div> benchmark suite, large language models, Korean financial domain, evaluation, financial knowledge

Summary:
The article introduces KFinEval-Pilot, a benchmark suite specifically created to evaluate large language models in the Korean financial sector. It consists of over 1,000 curated questions covering financial knowledge, legal reasoning, and financial toxicity. The benchmark is developed using a semi-automated pipeline that combines GPT-4-generated prompts with expert validation to ensure domain relevance and factual accuracy. Performance variations are observed among representative LLMs, showcasing trade-offs between task accuracy and output safety. Challenges persist in applying LLMs to high-stakes financial tasks, particularly in reasoning and safety. Grounded in real-world financial scenarios and aligned with the Korean regulatory and linguistic context, KFinEval-Pilot acts as a diagnostic tool for developing more secure and trustworthy financial AI systems. 

Summary: <div>
arXiv:2504.13216v1 Announce Type: new 
Abstract: We introduce KFinEval-Pilot, a benchmark suite specifically designed to evaluate large language models (LLMs) in the Korean financial domain. Addressing the limitations of existing English-centric benchmarks, KFinEval-Pilot comprises over 1,000 curated questions across three critical areas: financial knowledge, legal reasoning, and financial toxicity. The benchmark is constructed through a semi-automated pipeline that combines GPT-4-generated prompts with expert validation to ensure domain relevance and factual accuracy. We evaluate a range of representative LLMs and observe notable performance differences across models, with trade-offs between task accuracy and output safety across different model families. These results highlight persistent challenges in applying LLMs to high-stakes financial applications, particularly in reasoning and safety. Grounded in real-world financial use cases and aligned with the Korean regulatory and linguistic context, KFinEval-Pilot serves as an early diagnostic tool for developing safer and more reliable financial AI systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainability via LLM Right-sizing</title>
<link>https://arxiv.org/abs/2504.13217</link>
<guid>https://arxiv.org/abs/2504.13217</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sustainability, performance benchmarks, task evaluation, responsible deployment<br />
Summary:<br />
This study evaluates the performance of eleven large language models (LLMs) across ten everyday occupational tasks, considering factors such as output quality, factual accuracy, and ethical responsibility. The authors introduce a dual-LLM-based evaluation framework to automate task execution and standardize evaluation criteria. Results show that smaller models like Gemma-3 and Phi-4 can achieve strong results in tasks, highlighting their viability in cost-efficient, locally deployable, or private contexts. The study identifies three model groups - premium all-rounders, competent generalists, and limited but safe performers - underscoring trade-offs between quality, control, and sustainability. Task type significantly influences model effectiveness, with conceptual tasks posing challenges while aggregation and transformation tasks leading to better performances. The authors advocate for a shift towards task- and context-aware sufficiency assessments for responsible LLM deployment, offering actionable guidance for organizations. <br /><br /> <div>
arXiv:2504.13217v1 Announce Type: new 
Abstract: Large language models (LLMs) have become increasingly embedded in organizational workflows. This has raised concerns over their energy consumption, financial costs, and data sovereignty. While performance benchmarks often celebrate cutting-edge models, real-world deployment decisions require a broader perspective: when is a smaller, locally deployable model "good enough"? This study offers an empirical answer by evaluating eleven proprietary and open-weight LLMs across ten everyday occupational tasks, including summarizing texts, generating schedules, and drafting emails and proposals. Using a dual-LLM-based evaluation framework, we automated task execution and standardized evaluation across ten criteria related to output quality, factual accuracy, and ethical responsibility. Results show that GPT-4o delivers consistently superior performance but at a significantly higher cost and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4 achieved strong and reliable results on most tasks, suggesting their viability in contexts requiring cost-efficiency, local deployment, or privacy. A cluster analysis revealed three model groups -- premium all-rounders, competent generalists, and limited but safe performers -- highlighting trade-offs between quality, control, and sustainability. Significantly, task type influenced model effectiveness: conceptual tasks challenged most models, while aggregation and transformation tasks yielded better performances. We argue for a shift from performance-maximizing benchmarks to task- and context-aware sufficiency assessments that better reflect organizational priorities. Our approach contributes a scalable method to evaluate AI models through a sustainability lens and offers actionable guidance for responsible LLM deployment in practice.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIDS: Domain Impact-aware Data Sampling for Large Language Model Training</title>
<link>https://arxiv.org/abs/2504.13227</link>
<guid>https://arxiv.org/abs/2504.13227</guid>
<content:encoded><![CDATA[
<div> algorithm, language model, data sampling, domain impact, training efficiency

Summary:
The paper introduces the Domain Impact-aware Data Sampling (DIDS) framework for optimizing domain-level data sampling strategies in large language models (LLMs). DIDS addresses the challenges of maintaining intra-domain consistency and accurately measuring domain impact. It uses a gradient clustering algorithm to group training data based on their learning effects while employing a proxy language model and dimensionality reduction for computational efficiency. To measure domain impact, a Fisher Information Matrix (FIM) guided metric is developed to quantify how domain-specific parameter updates affect model output distributions on downstream tasks, with theoretical guarantees. DIDS determines optimal sampling ratios by combining FIM-guided domain impact assessment and loss learning trajectories. Experimental results show that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency.<br /><br />Summary: <div>
arXiv:2504.13227v1 Announce Type: new 
Abstract: Large language models (LLMs) are commonly trained on multi-domain datasets, where domain sampling strategies significantly impact model performance due to varying domain importance across downstream tasks. Existing approaches for optimizing domain-level sampling strategies struggle with maintaining intra-domain consistency and accurately measuring domain impact. In this paper, we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain consistency, a gradient clustering algorithm is proposed to group training data based on their learning effects, where a proxy language model and dimensionality reduction are employed to reduce computational overhead. To accurately measure domain impact, we develop a Fisher Information Matrix (FIM) guided metric that quantifies how domain-specific parameter updates affect the model's output distributions on downstream tasks, with theoretical guarantees. Furthermore, to determine optimal sampling ratios, DIDS combines both the FIM-guided domain impact assessment and loss learning trajectories that indicate domain-specific potential, while accounting for diminishing marginal returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs</title>
<link>https://arxiv.org/abs/2504.13237</link>
<guid>https://arxiv.org/abs/2504.13237</guid>
<content:encoded><![CDATA[
<div> Keywords: delta compression, large language models, sparsification, importance-aware, singular vectors

Summary:<br /><br />
The paper introduces ImPart, a new importance-aware delta sparsification approach for compressing task-specific large language models. Unlike previous methods that either ignore parameter importance or evaluate it at a coarse granularity, ImPart dynamically adjusts sparsity ratios of singular vectors based on their importance after SVD. This allows crucial task-specific knowledge to be retained even at high sparsity ratios, leading to state-of-the-art compression performance. Experiments show that ImPart achieves a 2x higher compression ratio compared to baselines while maintaining the same performance level. When combined with existing methods, ImPart establishes a new state-of-the-art in delta quantization and model merging. <div>
arXiv:2504.13237v1 Announce Type: new 
Abstract: With the proliferation of task-specific large language models, delta compression has emerged as a method to mitigate the resource challenges of deploying numerous such models by effectively compressing the delta model parameters. Previous delta-sparsification methods either remove parameters randomly or truncate singular vectors directly after singular value decomposition (SVD). However, these methods either disregard parameter importance entirely or evaluate it with too coarse a granularity. In this work, we introduce ImPart, a novel importance-aware delta sparsification approach. Leveraging SVD, it dynamically adjusts sparsity ratios of different singular vectors based on their importance, effectively retaining crucial task-specific knowledge even at high sparsity ratios. Experiments show that ImPart achieves state-of-the-art delta sparsification performance, demonstrating $2\times$ higher compression ratio than baselines at the same performance level. When integrated with existing methods, ImPart sets a new state-of-the-art on delta quantization and model merging.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models</title>
<link>https://arxiv.org/abs/2504.13261</link>
<guid>https://arxiv.org/abs/2504.13261</guid>
<content:encoded><![CDATA[
<div> benchmark, pedagogical grammar, language models, foreign language instruction, evaluation

Summary:
CPG-EVAL is introduced as a benchmark to evaluate large language models' (LLMs) knowledge of pedagogical grammar in foreign language instruction. The benchmark consists of five tasks focusing on grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Smaller LLMs perform well in single language instance tasks but struggle with multiple instances and interference. Larger models show better resistance to interference but still have room for improvement in accuracy. The study highlights the need for better alignment of instructional practices and more rigorous benchmarks to guide the use of LLMs in education effectively. CPG-EVAL provides insights for educators, policymakers, and model developers to assess LLMs in educational settings and lays the foundation for future research on model alignment and improving educational suitability in foreign language instruction. 

<br /><br />Summary: <div>
arXiv:2504.13261v1 Announce Type: new 
Abstract: Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT has significantly impacted foreign language education, yet their pedagogical grammar competence remains under-assessed. This paper introduces CPG-EVAL, the first dedicated benchmark specifically designed to evaluate LLMs' knowledge of pedagogical grammar within the context of foreign language instruction. Methodology: The benchmark comprises five tasks designed to assess grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Findings: Smaller-scale models can succeed in single language instance tasks, but struggle with multiple instance tasks and interference from confusing instances. Larger-scale models show better resistance to interference but still have significant room for accuracy improvement. The evaluation indicates the need for better instructional alignment and more rigorous benchmarks, to effectively guide the deployment of LLMs in educational contexts. Value: This study offers the first specialized, theory-driven, multi-tiered benchmark framework for systematically evaluating LLMs' pedagogical grammar competence in Chinese language teaching contexts. CPG-EVAL not only provides empirical insights for educators, policymakers, and model developers to better gauge AI's current abilities in educational settings, but also lays the groundwork for future research on improving model alignment, enhancing educational suitability, and ensuring informed decision-making concerning LLM integration in foreign language instruction.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal</title>
<link>https://arxiv.org/abs/2504.13284</link>
<guid>https://arxiv.org/abs/2504.13284</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet penetration, smartphones, social networks, Senegal, sentiment analysis 

Summary: 

Internet penetration rates in Africa, particularly Senegal, are increasing, with a significant boost from the availability of smartphones. Young people in Senegal are heavily reliant on social networks as their primary means of expression. Despite the growing accessibility to the Internet, limited options from few operators restrict alternatives for mobile Internet services in terms of value for money. To understand young people's perspectives on mobile Internet pricing in Senegal, sentiment analysis was conducted on Twitter and Facebook comments. The study aimed to gauge sentiments towards the price of mobile Internet in relation to the perceived quality of service. Through this analysis, insights were derived from the general feelings expressed by the youth regarding the affordability and quality of mobile Internet services in Senegal. <div>
arXiv:2504.13284v1 Announce Type: new 
Abstract: Internet penetration rates in Africa are rising steadily, and mobile Internet is getting an even bigger boost with the availability of smartphones. Young people are increasingly using the Internet, especially social networks, and Senegal is no exception to this revolution. Social networks have become the main means of expression for young people. Despite this evolution in Internet access, there are few operators on the market, which limits the alternatives available in terms of value for money. In this paper, we will look at how young people feel about the price of mobile Internet in Senegal, in relation to the perceived quality of the service, through their comments on social networks. We scanned a set of Twitter and Facebook comments related to the subject and applied a sentiment analysis model to gather their general feelings.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models</title>
<link>https://arxiv.org/abs/2504.13367</link>
<guid>https://arxiv.org/abs/2504.13367</guid>
<content:encoded><![CDATA[
<div> difficulty, token allocation, calibration, reasoning models, THOUGHTTERMINATOR

Summary: 
The article discusses the issue of overthinking in reasoning models and introduces approximate measures of problem-level difficulty to optimize token allocation. It shows a clear relationship between problem difficulty and optimal token spend, highlighting the poor calibration of reasoning models in efficiently allocating tokens, especially on easy problems. The DUMB500 dataset is introduced for evaluating calibration on easy questions, contrasting with existing difficult benchmarks. Additionally, THOUGHTTERMINATOR, a training-free black box decoding technique, is proposed to improve reasoning model calibration effectively. <div>
arXiv:2504.13367v1 Announce Type: new 
Abstract: Reasoning models have demonstrated impressive performance on difficult tasks that traditional language models struggle at. However, many are plagued with the problem of overthinking--generating large amounts of unnecessary tokens which don't improve accuracy on a question. We introduce approximate measures of problem-level difficulty and demonstrate that a clear relationship between problem difficulty and optimal token spend exists, and evaluate how well calibrated a variety of reasoning models are in terms of efficiently allocating the optimal token count. We find that in general, reasoning models are poorly calibrated, particularly on easy problems. To evaluate calibration on easy questions we introduce DUMB500, a dataset of extremely easy math, reasoning, code, and task problems, and jointly evaluate reasoning model on these simple examples and extremely difficult examples from existing frontier benchmarks on the same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free black box decoding technique that significantly improves reasoning model calibration.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering</title>
<link>https://arxiv.org/abs/2504.13425</link>
<guid>https://arxiv.org/abs/2504.13425</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Enterprise Settings, Data Security, Secure Multifaceted-RAG, External LLM-generated knowledge

Summary:
The article introduces the Secure Multifaceted-RAG (SecMulti-RAG) framework to address challenges faced by existing Retrieval-Augmented Generation (RAG) systems in enterprise settings. SecMulti-RAG improves response accuracy and completeness by retrieving from internal documents, pre-generated expert knowledge, and on-demand external Large Language Models (LLMs). Security risks are mitigated by using a local open-source generator and selectively employing external LLMs with a filtering mechanism. In an evaluation on a report generation task in the automotive industry, SecMulti-RAG outperforms traditional RAG in correctness, richness, and helpfulness according to LLM-based evaluation. Human evaluation also shows significant improvements. This approach enhances completeness, prevents data leakage, and reduces costs, making SecMulti-RAG a practical and secure solution for enterprise RAG.<br /><br />Summary: <div>
arXiv:2504.13425v1 Announce Type: new 
Abstract: Existing Retrieval-Augmented Generation (RAG) systems face challenges in enterprise settings due to limited retrieval scope and data security risks. When relevant internal documents are unavailable, the system struggles to generate accurate and complete responses. Additionally, using closed-source Large Language Models (LLMs) raises concerns about exposing proprietary information. To address these issues, we propose the Secure Multifaceted-RAG (SecMulti-RAG) framework, which retrieves not only from internal documents but also from two supplementary sources: pre-generated expert knowledge for anticipated queries and on-demand external LLM-generated knowledge. To mitigate security risks, we adopt a local open-source generator and selectively utilize external LLMs only when prompts are deemed safe by a filtering mechanism. This approach enhances completeness, prevents data leakage, and reduces costs. In our evaluation on a report generation task in the automotive industry, SecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9 percent win rates across correctness, richness, and helpfulness in LLM-based evaluation, and 56.3 to 70.4 percent in human evaluation. This highlights SecMulti-RAG as a practical and secure solution for enterprise RAG.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model</title>
<link>https://arxiv.org/abs/2504.13439</link>
<guid>https://arxiv.org/abs/2504.13439</guid>
<content:encoded><![CDATA[
<div> distractor generator, multiple-choice evaluation, D-GEN, ranking alignment, entropy analysis

Summary:
The article introduces D-GEN, an open-source distractor generator model that transforms open-ended data into a multiple-choice (MC) format. To evaluate the quality of the generated distractors, two novel methods are proposed: ranking alignment and entropy analysis. The results demonstrate that D-GEN maintains ranking consistency and closely matches the entropy distribution of ground-truth distractors. Human evaluation validates the fluency, coherence, distractiveness, and incorrectness of the generated distractors. This work advances the field by providing a robust and efficient method for automated distractor generation and evaluation, setting a new standard for MC evaluation. <div>
arXiv:2504.13439v1 Announce Type: new 
Abstract: Evaluating generative models with open-ended generation is challenging due to inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates this issue, but generating high-quality distractors is time-consuming and labor-intensive. We introduce D-GEN, the first open-source distractor generator model that transforms open-ended data into an MC format. To evaluate distractor quality, we propose two novel methods: (1) ranking alignment, ensuring generated distractors retain the discriminatory power of ground-truth distractors, and (2) entropy analysis, comparing model confidence distributions. Our results show that D-GEN preserves ranking consistency (Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy distribution of ground-truth distractors. Human evaluation further confirms the fluency, coherence, distractiveness, and incorrectness. Our work advances robust and efficient distractor generation with automated evaluation, setting a new standard for MC evaluation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs</title>
<link>https://arxiv.org/abs/2504.13471</link>
<guid>https://arxiv.org/abs/2504.13471</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, NLP pipeline, model compression, knowledge transfer, cost-efficient deployment pipeline 

Summary:
In this paper, a three-stage cost-efficient end-to-end deployment pipeline for Large Language Models (LLMs) is introduced to address the cost-performance dilemma in LLM-based frameworks. The approach involves prototyping, knowledge transfer, and model compression stages to optimize cost and performance for online systems. Initially, a high-performance prototype system is built to generate high-quality data as a teacher model. Techniques like rejection fine-tuning, reinforcement learning, and knowledge distillation are then used to transfer knowledge to a smaller student model, resulting in effective performance at minimal cost. Finally, quantization and pruning are applied to compress the model further, achieving ultra-low latency and cost. The modular design and cross-domain capabilities of the framework suggest potential applicability in various NLP areas. 

<br /><br />Summary: <div>
arXiv:2504.13471v1 Announce Type: new 
Abstract: In recent years, Large Language Models (LLMs) have significantly advanced artificial intelligence by optimizing traditional Natural Language Processing (NLP) pipelines, improving performance and generalization. This has spurred their integration into various systems. Many NLP systems, including ours, employ a "one-stage" pipeline directly incorporating LLMs. While effective, this approach incurs substantial costs and latency due to the need for large model parameters to achieve satisfactory outcomes. This paper introduces a three-stage cost-efficient end-to-end LLM deployment pipeline-including prototyping, knowledge transfer, and model compression-to tackle the cost-performance dilemma in LLM-based frameworks. Our approach yields a super tiny model optimized for cost and performance in online systems, simplifying the system architecture. Initially, by transforming complex tasks into a function call-based LLM-driven pipeline, an optimal performance prototype system is constructed to produce high-quality data as a teacher model. The second stage combine techniques like rejection fine-tuning, reinforcement learning and knowledge distillation to transfer knowledge to a smaller 0.5B student model, delivering effective performance at minimal cost. The final stage applies quantization and pruning to extremely compress model to 0.4B, achieving ultra-low latency and cost. The framework's modular design and cross-domain capabilities suggest potential applicability in other NLP areas.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Sensitivity Evaluation Framework for Clinical Diagnosis</title>
<link>https://arxiv.org/abs/2504.13475</link>
<guid>https://arxiv.org/abs/2504.13475</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, clinical diagnosis, sensitivity, key medical information, diagnostic decision-making

Summary: 
Large language models (LLMs) have shown impressive performance in various domains, but for clinical diagnosis, higher expectations are needed for their sensitivity and reliability. Existing research mostly focuses on LLMs' sensitivity to irrelevant context rather than key medical information crucial for diagnostic reasoning. This study evaluates the sensitivity of LLMs, such as GPT-3.5, GPT-4, Gemini, Claude3, and LLaMA2-7b, to key medical information using different perturbation strategies. The results demonstrate current LLMs' limitations in accurately capturing crucial medical information for diagnostic decision-making. Future advancements in LLMs should prioritize improving reliability and sensitivity to key information to enhance trust and practical use in real-world scenarios. The code and dataset for this study are publicly available at https://github.com/chenwei23333/DiagnosisQA.

Summary: <div>
arXiv:2504.13475v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance across various domains. However, for clinical diagnosis, higher expectations are required for LLM's reliability and sensitivity: thinking like physicians and remaining sensitive to key medical information that affects diagnostic reasoning, as subtle variations can lead to different diagnosis results. Yet, existing works focus mainly on investigating the sensitivity of LLMs to irrelevant context and overlook the importance of key information. In this paper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini, Claude3 and LLaMA2-7b, to key medical information by introducing different perturbation strategies. The evaluation results highlight the limitations of current LLMs in remaining sensitive to key medical information for diagnostic decision-making. The evolution of LLMs must focus on improving their reliability, enhancing their ability to be sensitive to key information, and effectively utilizing this information. These improvements will enhance human trust in LLMs and facilitate their practical application in real-world scenarios. Our code and dataset are available at https://github.com/chenwei23333/DiagnosisQA.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning</title>
<link>https://arxiv.org/abs/2504.13500</link>
<guid>https://arxiv.org/abs/2504.13500</guid>
<content:encoded><![CDATA[
<div> Keywords: process prejudge, LLM reasoning, dynamic tree-searching, supervised fine-tuning, reinforcement learning

Summary: 
In this paper, a new process prejudge strategy is introduced in LLM reasoning to enhance adaptive reasoning capabilities. The strategy involves anticipating errors in reasoning steps similar to human decision-making processes. A prejudge node is defined in the rationale to represent a step where errors may occur. An automated reasoning framework with dynamic tree-searching is presented, requiring only one LLM to perform various tasks. A two-phase training mechanism involving supervised fine-tuning and reinforcement learning further improves reasoning abilities. Experimental results show significant enhancements in LLM reasoning skills, with the model learning to prejudge before proceeding. The code and data are available at the provided GitHub repository.<br /><br />Summary: <div>
arXiv:2504.13500v1 Announce Type: new 
Abstract: In this paper, we introduce a new \emph{process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent reasoning steps, similar to people sometimes pausing to think about what mistakes may occur and how to avoid them, rather than relying solely on trial and error. Specifically, we define a prejudge node in the rationale, which represents a reasoning step, with at least one step that follows the prejudge node that has no paths toward the correct answer. To synthesize the prejudge reasoning process, we present an automated reasoning framework with a dynamic tree-searching strategy. This framework requires only one LLM to perform answer judging, response critiquing, prejudge generation, and thought completion. Furthermore, we develop a two-phase training mechanism with supervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance the reasoning capabilities of LLMs. Experimental results from competition-level complex reasoning demonstrate that our method can teach the model to prejudge before thinking and significantly enhance the reasoning ability of LLMs. Code and data is released at https://github.com/wjn1996/Prejudge-Before-Think.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.13534</link>
<guid>https://arxiv.org/abs/2504.13534</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, Chain-of-Thought Reasoning, Large Language Models, Reasoning Framework, Retrieval-Augmented Generation

Summary:
CoT-RAG is introduced as a new reasoning framework addressing challenges in chain-of-thought reasoning for large language models. It incorporates knowledge graphs to improve the credibility of reasoning chains generated by LLMs. Additionally, it integrates retrieval-augmented generation (RAG) into knowledge graphs to provide LLMs with relevant sub-cases and sub-descriptions for enhanced reasoning. Moreover, the framework encourages LLMs to execute reasoning tasks in pseudo-programs to increase logical rigor. Evaluation across nine public datasets demonstrates a significant accuracy improvement compared to existing methods, ranging from 4.0% to 23.0%. Testing on domain-specific datasets showcases CoT-RAG's accuracy and efficiency, underscoring its practical applicability and scalability.

<br /><br />Summary: <div>
arXiv:2504.13534v1 Announce Type: new 
Abstract: While chain-of-thought (CoT) reasoning improves the performance of large language models (LLMs) in complex tasks, it still has two main challenges: the low reliability of relying solely on LLMs to generate reasoning chains and the interference of natural language reasoning chains on the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to execute reasoning tasks in pseudo-programs with greater logical rigor. We conduct a comprehensive evaluation on nine public datasets, covering three reasoning problems. Compared with the-state-of-the-art methods, CoT-RAG exhibits a significant accuracy improvement, ranging from 4.0% to 23.0%. Furthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable accuracy and efficient execution, highlighting its strong practical applicability and scalability.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content</title>
<link>https://arxiv.org/abs/2504.13545</link>
<guid>https://arxiv.org/abs/2504.13545</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, banking sector, multilingual capabilities, explainable outputs, aspect-based analysis

Summary: 
This research introduces a hybrid aspect-based sentiment analysis framework for the banking sector, focusing on multilingual customer feedback in languages like English, Sinhala, Singlish, and code-mixed text. The framework combines fine-tuned models such as XLM-RoBERTa for Sinhala and code-mixed text, BERT-base-uncased for English, and domain-specific lexicon correction. It classifies sentiment with confidence scores and utilizes SHAP and LIME techniques for real-time sentiment explanations, achieving high accuracy and F1-scores. The research emphasizes the importance of interpretability in sentiment analysis and delivers aspect-wise sentiment insights through a user-friendly interface. Overall, this study enhances sentiment analysis in the banking sector by addressing challenges in multilingual, low-resource languages and providing transparent and reliable outputs. 

<br /><br />Summary: <div>
arXiv:2504.13545v1 Announce Type: new 
Abstract: Sentiment analysis is crucial for brand reputation management in the banking sector, where customer feedback spans English, Sinhala, Singlish, and code-mixed text. Existing models struggle with low-resource languages like Sinhala and lack interpretability for practical use. This research develops a hybrid aspect-based sentiment analysis framework that enhances multilingual capabilities with explainable outputs. Using cleaned banking customer reviews, we fine-tune XLM-RoBERTa for Sinhala and code-mixed text, integrate domain-specific lexicon correction, and employ BERT-base-uncased for English. The system classifies sentiment (positive, neutral, negative) with confidence scores, while SHAP and LIME improve interpretability by providing real-time sentiment explanations. Experimental results show that our approaches outperform traditional transformer-based classifiers, achieving 92.3 percent accuracy and an F1-score of 0.89 in English and 88.4 percent in Sinhala and code-mixed content. An explainability analysis reveals key sentiment drivers, improving trust and transparency. A user-friendly interface delivers aspect-wise sentiment insights, ensuring accessibility for businesses. This research contributes to robust, transparent sentiment analysis for financial applications by bridging gaps in multilingual, low-resource NLP and explainability.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification</title>
<link>https://arxiv.org/abs/2504.13562</link>
<guid>https://arxiv.org/abs/2504.13562</guid>
<content:encoded><![CDATA[
<div> attention modification, jailbreak attacks, Large Language Models, defense, DETAM

Summary:
DETAM introduces a defense approach against jailbreak attacks on Large Language Models (LLMs) without the need for finetuning. By analyzing attention scores, sensitive attention heads are identified and reallocated during inference to emphasize the user's intent and minimize interference from attack tokens. Experimental results show DETAM outperforming various baselines in jailbreak defense and maintaining effectiveness across different attacks and models, even on in-the-wild data. Utility evaluation with over-defense datasets further validates DETAM's superior performance in defending against jailbreak attacks. The code for DETAM will be released upon acceptance. <div>
arXiv:2504.13562v1 Announce Type: new 
Abstract: With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DETAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize the user's core intention, minimizing interference from attack tokens. Our experimental results demonstrate that DETAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, in evaluating the model's utility, we incorporated over-defense datasets, which further validate the superior performance of our approach. The code will be released immediately upon acceptance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling</title>
<link>https://arxiv.org/abs/2504.13592</link>
<guid>https://arxiv.org/abs/2504.13592</guid>
<content:encoded><![CDATA[
<div> intent detection, task-oriented dialogue, reinforcement learning, reward-based curriculum sampling, generalization <br />
Summary: <br />
Intent detection in task-oriented dialogue systems faces challenges in adapting to new integrable tools. This study proposes using Reinforcement Learning (RL) with Reward-based Curriculum Sampling (RCS) during training to improve generalization in unseen tasks. RL-trained models outperform supervised fine-tuning (SFT) baselines, with RCS enhancing the effectiveness of RL by focusing on challenging cases. Chain-of-Thought (COT) processes in RL help improve generalization in complex intent detection tasks. This work advances intent detection generalization, offering insights for deploying adaptable dialogue systems. <div>
arXiv:2504.13592v1 Announce Type: new 
Abstract: Intent detection, a critical component in task-oriented dialogue (TOD) systems, faces significant challenges in adapting to the rapid influx of integrable tools with complex interrelationships. Existing approaches, such as zero-shot reformulations and LLM-based dynamic recognition, struggle with performance degradation when encountering unseen intents, leading to erroneous task routing. To enhance the model's generalization performance on unseen tasks, we employ Reinforcement Learning (RL) combined with a Reward-based Curriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO) training in intent detection tasks. Experiments demonstrate that RL-trained models substantially outperform supervised fine-tuning (SFT) baselines in generalization. Besides, the introduction of the RCS, significantly bolsters the effectiveness of RL in intent detection by focusing the model on challenging cases during training. Moreover, incorporating Chain-of-Thought (COT) processes in RL notably improves generalization in complex intent detection tasks, underscoring the importance of thought in challenging scenarios. This work advances the generalization of intent detection tasks, offering practical insights for deploying adaptable dialogue systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Pre-Training is (not) What You Need in Domain Adaption</title>
<link>https://arxiv.org/abs/2504.13603</link>
<guid>https://arxiv.org/abs/2504.13603</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Legal research, Legal reasoning, Domain adaptation, Legal AI  
Summary:  
- Legal Large Language Models (LLMs) have transformed legal research and practice by automating tasks and supporting decision-making processes.  
- Adapting LLMs to the legal domain is challenging due to the complexity of legal reasoning and specialized language interpretation.  
- Domain-Adaptive Continual Pre-Training (DACP) enhances domain-specific knowledge but does not uniformly improve performance across all legal tasks.  
- DACP impacts model generalization and performance in prompt-based tasks.  
- Future research should focus on optimizing domain adaptation strategies in legal AI.  

Summary: <div>
arXiv:2504.13603v1 Announce Type: new 
Abstract: The recent advances in Legal Large Language Models (LLMs) have transformed the landscape of legal research and practice by automating tasks, enhancing research precision, and supporting complex decision-making processes. However, effectively adapting LLMs to the legal domain remains challenging due to the complexity of legal reasoning, the need for precise interpretation of specialized language, and the potential for hallucinations. This paper examines the efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the legal reasoning capabilities of LLMs. Through a series of experiments on legal reasoning tasks within the Taiwanese legal framework, we demonstrate that while DACP enhances domain-specific knowledge, it does not uniformly improve performance across all legal tasks. We discuss the trade-offs involved in DACP, particularly its impact on model generalization and performance in prompt-based tasks, and propose directions for future research to optimize domain adaptation strategies in legal AI.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-context Non-factoid Question Answering in Indic Languages</title>
<link>https://arxiv.org/abs/2504.13615</link>
<guid>https://arxiv.org/abs/2504.13615</guid>
<content:encoded><![CDATA[
<div> semantic scores, token-level scores, context-shortening techniques, Indic languages, Question Answering 

Summary:<br />
- Context-shortening techniques such as Open Information Extraction, coreference resolution, and Answer Paragraph Selection improve QA performance for long contexts in Indic languages.
- Experiments on Hindi, Tamil, Telugu, and Urdu show an average increase of 4% in semantic scores and 47% in token-level scores without fine-tuning.
- With fine-tuning, there is an average increase of 2% in both semantic and token-level scores. 
- Techniques reduce computational overhead and improve efficiency.
- Explainability techniques show that APS confidently identifies answer-containing paragraphs.
- Limitations exist for LLM-based QA systems in addressing non-factoid questions.
- Verbalizing OIE triples does not enhance system performance.
- Context-shortening techniques have the potential to enhance LLM-based QA systems for low-resource languages. 

Summary: <div>
arXiv:2504.13615v1 Announce Type: new 
Abstract: Question Answering (QA) tasks, which involve extracting answers from a given context, are relatively straightforward for modern Large Language Models (LLMs) when the context is short. However, long contexts pose challenges due to the quadratic complexity of the self-attention mechanism. This challenge is compounded in Indic languages, which are often low-resource. This study explores context-shortening techniques, including Open Information Extraction (OIE), coreference resolution, Answer Paragraph Selection (APS), and their combinations, to improve QA performance. Compared to the baseline of unshortened (long) contexts, our experiments on four Indic languages (Hindi, Tamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield an average improvement of 4\% in semantic scores and 47\% in token-level scores when evaluated on three popular LLMs without fine-tuning. Furthermore, with fine-tuning, we achieve an average increase of 2\% in both semantic and token-level scores. Additionally, context-shortening reduces computational overhead. Explainability techniques like LIME and SHAP reveal that when the APS model confidently identifies the paragraph containing the answer, nearly all tokens within the selected text receive high relevance scores. However, the study also highlights the limitations of LLM-based QA systems in addressing non-factoid questions, particularly those requiring reasoning or debate. Moreover, verbalizing OIE-generated triples does not enhance system performance. These findings emphasize the potential of context-shortening techniques to improve the efficiency and effectiveness of LLM-based QA systems, especially for low-resource languages. The source code and resources are available at https://github.com/ritwikmishra/IndicGenQA.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2504.13626</link>
<guid>https://arxiv.org/abs/2504.13626</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, overthinking, computational cost reduction, safety alignment, CoT generator 

Summary:<br /><br />
The article introduces a new approach, ThoughtMani, to address the issue of overthinking in large reasoning models (LRMs). LRMs often generate redundant reasoning steps during computation, resulting in limited performance gains. ThoughtMani leverages external Context of Thought (CoT) generated by smaller models to reduce unnecessary intermediate steps and computational costs significantly. Experimental results show that applying ThoughtMani to LRMs such as QwQ-32B on the LiveBench/Code dataset maintains original performance while reducing output token counts by about 30%. Additionally, ThoughtMani improves safety alignment by an average of 10%. This approach makes LRMs more efficient and cost-effective for real-world applications, especially when model vendors serve models of varying sizes simultaneously. <div>
arXiv:2504.13626v1 Announce Type: new 
Abstract: Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from "overthinking" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization.
  Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token ($\texttt{}$ and $\texttt{)}$ can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing</title>
<link>https://arxiv.org/abs/2504.13629</link>
<guid>https://arxiv.org/abs/2504.13629</guid>
<content:encoded><![CDATA[
<div> adoption patterns, writing convergence, academic disciplines, LLM usage, stylistic shifts  
Summary:  
Large Language Models (LLMs), like ChatGPT, are transforming academic writing by providing AI-assisted generative revisions. Research investigates the impact of these revisions on over 627,000 academic papers from arXiv. Results show disparities in LLM adoption based on academic disciplines, gender, native language status, and career stage. LLM usage improves clarity, conciseness, and adherence to formal writing conventions, with effects varying by revision type. The study reveals a rapid evolution in scholarly writing styles and suggests that LLMs drive convergence in academic writing. Early adopters, male researchers, non-native speakers, and junior scholars exhibit the most significant stylistic shifts, aligning their writing more closely with established researchers. <div>
arXiv:2504.13629v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as ChatGPT, are reshaping content creation and academic writing. This study investigates the impact of AI-assisted generative revisions on research manuscripts, focusing on heterogeneous adoption patterns and their influence on writing convergence. Leveraging a dataset of over 627,000 academic papers from arXiv, we develop a novel classification framework by fine-tuning prompt- and discipline-specific large language models to detect the style of ChatGPT-revised texts. Our findings reveal substantial disparities in LLM adoption across academic disciplines, gender, native language status, and career stage, alongside a rapid evolution in scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness, and adherence to formal writing conventions, with improvements varying by revision type. Finally, a difference-in-differences analysis shows that while LLMs drive convergence in academic writing, early adopters, male researchers, non-native speakers, and junior scholars exhibit the most pronounced stylistic shifts, aligning their writing more closely with that of established researchers.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling</title>
<link>https://arxiv.org/abs/2504.13630</link>
<guid>https://arxiv.org/abs/2504.13630</guid>
<content:encoded><![CDATA[
<div> metric framework, translation evaluation, neural metrics, quality translation, preference data
<br />
Summary:<br />
ReMedy is a novel MT metric framework that addresses the challenge of noisy human ratings in translation evaluation. By reformulating evaluation as a reward modeling task and using pairwise preference data to learn relative translation quality, ReMedy provides a more reliable assessment of translation performance. In extensive experiments across multiple WMT shared tasks, ReMedy outperforms state-of-the-art metrics at both segment- and system-level evaluation. It exceeds larger WMT winners and massive closed LLMs, demonstrating superior capability in detecting translation errors and evaluating low-quality translations. By focusing on relative quality rather than direct regression on human ratings, ReMedy offers a more robust and accurate means of evaluating machine translation systems. 
<br /> <div>
arXiv:2504.13630v1 Announce Type: new 
Abstract: A key challenge in MT evaluation is the inherent noise and inconsistency of human ratings. Regression-based neural metrics struggle with this noise, while prompting LLMs shows promise at system-level evaluation but performs poorly at segment level. In this work, we propose ReMedy, a novel MT metric framework that reformulates translation evaluation as a reward modeling task. Instead of regressing on imperfect human ratings directly, ReMedy learns relative translation quality using pairwise preference data, resulting in a more reliable evaluation. In extensive experiments across WMT22-24 shared tasks (39 language pairs, 111 MT systems), ReMedy achieves state-of-the-art performance at both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses larger WMT winners and massive closed LLMs such as MetricX-13B, XCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses demonstrate that ReMedy delivers superior capability in detecting translation errors and evaluating low-quality translations.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning</title>
<link>https://arxiv.org/abs/2504.13643</link>
<guid>https://arxiv.org/abs/2504.13643</guid>
<content:encoded><![CDATA[
<div> persona, dialogue policy planning, user characteristics, user-tailored, user feedback

Summary:
The article discusses the importance of incorporating user characteristics in dialogue policy planning for conversational systems. Existing approaches often overlook the individual traits of users, such as personality, preferences, and goals, which are crucial for real-world applications like conversational search and recommendation. The User-Tailored Dialogue Policy Planning (UDP) framework is proposed to address this gap. UDP incorporates an Intrinsic User World Model to model user traits and feedback, operating in three stages: User Persona Portraying, User Feedback Anticipating, and User-Tailored Policy Planning. An active learning approach is also introduced to prioritize challenging user personas during training. Comprehensive experiments on collaborative and non-collaborative settings demonstrate the effectiveness of UDP in learning user-specific dialogue strategies, highlighting its robustness, adaptability, and potential to advance user-centric dialogue systems.<br /><br />Summary: <div>
arXiv:2504.13643v1 Announce Type: new 
Abstract: Recent advancements in dialogue policy planning have emphasized optimizing system agent policies to achieve predefined goals, focusing on strategy design, trajectory acquisition, and efficient training paradigms. However, these approaches often overlook the critical role of user characteristics, which are essential in real-world scenarios like conversational search and recommendation, where interactions must adapt to individual user traits such as personality, preferences, and goals. To address this gap, we first conduct a comprehensive study utilizing task-specific user personas to systematically assess dialogue policy planning under diverse user behaviors. By leveraging realistic user profiles for different tasks, our study reveals significant limitations in existing approaches, highlighting the need for user-tailored dialogue policy planning. Building on this foundation, we present the User-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an Intrinsic User World Model to model user traits and feedback. UDP operates in three stages: (1) User Persona Portraying, using a diffusion model to dynamically infer user profiles; (2) User Feedback Anticipating, leveraging a Brownian Bridge-inspired anticipator to predict user reactions; and (3) User-Tailored Policy Planning, integrating these insights to optimize response strategies. To ensure robust performance, we further propose an active learning approach that prioritizes challenging user personas during training. Comprehensive experiments on benchmarks, including collaborative and non-collaborative settings, demonstrate the effectiveness of UDP in learning user-specific dialogue strategies. Results validate the protocol's utility and highlight UDP's robustness, adaptability, and potential to advance user-centric dialogue systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Embedding Techniques for Classification of Star Ratings</title>
<link>https://arxiv.org/abs/2504.13653</link>
<guid>https://arxiv.org/abs/2504.13653</guid>
<content:encoded><![CDATA[
<div> Word Embedding, Natural Language Processing, Telecom Services, Text Classification, PCA <br />
Summary: <br />
Telecom services are essential for society, and customer feedback is crucial in improving service quality. This study examines how different word embedding models impact text classification in analyzing telecom customer reviews. Various state-of-the-art word embedding techniques, including BERT, Word2Vec, and Doc2Vec, were evaluated alongside different classification algorithms. Feature engineering and dimensionality reduction were also explored, with a focus on PCA-based approaches. The study found that certain word embeddings, such as BERT combined with PCA, consistently outperformed others in terms of precision, recall, and F1-Score. Additionally, a novel PCA approach of combining word vectors using the first principal component showed improved performance compared to the traditional averaging method. The research also considered the energy consumption associated with different word embeddings. Overall, the findings provide valuable insights for utilizing NLP tools to enhance telecom services based on customer feedback. <br /> <div>
arXiv:2504.13653v1 Announce Type: new 
Abstract: Telecom services are at the core of today's societies' everyday needs. The availability of numerous online forums and discussion platforms enables telecom providers to improve their services by exploring the views of their customers to learn about common issues that the customers face. Natural Language Processing (NLP) tools can be used to process the free text collected.
  One way of working with such data is to represent text as numerical vectors using one of many word embedding models based on neural networks. This research uses a novel dataset of telecom customers' reviews to perform an extensive study showing how different word embedding algorithms can affect the text classification process. Several state-of-the-art word embedding techniques are considered, including BERT, Word2Vec and Doc2Vec, coupled with several classification algorithms. The important issue of feature engineering and dimensionality reduction is addressed and several PCA-based approaches are explored. Moreover, the energy consumption used by the different word embeddings is investigated. The findings show that some word embedding models can lead to consistently better text classifiers in terms of precision, recall and F1-Score. In particular, for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics. Moreover, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2504.13655</link>
<guid>https://arxiv.org/abs/2504.13655</guid>
<content:encoded><![CDATA[
<div> contextual information, conversational recommender systems, multi-type, mixture-of-experts, structured information

Summary: 
The paper introduces a multi-type context-aware conversational recommender system (MCCRS) that leverages various types of contextual information to enhance recommendations. MCCRS combines structured knowledge graph, unstructured conversation history, and item reviews through a mixture-of-experts approach, with each expert specializing in a specific domain. A ChairBot coordinates multiple experts to generate final recommendations, addressing the challenge of integrating different contextual information effectively. Experimental results show that MCCRS outperforms existing baselines, demonstrating the effectiveness of leveraging multi-type contextual information and expert specialization in conversational recommender systems. <div>
arXiv:2504.13655v1 Announce Type: new 
Abstract: Conversational recommender systems enable natural language conversations and thus lead to a more engaging and effective recommendation scenario. As the conversations for recommender systems usually contain limited contextual information, many existing conversational recommender systems incorporate external sources to enrich the contextual information. However, how to combine different types of contextual information is still a challenge. In this paper, we propose a multi-type context-aware conversational recommender system, called MCCRS, effectively fusing multi-type contextual information via mixture-of-experts to improve conversational recommender systems. MCCRS incorporates both structured information and unstructured information, including the structured knowledge graph, unstructured conversation history, and unstructured item reviews. It consists of several experts, with each expert specialized in a particular domain (i.e., one specific contextual information). Multiple experts are then coordinated by a ChairBot to generate the final results. Our proposed MCCRS model takes advantage of different contextual information and the specialization of different experts followed by a ChairBot breaks the model bottleneck on a single contextual information. Experimental results demonstrate that our proposed MCCRS method achieves significantly higher performance compared to existing baselines.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results</title>
<link>https://arxiv.org/abs/2504.13677</link>
<guid>https://arxiv.org/abs/2504.13677</guid>
<content:encoded><![CDATA[
<div> AUROC, Uncertainty Quantification, Language Models, Correctness Functions, Bias 

Summary: 
- Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving safety and reliability, often evaluated using performance metrics like AUROC. 
- Evaluations using correctness functions like ROUGE-L may bias UQ assessments by inflating the performance of certain methods. 
- Analysis of 7 correctness functions across various datasets, models, and UQ methods reveals length biases causing distortions in assessments. 
- Error in correctness functions interacts with length biases in UQ methods, impacting evaluation accuracy. 
- LLM-as-a-judge approaches are identified as less length-biased choices, presenting a potential solution to mitigate biases. 

<br /><br /> <div>
arXiv:2504.13677v1 Announce Type: new 
Abstract: Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functions -- from lexical-based and embedding-based metrics to LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LLM-as-a-judge approaches as among the least length-biased choices and hence a potential solution to mitigate these biases.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep literature reviews: an application of fine-tuned language models to migration research</title>
<link>https://arxiv.org/abs/2504.13685</link>
<guid>https://arxiv.org/abs/2504.13685</guid>
<content:encoded><![CDATA[
<div> climate-induced migration, environmental hazards, human health, air and water pollution, infectious diseases 
Summary:
A hybrid framework for literature reviews combines traditional bibliometric methods with large language models (LLMs). By fine-tuning LLMs, qualitative insights can be extracted at scale from a large volume of research content. An error-focused validation process enhances annotation efficiency and consistency. Applying this framework to over 20,000 articles on human migration reveals a growing focus on climate-induced migration. However, there is a disproportionate emphasis on certain environmental hazards, neglecting others like air and water pollution and infectious diseases that impact human health. More comprehensive research is needed to explore the ecological and societal consequences of environmental changes on migration patterns. The study showcases the ability of LLMs to improve efficiency and depth of literature reviews, leading to enhanced knowledge synthesis and scientific discovery. 

<br /><br />Summary: <div>
arXiv:2504.13685v1 Announce Type: new 
Abstract: This paper presents a hybrid framework for literature reviews that augments traditional bibliometric methods with large language models (LLMs). By fine-tuning open-source LLMs, our approach enables scalable extraction of qualitative insights from large volumes of research content, enhancing both the breadth and depth of knowledge synthesis. To improve annotation efficiency and consistency, we introduce an error-focused validation process in which LLMs generate initial labels and human reviewers correct misclassifications. Applying this framework to over 20000 scientific articles about human migration, we demonstrate that a domain-adapted LLM can serve as a "specialist" model - capable of accurately selecting relevant studies, detecting emerging trends, and identifying critical research gaps. Notably, the LLM-assisted review reveals a growing scholarly interest in climate-induced migration. However, existing literature disproportionately centers on a narrow set of environmental hazards (e.g., floods, droughts, sea-level rise, and land degradation), while overlooking others that more directly affect human health and well-being, such as air and water pollution or infectious diseases. This imbalance highlights the need for more comprehensive research that goes beyond physical environmental changes to examine their ecological and societal consequences, particularly in shaping migration as an adaptive response. Overall, our proposed framework demonstrates the potential of fine-tuned LLMs to conduct more efficient, consistent, and insightful literature reviews across disciplines, ultimately accelerating knowledge synthesis and scientific discovery.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence</title>
<link>https://arxiv.org/abs/2504.13730</link>
<guid>https://arxiv.org/abs/2504.13730</guid>
<content:encoded><![CDATA[
<div> Keywords: Open-source intelligence, textual data, territorial control prediction, large language models, few-shot methods <br />
Summary: <br />
- The framework CONTACT utilizes large language models (LLMs) and minimal supervision for territorial control prediction, focusing on assessing ISIS activity in Syria and Iraq using news articles.
- Two approaches are evaluated: SetFit, an embedding-based few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a multilingual generative LLM. 
- The model is trained on a small hand-labeled dataset and employs prompt-conditioned extraction of control-relevant signals like military operations and casualties.
- Results indicate that the BLOOMZ-based model outperforms the SetFit baseline, showcasing the effectiveness of prompt-based supervision for generalization in low-resource settings.
- CONTACT demonstrates the potential of LLMs fine-tuned with few-shot methods to reduce annotation burdens and enable structured inference from open-ended OSINT streams. <br /> 
Summary: <div>
arXiv:2504.13730v1 Announce Type: new 
Abstract: Open-source intelligence provides a stream of unstructured textual data that can inform assessments of territorial control. We present CONTACT, a framework for territorial control prediction using large language models (LLMs) and minimal supervision. We evaluate two approaches: SetFit, an embedding-based few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a multilingual generative LLM. Our model is trained on a small hand-labeled dataset of news articles covering ISIS activity in Syria and Iraq, using prompt-conditioned extraction of control-relevant signals such as military operations, casualties, and location references. We show that the BLOOMZ-based model outperforms the SetFit baseline, and that prompt-based supervision improves generalization in low-resource settings. CONTACT demonstrates that LLMs fine-tuned using few-shot methods can reduce annotation burdens and support structured inference from open-ended OSINT streams. Our code is available at https://github.com/PaulKMandal/CONTACT/.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models</title>
<link>https://arxiv.org/abs/2504.13775</link>
<guid>https://arxiv.org/abs/2504.13775</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attack, large language model, adaptive optimization mechanism, prompt adaptability, semantic consistency

Summary:
<br /><br />
The paper introduces a new backdoor attack, BadApex, that utilizes a black-box large language model (LLM) to generate poisoned text through a refined prompt. An Adaptive Optimization Mechanism is designed to iteratively refine the prompt using generation and modification agents, improving prompt adaptability, semantic consistency, and text quality. Extensive experiments on three datasets with six backdoor attacks and two defenses show that BadApex outperforms state-of-the-art attacks. It achieves high attack success rates even when defenses are applied, demonstrating its effectiveness in improving prompt adaptability, semantic consistency, and text quality. <div>
arXiv:2504.13775v1 Announce Type: new 
Abstract: Previous insertion-based and paraphrase-based backdoors have achieved great success in attack efficacy, but they ignore the text quality and semantic consistency between poisoned and clean texts. Although recent studies introduce LLMs to generate poisoned texts and improve the stealthiness, semantic consistency, and text quality, their hand-crafted prompts rely on expert experiences, facing significant challenges in prompt adaptability and attack performance after defenses. In this paper, we propose a novel backdoor attack based on adaptive optimization mechanism of black-box large language models (BadApex), which leverages a black-box LLM to generate poisoned text through a refined prompt. Specifically, an Adaptive Optimization Mechanism is designed to refine an initial prompt iteratively using the generation and modification agents. The generation agent generates the poisoned text based on the initial prompt. Then the modification agent evaluates the quality of the poisoned text and refines a new prompt. After several iterations of the above process, the refined prompt is used to generate poisoned texts through LLMs. We conduct extensive experiments on three dataset with six backdoor attacks and two defenses. Extensive experimental results demonstrate that BadApex significantly outperforms state-of-the-art attacks. It improves prompt adaptability, semantic consistency, and text quality. Furthermore, when two defense methods are applied, the average attack success rate (ASR) still up to 96.75%.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations</title>
<link>https://arxiv.org/abs/2504.13816</link>
<guid>https://arxiv.org/abs/2504.13816</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, knowledge boundaries, cross-lingual analysis, language differences, fine-tuning

Summary:<br /><br />LLMs encode perceptions of knowledge boundaries in middle to middle-upper layers across different languages, according to a new study. Language differences in knowledge boundary perception follow a linear structure, prompting the development of a training-free alignment method to transfer this ability across languages and reduce hallucination risk in low-resource languages. Fine-tuning on bilingual question pair translation further improves LLMs' recognition of knowledge boundaries across languages. Additionally, a multilingual evaluation suite with various types of knowledge boundary data has been created due to the lack of standard testbeds for cross-lingual analysis. The code and datasets for this research are openly available at the specified GitHub repository. <div>
arXiv:2504.13816v1 Announce Type: new 
Abstract: While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs' perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs' recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models</title>
<link>https://arxiv.org/abs/2504.13825</link>
<guid>https://arxiv.org/abs/2504.13825</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge distillation, teacher models, student models, attention mechanisms, language models

Summary: 
Knowledge distillation (KD) is a technique that transfers knowledge from complex teacher models to simpler student models, improving efficiency and accuracy. Recent innovations in KD methods, such as attention-based approaches and block-wise logit distillation, have enhanced student model performance by focusing on stimulus complexity and global information capture. KD has also been successful in compressing large language models while maintaining accuracy and improving inference speed. This survey synthesizes the latest literature on KD, highlighting key findings and contributions, and discussing future directions for research and application in artificial intelligence and machine learning.

<br /><br />Summary: <div>
arXiv:2504.13825v1 Announce Type: new 
Abstract: Knowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, language modeling, text classification, and sentiment analysis. Recent innovations in KD methods, such as attention-based approaches, block-wise logit distillation, and decoupling distillation, have notably improved student model performance. These techniques focus on stimulus complexity, attention mechanisms, and global information capture to optimize knowledge transfer. In addition, KD has proven effective in compressing large language models while preserving accuracy, reducing computational overhead, and improving inference speed. This survey synthesizes the latest literature, highlighting key findings, contributions, and future directions in knowledge distillation to provide insights for researchers and practitioners on its evolving role in artificial intelligence and machine learning.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI Act II: Test Time Scaling Drives Cognition Engineering</title>
<link>https://arxiv.org/abs/2504.13828</link>
<guid>https://arxiv.org/abs/2504.13828</guid>
<content:encoded><![CDATA[
<div> scaling, cognition engineering, Large Language Models, prompt engineering, knowledge latency

Summary:
In this paper, the authors discuss the transition from "Act I" to "Act II" in the field of generative AI, focusing on cognition engineering and test-time scaling techniques. The first generation of Large Language Models achieved success but had limitations in knowledge latency and shallow reasoning. Prompt engineering was crucial for communication with AI through natural language. In "Act II", models are evolving into thought-construction engines through scaling methods, creating a mind-level connection with AI. The paper provides tutorials and implementations to democratize access to cognition engineering. The authors emphasize the importance of this moment for the development of cognition engineering and offer a GitHub repository for updated resources on test-time scaling. This shift marks a critical phase in advancing AI capabilities and enabling practitioners to engage in AI's second act. 

<br /><br />Summary: <div>
arXiv:2504.13828v1 Announce Type: new 
Abstract: The first generation of Large Language Models - what might be called "Act I" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of "Act II" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI's second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Hierarchography: Hierarchical Organization of Science Literature</title>
<link>https://arxiv.org/abs/2504.13834</link>
<guid>https://arxiv.org/abs/2504.13834</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific literature, hierarchical structure, clustering algorithms, LLM-based prompting, interdisciplinary research

Summary:
SCIENCE HIERARCHOGRAPHY aims to organize scientific literature into a hierarchical structure for categorization across varying levels of abstraction. The approach involves combining fast embedding-based clustering with LLM-based prompting to efficiently balance computational speed and semantic precision. This method proves to be more effective than relying solely on LLM prompting. The hierarchical framework captures multiple dimensions of categorization beyond simple topics, reflecting the interdisciplinary nature of research papers. Evaluations demonstrate the framework's utility in enhancing interpretability, supporting trend discovery, and providing an alternative pathway for exploring scientific literature. The code, data, and demo for SCIENCE HIERARCHOGRAPHY are available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2504.13834v1 Announce Type: new 
Abstract: Scientific knowledge is growing rapidly, making it challenging to track progress and high-level conceptual links across broad disciplines. While existing tools like citation networks and search engines make it easy to access a few related papers, they fundamentally lack the flexible abstraction needed to represent the density of activity in various scientific subfields. We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that allows for the categorization of scientific work across varying levels of abstraction, from very broad fields to very specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve the goals of SCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach combines fast embedding-based clustering with LLM-based prompting to balance the computational efficiency of embedding methods with the semantic precision offered by LLM prompting. We demonstrate that this approach offers the best trade-off between quality and speed compared to methods that heavily rely on LLM prompting, such as iterative tree construction with LLMs. To better reflect the interdisciplinary and multifaceted nature of research papers, our hierarchy captures multiple dimensions of categorization beyond simple topic labels. We evaluate the utility of our framework by assessing how effectively an LLM-based agent can locate target papers using the hierarchy. Results show that this structured approach enhances interpretability, supports trend discovery, and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo: $\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space</title>
<link>https://arxiv.org/abs/2504.13835</link>
<guid>https://arxiv.org/abs/2504.13835</guid>
<content:encoded><![CDATA[
<div> Keywords: Data quality, diversity, instruction-tuning datasets, semantic space, information gain

Summary:
The article introduces a new method for automatically selecting high-quality and diverse subsets from instruction-tuning datasets. Existing methods often rely on heuristic rules, leading to suboptimal results. The proposed method quantifies the information content of datasets by modeling the semantic space using a label graph. Diversity is measured based on the distribution of information within the graph. An efficient sampling method, called MIG (Maximize Information Gain), is introduced to select data samples iteratively in the semantic space. Experimental results show that MIG outperforms state-of-the-art methods consistently. Fine-tuning a model with 5% Tulu3 data sampled using MIG achieves comparable performance to the official SFT model trained on the full dataset, with significant improvements on AlpacaEval and Wildbench datasets. <div>
arXiv:2504.13835v1 Announce Type: new 
Abstract: Data quality and diversity are key to the construction of effective instruction-tuning datasets. %
With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. %
Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. %
However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. %
Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. %
To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. %
Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to \textbf{M}aximize the \textbf{I}nformation \textbf{G}ain (MIG) in semantic space. %
Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. %
Notably, the model fine-tuned with 5\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\% on AlpacaEval and +6.89\% on Wildbench.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Quantum LLM: Modeling Semantic Spaces with Quantum Principles</title>
<link>https://arxiv.org/abs/2504.13202</link>
<guid>https://arxiv.org/abs/2504.13202</guid>
<content:encoded><![CDATA[
<div> framework, semantic representation, quantum mechanics, language models, quantum computing
Summary:
In the article, a quantum-inspired framework for modeling semantic representation and processing in Large Language Models (LLMs) is presented. The framework is based on mathematical tools and analogies from quantum mechanics. Six key principles governing semantic representation, interaction, and dynamics within LLMs are detailed, demonstrating the validity of the quantum-inspired approach. The framework provides insights into information processing and response generation in LLMs. The potential of utilizing quantum computing to enhance the efficiency and power of LLMs based on these principles is discussed. <div>
arXiv:2504.13202v1 Announce Type: cross 
Abstract: In the previous article, we presented a quantum-inspired framework for modeling semantic representation and processing in Large Language Models (LLMs), drawing upon mathematical tools and conceptual analogies from quantum mechanics to offer a new perspective on these complex systems. In this paper, we clarify the core assumptions of this model, providing a detailed exposition of six key principles that govern semantic representation, interaction, and dynamics within LLMs. The goal is to justify that a quantum-inspired framework is a valid approach to studying semantic spaces. This framework offers valuable insights into their information processing and response generation, and we further discuss the potential of leveraging quantum computing to develop significantly more powerful and efficient LLMs based on these principles.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents</title>
<link>https://arxiv.org/abs/2504.13203</link>
<guid>https://arxiv.org/abs/2504.13203</guid>
<content:encoded><![CDATA[
<div> Framework, Language Models, Multi-turn Interactions, Safety, Attacks  
Summary:  
X-Teaming is introduced as a scalable framework for exploring how seemingly harmless interactions can escalate into harmful outcomes in multi-turn interactions with language models (LMs). This framework utilizes collaborative agents for planning, attack optimization, and verification, achieving high success rates in multi-turn jailbreak effectiveness. X-Teaming demonstrates state-of-the-art performance in generating attack scenarios against both open-weight and closed-source models, including achieving a 96.2% success rate against the previously considered immune Claude 3.7 Sonnet model. Additionally, XGuard-Train, a large-scale multi-turn safety training dataset, is introduced to enhance the robustness of multi-turn safety alignment for LMs. These tools and insights provided by the study contribute towards mitigating sophisticated conversational attacks and advancing the safety of LMs in multi-turn interactions.  
<br /><br />Summary: <div>
arXiv:2504.13203v1 Announce Type: cross 
Abstract: Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces</title>
<link>https://arxiv.org/abs/2504.13277</link>
<guid>https://arxiv.org/abs/2504.13277</guid>
<content:encoded><![CDATA[
<div> Keywords: Suicide, Suicidal Ideation, Interpersonal Theory of Suicide, Machine Learning, AI chatbots

Summary: 
This study examines suicidal ideation (SI) posts on Reddit's r/SuicideWatch using the Interpersonal Theory of Suicide (IPTS). Analysis of 59,607 posts categorizes them into SI dimensions and risk factors, revealing that high-risk SI posts often express planning, attempts, and pain. Supportive responses varied based on the stage of SI posts. The study also explores the effectiveness of AI chatbots in providing support, finding improvements in structural coherence but limitations in personalized and empathetic responses. The findings emphasize the importance of understanding and developing AI-driven interventions for mental health support. <br /><br />Summary: <div>
arXiv:2504.13277v1 Announce Type: cross 
Abstract: Suicide is a critical global public health issue, with millions experiencing suicidal ideation (SI) each year. Online spaces enable individuals to express SI and seek peer support. While prior research has revealed the potential of detecting SI using machine learning and natural language analysis, a key limitation is the lack of a theoretical framework to understand the underlying factors affecting high-risk suicidal intent. To bridge this gap, we adopted the Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607 posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions (Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired Capability of Suicide). We found that high-risk SI posts express planning and attempts, methods and tools, and weaknesses and pain. In addition, we also examined the language of supportive responses through psycholinguistic and content analyses to find that individuals respond differently to different stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI chatbots in providing effective supportive responses to suicidal ideation posts. We found that although AI improved structural coherence, expert evaluations highlight persistent shortcomings in providing dynamic, personalized, and deeply empathetic support. These findings underscore the need for careful reflection and deeper understanding in both the development and consideration of AI-driven interventions for effective mental health support.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope</title>
<link>https://arxiv.org/abs/2504.13308</link>
<guid>https://arxiv.org/abs/2504.13308</guid>
<content:encoded><![CDATA[
<div> Keywords: Acoustic-to-Articulatory Inversion, Speaker Dependent, Speaker Independent, Automatic Speech Recognition, Machine Learning

Summary:
This review discusses data-driven approaches in Acoustic-to-Articulatory Inversion (AAI) of speech over the last decade. The focus is on Speaker Dependent and Speaker Independent AAI for tasks such as Articulatory approximation, Articulatory Feature space selection, and Automatic Speech Recognition. Various medical imaging models and speech corpora are used, including Electromagnetic Articulography and real-time Magnetic Resonance Imaging. Machine learning methods are predominantly employed in recent works, with evaluation based on metrics like Correlation Coefficient and Root Mean Square Error. The practical applications of AAI include providing interpretable feedback on articulatory positions, particularly tongue movements, with potential applications in phonetic, language, and speech therapy for pathological subjects.<br /><br />Summary: <div>
arXiv:2504.13308v1 Announce Type: cross 
Abstract: This review is focused on the data-driven approaches applied in different applications of Acoustic-to-Articulatory Inversion (AAI) of speech. This review paper considered the relevant works published in the last ten years (2011-2021). The selection criteria includes (a) type of AAI - Speaker Dependent and Speaker Independent AAI, (b) objectives of the work - Articulatory approximation, Articulatory Feature space selection and Automatic Speech Recognition (ASR), explore the correlation between acoustic and articulatory features, and framework for Computer-assisted language training, (c) Corpus - Simultaneously recorded speech (wav) and medical imaging models such as ElectroMagnetic Articulography (EMA), Electropalatography (EPG), Laryngography, Electroglottography (EGG), X-ray Cineradiography, Ultrasound, and real-time Magnetic Resonance Imaging (rtMRI), (d) Methods or models - recent works are considered, and therefore all the works are based on machine learning, (e) Evaluation - as AAI is a non-linear regression problem, the performance evaluation is mostly done by Correlation Coefficient (CC), Root Mean Square Error (RMSE), and also considered Mean Square Error (MSE), and Mean Format Error (MFE). The practical application of the AAI model can provide a better and user-friendly interpretable image feedback system of articulatory positions, especially tongue movement. Such trajectory feedback system can be used to provide phonetic, language, and speech therapy for pathological subjects.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-of-Pass: An Economic Framework for Evaluating Language Models</title>
<link>https://arxiv.org/abs/2504.13359</link>
<guid>https://arxiv.org/abs/2504.13359</guid>
<content:encoded><![CDATA[
<div> production theory, language models, cost-of-pass, economic value, inference cost

Summary:
The article proposes a framework for evaluating language models based on accuracy and inference cost using production theory. It introduces the concept of "cost-of-pass" as the expected monetary cost of generating a correct solution and the "frontier cost-of-pass" as the minimum achievable cost across available models. The analysis reveals that lightweight models are cost-effective for basic tasks, large models for knowledge-intensive tasks, and reasoning models for complex quantitative problems. Progress over the past year shows significant cost reductions, particularly for complex tasks. Innovations in lightweight, large, and reasoning models have been key drivers of cost-efficiency. Common inference-time techniques like majority voting and self-refinement may not justify their costs in terms of marginal accuracy gains. The findings emphasize the importance of complementary model-level innovations in driving cost-efficiency, and the economic framework provided can guide deployment strategies. 

Summary: <div>
arXiv:2504.13359v1 Announce Type: cross 
Abstract: The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. We propose a framework grounded in production theory for evaluating language models by combining accuracy and inference cost. We introduce "cost-of-pass", the expected monetary cost of generating a correct solution. We then define the "frontier cost-of-pass" as the minimum cost-of-pass achievable across available models or the "human-expert, using the approximate cost of hiring an expert. Our analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, we examine counterfactual frontiers: estimates of cost-efficiency without specific model classes. We find that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledge-intensive, and complex quantitative tasks, respectively. Finally, we assess the cost-reductions afforded by common inference-time techniques like majority voting and self-refinement, finding that their marginal accuracy gains rarely justify their costs. Our findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and our economic framework provides a principled tool for measuring this progress and guiding deployment.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mean teacher algorithm for unlearning of language models</title>
<link>https://arxiv.org/abs/2504.13388</link>
<guid>https://arxiv.org/abs/2504.13388</guid>
<content:encoded><![CDATA[
<div> mean teacher algorithm, language model unlearning, memorization reduction, natural gradient descent, negative log-unlikelihood<br />
Summary:<br />
This paper explores the application of the mean teacher algorithm in language model unlearning to reduce memorization while preserving general abilities. The mean teacher algorithm approximates a slow natural gradient descent trajectory to minimize model degradation during unlearning. To prevent vanishing gradients, a new unlearning loss called negative log-unlikelihood is introduced. The combination of mean teacher and negative log-unlikelihood demonstrates improvements on MUSE benchmarks. This approach addresses the challenge of reducing memorization in large datasets without compromising model utility, offering a promising solution for continual learning and model adaptability in natural language processing tasks. <div>
arXiv:2504.13388v1 Announce Type: cross 
Abstract: One of the goals of language model unlearning is to reduce memorization of selected text instances while retaining the model's general abilities. Despite various proposed methods, reducing memorization of large datasets without noticeable degradation in model utility remains challenging. In this paper, we investigate the mean teacher algorithm (Tarvainen & Valpola, 2017), a simple proximal optimization method from continual learning literature that gradually modifies the teacher model. We show that the mean teacher can approximate a trajectory of a slow natural gradient descent (NGD), which inherently seeks low-curvature updates that are less likely to degrade the model utility. While slow NGD can suffer from vanishing gradients, we introduce a new unlearning loss called "negative log-unlikelihood" (NLUL) that avoids this problem. We show that the combination of mean teacher and NLUL improves some metrics on the MUSE benchmarks (Shi et al., 2024).
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangCoop: Collaborative Driving with Language</title>
<link>https://arxiv.org/abs/2504.13406</link>
<guid>https://arxiv.org/abs/2504.13406</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent collaboration, autonomous driving, natural language, communication, information sharing

Summary: LangCoop proposes a new approach to enhance collaborative autonomous driving systems by utilizing natural language for inter-agent communication. It introduces M$^3$CoT for structured zero-shot vision-language reasoning and LangPack for efficient information packaging. Through experiments in CARLA simulations, LangCoop significantly reduces communication bandwidth (< 2KB per message) compared to image-based methods while maintaining high driving performance. This innovative paradigm addresses challenges such as high bandwidth demands, agent heterogeneity, and information loss in multi-agent communication. <div>
arXiv:2504.13406v1 Announce Type: cross 
Abstract: Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that leverages natural language as a compact yet expressive medium for inter-agent communication. LangCoop features two key innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured zero-shot vision-language reasoning and Natural Language Information Packaging (LangPack) for efficiently packaging information into concise, language-based messages. Through extensive experiments conducted in the CARLA simulations, we demonstrate that LangCoop achieves a remarkable 96\% reduction in communication bandwidth (< 2KB per message) compared to image-based communication, while maintaining competitive driving performance in the closed-loop evaluation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings</title>
<link>https://arxiv.org/abs/2504.13416</link>
<guid>https://arxiv.org/abs/2504.13416</guid>
<content:encoded><![CDATA[
<div> watermarking, language models, dataset membership, data contamination, pretraining corpora <br />
Summary: <br />
The paper introduces STAMP, a framework designed to detect dataset membership in the pretraining corpora of large language models (LLMs). STAMP involves embedding unique watermarks in multiple rephrased versions of an original piece of content, with one version made public and others kept private. By comparing model likelihoods between the public and private versions, creators can use paired statistical tests to prove dataset inclusion. The framework successfully detects dataset contamination in four benchmarks, outperforming other detection methods. STAMP maintains the semantic meaning and utility of the original data while comparing models. The approach is applied to detect the inclusion of paper abstracts and blog articles in pretraining corpora, addressing concerns about proprietary data usage and dataset integrity in model training processes. <br /> <div>
arXiv:2504.13416v1 Announce Type: cross 
Abstract: Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership-i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and the utility of the original data in comparing different models. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation</title>
<link>https://arxiv.org/abs/2504.13472</link>
<guid>https://arxiv.org/abs/2504.13472</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, code generation, evaluation, multisource domain knowledge, complex code<br />
<br />
Summary: <br />
The article introduces CodeVisionary, a novel framework for evaluating Large Language Models (LLMs) in code generation tasks. CodeVisionary addresses the limitations of existing evaluation approaches by incorporating a multisource knowledge analysis stage to gather comprehensive domain knowledge and a negotiation-based scoring stage where judges engage in discussions to better understand complex code. Through extensive experiments, CodeVisionary outperforms baseline methods in evaluation metrics such as Pearson, Spearman, and Kendall-Tau coefficients. Additionally, CodeVisionary provides detailed evaluation reports to help developers identify areas for improvement. The resources for CodeVisionary are available online, making it a valuable tool for evaluating LLMs in code generation tasks. <div>
arXiv:2504.13472v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities and superior efficiency. However, the performance of LLM-based approaches remains limited due to: (1) lack of multisource domain knowledge, and (2) insufficient comprehension of complex code.
  To mitigate the limitations, we propose CodeVisionary, the first LLM-based agent framework for evaluating LLMs in code generation. CodeVisionary consists of two stages: (1) Multiscore knowledge analysis stage, which aims to gather multisource and comprehensive domain knowledge by formulating and executing a stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves multiple judges engaging in discussions to better comprehend the complex code and reach a consensus on the evaluation score. Extensive experiments demonstrate that CodeVisionary achieves the best performance for evaluating LLMs in code generation, outperforming the best baseline methods with average improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. Besides, CodeVisionary provides detailed evaluation reports, which assist developers in identifying shortcomings and making improvements. The resources of CodeVisionary are available at https://anonymous.4open.science/r/CodeVisionary.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Locality-Aware Attention with Transformers for General Geometry PDEs</title>
<link>https://arxiv.org/abs/2504.13480</link>
<guid>https://arxiv.org/abs/2504.13480</guid>
<content:encoded><![CDATA[
<div> Transformer-based neural operators, such as LA2Former, improve predictive accuracy in solving PDEs. Keywords: Neural operators, partial differential equations, Transformer, attention mechanism, local interactions <br />Summary: LA2Former is a new approach for solving PDEs using Transformer-based neural operators that leverage K-nearest neighbors for dynamic patchifying and integrate global-local attention for enhanced PDE modeling. By combining global context encoding with pairwise attention for local interactions, LA2Former achieves optimal balance between efficiency and accuracy. Extensive evaluations show LA2Former improves predictive accuracy by over 50% compared to existing methods, outperforming full pairwise attention under optimal conditions. This work highlights the importance of localized feature learning in advancing Transformer-based neural operators for solving PDEs on complex and irregular domains. <br /><br /> <div>
arXiv:2504.13480v1 Announce Type: cross 
Abstract: Neural operators have emerged as promising frameworks for learning mappings governed by partial differential equations (PDEs), serving as data-driven alternatives to traditional numerical methods. While methods such as the Fourier neural operator (FNO) have demonstrated notable performance, their reliance on uniform grids restricts their applicability to complex geometries and irregular meshes. Recently, Transformer-based neural operators with linear attention mechanisms have shown potential in overcoming these limitations for large-scale PDE simulations. However, these approaches predominantly emphasize global feature aggregation, often overlooking fine-scale dynamics and localized PDE behaviors essential for accurate solutions. To address these challenges, we propose the Locality-Aware Attention Transformer (LA2Former), which leverages K-nearest neighbors for dynamic patchifying and integrates global-local attention for enhanced PDE modeling. By combining linear attention for efficient global context encoding with pairwise attention for capturing intricate local interactions, LA2Former achieves an optimal balance between computational efficiency and predictive accuracy. Extensive evaluations across six benchmark datasets demonstrate that LA2Former improves predictive accuracy by over 50% relative to existing linear attention methods, while also outperforming full pairwise attention under optimal conditions. This work underscores the critical importance of localized feature learning in advancing Transformer-based neural operators for solving PDEs on complex and irregular domains.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation</title>
<link>https://arxiv.org/abs/2504.13551</link>
<guid>https://arxiv.org/abs/2504.13551</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial attacks, language models, black-box attacks, Q-faker, surrogate model 

Summary: 
Q-faker is a novel method for generating adversarial examples without accessing the target model, using a surrogate model instead. The approach aims to overcome the limitations of traditional adversarial attack methods by providing a query-free hard black-box attacker. By leveraging controlled generation techniques, the method demonstrates high transferability and the production of high-quality adversarial examples across various datasets. The proposed approach eliminates the need for extensive queries and expensive training costs associated with existing attack methods, making it practical for real-world scenarios where the target model is closed and inaccessible. Experimental results validate the effectiveness of Q-faker in hard black-box settings, showcasing its potential for generating adversarial examples efficiently and accurately. 

<br /><br />Summary: <div>
arXiv:2504.13551v1 Announce Type: cross 
Abstract: Many adversarial attack approaches are proposed to verify the vulnerability of language models. However, they require numerous queries and the information on the target model. Even black-box attack methods also require the target model's output information. They are not applicable in real-world scenarios, as in hard black-box settings where the target model is closed and inaccessible. Even the recently proposed hard black-box attacks still require many queries and demand extremely high costs for training adversarial generators. To address these challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a novel and efficient method that generates adversarial examples without accessing the target model. To avoid accessing the target model, we use a surrogate model instead. The surrogate model generates adversarial sentences for a target-agnostic attack. During this process, we leverage controlled generation techniques. We evaluate our proposed method on eight datasets. Experimental results demonstrate our method's effectiveness including high transferability and the high quality of the generated adversarial examples, and prove its practical in hard black-box settings.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs</title>
<link>https://arxiv.org/abs/2504.13644</link>
<guid>https://arxiv.org/abs/2504.13644</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, probabilistic reasoning, information retrieval, automated decision systems, uncertainty quantification

Summary: 
Large language models (LLMs) have become essential tools in information retrieval and automated decision systems. However, a crucial aspect of their performance lies in the representation of probabilistic reasoning. Despite previous beliefs that LLMs can exhibit complex reasoning and accurate uncertainty quantification, recent studies reveal shortcomings in their ability to provide logical and coherent probabilistic beliefs. To address this, a new dataset of claims with uncertain truth values was introduced, and established techniques for uncertainty quantification were employed to assess the LLMs' adherence to foundational principles of probabilistic reasoning. The findings indicate that current iterations of LLMs fall short in delivering rational and consistent probabilistic representations, highlighting the need for further research and development in this area. 

<br /><br />Summary: <div>
arXiv:2504.13644v1 Announce Type: cross 
Abstract: Advances in the general capabilities of large language models (LLMs) have led to their use for information retrieval, and as components in automated decision systems. A faithful representation of probabilistic reasoning in these models may be essential to ensure trustworthy, explainable and effective performance in these tasks. Despite previous work suggesting that LLMs can perform complex reasoning and well-calibrated uncertainty quantification, we find that current versions of this class of model lack the ability to provide rational and coherent representations of probabilistic beliefs. To demonstrate this, we introduce a novel dataset of claims with indeterminate truth values and apply a number of well-established techniques for uncertainty quantification to measure the ability of LLM's to adhere to fundamental properties of probabilistic reasoning.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm</title>
<link>https://arxiv.org/abs/2504.13667</link>
<guid>https://arxiv.org/abs/2504.13667</guid>
<content:encoded><![CDATA[
<div> education, Large Language Models, technology, interactive systems designers, self-ethnographic study

Summary: The paper discusses the potential impacts of Large Language Models (LLMs) on education and technology interaction for children. It highlights the relatively minor effects of LLMs on education thus far and predicts significant upcoming changes. A scenario and self-ethnographic study are presented to demonstrate these changes. Additionally, five key considerations for interactive systems designers in the future are identified. These considerations emphasize the need to adapt to the evolving role of LLMs in shaping learning experiences and technological interactions for children. <div>
arXiv:2504.13667v1 Announce Type: cross 
Abstract: This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology. We review the effects of LLMs on education so far, and make the case that these effects are minor compared to the upcoming changes that are occurring. We present a small scenario and self-ethnographic study demonstrating the effects of these changes, and define five significant considerations that interactive systems designers will have to accommodate in the future.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation</title>
<link>https://arxiv.org/abs/2504.13707</link>
<guid>https://arxiv.org/abs/2504.13707</guid>
<content:encoded><![CDATA[
<div> evaluate, large language models, deception risks, OpenDeception, agent applications
Summary: 
- OpenDeception is introduced as a novel framework to evaluate deception risks in large language models (LLMs) used in agent applications.
- The framework includes an open-ended scenario dataset to assess both the deception intention and capabilities of LLM-based agents by analyzing their internal reasoning process.
- Five common use case types with ten diverse scenarios each are constructed for evaluation.
- To avoid ethical concerns, simulations of multi-turn dialogues are proposed instead of interactions with human testers.
- Evaluation of eleven mainstream LLMs on OpenDeception reveals a high deception intention ratio and success rate, calling for measures to address deception risks and security concerns in LLM-based agents. Stronger LLM capabilities are linked to a higher risk of deception, emphasizing the need for efforts to curb deceptive behaviors.br><br />Summary: <div>
arXiv:2504.13707v1 Announce Type: cross 
Abstract: As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Attribute with Attention</title>
<link>https://arxiv.org/abs/2504.13752</link>
<guid>https://arxiv.org/abs/2504.13752</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, token attribution, attention weights, Attribution with Attention, question answering

Summary:
The study introduces a method called Attribution with Attention (AT2) to efficiently attribute the influence of preceding tokens in language models. Traditional methods involve ablating tokens, but AT2 leverages attention weights as features to accurately estimate a token's impact on model behavior. By treating attention weights from different attention heads as features, AT2 delivers reliable attributions comparable to ablation-based methods but with significantly lower computational cost. The authors demonstrate the efficacy of AT2 by applying it to prune less important parts of context in a question answering task, leading to improved answer quality. The code for AT2 is available on GitHub for further exploration and implementation. Overall, AT2 offers a practical and efficient solution for token attribution in language models, with potential applications in various natural language processing tasks. 

<br /><br />Summary: <div>
arXiv:2504.13752v1 Announce Type: cross 
Abstract: Given a sequence of tokens generated by a language model, we may want to identify the preceding tokens that influence the model to generate this sequence. Performing such token attribution is expensive; a common approach is to ablate preceding tokens and directly measure their effects. To reduce the cost of token attribution, we revisit attention weights as a heuristic for how a language model uses previous tokens. Naive approaches to attribute model behavior with attention (e.g., averaging attention weights across attention heads to estimate a token's influence) have been found to be unreliable. To attain faithful attributions, we propose treating the attention weights of different attention heads as features. This way, we can learn how to effectively leverage attention weights for attribution (using signal from ablations). Our resulting method, Attribution with Attention (AT2), reliably performs on par with approaches that involve many ablations, while being significantly more efficient. To showcase the utility of AT2, we use it to prune less important parts of a provided context in a question answering setting, improving answer quality. We provide code for AT2 at https://github.com/MadryLab/AT2 .
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling sparse feature circuit finding for in-context learning</title>
<link>https://arxiv.org/abs/2504.13756</link>
<guid>https://arxiv.org/abs/2504.13756</guid>
<content:encoded><![CDATA[
arXiv:2504.13756v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) are a popular tool for interpreting large language model activations, but their utility in addressing open questions in interpretability remains unclear. In this work, we demonstrate their effectiveness by using SAEs to deepen our understanding of the mechanism behind in-context learning (ICL). We identify abstract SAE features that (i) encode the model's knowledge of which task to execute and (ii) whose latent vectors causally induce the task zero-shot. This aligns with prior work showing that ICL is mediated by task vectors. We further demonstrate that these task vectors are well approximated by a sparse sum of SAE latents, including these task-execution features. To explore the ICL mechanism, we adapt the sparse feature circuits methodology of Marks et al. (2024) to work for the much larger Gemma-1 2B model, with 30 times as many parameters, and to the more complex task of ICL. Through circuit finding, we discover task-detecting features with corresponding SAE latents that activate earlier in the prompt, that detect when tasks have been performed. They are causally linked with task-execution features through the attention and MLP sublayers.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13818</link>
<guid>https://arxiv.org/abs/2504.13818</guid>
<content:encoded><![CDATA[
arXiv:2504.13818v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing reasoning capabilities in large language models, but faces a fundamental asymmetry in computation and memory requirements: inference is embarrassingly parallel with a minimal memory footprint, while policy updates require extensive synchronization and are memory-intensive. To address this asymmetry, we introduce PODS (Policy Optimization with Down-Sampling), a framework that strategically decouples these phases by generating numerous rollouts in parallel but updating only on an informative subset. Within this framework, we develop max-variance down-sampling, a theoretically motivated method that selects rollouts with maximally diverse reward signals. We prove that this approach has an efficient algorithmic solution, and empirically demonstrate that GRPO with PODS using max-variance down-sampling achieves superior performance over standard GRPO on the GSM8K benchmark.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
arXiv:2504.13837v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\textit{k} metric with large values of \textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\eg, $k$=1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation</title>
<link>https://arxiv.org/abs/2401.07456</link>
<guid>https://arxiv.org/abs/2401.07456</guid>
<content:encoded><![CDATA[
arXiv:2401.07456v2 Announce Type: replace 
Abstract: Federated learning (FL) is a promising distributed machine learning paradigm that enables multiple clients to collaboratively train a global model. In this paper, we focus on a practical federated multilingual learning setup where clients with their own language-specific data aim to collaboratively construct a high-quality neural machine translation (NMT) model. However, communication constraints in practical network systems present challenges for exchanging large-scale NMT engines between FL parties. We propose a meta-learning-based adaptive parameter selection methodology, MetaSend, that improves the communication efficiency of model transmissions from clients during FL-based multilingual NMT training. Our approach learns a dynamic threshold for filtering parameters prior to transmission without compromising the NMT model quality, based on the tensor deviations of clients between different FL rounds. Through experiments on two NMT datasets with different language distributions, we demonstrate that MetaSend obtains substantial improvements over baselines in translation quality in the presence of a limited communication budget.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of LLM Sampling: Part Descriptive and Part Prescriptive</title>
<link>https://arxiv.org/abs/2402.11005</link>
<guid>https://arxiv.org/abs/2402.11005</guid>
<content:encoded><![CDATA[
arXiv:2402.11005v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly utilized in autonomous decision-making, where they sample options from vast action spaces. However, the heuristics that guide this sampling process remain under-explored. We study this sampling behavior and show that this underlying heuristics resembles that of human decision-making: comprising a descriptive component (reflecting statistical norm) and a prescriptive component (implicit ideal encoded in the LLM) of a concept. We show that this deviation of a sample from the statistical norm towards a prescriptive component consistently appears in concepts across diverse real-world domains like public health, and economic trends. To further illustrate the theory, we demonstrate that concept prototypes in LLMs are affected by prescriptive norms, similar to the concept of normality in humans. Through case studies and comparison with human studies, we illustrate that in real-world applications, the shift of samples toward an ideal value in LLMs' outputs can result in significantly biased decision-making, raising ethical concerns.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction</title>
<link>https://arxiv.org/abs/2402.12170</link>
<guid>https://arxiv.org/abs/2402.12170</guid>
<content:encoded><![CDATA[
arXiv:2402.12170v3 Announce Type: replace 
Abstract: Large language models require updates to remain up-to-date or adapt to new domains by fine-tuning them with new documents. One key is memorizing the latest information in a way that the memorized information is extractable with a query prompt. However, LLMs suffer from a phenomenon called perplexity curse; despite minimizing document perplexity during fine-tuning, LLMs struggle to extract information through a prompt sentence. In this new knowledge acquisition and extraction, we find a very intriguing fact that LLMs can accurately answer questions about the first sentence, but they struggle to extract information described in the middle or end of the documents used for fine-tuning. Our study suggests that the auto-regressive training causes this issue; each token is prompted by reliance on all previous tokens, which hinders the model from recalling information from training documents by question prompts. To conduct the in-depth study, we publish both synthetic and real datasets, enabling the evaluation of the QA performance w.r.t. the position of the corresponding answer in a document. Our investigation shows that even a large model suffers from the perplexity curse, but regularization such as denoising auto-regressive loss can enhance the information extraction from diverse positions. These findings will be (i) a key to improving knowledge extraction from LLMs and (ii) new elements to discuss the trade-off between RAG and fine-tuning in adapting LLMs to a new domain.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argumentative Large Language Models for Explainable and Contestable Claim Verification</title>
<link>https://arxiv.org/abs/2405.02079</link>
<guid>https://arxiv.org/abs/2405.02079</guid>
<content:encoded><![CDATA[
arXiv:2405.02079v3 Announce Type: replace 
Abstract: The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their inability to provide outputs which can be faithfully explained and effectively contested to correct mistakes. In this paper, we attempt to reconcile these strengths and weaknesses by introducing \emph{argumentative LLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning. Concretely, ArgLLMs construct argumentation frameworks, which then serve as the basis for formal reasoning in support of decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs' performance experimentally in comparison with state-of-the-art techniques, in the context of the decision-making task of claim verification. We also define novel properties to characterise contestability and assess ArgLLMs formally in terms of these properties.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations</title>
<link>https://arxiv.org/abs/2405.13828</link>
<guid>https://arxiv.org/abs/2405.13828</guid>
<content:encoded><![CDATA[
arXiv:2405.13828v2 Announce Type: replace 
Abstract: Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contributes to word learning efficiency in language models. We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Our experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters, and we highlight the significance of both trials and demonstrations. We further show that the teacher's choices of words influence students' word-specific learning efficiency, and a practice-makes-perfect effect is evident by a strong correlation between the frequency of words in trials and their respective learning curves. Our findings suggest that interactive language learning, with teacher demonstrations and active trials, can facilitate efficient word learning in language models.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is In-Context Learning Sufficient for Instruction Following in LLMs?</title>
<link>https://arxiv.org/abs/2405.19874</link>
<guid>https://arxiv.org/abs/2405.19874</guid>
<content:encoded><![CDATA[
arXiv:2405.19874v3 Announce Type: replace 
Abstract: In-context learning (ICL) allows LLMs to learn from examples without changing their weights: this is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on the established benchmark MT-Bench, especially with more capable base LLMs. We then uncover the most relevant elements for successful in-context alignment, finding the crucial role of the decoding parameters. Based on these insights, we show that the approach of URIAL can indeed be improved by adding high-quality, potentially carefully selected via greedy search, demonstrations in context, getting closer to the performance of instruct models. Finally, we provide the first, to our knowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for instruction following in the low data regime, where ICL can be a viable alternative to IFT. Overall, our work advances the understanding of ICL as an alignment technique and its relationship to IFT. We provide our code at https://github.com/tml-epfl/icl-alignment.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection</title>
<link>https://arxiv.org/abs/2406.11260</link>
<guid>https://arxiv.org/abs/2406.11260</guid>
<content:encoded><![CDATA[
arXiv:2406.11260v3 Announce Type: replace 
Abstract: The spread of fake news harms individuals and presents a critical social challenge that must be addressed. Although numerous algorithmic and insightful features have been developed to detect fake news, many of these features can be manipulated with style-conversion attacks, especially with the emergence of advanced language models, making it more difficult to differentiate from genuine news. This study proposes adversarial style augmentation, AdStyle, designed to train a fake news detector that remains robust against various style-conversion attacks. The primary mechanism involves the strategic use of LLMs to automatically generate a diverse and coherent array of style-conversion attack prompts, enhancing the generation of particularly challenging prompts for the detector. Experiments indicate that our augmentation strategy significantly improves robustness and detection performance when evaluated on fake news benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?</title>
<link>https://arxiv.org/abs/2406.12307</link>
<guid>https://arxiv.org/abs/2406.12307</guid>
<content:encoded><![CDATA[
arXiv:2406.12307v4 Announce Type: replace 
Abstract: Recent advancements in integrating large language models (LLMs) with tools have allowed the models to interact with real-world environments. However, these tool-augmented LLMs often encounter incomplete scenarios when users provide partial information or the necessary tools are unavailable. Recognizing and managing such scenarios is crucial for LLMs to ensure their reliability, but this exploration remains understudied. This study examines whether LLMs can identify incomplete conditions and appropriately determine when to refrain from using tools. To this end, we address a dataset by manipulating instances from two datasets by removing necessary tools or essential information for tool invocation. Our experiments show that LLMs often struggle to identify the absence of information required to utilize specific tools and recognize the absence of appropriate tools. We further analyze model behaviors in different environments and compare their performance against humans. Our research can contribute to advancing reliable LLMs by addressing common scenarios during interactions between humans and LLMs. Our code and dataset will be publicly available.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences of LLM Evaluators</title>
<link>https://arxiv.org/abs/2406.12319</link>
<guid>https://arxiv.org/abs/2406.12319</guid>
<content:encoded><![CDATA[
arXiv:2406.12319v4 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly used as evaluators for natural language generation tasks, ensuring unbiased assessments is essential. However, LLM evaluators often display biased preferences, such as favoring verbosity and authoritative tones. Our empirical analysis reveals that these biases are exacerbated in pairwise evaluation, where LLMs directly compare two outputs and easily prioritize superficial attributes. In contrast, pointwise evaluation, which assesses outputs independently, is less susceptible to such bias because each output is judged in isolation. To address the limitations of the pairwise evaluation, we introduce a novel evaluation method, PRePair, which integrates pointwise reasoning within a pairwise framework. PRePair effectively alleviates biased preference, improving performance on the adversarial benchmark (LLMBar) while outperforming pointwise evaluation on the standard benchmark (MT-Bench).
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Refusal Training in LLMs Generalize to the Past Tense?</title>
<link>https://arxiv.org/abs/2407.11969</link>
<guid>https://arxiv.org/abs/2407.11969</guid>
<content:encoded><![CDATA[
arXiv:2407.11969v4 Announce Type: replace 
Abstract: Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini, o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For example, the success rate of this simple attack on GPT-4o increases from 1% using direct requests to 88% using 20 past tense reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending against past reformulations is feasible when past tense examples are explicitly included in the fine-tuning data. Overall, our findings highlight that the widely used alignment techniques -- such as SFT, RLHF, and adversarial training -- employed to align the studied models can be brittle and do not always generalize as intended. We provide code and jailbreak artifacts at https://github.com/tml-epfl/llm-past-tense.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind</title>
<link>https://arxiv.org/abs/2408.12022</link>
<guid>https://arxiv.org/abs/2408.12022</guid>
<content:encoded><![CDATA[
arXiv:2408.12022v2 Announce Type: replace 
Abstract: How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic ``language-of-thought'' with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts</title>
<link>https://arxiv.org/abs/2409.11056</link>
<guid>https://arxiv.org/abs/2409.11056</guid>
<content:encoded><![CDATA[
arXiv:2409.11056v2 Announce Type: replace 
Abstract: With the advent of Large Language Models (LLMs), generating rule-based data for real-world applications has become more accessible. Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one. To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-to-MIP instances. Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing the Scope of Language Models</title>
<link>https://arxiv.org/abs/2410.21597</link>
<guid>https://arxiv.org/abs/2410.21597</guid>
<content:encoded><![CDATA[
arXiv:2410.21597v2 Announce Type: replace 
Abstract: We now deploy language models in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions about documentation or acting as coding assistants, but they require general language understanding. Under these circumstances these models should not be able to answer irrelevant requests such as, poetry generation or questions about physics, etc. Instead we would like language models to only answer to queries corresponding to desired behavior and refuse all other requests, which we refer to as scoping. We conduct a comprehensive empirical evaluation of potential methods from prompting to fine-tuning to preference learning to a recently proposed method for general alignment called Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that, when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping language models.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2411.18337</link>
<guid>https://arxiv.org/abs/2411.18337</guid>
<content:encoded><![CDATA[
arXiv:2411.18337v2 Announce Type: replace 
Abstract: Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model Uncertainty for Question Difficulty Estimation</title>
<link>https://arxiv.org/abs/2412.11831</link>
<guid>https://arxiv.org/abs/2412.11831</guid>
<content:encoded><![CDATA[
arXiv:2412.11831v2 Announce Type: replace 
Abstract: In an educational setting, an estimate of the difficulty of multiple-choice questions (MCQs), a commonly used strategy to assess learning progress, constitutes very useful information for both teachers and students. Since human assessment is costly from multiple points of view, automatic approaches to MCQ item difficulty estimation are investigated, yielding however mixed success until now. Our approach to this problem takes a different angle from previous work: asking various Large Language Models to tackle the questions included in three different MCQ datasets, we leverage model uncertainty to estimate item difficulty. By using both model uncertainty features as well as textual features in a Random Forest regressor, we show that uncertainty features contribute substantially to difficulty prediction, where difficulty is inversely proportional to the number of students who can correctly answer a question. In addition to showing the value of our approach, we also observe that our model achieves state-of-the-art results on the USMLE and CMCQRD publicly available datasets.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StaICC: Standardized Evaluation for Classification Task in In-context Learning</title>
<link>https://arxiv.org/abs/2501.15708</link>
<guid>https://arxiv.org/abs/2501.15708</guid>
<content:encoded><![CDATA[
arXiv:2501.15708v3 Announce Type: replace 
Abstract: Classification tasks are widely investigated in the In-Context Learning (ICL) paradigm. However, current efforts are evaluated on disjoint benchmarks and settings, while their performances are significantly influenced by some trivial variables, such as prompt templates, data sampling, instructions, etc., which leads to significant inconsistencies in the results reported across various literature, preventing fair comparison or meta-analysis across different papers. Therefore, this paper proposes a standardized and easy-to-use evaluation toolkit (StaICC) for in-context classification. Including, for the normal classification task, we provide StaICC-Normal, selecting 10 widely used datasets, and generating prompts with a fixed form, to mitigate the variance among the experiment implementations. To enrich the usage of our benchmark, we also provide a sub-benchmark StaICC-Diag for diagnosing ICL from several aspects, aiming for a more robust inference processing.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-MEM: Agentic Memory for LLM Agents</title>
<link>https://arxiv.org/abs/2502.12110</link>
<guid>https://arxiv.org/abs/2502.12110</guid>
<content:encoded><![CDATA[
arXiv:2502.12110v5 Announce Type: replace 
Abstract: While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/AgenticMemory, while the source code of agentic memory system is available at https://github.com/agiresearch/A-mem.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models</title>
<link>https://arxiv.org/abs/2502.14427</link>
<guid>https://arxiv.org/abs/2502.14427</guid>
<content:encoded><![CDATA[
arXiv:2502.14427v2 Announce Type: replace 
Abstract: Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a well-established UQ technique in classification tasks - for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores. Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks. Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2503.21729</link>
<guid>https://arxiv.org/abs/2503.21729</guid>
<content:encoded><![CDATA[
arXiv:2503.21729v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Layer-skipping in Pre-trained LLMs</title>
<link>https://arxiv.org/abs/2503.23798</link>
<guid>https://arxiv.org/abs/2503.23798</guid>
<content:encoded><![CDATA[
arXiv:2503.23798v2 Announce Type: replace 
Abstract: Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models</title>
<link>https://arxiv.org/abs/2504.05050</link>
<guid>https://arxiv.org/abs/2504.05050</guid>
<content:encoded><![CDATA[
arXiv:2504.05050v2 Announce Type: replace 
Abstract: Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible "dark patterns" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local "safety regions" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Token to Line: Enhancing Code Generation with a Long-Term Perspective</title>
<link>https://arxiv.org/abs/2504.07433</link>
<guid>https://arxiv.org/abs/2504.07433</guid>
<content:encoded><![CDATA[
arXiv:2504.07433v2 Announce Type: replace 
Abstract: The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can postgraduate translation students identify machine-generated text?</title>
<link>https://arxiv.org/abs/2504.09164</link>
<guid>https://arxiv.org/abs/2504.09164</guid>
<content:encoded><![CDATA[
arXiv:2504.09164v2 Announce Type: replace 
Abstract: Given the growing use of generative artificial intelligence as a tool for creating multilingual content and bypassing both machine and traditional translation methods, this study explores the ability of linguistically trained individuals to discern machine-generated output from human-written text (HT). After brief training sessions on the textual anomalies typically found in synthetic text (ST), twenty-three postgraduate translation students analysed excerpts of Italian prose and assigned likelihood scores to indicate whether they believed they were human-written or AI-generated (ChatGPT-4o). The results show that, on average, the students struggled to distinguish between HT and ST, with only two participants achieving notable accuracy. Closer analysis revealed that the students often identified the same textual anomalies in both HT and ST, although features such as low burstiness and self-contradiction were more frequently associated with ST. These findings suggest the need for improvements in the preparatory training. Moreover, the study raises questions about the necessity of editing synthetic text to make it sound more human-like and recommends further research to determine whether AI-generated text is already sufficiently natural-sounding not to require further refinement.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset</title>
<link>https://arxiv.org/abs/2504.09958</link>
<guid>https://arxiv.org/abs/2504.09958</guid>
<content:encoded><![CDATA[
arXiv:2504.09958v2 Announce Type: replace 
Abstract: Stance detection has become an essential tool for analyzing public discussions on social media. Current methods face significant challenges, particularly in Chinese language processing and multi-turn conversational analysis. To address these limitations, we introduce C-MTCSD, the largest Chinese multi-turn conversational stance detection dataset, comprising 24,264 carefully annotated instances from Sina Weibo, which is 4.2 times larger than the only prior Chinese conversational stance detection dataset. Our comprehensive evaluation using both traditional approaches and large language models reveals the complexity of C-MTCSD: even state-of-the-art models achieve only 64.07% F1 score in the challenging zero-shot setting, while performance consistently degrades with increasing conversation depth. Traditional models particularly struggle with implicit stance detection, achieving below 50% F1 score. This work establishes a challenging new benchmark for Chinese stance detection research, highlighting significant opportunities for future improvements.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination</title>
<link>https://arxiv.org/abs/2504.10020</link>
<guid>https://arxiv.org/abs/2504.10020</guid>
<content:encoded><![CDATA[
arXiv:2504.10020v2 Announce Type: replace 
Abstract: Contrastive decoding strategies are widely used to reduce hallucinations in multimodal large language models (MLLMs). These methods work by constructing contrastive samples to induce hallucinations and then suppressing them in the output distribution. However, this paper demonstrates that such approaches fail to effectively mitigate the hallucination problem. The performance improvements observed on POPE Benchmark are largely driven by two misleading factors: (1) crude, unidirectional adjustments to the model's output distribution and (2) the adaptive plausibility constraint, which reduces the sampling strategy to greedy search. To further illustrate these issues, we introduce a series of spurious improvement methods and evaluate their performance against contrastive decoding techniques. Experimental results reveal that the observed performance gains in contrastive decoding are entirely unrelated to its intended goal of mitigating hallucinations. Our findings challenge common assumptions about the effectiveness of contrastive decoding strategies and pave the way for developing genuinely effective solutions to hallucinations in MLLMs.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems</title>
<link>https://arxiv.org/abs/2405.19653</link>
<guid>https://arxiv.org/abs/2405.19653</guid>
<content:encoded><![CDATA[
arXiv:2405.19653v4 Announce Type: replace-cross 
Abstract: Surrogate models are used to predict the behavior of complex energy systems that are too expensive to simulate with traditional numerical methods. Our work introduces the use of language descriptions, which we call ``system captions'' or SysCaps, to interface with such surrogates. We argue that interacting with surrogates through text, particularly natural language, makes these models more accessible for both experts and non-experts. We introduce a lightweight multimodal text and timeseries regression model and a training pipeline that uses large language models (LLMs) to synthesize high-quality captions from simulation metadata. Our experiments on two real-world simulators of buildings and wind farms show that our SysCaps-augmented surrogates have better accuracy on held-out systems than traditional methods while enjoying new generalization abilities, such as handling semantically related descriptions of the same test system. Additional experiments also highlight the potential of SysCaps to unlock language-driven design space exploration and to regularize training through prompt augmentation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Select: Feature Selection with Large Language Models</title>
<link>https://arxiv.org/abs/2407.02694</link>
<guid>https://arxiv.org/abs/2407.02694</guid>
<content:encoded><![CDATA[
arXiv:2407.02694v2 Announce Type: replace-cross 
Abstract: In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the standard tools of data science. Remarkably, these models exhibit this capacity across various query mechanisms. For example, we zero-shot prompt an LLM to output a numerical importance score for a feature (e.g., "blood pressure") in predicting an outcome of interest (e.g., "heart failure"), with no additional context. In particular, we find that the latest models, such as GPT-4, can consistently identify the most predictive features regardless of the query mechanism and across various prompting strategies. We illustrate these findings through extensive experiments on real-world data, where we show that LLM-based feature selection consistently achieves strong performance competitive with data-driven methods such as the LASSO, despite never having looked at the downstream training data. Our findings suggest that LLMs may be useful not only for selecting the best features for training but also for deciding which features to collect in the first place. This could benefit practitioners in domains like healthcare and the social sciences, where collecting high-quality data comes at a high cost.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2407.07880</link>
<guid>https://arxiv.org/abs/2407.07880</guid>
<content:encoded><![CDATA[
arXiv:2407.07880v2 Announce Type: replace-cross 
Abstract: This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low-quality data points, and pairwise noise, which encompasses erroneous data pair associations that affect preference rankings. Utilizing Distributionally Robust Optimization (DRO), we enhance DPO's resilience to these types of noise. Our theoretical insights reveal that DPO inherently embeds DRO principles, conferring robustness to pointwise noise, with the regularization coefficient $\beta$ playing a critical role in its noise resistance. Extending this framework, we introduce Distributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing against worst-case pairwise scenarios. The novel hyperparameter $\beta'$ in Dr. DPO allows for fine-tuned control over data pair reliability, providing a strategic balance between exploration and exploitation in noisy training environments. Empirical evaluations demonstrate that Dr. DPO substantially improves the quality of generated text and response accuracy in preference datasets, showcasing enhanced performance in both noisy and noise-free settings. The code is available at https://github.com/junkangwu/Dr_DPO.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spin glass model of in-context learning</title>
<link>https://arxiv.org/abs/2408.02288</link>
<guid>https://arxiv.org/abs/2408.02288</guid>
<content:encoded><![CDATA[
arXiv:2408.02288v3 Announce Type: replace-cross 
Abstract: Large language models show a surprising in-context learning ability -- being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and further clarifies why an unseen function can be predicted by providing only a prompt yet without further training. Our theory reveals that for single-instance learning, increasing the task diversity leads to the emergence of in-context learning, by allowing the Boltzmann distribution to converge to a unique correct solution of weight parameters. Therefore the pre-trained transformer displays a prediction power in a novel prompt setting. The proposed analytically tractable model thus offers a promising avenue for thinking about how to interpret many intriguing but puzzling properties of large language models.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</title>
<link>https://arxiv.org/abs/2408.15545</link>
<guid>https://arxiv.org/abs/2408.15545</guid>
<content:encoded><![CDATA[
arXiv:2408.15545v5 Announce Type: replace-cross 
Abstract: Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.
  To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.cIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks.
  Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains. (2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning in less-represented scientific domains. (3) SciLitLLM achieves promising performance improvements on scientific literature understanding benchmarks.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections</title>
<link>https://arxiv.org/abs/2409.09045</link>
<guid>https://arxiv.org/abs/2409.09045</guid>
<content:encoded><![CDATA[
arXiv:2409.09045v2 Announce Type: replace-cross 
Abstract: "Synthetic samples" based on large language models (LLMs) have been argued to serve as efficient alternatives to surveys of humans, assuming that their training data includes information on human attitudes and behavior. However, LLM-synthetic samples might exhibit bias, for example due to training data and fine-tuning processes being unrepresentative of diverse contexts. Such biases risk reinforcing existing biases in research, policymaking, and society. Therefore, researchers need to investigate if and under which conditions LLM-generated synthetic samples can be used for public opinion prediction. In this study, we examine to what extent LLM-based predictions of individual public opinion exhibit context-dependent biases by predicting the results of the 2024 European Parliament elections. Prompting three LLMs with individual-level background information of 26,000 eligible European voters, we ask the LLMs to predict each person's voting behavior. By comparing them to the actual results, we show that LLM-based predictions of future voting behavior largely fail, their accuracy is unequally distributed across national and linguistic contexts, and they require detailed attitudinal information in the prompt. The findings emphasize the limited applicability of LLM-synthetic samples to public opinion prediction. In investigating their contextual biases, this study contributes to the understanding and mitigation of inequalities in the development of LLMs and their applications in computational social science.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</title>
<link>https://arxiv.org/abs/2410.09024</link>
<guid>https://arxiv.org/abs/2410.09024</guid>
<content:encoded><![CDATA[
arXiv:2410.09024v3 Announce Type: replace-cross 
Abstract: The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2410.21465</link>
<guid>https://arxiv.org/abs/2410.21465</guid>
<content:encoded><![CDATA[
arXiv:2410.21465v2 Announce Type: replace-cross 
Abstract: With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation</title>
<link>https://arxiv.org/abs/2411.16750</link>
<guid>https://arxiv.org/abs/2411.16750</guid>
<content:encoded><![CDATA[
arXiv:2411.16750v2 Announce Type: replace-cross 
Abstract: Traditional monocular depth estimation suffers from inherent ambiguity and visual nuisance. We argue that language prior can enhance monocular depth estimation by leveraging the inductive bias learned during the text-to-image pre-training of diffusion models. The ability of these models to generate images that align with text indicates that they have learned the spatial relationships, size, and shape of specified objects, which can be applied to improve depth estimation. Thus, we propose PriorDiffusion, using a pre-trained text-to-image diffusion model that takes both images and corresponding text descriptions to infer affine-invariant depth through a denoising process. We also show that language prior enhances the model's perception of specific regions of images that users care about and describe. Simultaneously, language prior acts as a constraint to accelerate the convergence of both training and the inference diffusion trajectory. By training on HyperSim and Virtual KITTI, we achieve faster training convergence, fewer inference diffusion steps, and state-of-the-art zero-shot performance across NYUv2, KITTI, ETH3D, and ScanNet. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant</title>
<link>https://arxiv.org/abs/2501.17176</link>
<guid>https://arxiv.org/abs/2501.17176</guid>
<content:encoded><![CDATA[
arXiv:2501.17176v2 Announce Type: replace-cross 
Abstract: The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs</title>
<link>https://arxiv.org/abs/2502.19413</link>
<guid>https://arxiv.org/abs/2502.19413</guid>
<content:encoded><![CDATA[
arXiv:2502.19413v2 Announce Type: replace-cross 
Abstract: Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We propose a new idea for the community to adopt: convert scholarly documents into knowledge preserving, but style agnostic representations we term Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95\%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psycholinguistic Analyses in Software Engineering Text: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2503.05992</link>
<guid>https://arxiv.org/abs/2503.05992</guid>
<content:encoded><![CDATA[
arXiv:2503.05992v2 Announce Type: replace-cross 
Abstract: Context: A deeper understanding of human factors in software engineering (SE) is essential for improving team collaboration, decision-making, and productivity. Communication channels like code reviews and chats provide insights into developers' psychological and emotional states. While large language models excel at text analysis, they often lack transparency and precision. Psycholinguistic tools like Linguistic Inquiry and Word Count (LIWC) offer clearer, interpretable insights into cognitive and emotional processes exhibited in text. Despite its wide use in SE research, no comprehensive review of LIWC's use has been conducted. Objective: We examine the importance of psycholinguistic tools, particularly LIWC, and provide a thorough analysis of its current and potential future applications in SE research. Methods: We conducted a systematic review of six prominent databases, identifying 43 SE-related papers using LIWC. Our analysis focuses on five research questions. Results: Our findings reveal a wide range of applications, including analyzing team communication to detect developer emotions and personality, developing ML models to predict deleted Stack Overflow posts, and more recently comparing AI-generated and human-written text. LIWC has been primarily used with data from project management platforms (e.g., GitHub) and Q&amp;A forums (e.g., Stack Overflow). Key BSE concepts include Communication, Organizational Climate, and Positive Psychology. 26 of 43 papers did not formally evaluate LIWC. Concerns were raised about some limitations, including difficulty handling SE-specific vocabulary. Conclusion: We highlight the potential of psycholinguistic tools and their limitations, and present new use cases for advancing the research of human factors in SE (e.g., bias in human-LLM conversations).
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocAgent: A Multi-Agent System for Automated Code Documentation Generation</title>
<link>https://arxiv.org/abs/2504.08725</link>
<guid>https://arxiv.org/abs/2504.08725</guid>
<content:encoded><![CDATA[
arXiv:2504.08725v2 Announce Type: replace-cross 
Abstract: High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Judging Bias in Large Reasoning Models: An Empirical Study</title>
<link>https://arxiv.org/abs/2504.09946</link>
<guid>https://arxiv.org/abs/2504.09946</guid>
<content:encoded><![CDATA[
arXiv:2504.09946v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel "superficial reflection bias" where phrases mimicking reasoning (e.g., "wait, let me think...") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\% in preference alignment datasets and 14\% in fact-related datasets, in-context learning that provides up to 27\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\% in preference datasets and 16\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>