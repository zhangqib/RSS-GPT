<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.SI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.SI</link>


<item>
<title>Amplifying Your Social Media Presence: Personalized Influential Content Generation with LLMs</title>
<link>https://arxiv.org/abs/2505.01698</link>
<guid>https://arxiv.org/abs/2505.01698</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social media, content generation, influence, network structure <br />
Summary: <br />
The paper explores the potential of Large Language Models (LLMs) in generating personalized influential content to enhance a user's presence on social media. It highlights the limitations of current content generation techniques in addressing real-world social media challenges. By incorporating network information into content prompts, the research aims to boost post influence by leveraging underlying network structures. Multiple content-centric and structure-aware prompts are designed and evaluated through empirical experiments across LLMs. The findings demonstrate the effectiveness of injecting network information into prompt for content generation, shedding light on strategies that can significantly improve post influence. The research provides insights on enhancing visibility and influence on social media through innovative content generation approaches. The code for the study is accessible on GitHub for further exploration and experimentation. <br /> <div>
arXiv:2505.01698v1 Announce Type: new 
Abstract: The remarkable advancements in Large Language Models (LLMs) have revolutionized the content generation process in social media, offering significant convenience in writing tasks. However, existing applications, such as sentence completion and fluency enhancement, do not fully address the complex challenges in real-world social media contexts. A prevalent goal among social media users is to increase the visibility and influence of their posts. This paper, therefore, delves into the compelling question: Can LLMs generate personalized influential content to amplify a user's presence on social media? We begin by examining prevalent techniques in content generation to assess their impact on post influence. Acknowledging the critical impact of underlying network structures in social media, which are instrumental in initiating content cascades and highly related to the influence/popularity of a post, we then inject network information into prompt for content generation to boost the post's influence. We design multiple content-centric and structure-aware prompts. The empirical experiments across LLMs validate their ability in improving the influence and draw insights on which strategies are more effective. Our code is available at https://github.com/YuyingZhao/LLM-influence-amplifier.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDTok: A Dataset for Eating Disorder Content on TikTok</title>
<link>https://arxiv.org/abs/2505.02250</link>
<guid>https://arxiv.org/abs/2505.02250</guid>
<content:encoded><![CDATA[
<div> Keywords: eating disorders, TikTok, social media, digital health, mental health

Summary:
Eating disorders, such as anorexia nervosa and bulimia nervosa, have seen an increase during the COVID-19 pandemic, potentially exacerbated by exposure to idealized body images online. TikTok, a popular platform with a large adolescent user base, has become a notable space for the sharing of eating disorder content, raising concerns about its impact on vulnerable populations. A dataset of 43,040 TikTok videos related to eating disorders collected from January 2019 to June 2024 offers insights into content spread, moderation, user engagement, and the pandemic's influence on eating disorder trends. This dataset fills research gaps and can inform strategies to reduce the risks associated with harmful content. It contributes valuable insights to the study of digital health and the role of social media in shaping mental health. <div>
arXiv:2505.02250v1 Announce Type: new 
Abstract: Eating disorders, which include anorexia nervosa and bulimia nervosa, have been exacerbated by the COVID-19 pandemic, with increased diagnoses linked to heightened exposure to idealized body images online. TikTok, a platform with over a billion predominantly adolescent users, has become a key space where eating disorder content is shared, raising concerns about its impact on vulnerable populations. In response, we present a curated dataset of 43,040 TikTok videos, collected using keywords and hashtags related to eating disorders. Spanning from January 2019 to June 2024, this dataset, offers a comprehensive view of eating disorder-related content on TikTok. Our dataset has the potential to address significant research gaps, enabling analysis of content spread and moderation, user engagement, and the pandemic's influence on eating disorder trends. This work aims to inform strategies for mitigating risks associated with harmful content, contributing valuable insights to the study of digital health and social media's role in shaping mental health.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A longitudinal analysis of misinformation, polarization and toxicity on Bluesky after its public launch</title>
<link>https://arxiv.org/abs/2505.02317</link>
<guid>https://arxiv.org/abs/2505.02317</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, decentralized platform, user activity, political leanings, moderation efforts

Summary: 
Bluesky is a decentralized social media platform similar to Twitter that recently opened to the public, leading to a surge in user activity. Analysis of user behavior revealed a balanced distribution of original and reshared content, with low toxicity levels on the platform. Most Bluesky users lean left politically and share content from reliable sources. The influx of new users after the public launch, particularly those posting in English and Japanese, increased platform activity, but some accounts exhibited suspicious behavior and were flagged for spam or suspended, indicating effective moderation efforts. The study also highlighted misinformation dynamics and engagement in harmful conversations, showing that Bluesky maintains a relatively positive and credible environment for social interactions. 

<br /><br />Summary: <div>
arXiv:2505.02317v1 Announce Type: new 
Abstract: Bluesky is a decentralized, Twitter-like social media platform that has rapidly gained popularity. Following an invite-only phase, it officially opened to the public on February 6th, 2024, leading to a significant expansion of its user base. In this paper, we present a longitudinal analysis of user activity in the two months surrounding its public launch, examining how the platform evolved due to this rapid growth. Our analysis reveals that Bluesky exhibits an activity distribution comparable to more established social platforms, yet it features a higher volume of original content relative to reshared posts and maintains low toxicity levels. We further investigate the political leanings of its user base, misinformation dynamics, and engagement in harmful conversations. Our findings indicate that Bluesky users predominantly lean left politically and tend to share high-credibility sources. After the platform's public launch, an influx of new users, particularly those posting in English and Japanese, contributed to a surge in activity. Among them, several accounts displayed suspicious behaviors, such as mass-following users and sharing content from low-credibility news sources. Some of these accounts have already been flagged as spam or suspended, suggesting that Bluesky's moderation efforts have been effective.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Correction on Social Media: A Quantitative Analysis of Comment Behaviour and Reliability</title>
<link>https://arxiv.org/abs/2505.02343</link>
<guid>https://arxiv.org/abs/2505.02343</guid>
<content:encoded><![CDATA[
<div> Keywords: social correction, social media, online experiment, credibility evaluations, commenting behavior 

Summary: 
An online experiment focused on the phenomenon of Social Correction, examining how users' credibility evaluations and confidence, combined with online reputational concerns, influence their commenting behavior on social media posts. Results showed that users tended to be more cautious and conservative when giving disputing comments compared to endorsing ones. However, participants were more discerning and critical in their disputing comments, highlighting a cautious approach towards correcting misinformation. These findings contribute to understanding the dynamics of social correction on social media, shedding light on the factors that influence users' commenting behavior and the reliability of their comments. The study underscores the importance of considering the credibility evaluations of social media users and the impact of online reputational concerns in the context of combating misinformation. 

<br /><br />Summary: <div>
arXiv:2505.02343v1 Announce Type: new 
Abstract: Corrections given by ordinary social media users, also referred to as Social Correction have emerged as a viable intervention against misinformation as per the recent literature. However, little is known about how often users give disputing or endorsing comments and how reliable those comments are. An online experiment was conducted to investigate how users' credibility evaluations of social media posts and their confidence in those evaluations combined with online reputational concerns affect their commenting behaviour. The study found that participants exhibited a more conservative approach when giving disputing comments compared to endorsing ones. Nevertheless, participants were more discerning in their disputing comments than endorsing ones. These findings contribute to a better understanding of social correction on social media and highlight the factors influencing comment behaviour and reliability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dyGRASS: Dynamic Spectral Graph Sparsification via Localized Random Walks on GPUs</title>
<link>https://arxiv.org/abs/2505.02741</link>
<guid>https://arxiv.org/abs/2505.02741</guid>
<content:encoded><![CDATA[
<div> algorithm, spectral sparsification, dynamic graphs, random walk, GPU-based

Summary:
The work introduces dyGRASS, a dynamic algorithm for spectral sparsification of large undirected graphs with streaming edge updates. It utilizes random-walk-based methods to estimate node-to-node distances efficiently in both the original graph and its sparsifier for incremental and decremental updates. dyGRASS identifies spectrally critical edges among updates for capturing structural changes and recovers important edges during deletions. The algorithm leverages GPU-based non-backtracking random walks for parallel operation, enhancing performance and scalability. Experimental results demonstrate a 10x speedup over the state-of-the-art algorithm inGRASS, eliminating setup overhead and improving solution quality. dyGRASS excels in fully dynamic graph sparsification, accommodating both edge inserts and deletes across diverse graph instances from various domains like integrated circuits, finite element analysis, and social networks. <br /><br />Summary: <div>
arXiv:2505.02741v1 Announce Type: new 
Abstract: This work presents dyGRASS, an efficient dynamic algorithm for spectral sparsification of large undirected graphs that undergo streaming edge insertions and deletions. At its core, dyGRASS employs a random-walk-based method to efficiently estimate node-to-node distances in both the original graph (for decremental update) and its sparsifier (for incremental update). For incremental updates, dyGRASS enables the identification of spectrally critical edges among the updates to capture the latest structural changes. For decremental updates, dyGRASS facilitates the recovery of important edges from the original graph back into the sparsifier. To further enhance computational efficiency, dyGRASS employs a GPU-based non-backtracking random walk scheme that allows multiple walkers to operate simultaneously across various target updates. This parallelization significantly improves both the performance and scalability of the proposed dyGRASS framework. Our comprehensive experimental evaluations reveal that dyGRASS achieves approximately a 10x speedup compared to the state-of-the-art incremental sparsification (inGRASS) algorithm while eliminating the setup overhead and improving solution quality in incremental spectral sparsification tasks. Moreover, dyGRASS delivers high efficiency and superior solution quality for fully dynamic graph sparsification, accommodating both edge insertions and deletions across a diverse range of graph instances originating from integrated circuit simulations, finite element analysis, and social networks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Governance (HAIG): A Trust-Utility Approach</title>
<link>https://arxiv.org/abs/2505.01651</link>
<guid>https://arxiv.org/abs/2505.01651</guid>
<content:encoded><![CDATA[
<div> Trust dynamics, HAIG framework, evolving relationships, AI systems, human-AI<br />
<br />
Summary: This paper introduces the HAIG framework to analyze trust dynamics in evolving human-AI relationships. It addresses the limitations of current categorical frameworks in capturing the evolving nature of AI systems from tools to partners. The HAIG framework operates on three levels: dimensions, continua, and thresholds, focusing on maintaining appropriate trust relationships while maximizing utility and ensuring safeguards. It takes a trust-utility orientation rather than risk-based or principle-based approaches. The analysis highlights how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution in various contexts. Case studies in healthcare and European regulation demonstrate the framework's effectiveness in complementing existing models and anticipating governance challenges. <div>
arXiv:2505.01651v1 Announce Type: cross 
Abstract: This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., "human-in-the-loop" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning</title>
<link>https://arxiv.org/abs/2505.02027</link>
<guid>https://arxiv.org/abs/2505.02027</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph In-Context Learning, Prompt Generation, Prompt Selection, Prompt Augmentation, Pre-trained Models

Summary: 
Graph In-Context Learning has gained attention for adapting pre-trained graph models to new graphs without updating parameters. Existing methods use random prompts, leading to noise and lower performance. GraphPrompter introduces a multi-stage adaptive prompt optimization approach, enhancing in-context learning. The Prompt Generator highlights informative edges for prompt construction, reducing noise. The Prompt Selector dynamically selects relevant prompts using a $k$-nearest neighbors algorithm. The Prompt Augmenter enhances model generalization with a cache replacement strategy. GraphPrompter outperforms baselines by over 8% on average. The code is available at https://github.com/karin0018/GraphPrompter. 

<br /><br />Summary: <div>
arXiv:2505.02027v1 Announce Type: cross 
Abstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Democratic Federation: Fair Governance of Large-Scale, Decentralized, Sovereign Digital Communities</title>
<link>https://arxiv.org/abs/2505.02208</link>
<guid>https://arxiv.org/abs/2505.02208</guid>
<content:encoded><![CDATA[
<div> Grassroots Democratic Federation, large-scale digital communities, fair democratic governance, sortition, federation<br />
Summary:<br />
The article discusses the concept of Grassroots Democratic Federation for large-scale digital communities, aiming to achieve egalitarian formation and fair democratic governance. The federation evolves through grassroots formation and consensual federation of digital communities based on various criteria. Small communities govern themselves, while larger ones are governed by assemblies elected by sortition. The article focuses on the dynamic evolution of the federation, adapting fairness conditions to this setting. It emphasizes fair representation and participation, ensuring these conditions hold as the federation grows. A protocol is presented to meet these fairness requirements, aiming to stabilize the federation structure over time. The approach addresses the dynamic nature of the federation, striving for inclusive and democratic governance of digital communities. <br />Summary: <div>
arXiv:2505.02208v1 Announce Type: cross 
Abstract: Grassroots Democratic Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots democratic federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, or causes. Small communities (say up to 100 members) govern themselves; larger communities -- no matter how large -- are governed by a small assembly elected by sortition among its members. Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.
  In practice, we expect a grassroots democratic federation to grow and evolve dynamically and in all directions -- bottom-up, top-down, and middle-out. To address that, we formally specify this dynamic setting and adapt the static fairness conditions to it: The ex post condition on the fair representation of a child community becomes a condition that must always hold; the ex ante conditions in expectation on the fair participation of an individual and on the fair representation of a child community become conditions satisfied in actuality in the limit, provided the federation structure eventually stabilizes. We then present a protocol that satisfies these fairness conditions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Federated Graph Learning: A Data Condensation Perspective</title>
<link>https://arxiv.org/abs/2505.02573</link>
<guid>https://arxiv.org/abs/2505.02573</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, federated graph learning, condensed graph, FedGM, communication efficiency<br />
<br />
Summary:
The article introduces FedGM, a novel approach for federated graph learning that addresses data heterogeneity and privacy risks. It utilizes condensed graphs to aggregate knowledge from distributed graphs, reducing communication costs and privacy risks. Experiment results on six datasets demonstrate FedGM's superiority over existing methods, highlighting its potential as a new FGL paradigm. <div>
arXiv:2505.02573v1 Announce Type: cross 
Abstract: Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client graphs.However, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference and Visualization of Community Structure in Attributed Hypergraphs Using Mixed-Membership Stochastic Block Models</title>
<link>https://arxiv.org/abs/2401.00688</link>
<guid>https://arxiv.org/abs/2401.00688</guid>
<content:encoded><![CDATA[
<div> Hypergraphs, community structure, mixed-membership stochastic block models, node attributes, dimensionality reduction <br />
Summary: 
The study proposes the HyperNEO framework, combining mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods to infer community structure. This approach aims to simplify the visualization and interpretation of community structure in hypergraphs by generating a node layout that preserves node community memberships. Testing on synthetic and empirical hypergraphs with node attributes, the framework shows promise in broadening the exploration of higher-order community structure in complex systems. <div>
arXiv:2401.00688v2 Announce Type: replace 
Abstract: Hypergraphs represent complex systems involving interactions among more than two entities and allow the investigation of higher-order structure and dynamics in complex systems. Node attribute data, which often accompanies network data, can enhance the inference of community structure in complex systems. While mixed-membership stochastic block models have been employed to infer community structure in hypergraphs, they complicate the visualization and interpretation of inferred community structure by assuming that nodes may possess soft community memberships. In this study, we propose a framework, HyperNEO, that combines mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods. Our approach generates a node layout that largely preserves the community memberships of nodes. We evaluate our framework on both synthetic and empirical hypergraphs with node attributes. We expect our framework will broaden the investigation and understanding of higher-order community structure in complex systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easy-access online social media metrics can foster the identification of misinformation sharing users</title>
<link>https://arxiv.org/abs/2408.15186</link>
<guid>https://arxiv.org/abs/2408.15186</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, social media, user identification, factuality, online metrics <br />
Summary: <br />
Researchers have long studied the challenge of misinformation, but identifying primary sharers is difficult and time-consuming. This study proposes a low-barrier method to differentiate social media users likely to share misinformation by analyzing easily accessible online metrics. The research suggests that high tweet frequency and newer account age are associated with sharing low factuality content. Additionally, the number of accounts followed and the number of tweets produced may impact the spread of misinformation, depending on the user's follower count. By utilizing these simple social network metrics, platforms like Twitter can effectively identify users who are prone to spreading misinformation, aiding in combating the issue on social media. <div>
arXiv:2408.15186v2 Announce Type: replace 
Abstract: Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is time-consuming and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drilling into Erasmus learning mobility flows between countries 2014-2024</title>
<link>https://arxiv.org/abs/2505.00889</link>
<guid>https://arxiv.org/abs/2505.00889</guid>
<content:encoded><![CDATA[
<div> Keywords: Erasmus, mobility network, weighted networks, visualization, clustering

Summary:
The study focuses on analyzing the Erasmus mobility network, highlighting typical issues and methods in examining weighted networks. Various alternative exploratory perspectives are proposed for the dense network of 35 countries with a wide range of visit weights. Transformation techniques are employed to address the vast weight range. Skeleton reduction methods reveal Spain as a key node in the network, along with dominant roles of Germany, France, and Italy. Matrix representations unveil block patterns showcasing clustering of countries into developed and less developed clusters. Balassa normalization matrices indicate deviations from expected visit patterns, with certain clusters exceeding or falling below expectations. Overall, the study offers insights into network structure, highlighting key players and patterns of mobility flow within the Erasmus network.<br /><br />Summary: <div>
arXiv:2505.00889v1 Announce Type: new 
Abstract: Analyzing the Erasmus mobility network, we illustrate typical problems and approaches in analyzing weighted networks. We propose alternative exploratory views on the network "Erasmus+ learning mobility flows since 2014". The network has 35 nodes (countries), is very dense, and the range of link weights (number of visits) is huge (from 1 to 217003). An increasing transformation is used to reduce the range. The traditional graph-based visualization is unreadable. To gain insight into the structure of a dense network, it can be reduced to a skeleton by removing less essential links and/or nodes. We have determined the 1-neighbors and 2-neighbors subnetworks. The 1-neighbors skeleton highlights Spain as the main attractor in the network. The 2-neighbors skeleton shows the dominant role of Spain, Germany, France, and Italy. The hubs and authorities, Pathfinder and Ps cores methods confirm these observations.
  Using the "right" order of the nodes in a matrix representation can reveal the network structure as block patterns in the displayed matrix. The clustering of network nodes based on corrected Salton dissimilarity again shows the dominant role of Spain, Germany, France, and Italy, but also two main clusters of the division into developed/less developed countries. The Balassa normalization (log(measured/expected) visits) matrix shows that most visits within the two main clusters are above expected, while most visits between them are below expected; within the clusters of Balkan countries, Baltic countries, {SK, CZ, HU}, {IS, DK, NO} visits are much above expected, etc.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a format for describing networks / 1. Networks and knowledge graphs</title>
<link>https://arxiv.org/abs/2505.00912</link>
<guid>https://arxiv.org/abs/2505.00912</guid>
<content:encoded><![CDATA[
<div> Keywords: network, knowledge graph, RDF, Semantic Web, network analysis<br />
Summary:<br />
The article explores the relationship between networks and knowledge graphs, identifying knowledge graphs as a specialized form of network. It discusses how a knowledge graph can be transformed into various networks and subject to network analysis procedures. RDF is highlighted as a formalization of the knowledge graph idea within the context of the Semantic Web, with potential applicability to general network descriptions. The discussion underscores the interchangeability of concepts between knowledge graphs and networks, emphasizing the utility of knowledge graphs in generating diverse network structures. The article suggests that analysis techniques developed for knowledge graphs can be extended to network analysis, indicating a cross-pollination of methodologies in these domains. <div>
arXiv:2505.00912v1 Announce Type: new 
Abstract: The relationship between the concepts of network and knowledge graph is explored. A knowledge graph can be considered a special type of network. When using a knowledge graph, various networks can be obtained from it, and network analysis procedures can be applied to them. RDF is a formalization of the knowledge graph concept for the Semantic Web, but some of its solutions are also extensible to a format for describing general networks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a format for describing networks / 2. Format elements</title>
<link>https://arxiv.org/abs/2505.00921</link>
<guid>https://arxiv.org/abs/2505.00921</guid>
<content:encoded><![CDATA[
<div> networks, common format, key elements, describing, discussed <br />
Summary: 
This article delves into the essential components that a standardized format for describing networks should encompass. The discussion highlights the significance of establishing a common structure to accurately convey network information. Key elements identified for inclusion in such a format involve comprehensive descriptions of network configurations and characteristics. The necessity of incorporating specific details regarding network components, connections, and functionalities is emphasized to enhance the clarity and utility of network descriptions. The article underscores the importance of standardizing the language and terminology used to define network attributes to facilitate accurate communication and understanding among stakeholders. In conclusion, the article advocates for a systematic approach towards developing a universal format that can effectively capture the complexities and nuances of diverse network systems. <div>
arXiv:2505.00921v1 Announce Type: new 
Abstract: The key elements that a common format for describing networks should include are discussed.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Persistent Homology Distinguishes Simple and Complex Contagions with High Accuracy</title>
<link>https://arxiv.org/abs/2505.00958</link>
<guid>https://arxiv.org/abs/2505.00958</guid>
<content:encoded><![CDATA[
<div> classification, regression, extended persistent homology, simple contagion, complex contagion

Summary: The study explores distinguishing simple and complex contagions using extended persistent homology (EPH) in the context of network dynamics. Traditional methods struggle due to confounding factors and individual heterogeneity. EPH, applied to simulated contagion dynamics on real-world networks, effectively differentiates between simple and complex contagion processes and predicts their parameters. The models exhibit high predictive performance across various contagion parameters, even with noise and partial observability. EPH captures the influence of cycles of different lengths on contagion dynamics, providing a valuable metric for model classification and parameter prediction. The findings suggest that topological data analysis tools can aid in solving network optimization problems like seeding and vaccination strategies, as well as network inference and reconstruction challenges. <div>
arXiv:2505.00958v1 Announce Type: new 
Abstract: The social contagion literature makes a distinction between simple (independent cascade or bond percolation processes that pass infections through edges) and complex contagions (bootstrap percolation or threshold processes that require local reinforcement to spread). However, distinguishing simple and complex contagions using observational data poses a significant challenge in practice. Estimating population-level activation functions from observed contagion dynamics is hindered by confounding factors that influence adoptions (other than neighborhood interactions), as well as heterogeneity in individual behaviors and modeling variations that make it difficult to design appropriate null models for inferring contagion types. Here, we show that a new tool from topological data analysis (TDA), called extended persistent homology (EPH), when applied to contagion processes over networks, can effectively detect simple and complex contagion processes, as well as predict their parameters. We train classification and regression models using EPH-based topological summaries computed on simulated simple and complex contagion dynamics on three real-world network datasets and obtain high predictive performance over a wide range of contagion parameters and under a variety of informational constraints, including uncertainty in model parameters, noise, and partial observability of contagion dynamics. EPH captures the role of cycles of varying lengths in the observed contagion dynamics and offers a useful metric to classify contagion models and predict their parameters. Analyzing geometrical features of network contagion using TDA tools such as EPH can find applications in other network problems such as seeding, vaccination, and quarantine optimization, as well as network inference and reconstruction problems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-moderation in the decentralized era: decoding blocking behavior on Bluesky</title>
<link>https://arxiv.org/abs/2505.01174</link>
<guid>https://arxiv.org/abs/2505.01174</guid>
<content:encoded><![CDATA[
<div> Keywords: moderation, blocking behavior, decentralized networks, online communities, user blocking

Summary:
This study focuses on self-moderation through blocking behavior on the decentralized social networking platform Bluesky. By analyzing user activity over three months, the research aims to understand the connection between online behavior and the likelihood of being blocked. The study defines user profiles based on various features related to user activity, content characteristics, and network interactions. The research addresses two primary questions: whether users' blocking likelihood can be predicted from their behavior, and which behavioral features are linked to a higher chance of being blocked. The findings provide valuable insights into moderation on decentralized social networks and offer a robust analytical framework for future research in this area.<br /><br />Summary: Keywords: moderation, blocking behavior, decentralized networks, online communities, user blocking <div>
arXiv:2505.01174v1 Announce Type: new 
Abstract: Moderation and blocking behavior, both closely related to the mitigation of abuse and misinformation on social platforms, are fundamental mechanisms for maintaining healthy online communities. However, while centralized platforms typically employ top-down moderation, decentralized networks rely on users to self-regulate through mechanisms like blocking actions to safeguard their online experience. Given the novelty of the decentralized paradigm, addressing self-moderation is critical for understanding how community safety and user autonomy can be effectively balanced. This study examines user blocking on Bluesky, a decentralized social networking platform, providing a comprehensive analysis of over three months of user activity through the lens of blocking behaviour. We define profiles based on 86 features that describe user activity, content characteristics, and network interactions, addressing two primary questions: (1) Is the likelihood of a user being blocked inferable from their online behavior? and (2) What behavioral features are associated with an increased likelihood of being blocked? Our findings offer valuable insights and contribute with a robust analytical framework to advance research in moderation on decentralized social networks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell me who its founders are and I'll tell you what your online community looks like: Online community founders' personality and community attributes</title>
<link>https://arxiv.org/abs/2505.01219</link>
<guid>https://arxiv.org/abs/2505.01219</guid>
<content:encoded><![CDATA[
<div> personality traits, online communities, founders, sustainability, engagement

Summary:<br />
This study focuses on the personality traits of online community founders and their impact on community sustainability and attributes. By analyzing the Big Five personality traits of 35,164 founders in 8,625 Reddit communities, the study finds that founder traits play a significant role in determining community engagement, social network structure, and founder activity within the community. The research highlights the importance of considering behavioral and psychological aspects of community members and leaders in understanding and predicting community outcomes. The findings suggest that founder traits can serve as predictors of community success and offer valuable insights into the factors that contribute to the growth and longevity of online communities. <div>
arXiv:2505.01219v1 Announce Type: new 
Abstract: Online communities are an increasingly important stakeholder for firms, and despite the growing body of research on them, much remains to be learned about them and about the factors that determine their attributes and sustainability. Whereas most of the literature focuses on predictors such as community activity, network structure, and platform interface, there is little research about behavioral and psychological aspects of community members and leaders. In the present study we focus on the personality traits of community founders as predictors of community attributes and sustainability. We develop a tool to estimate community members' Big Five personality traits from their social media text and use it to estimate the traits of 35,164 founders in 8,625 Reddit communities. We find support for most of our predictions about the relationships between founder traits and community sustainability and attributes, including the level of engagement within the community, aspects of its social network structure, and whether the founders themselves remain active in it.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMSAT: A Multimodal Acoustic Dataset and Deep Contrastive Learning Framework for Affective and Physiological Modeling of Spiritual Meditation</title>
<link>https://arxiv.org/abs/2505.00839</link>
<guid>https://arxiv.org/abs/2505.00839</guid>
<content:encoded><![CDATA[
<div> Keywords: auditory stimuli, affective computing, biometric signals, deep learning, stress monitoring <br />
Summary: 
This study examines the emotional and physiological effects of spiritual meditation, music, and natural silence on individuals using a new dataset called SMSAT. The researchers developed a deep learning model to extract features from the acoustic time series data, achieving high accuracy in classifying affective states. They also introduced the Calmness Analysis Model (CAM), a deep learning framework that combines handcrafted and learned features to classify affective states with 99.99% accuracy. The study found significant differences in cardiac response characteristics among the different auditory conditions, with spiritual meditation inducing the most pronounced physiological fluctuations. The proposed models outperformed existing methods in affective state classification tasks, indicating potential applications in stress monitoring, mental well-being, and therapeutic audio-based interventions. <br /><br />Summary: <div>
arXiv:2505.00839v1 Announce Type: cross 
Abstract: Understanding how auditory stimuli influence emotional and physiological states is fundamental to advancing affective computing and mental health technologies. In this paper, we present a multimodal evaluation of the affective and physiological impacts of three auditory conditions, that is, spiritual meditation (SM), music (M), and natural silence (NS), using a comprehensive suite of biometric signal measures. To facilitate this analysis, we introduce the Spiritual, Music, Silence Acoustic Time Series (SMSAT) dataset, a novel benchmark comprising acoustic time series (ATS) signals recorded under controlled exposure protocols, with careful attention to demographic diversity and experimental consistency. To model the auditory induced states, we develop a contrastive learning based SMSAT audio encoder that extracts highly discriminative embeddings from ATS data, achieving 99.99% classification accuracy in interclass and intraclass evaluations. Furthermore, we propose the Calmness Analysis Model (CAM), a deep learning framework integrating 25 handcrafted and learned features for affective state classification across auditory conditions, attaining robust 99.99% classification accuracy. In contrast, pairwise t tests reveal significant deviations in cardiac response characteristics (CRC) between SM analysis via ANOVA inducing more significant physiological fluctuations. Compared to existing state of the art methods reporting accuracies up to 90%, the proposed model demonstrates substantial performance gains (up to 99%). This work contributes a validated multimodal dataset and a scalable deep learning framework for affective computing applications in stress monitoring, mental well-being, and therapeutic audio-based interventions.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Data-centric Directed Graph Learning: An Entropy-driven Approach</title>
<link>https://arxiv.org/abs/2505.00983</link>
<guid>https://arxiv.org/abs/2505.00983</guid>
<content:encoded><![CDATA[
<div> Keywords: DiGraph Neural Networks, knowledge distillation, hierarchical encoding theory, topology, graph datasets

Summary: 
The paper introduces EDEN, a novel approach for data-centric learning in directed graphs. EDEN leverages hierarchical knowledge trees constructed from directed structural measurements to refine knowledge flow and enhance data-centric knowledge distillation during model training. By quantifying mutual information between node profiles, EDEN significantly improves the predictive performance of (Di)Graph Neural Networks across various graph datasets and downstream tasks. The proposed framework not only achieves state-of-the-art results but also demonstrates strong enhancements for existing (Di)GNN models. This approach paves the way for a deeper exploration of correlations between directed edges and node profiles in complex topology systems, highlighting the importance of data-centric perspectives for enhancing model-centric neural networks.

<br /><br />Summary: <div>
arXiv:2505.00983v1 Announce Type: cross 
Abstract: The directed graph (digraph), as a generalization of undirected graphs, exhibits superior representation capability in modeling complex topology systems and has garnered considerable attention in recent years. Despite the notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage directed edges, they still fail to comprehensively delve into the abundant data knowledge concealed in the digraphs. This data-level limitation results in model-level sub-optimal predictive performance and underscores the necessity of further exploring the potential correlations between the directed edges (topology) and node profiles (feature and labels) from a data-centric perspective, thereby empowering model-centric neural networks with stronger encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a data-centric digraph learning paradigm or a model-agnostic hot-and-plug data-centric Knowledge Distillation (KD) module. The core idea is to achieve data-centric ML, guided by our proposed hierarchical encoding theory for structured data. Specifically, EDEN first utilizes directed structural measurements from a topology perspective to construct a coarse-grained Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual information of node profiles to refine knowledge flow in the HKT, enabling data-centric KD supervision within model training. As a general framework, EDEN can also naturally extend to undirected scenarios and demonstrate satisfactory performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph datasets (homophily and heterophily) and across 4 downstream tasks. The results demonstrate that EDEN attains SOTA performance and exhibits strong improvement for prevalent (Di)GNNs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering complementary information sharing in spider monkey collective foraging using higher-order spatial networks</title>
<link>https://arxiv.org/abs/2505.01167</link>
<guid>https://arxiv.org/abs/2505.01167</guid>
<content:encoded><![CDATA[
<div> Collectives, distributed processing, fission-fusion dynamics, foraging information, simplicial complexes <br />
Summary:<br />
The study focuses on how collectives can process information in a distributed manner through fission-fusion dynamics. By analyzing the overlaps between individual core ranges that represent seasonal knowledge, the research identifies sets of individuals with balanced overlap between redundantly and uniquely known areas. Using simplicial complexes, higher-order interactions are represented, revealing complementarity in shared foraging information. The complex spatial networks from fission-fusion dynamics enable adaptive collective processing of foraging information in dynamic environments. <div>
arXiv:2505.01167v1 Announce Type: cross 
Abstract: Collectives are often able to process information in a distributed fashion, surpassing each individual member's processing capacity. In fission-fusion dynamics, where group members come together and split from others often, sharing complementary information about uniquely known foraging areas could allow a group to track a heterogenous foraging environment better than any group member on its own. We analyse the partial overlaps between individual core ranges, which we assume represent the knowledge of an individual during a given season. We identify sets of individuals whose overlap shows a balance between redundantly and uniquely known portions and we use simplicial complexes to represent these higher-order interactions. The structure of the simplicial complexes shows holes in various dimensions, revealing complementarity in the foraging information that is being shared. We propose that the complex spatial networks arising from fission-fusion dynamics allow for adaptive, collective processing of foraging information in dynamic environments.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversation-Based Multimodal Abuse Detection Through Text and Graph Embeddings</title>
<link>https://arxiv.org/abs/2503.12994</link>
<guid>https://arxiv.org/abs/2503.12994</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, abuse detection, representation learning, textual content, conversational graphs

Summary:
The article addresses the common issue of abusive behavior on online social networks and proposes a novel approach using representation learning methods to generate embeddings of both textual content and conversational graphs. Two methods are proposed to learn whole-graph representations using edge directions, weights, signs, and vertex attributes. The study experiments with various textual and graph embedding methods on a dataset annotated for abuse detection, achieving high F-measure scores of 81.02 using text alone and 80.61 using graphs alone. Combining both modalities through fusion strategies significantly improves abuse detection performance, increasing the F-measure to 87.06. The study also identifies specific engineered features captured by the embedding methods, shedding light on the discriminative information considered by the representation learning methods. <div>
arXiv:2503.12994v2 Announce Type: replace 
Abstract: Abusive behavior is common on online social networks, and forces the hosts of such platforms to find new solutions to address this problem. Various methods have been proposed to automate this task in the past decade. Most of them rely on the exchanged content, but ignore the structure and dynamics of the conversation, which could provide some relevant information. In this article, we propose to use representation learning methods to automatically produce embeddings of this textual content and of the conversational graphs depicting message exchanges. While the latter could be enhanced by including additional information on top of the raw conversational structure, no method currently exists to learn wholegraph representations using simultaneously edge directions, weights, signs, and vertex attributes. We propose two such methods to fill this gap in the literature. We experiment with 5 textual and 13 graph embedding methods, and apply them to a dataset of online messages annotated for abuse detection. Our best results achieve an F -measure of 81.02 using text alone and 80.61 using graphs alone. We also combine both modalities of information (text and graphs) through three fusion strategies, and show that this strongly improves abuse detection performance, increasing the F -measure to 87.06. Finally, we identify which specific engineered features are captured by the embedding methods under consideration. These features have clear interpretations and help explain what information the representation learning methods deem discriminative.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief System Dynamics as Network of Single Layered Neural Network</title>
<link>https://arxiv.org/abs/2505.00005</link>
<guid>https://arxiv.org/abs/2505.00005</guid>
<content:encoded><![CDATA[
<div> belief propagation, social network, polarization, misinformation, neural network

Summary:
In this study, a modified model of the Friedkin-Johnsen model was proposed to investigate belief propagation on social networks. The model treated individuals as single-layer neural networks, with confidence levels on evidence as inputs and belief as the output. The research reaffirmed Madison's remedy for factionalism and found that a network with a giant component reduced belief distribution variance more than a network with two communities, despite creating more social pressure. Additionally, a community structure decreased sensitivity of belief distribution variance to individual confidence levels. The model's insights have implications for political polarization, misinformation, economic conflicts, as well as applications in personality theory and behavioral psychology. <div>
arXiv:2505.00005v1 Announce Type: new 
Abstract: As problems in political polarization and the spread of misinformation become serious, belief propagation on a social network becomes an important question to explore. Previous breakthroughs have been made in algorithmic approaches to understanding how group consensus or polarization can occur in a population. This paper proposed a modified model of the Friedkin-Johnsen model that tries to explain the underlying stubbornness of individual as well as possible back fire effect by treating each individual as a single layer neural network on a set of evidence for a particular statement with input being confidence level on each evidence, and belief of the statement is the output of this neural network.
  In this papar, we reafirmed the importance of Madison's cure for the mischief of faction, and found that when structure of understanding is polarized, a network with a giant component can decrease the variance in the belief distribution more than a network with two communities, but creates more social pressure by doing so. We also found that when community structure is formed, variance in the belief distribution become less sensitive to confidence level of individuals. The model can have various applications to political and historical problems caused by misinfomation and conflicting economic interest as well as applications to personality theory and behavior psychology.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams</title>
<link>https://arxiv.org/abs/2505.00242</link>
<guid>https://arxiv.org/abs/2505.00242</guid>
<content:encoded><![CDATA[
<div> Interpretable, Partial Differential Equation, Tensor Decomposition, Time-Varying, Forecasting <br />
<br />
Summary: 
The paper introduces D-Tracker, a method for capturing time-varying temporal patterns in social activity tensor data streams and forecasting future activities. D-Tracker utilizes a tensor decomposition framework incorporating partial differential equations to interpret trends, seasonality, and interest diffusion between locations. It automatically models tensor data streams without the need for hyperparameters and is computationally scalable. Experiments using web search volume and COVID-19 infection data demonstrate D-Tracker's superior forecasting accuracy and efficiency compared to existing methods, highlighting its ability to extract location-based interest diffusion information. The D-Tracker source code and datasets are freely available for access, enabling further research and applications in analyzing and predicting social activity patterns.  <br /><br />Summary: <div>
arXiv:2505.00242v1 Announce Type: new 
Abstract: Large quantities of social activity data, such as weekly web search volumes and the number of new infections with infectious diseases, reflect peoples' interests and activities. It is important to discover temporal patterns from such data and to forecast future activities accurately. However, modeling and forecasting social activity data streams is difficult because they are high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and interest diffusion. In this paper, we propose D-Tracker, a method for continuously capturing time-varying temporal patterns within social activity tensor data streams and forecasting future activities. Our proposed method has the following properties: (a) Interpretable: it incorporates the partial differential equation into a tensor decomposition framework and captures time-varying temporal patterns such as trends, seasonality, and interest diffusion between locations in an interpretable manner; (b) Automatic: it has no hyperparameters and continuously models tensor data streams fully automatically; (c) Scalable: the computation time of D-Tracker is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data Repository show that our method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the interest diffusion between locations. Our source code and datasets are available at {https://github.com/Higashiguchi-Shingo/D-Tracker.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avatar Communication Provides More Efficient Online Social Support Than Text Communication</title>
<link>https://arxiv.org/abs/2505.00287</link>
<guid>https://arxiv.org/abs/2505.00287</guid>
<content:encoded><![CDATA[
<div> avatar communication, online social support, social relationships, offline social resources, metaverse societies
<br />
Summary: 
The study investigates the differences in online social support between avatar communication service users and text communication service users. Avatar communication users received more online social support, had more stable relationships, and had fewer offline social resources compared to text communication users. However, the positive association between online and offline social support was stronger for avatar communication users. The study emphasizes the importance of realistic online communication experiences through avatars, including nonverbal and real-time interactions. It also highlights the challenges faced by avatar communication users in the physical world, such as the lack of offline social resources. Enhancing online social support through avatars could help address these issues and potentially improve social resource problems in both online and offline settings in future metaverse societies. 
<br /> <div>
arXiv:2505.00287v1 Announce Type: new 
Abstract: Online communication via avatars provides a richer online social experience than text communication. This reinforces the importance of online social support. Online social support is effective for people who lack social resources because of the anonymity of online communities. We aimed to understand online social support via avatars and their social relationships to provide better social support to avatar users. Therefore, we administered a questionnaire to three avatar communication service users (Second Life, ZEPETO, and Pigg Party) and three text communication service users (Facebook, X, and Instagram) (N=8,947). There was no duplication of users for each service. By comparing avatar and text communication users, we examined the amount of online social support, stability of online relationships, and the relationships between online social support and offline social resources (e.g., offline social support). We observed that avatar communication service users received more online social support, had more stable relationships, and had fewer offline social resources than text communication service users. However, the positive association between online and offline social support for avatar communication users was more substantial than for text communication users. These findings highlight the significance of realistic online communication experiences through avatars, including nonverbal and real-time interactions with co-presence. The findings also highlighted avatar communication service users' problems in the physical world, such as the lack of offline social resources. This study suggests that enhancing online social support through avatars can address these issues. This could help resolve social resource problems, both online and offline in future metaverse societies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Sexual Predation and Victimization Through Warnings and Awareness among High-Risk Users</title>
<link>https://arxiv.org/abs/2505.00293</link>
<guid>https://arxiv.org/abs/2505.00293</guid>
<content:encoded><![CDATA[
<div> Keywords: online sexual predators, prevention strategy, high-risk individuals, machine learning, randomized controlled trial

Summary:
This study focused on preventing online sexual predation by targeting high-risk individuals through warnings and awareness-building messages based on criminal psychology theories. Using a machine learning model, high-risk users on an avatar-based communication platform were identified and divided into intervention and control groups. The intervention successfully reduced violations and victimization among women for a significant period, highlighting the effectiveness of targeted interventions in preventing online sexual abuse. However, the impact on men was not as pronounced, indicating a need for gender-specific prevention strategies. These findings contribute to the ongoing efforts to combat online sexual predators and enhance understanding of criminal psychology in the digital age. 

<br /><br />Summary: <div>
arXiv:2505.00293v1 Announce Type: new 
Abstract: Online sexual predators target children by building trust, creating dependency, and arranging meetings for sexual purposes. This poses a significant challenge for online communication platforms that strive to monitor and remove such content and terminate predators' accounts. However, these platforms can only take such actions if sexual predators explicitly violate the terms of service, not during the initial stages of relationship-building. This study designed and evaluated a strategy to prevent sexual predation and victimization by delivering warnings and raising awareness among high-risk individuals based on the routine activity theory in criminal psychology. We identified high-risk users as those with a high probability of committing or being subjected to violations, using a machine learning model that analyzed social networks and monitoring data from the platform. We conducted a randomized controlled trial on a Japanese avatar-based communication application, Pigg Party. High-risk players in the intervention group received warnings and awareness-building messages, while those in the control group did not receive the messages, regardless of their risk level. The trial involved 12,842 high-risk players in the intervention group and 12,844 in the control group for 138 days. The intervention successfully reduced violations and being violated among women for 12 weeks, although the impact on men was limited. These findings contribute to efforts to combat online sexual abuse and advance understanding of criminal psychology.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a digital twin of U.S. Congress</title>
<link>https://arxiv.org/abs/2505.00006</link>
<guid>https://arxiv.org/abs/2505.00006</guid>
<content:encoded><![CDATA[
<div> virtual model, U.S. congresspersons, language models, digital twin, Tweets 

Summary: 
This paper presents a virtual model of U.S. congresspersons using language models, which qualifies as a digital twin. A dataset containing Tweets from congresspersons is analyzed, and language models simulate their Tweets accurately. These generated Tweets can predict voting behavior and the likelihood of bipartisanship, aiding in resource allocation and legislative dynamics. The study discusses the analysis's limitations and potential extensions. <div>
arXiv:2505.00006v1 Announce Type: cross 
Abstract: In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S3AND: Efficient Subgraph Similarity Search Under Aggregated Neighbor Difference Semantics (Technical Report)</title>
<link>https://arxiv.org/abs/2505.00393</link>
<guid>https://arxiv.org/abs/2505.00393</guid>
<content:encoded><![CDATA[
<div> semantic, subgraph similarity search, keyword set, aggregated neighbor difference, indexing mechanism  
Summary:  
- This paper introduces the Subgraph Similarity Search under Aggregated Neighbor Difference Semantics (S$^3$AND) problem, which aims to find subgraphs in a data graph that are similar to a query graph by considering keywords and graph structures. 
- The authors propose two pruning methods, namely keyword set and aggregated neighbor difference lower bound pruning, to reduce the search space by eliminating false alarms of candidate vertices/subgraphs. 
- An effective indexing mechanism is designed to support the efficient S$^3$AND query answering algorithm. 
- Extensive experiments show the effectiveness and efficiency of the S$^3$AND approach on both real and synthetic graphs across various parameter settings.  
Summary: <div>
arXiv:2505.00393v1 Announce Type: cross 
Abstract: For the past decades, the \textit{subgraph similarity search} over a large-scale data graph has become increasingly important and crucial in many real-world applications, such as social network analysis, bioinformatics network analytics, knowledge graph discovery, and many others. While previous works on subgraph similarity search used various graph similarity metrics such as the graph isomorphism, graph edit distance, and so on, in this paper, we propose a novel problem, namely \textit{subgraph similarity search under aggregated neighbor difference semantics} (S$^3$AND), which identifies subgraphs $g$ in a data graph $G$ that are similar to a given query graph $q$ by considering both keywords and graph structures (under new keyword/structural matching semantics). To efficiently tackle the S$^3$AND problem, we design two effective pruning methods, \textit{keyword set} and \textit{aggregated neighbor difference lower bound pruning}, which rule out false alarms of candidate vertices/subgraphs to reduce the S$^3$AND search space. Furthermore, we construct an effective indexing mechanism to facilitate our proposed efficient S$^3$AND query answering algorithm. Through extensive experiments, we demonstrate the effectiveness and efficiency of our S$^3$AND approach over both real and synthetic graphs under various parameter settings.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Cultural and Digital Divides: A Low-Latency JackTrip Framework for Equitable Music Education in the Global South</title>
<link>https://arxiv.org/abs/2505.00550</link>
<guid>https://arxiv.org/abs/2505.00550</guid>
<content:encoded><![CDATA[
<div> Keywords: digital technologies, music education, JackTrip framework, Global South, cultural preservation

Summary: 
This paper introduces a low-latency JackTrip framework that addresses infrastructural and cultural challenges in music education in the Global South. The framework utilizes an open-source UDP-based audio streaming protocol to overcome technical constraints like limited bandwidth and high latency prevalent in rural and underserved regions. A comparison with conventional platforms like Zoom shows that JackTrip achieves sub-30 ms latency under simulated low-resource conditions while maintaining intricate audio details crucial for non-Western musical traditions. Spectral analysis confirms JackTrip's ability to handle microtonal scales, complex rhythms, and harmonic textures, providing an authentic medium for real-time ensemble performance and music education. These results highlight the potential of decentralized, edge-computing solutions in promoting technological equity and cultural preservation among educators and musicians in the Global South.<br /><br />Summary: <div>
arXiv:2505.00550v1 Announce Type: cross 
Abstract: The rapid expansion of digital technologies has transformed educational landscapes worldwide, yet significant infrastructural and cultural challenges persist in the Global South. This paper introduces a low-latency JackTrip framework designed to bridge both the cultural and digital divides in music education. By leveraging an open-source, UDP-based audio streaming protocol originally developed at Stanford's CCRMA, the framework is tailored to address technical constraints such as intermittent connectivity, limited bandwidth, and high latency that characterize many rural and underserved regions. The study systematically compares the performance of JackTrip with conventional platforms like Zoom, demonstrating that JackTrip achieves sub-30~ms latency under simulated low-resource conditions while preserving the intricate audio details essential for non-Western musical traditions. Spectral analysis confirms that JackTrip's superior handling of microtonal scales, complex rhythms, and harmonic textures provides a culturally authentic medium for real-time ensemble performance and music education. These findings underscore the transformative potential of decentralized, edge-computing solutions in empowering educators and musicians across the Global South, promoting both technological equity and cultural preservation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new sociology of humans and machines</title>
<link>https://arxiv.org/abs/2402.14410</link>
<guid>https://arxiv.org/abs/2402.14410</guid>
<content:encoded><![CDATA[
<div> Keywords: fake social media accounts, generative artificial intelligence, complex social systems, human-machine interactions, collective decision-making

Summary:<br /><br />
The article discusses the proliferation of robots, bots, and algorithms in various aspects of society and the need to study the interactions between humans and intelligent machines. It reviews research on competition, coordination, cooperation, contagion, and collective decision-making in complex social systems. Examples are provided from high-frequency trading markets, social media platforms, open collaboration communities, and discussion forums. The importance of a new sociology of humans and machines is emphasized, highlighting the need for researchers to use complex system methods, engineers to design AI for human-machine and machine-machine interactions, and regulators to govern the development of human-machine communities. By understanding and addressing the dynamics and patterns in human-machine interactions, we can ensure the resilience and robustness of these communities in the face of increasing technological integration. <div>
arXiv:2402.14410v3 Announce Type: replace 
Abstract: From fake social media accounts and generative artificial intelligence chatbots to trading algorithms and self-driving vehicles, robots, bots and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions and transportation arteries. Networks of multiple interdependent and interacting humans and intelligent machines constitute complex social systems for which the collective outcomes cannot be deduced from either human or machine behaviour alone. Under this paradigm, we review recent research and identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion and collective decision-making, with context-rich examples from high-frequency trading markets, a social media platform, an open collaboration community and a discussion forum. To ensure more robust and resilient human-machine communities, we require a new sociology of humans and machines. Researchers should study these communities using complex system methods; engineers should explicitly design artificial intelligence for human-machine and machine-machine interactions; and regulators should govern the ecological diversity and social co-development of humans and machines.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The illusion of households as entities in social networks</title>
<link>https://arxiv.org/abs/2502.14764</link>
<guid>https://arxiv.org/abs/2502.14764</guid>
<content:encoded><![CDATA[
<div> household, individual, social network, entitativity, data analysis  
Summary:  
This article discusses the differences between household and individual social networks and the importance of choosing the correct network for study design and data analysis. The study explores how the results of social network analysis can vary depending on whether the household or individual network is studied, impacting findings on assortativity, influence-maximizing nodes, and information spread within households. The authors propose systematic recommendations for determining the relevant network representation to study, considering entitativity criteria and cultural or experimental contexts. They highlight the illusion of entitativity as a factor where grouping individuals into households may not adequately capture social dynamics. Understanding which network to study is crucial for researchers and practitioners analyzing social network data, and this work aims to provide guidance for making informed decisions in data collection and analysis.  
<br /><br />Summary: <div>
arXiv:2502.14764v2 Announce Type: replace 
Abstract: Data recording connections between people in communities and villages are collected and analyzed in various ways, most often as either networks of individuals or as networks of households. These two networks can differ in substantial ways. The methodological choice of which network to study, therefore, is an important aspect in both study design and data analysis. In this work, we consider various key differences between household and individual social network structure, and ways in which the networks cannot be used interchangeably. In addition to formalizing the choices for representing each network, we explore the consequences of how the results of social network analysis change depending on the choice between studying the individual and household network -- from determining whether networks are assortative or disassortative to the ranking of influence-maximizing nodes. As our main contribution, we draw upon related work to propose a set of systematic recommendations for determining the relevant network representation to study. Our recommendations include assessing a series of entitativity criteria and relating these criteria to theories and observations about patterns and norms in social dynamics at the household level: notably, how information spreads within households and how power structures and gender roles affect this spread. We draw upon the definition of an illusion of entitativity to identify cases wherein grouping people into households does not satisfy these criteria or adequately represent given cultural or experimental contexts. Given the widespread use of social network data for studying communities, there is broad impact in understanding which network to study and the consequences of that decision. We hope that this work gives guidance to practitioners and researchers collecting and studying social network data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omitted Labels Induce Nontransitive Paradoxes in Causality</title>
<link>https://arxiv.org/abs/2311.06840</link>
<guid>https://arxiv.org/abs/2311.06840</guid>
<content:encoded><![CDATA[
<div> omitted label contexts, training data, specialized human experts, Simpson's paradox, nontransitive structures <br />
Summary: 
The article discusses the concept of omitted label contexts in training data, common in specialized fields or focused studies. It explores how adjustments in such contexts may require non-exchangeable treatment and control groups. Through studying Simpson's paradox, the article identifies the existence of nontransitivity in networks of conclusions drawn from different contexts. It demonstrates that the space of possible nontransitive structures in these networks corresponds to structures formed from aggregating ranked-choice votes. Overall, the study sheds light on the complexities of analyzing datasets with limited label contexts and the implications of nontransitivity in drawing conclusions from diverse sets of data. <div>
arXiv:2311.06840v4 Announce Type: replace-cross 
Abstract: We explore "omitted label contexts," in which training data is limited to a subset of the possible labels. This setting is standard among specialized human experts or specific, focused studies. By studying Simpson's paradox, we observe that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. A generalization of Simpson's paradox leads us to study networks of conclusions drawn from different contexts, within which a paradox of nontransitivity arises. We prove that the space of possible nontransitive structures in these networks exactly corresponds to structures that form from aggregating ranked-choice votes.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Updating Katz centrality by counting walks</title>
<link>https://arxiv.org/abs/2411.19560</link>
<guid>https://arxiv.org/abs/2411.19560</guid>
<content:encoded><![CDATA[
<div> Efficient, effective, update, Katz centralities, node removal, edge removal, loss of walks, network, algorithms, F-avoiding first-passage walks, total network communicability, numerical experiments, synthetic networks, real-world networks.

Summary:
This article introduces strategies for updating Katz centralities in simple graphs following node and edge removal. Formulas for measuring the "loss of walks" in a network due to these removals are provided, based on the concept of F-avoiding first-passage walks. The article also presents algorithms informed by these formulas and derives bounds on changes in total network communicability. Extensive numerical experiments on both synthetic and real-world networks validate the theoretical findings. The study emphasizes efficient and effective approaches for maintaining centrality measures in networks undergoing structural changes. <div>
arXiv:2411.19560v2 Announce Type: replace-cross 
Abstract: We develop efficient and effective strategies for the update of Katz centralities after node and edge removal in simple graphs. We provide explicit formulas for the ``loss of walks" a network suffers when nodes/edges are removed, and use these to inform our algorithms. The theory builds on the newly introduced concept of $\cF$-avoiding first-passage walks. Further, bounds on the change of total network communicability are also derived. Extensive numerical experiments on synthetic and real-world networks complement our theoretical results.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Small Communities in the Planted Partition Model</title>
<link>https://arxiv.org/abs/2504.01663</link>
<guid>https://arxiv.org/abs/2504.01663</guid>
<content:encoded><![CDATA[
<div> community recovery, planted partition model, correlation coefficient, Diamond Percolation, power-law distribution<br />
Summary:<br />
In this paper, the authors analyze community recovery in the planted partition model (PPM) with a focus on scenarios with a large number of communities. They redefine recovery regimes using the correlation coefficient to accommodate varying community sizes. The Diamond Percolation algorithm is introduced as an effective method for recovering communities with minimal constraints on community numbers and sizes. The algorithm shows promising results under mild assumptions on edge probabilities. Additionally, the study considers unbalanced partitions, particularly when community sizes follow a power-law distribution, which is common in real-world networks. These findings present valuable insights into community recovery techniques in complex network structures. <div>
arXiv:2504.01663v2 Announce Type: replace-cross 
Abstract: We analyze community recovery in the planted partition model (PPM) in regimes where the number of communities is arbitrarily large. We examine the three standard recovery regimes: exact recovery, almost exact recovery, and weak recovery. When communities vary in size, traditional accuracy- or alignment-based metrics become unsuitable for assessing the correctness of a predicted partition. To address this, we redefine these recovery regimes using the correlation coefficient, a more versatile metric for comparing partitions. We then demonstrate that $\textit{Diamond Percolation}$, an algorithm based on common-neighbors, successfully recovers communities under mild assumptions on edge probabilities, with minimal restrictions on the number and sizes of communities. As a key application, we consider the case where community sizes follow a power-law distribution, a characteristic frequently found in real-world networks. To the best of our knowledge, we provide the first recovery results for such unbalanced partitions.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection</title>
<link>https://arxiv.org/abs/2504.21357</link>
<guid>https://arxiv.org/abs/2504.21357</guid>
<content:encoded><![CDATA[
<div> Keywords: information cocoon, social media debates, double-layer network, graph auto-encoder, community detection algorithms

Summary: 
This paper addresses the issue of information cocoons in social media debates resulting from homogeneous viewpoints and preferences clustering users into sub-networks. The authors propose a double-layer network model considering relational ties and feature-based user similarity. They develop graph auto-encoder based community detection algorithms to identify and break information cocoons. Testing on real and synthetic datasets shows the proposed algorithms outperform existing methods in partitioning user communities. An intervention strategy based on influence is introduced, showing how the algorithms can effectively reduce polarization and information cocoon formation with minimal intervention. The Markov states transition model is used to simulate intervention effects, demonstrating the effectiveness of the proposed algorithms in mitigating information cocoons. <br /><br />Summary: <div>
arXiv:2504.21357v1 Announce Type: new 
Abstract: With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Machine Learning for characterizing social networks Agent-based models</title>
<link>https://arxiv.org/abs/2504.21609</link>
<guid>https://arxiv.org/abs/2504.21609</guid>
<content:encoded><![CDATA[
<div> Keywords: social media networks, agent-based modeling, High Performance Computing, Machine Learning, user behaviors

Summary: 
Agent-based modeling (ABM) is a valuable tool for studying social media networks, allowing for the simulation of individual behaviors and system-level evolution. However, the complexity of modeling social networks requires superior data processing and storage capabilities, which can be provided by High Performance Computing (HPC). By leveraging Machine Learning (ML) methods, researchers can efficiently analyze vast amounts of data from social media users to better understand behaviors, preferences, and trends. This proposal aims to use ML to characterize user attributes and develop a general user model for ABM simulations of social networks on HPC systems. By combining ABM, HPC, and ML, researchers can gain valuable insights into social network dynamics and user interactions. <div>
arXiv:2504.21609v1 Announce Type: new 
Abstract: Nowadays, social media networks are increasingly significant to our lives, the imperative to study social media networks becomes more and more essential. With billions of users across platforms and constant updates, the complexity of modeling social networks is immense. Agent-based modeling (ABM) is widely employed to study social networks community, allowing us to define individual behaviors and simulate system-level evolution. It can be a powerful tool to test how the algorithms affect users behavior. To fully leverage agent-based models,superior data processing and storage capabilities are essential. High Performance Computing (HPC) presents an optimal solution, adept at managing complex computations and analysis, particularly for voluminous or iteration-intensive tasks. We utilize Machine Learning (ML) methods to analyze social media users due to their ability to efficiently process vast amounts of data and derive insights that aid in understanding user behaviors, preferences, and trends. Therefore, our proposal involves ML to characterize user attributes and to develop a general user model for ABM simulation of in social networks on HPC systems.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models</title>
<link>https://arxiv.org/abs/2504.21026</link>
<guid>https://arxiv.org/abs/2504.21026</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual users, abusive language detection, code-mixed text, low-resource languages, NLP<br />
Summary:<br />
- Detecting abusive language in code-mixed text poses challenges due to linguistic blending and context dependency.
- A manually annotated dataset of Telugu-English and Nepali-English code-mixed comments was introduced for abusive language detection.
- Different machine learning and deep learning models were experimented with, including Logistic Regression, Neural Networks, and Large Language Models.
- Performance was optimized through hyperparameter tuning and evaluated using 10-fold cross-validation.
- The study provides insights into the difficulties of detecting abusive language in code-mixed settings and establishes benchmarks for abusive language detection in low-resource languages like Telugu and Nepali. This can aid in the development of more robust moderation strategies for multilingual social media environments.<br /> <div>
arXiv:2504.21026v1 Announce Type: cross 
Abstract: With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construct to Commitment: The Effect of Narratives on Economic Growth</title>
<link>https://arxiv.org/abs/2504.21060</link>
<guid>https://arxiv.org/abs/2504.21060</guid>
<content:encoded><![CDATA[
<div> Keywords: government-led narratives, mass media, Innovation-Driven Development Strategy, total factor productivity, economic growth

Summary:
The article presents the "Narratives-Construct-Commitment (NCC)" framework, which explores how government-led narratives evolve from framing expectations to becoming sustainable pillars for growth. By analyzing the Innovation-Driven Development Strategy of 2016 as a case study, the study identifies the impact of narrative shocks and their influence on investment incentives, R&amp;D resources, and total factor productivity (TFP). The findings highlight the role of credible narratives in shaping expectations, driving economic growth, and institutionalizing vision for sustained improvements. The research provides insights into the transformation of visions into tangible economic outcomes through the strategic use of narratives in policy-making and development initiatives. <div>
arXiv:2504.21060v1 Announce Type: cross 
Abstract: We study how government-led narratives through mass media evolve from construct, a mechanism for framing expectations, into commitment, a sustainable pillar for growth. We propose the ``Narratives-Construct-Commitment (NCC)" framework outlining the mechanism and institutionalization of narratives, and formalize it as a dynamic Bayesian game. Using the Innovation-Driven Development Strategy (2016) as a case study, we identify the narrative shock from high-frequency financial data and trace its impact using local projection method. By shaping expectations, credible narratives institutionalize investment incentives, channel resources into R\&amp;D, and facilitate sustained improvements in total factor productivity (TFP). Our findings strive to provide insights into the New Quality Productive Forces initiative, highlighting the role of narratives in transforming vision into tangible economic growth.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Manipulated Contents Using Knowledge-Grounded Inference</title>
<link>https://arxiv.org/abs/2504.21165</link>
<guid>https://arxiv.org/abs/2504.21165</guid>
<content:encoded><![CDATA[
<div> Fake news, manipulated content, detection, zero-day, mainstream search engines <br />
<br />
Summary: 
The article introduces Manicod, a tool designed to detect zero-day manipulated content by utilizing contextual information from mainstream search engines. By sourcing real-time context and using a large language model (LLM) with retrieval-augmented generation (RAG), Manicod can determine if a piece of content is truthful or manipulated, providing an explanation for its decision. The tool is validated using a dataset of 4270 manipulated fake news articles and achieves an overall F1 score of 0.856, outperforming existing methods in fact-checking and claim verification. Manicod addresses the challenge of zero-day manipulated content, offering a promising solution for identifying fake news in real-time scenarios. <div>
arXiv:2504.21165v1 Announce Type: cross 
Abstract: The detection of manipulated content, a prevalent form of fake news, has been widely studied in recent years. While existing solutions have been proven effective in fact-checking and analyzing fake news based on historical events, the reliance on either intrinsic knowledge obtained during training or manually curated context hinders them from tackling zero-day manipulated content, which can only be recognized with real-time contextual information. In this work, we propose Manicod, a tool designed for detecting zero-day manipulated content. Manicod first sources contextual information about the input claim from mainstream search engines, and subsequently vectorizes the context for the large language model (LLM) through retrieval-augmented generation (RAG). The LLM-based inference can produce a "truthful" or "manipulated" decision and offer a textual explanation for the decision. To validate the effectiveness of Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake news derived from 2500 recent real-world news headlines. Manicod achieves an overall F1 score of 0.856 on this dataset and outperforms existing methods by up to 1.9x in F1 score on their benchmarks on fact-checking and claim verification.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Metric Dimension of Sparse Random Graphs</title>
<link>https://arxiv.org/abs/2504.21244</link>
<guid>https://arxiv.org/abs/2504.21244</guid>
<content:encoded><![CDATA[
<div> bounds, metric dimension, random graphs, Erds-Rnyi, connectivity transition <br />
Summary: 
The article presents upper and lower bounds on the likely metric dimension of Erds-Rnyi random graphs. Previous research had provided bounds for random graphs with expected degrees greater than or equal to log^5 n, leaving a gap for sparser graphs with lower expected degrees. The new bounds cover the range just above the connectivity transition, where the expected degree is a constant multiple of the logarithm of n, up to log^5 n. The lower bound is based on an entropic argument, offering a more general approach compared to previous methodologies, while the upper bound is similar to existing results. These findings contribute to understanding the metric properties of random graphs across a wide range of densities. <br /><br />Summary: <div>
arXiv:2504.21244v1 Announce Type: cross 
Abstract: In 2013, Bollob\'as, Mitsche, and Pralat at gave upper and lower bounds for the likely metric dimension of random Erd\H{o}s-R\'enyi graphs $G(n,p)$ for a large range of expected degrees $d=pn$. However, their results only apply when $d \ge \log^5 n$, leaving open sparser random graphs with $d < \log^5 n$. Here we provide upper and lower bounds on the likely metric dimension of $G(n,p)$ from just above the connectivity transition, i.e., where $d=pn=c \log n$ for some $c > 1$, up to $d=\log^5 n$. Our lower bound technique is based on an entropic argument which is more general than the use of Suen's inequality by Bollob\'as, Mitsche, and Pralat, whereas our upper bound is similar to theirs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defense Against Shortest Path Attacks</title>
<link>https://arxiv.org/abs/2305.19083</link>
<guid>https://arxiv.org/abs/2305.19083</guid>
<content:encoded><![CDATA[
<div> defense, shortest paths, graph manipulation, Stackelberg game, NP-hard

Summary:
This paper addresses the issue of defending against malicious manipulation of graphs to control traffic flow between nodes. The proposed defense strategy involves modifying edge weights in order to recommend shortest paths to users while maintaining the integrity of the original graph. The defender aims to minimize the probability of attacks from malicious actors while minimizing negative impacts on benign users. The defense is formulated as a Stackelberg game, with the defender taking the leading role. The problem is proven to be NP-hard, and heuristic solutions are proposed for both zero-sum and non-zero-sum scenarios. By formulating a linear program for local optimization, the defense strategy achieves results close to the lower bound of the defender's cost. Experimental results with synthetic and real networks demonstrate the effectiveness of the proposed methods in defending against graph manipulation attacks. 

<br /><br />Summary: <div>
arXiv:2305.19083v2 Announce Type: replace 
Abstract: Identifying shortest paths between nodes in a network is an important task in many applications. Recent work has shown that a malicious actor can manipulate a graph to make traffic between two nodes of interest follow their target path. In this paper, we develop a defense against such attacks by modifying the edge weights that users observe. The defender must balance inhibiting the attacker against any negative effects on benign users. Specifically, the defender's goals are: (a) recommend the shortest paths to users, (b) make the lengths of the shortest paths in the published graph close to those of the same paths in the true graph, and (c) minimize the probability of an attack. We formulate the defense as a Stackelberg game in which the defender is the leader and the attacker is the follower. We also consider a zero-sum version of the game in which the defender's goal is to minimize cost while achieving the minimum possible attack probability. We show that the defense problem is NP-hard and propose heuristic solutions for both the zero-sum and non-zero-sum settings. By relaxing some constraints of the original problem, we formulate a linear program for local optimization around a feasible point. We present defense results with both synthetic and real networks and show that our methods often reach the lower bound of the defender's cost.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOPIM: Bayesian Optimization for influence maximization on temporal networks</title>
<link>https://arxiv.org/abs/2308.04700</link>
<guid>https://arxiv.org/abs/2308.04700</guid>
<content:encoded><![CDATA[
<div> Bayesian Optimization, Influence Maximization, Temporal Networks, Gaussian Process Regression, Expected Improvement <br />
Summary:<br />
The study introduces BOPIM, a Bayesian Optimization approach for Influence Maximization on temporal networks. The challenges addressed include constructing kernel functions based on Hamming distance and Jaccard coefficient, and optimizing the acquisition function using Expected Improvement with noise adjustment. Numerical experiments on real-world networks show that BOPIM outperforms other methods and achieves comparable influence spreads to a gold-standard greedy algorithm, with a significantly faster runtime. Surprisingly, the Hamming kernel performs better than the Jaccard kernel. The study also explores ways to quantify uncertainty in optimal seed sets, a novel approach in Influence Maximization research. <div>
arXiv:2308.04700v4 Announce Type: replace 
Abstract: The goal of influence maximization (IM) is to select a small set of seed nodes which maximizes the spread of influence on a network. In this work, we propose BOPIM, a Bayesian Optimization (BO) algorithm for IM on temporal networks. The IM task is well-suited for a BO solution due to its expensive and complicated objective function. There are at least two key challenges, however, that must be overcome, primarily due to the inputs coming from a cardinality-constrained, non-Euclidean, combinatorial space. The first is constructing the kernel function for the Gaussian Process regression. We propose two kernels, one based on the Hamming distance between seed sets and the other leveraging the Jaccard coefficient between node's neighbors. The second challenge is the acquisition function. For this, we use the Expected Improvement function, suitably adjusting for noise in the observations, and optimize it using a greedy algorithm to account for the cardinality constraint. In numerical experiments on real-world networks, we prove that BOPIM outperforms competing methods and yields comparable influence spreads to a gold-standard greedy algorithm while being as much as ten times faster. In addition, we find that the Hamming kernel performs favorably compared to the Jaccard kernel in nearly all settings, a somewhat surprising result as the former does not explicitly account for the graph structure. Finally, we demonstrate two ways that the proposed method can quantify uncertainty in optimal seed sets. To our knowledge, this is the first attempt to look at uncertainty in the seed sets for IM.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes</title>
<link>https://arxiv.org/abs/2408.05794</link>
<guid>https://arxiv.org/abs/2408.05794</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Multimodal Models, HateSieve, Hateful Memes, Contrastive Meme Generator, Memes 

Summary:<br /><br />Amidst the increasing use of Large Multimodal Models (LMMs) in creating and interpreting complex content, the threat of spreading biased and harmful memes persists. Current safety protocols often struggle to uncover hate speech subtly embedded in "Confounder Memes." To tackle this issue, the researchers introduce HateSieve, a new framework focused on improving the detection and segmentation of hateful elements in memes. HateSieve employs a unique Contrastive Meme Generator to create semantically paired memes, a tailored triplet dataset for contrastive learning, and an Image-Text Alignment module to generate context-aware embeddings for precise meme segmentation. Experimental results using the Hateful Meme Dataset demonstrate that HateSieve outperforms existing LMMs in accuracy with fewer parameters while providing a reliable method for pinpointing and isolating hate speech within memes. Viewer discretion is advised due to the academic discussions of hate speech. <div>
arXiv:2408.05794v2 Announce Type: replace-cross 
Abstract: Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within ``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new framework designed to enhance the detection and segmentation of hateful elements in memes. \textsc{HateSieve} features a novel Contrastive Meme Generator that creates semantically paired memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. \textcolor{red}{Caution: Contains academic discussions of hate speech; viewer discretion advised.}
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triadic Closure-Heterogeneity-Harmony GCN for Link Prediction</title>
<link>https://arxiv.org/abs/2504.20492</link>
<guid>https://arxiv.org/abs/2504.20492</guid>
<content:encoded><![CDATA[
<div> Link prediction, TriHetGCN, Graph Convolutional Networks, topological indicators, connection probability <br />
<br />
Summary: TriHetGCN is proposed to enhance link prediction in complex networks. It integrates topological indicators like triadic closure and degree heterogeneity into the Graph Convolutional Networks (GCNs) framework. The model consists of three modules: topology feature construction, graph structural representation, and connection probability prediction. By incorporating node features, TriHetGCN improves global structure perception and effectively captures intrinsic structural relationships between node pairs. Evaluated on various real-world datasets, TriHetGCN outperforms existing methods and demonstrates strong generalizability across different network types. This work bridges statistical physics and graph deep learning, offering a promising framework for diverse applications in link prediction. <div>
arXiv:2504.20492v1 Announce Type: new 
Abstract: Link prediction aims to estimate the likelihood of connections between pairs of nodes in complex networks, which is beneficial to many applications from friend recommendation to metabolic network reconstruction. Traditional heuristic-based methodologies in the field of complex networks typically depend on predefined assumptions about node connectivity, limiting their generalizability across diverse networks. While recent graph neural network (GNN) approaches capture global structural features effectively, they often neglect node attributes and intrinsic structural relationships between node pairs. To address this, we propose TriHetGCN, an extension of traditional Graph Convolutional Networks (GCNs) that incorporates explicit topological indicators -- triadic closure and degree heterogeneity. TriHetGCN consists of three modules: topology feature construction, graph structural representation, and connection probability prediction. The topology feature module constructs node features using shortest path distances to anchor nodes, enhancing global structure perception. The graph structural module integrates topological indicators into the GCN framework to model triadic closure and heterogeneity. The connection probability module uses deep learning to predict links. Evaluated on nine real-world datasets, from traditional networks without node attributes to large-scale networks with rich features, TriHetGCN achieves state-of-the-art performance, outperforming mainstream methods. This highlights its strong generalization across diverse network types, offering a promising framework that bridges statistical physics and graph deep learning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts</title>
<link>https://arxiv.org/abs/2504.20065</link>
<guid>https://arxiv.org/abs/2504.20065</guid>
<content:encoded><![CDATA[
<div> Plato, Aristotle, references, network analysis, historical works
Summary: The study used computational methods to analyze references in 2,245 philosophical texts from 550 BCE to 1940 AD. It mapped over 294,970 references between authors to measure how philosophical ideas spread over time. Plato and Aristotle accounted for nearly 10% of all references, indicating their significant influence. The analysis supported the view of St. Thomas Aquinas as a synthesizer between Aristotelian and Christian philosophy. The results were presented through an interactive visualization tool, allowing users to explore the networks dynamically. The methodology demonstrated the value of applying network analysis to study the intellectual lineages of philosophical scholarship through textual references. <div>
arXiv:2504.20065v1 Announce Type: cross 
Abstract: We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication in Agile Software Development - A Mapping Study</title>
<link>https://arxiv.org/abs/2504.20186</link>
<guid>https://arxiv.org/abs/2504.20186</guid>
<content:encoded><![CDATA[
<div> Keywords: Software industry, Agile software development, Communication, Review, Research gaps

Summary: 
In the fast-paced software industry, Agile software development (ASD) is crucial to ensure fast and efficient development processes. However, despite ASD being prevalent for over two decades, there are still many unknowns related to it. This study focuses on the critical factor of communication within ASD. Through a review of 14 studies, the areas of interest and research gaps in ASD communication were identified. The community's interest in communication within ASD was highlighted, shedding light on the importance of effective communication in agile development processes. Addressing these research gaps can lead to a better understanding of how communication impacts the success of ASD initiatives. Overall, this study emphasizes the significance of clear and efficient communication in Agile software development practices. 

Summary: <div>
arXiv:2504.20186v1 Announce Type: cross 
Abstract: Software industry is a fast-moving industry and to keep up with this pace the development process also needs to be fast and efficient and Agile software development (ASD) is the answer to this problem. Even though ASD has been in there for over two decades there are still multiple unknown questions tied to ASD that need to be addressed. In this study we are going to address one of the most critical factors of ASD i.e. Communication. We conducted a review of 14 studies and found the areas under ASD communication that the community is interested in as well as research gaps.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection in multi-layer networks by regularized debiased spectral clustering</title>
<link>https://arxiv.org/abs/2409.07956</link>
<guid>https://arxiv.org/abs/2409.07956</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, multi-layer networks, regularized Laplacian matrix, stochastic block model, modularity

Summary:<br /><br />
This study introduces the regularized debiased sum of squared adjacency matrices (RDSoS) method for community detection in multi-layer networks. RDSoS extends the classical regularized Laplacian matrix to handle multi-layer networks, showing potential in various applications such as gene function prediction and fraud detection. The method is consistent under the multi-layer stochastic block model and its degree-corrected version. A new metric, sum of squared adjacency matrices modularity (SoS-modularity), is introduced to assess community quality and estimate the number of communities. Experimental results demonstrate the method's superiority over state-of-the-art techniques, insensitivity to regularizer selection, and ability to reveal the assortative property of real networks. SoS-modularity provides a more accurate evaluation of community quality compared to traditional metrics. This work opens up new possibilities for community detection in complex multi-layer networks. <div>
arXiv:2409.07956v2 Announce Type: replace-cross 
Abstract: Community detection is a crucial problem in the analysis of multi-layer networks. While regularized spectral clustering methods using the classical regularized Laplacian matrix have shown great potential in handling sparse single-layer networks, to our knowledge, their potential in multi-layer network community detection remains unexplored. To address this gap, in this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical one to multi layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. Additionally, we introduce a sum of squared adjacency matrices modularity (SoS-modularity) to measure the quality of community partitions in multi-layer networks and estimate the number of communities by maximizing this metric. Our methods offer promising applications for predicting gene functions, improving recommender systems, detecting medical insurance fraud, and facilitating link prediction. Experimental results demonstrate that our methods exhibit insensitivity to the selection of the regularizer, generally outperform state-of-the-art techniques, uncover the assortative property of real networks, and that our SoS-modularity provides a more accurate assessment of community quality compared to the average of the Newman-Girvan modularity across layers.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events</title>
<link>https://arxiv.org/abs/2504.18837</link>
<guid>https://arxiv.org/abs/2504.18837</guid>
<content:encoded><![CDATA[
<div> Keywords: extreme weather events, sentiment analysis, social media, climate change, wildfires 

Summary: 
Sentiment analysis plays a crucial role in understanding public perception of extreme weather events driven by climate change, such as wildfires and floods, on social media platforms. The survey explores various methods for sentiment analysis, including lexicon-based, machine learning models, and large language models. It also discusses challenges and ethical considerations related to analyzing sentiment during real-time, high-impact situations like the 2025 Los Angeles forest fires. Data collection and annotation techniques, such as weak supervision and real-time event tracking, are important for accurate sentiment analysis. Open problems include misinformation detection, multimodal sentiment extraction, and ensuring alignment with human values. The goal of the survey is to provide guidance for researchers and practitioners in effectively understanding sentiment during the climate crisis era. 

Summary: <div>
arXiv:2504.18837v1 Announce Type: new 
Abstract: Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Substructure Discovery Algorithm For Homogeneous Multilayer Networks</title>
<link>https://arxiv.org/abs/2504.19328</link>
<guid>https://arxiv.org/abs/2504.19328</guid>
<content:encoded><![CDATA[
<div> Keywords: substructure discovery, graph mining, multilayer networks, decoupling approach, distributed processing<br />
Summary:<br />
Graph mining focuses on finding core substructures in real-world graphs. Substructure discovery involves identifying meaningful patterns in large datasets. Multilayer networks (MLNs) are effective for modeling complex datasets with multiple entity types and relationships. This paper proposes a novel decoupling-based approach for substructure discovery in homogeneous MLNs. The approach processes each layer independently and then composes results from multiple layers to identify substructures in the entire network. The algorithm is implemented using the Map/Reduce paradigm for scalability. Experimental analysis on synthetic and real-world datasets demonstrates the correctness, speedup, and response time of the algorithm. <div>
arXiv:2504.19328v1 Announce Type: new 
Abstract: Graph mining analyzes real-world graphs to find core substructures (connected subgraphs) in applications modeled as graphs. Substructure discovery is a process that involves identifying meaningful patterns, structures, or components within a large data set. These substructures can be of various types, such as frequent patterns, motifs, or other relevant features within the data.
  To model complex data sets -- with multiple types of entities and relationships -- multilayer networks (or MLNs) have been shown to be more effective as compared to simple and attributed graphs. Analysis algorithms on MLNs using the decoupling approach have been shown to be both efficient and accurate. Hence, this paper focuses on substructure discovery in homogeneous multilayer networks (one type of MLN) using a novel decoupling-based approach. In this approach, each layer is processed independently, and then the results from two or more layers are composed to identify substructures in the entire MLN. The algorithm is designed and implemented, including the composition part, using one of the distributed processing frameworks (the Map/Reduce paradigm) to provide scalability.
  After establishing the correctness, we analyze the speedup and response time of the proposed algorithm and approach through extensive experimental analysis on large synthetic and real-world data sets with diverse graph characteristics.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleScope: A Longitudinal Dataset for Investigating Online Discourse and Information Interaction on Telegram</title>
<link>https://arxiv.org/abs/2504.19536</link>
<guid>https://arxiv.org/abs/2504.19536</guid>
<content:encoded><![CDATA[
<div> Keyword: Telegram, dataset, social media, analysis, research <br />
Summary:<br />
This paper introduces TeleScope, a comprehensive dataset suite for analyzing Telegram channels. The dataset includes metadata for 500K channels and message data for 71K public channels, totaling 120M messages. It also provides channel connections and user interaction data for studying information spread and message forwarding patterns. Enrichments like language detection and message posting periods enhance the dataset for in-depth discourse analysis. The dataset enables diverse applications and reproducible social media studies, similar to those on platforms like Twitter. 

<br /><br />Summary: <div>
arXiv:2504.19536v1 Announce Type: new 
Abstract: Telegram is a globally popular instant messaging platform known for its strong emphasis on security, privacy, and unique social networking features. It has recently emerged as the host for various cross-domain analysis and research works, such as social media influence, propaganda studies, and extremism. This paper introduces TeleScope, an extensive dataset suite that, to our knowledge, is the largest of its kind. It comprises metadata for about 500K Telegram channels and downloaded message metadata for about 71K public channels, accounting for around 120M crawled messages. We also release channel connections and user interaction data built using Telegram's message-forwarding feature to study multiple use cases, such as information spread and message forwarding patterns. In addition, we provide data enrichments, such as language detection, active message posting periods for each channel, and Telegram entities extracted from messages, that enable online discourse analysis beyond what is possible with the original data alone. The dataset is designed for diverse applications, independent of specific research objectives, and sufficiently versatile to facilitate the replication of social media studies comparable to those conducted on platforms like X (formerly Twitter)
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Italian Telegram Ecosystem</title>
<link>https://arxiv.org/abs/2504.19594</link>
<guid>https://arxiv.org/abs/2504.19594</guid>
<content:encoded><![CDATA[
<div> Keywords: Telegram, Italian, network analysis, toxicity, extremism<br />
Summary: <br />
This study analyzes the Italian Telegram sphere, examining the spread of misinformation, extremism, and toxicity within the platform's unmoderated environment. Using a dataset of 186 million messages from 13,151 chats collected in 2023, the research employs network analysis and Large Language Models to explore thematic communities and ideological alignment. Results reveal strong thematic and ideological homophily, with far-left and far-right rhetoric coexisting in mixed ideological communities on certain issues. Toxicity is found to be normalized within highly toxic communities, with Italians primarily targeting Black people, Jews, and gay individuals. Additionally, intra-national hostility is observed, reflecting regional and intra-regional cultural conflicts rooted in historical divisions. This comprehensive analysis provides valuable insights into the dynamics of the Italian Telegram ecosystem and sheds light on online toxicity in various cultural and linguistic contexts. <br /> <div>
arXiv:2504.19594v1 Announce Type: new 
Abstract: Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observational Learning with a Budget</title>
<link>https://arxiv.org/abs/2504.19396</link>
<guid>https://arxiv.org/abs/2504.19396</guid>
<content:encoded><![CDATA[
<div> Bayesian learning, observational model, signal, central planner, budget allocation <br />
Summary: <br />
The article discusses a Bayesian observational learning model in which agents receive private signals about a binary state and make decisions based on their signals and previous observations. A central planner aims to enhance signal quality across agents by allocating a limited budget. The budget allocation problem is formulated and analyzed, and two optimal strategies are proposed. One of these strategies maximizes the likelihood of achieving a correct information cascade. <div>
arXiv:2504.19396v1 Announce Type: cross 
Abstract: We consider a model of Bayesian observational learning in which a sequence of agents receives a private signal about an underlying binary state of the world. Each agent makes a decision based on its own signal and its observations of previous agents. A central planner seeks to improve the accuracy of these signals by allocating a limited budget to enhance signal quality across agents. We formulate and analyze the budget allocation problem and propose two optimal allocation strategies. At least one of these strategies is shown to maximize the probability of achieving a correct information cascade.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Cohesive Are Community Search Results on Online Social Networks?: An Experimental Evaluation</title>
<link>https://arxiv.org/abs/2504.19489</link>
<guid>https://arxiv.org/abs/2504.19489</guid>
<content:encoded><![CDATA[
<div> community search algorithms, cohesiveness measures, online social networks, group cohesion, CHASE framework

Summary: This paper evaluates the effectiveness of community search algorithms in online social networks based on cohesiveness measures. While current methods primarily use structural or attribute-based approaches to measure cohesiveness, this study introduces five psychology-informed measures based on group cohesion theory from social psychology. The novel CHASE framework is proposed to evaluate eight representative algorithms on these measures. The analysis reveals a lack of correlation between structural and psychological cohesiveness, highlighting the challenge in identifying psychologically cohesive communities in online social networks. This study provides valuable insights for the development of future community search methods. <br /><br /> <div>
arXiv:2504.19489v1 Announce Type: cross 
Abstract: Recently, numerous community search methods for large graphs have been proposed, at the core of which is defining and measuring cohesion. This paper experimentally evaluates the effectiveness of these community search algorithms w.r.t. cohesiveness in the context of online social networks. Social communities are formed and developed under the influence of group cohesion theory, which has been extensively studied in social psychology. However, current generic methods typically measure cohesiveness using structural or attribute-based approaches and overlook domain-specific concepts such as group cohesion. We introduce five novel psychology-informed cohesiveness measures, based on the concept of group cohesion from social psychology, and propose a novel framework called CHASE for evaluating eight representative CS algorithms w.r.t.these measures on online social networks. Our analysis reveals that there is no clear correlation between structural and psychological cohesiveness, and no algorithm effectively identifies psychologically cohesive communities in online social networks. This study provides new insights that could guide the development of future community search methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding</title>
<link>https://arxiv.org/abs/2504.19734</link>
<guid>https://arxiv.org/abs/2504.19734</guid>
<content:encoded><![CDATA[
<div> Keywords: Dialogue data, Large Language Models, Automated coding, Communicative acts, Contextual complexity

Summary:
- The study introduces a novel LLM-assisted automated coding approach for dialogue data.
- Code prediction for utterances is based on dialogue-specific characteristics using separate prompts.
- Multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek are engaged in collaborative code prediction.
- Contextual consistency checking using GPT-4o substantially improved accuracy.
- Accuracy of act predictions was consistently higher than that of event predictions.

<br /><br />Summary: This study presents a new methodological framework for improving the precision of automated coding of dialogue data by leveraging Large Language Models. It introduces a novel approach where code prediction for utterances is based on specific dialogue characteristics through separate prompts. Multiple LLMs are used for collaborative code prediction, and a contextual consistency checking method significantly enhances accuracy. The study highlights the importance of understanding communicative acts and events in dialogue analysis, with act predictions consistently outperforming event predictions. This innovative framework provides a scalable solution for addressing contextual challenges in dialogue analysis. <div>
arXiv:2504.19734v1 Announce Type: cross 
Abstract: Dialogue data has been a key source for understanding learning processes, offering critical insights into how students engage in collaborative discussions and how these interactions shape their knowledge construction. The advent of Large Language Models (LLMs) has introduced promising opportunities for advancing qualitative research, particularly in the automated coding of dialogue data. However, the inherent contextual complexity of dialogue presents unique challenges for these models, especially in understanding and interpreting complex contextual information. This study addresses these challenges by developing a novel LLM-assisted automated coding approach for dialogue data. The novelty of our proposed framework is threefold: 1) We predict the code for an utterance based on dialogue-specific characteristics -- communicative acts and communicative events -- using separate prompts following the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We leveraged the interrelation between events and acts to implement consistency checking using GPT-4o. In particular, our contextual consistency checking provided a substantial accuracy improvement. We also found the accuracy of act predictions was consistently higher than that of event predictions. This study contributes a new methodological framework for enhancing the precision of automated coding of dialogue data as well as offers a scalable solution for addressing the contextual challenges inherent in dialogue analysis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets</title>
<link>https://arxiv.org/abs/2410.13105</link>
<guid>https://arxiv.org/abs/2410.13105</guid>
<content:encoded><![CDATA[
<div> DeFi, lending, liquidity pools, interest rate curves, decentralized

Summary:
The article presents a dynamic model for the lending market in Decentralized Finance (DeFi) that addresses inefficiencies and risks in current platforms like Aave and Compound. The proposed model incorporates evolving demand and supply curves along with an adaptive interest rate controller that reacts in real-time to market changes. By using a Recursive Least Squares algorithm, the controller ensures stable utilization and manages default and liquidation risks. The algorithm offers theoretical guarantees on interest rate convergence and utilization stability while reducing vulnerability to adversarial manipulation compared to static curves. Two approaches are suggested to counter adversarial manipulation, including a detection method for extreme fluctuations and a market-based strategy to enhance elasticity. The model's performance is validated through Aave data, demonstrating low best-fit error, and improved utilization and liquidation management compared to static curve protocols. 
<br /><br />Summary: <div>
arXiv:2410.13105v4 Announce Type: replace 
Abstract: Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller tracks the external market and achieves stable utilization, while also controlling default and liquidation risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. We propose two complementary approaches to mitigating adversarial manipulation: an algorithmic method that detects extreme demand and supply fluctuations and a market-based strategy that enhances elasticity, potentially via interest rate derivative markets. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Guide to Misinformation Detection Data and Evaluation</title>
<link>https://arxiv.org/abs/2411.05060</link>
<guid>https://arxiv.org/abs/2411.05060</guid>
<content:encoded><![CDATA[
<div> Keywords: Misinformation, Datasets, Evaluation Quality Assurance, Detection model, Research

Summary:
This study addresses the challenging issue of misinformation by curating a comprehensive collection of datasets, totaling 75, for empirical research. The evaluation of these datasets reveals flaws in many, such as spurious correlations and ambiguous examples, impacting their reliability. State-of-the-art baselines are provided, highlighting the limitations of categorical labels in assessing detection model performance accurately. The authors propose Evaluation Quality Assurance (EQA) as a tool to guide the field towards systemic solutions and improve research quality in misinformation detection. The ultimate goal of this guide is to promote higher quality data and grounded evaluations to enhance the field's understanding and combat misinformation effectively.
<br /><br />Summary: <div>
arXiv:2411.05060v3 Announce Type: replace 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at misinfo-datasets.complexdatalab.com.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Participation in Collective Action from Social Media</title>
<link>https://arxiv.org/abs/2501.07368</link>
<guid>https://arxiv.org/abs/2501.07368</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, collective action, text classifiers, Reddit, computational social science

Summary:
Social media is crucial for mobilizing collective action and understanding individual engagement in global challenges. However, research in this area lacks granular data on participation levels. To address this gap, this study presents text classifiers that can identify participation expressions across different levels, from recognizing issues to active involvement. By training BERT and Llama3 models on Reddit data, the study demonstrates the effectiveness of smaller language models in detecting participation nuances. Applying this methodology to Reddit enables a more robust characterization of online communities compared to existing methods. This framework provides reliable annotations for Computational Social Science research to analyze the dynamics of collective action in online spaces. <div>
arXiv:2501.07368v2 Announce Type: replace 
Abstract: Social media play a key role in mobilizing collective action, holding the potential for studying the pathways that lead individuals to actively engage in addressing global challenges. However, quantitative research in this area has been limited by the absence of granular and large-scale ground truth about the level of participation in collective action among individual social media users. To address this limitation, we present a novel suite of text classifiers designed to identify expressions of participation in collective action from social media posts, in a topic-agnostic fashion. Grounded in the theoretical framework of social movement mobilization, our classification captures participation and categorizes it into four levels: recognizing collective issues, engaging in calls-to-action, expressing intention of action, and reporting active involvement. We constructed a labeled training dataset of Reddit comments through crowdsourcing, which we used to train BERT classifiers and fine-tune Llama3 models. Our findings show that smaller language models can reliably detect expressions of participation (weighted F1=0.71), and rival larger models in capturing nuanced levels of participation. By applying our methodology to Reddit, we illustrate its effectiveness as a robust tool for characterizing online communities in innovative ways compared to topic modeling, stance detection, and keyword-based methods. Our framework contributes to Computational Social Science research by providing a new source of reliable annotations useful for investigating the social dynamics of collective action.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Unknown Social Networks for Discovering Hidden Nodes</title>
<link>https://arxiv.org/abs/2501.12571</link>
<guid>https://arxiv.org/abs/2501.12571</guid>
<content:encoded><![CDATA[
<div> Hidden-node discovery, social networks, graph exploration, machine learning, node embeddings
<br />
Summary:
In this paper, the authors address the challenge of discovering hidden nodes in unknown social networks by formulating three types of hidden-node discovery problems: Sybil-node discovery, peripheral-node discovery, and influencer discovery. They employ a graph exploration framework grounded in machine learning to tackle these problems, constructing prediction models based on the subgraph structure obtained during exploration. Empirical investigations on real social graphs demonstrate the efficiency of graph exploration strategies in uncovering hidden nodes, with query cost multipliers of 1.2 for discovering 10% and 1.4 for discovering 90% of hidden nodes compared to the known topology case. The use of node embeddings for hidden-node discovery is found to be effective in certain scenarios but can degrade efficiency in others. The authors propose a bandit algorithm to combine prediction models using node embeddings with those that do not, showing that this approach achieves efficient node discovery across various settings.
<br /><br />Summary: <div>
arXiv:2501.12571v2 Announce Type: replace 
Abstract: In this paper, we address the challenge of discovering hidden nodes in unknown social networks, formulating three types of hidden-node discovery problems, namely, Sybil-node discovery, peripheral-node discovery, and influencer discovery. We tackle these problems by employing a graph exploration framework grounded in machine learning. Leveraging the structure of the subgraph gradually obtained from graph exploration, we construct prediction models to identify target hidden nodes in unknown social graphs. Through empirical investigations of real social graphs, we investigate the efficiency of graph exploration strategies in uncovering hidden nodes. Our results show that our graph exploration strategies discover hidden nodes with an efficiency comparable to that when the graph structure is known. Specifically, the query cost of discovering 10% of the hidden nodes is at most only 1.2 times that when the topology is known, and the query-cost multiplier for discovering 90% of the hidden nodes is at most only 1.4. Furthermore, our results suggest that using node embeddings, which are low-dimensional vector representations of nodes, for hidden-node discovery is a double-edged sword: it is effective in certain scenarios but sometimes degrades the efficiency of node discovery. Guided by this observation, we examine the effectiveness of using a bandit algorithm to combine the prediction models that use node embeddings with those that do not, and our analysis shows that the bandit-based graph exploration strategy achieves efficient node discovery across a wide array of settings.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Wisdom of Intellectually Humble Networks</title>
<link>https://arxiv.org/abs/2502.02015</link>
<guid>https://arxiv.org/abs/2502.02015</guid>
<content:encoded><![CDATA[
<div> Keywords: collective wisdom, intellectual humility, social networks, agent-based modeling, polarization

Summary: 
This paper examines the impact of intellectual humility on collective wisdom within social networks. Using agent-based modeling and data-calibrated simulations, the researchers demonstrate that intellectual humility can lead to more accurate estimations and reduce polarization in social networks. By fostering a mindset of openness to perspectives and willingness to revise beliefs, intellectual humility helps individuals avoid cognitive and social biases that can hinder collective wisdom. The study shows that interventions promoting intellectual humility can enhance the overall accuracy of group estimations while maintaining cohesion within social networks. The findings are robust across different task settings and network structures, suggesting the potential for practical applications in improving decision-making processes and policy outcomes in democratic societies. By understanding how intellectual humility influences group dynamics, policymakers and stakeholders can leverage this trait to enhance the quality of collective decision-making. 

<br /><br />Summary: <div>
arXiv:2502.02015v2 Announce Type: replace 
Abstract: People's collectively shared beliefs can have significant social implications, including on democratic processes and policies. Unfortunately, as people interact with peers to form and update their beliefs, various cognitive and social biases can hinder their collective wisdom. In this paper, we probe whether and how the psychological construct of intellectual humility can modulate collective wisdom in a networked interaction setting. Through agent-based modeling and data-calibrated simulations, we provide a proof of concept demonstrating that intellectual humility can foster more accurate estimations while mitigating polarization in social networks. We investigate the mechanisms behind the performance improvements and confirm robustness across task settings and network structures. Our work can guide intervention designs to capitalize on the promises of intellectual humility in boosting collective wisdom in social networks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The dynamics of leadership and success in software development teams</title>
<link>https://arxiv.org/abs/2404.18833</link>
<guid>https://arxiv.org/abs/2404.18833</guid>
<content:encoded><![CDATA[
<div> Keywords: teamwork, software development, collaborative processes, leadership change, success growth

Summary: 
Teams in software development projects were analyzed using fine-grained temporal data from Rust, JavaScript, and Python ecosystems. The study found that workload distribution within teams is uneven, with greater heterogeneity leading to higher success rates. A lead developer often emerges early on, shouldering the majority of work. A significant number of projects undergo a change in lead developer, with this transition more likely in projects led by inexperienced users. Leadership changes are associated with faster growth in project success. This research provides insights into the dynamics of online collaborative projects, highlighting the importance of understanding team evolution and its impact on success in collaborative processes.<br /><br />Summary: <div>
arXiv:2404.18833v3 Announce Type: replace-cross 
Abstract: From science to industry, teamwork plays a crucial role in knowledge production and innovation. Most studies consider teams as static groups of individuals, thereby failing to capture how the micro-dynamics of collaborative processes and organizational changes determine team success. Here, we leverage fine-grained temporal data on software development teams from three software ecosystems -- Rust, JavaScript, and Python -- to gain insights into the dynamics of online collaborative projects. Our analysis reveals an uneven workload distribution in teams, with stronger heterogeneity correlated with higher success, and the early emergence of a lead developer carrying out the majority of work. Moreover, we find that a sizeable fraction of projects experience a change of lead developer, with such a transition being more likely in projects led by inexperienced users. Finally, we show that leadership change is associated with faster success growth. Our work contributes to a deeper understanding of the link between team evolution and success in collaborative processes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models for newspaper sentiment analysis during COVID-19: The Guardian</title>
<link>https://arxiv.org/abs/2405.13056</link>
<guid>https://arxiv.org/abs/2405.13056</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, sentiment analysis, newspapers, The Guardian, public response

Summary:<br /><br />During the COVID-19 pandemic, The Guardian newspaper was analyzed for sentiment trends using large language models. The study focused on various stages of the pandemic, including initial transmission, lockdowns, and vaccination. The analysis revealed a shift in public sentiment from urgent crisis response to concerns about health and the economy. The study found a predominance of negative sentiments, such as sadness, annoyance, anxiety, and denial, in The Guardian's coverage, both before and during the pandemic. This contrasts with social media sentiment analyses, which showed a more diverse emotional reflection. Overall, The Guardian portrayed a grim narrative with negative sentiments prevailing across news sections for countries like Australia, the UK, and the world. Sentiment analysis of newspaper sources during COVID-19 can offer insights into how the media covered the pandemic and captured the evolving public response. <br /><br />Summary: <div>
arXiv:2405.13056v2 Announce Type: replace-cross 
Abstract: During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Compounded Burr Probability Distribution for Fitting Heavy-Tailed Data with Applications to Biological Networks</title>
<link>https://arxiv.org/abs/2407.04465</link>
<guid>https://arxiv.org/abs/2407.04465</guid>
<content:encoded><![CDATA[
<div> Keywords: biological networks, scale-free structures, Compounded Burr distribution, heavy-tailed degree distributions, maximum likelihood estimation <br />
Summary: <br />
Complex biological networks often exhibit scale-free structures, but empirical studies show deviations from ideal power law behavior. The Compounded Burr (CBurr) distribution, a novel four parameter family, is proposed to accurately model network degree distributions, with a focus on biological networks. Statistical properties of the CBurr distribution are rigorously derived, and an efficient maximum likelihood estimation framework is developed. The CBurr model showcases broad applicability in various domains, outperforming classical models like power-law and log-normal on biological network datasets. By providing a statistically grounded framework, the CBurr model enhances our understanding of the structural heterogeneity of biological networks. <div>
arXiv:2407.04465v3 Announce Type: replace-cross 
Abstract: Complex biological networks, encompassing metabolic pathways, gene regulatory systems, and protein-protein interaction networks, often exhibit scale-free structures characterized by heavy-tailed degree distributions. However, empirical studies reveal significant deviations from ideal power law behavior, underscoring the need for more flexible and accurate probabilistic models. In this work, we propose the Compounded Burr (CBurr) distribution, a novel four parameter family derived by compounding the Burr distribution with a discrete mixing process. This model is specifically designed to capture both the body and tail behavior of real-world network degree distributions with applications to biological networks. We rigorously derive its statistical properties, including moments, hazard and risk functions, and tail behavior, and develop an efficient maximum likelihood estimation framework. The CBurr model demonstrates broad applicability to networks with complex connectivity patterns, particularly in biological, social, and technological domains. Extensive experiments on large-scale biological network datasets show that CBurr consistently outperforms classical power-law, log-normal, and other heavy-tailed models across the full degree spectrum. By providing a statistically grounded and interpretable framework, the CBurr model enhances our ability to characterize the structural heterogeneity of biological networks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Collective Accuracy in Socially Connected Networks</title>
<link>https://arxiv.org/abs/2411.08625</link>
<guid>https://arxiv.org/abs/2411.08625</guid>
<content:encoded><![CDATA[
<div> network-mediated influence, collective decision-making, social networks, binary choices, group performance 

Summary:
In this study, the accuracy of collective decision-making in socially connected populations was analyzed. Agents in the network update binary choices based on private signals that are slightly biased towards the correct alternative. Through local interactions on the network, social influence plays a crucial role in aggregating these signals. The research found that in large-population scenarios, the probability of a correct majority converges to a specific mathematical expression involving the regularized incomplete beta function. Surprisingly, the collective accuracy exceeds that of individual agents when private signals are better than random, indicating that network-mediated influence can improve group performance. These results have implications for designing resilient decision-making systems in various networks, including social, biological, and engineered systems, where accuracy relies on the interactions of interdependent and noisy agents.<br /><br />Summary: <div>
arXiv:2411.08625v2 Announce Type: replace-cross 
Abstract: We analyze the accuracy of collective decision-making in socially connected populations, where agents update binary choices through local interactions on a network. Each agent receives a private signal that is biased -- even marginally -- toward the correct alternative, and social influence mediates the aggregation of these signals. We show analytically that, in the large-population limit, the probability of a correct majority converges to a nontrivial expression involving the regularized incomplete beta function. Remarkably, this collective accuracy surpasses that of any individual agent whenever private signals are better than random, revealing that network-mediated influence can enhance, rather than impair, group performance. Our findings may inform the design of resilient decision-making systems in social, biological, and engineered networks, where accuracy must emerge from interdependent and noisy agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Egocentric Temporal Neighborhoods to probe for spatial correlations in temporal networks and infer their topology</title>
<link>https://arxiv.org/abs/2501.16070</link>
<guid>https://arxiv.org/abs/2501.16070</guid>
<content:encoded><![CDATA[
<div> Keywords: motifs, temporal networks, triangles, edge-centered motifs, graph tiling theory

Summary:
Motifs are fundamental components of social face-to-face interaction temporal networks, with traditional motifs lacking in inclusivity or efficiency. This study introduces edge-centered motifs that encompass triangles and can be efficiently mined in any temporal network. Analytical comparisons with Egocentric Temporal Neighborhoods motifs show that edge-centered motifs provide relevant information. Empirical data analysis supports the significance of edge-centered motifs in probing spatial correlations in network dynamics. The distribution of edge-centered motifs in social face-to-face interaction networks is approximated. Exploration of using edge-centered motif statistics to infer complete network topology leads to the development of graph tiling theory, a new mathematical framework. The study highlights the importance of considering triangles and spatial correlations in understanding social systems through temporal networks. 

<br /><br />Summary: <div>
arXiv:2501.16070v2 Announce Type: replace-cross 
Abstract: Motifs are thought to be some fundamental components of social face-to-face interaction temporal networks. However, the motifs previously considered are either limited to a handful of nodes and edges, or do not include triangles, which are thought to be of critical relevance to understand the dynamics of social systems. Thus, we introduce a new class of motifs, that include these triangles, are not limited in their number of nodes or edges, and yet can be mined efficiently in any temporal network. Referring to these motifs as the edge-centered motifs, we show analytically how they subsume the Egocentric Temporal Neighborhoods motifs of the literature. We also confirm in empirical data that the edge-centered motifs bring relevant information with respect to the Egocentric motifs by using a principle of maximum entropy. Then, we show how mining for the edge-centered motifs in a network can be used to probe for spatial correlations in the underlying dynamics that have produced that network. We deduce an approximate formula for the distribution of the edge-centered motifs in empirical networks of social face-to-face interactions. In the last section of this paper, we explore how the statistics of the edge-centered motifs can be used to infer the complete topology of the network they were sampled from. This leads to the needs of mathematical development, that we inaugurate here under the name of graph tiling theory.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arab Spring's Impact on Science through the Lens of Scholarly Attention, Funding, and Migration</title>
<link>https://arxiv.org/abs/2503.13238</link>
<guid>https://arxiv.org/abs/2503.13238</guid>
<content:encoded><![CDATA[
<div> Keywords: Arab Spring, scholarly attention, academic discourse, funding, migration networks 

Summary: 
The study examines the impact of the Arab Spring on scholarly attention in 10 countries in the Middle East and North Africa region. Using a difference-in-difference statistical framework on over 25 million articles published from 2002 to 2019, the researchers found that most target countries experienced a significant increase in scholarly attention post-Arab Spring compared to other regions, with Egypt garnering the most attention. The study also delves into the role of funding and migration networks in shaping scholarly attention, highlighting Saudi Arabia as a key player in attracting researchers and funding projects in the region. This research sheds light on the evolving academic landscape in the aftermath of the Arab Spring and underscores the importance of analyzing the influence of socio-political movements on scholarly discourse in the region. 

<br /><br />Summary: <div>
arXiv:2503.13238v2 Announce Type: replace-cross 
Abstract: The Arab Spring is a major socio-political movement that reshaped democratic aspirations in the Middle East and North Africa, attracting global attention through news, social media, and academic discourse. However, its consequences on the academic landscape in the region are still unclear. Here, we conduct the first study of scholarly attention toward 10 target countries affected by the Arab Spring by analyzing more than 25 million articles published from 2002 to 2019. Using a difference-in-difference statistical framework, we find that most target countries have experienced a significant increase in scholarly attention post-Arab Spring compared to the rest of the world, with Egypt attracting the most attention. We investigate how funding and migration networks relate to scholarly attention and reveal that Saudi Arabia has emerged as a key player among Western nations by attracting researchers and funding projects that shape research on the region.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflection on Code Contributor Demographics and Collaboration Patterns in the Rust Community</title>
<link>https://arxiv.org/abs/2503.22066</link>
<guid>https://arxiv.org/abs/2503.22066</guid>
<content:encoded><![CDATA[
<div> GitHub, pull request, Rust, diversity, social network analysis
Summary:
The study examines the demographic composition and interaction patterns of contributors in the Rust programming language ecosystem, focusing on key projects like Rust, Rust Analyzer, and Cargo. Using GitHub pull request data and social network analysis, disparities in gender and geographic representation among pivotal contributors are revealed, highlighting the need for more inclusive practices. The results suggest that while the Rust community is globally active, the contributor base lacks diversity, indicating a disconnect with the wider user community. To encourage broader participation and ensure alignment with the diverse global community, inclusive measures are necessary within the Rust ecosystem. <div>
arXiv:2503.22066v3 Announce Type: replace-cross 
Abstract: Open-source software communities thrive on global collaboration and contributions from diverse participants. This study explores the Rust programming language ecosystem to understand its contributors' demographic composition and interaction patterns. Our objective is to investigate the phenomenon of participation inequality in key Rust projects and the presence of diversity among them. We studied GitHub pull request data from the year leading up to the release of the latest completed Rust community annual survey in 2023. Specifically, we extracted information from three leading repositories: Rust, Rust Analyzer, and Cargo, and used social network graphs to visualize the interactions and identify central contributors and sub-communities. Social network analysis has shown concerning disparities in gender and geographic representation among contributors who play pivotal roles in collaboration networks and the presence of varying diversity levels in the sub-communities formed. These results suggest that while the Rust community is globally active, the contributor base does not fully reflect the diversity of the wider user community. We conclude that there is a need for more inclusive practices to encourage broader participation and ensure that the contributor base aligns more closely with the diverse global community that utilizes Rust.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learning on Large Graphs using a Densifying Regularity Lemma</title>
<link>https://arxiv.org/abs/2504.18273</link>
<guid>https://arxiv.org/abs/2504.18273</guid>
<content:encoded><![CDATA[
<div> Keywords: large graphs, Intersecting Block Graph (IBG), weak regularity lemma, graph neural network, node classification<br />
Summary: 
Large graphs pose challenges for traditional Message Passing Neural Networks due to computational and memory costs scaling linearly with the number of edges. The Intersecting Block Graph (IBG) is introduced as a low-rank factorization of large directed graphs, based on intersecting bipartite components. By weighting non-edges less, any graph can be efficiently approximated by a dense IBG. A constructive version of the weak regularity lemma is proven, showing that any graph can be approximated by a dense IBG with rank depending only on the chosen accuracy. A graph neural network architecture operating on the IBG representation shows competitive performance on node classification, spatio-temporal graph analysis, and knowledge graph completion with linear memory and computational complexity in the number of nodes. This approach offers a more efficient solution for learning on large graphs compared to traditional methods. <br /><br /> <div>
arXiv:2504.18273v1 Announce Type: new 
Abstract: Learning on large graphs presents significant challenges, with traditional Message Passing Neural Networks suffering from computational and memory costs scaling linearly with the number of edges. We introduce the Intersecting Block Graph (IBG), a low-rank factorization of large directed graphs based on combinations of intersecting bipartite components, each consisting of a pair of communities, for source and target nodes. By giving less weight to non-edges, we show how to efficiently approximate any graph, sparse or dense, by a dense IBG. Specifically, we prove a constructive version of the weak regularity lemma, showing that for any chosen accuracy, every graph, regardless of its size or sparsity, can be approximated by a dense IBG whose rank depends only on the accuracy. This dependence of the rank solely on the accuracy, and not on the sparsity level, is in contrast to previous forms of the weak regularity lemma. We present a graph neural network architecture operating on the IBG representation of the graph and demonstrating competitive performance on node classification, spatio-temporal graph analysis, and knowledge graph completion, while having memory and computational complexity linear in the number of nodes rather than edges.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Algorithmic Audits of TikTok: Poor Reproducibility and Short-term Validity of Findings</title>
<link>https://arxiv.org/abs/2504.18140</link>
<guid>https://arxiv.org/abs/2504.18140</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, algorithmic audits, reproducibility, generalizability, TikTok

Summary:
In this study, the researchers examine the reproducibility of previous sockpuppeting audits on TikTok's recommender systems and the generalizability of their findings. They encounter challenges due to changes in the platform and content evolution, as well as limitations in the previous research methods. The experiments reveal that one-shot audit findings may only hold in the short term, emphasizing the importance of reproducible audits to track changes over time. The audit reproducibility relies heavily on methodological choices and the state of algorithms and content on the platform. This highlights the need for systematic social media algorithmic audits to ensure users are not confined to filter bubbles and not exposed to problematic content. <div>
arXiv:2504.18140v1 Announce Type: cross 
Abstract: Social media platforms are constantly shifting towards algorithmically curated content based on implicit or explicit user feedback. Regulators, as well as researchers, are calling for systematic social media algorithmic audits as this shift leads to enclosing users in filter bubbles and leading them to more problematic content. An important aspect of such audits is the reproducibility and generalisability of their findings, as it allows to draw verifiable conclusions and audit potential changes in algorithms over time. In this work, we study the reproducibility of the existing sockpuppeting audits of TikTok recommender systems, and the generalizability of their findings. In our efforts to reproduce the previous works, we find multiple challenges stemming from social media platform changes and content evolution, but also the research works themselves. These drawbacks limit the audit reproducibility and require an extensive effort altogether with inevitable adjustments to the auditing methodology. Our experiments also reveal that these one-shot audit findings often hold only in the short term, implying that the reproducibility and generalizability of the audits heavily depend on the methodological choices and the state of algorithms and content on the platform. This highlights the importance of reproducible audits that allow us to determine how the situation changes in time.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Freshness in Dynamic Gossip Networks</title>
<link>https://arxiv.org/abs/2504.18504</link>
<guid>https://arxiv.org/abs/2504.18504</guid>
<content:encoded><![CDATA[
<div> version age of information, time-varying connections, network topology, continuous time Markov chain, information freshness

Summary:
- The article examines a source sharing updates with a network of gossiping nodes where the network's topology switches between two arbitrary topologies based on a continuous time Markov chain (CTMC).
- It assesses the impact of time-varying connections on information freshness using the version age of information metric.
- If the two networks have differing static long-term average version ages, the version age of the varying-topologies network is influenced by the transition rates in the CTMC.
- When the CTMC transition rates exceed the faster of the two static network's average version ages, the average version age of the varying-topologies network aligns with the faster average version age.
- The behavior of a small fraction of nodes can significantly affect the network's long-term average version age negatively, leading to the definition of a typical set of nodes.
- The study also evaluates the impact of fast and slow CTMC transition rates on this typical set of nodes.<br /><br />Summary: <div>
arXiv:2504.18504v1 Announce Type: cross 
Abstract: We consider a source that shares updates with a network of $n$ gossiping nodes. The network's topology switches between two arbitrary topologies, with switching governed by a two-state continuous time Markov chain (CTMC) process. Information freshness is well-understood for static networks. This work evaluates the impact of time-varying connections on information freshness. In order to quantify the freshness of information, we use the version age of information metric. If the two networks have static long-term average version ages of $f_1(n)$ and $f_2(n)$ with $f_1(n) \ll f_2(n)$, then the version age of the varying-topologies network is related to $f_1(n)$, $f_2(n)$, and the transition rates in the CTMC. If the transition rates in the CTMC are faster than $f_1(n)$, the average version age of the varying-topologies network is $f_1(n)$. Further, we observe that the behavior of a vanishingly small fraction of nodes can severely impact the long-term average version age of a network in a negative way. This motivates the definition of a typical set of nodes in the network. We evaluate the impact of fast and slow CTMC transition rates on the typical set of nodes.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing The Secret Power: How Algorithms Can Influence Content Visibility on Social Media</title>
<link>https://arxiv.org/abs/2410.17390</link>
<guid>https://arxiv.org/abs/2410.17390</guid>
<content:encoded><![CDATA[
<div> transparency, social networks, recommendation algorithms, shadow banning, public discourse <br />
<br />
Summary: This paper explores the impact of visibility alterations on Twitter discussions regarding the Ukraine-Russia conflict and the 2024 US Presidential Elections. The study, based on two large datasets, reveals that tweets containing external links are systematically penalized in terms of visibility, regardless of their ideological stance or reliability. The algorithm also appears to favor or penalize content based on the specific accounts producing it, as evident in the comparison between tweets from different sources or political figures. The findings underscore the critical need for transparency in content moderation and recommendation systems to safeguard public discourse integrity and provide fair access to online platforms. <div>
arXiv:2410.17390v2 Announce Type: replace 
Abstract: In recent years, the opaque design and the limited public understanding of social networks' recommendation algorithms have raised concerns about potential manipulation of information exposure. While reducing content visibility, aka shadow banning, may help limit harmful content, it can also be used to suppress dissenting voices. This prompts the need for greater transparency and a better understanding of this practice.
  In this paper, we investigate the presence of visibility alterations through a large-scale quantitative analysis of two Twitter/X datasets comprising over 40 million tweets from more than 9 million users, focused on discussions surrounding the Ukraine-Russia conflict and the 2024 US Presidential Elections. We use view counts to detect patterns of reduced or inflated visibility and examine how these correlate with user opinions, social roles, and narrative framings. Our analysis shows that the algorithm systematically penalizes tweets containing links to external resources, reducing their visibility by up to a factor of eight, regardless of the ideological stance or source reliability. Rather, content visibility may be penalized or favored depending on the specific accounts producing it, as observed when comparing tweets from the Kyiv Independent and RT.com or tweets by Donald Trump and Kamala Harris. Overall, our work highlights the importance of transparency in content moderation and recommendation systems in protecting the integrity of public discourse and ensuring equitable access to online platforms.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forensics and security issues in the Internet of Things</title>
<link>https://arxiv.org/abs/2309.02707</link>
<guid>https://arxiv.org/abs/2309.02707</guid>
<content:encoded><![CDATA[
<div> Keywords: IoT, forensics, security, smart home system, blockchain-based authentication mechanism

Summary: 
IoT devices face security challenges due to the lack of standardized measures, making them vulnerable to cyberattacks. Unauthorized access can lead to data compromises and control over critical infrastructure. To address these issues, a FLIP-based smart home system can be developed with security-conscious design. Implementing a blockchain-based authentication mechanism with a multi-chain structure can enhance security between trust domains. Deep learning can aid in creating a network forensics framework for effective detection and tracking of cyberattacks. Limiting data usage in big data applications for IoT-based systems is crucial. Researchers are encouraged to explore solutions to these challenges to advance the IoT field.<br /><br />Summary: <div>
arXiv:2309.02707v2 Announce Type: replace-cross 
Abstract: Given the exponential expansion of the internet, the possibilities of security attacks and cybercrimes have increased accordingly. However, poorly implemented security mechanisms in the Internet of Things (IoT) devices make them susceptible to cyberattacks, which can directly affect users. IoT forensics is thus needed to investigate and mitigate such attacks. While many works have examined IoT applications and challenges, only a few have focused on both the forensic and security issues in IoT. Therefore, this paper reviews forensic and security issues associated with IoT in different fields. Prospects and challenges in IoT research and development are also highlighted. As the literature demonstrates, most IoT devices are vulnerable to attacks due to a lack of standardized security measures. Unauthorized users could get access, compromise data, and even benefit from control of critical infrastructure. To fulfill the security-conscious needs of consumers, IoT can be used to develop a smart home system by designing the security-conscious needs of consumers; IoT can be used to create a smart home system by designing an IoT can be used to develop a smart home system by designing a FLIP-based system that is highly scalable and adaptable. A blockchain-based authentication mechanism with a multi-chain structure can provide additional security protection between different trust domains. Deep learning can be utilized to develop a network forensics framework with a high-performing system for detecting and tracking cyberattack incidents. Moreover, researchers should consider limiting the amount of data created and delivered when using big data to develop IoT-based smart systems. The findings of this review will stimulate academics to seek potential solutions for the identified issues, thereby advancing the IoT field.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-driven embedding of networks in hyperbolic space</title>
<link>https://arxiv.org/abs/2406.10711</link>
<guid>https://arxiv.org/abs/2406.10711</guid>
<content:encoded><![CDATA[
<div> Bayesian hyperbolic random graph model, MCMC algorithm, uncertainty quantification, embeddings, network properties <br />
<br />
Summary: BIGUE is a novel MCMC algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model, allowing for uncertainty quantification in inferred network coordinates. This algorithm can provide credible intervals for coordinates and network properties, offering a more comprehensive analysis than current algorithms. It highlights that some networks may have multiple plausible embeddings, a factor that traditional optimization methods might overlook. BIGUE's exploration of the posterior distribution aligns with existing algorithms while enhancing the understanding of hyperbolic network models. <div>
arXiv:2406.10711v2 Announce Type: replace-cross 
Abstract: Hyperbolic models are known to produce networks with properties observed empirically in most network datasets, including heavy-tailed degree distribution, high clustering, and hierarchical structures. As a result, several embeddings algorithms have been proposed to invert these models and assign hyperbolic coordinates to network data. Current algorithms for finding these coordinates, however, do not quantify uncertainty in the inferred coordinates. We present BIGUE, a Markov chain Monte Carlo (MCMC) algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model. We show that the samples are consistent with current algorithms while providing added credible intervals for the coordinates and all network properties. We also show that some networks admit two or more plausible embeddings, a feature that an optimization algorithm can easily overlook.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2Vec: Self-Supervised Geospatial Embeddings</title>
<link>https://arxiv.org/abs/2504.16942</link>
<guid>https://arxiv.org/abs/2504.16942</guid>
<content:encoded><![CDATA[
<div> S2Vec, geospatial embeddings, S2 Geometry library, self-supervised framework, rasterization, masked autoencoding

Summary: 
S2Vec is introduced as a self-supervised framework for creating geospatial embeddings using the S2 Geometry library to partition areas into cells, rasterizing feature vectors within cells as images, and applying masked autoencoding for encoding. These embeddings capture local feature characteristics and spatial relationships for various geospatial tasks. The framework is evaluated on socioeconomic prediction tasks, demonstrating competitive performance against image-based embeddings. Combining S2Vec with image-based embeddings through multimodal fusion shows improved performance. The results showcase S2Vec's ability to learn effective general-purpose geospatial representations and complement other data modalities in geospatial artificial intelligence.<br /><br />Summary: <div>
arXiv:2504.16942v1 Announce Type: new 
Abstract: Scalable general-purpose representations of the built environment are crucial for geospatial artificial intelligence applications. This paper introduces S2Vec, a novel self-supervised framework for learning such geospatial embeddings. S2Vec uses the S2 Geometry library to partition large areas into discrete S2 cells, rasterizes built environment feature vectors within cells as images, and applies masked autoencoding on these rasterized images to encode the feature vectors. This approach yields task-agnostic embeddings that capture local feature characteristics and broader spatial relationships. We evaluate S2Vec on three large-scale socioeconomic prediction tasks, showing its competitive performance against state-of-the-art image-based embeddings. We also explore the benefits of combining S2Vec embeddings with image-based embeddings downstream, showing that such multimodal fusion can often improve performance. Our results highlight how S2Vec can learn effective general-purpose geospatial representations and how it can complement other data modalities in geospatial artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Burning some myths on privacy properties of social networks against active attacks</title>
<link>https://arxiv.org/abs/2504.16944</link>
<guid>https://arxiv.org/abs/2504.16944</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, privacy, active attacks, (k,l)-anonymity, graph theory <br />
Summary: <br />
The research focuses on challenging the common belief that social networks have no privacy. It discusses active attacks on social network privacy and the (k,l)-anonymity measure to quantify privacy against such attacks. Utilizing the concept of k-metric antidimensional graphs, the study highlights that social networks may not be as insecure as believed. Computational experiments on random and real networks reveal that only a small number of graphs are vulnerable to privacy breaches. The research also explores mathematical properties of k-metric antidimensional graphs and proposes operations to obscure privacy vulnerabilities in graphs. This work contributes to reevaluating the privacy risks in social networks and offers insights into enhancing privacy protection against active attacks. <br /> <div>
arXiv:2504.16944v1 Announce Type: new 
Abstract: This work focuses on showing some arguments addressed to dismantle the extended idea about that social networks completely lacks of privacy properties. We consider the so-called active attacks to the privacy of social networks and the counterpart $(k,\ell)$-anonymity measure, which is used to quantify the privacy satisfied by a social network against active attacks. To this end, we make use of the graph theoretical concept of $k$-metric antidimensional graphs for which the case $k=1$ represents those graphs achieving the worst scenario in privacy whilst considering the $(k,\ell)$-anonymity measure.
  As a product of our investigation, we present a large number of computational results stating that social networks might not be as insecure as one often thinks. In particular, we develop a large number of experiments on random graphs which show that the number of $1$-metric antidimensional graphs is indeed ridiculously small with respect to the total number of graphs that can be considered. Moreover, we search on several real networks in order to check if they are $1$-metric antidimensional, and obtain that none of them are such. Along the way, we show some theoretical studies on the mathematical properties of the $k$-metric antidimensional graphs for any suitable $k\ge 1$. In addition, we also describe some operations on graphs that are $1$-metric antidimensional so that they get embedded into another larger graphs that are not such, in order to obscure their privacy properties against active attacks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation</title>
<link>https://arxiv.org/abs/2504.16946</link>
<guid>https://arxiv.org/abs/2504.16946</guid>
<content:encoded><![CDATA[
<div> simulation framework, urban mobility, behavioral choices, transportation modes, population groups
<br />
The article introduces a new simulation framework for generative agents to simulate realistic urban behaviors. It addresses the limitations of existing methods by incorporating multiple functional buildings and transportation modes in a virtual city. Extensive surveys were conducted to model behavioral choices and mobility preferences among population groups, resulting in a scalable framework capable of simulating over 4,000 agents. The framework captures the complexity of urban mobility while enabling in-depth analyses of crowd density prediction and vehicle preferences across agent demographics. The realism of the generated behaviors was assessed through micro and macro-level analyses, showcasing the framework's ability to simulate large-scale urban scenarios with accuracy and efficiency.
<br /><br />Summary: <div>
arXiv:2504.16946v1 Announce Type: new 
Abstract: Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices in modern cities, and require prohibitive computational resources for large-scale population simulation. To address these limitations, we first present a virtual city that features multiple functional buildings and transportation modes. Then, we conduct extensive surveys to model behavioral choices and mobility preferences among population groups. Building on these insights, we introduce a simulation framework that captures the complexity of urban mobility while remaining scalable, enabling the simulation of over 4,000 agents. To assess the realism of the generated behaviors, we perform a series of micro and macro-level analyses. Beyond mere performance comparison, we explore insightful experiments, such as predicting crowd density from movement patterns and identifying trends in vehicle preferences across agent demographics.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments</title>
<link>https://arxiv.org/abs/2504.16947</link>
<guid>https://arxiv.org/abs/2504.16947</guid>
<content:encoded><![CDATA[
<div> Framework, prediction, social computing, community responses, sentiment<br />
<br />
Summary: <br />
SCRAG is a prediction framework inspired by social computing, designed to forecast community responses to social media posts. It integrates large language models with a Retrieval-Augmented Generation technique to overcome challenges like reliance on static training data and hallucinations. By retrieving historical responses and external knowledge, SCRAG can accurately predict how a community will respond to new narratives. Extensive experiments on the X platform show over 10% improvements in key metrics. The framework is effective in capturing diverse ideologies and nuances, providing concrete insights into community responses for applications like public sentiment prediction and crisis management. <div>
arXiv:2504.16947v1 Announce Type: new 
Abstract: This paper introduces SCRAG, a prediction framework inspired by social computing, designed to forecast community responses to real or hypothetical social media posts. SCRAG can be used by public relations specialists (e.g., to craft messaging in ways that avoid unintended misinterpretations) or public figures and influencers (e.g., to anticipate social responses), among other applications related to public sentiment prediction, crisis management, and social what-if analysis. While large language models (LLMs) have achieved remarkable success in generating coherent and contextually rich text, their reliance on static training data and susceptibility to hallucinations limit their effectiveness at response forecasting in dynamic social media environments. SCRAG overcomes these challenges by integrating LLMs with a Retrieval-Augmented Generation (RAG) technique rooted in social computing. Specifically, our framework retrieves (i) historical responses from the target community to capture their ideological, semantic, and emotional makeup, and (ii) external knowledge from sources such as news articles to inject time-sensitive context. This information is then jointly used to forecast the responses of the target community to new posts or narratives. Extensive experiments across six scenarios on the X platform (formerly Twitter), tested with various embedding models and LLMs, demonstrate over 10% improvements on average in key evaluation metrics. A concrete example further shows its effectiveness in capturing diverse ideologies and nuances. Our work provides a social computing tool for applications where accurate and concrete insights into community responses are crucial.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Sampling: An Overview and Comparative Analysis</title>
<link>https://arxiv.org/abs/2504.17701</link>
<guid>https://arxiv.org/abs/2504.17701</guid>
<content:encoded><![CDATA[
<div> node-based, edge-based, exploration-based, network sampling, temporal networks

Summary:
- Network sampling is essential for analyzing large or partially observable networks.
- Different sampling methods vary in effectiveness depending on the context.
- Empirical comparison of node-based, edge-based, and exploration-based sampling methods.
- Advanced methods show better accuracy on static networks, while simpler techniques are more effective for temporal networks.
- The choice of sampling strategy should consider both network structure and analytical objectives. 

<br /><br />Summary: <div>
arXiv:2504.17701v1 Announce Type: new 
Abstract: Network sampling is a crucial technique for analyzing large or partially observable networks. However, the effectiveness of different sampling methods can vary significantly depending on the context. In this study, we empirically compare representative methods from three main categories: node-based, edge-based, and exploration-based sampling. We used two real-world datasets for our analysis: a scientific collaboration network and a temporal message-sending network. Our results indicate that no single sampling method consistently outperforms the others in both datasets. Although advanced methods tend to provide better accuracy on static networks, they often perform poorly on temporal networks, where simpler techniques can be more effective. These findings suggest that the best sampling strategy depends not only on the structural characteristics of the network but also on the specific metrics that need to be preserved or analyzed. Our work offers practical insights for researchers in choosing sampling approaches that are tailored to different types of networks and analytical objectives.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Percolation as Decision Threshold for Risk Management in Cross-Country Thermal Soaring</title>
<link>https://arxiv.org/abs/2504.16945</link>
<guid>https://arxiv.org/abs/2504.16945</guid>
<content:encoded><![CDATA[
<div> Keywords: soaring, updrafts, graph percolation, risk management, flight logs

Summary:
Graph percolation theory is used to analyze the risk of failure to find updrafts for long-range flight by fixed-wing aircraft without propulsion systems. By evaluating flight logs from human soaring pilots, it is found that pilots rarely fly in conditions that do not satisfy graph percolation, indicating a desired minimum node degree. Pilots accept reduced climb rates to maintain percolation, showcasing the importance of this risk measure in optimizing speed and managing risk in soaring flight. The uncertainty of updraft locations underscores the necessity of determining when to exploit updrafts for continued flight, crucial for successful long-distance soaring. The study highlights the practical utility of graph percolation as a tool for evaluating and improving in-flight decision-making in soaring aircraft, aiding pilots in maximizing their efficiency and safety during flight.<br /><br />Summary: <div>
arXiv:2504.16945v1 Announce Type: cross 
Abstract: Long range flight by fixed-wing aircraft without propulsion systems can be accomplished by "soaring" -- exploiting randomly located updrafts to gain altitude which is expended in gliding flight. As the location of updrafts is uncertain and cannot be determined except through in situ observation, aircraft exploiting this energy source are at risk of failing to find a subsequent updraft. Determining when an updraft must be exploited to continue flight is essential to managing risk and optimizing speed. Graph percolation offers a theoretical explanation for this risk, and a framework for evaluating it using information available to the operator of a soaring aircraft in flight. The utility of graph percolation as a risk measure is examined by analyzing flight logs from human soaring pilots. This analysis indicates that in sport soaring pilots rarely operate in a condition which does not satisfy graph percolation, identifies an apparent desired minimum node degree, and shows that pilots accept reduced climb rates in order to maintain percolation.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social sustainability through engagement in a training context with tools such as the Native Podcast and Facebook social network</title>
<link>https://arxiv.org/abs/2504.16964</link>
<guid>https://arxiv.org/abs/2504.16964</guid>
<content:encoded><![CDATA[
<div> Keywords: sustainability, social dimension, EUTIC 2023 symposium, engagement process, digital tools

Summary:
The article addresses the previously neglected social dimension of sustainability within literature, with a particular focus on the upcoming EUTIC 2023 symposium. It introduces an engagement process designed to promote sustainable development using digital tools that are inspired by everyday life and cater to lifelong learning in training contexts. Rooted in information and communication sciences, the work advocates for a multi-disciplinary approach that can be applied across various disciplines. The authors aim to challenge current perspectives and generate insights on the intersection of digital tools, sustainability, and lifelong learning. This innovative approach holds promise for addressing sustainability concerns and promoting social well-being through interactive and accessible digital solutions. <div>
arXiv:2504.16964v1 Announce Type: cross 
Abstract: The social dimension of sustainability seems to have been a notion rarely addressed in the literature (Dubois et al., 2001) until the early 2000s. The EUTIC 2023 symposium provides an opportunity to take up this topical issue. To this end, we are presenting an engagement process that is part of a sustainable development dynamic, based on digital tools inspired by everyday life, for applications in the context of training, with a view to lifelong learning. Our work, which stems from the information and communication sciences, is rooted in a multi-disciplinary approach that we believe can be echoed in a variety of disciplines, but which it is interesting to challenge, hence the purpose of this contribution.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2504.17099</link>
<guid>https://arxiv.org/abs/2504.17099</guid>
<content:encoded><![CDATA[
<div> geometric information, entity representations, knowledge graphs, RDF2Vec, location-aware embeddings
Summary:
- The paper introduces a new approach that incorporates geometric information to learn location-aware entity embeddings.
- Existing methods for learning entity representations do not consider geometries stored in knowledge graphs.
- The approach expands nodes by flooding the graph from geographic nodes to ensure all reachable nodes are considered.
- A modified version of RDF2Vec, biased with spatial weights from the flooded graph, is then applied to learn location-aware embeddings.
- Evaluations on benchmark datasets show that the proposed approach outperforms non-location-aware RDF2Vec and GeoTransE.<br /><br />Summary: <div>
arXiv:2504.17099v1 Announce Type: cross 
Abstract: Many knowledge graphs contain a substantial number of spatial entities, such as cities, buildings, and natural landmarks. For many of these entities, exact geometries are stored within the knowledge graphs. However, most existing approaches for learning entity representations do not take these geometries into account. In this paper, we introduce a variant of RDF2Vec that incorporates geometric information to learn location-aware embeddings of entities. Our approach expands different nodes by flooding the graph from geographic nodes, ensuring that each reachable node is considered. Based on the resulting flooded graph, we apply a modified version of RDF2Vec that biases graph walks using spatial weights. Through evaluations on multiple benchmark datasets, we demonstrate that our approach outperforms both non-location-aware RDF2Vec and GeoTransE.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Dynamic Time Warping for Pandemic Surveillance: Understanding the Relationship between Google Trends Network Metrics and COVID-19 Incidences</title>
<link>https://arxiv.org/abs/2504.17146</link>
<guid>https://arxiv.org/abs/2504.17146</guid>
<content:encoded><![CDATA[
<div> Keywords: network statistics, Google Trends data, COVID-19 disease progression, dynamic time warping, infodemiology

Summary: 
The study utilized network statistics derived from Google Trends data to predict COVID-19 progression in Metro Manila, Philippines. By applying dynamic time warping (DTW), the temporal alignment between network metrics and COVID-19 case trajectories was measured. The study explored various parameters and found that network density and data preprocessing methods significantly influenced alignment quality. The optimal configuration, using network density with Rescaling Daily Data transformation, achieved substantial temporal alignment with COVID-19 confirmed cases data. The findings suggest that online search behavior can be a useful indicator for epidemic surveillance in urban areas like Metro Manila, leveraging the Philippines' high online usage during the pandemic. This approach offers a valuable tool for early disease spread detection and complements public health monitoring in resource-limited settings. 

<br /><br />Summary: <div>
arXiv:2504.17146v1 Announce Type: cross 
Abstract: The premise of network statistics derived from Google Trends data to foresee COVID-19 disease progression is gaining momentum in infodemiology. This approach was applied in Metro Manila, National Capital Region, Philippines. Through dynamic time warping (DTW), the temporal alignment was quantified between network metrics and COVID-19 case trajectories, and systematically explored 320 parameter configurations including two network metrics (network density and clustering coefficient), two data preprocessing methods (Rescaling Daily Data and MSV), multiple thresholds, two correlation window sizes, and Sakoe-Chiba band constraints. Results from the Kruskal-Wallis tests revealed that five of the six parameters significantly influenced alignment quality, with the disease comparison type (active cases vs. confirmed cases) demonstrating the strongest effect. The optimal configuration, which is using the network density statistic with a Rescaling Daily Data transformation, a threshold of 0.8, a 15-day window, and a 50-day radius constraint, achieved a DTW score of 36.30. This indicated substantial temporal alignment with the COVID-19 confirmed cases data. The discoveries demonstrate that network metrics rooted from online search behavior can serve as complementary indicators for epidemic surveillance in urban locations like Metro Manila. This strategy leverages the Philippines' extensive online usage during the pandemic to provide potentially valuable early signals of disease spread, and offers a supplementary tool for public health monitoring in resource-limited situations.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online posting effects: Unveiling the non-linear journeys of users in depression communities on Reddit</title>
<link>https://arxiv.org/abs/2311.17684</link>
<guid>https://arxiv.org/abs/2311.17684</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, depression, online self-disclosure, mental health discourse, Reddit

Summary:
Through an analysis of user interactions on Reddit boards focused on depression, this study investigates the consequences of online self-disclosure on mental health discourse. Utilizing a data-informed framework, the research identifies 4 distinct clusters representing different psychological states among users. The findings reveal that online exposure to peers' emotional and semantic content can lead to transitions in users' psychological states. These transitions do not follow a linear progression but rather a spiral journey through positive and negative phases. The study suggests that the type and layout of online social interactions play a significant role in users' experiences and outcomes when discussing depression. The insights gained from this research can contribute to understanding the impact of online social platforms as self-help forums for individuals seeking support for mental health issues. 

<br /><br />Summary: Through an analysis of user interactions on Reddit boards focused on depression, this study identified 4 distinct clusters representing different psychological states. Online exposure to peers' content can lead to transitions in users' psychological states in a spiral journey. The study suggests that the type and layout of online social interactions play a significant role in users' experiences and outcomes when discussing depression. <div>
arXiv:2311.17684v3 Announce Type: replace 
Abstract: Social media platforms have become pivotal as self-help forums, enabling individuals to share personal experiences and seek support. However, on topics as sensitive as depression, what are the consequences of online self-disclosure? Here, we delve into the dynamics of mental health discourse on various Reddit boards focused on depression. To this aim, we introduce a data-informed framework reconstructing online dynamics from 303k users interacting over two years. Through user-generated content, we identify 4 distinct clusters representing different psychological states. Our analysis unveils online posting effects: a user can transition to another psychological state after online exposure to peers' emotional/semantic content. As described by conditional Markov chains and different levels of social exposure, users' transitions reveal navigation through both positive and negative phases in a spiral rather than a linear progression. Interpreted in light of psychological literature, related particularly to the Patient Health Engagement (PHE) model, our findings can provide evidence that the type and layout of online social interactions have an impact on users' "journeys" when posting about depression.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exposing Cross-Platform Coordinated Inauthentic Activity in the Run-Up to the 2024 U.S. Election</title>
<link>https://arxiv.org/abs/2410.22716</link>
<guid>https://arxiv.org/abs/2410.22716</guid>
<content:encoded><![CDATA[
<div> Russian interference, coordinated information operations, social media, cross-platform, influence campaigns <br />
<br />
Summary: <br />
This study examines coordinated information operations on social media platforms, focusing on interactions across Twitter (now $\mathbb{X$), Facebook, and Telegram related to the 2024 U.S. Election. By analyzing similarity networks, the research identifies coordinated communities engaging in suspicious sharing behaviors within and between platforms. The study uncovers potential foreign interference, with Russian-affiliated media systematically promoted on Telegram and $\mathbb{X$. In addition, the analysis reveals significant intra- and cross-platform coordinated inauthentic activity driving the dissemination of highly partisan, low-credibility, and conspiratorial content. The findings emphasize the need for regulatory measures that go beyond individual platforms to effectively combat the challenges posed by cross-platform coordinated influence campaigns. <div>
arXiv:2410.22716v4 Announce Type: replace 
Abstract: Coordinated information operations remain a persistent challenge on social media, despite platform efforts to curb them. While previous research has primarily focused on identifying these operations within individual platforms, this study shows that coordination frequently transcends platform boundaries. Leveraging newly collected data of online conversations related to the 2024 U.S. Election across $\mathbb{X}$ (formerly, Twitter), Facebook, and Telegram, we construct similarity networks to detect coordinated communities exhibiting suspicious sharing behaviors within and across platforms. Proposing an advanced coordination detection model, we reveal evidence of potential foreign interference, with Russian-affiliated media being systematically promoted across Telegram and $\mathbb{X}$. Our analysis also uncovers substantial intra- and cross-platform coordinated inauthentic activity, driving the spread of highly partisan, low-credibility, and conspiratorial content. These findings highlight the urgent need for regulatory measures that extend beyond individual platforms to effectively address the growing challenge of cross-platform coordinated influence campaigns.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</title>
<link>https://arxiv.org/abs/2410.05401</link>
<guid>https://arxiv.org/abs/2410.05401</guid>
<content:encoded><![CDATA[
<div> demographic targeting, fairness, large language models, climate change communication, social media

Summary:
Large language models (LLMs) were used to analyze microtargeting practices in climate change communication on social media, focusing on demographic targeting and fairness. The study achieved an 88.55% accuracy in predicting demographic targets and revealed distinct messaging strategies for different audiences. Young adults were targeted with activism and environmental themes, while women were engaged through caregiving and advocacy topics. LLMs provided transparent explanations for their classifications, highlighting the thematic elements used. A fairness analysis uncovered biases in predicting senior citizens and male audiences, underscoring the need for accountability and inclusivity in social media-driven climate campaigns. This study's framework demonstrates the effectiveness of LLMs in dissecting targeted communication strategies and emphasizes the importance of addressing biases for a more transparent and inclusive climate communication approach.

<br /><br />Summary: <div>
arXiv:2410.05401v2 Announce Type: replace-cross 
Abstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating DAO Sustainability and Longevity Through On-Chain Governance Metrics</title>
<link>https://arxiv.org/abs/2504.11341</link>
<guid>https://arxiv.org/abs/2504.11341</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralised Autonomous Organisations, governance efficiency, financial robustness, decentralisation, community engagement

Summary: 
Decentralised Autonomous Organisations (DAOs) automate governance and resource allocation through smart contracts, aiming to shift decision-making to distributed token holders. This paper addresses sustainability challenges faced by DAOs, such as limited user participation and concentrated voting power. By introducing a framework of Key Performance Indicators (KPIs) focusing on governance efficiency, financial robustness, decentralisation, and community engagement, the study identifies recurring governance patterns in real-world DAOs. Analysis of a custom dataset using non-parametric methods highlights issues like low participation rates and high proposer concentration. The proposed KPIs provide a data-driven method for assessing and improving DAO governance structures, supporting a comprehensive evaluation of decentralised systems. This research offers practical tools for enhancing the resilience and effectiveness of DAO-based governance models.

Summary: <div>
arXiv:2504.11341v2 Announce Type: replace-cross 
Abstract: Decentralised Autonomous Organisations (DAOs) automate governance and resource allocation through smart contracts, aiming to shift decision-making to distributed token holders. However, many DAOs face sustainability challenges linked to limited user participation, concentrated voting power, and technical design constraints. This paper addresses these issues by identifying research gaps in DAO evaluation and introducing a framework of Key Performance Indicators (KPIs) that capture governance efficiency, financial robustness, decentralisation, and community engagement. We apply the framework to a custom-built dataset of real-world DAOs constructed from on-chain data and analysed using non-parametric methods. The results reveal recurring governance patterns, including low participation rates and high proposer concentration, which may undermine long-term viability. The proposed KPIs offer a replicable, data-driven method for assessing DAO governance structures and identifying potential areas for improvement. These findings support a multidimensional approach to evaluating decentralised systems and provide practical tools for researchers and practitioners working to improve the resilience and effectiveness of DAO-based governance models.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schelling segregation dynamics in densely-connected social network graphs</title>
<link>https://arxiv.org/abs/2504.16307</link>
<guid>https://arxiv.org/abs/2504.16307</guid>
<content:encoded><![CDATA[
<div> network model, segregation, polarization, agent-based, social<br />
Summary: <br />
- Schelling segregation model used to study segregation dynamics in dense social networks.
- Dense networks show less segregation compared to sparse networks.
- Agents do not end up more segregated than they desire to be.
- Polarization is difficult in networks with one smaller group and unstable with an extremely small group.
- Mixed evidence for the "paradox of weak minority preferences" in a densely connected social network. <br /> <div>
arXiv:2504.16307v1 Announce Type: new 
Abstract: Schelling segregation is a well-established model used to investigate the dynamics of segregation in agent-based models. Since we consider segregation to be key for the development of political polarisation, we are interested in what insights it could give for this problem. We tested basic questions of segregation on an agent-based social network model where agents' connections were not restricted by their spatial position, and made the network graph much denser than previous tests of Schelling segregation in social networks.
  We found that a dense social network does not become as strongly segregated as a sparse network, and that agents' numbers of same-group neighbours do not greatly exceed their desired numbers (i.e. they do not end up more segregated than they desire to be). Furthermore, we found that the network was very difficult to polarise when one group was somewhat smaller than the other, and that it became unstable when one group was extremely small, both of which provide insights into real-world polarisation dynamics. Finally, we tested the question of whether an increase in the minority group's desire for same-group neighbours created more segregation than a similar increase for the majority group -- the "paradox of weak minority preferences" -- and found mixed evidence for this effect in a densely connected social network.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Social Computing Tools for Undergraduate Research Community Building</title>
<link>https://arxiv.org/abs/2504.16366</link>
<guid>https://arxiv.org/abs/2504.16366</guid>
<content:encoded><![CDATA[
<div> impostor syndrome, undergraduate students, social computing tools, research community, sense of belonging <br />
Summary: <br />
The article proposes the use of spontaneous online social networks (SOSNs) to overcome barriers faced by new members, particularly undergraduate students, in research communities. By integrating a photo sharing bot inspired by SOSNs like BeReal into a computing research lab, the study aimed to enhance sense of belonging, peripheral awareness, and feelings of togetherness. Through surveys and interviews with 17 lab members over 2 weeks, an increase in sense of togetherness was observed. The approach facilitated greater awareness of peers' personal lives, fostering a stronger sense of community and reducing feelings of disconnectedness. Overall, the integration of social computing tools in small research communities shows promise in promoting a more inclusive and supportive environment for new members. <br /> <div>
arXiv:2504.16366v1 Announce Type: new 
Abstract: Many barriers exist when new members join a research community, including impostor syndrome. These barriers can be especially challenging for undergraduate students who are new to research. In our work, we explore how the use of social computing tools in the form of spontaneous online social networks (SOSNs) can be used in small research communities to improve sense of belonging, peripheral awareness, and feelings of togetherness within an existing CS research community. Inspired by SOSNs such as BeReal, we integrated a Wizard-of-Oz photo sharing bot into a computing research lab to foster community building among members. Through a small sample of lab members (N = 17) over the course of 2 weeks, we observed an increase in participants' sense of togetherness based on pre- and post-study surveys. Our surveys and semi-structured interviews revealed that this approach has the potential to increase awareness of peers' personal lives, increase feelings of community, and reduce feelings of disconnectedness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.16323</link>
<guid>https://arxiv.org/abs/2504.16323</guid>
<content:encoded><![CDATA[
<div> Keywords: digital media, Media Content Atlas, multimodal large language models, content analysis, screen data

Summary: 
The study introduces the Media Content Atlas (MCA), a pipeline for investigating large-scale screen data using multimodal large language models (MLLMs). MCA allows for moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Tested on 1.12 million smartphone screenshots from 112 adults over a month, MCA supports open-ended exploration and hypothesis generation, as well as hypothesis-driven investigations. Expert evaluators found MCA to be usable and promising for research and intervention design, with high relevance and accuracy ratings for clustering results and descriptions. By combining methodological possibilities with specific research needs, MCA enables both inductive and deductive inquiry, offering new opportunities for media and HCI research.<br /><br />Summary: <div>
arXiv:2504.16323v1 Announce Type: cross 
Abstract: As digital media use continues to evolve and influence various aspects of life, developing flexible and scalable tools to study complex media experiences is essential. This study introduces the Media Content Atlas (MCA), a novel pipeline designed to help researchers investigate large-scale screen data beyond traditional screen-use metrics. Leveraging multimodal large language models (MLLMs), MCA enables moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Evaluated on 1.12 million smartphone screenshots continuously captured during screen use from 112 adults over an entire month, MCA facilitates open-ended exploration and hypothesis generation as well as hypothesis-driven investigations at an unprecedented scale. Expert evaluators underscored its usability and potential for research and intervention design, with clustering results rated 96% relevant and descriptions 83% accurate. By bridging methodological possibilities with domain-specific needs, MCA accelerates both inductive and deductive inquiry, presenting new opportunities for media and HCI research.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</title>
<link>https://arxiv.org/abs/2504.16604</link>
<guid>https://arxiv.org/abs/2504.16604</guid>
<content:encoded><![CDATA[
<div> Keywords: Counterspeech, Large Language Models, Conspiracy theories, GPT-4o, Llama 3

Summary:
Large Language Models (LLMs) like GPT-4o, Llama 3, and Mistral are being explored for countering conspiracy theories online through counterspeech. However, the study found that these models often generate generic or superficial responses when provided with expert-crafted counterspeech prompts. They also tend to over-acknowledge fear and frequently fabricate facts or sources, posing challenges for practical application. Unlike hate speech, there is a lack of datasets pairing conspiracy theory comments with expert counterspeech, highlighting the need for further research in this area. The study underscores the limitations of using prompt-based approaches with LLMs for countering harmful online content, indicating the importance of developing more effective strategies for addressing conspiracy theories. 

<br /><br />Summary: 
- Large Language Models like GPT-4o, Llama 3, and Mistral are explored for countering conspiracy theories through counterspeech. 
- These models often generate generic or superficial responses to expert-crafted counterspeech prompts. 
- They tend to over-acknowledge fear and frequently fabricate facts or sources, challenging their practical application. 
- Lack of datasets pairing conspiracy theory comments with expert counterspeech highlights the need for further research in this area. 
- Limitations of using prompt-based approaches with LLMs for countering harmful online content are underscored, emphasizing the need for more effective strategies. <div>
arXiv:2504.16604v1 Announce Type: cross 
Abstract: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Science (SecSci), Basic Concepts and Mathematical Foundations</title>
<link>https://arxiv.org/abs/2504.16617</link>
<guid>https://arxiv.org/abs/2504.16617</guid>
<content:encoded><![CDATA[
<div> Keywords: textbook, security courses, lecture notes, research problems, advanced courses

Summary: 
This textbook is a compilation of lecture notes from security courses taught at various universities, including Oxford, Royal Holloway, and Hawaii. The early chapters are designed for a first course in security, while the middle chapters have been used in advanced courses. Towards the end of the textbook, there are also research problems included. The material covered in the textbook spans different levels of expertise, making it suitable for a range of students from beginners to more advanced learners. The inclusion of research problems at the end of the textbook provides an opportunity for students to engage with the material in a more in-depth and practical manner. Overall, this textbook offers a comprehensive overview of security concepts and is a valuable resource for students studying the subject. 

<br /><br />Summary: <div>
arXiv:2504.16617v1 Announce Type: cross 
Abstract: This textbook compiles the lecture notes from security courses taught at Oxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii. The early chapters are suitable for a first course in security. The middle chapters have been used in advanced courses. Towards the end there are also some research problems.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Traps and Algorithmic Interventions</title>
<link>https://arxiv.org/abs/2406.04462</link>
<guid>https://arxiv.org/abs/2406.04462</guid>
<content:encoded><![CDATA[
<div> philosophical literature, pessimism traps, information cascades, economics, mathematics <br />
Summary: <br />
This paper explores the relationship between philosophical concepts of pessimism traps and formal models of information cascades in economics and mathematics. Pessimism traps describe how individuals in a community mimic sub-optimal actions of others in uncertain situations, similar to how information cascades involve agents making decisions based on private signals and public histories. The study shows that information cascades inevitably occur in many scenarios, and populations can easily fall into incorrect cascades based on the strength of signals. Once formed, cascades are difficult to break without external intervention. The paper proposes an intervention strategy to redirect populations from incorrect to correct cascades, demonstrating both theoretical and empirical analyses of its efficacy. <div>
arXiv:2406.04462v3 Announce Type: replace 
Abstract: In this paper, we relate the philosophical literature on pessimism traps to information cascades, a formal model derived from the economics and mathematics literature. A pessimism trap is a social pattern in which individuals in a community, in situations of uncertainty, begin to copy the sub-optimal actions of others, despite their individual beliefs. This maps nicely onto the concept of an information cascade, which involves a sequence of agents making a decision between two alternatives, with a private signal of the superior alternative and a public history of others' actions. Key results from the economics literature show that information cascades occur with probability one in many contexts, and depending on the strength of the signal, populations can fall into the incorrect cascade very easily and quickly. Once formed, in the absence of external perturbation, a cascade cannot be broken -- therefore, we derive an intervention that can be used to nudge a population from an incorrect to a correct cascade and, importantly, maintain the cascade once the subsidy is discontinued. We study this both theoretically and empirically.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness-Aware Dense Subgraph Discovery</title>
<link>https://arxiv.org/abs/2412.02604</link>
<guid>https://arxiv.org/abs/2412.02604</guid>
<content:encoded><![CDATA[
<div> Keywords: Dense subgraph discovery, Fairness, Graph mining, Subgraph density, Tractable formulations <br />
Summary: 
This study introduces two tractable formulations for fair Dense subgraph discovery (DSD), focusing on promoting fairness in detecting dense subgraphs that represent diverse subgroups within the vertex set. Existing methods for fair DSD have limitations in NP-hard formulations and lack flexibility in defining fairness levels. The proposed methods offer structured approaches to incorporate fairness with varying levels and introduce a measure of fairness-induced relative loss in subgraph density. Results from extensive experiments on real-world datasets demonstrate that the new methods outperform existing solutions, with significantly lower subgraph density loss in some cases, particularly excelling in scenarios with extreme subgroup imbalances. By addressing these limitations, the study contributes to a better understanding of the trade-off between density and fairness in DSD. <br /> <div>
arXiv:2412.02604v2 Announce Type: replace 
Abstract: Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition. In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density. Existing methods for promoting fairness in DSD have important limitations - the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness. In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness. Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels. We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off. We are the first to study such a notion in the context of detecting fair dense subgraphs. Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels. Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerable Connectivity Caused by Local Communities in Spatial Networks</title>
<link>https://arxiv.org/abs/2412.14513</link>
<guid>https://arxiv.org/abs/2412.14513</guid>
<content:encoded><![CDATA[
<div> Keywords: local communities, spatial networks, robustness, infrastructure, long-distance links

Summary:
This study examines the impact of local communities on the robustness of connectivity in spatial networks with a focus on planar infrastructure. The research reveals that networks with strong local communities have weaker robustness against malicious attacks. The presence of concentrated nodes connected with short links within local communities leads to vulnerabilities in the network's connectivity. However, the study also suggests that introducing long-distance links can help mitigate the negative effects of local communities on network robustness. By incorporating long-distance connections, the network's ability to withstand attacks and maintain connectivity can be improved. Overall, the findings emphasize the importance of considering the structure of local communities in spatial networks when assessing network robustness and the potential benefits of incorporating longer links to enhance resilience. 

<br /><br />Summary: <div>
arXiv:2412.14513v4 Announce Type: replace 
Abstract: Local communities by concentration of nodes connected with short links are widely observed in spatial networks. However, how such structure affects robustness of connectivity against malicious attacks remains unclear. This study investigates the impact of local communities on the robustness by modeling planar infrastructure reveals that the robustness is weakened by strong local communities in spatial networks. These results highlight the potential of long-distance links in mitigating the negative effects of local community on the robustness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNN-based Anchor Embedding for Exact Subgraph Matching</title>
<link>https://arxiv.org/abs/2502.00031</link>
<guid>https://arxiv.org/abs/2502.00031</guid>
<content:encoded><![CDATA[
<div> Keyword: subgraph matching, graph neural network, anchor embedding, exact matching, query plan
Summary:
The paper introduces a novel graph neural network (GNN)-based anchor embedding framework (GNN-AE) to address the subgraph matching problem with exact results. Unlike other approaches that provide only approximate solutions, the GNN-AE approach leverages anchor concepts and embeddings to ensure no false dismissals in subgraph matching. By transforming the problem into a search task in the embedding space, the proposed method guarantees exact matching. An efficient matching growth algorithm is developed to retrieve all exact matches in parallel, supported by a cost-model-based DFS query plan for enhanced performance. Experimental results on various datasets validate the effectiveness and efficiency of the GNN-AE approach for exact subgraph matching.<br /><br />Summary: <div>
arXiv:2502.00031v3 Announce Type: replace 
Abstract: Subgraph matching query is a classic problem in graph data management and has a variety of real-world applications, such as discovering structures in biological or chemical networks, finding communities in social network analysis, explaining neural networks, and so on. To further solve the subgraph matching problem, several recent advanced works attempt to utilize deep-learning-based techniques to handle the subgraph matching query. However, most of these works only obtain approximate results for subgraph matching without theoretical guarantees of accuracy. In this paper, we propose a novel and effective graph neural network (GNN)-based anchor embedding framework (GNN-AE), which allows exact subgraph matching. Unlike GNN-based approximate subgraph matching approaches that only produce inexact results, in this paper, we pioneer a series of concepts related to anchor (including anchor, anchor graph/path, etc.) in subgraph matching and carefully devise the anchor (graph) embedding technique based on GNN models. We transform the subgraph matching problem into a search problem in the embedding space via the anchor (graph & path) embedding techniques. With the proposed anchor matching mechanism, GNN-AE can guarantee subgraph matching has no false dismissals. We design an efficient matching growth algorithm, which can retrieve the locations of all exact matches in parallel. We also propose a cost-model-based DFS query plan to enhance the parallel matching growth algorithm. Through extensive experiments on 6 real-world and 3 synthetic datasets, we confirm the effectiveness and efficiency of our GNN-AE approach for exact subgraph matching.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic hashtag recommendation in social media with trend shift detection and adaptation</title>
<link>https://arxiv.org/abs/2504.00044</link>
<guid>https://arxiv.org/abs/2504.00044</guid>
<content:encoded><![CDATA[
<div> recommendation systems, hashtags, social media, trend shifts, Apache Storm

Summary:
Hashtag recommendation systems play a crucial role in enhancing content categorization and search on social media platforms. However, existing static models face challenges in adapting to the dynamic nature of social media conversations where new hashtags constantly emerge and existing ones evolve semantically. To address this issue, the H-ADAPTS methodology is introduced, which incorporates a trend-aware mechanism to detect shifts in hashtag usage, reflecting evolving trends in social media discussions. By leveraging the Apache Storm framework for scalable and fault-tolerant analysis of high-velocity social data, H-ADAPTS can efficiently adapt its recommendation model based on recent posts. Experimental results from real-world case studies, such as the COVID-19 pandemic and the 2020 US presidential election, demonstrate that H-ADAPTS outperforms existing solutions by providing timely and relevant hashtag recommendations that align with emerging trends in social media conversations. <br /><br />Summary: <div>
arXiv:2504.00044v2 Announce Type: replace 
Abstract: Hashtag recommendation systems have emerged as a key tool for automatically suggesting relevant hashtags and enhancing content categorization and search. However, existing static models struggle to adapt to the highly dynamic nature of social media conversations, where new hashtags constantly emerge and existing ones undergo semantic shifts. To address these challenges, this paper introduces H-ADAPTS (Hashtag recommendAtion by Detecting and adAPting to Trend Shifts), a dynamic hashtag recommendation methodology that employs a trend-aware mechanism to detect shifts in hashtag usage-reflecting evolving trends and topics within social media conversations-and triggers efficient model adaptation based on a (small) set of recent posts. Additionally, the Apache Storm framework is leveraged to support scalable and fault-tolerant analysis of high-velocity social data, enabling the timely detection of trend shifts. Experimental results from two real-world case studies, including the COVID-19 pandemic and the 2020 US presidential election, demonstrate the effectiveness of H-ADAPTS in providing timely and relevant hashtag recommendations by adapting to emerging trends, significantly outperforming existing solutions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Noise Reduction in Dense Mixed-Membership Stochastic Block Models under Diverging Spiked Eigenvalues Condition</title>
<link>https://arxiv.org/abs/2307.14530</link>
<guid>https://arxiv.org/abs/2307.14530</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, overlapping community, stochastic block model, estimation error, lower bound

Summary: 
Community detection in networks is a crucial issue with applications in various fields. This study focuses on the Mixed-Membership Stochastic Block Model (MMSB) for overlapping community detection. The goal is to reconstruct relationships between communities based on network observations. Different approaches are compared, and a new estimator is proposed that matches the minimax lower bound on estimation error. Theoretical results are established under general model conditions, and experiments are conducted to demonstrate the theory. This work contributes to advancing the understanding and techniques for identifying overlapping communities in networks.<br /><br />Summary: <div>
arXiv:2307.14530v2 Announce Type: replace-cross 
Abstract: Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics</title>
<link>https://arxiv.org/abs/2504.15927</link>
<guid>https://arxiv.org/abs/2504.15927</guid>
<content:encoded><![CDATA[
<div> community detection, semi-supervised, crystallization kinetics, CLANN, Transitive Annealer

Summary:
The paper introduces CLANN, a novel semi-supervised community detection method inspired by crystallization kinetics. Traditional methods face challenges with scalability and unreasonable starting points. CLANN integrates annealing principles to optimize the identification of community cores, mimicking a crystal subgrain expanding into a complete grain. Through a Transitive Annealer, neighboring cliques are merged, and the community core is repositioned for spontaneous growth, improving scalability. Extensive experiments across 43 network settings show CLANN outperforms existing methods on multiple real-world datasets, demonstrating its effectiveness and efficiency in community detection. <br /><br />Summary: <div>
arXiv:2504.15927v1 Announce Type: new 
Abstract: Semi-supervised community detection methods are widely used for identifying specific communities due to the label scarcity. Existing semi-supervised community detection methods typically involve two learning stages learning in both initial identification and subsequent adjustment, which often starts from an unreasonable community core candidate. Moreover, these methods encounter scalability issues because they depend on reinforcement learning and generative adversarial networks, leading to higher computational costs and restricting the selection of candidates. To address these limitations, we draw a parallel between crystallization kinetics and community detection to integrate the spontaneity of the annealing process into community detection. Specifically, we liken community detection to identifying a crystal subgrain (core) that expands into a complete grain (community) through a process similar to annealing. Based on this finding, we propose CLique ANNealing (CLANN), which applies kinetics concepts to community detection by integrating these principles into the optimization process to strengthen the consistency of the community core. Subsequently, a learning-free Transitive Annealer was employed to refine the first-stage candidates by merging neighboring cliques and repositioning the community core, enabling a spontaneous growth process that enhances scalability. Extensive experiments on \textbf{43} different network settings demonstrate that CLANN outperforms state-of-the-art methods across multiple real-world datasets, showcasing its exceptional efficacy and efficiency in community detection.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Widely Known Findings Easier to Retract?</title>
<link>https://arxiv.org/abs/2504.15504</link>
<guid>https://arxiv.org/abs/2504.15504</guid>
<content:encoded><![CDATA[
<div> Keywords: retraction, failures, science, citation, Altmetric

Summary: 
Retraction failures are common in science, and this study aims to understand why they occur and what factors make findings harder or easier to retract. Utilizing data from Microsoft Academic Graph, Retraction Watch, and Altmetric, the study tests the hypothesis that the social spread of scientific information contributes to retraction failures. Surprisingly, the study finds that widely known or well-established results are actually easier to retract, as their retractions are more relevant to a larger number of scientists. Highly cited papers experience more significant reductions in citations post-retraction and attract more attention to their retractions. These findings suggest that the popularity and impact of a study can influence the ease of retraction, highlighting the importance of social spread and scientific dissemination in the retractions process. 

Summary: <div>
arXiv:2504.15504v1 Announce Type: cross 
Abstract: Failures of retraction are common in science. Why do these failures occur? And, relatedly, what makes findings harder or easier to retract? We use data from Microsoft Academic Graph, Retraction Watch, and Altmetric -- including retracted papers, citation records, and Altmetric scores and mentions -- to test recently proposed answers to these questions. A recent previous study by LaCroix et al. employ simple network models to argue that the social spread of scientific information helps explain failures of retraction. One prediction of their models is that widely known or well established results, surprisingly, should be easier to retract, since their retraction is more relevant to more scientists. Our results support this conclusion. We find that highly cited papers show more significant reductions in citation after retraction and garner more attention to their retractions as they occur.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing well-balanced spanning trees of unweighted networks</title>
<link>https://arxiv.org/abs/2205.06628</link>
<guid>https://arxiv.org/abs/2205.06628</guid>
<content:encoded><![CDATA[
<div> algorithm, spanning tree, network, breadth-first search, graph

Summary:<br /><br />
Spanning trees are crucial for simplifying and sampling networks. Prim's and Kruskal's algorithms are commonly used for weighted networks, but for unweighted networks, breadth-first search may be a better choice as it preserves network structure better and produces more compact and well-balanced spanning trees. The study conducted experiments on synthetic and real networks to compare the performance of different algorithms. The results suggest that breadth-first search algorithm outperforms priority-first search algorithms in terms of preserving network structure and producing efficient spanning trees. This research provides valuable insights into the impact of different algorithms on spanning tree construction and highlights the importance of choosing the right algorithm based on the network's characteristics. <div>
arXiv:2205.06628v2 Announce Type: replace 
Abstract: A spanning tree of a network or graph is a subgraph that connects all nodes with the least number or weight of edges. The spanning tree is one of the most straightforward techniques for network simplification and sampling, and for discovering its backbone or skeleton. Prim's algorithm and Kruskal's algorithm are well-known algorithms for computing a spanning tree of a weighted network, and are therefore also the default procedure for unweighted networks in the most popular network libraries. In this paper, we empirically study the performance of these algorithms on unweighted networks and compare them with different priority-first search algorithms. We show that the structure of a network, such as the distances between the nodes, is better preserved by a simpler algorithm based on breadth-first search. The spanning trees are also most compact and well-balanced as measured by classical graph indices. We support our findings with experiments on synthetic graphs and more than a thousand real networks, and demonstrate practical applications of the computed spanning trees. We conclude that if a spanning tree is to maintain the structure of an unweighted network, the breadth-first search algorithm should be the preferred choice.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-Based Model for Vehicle-Centric Data Sharing Ecosystem</title>
<link>https://arxiv.org/abs/2410.22897</link>
<guid>https://arxiv.org/abs/2410.22897</guid>
<content:encoded><![CDATA[
<div> Connected services, autonomous driving, data collection, privacy concerns, ontology<br />
Summary:<br />
The automotive industry is undergoing a transformation towards connected services and autonomous driving, leading to increased data collection and sharing in vehicles. This shift raises privacy concerns, prompting the need for understanding how modern vehicles handle data exchange among different parties. A high-level conceptual graph-based model was developed using the ontology 101 methodology, incorporating information from various sources including privacy policy analysis and literature review. The model provides insights into data sharing practices and can be expanded for diverse contexts. Realistic examples demonstrate the model's effectiveness in uncovering privacy issues related to vehicle-related data sharing. Future research directions include exploring advanced ontology languages for reasoning, conducting topological analysis to identify data privacy risks, and developing tools for comparative analysis. These efforts aim to enhance understanding of the vehicle-centric data sharing ecosystem. <div>
arXiv:2410.22897v3 Announce Type: replace 
Abstract: The development of technologies has prompted a paradigm shift in the automotive industry, with an increasing focus on connected services and autonomous driving capabilities. This transformation allows vehicles to collect and share vast amounts of vehicle-specific and personal data. While these technological advancements offer enhanced user experiences, they also raise privacy concerns. To understand the ecosystem of data collection and sharing in modern vehicles, we adopted the ontology 101 methodology to incorporate information extracted from different sources, including analysis of privacy policies using GPT-4, a small-scale systematic literature review, and an existing ontology, to develop a high-level conceptual graph-based model, aiming to get insights into how modern vehicles handle data exchange among different parties. This serves as a foundational model with the flexibility and scalability to further expand for modelling and analysing data sharing practices across diverse contexts. Two realistic examples were developed to demonstrate the usefulness and effectiveness of discovering insights into privacy regarding vehicle-related data sharing. We also recommend several future research directions, such as exploring advanced ontology languages for reasoning tasks, supporting topological analysis for discovering data privacy risks/concerns, and developing useful tools for comparative analysis, to strengthen the understanding of the vehicle-centric data sharing ecosystem.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the accurate computation of expected modularity in probabilistic networks</title>
<link>https://arxiv.org/abs/2408.07161</link>
<guid>https://arxiv.org/abs/2408.07161</guid>
<content:encoded><![CDATA[
<div> modularity, probabilistic networks, expected value, FPWP, computation <br />
Summary: <br />
Modularity is a key measure for assessing community structures in networks. In probabilistic networks with uncertain edge existence, computing the expected modularity is challenging. The proposed FPWP technique efficiently computes the probability distribution and expected value of modularity. Comparison with other methods reveals that removing low-probability edges or treating probabilities as weights leads to inaccuracies. Monte Carlo sampling has variable convergence based on network parameters. Brute-force computation, while accurate, is slow. In contrast, FPWP is fast and guarantees precise results. Comprehensive experiments on real-world and synthetic networks showcase the accuracy and time efficiency of the FPWP technique. <div>
arXiv:2408.07161v3 Announce Type: replace-cross 
Abstract: Modularity is one of the most widely used measures for evaluating communities in networks. In probabilistic networks, where the existence of edges is uncertain and uncertainty is represented by probabilities, the expected value of modularity can be used instead. However, efficiently computing expected modularity is challenging. To address this challenge, we propose a novel and efficient technique (FPWP) for computing the probability distribution of modularity and its expected value. In this paper, we implement and compare our method and various general approaches for expected modularity computation in probabilistic networks. These include: (1) translating probabilistic networks into deterministic ones by removing low-probability edges or treating probabilities as weights, (2) using Monte Carlo sampling to approximate expected modularity, and (3) brute-force computation. We evaluate the accuracy and time efficiency of FPWP through comprehensive experiments on both real-world and synthetic networks with diverse characteristics. Our results demonstrate that removing low-probability edges or treating probabilities as weights produces inaccurate results, while the convergence of the sampling method varies with the parameters of the network. Brute-force computation, though accurate, is prohibitively slow. In contrast, our method is much faster than brute-force computation, but guarantees an accurate result.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Model of Silence, or the Probability of "Un Ange Passe"</title>
<link>https://arxiv.org/abs/2504.13931</link>
<guid>https://arxiv.org/abs/2504.13931</guid>
<content:encoded><![CDATA[
<div> Keywords: silence, conversation dynamics, Markov chain model, phase-transition-like phenomenon, intercultural communication 

Summary: 
The study introduces the concept of "Un ange passe" or "An angel passes," a universal phenomenon of sudden silence in a co-present group. The significance and interpretation of silence vary across cultures, impacting organizational productivity, creativity, and medical settings. Mathematical modeling of silence dynamics is relatively unexplored, prompting the development of a Markov chain model to analyze silence behavior. Results indicate a phase-transition-like occurrence where silence abruptly ends once individuals lose awareness of the conversation, underscoring the necessity of mutual awareness for silence to arise. This model not only enhances understanding of conversational dynamics but also holds potential for enhancing intercultural communication, organizational efficiency, and medical practices. 

<br /><br />Summary: <div>
arXiv:2504.13931v1 Announce Type: new 
Abstract: In French, the phrase "Un ange passe" ("An angel passes") refers to the sudden silence that falls over a co-present group -- that is, a group of people sharing the same physical space. As evidenced by the presence of similar expressions across languages and cultures, this phenomenon represents a universal feature of human conversation. At the same time, the meaning attributed to silence can differ greatly across national, cultural, and interpersonal contexts. Consequently, a wide range of studies have focused on the impact of silence on organizational productivity, its relationship to ideas and creativity, and its potential effectiveness in medical settings. Despite the important role that silence plays, very few studies have attempted to characterize its features using mathematical modeling. In this study, we propose a Markov chain model to describe the dynamics of silence in a co-present group and attempt to analyze its behavior. Our results reveal a phase-transition-like phenomenon, where the probability of silence abruptly drops to zero once individuals' awareness of the surrounding conversation falls below a critical threshold. In other words, such silence can emerge only when individuals retain a minimal degree of mutual awareness of those around them. The model proposed in this study not only offers a deeper understanding of conversational dynamics, but also holds potential for contributing to intercultural communication, organizational productivity, and medical practice.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Writing Patterns Reveal a Hidden Division of Labor in Scientific Teams</title>
<link>https://arxiv.org/abs/2504.14093</link>
<guid>https://arxiv.org/abs/2504.14093</guid>
<content:encoded><![CDATA[
<div> Keywords: individual contributions, coauthored papers, author order, writing signatures, labor specialization

Summary:
This study proposes a behavior-based approach to identifying individual contributions in scientific papers by analyzing author-specific LaTeX macros used as writing signatures in over 730,000 arXiv papers. The research covers the period from 1991 to 2023 and includes over half a million scientists. The method is validated against self-reports, author order, disciplinary norms, and Overleaf records to reliably infer author-level writing activity. The findings reveal a hidden division of labor in collaborative research, with first authors focusing on technical sections and last authors contributing to conceptual sections. This empirical evidence of labor specialization at scale provides new tools to improve credit allocation in scientific collaborations. <div>
arXiv:2504.14093v1 Announce Type: new 
Abstract: The recognition of individual contributions is central to the scientific reward system, yet coauthored papers often obscure who did what. Traditional proxies like author order assume a simplistic decline in contribution, while emerging practices such as self-reported roles are biased and limited in scope. We introduce a large-scale, behavior-based approach to identifying individual contributions in scientific papers. Using author-specific LaTeX macros as writing signatures, we analyze over 730,000 arXiv papers (1991-2023), covering over half a million scientists. Validated against self-reports, author order, disciplinary norms, and Overleaf records, our method reliably infers author-level writing activity. Section-level traces reveal a hidden division of labor: first authors focus on technical sections (e.g., Methods, Results), while last authors primarily contribute to conceptual sections (e.g., Introduction, Discussion). Our findings offer empirical evidence of labor specialization at scale and new tools to improve credit allocation in collaborative research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking mob Dynamics in online social networks Using epidemiology model based on Mobility Equations</title>
<link>https://arxiv.org/abs/2504.14172</link>
<guid>https://arxiv.org/abs/2504.14172</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, epidemiological models, COVID-19 spread, Twitter activity, mobility dynamics

Summary:
<br />
This research focuses on analyzing the social behavior related to the spread of COVID-19 using Twitter activity data from April to June 2020. A mathematical model is introduced to integrate mobility dynamics, derived from real data, to adjust outbreak rates based on social interactions. The model considers mobility as a time-varying parameter, accounting for fluctuations in contact rates influenced by factors like personal behavior and external factors such as lockdowns and quarantines. The study identifies a threshold number, examines the existence of bifurcation, and establishes the stability of steady states. Numerical simulations and sensitivity analysis of relevant parameters are conducted to track public sentiment and engagement trends during the pandemic. <div>
arXiv:2504.14172v1 Announce Type: new 
Abstract: Nowadays, social media is the main tool in our new lives. The outbreak news and all related obtained from social media, and mob events affect the of spread these news fast. Recently, epidemiological models to study disease spread and analyze the behavior of mob groups by dealing with "contagions" that propagate through user networks. In this research, we introduced a mathematical model to analyze social behavior related to COVID-19 spread by examining Twitter activity from April 2020 to June 2020. The main feature of this model is the integration of mobility dynamics that be derived from the above real data, to adjust the rate of outbreak based on the response of social interactions. Consider mobility as a parameter of time-varying, and fluctuations in the rate of contact that is driven by factors like personal behavior or external affecting such as "lockdown" and "quarantine" etc., to track public sentiment and engagement trends during the pandemic. The threshold number is derived, and the existence of bifurcation and the stability of the steady states are established. Numerical simulations and sensitivity analysis of relevant parameters are also carried out.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Diffusion and Preferential Attachment in a Network of Large Language Models</title>
<link>https://arxiv.org/abs/2504.14438</link>
<guid>https://arxiv.org/abs/2504.14438</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, information diffusion, two-time-scale dynamical model, reputation-based preferential attachment mechanism, numerical experiments

Summary:
This paper presents a model for information diffusion in a network of Large Language Models (LLMs) that can provide answers to queries from distributed datasets, even potentially hallucinating the answer. The model incorporates a two-time-scale dynamical system where opinions evolve rapidly while network structure changes slowly. By using a mean-field approximation, the paper establishes conditions for a stable equilibrium where all LLMs remain truthful. Additionally, approximation guarantees for the mean-field and singularly perturbed approximations are provided. To address hallucination and enhance the influence of truthful nodes, a reputation-based preferential attachment mechanism is proposed to reconfigure the network based on evaluations by LLMs of their neighbors. Numerical experiments conducted on an open-source LLM validate the effectiveness of the preferential attachment mechanism and demonstrate optimization of a cost function for the two-time-scale system. <br /><br />Summary: <div>
arXiv:2504.14438v1 Announce Type: new 
Abstract: This paper models information diffusion in a network of Large Language Models (LLMs) that is designed to answer queries from distributed datasets, where the LLMs can hallucinate the answer. We introduce a two-time-scale dynamical model for the centrally administered network, where opinions evolve faster while the network's degree distribution changes more slowly. Using a mean-field approximation, we establish conditions for a locally asymptotically stable equilibrium where all LLMs remain truthful. We provide approximation guarantees for the mean-field approximation and a singularly perturbed approximation of the two-time-scale system. To mitigate hallucination and improve the influence of truthful nodes, we propose a reputation-based preferential attachment mechanism that reconfigures the network based on LLMs' evaluations of their neighbors. Numerical experiments on an open-source LLM (LLaMA-3.1-8B) validate the efficacy of our preferential attachment mechanism and demonstrate the optimization of a cost function for the two-time-scale system.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Local Separators Shape Community Structure in Large Networks</title>
<link>https://arxiv.org/abs/2504.14501</link>
<guid>https://arxiv.org/abs/2504.14501</guid>
<content:encoded><![CDATA[
<div> community detection, local separators, network structure, modularity optimization, hierarchical structures

Summary:
This study explores the use of local separators for community detection in large networks. Local 1-separators are found to effectively identify densely connected communities, outperforming traditional modularity-based methods. On the other hand, local 2-separators reveal hierarchical structures within networks, although they may lead to over-fragmentation of small clusters. These findings are particularly relevant for road networks, suggesting potential applications in transportation and infrastructure analysis. Overall, local separators offer a scalable and interpretable alternative to traditional community detection algorithms, providing a more nuanced understanding of network structure based on structural bottlenecks rather than global connectivity.<br /><br />Summary: <div>
arXiv:2504.14501v1 Announce Type: new 
Abstract: Community detection is a key tool for analyzing the structure of large networks. Standard methods, such as modularity optimization, focus on identifying densely connected groups but often overlook natural local separations in the graph. In this paper, we investigate local separator methods, which decompose networks based on structural bottlenecks rather than global connectivity. We systematically compare them with well-established community detection algorithms on large real-world networks. Our results show that local 1-separators consistently identify the densest communities, outperforming modularity-based methods in this regard, while local 2-separators reveal hierarchical structures but may over-fragment small clusters. These findings are particularly strong for road networks, suggesting practical applications in transportation and infrastructure analysis. Our study highlights local separators as a scalable and interpretable alternative for network decomposition.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform</title>
<link>https://arxiv.org/abs/2504.14904</link>
<guid>https://arxiv.org/abs/2504.14904</guid>
<content:encoded><![CDATA[
<div> benchmark, content moderation, short video platforms, user feedback, KuaiMod <br />
Summary: <br />
Exponentially growing short video platforms (SVPs) struggle with content moderation, especially concerning minors' mental health. Existing methods have limitations such as human bias, lack of understanding nuanced content, and slow update cycles. This paper introduces a new SVP content moderation benchmark with authentic user feedback. The proposed KuaiMod framework addresses these challenges through training data construction, offline adaptation, and online deployment & refinement using large vision language models and Chain-of-Thought reasoning. KuaiMod outperforms other methods in moderation performance on the benchmark, reducing user reporting rates and increasing user engagement on Kuaishou. The open-sourced benchmark aims to advance research in SVP content moderation. <br /> <div>
arXiv:2504.14904v1 Announce Type: new 
Abstract: Exponentially growing short video platforms (SVPs) face significant challenges in moderating content detrimental to users' mental health, particularly for minors. The dissemination of such content on SVPs can lead to catastrophic societal consequences. Although substantial efforts have been dedicated to moderating such content, existing methods suffer from critical limitations: (1) Manual review is prone to human bias and incurs high operational costs. (2) Automated methods, though efficient, lack nuanced content understanding, resulting in lower accuracy. (3) Industrial moderation regulations struggle to adapt to rapidly evolving trends due to long update cycles. In this paper, we annotate the first SVP content moderation benchmark with authentic user/reviewer feedback to fill the absence of benchmark in this field. Then we evaluate various methods on the benchmark to verify the existence of the aforementioned limitations. We further propose our common-law content moderation framework named KuaiMod to address these challenges. KuaiMod consists of three components: training data construction, offline adaptation, and online deployment & refinement. Leveraging large vision language model (VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video toxicity based on sparse user feedback and fosters dynamic moderation policy with rapid update speed and high accuracy. Offline experiments and large-scale online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the best moderation performance on our benchmark. The deployment of KuaiMod reduces the user reporting rate by 20% and its application in video recommendation increases both Daily Active User (DAU) and APP Usage Time (AUT) on several Kuaishou scenarios. We have open-sourced our benchmark at https://kuaimod.github.io.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis</title>
<link>https://arxiv.org/abs/2504.15072</link>
<guid>https://arxiv.org/abs/2504.15072</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion propagation dynamics, multi-dimensional Hawkes processes, Graph Neural Network, VISTA dataset 

Summary:
The article introduces an innovative approach that combines multi-dimensional Hawkes processes with Graph Neural Network to model opinion propagation dynamics in social media. The extended multi-dimensional Hawkes process captures hierarchical structures, multi-dimensional interactions, and mutual influences across various topics, forming a complex propagation network. Additionally, the authors present the VISTA dataset, comprising 159 trending topics with detailed sentiment annotations across 11 categories. This dataset allows for a comprehensive understanding of public opinion dynamics across different domains like politics, entertainment, sports, health, and medicine. The approach offers strong interpretability by linking sentiment propagation to comment hierarchy and temporal evolution, serving as a robust baseline for future research. 

<br /><br />Summary: <div>
arXiv:2504.15072v1 Announce Type: new 
Abstract: The rapid development of social media has significantly reshaped the dynamics of public opinion, resulting in complex interactions that traditional models fail to effectively capture. To address this challenge, we propose an innovative approach that integrates multi-dimensional Hawkes processes with Graph Neural Network, modeling opinion propagation dynamics among nodes in a social network while considering the intricate hierarchical relationships between comments. The extended multi-dimensional Hawkes process captures the hierarchical structure, multi-dimensional interactions, and mutual influences across different topics, forming a complex propagation network. Moreover, recognizing the lack of high-quality datasets capable of comprehensively capturing the evolution of public opinion dynamics, we introduce a new dataset, VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015 second-level comments, and 29,578 third-level comments, covering diverse domains such as politics, entertainment, sports, health, and medicine. The dataset is annotated with detailed sentiment labels across 11 categories and clearly defined hierarchical relationships. When combined with our method, it offers strong interpretability by linking sentiment propagation to the comment hierarchy and temporal evolution. Our approach provides a robust baseline for future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Binary Opinions: A Deep Reinforcement Learning-Based Approach to Uncertainty-Aware Competitive Influence Maximization</title>
<link>https://arxiv.org/abs/2504.15131</link>
<guid>https://arxiv.org/abs/2504.15131</guid>
<content:encoded><![CDATA[
<div> Keywords: Competitive Influence Maximization, Deep Reinforcement Learning, Subjective Logic, Uncertainty, Online Social Networks

Summary: <br /><br />The article introduces DRIM, a novel framework for the Competitive Influence Maximization (CIM) problem in online social networks. DRIM leverages Deep Reinforcement Learning (DRL) and Subjective Logic to model uncertainty in user opinions and preferences, enhancing decision-making in seed selection for information propagation. The framework incorporates an Uncertainty-based Opinion Model (UOM) to capture realistic user uncertainty and optimize the spread of true information while countering false information. Results demonstrate that DRIM-based CIM schemes outperform existing methods in influence spread and efficiency. Sensitivity analysis reveals that network observability and information propagation positively impact performance, while high network activity can mitigate the effects of initial user biases. Overall, DRIM offers a comprehensive approach to CIM that embraces uncertainty and improves influence maximization strategies in online social networks. <div>
arXiv:2504.15131v1 Announce Type: new 
Abstract: The Competitive Influence Maximization (CIM) problem involves multiple entities competing for influence in online social networks (OSNs). While Deep Reinforcement Learning (DRL) has shown promise, existing methods often assume users' opinions are binary and ignore their behavior and prior knowledge. We propose DRIM, a multi-dimensional uncertainty-aware DRL-based CIM framework that leverages Subjective Logic (SL) to model uncertainty in user opinions, preferences, and DRL decision-making. DRIM introduces an Uncertainty-based Opinion Model (UOM) for a more realistic representation of user uncertainty and optimizes seed selection for propagating true information while countering false information. In addition, it quantifies uncertainty in balancing exploration and exploitation. Results show that UOM significantly enhances true information spread and maintains influence against advanced false information strategies. DRIM-based CIM schemes outperform state-of-the-art methods by up to 57% and 88% in influence while being up to 48% and 77% faster. Sensitivity analysis indicates that higher network observability and greater information propagation boost performance, while high network activity mitigates the effect of users' initial biases.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An experimental study of the influence of anonymous information on social media users</title>
<link>https://arxiv.org/abs/2504.15215</link>
<guid>https://arxiv.org/abs/2504.15215</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, anonymous information, influence, opinions, agent-based modeling

Summary: 
The study aimed to investigate the impact of anonymous information on individuals' opinions. Through an online experiment with participants from the U.S., the results showed that anonymous comments can influence up to half of the participants' opinion selections, potentially altering popularity rankings. Agent-based modeling was used to understand how this influence occurs, revealing a straightforward mechanism at play. The study also found that the strength of influence lessens as participants' confidence in their selections increases. Additionally, participants with higher confidence in their initial opinions were less likely to change them based on anonymous information. This study emphasizes the significant role that anonymous information from social media can play in shaping individuals' opinions, indicating the need for caution and critical thinking when consuming such content.<br /><br />Summary: <div>
arXiv:2504.15215v1 Announce Type: new 
Abstract: Increasingly, people use social media for their day-to-day interactions and as a source of information, even though much of this information is practically anonymous. This raises the question: does anonymous information influence its recipients? We conducted an online, two-phase, preregistered experiment using a nationally representative sample of participants from the U.S. to find the answer. To avoid biases of opinions among participants, in the first phase, each participant examines ten Rorschach inkblots and chooses one of four opinions assigned to each inkblot. In the second phase, the participants are randomly assigned to one of four distinct information conditions and are asked to revisit their opinions for the same ten inkblots. Conditions ranged from repeating phase one to receiving anonymous comments about certain opinions. Results were consistent with the preregistration. Importantly, anonymous comments shown in phase two influence up to half of the participants' opinion selections. To better understand the role of anonymous comments in influencing the selections of opinions, we implemented agent-based modeling (ABM). ABM results suggest that a straightforward mechanism can explain the impact of such information. Overall, our results indicate that even anonymous information can have a significant impact on its recipients, potentially altering their popularity rankings. However, the strength of such influence weakens when recipients' confidence in their selections increases. Additionally, we found that participants' confidence in the first phase is inversely related to the number of change opinions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Theoretic Approach for Exploring the Relationship between EV Adoption and Charging Infrastructure Growth</title>
<link>https://arxiv.org/abs/2504.13902</link>
<guid>https://arxiv.org/abs/2504.13902</guid>
<content:encoded><![CDATA[
<div> Electric Vehicles, Charging Infrastructure, Graph Model, Time Granularity, Causality <br />
<br />
Summary: This study explores the relationship between Electric Vehicle (EV) adoption and Charging Infrastructure (CI) growth in 137 counties across six states in the U.S. Using a graph model and different time granularities, the analysis reveals that lower levels of time granularity lead to more homogeneous clusters, showing differences in EV adoption and CI growth. Causal relationships between EV adoption and CI growth are identified, with causality more prevalent in Early Adoption scenarios compared to Late Adoption ones. However, causal effects in Early Adoption are slower than in Late Adoption. The findings highlight the importance of understanding the complex dynamics between EV adoption and CI growth to facilitate the widespread adoption of electrified vehicles and address challenges in reducing CO2 emissions and natural resource depletion. <br /> <div>
arXiv:2504.13902v1 Announce Type: cross 
Abstract: The increasing global demand for conventional energy has led to significant challenges, particularly due to rising CO2 emissions and the depletion of natural resources. In the U.S., light-duty vehicles contribute significantly to transportation sector emissions, prompting a global shift toward electrified vehicles (EVs). Among the challenges that thwart the widespread adoption of EVs is the insufficient charging infrastructure (CI). This study focuses on exploring the complex relationship between EV adoption and CI growth. Employing a graph theoretic approach, we propose a graph model to analyze correlations between EV adoption and CI growth across 137 counties in six states. We examine how different time granularities impact these correlations in two distinct scenarios: Early Adoption and Late Adoption. Further, we conduct causality tests to assess the directional relationship between EV adoption and CI growth in both scenarios. Our main findings reveal that analysis using lower levels of time granularity result in more homogeneous clusters, with notable differences between clusters in EV adoption and those in CI growth. Additionally, we identify causal relationships between EV adoption and CI growth in 137 counties, and show that causality is observed more frequently in Early Adoption scenarios than in Late Adoption ones. However, the causal effects in Early Adoption are slower than those in Late Adoption.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Conspiratorial Narratives within Arabic Online Content</title>
<link>https://arxiv.org/abs/2504.14037</link>
<guid>https://arxiv.org/abs/2504.14037</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, Arabic digital spaces, Named Entity Recognition, Topic Modeling, socio-political contexts

Summary: 
This study examines the spread of conspiracy theories in Arabic online platforms by using computational analysis techniques. The researchers apply Named Entity Recognition and Topic Modeling, particularly the Top2Vec algorithm, to analyze data from Arabic blogs and Facebook. They identify and classify various conspiratorial narratives, including gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering theories. The research demonstrates how these narratives are deeply ingrained in Arabic social media discussions, influenced by regional historical, cultural, and socio-political factors. By leveraging advanced Natural Language Processing methods with Arabic content, this study fills a gap in conspiracy theory research that has traditionally focused on English-language or offline data. The findings offer fresh insights into how conspiracy theories manifest and evolve in Arabic digital spaces, contributing to a better understanding of their impact on public discourse in the Arab world.

<br /><br />Summary: <div>
arXiv:2504.14037v1 Announce Type: cross 
Abstract: This study investigates the spread of conspiracy theories in Arabic digital spaces through computational analysis of online content. By combining Named Entity Recognition and Topic Modeling techniques, specifically the Top2Vec algorithm, we analyze data from Arabic blogs and Facebook to identify and classify conspiratorial narratives. Our analysis uncovers six distinct categories: gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering. The research highlights how these narratives are deeply embedded in Arabic social media discourse, shaped by regional historical, cultural, and sociopolitical contexts. By applying advanced Natural Language Processing methods to Arabic content, this study addresses a gap in conspiracy theory research, which has traditionally focused on English-language content or offline data. The findings provide new insights into the manifestation and evolution of conspiracy theories in Arabic digital spaces, enhancing our understanding of their role in shaping public discourse in the Arab world.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matrix Factorization with Dynamic Multi-view Clustering for Recommender System</title>
<link>https://arxiv.org/abs/2504.14565</link>
<guid>https://arxiv.org/abs/2504.14565</guid>
<content:encoded><![CDATA[
<div> dynamic multi-view clustering, matrix factorization, recommender systems, representation learning, interpretability

Summary:<br /><br />Matrix Factorization with Dynamic Multi-view Clustering (MFDMC) is proposed as a unified framework for efficient and comprehensive representation learning in recommender systems. MFDMC addresses the challenges of large-scale applications by performing end-to-end training and leveraging dynamic multi-view clustering. This approach improves efficiency and interpretability by adaptively pruning poorly formed clusters and capturing diverse roles of entities across views. The proposed framework demonstrates superior performance in recommender systems and other representation learning domains, such as computer vision. MFDMC's scalability and versatility make it a promising solution for web-scale applications. <div>
arXiv:2504.14565v1 Announce Type: cross 
Abstract: Matrix factorization (MF), a cornerstone of recommender systems, decomposes user-item interaction matrices into latent representations. Traditional MF approaches, however, employ a two-stage, non-end-to-end paradigm, sequentially performing recommendation and clustering, resulting in prohibitive computational costs for large-scale applications like e-commerce and IoT, where billions of users interact with trillions of items. To address this, we propose Matrix Factorization with Dynamic Multi-view Clustering (MFDMC), a unified framework that balances efficient end-to-end training with comprehensive utilization of web-scale data and enhances interpretability. MFDMC leverages dynamic multi-view clustering to learn user and item representations, adaptively pruning poorly formed clusters. Each entity's representation is modeled as a weighted projection of robust clusters, capturing its diverse roles across views. This design maximizes representation space utilization, improves interpretability, and ensures resilience for downstream tasks. Extensive experiments demonstrate MFDMC's superior performance in recommender systems and other representation learning domains, such as computer vision, highlighting its scalability and versatility.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Telegram as a Battlefield: Kremlin-related Communications during the Russia-Ukraine Conflict</title>
<link>https://arxiv.org/abs/2501.01884</link>
<guid>https://arxiv.org/abs/2501.01884</guid>
<content:encoded><![CDATA[
<div> Dataset, Telegram, Russia-Ukraine conflict, content moderation, misinformation 

Summary: 
The paper introduces a dataset of posts from both pro-Kremlin and anti-Kremlin Telegram channels, collected before and after the Russian invasion of Ukraine. The dataset includes over 4 million posts from 404 pro-Kremlin channels and over 1 million posts from 114 anti-Kremlin channels. It outlines the data collection process, processing methods, and characteristics of the dataset. The study highlights the dissemination of Pro-Kremlin narratives and potential misinformation on Telegram due to its minimal content moderation policies. It also notes the spread of anti-Kremlin narratives, including war footage, troop movements, and air raid warnings. The paper concludes by discussing the research opportunities this dataset presents for scholars in various disciplines. <br /><br />Summary: <div>
arXiv:2501.01884v3 Announce Type: replace 
Abstract: Telegram emerged as a crucial platform for both parties during the conflict between Russia and Ukraine. Per its minimal policies for content moderation, Pro-Kremlin narratives and potential misinformation were spread on Telegram, while anti-Kremlin narratives with related content were also propagated, such as war footage, troop movements, maps of bomb shelters, and air raid warnings. This paper presents a dataset of posts from both pro-Kremlin and anti-Kremlin Telegram channels, collected over a period spanning a year before and a year after the Russian invasion. The dataset comprises 404 pro-Kremlin channels with 4,109,645 posts and 114 anti-Kremlin channels with 1,117,768 posts. We provide details on the data collection process, processing methods, and dataset characterization. Lastly, we discuss the potential research opportunities this dataset may enable researchers across various disciplines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences</title>
<link>https://arxiv.org/abs/2504.09428</link>
<guid>https://arxiv.org/abs/2504.09428</guid>
<content:encoded><![CDATA[
<div> Keywords: online games, friend recommendation, user features, structural information, FROG<br />
<br />
Summary: 
The article discusses the importance of friend recommendation in online games due to the popularity of mobile devices. Existing approaches have limitations in incorporating multi-modal user features and structural information from friendship graphs effectively. These limitations include ignoring high-order structural proximity between users, failing to learn pairwise relevance between users at a modality-specific level, and not capturing both local and global user preferences on different modalities. To address these issues, the paper introduces an end-to-end model called FROG that better models user preferences for potential friends. The model has been evaluated through comprehensive experiments, including offline evaluation and online deployment at Tencent, demonstrating its superiority over existing approaches. <div>
arXiv:2504.09428v2 Announce Type: replace 
Abstract: Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (e.g., images and texts) with the structural information in the friendship graph, due to the following limitations: (1) some of them ignore the high-order structural proximity between users, (2) some fail to learn the pairwise relevance between users at modality-specific level, and (3) some cannot capture both the local and global user preferences on different modalities. By addressing these issues, in this paper, we propose an end-to-end model FROG that better models the user preferences on potential friends. Comprehensive experiments on both offline evaluation and online deployment at Tencent have demonstrated the superiority of FROG over existing approaches.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrustMap: Mapping Truthfulness Stance of Social Media Posts on Factual Claims for Geographical Analysis</title>
<link>https://arxiv.org/abs/2504.10511</link>
<guid>https://arxiv.org/abs/2504.10511</guid>
<content:encoded><![CDATA[
<div> TrustMap, social media, factual claims, misinformation, stance detection  
Summary:  
TrustMap is a tool that categorizes social media posts into positive, negative, or neutral stances based on the truthfulness of factual claims, using advanced language models for automatic classification. The tool analyzes regional variations in stance patterns across the U.S., allowing users to explore how public opinions differ geographically. By connecting stance detection with geographical analysis, TrustMap provides valuable insights into how people engage with factual claims on social media. This innovative approach helps to combat the spread of misinformation and understand how individuals form opinions and make decisions based on the information they encounter online. <div>
arXiv:2504.10511v2 Announce Type: replace 
Abstract: Factual claims and misinformation circulate widely on social media and affect how people form opinions and make decisions. This paper presents a truthfulness stance map (TrustMap), an application that identifies and maps public stances toward factual claims across U.S. regions. Each social media post is classified as positive, negative, or neutral/no stance, based on whether it believes a factual claim is true or false, expresses uncertainty about the truthfulness, or does not explicitly take a position on the claim's truthfulness. The tool uses a retrieval-augmented model with fine-tuned language models for automatic stance classification. The stance classification results and social media posts are grouped by location to show how stance patterns vary geographically. TrustMap allows users to explore these patterns by claim and region and connects stance detection with geographical analysis to better understand public engagement with factual claims.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Graph Rewiring and Feature Denoising via Spectral Resonance</title>
<link>https://arxiv.org/abs/2408.07191</link>
<guid>https://arxiv.org/abs/2408.07191</guid>
<content:encoded><![CDATA[
<div> Algorithm, Denoise, Rewire, Graph Data, Node Classification

Summary: 
The proposed algorithm, Jointly Denoise and Rewire (JDR), simultaneously denoises node features and rewires the graph to enhance the performance of graph neural networks for node classification. By aligning the leading spectral spaces of the graph and feature matrices, JDR effectively handles graphs with multiple classes and varying levels of homophily or heterophily. The non-convex optimization problem associated with JDR is approximately solved, leading to improved results compared to existing rewiring methods across various synthetic and real-world node classification tasks. The theoretical justification for JDR in a simplified scenario further supports its effectiveness in enhancing the accuracy of downstream GNNs. <div>
arXiv:2408.07191v4 Announce Type: replace-cross 
Abstract: When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to jointly denoise the features and rewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Another Hour on TikTok: Reverse-engineering unique identifiers to obtain a complete slice of TikTok</title>
<link>https://arxiv.org/abs/2504.13279</link>
<guid>https://arxiv.org/abs/2504.13279</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, platform, dataset, metadata, posts

Summary:
Through a newly developed method, researchers extracted a representative sample from TikTok to collect a vast amount of post data. The dataset obtained includes post metadata, video media data, and comments, providing a close to complete snapshot of TikTok activity. Critical statistics of the platform were reported, estimating that 117 million posts were produced on the day observed. This method allowed for a deeper understanding of TikTok's impact on global events and addressed issues in determining fundamental characteristics of the platform. The research sheds light on the massive scale of TikTok and its significance in today's digital landscape. <div>
arXiv:2504.13279v1 Announce Type: new 
Abstract: TikTok is now a massive platform, and has a deep impact on global events. But for all the preliminary studies done on it, there are still issues with determining fundamental characteristics of the platform. We develop a method to extract a representative sample from a specific time range on TikTok, and use it to collect >99\% of posts from a full hour on the platform, alongside a dataset of >99\% of posts from a single minute from each hour of a day. Through this, we obtain post metadata, video media data, and comments from a close to complete slice of TikTok. Using this dataset, we report the critical statistics of the platform, notably estimating a total of 117 million posts produced on the day we looked at on TikTok.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Informed by Micro and Mesoscopic Statistical Physics Methods for Community Detection</title>
<link>https://arxiv.org/abs/2504.13538</link>
<guid>https://arxiv.org/abs/2504.13538</guid>
<content:encoded><![CDATA[
<div> machine learning, community detection, complex networks, ensemble learning, structural patterns

Summary: 
The article introduces a novel framework that combines machine learning with statistical physics to enhance community detection in complex networks. Previous methods have focused on mesoscopic structures but struggle with incorporating fine-grained node similarities. The proposed framework integrates micro-level node-pair similarities into mesoscopic community structures using ensemble learning models. Experimental evaluations on artificial and real-world networks show that the framework outperforms conventional methods by achieving higher modularity and improved accuracy in NMI and ARI metrics. When ground-truth labels are available, the framework yields the most accurate results, effectively recovering real-world community structures with minimal misclassifications. The analysis of the correlation between node-pair similarity and evaluation metrics indicates a strong relationship, emphasizing the importance of node-pair similarity in enhancing detection accuracy. Overall, the study highlights the synergistic relationship between machine learning and statistical physics in uncovering complex structural patterns in networks. 

<br /><br />Summary: <div>
arXiv:2504.13538v1 Announce Type: new 
Abstract: Community detection plays a crucial role in understanding the structural organization of complex networks. Previous methods, particularly those from statistical physics, primarily focus on the analysis of mesoscopic network structures and often struggle to integrate fine-grained node similarities. To address this limitation, we propose a low-complexity framework that integrates machine learning to embed micro-level node-pair similarities into mesoscopic community structures. By leveraging ensemble learning models, our approach enhances both structural coherence and detection accuracy. Experimental evaluations on artificial and real-world networks demonstrate that our framework consistently outperforms conventional methods, achieving higher modularity and improved accuracy in NMI and ARI. Notably, when ground-truth labels are available, our approach yields the most accurate detection results, effectively recovering real-world community structures while minimizing misclassifications. To further explain our framework's performance, we analyze the correlation between node-pair similarity and evaluation metrics. The results reveal a strong and statistically significant correlation, underscoring the critical role of node-pair similarity in enhancing detection accuracy. Overall, our findings highlight the synergy between machine learning and statistical physics, demonstrating how machine learning techniques can enhance network analysis and uncover complex structural patterns.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reinforcement Learning Method to Factual and Counterfactual Explanations for Session-based Recommendation</title>
<link>https://arxiv.org/abs/2504.13632</link>
<guid>https://arxiv.org/abs/2504.13632</guid>
<content:encoded><![CDATA[
<div> Session-based Recommendation, Explanation, FCESR, Factual, Counterfactual<br />
Summary:<br />
Session-based Recommendation (SR) systems often lack transparency in their recommendations. The FCESR framework aims to provide explanations for SR models by highlighting both factual and counterfactual factors influencing recommendations. This novel approach uses combinatorial optimization and reinforcement learning to identify the critical sequence of items impacting recommendations. By incorporating factual and counterfactual insights into a contrastive learning paradigm, the framework enhances SR accuracy significantly. Through extensive evaluations on various datasets and SR architectures, FCESR not only improves recommendation accuracy but also enhances the quality and interpretability of explanations. This advancement in transparency paves the way for more trustworthy recommendation systems. <br />Summary: <div>
arXiv:2504.13632v1 Announce Type: new 
Abstract: Session-based Recommendation (SR) systems have recently achieved considerable success, yet their complex, "black box" nature often obscures why certain recommendations are made. Existing explanation methods struggle to pinpoint truly influential factors, as they frequently depend on static user profiles or fail to grasp the intricate dynamics within user sessions. In response, we introduce FCESR (Factual and Counterfactual Explanations for Session-based Recommendation), a novel framework designed to illuminate SR model predictions by emphasizing both the sufficiency (factual) and necessity (counterfactual) of recommended items. By recasting explanation generation as a combinatorial optimization challenge and leveraging reinforcement learning, our method uncovers the minimal yet critical sequence of items influencing recommendations. Moreover, recognizing the intrinsic value of robust explanations, we innovatively utilize these factual and counterfactual insights within a contrastive learning paradigm, employing them as high-quality positive and negative samples to fine-tune and significantly enhance SR accuracy. Extensive qualitative and quantitative evaluations across diverse datasets and multiple SR architectures confirm that our framework not only boosts recommendation accuracy but also markedly elevates the quality and interpretability of explanations, thereby paving the way for more transparent and trustworthy recommendation systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propagational Proxy Voting</title>
<link>https://arxiv.org/abs/2504.13641</link>
<guid>https://arxiv.org/abs/2504.13641</guid>
<content:encoded><![CDATA[
<div> Fractional Votes, Expected Utility, Voting Matrix, Absorbing Markov Chains, Budget Allocation <br />
Summary: 
This paper introduces a novel voting process where voters can allocate fractional votes based on their expected utility across various domains. By using a voting matrix to reflect preferences, the approach allows for a more nuanced expression of preferences by calculating results and relevance within each node. The authors leverage absorbing Markov chains to determine consensus and assess influence within participating nodes. An experiment involving 69 students on budget allocation demonstrates the practical application of this method. The study showcases the effectiveness of this approach in capturing diverse preferences and providing a more detailed understanding of individual and collective decision-making processes. <div>
arXiv:2504.13641v1 Announce Type: new 
Abstract: This paper proposes a voting process in which voters allocate fractional votes to their expected utility in different domains: over proposals, other participants, and sets containing proposals and participants. This approach allows for a more nuanced expression of preferences by calculating the result and relevance within each node. We modeled this by creating a voting matrix that reflects their preference. We use absorbing Markov chains to gain the consensus, and also calculate the influence within the participating nodes. We illustrate this method in action through an experiment with 69 students using a budget allocation topic.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Stereotypes: Exploring How Minority College Students Experience Stigma on Reddit</title>
<link>https://arxiv.org/abs/2504.13674</link>
<guid>https://arxiv.org/abs/2504.13674</guid>
<content:encoded><![CDATA[
<div> stigma, minority students, college, stereotype, discrimination
<br />
Summary: 
The research focuses on the unique challenges faced by minority college students, influenced by their gender/sexual orientation, race, religion, and academic environment. The study examines stigma processes experienced by minority groups, particularly in online spaces like the r/college subreddit. By utilizing a Stereotype-BERT model, the study identifies stages of stigma processes such as labeling, stereotyping, separation, status loss, and discrimination. Professional identity posts are mainly associated with stereotyping, while racial identity posts are more prevalent in status loss and discrimination. Intersectional posts, reflecting compounded vulnerabilities due to intersecting identities, show a higher frequency of status loss and discrimination. The study emphasizes the importance of intersectional approaches in identifying stigma processes to promote equity for minority groups, especially racial minorities. 
<br /><br /> <div>
arXiv:2504.13674v1 Announce Type: new 
Abstract: Minority college students face unique challenges shaped by their identities based on their gender/sexual orientation, race, religion, and academic institutions, which influence their academic and social experiences. Although research has highlighted the challenges faced by individual minority groups, the stigma process-labeling, stereotyping, separation, status loss, and discrimination-that underpin these experiences remains underexamined, particularly in the online spaces where college students are highly active. We address these gaps by examining posts on subreddit, r/college, as indicators for stigma processes, our approach applies a Stereotype-BERT model, including stance toward each stereotype. We extend the stereotype model to encompass status loss and discrimination by using semantic distance with their reference sentences. Our analyses show that professional indicated posts are primarily labeled under the stereotyping stage, whereas posts indicating racial are highly represented in status loss and discrimination. Intersectional identified posts are more frequently associated with status loss and discrimination. The findings of this study highlight the need for multifaceted intersectional approaches to identifying stigma, which subsequently serve as indicators to promote equity for minority groups, especially racial minorities and those experiencing compounded vulnerabilities due to intersecting identities.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models</title>
<link>https://arxiv.org/abs/2504.13261</link>
<guid>https://arxiv.org/abs/2504.13261</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, pedagogical grammar, foreign language education, evaluation 
Summary: 

The paper introduces CPG-EVAL, a benchmark designed to assess large language models' (LLMs) knowledge of pedagogical grammar in foreign language instruction. The benchmark includes tasks to test grammar recognition, fine-grained distinctions, categorical discrimination, and resistance to linguistic interference. Smaller LLMs perform well in single language instance tasks but struggle with interference and multiple instances. Larger models show better resistance to interference but still need accuracy improvement. The study emphasizes the importance of aligning LLMs with educational goals and creating more rigorous benchmarks. CPG-EVAL aims to guide the deployment of LLMs in Chinese language teaching contexts. The evaluation provides insights for educators, policymakers, and developers to understand LLM capabilities in education and informs decisions on their integration in foreign language instruction. Further research is needed to enhance model alignment and educational suitability. 

<br /><br />Summary: <div>
arXiv:2504.13261v1 Announce Type: cross 
Abstract: Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT has significantly impacted foreign language education, yet their pedagogical grammar competence remains under-assessed. This paper introduces CPG-EVAL, the first dedicated benchmark specifically designed to evaluate LLMs' knowledge of pedagogical grammar within the context of foreign language instruction. Methodology: The benchmark comprises five tasks designed to assess grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Findings: Smaller-scale models can succeed in single language instance tasks, but struggle with multiple instance tasks and interference from confusing instances. Larger-scale models show better resistance to interference but still have significant room for accuracy improvement. The evaluation indicates the need for better instructional alignment and more rigorous benchmarks, to effectively guide the deployment of LLMs in educational contexts. Value: This study offers the first specialized, theory-driven, multi-tiered benchmark framework for systematically evaluating LLMs' pedagogical grammar competence in Chinese language teaching contexts. CPG-EVAL not only provides empirical insights for educators, policymakers, and model developers to better gauge AI's current abilities in educational settings, but also lays the groundwork for future research on improving model alignment, enhancing educational suitability, and ensuring informed decision-making concerning LLM integration in foreign language instruction.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces</title>
<link>https://arxiv.org/abs/2504.13277</link>
<guid>https://arxiv.org/abs/2504.13277</guid>
<content:encoded><![CDATA[
<div> Interpersonal Theory of Suicide, Suicidal Ideation, Machine Learning, Natural Language Analysis, AI Chatbots
<br />
Summary: 
The study utilized the Interpersonal Theory of Suicide to analyze posts from Reddit's r/SuicideWatch, identifying dimensions and risk factors of suicidal ideation. High-risk posts often included mentions of planning, attempts, and pain. Supportive responses were analyzed, showing varied reactions at different stages of suicidal ideation posts. AI chatbots were explored for providing support, showing improved structural coherence but lacking in dynamic and personalized responses. Overall, the study highlights the importance of understanding factors affecting high-risk suicidal intent and the need for careful consideration in developing AI-driven interventions for mental health support. <div>
arXiv:2504.13277v1 Announce Type: cross 
Abstract: Suicide is a critical global public health issue, with millions experiencing suicidal ideation (SI) each year. Online spaces enable individuals to express SI and seek peer support. While prior research has revealed the potential of detecting SI using machine learning and natural language analysis, a key limitation is the lack of a theoretical framework to understand the underlying factors affecting high-risk suicidal intent. To bridge this gap, we adopted the Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607 posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions (Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired Capability of Suicide). We found that high-risk SI posts express planning and attempts, methods and tools, and weaknesses and pain. In addition, we also examined the language of supportive responses through psycholinguistic and content analyses to find that individuals respond differently to different stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI chatbots in providing effective supportive responses to suicidal ideation posts. We found that although AI improved structural coherence, expert evaluations highlight persistent shortcomings in providing dynamic, personalized, and deeply empathetic support. These findings underscore the need for careful reflection and deeper understanding in both the development and consideration of AI-driven interventions for effective mental health support.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal</title>
<link>https://arxiv.org/abs/2504.13284</link>
<guid>https://arxiv.org/abs/2504.13284</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet penetration, Africa, mobile Internet, Senegal, social networks

Summary:
Young people in Senegal are increasingly using the Internet, especially social networks, as Internet penetration rates rise in Africa. The availability of smartphones has further boosted mobile Internet usage among the youth. However, the limited number of operators in the market restricts the choices available, impacting the value for money for users. In this study, Twitter and Facebook comments were analyzed to understand the sentiment of young people towards the price of mobile Internet in Senegal and its perceived quality. The sentiment analysis model revealed the general feelings of the users towards the service. Overall, the study sheds light on the importance of affordable and quality mobile Internet services in Senegal to meet the demands of the younger population who heavily rely on social networks for communication and self-expression.<br /><br />Summary: <div>
arXiv:2504.13284v1 Announce Type: cross 
Abstract: Internet penetration rates in Africa are rising steadily, and mobile Internet is getting an even bigger boost with the availability of smartphones. Young people are increasingly using the Internet, especially social networks, and Senegal is no exception to this revolution. Social networks have become the main means of expression for young people. Despite this evolution in Internet access, there are few operators on the market, which limits the alternatives available in terms of value for money. In this paper, we will look at how young people feel about the price of mobile Internet in Senegal, in relation to the perceived quality of the service, through their comments on social networks. We scanned a set of Twitter and Facebook comments related to the subject and applied a sentiment analysis model to gather their general feelings.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Can't believe I'm crying over an anime girl": Public Parasocial Grieving and Coping Towards VTuber Graduation and Termination</title>
<link>https://arxiv.org/abs/2504.13421</link>
<guid>https://arxiv.org/abs/2504.13421</guid>
<content:encoded><![CDATA[
<div> Keywords: Virtual YouTubers, parasocial relationships, VTuber retirements, coping methods, community dynamics

Summary:
This study explores the dynamics of viewer-VTuber relationships, focusing on how English-speaking viewers cope with the retirement of VTubers. The research categorizes different types of VTuber retirements and analyzes Reddit posts to understand viewer reactions. Findings show that viewers experience emotions like sadness, shock, and loyalty, with coping methods resembling those used when losing loved ones. As time passes, emotions like sadness and love decrease while regret and loyalty show opposite trends. Viewers' reactions also highlight the VTuber identity's place within a larger community of content creators and viewers. The study discusses design implications and their impact on the VTuber ecosystem, providing insights for future research in this emerging field.<br /><br />Summary: This study delves into how English-speaking viewers navigate the retirement of Virtual YouTubers, categorizing different types of retirements and analyzing viewer reactions on Reddit. The findings reveal a range of emotions experienced by viewers, with coping methods resembling those used in times of loss. Over time, emotions like sadness and love decrease, while regret and loyalty show contrasting trends. Viewer reactions also shed light on the VTuber identity's role within a broader community of creators and viewers. The study concludes by discussing design implications and their potential impact on the VTuber ecosystem, suggesting directions for further research in this evolving field. <div>
arXiv:2504.13421v1 Announce Type: cross 
Abstract: Despite the significant increase in popularity of Virtual YouTubers (VTubers), research on the unique dynamics of viewer-VTuber parasocial relationships is nascent. This work investigates how English-speaking viewers grieved VTubers whose identities are no longer used, an interesting context as the nakanohito (i.e., the person behind the VTuber identity) is usually alive post-retirement and might "reincarnate" as another VTuber. We propose a typology for VTuber retirements and analyzed 13,655 Reddit posts and comments spanning nearly three years using mixed-methods. Findings include how viewers coped using methods similar to when losing loved ones, alongside novel coping methods reflecting different attachment styles. Although emotions like sadness, shock, concern, disapproval, confusion, and love decreased with time, regret and loyalty showed opposite trends. Furthermore, viewers' reactions situated a VTuber identity within a community of content creators and viewers. We also discuss design implications alongside implications on the VTuber ecosystem and future research directions.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dataset of the Representatives Elected in France During the Fifth Republic</title>
<link>https://arxiv.org/abs/2504.02869</link>
<guid>https://arxiv.org/abs/2504.02869</guid>
<content:encoded><![CDATA[
<div> Keywords: electoral system, political representation, France, database, political change<br />
Summary:<br /> 
This article discusses the importance of the electoral system in shaping democracy, particularly in France. It highlights the challenges in accessing data on elected representatives and introduces a new relational database that compiles information on representatives in France since the Fifth Republic. This database provides a valuable resource for analyzing trends in political representation, party dynamics, gender equality, and the professionalization of politics. By offering a longitudinal view of elected representatives, the database enables researchers to study the institutional stability of the Fifth Republic and identify factors driving political change. Overall, this database enhances understanding of political processes in France and facilitates in-depth analyses of the country's political landscape over time. <br /><br />Summary: <div>
arXiv:2504.02869v2 Announce Type: replace 
Abstract: The electoral system is a cornerstone of democracy, shaping the structure of political competition, representation, and accountability. In the case of France, it is difficult to access data describing elected representatives, though, as they are scattered across a number of sources, including public institutions, but also academic and individual efforts. This article presents a unified relational database that aims at tackling this issue by gathering information regarding representatives elected in France over the whole Fifth Republic (1958-present). This database constitutes an unprecedented resource for analyzing the evolution of political representation in France, exploring trends in party system dynamics, gender equality, and the professionalization of politics. By providing a longitudinal view of French elected representatives, the database facilitates research on the institutional stability of the Fifth Republic, offering insights into the factors of political change.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning when to rank: Estimation of partial rankings from sparse, noisy comparisons</title>
<link>https://arxiv.org/abs/2501.02505</link>
<guid>https://arxiv.org/abs/2501.02505</guid>
<content:encoded><![CDATA[
<div> Bayesian methodology, partial rankings, pairwise comparisons, Bradley-Terry model, inference-based methods <br />
Summary: <br />
- The article introduces a Bayesian methodology for learning partial rankings from pairwise comparison data, addressing the challenge of distinguishing between items with limited or noisy comparisons.
- Current inference-based ranking methods often assign unique ranks to each item, even when there is insufficient evidence to differentiate their performance.
- The proposed framework allows for the incorporation of ties in rankings, only distinguishing between items when there is enough evidence from the data.
- An agglomerative algorithm is developed for Maximum A Posteriori (MAP) inference of partial rankings.
- The method is evaluated on real and synthetic network datasets, demonstrating its ability to provide a more parsimonious summary of the data compared to traditional ranking methods, especially in cases of sparse observations. 

<br />
Summary: <div>
arXiv:2501.02505v2 Announce Type: replace-cross 
Abstract: A common task arising in various domains is that of ranking items based on the outcomes of pairwise comparisons, from ranking players and teams in sports to ranking products or brands in marketing studies and recommendation systems. Statistical inference-based methods such as the Bradley-Terry model, which extract rankings based on an underlying generative model of the comparison outcomes, have emerged as flexible and powerful tools to tackle the task of ranking in empirical data. In situations with limited and/or noisy comparisons, it is often challenging to confidently distinguish the performance of different items based on the evidence available in the data. However, existing inference-based ranking methods overwhelmingly choose to assign each item to a unique rank or score, suggesting a meaningful distinction when there is none. Here, we address this problem by developing a principled Bayesian methodology for learning partial rankings -- rankings with ties -- that distinguishes among the ranks of different items only when there is sufficient evidence available in the data. Our framework is adaptable to any statistical ranking method in which the outcomes of pairwise observations depend on the ranks or scores of the items being compared. We develop a fast agglomerative algorithm to perform Maximum A Posteriori (MAP) inference of partial rankings under our framework and examine the performance of our method on a variety of real and synthetic network datasets, finding that it frequently gives a more parsimonious summary of the data than traditional ranking, particularly when observations are sparse.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of Bluesky</title>
<link>https://arxiv.org/abs/2504.12902</link>
<guid>https://arxiv.org/abs/2504.12902</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, rapid growth, evolving network structure, user migrations, viral information diffusion 

Summary: 
Bluesky, a social media platform, experienced rapid growth and network evolution from August 2023 to February 2025. Multiple waves of user migrations contributed to the platform's establishment of a stable, actively engaged user base. The growth process led to the formation of a dense follower network characterized by clustering and hub features, facilitating the viral diffusion of information. These developments underscore the similarities in engagement and network structure between Bluesky and established social media platforms. This study sheds light on the dynamics of user engagement and network evolution in the context of Bluesky's growth trajectory. <div>
arXiv:2504.12902v1 Announce Type: new 
Abstract: This study investigates the rapid growth and evolving network structure of Bluesky from August 2023 to February 2025. Through multiple waves of user migrations, the platform has reached a stable, persistently active user base. The growth process has given rise to a dense follower network with clustering and hub features that favor viral information diffusion. These developments highlight engagement and structural similarities between Bluesky and established platforms.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media</title>
<link>https://arxiv.org/abs/2504.12325</link>
<guid>https://arxiv.org/abs/2504.12325</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, taxonomy, large language models, factual claims, automated construction

Summary:
LLMTaxo is a new framework that utilizes large language models to automatically generate a taxonomy of factual claims from social media. By creating topics at multiple granular levels, LLMTaxo helps stakeholders navigate the complex landscape of online discourse. The framework was tested on three diverse datasets using various models, and was evaluated using both human assessments and GPT-4. Results indicated that LLMTaxo effectively categorizes factual claims and highlights the superior performance of certain models on specific datasets. This innovative approach offers valuable insights for analyzing and comprehending online content, providing a more efficient way to classify and understand information shared on social media platforms. <div>
arXiv:2504.12325v1 Announce Type: cross 
Abstract: With the vast expansion of content on social media platforms, analyzing and comprehending online discourse has become increasingly complex. This paper introduces LLMTaxo, a novel framework leveraging large language models for the automated construction of taxonomy of factual claims from social media by generating topics from multi-level granularities. This approach aids stakeholders in more effectively navigating the social media landscapes. We implement this framework with different models across three distinct datasets and introduce specially designed taxonomy evaluation metrics for a comprehensive assessment. With the evaluations from both human evaluators and GPT-4, the results indicate that LLMTaxo effectively categorizes factual claims from social media, and reveals that certain models perform better on specific datasets.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Embeddings Track Social Group Changes Across 70 Years in China</title>
<link>https://arxiv.org/abs/2504.12327</link>
<guid>https://arxiv.org/abs/2504.12327</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese state-controlled media, word embeddings, social group representation, societal beliefs, linguistic analysis 

Summary: 
The article presents a computational analysis of Chinese state-controlled media from 1950 to 2019, focusing on how societal beliefs about social groups are reflected in language. The study utilizes diachronic word embeddings to analyze the evolution of representations of social groups in Chinese media. The findings suggest significant differences in Chinese representations compared to Western contexts, particularly in terms of ethnicity, economic status, and gender. The study reveals that stereotypes related to ethnicity, age, and body type remain stable over time, while representations of gender and economic classes undergo dramatic shifts in alignment with historical transformations. This research contributes to our understanding of how language encodes societal beliefs and social structures, emphasizing the importance of non-Western perspectives in computational social science. 

<br /><br />Summary: <div>
arXiv:2504.12327v1 Announce Type: cross 
Abstract: Language encodes societal beliefs about social groups through word patterns. While computational methods like word embeddings enable quantitative analysis of these patterns, studies have primarily examined gradual shifts in Western contexts. We present the first large-scale computational analysis of Chinese state-controlled media (1950-2019) to examine how revolutionary social transformations are reflected in official linguistic representations of social groups. Using diachronic word embeddings at multiple temporal resolutions, we find that Chinese representations differ significantly from Western counterparts, particularly regarding economic status, ethnicity, and gender. These representations show distinct evolutionary dynamics: while stereotypes of ethnicity, age, and body type remain remarkably stable across political upheavals, representations of gender and economic classes undergo dramatic shifts tracking historical transformations. This work advances our understanding of how officially sanctioned discourse encodes social structure through language while highlighting the importance of non-Western perspectives in computational social science.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"It Listens Better Than My Therapist": Exploring Social Media Discourse on LLMs as Mental Health Tool</title>
<link>https://arxiv.org/abs/2504.12337</link>
<guid>https://arxiv.org/abs/2504.12337</guid>
<content:encoded><![CDATA[
<div> mental health support, AI chatbots, user engagement, TikTok comments, ethical scrutiny 
Summary: 
This study examines user engagement with large language models (LLMs) like ChatGPT as mental health tools through the analysis of over 10,000 TikTok comments. Approximately 20% of comments reflect personal use, with users reporting positive attitudes towards LLMs for mental health support. Accessibility, emotional support, and perceived therapeutic value are commonly cited benefits. However, concerns around privacy, generic responses, and the lack of professional oversight are prominent. The study highlights the need for clinical and ethical scrutiny in the use of AI for mental health support, as user feedback does not indicate alignment with any therapeutic framework. The findings emphasize the growing relevance of AI in everyday practices while underscoring the importance of addressing privacy, ethical, and clinical considerations when utilizing AI chatbots for mental health support. 
<br /><br />Summary: <div>
arXiv:2504.12337v1 Announce Type: cross 
Abstract: The emergence of generative AI chatbots such as ChatGPT has prompted growing public and academic interest in their role as informal mental health support tools. While early rule-based systems have been around for several years, large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability. This study explores how users engage with LLMs as mental health tools by analyzing over 10,000 TikTok comments from videos referencing LLMs as mental health tools. Using a self-developed tiered coding schema and supervised classification models, we identify user experiences, attitudes, and recurring themes. Results show that nearly 20% of comments reflect personal use, with these users expressing overwhelmingly positive attitudes. Commonly cited benefits include accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and the lack of professional oversight remain prominent. It is important to note that the user feedback does not indicate which therapeutic framework, if any, the LLM-generated output aligns with. While the findings underscore the growing relevance of AI in everyday practices, they also highlight the urgent need for clinical and ethical scrutiny in the use of AI for mental health support.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis</title>
<link>https://arxiv.org/abs/2408.09613</link>
<guid>https://arxiv.org/abs/2408.09613</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots, misinformation, Sina Weibo, dataset, information spread

Summary: 
Social bots play a significant role in spreading misinformation on the Sina Weibo platform, as shown in a dataset containing annotations of both misinformation and social bots. Through extensive experiments, it is evident that the dataset is comprehensive, with distinguishable misinformation and real information, and high-quality social bot annotations. The analysis reveals that social bots are actively involved in disseminating information, contributing to echo chambers by amplifying similar content on the same topics, and generating content to manipulate public opinions. This study sheds light on the interplay between social bots and misinformation on social media platforms, highlighting the need for further research and measures to combat the spread of false information. 

<br /><br />Summary: <div>
arXiv:2408.09613v2 Announce Type: replace 
Abstract: The social media platform is an ideal medium to spread misinformation, where social bots might accelerate the spread. This paper is the first to explore the interplay between social bots and misinformation on the Sina Weibo platform. We construct a large-scale dataset that contains annotations of misinformation and social bots. From the misinformation perspective, this dataset is multimodal, containing 11,393 pieces of misinformation and 16,416 pieces of real information. From the social bot perspective, this dataset contains 65,749 social bots and 345,886 genuine accounts, where we propose a weak-supervised annotator to annotate automatically. Extensive experiments prove that the dataset is the most comprehensive, misinformation and real information are distinguishable, and social bots have high annotation quality. Further analysis illustrates that: (i) social bots are deeply involved in information spread; (ii) misinformation with the same topics has similar content, providing the basis of echo chambers, and social bots amplify this phenomenon; and (iii) social bots generate similar content aiming to manipulate public opinions.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governance as a complex, networked, democratic, satisfiability problem</title>
<link>https://arxiv.org/abs/2412.03421</link>
<guid>https://arxiv.org/abs/2412.03421</guid>
<content:encoded><![CDATA[
<div> modeling, governance structures, decision-making, social network, democratic governments

Summary:
The article examines different governance structures within democratic governments through a social network framework. By modeling decision-making as a satisfiability problem and information flow as a social hypergraph, the study explores various governance strategies from dictatorships to direct democracy. The research suggests that effective governance can be achieved through small overlapping decision groups that make specific decisions and share information. This approach allows even polarized populations to make coherent decisions with low coordination costs. The conceptual framework can simulate different governance strategies and their effectiveness in addressing complex societal challenges. This new perspective on governance structures offers insights into improving decision-making processes in democratic societies. 

<br /><br />Summary: <div>
arXiv:2412.03421v2 Announce Type: replace-cross 
Abstract: Democratic governments comprise a subset of a population whose goal is to produce coherent decisions, solving societal challenges while respecting the will of the people. New governance frameworks represent this as a social network rather than as a hierarchical pyramid with centralized authority. But how should this network be structured? We model the decisions a population must make as a satisfiability problem and the structure of information flow involved in decision-making as a social hypergraph. This framework allows to consider different governance structures, from dictatorships to direct democracy. Between these extremes, we find a regime of effective governance where small overlapping decision groups make specific decisions and share information. Effective governance allows even incoherent or polarized populations to make coherent decisions at low coordination costs. Beyond simulations, our conceptual framework can explore a wide range of governance strategies and their ability to tackle decision problems that challenge standard governments.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models</title>
<link>https://arxiv.org/abs/2504.00046</link>
<guid>https://arxiv.org/abs/2504.00046</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, disaster response, LLMs, classification techniques, generative AI 

Summary: 
This paper discusses the use of social media in disaster response and management, highlighting the need for automated, aggregated, and customized data analysis to provide actionable insights for various stakeholders. The methodology presented utilizes Language Model-based approaches to analyze social media posts during disasters, focusing on user-reported issues and challenges. By employing full-spectrum Language Models like BERT for precise classification and generative models like ChatGPT for tailored report generation, the system bridges the gap between raw feedback and stakeholder-specific reports. The comparison between standard approaches and the advanced methodology shows superior performance in both quantitative metrics and qualitative assessments. The methodology enhances coordination of relief efforts, resource distribution, and media communication, delivering precise insights for diverse stakeholders involved in disaster response. <div>
arXiv:2504.00046v2 Announce Type: replace-cross 
Abstract: In recent years, social media has emerged as a primary channel for users to promptly share feedback and issues during disasters and emergencies, playing a key role in crisis management. While significant progress has been made in collecting and analyzing social media content, there remains a pressing need to enhance the automation, aggregation, and customization of this data to deliver actionable insights tailored to diverse stakeholders, including the press, police, EMS, and firefighters. This effort is essential for improving the coordination of activities such as relief efforts, resource distribution, and media communication. This paper presents a methodology that leverages the capabilities of LLMs to enhance disaster response and management. Our approach combines classification techniques with generative AI to bridge the gap between raw user feedback and stakeholder-specific reports. Social media posts shared during catastrophic events are analyzed with a focus on user-reported issues, service interruptions, and encountered challenges. We employ full-spectrum LLMs, using analytical models like BERT for precise, multi-dimensional classification of content type, sentiment, emotion, geolocation, and topic. Generative models such as ChatGPT are then used to produce human-readable, informative reports tailored to distinct audiences, synthesizing insights derived from detailed classifications. We compare standard approaches, which analyze posts directly using prompts in ChatGPT, to our advanced method, which incorporates multi-dimensional classification, sub-event selection, and tailored report generation. Our methodology demonstrates superior performance in both quantitative metrics, such as text coherence scores and latent representations, and qualitative assessments by automated tools and field experts, delivering precise insights for diverse disaster response stakeholders.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Alignment: Discovering the Gap Between Social Media and Real-World Sentiments in Persian Tweets and Images</title>
<link>https://arxiv.org/abs/2504.10662</link>
<guid>https://arxiv.org/abs/2504.10662</guid>
<content:encoded><![CDATA[
<div> social media, emotions, Persian community, sentiment analysis, real-world

Summary: 
- An analysis of the emotional expressions in the Persian community on social media platform X compared to real-world experiences was conducted.
- A novel pipeline using Transformers-based text and image sentiment analysis modules was designed to measure emotional similarity between online and offline interactions.
- Results showed a 28.67% similarity between images on social media and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings.
- The study included 105 participants, 393 friends providing insights, over 8,300 collected tweets, and 2,000 media images.
- Statistical analysis confirmed the disparities in sentiment proportions between social media posts and real-world emotions. 

<br /><br />Summary: <div>
arXiv:2504.10662v2 Announce Type: replace-cross 
Abstract: In contemporary society, widespread social media usage is evident in people's daily lives. Nevertheless, disparities in emotional expressions between the real world and online platforms can manifest. We comprehensively analyzed Persian community on X to explore this phenomenon. An innovative pipeline was designed to measure the similarity between emotions in the real world compared to social media. Accordingly, recent tweets and images of participants were gathered and analyzed using Transformers-based text and image sentiment analysis modules. Each participant's friends also provided insights into the their real-world emotions. A distance criterion was used to compare real-world feelings with virtual experiences. Our study encompassed N=105 participants, 393 friends who contributed their perspectives, over 8,300 collected tweets, and 2,000 media images. Results indicated a 28.67% similarity between images and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings. Additionally, the statistical significance confirmed that the observed disparities in sentiment proportions.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Markov stability for community detection at a scale learned based on the structure</title>
<link>https://arxiv.org/abs/2504.11621</link>
<guid>https://arxiv.org/abs/2504.11621</guid>
<content:encoded><![CDATA[
arXiv:2504.11621v1 Announce Type: new 
Abstract: Community detection, the unsupervised task of clustering nodes of a graph, finds applications across various fields. The common approaches for community detection involve optimizing an objective function to partition the nodes into communities at a single scale of granularity. However, the single-scale approaches often fall short of producing partitions that are robust and at a suitable scale. The existing algorithm, PyGenStability, returns multiple robust partitions for a network by optimizing the multi-scale Markov stability function. However, in cases where the suitable scale is not known or assumed by the user, there is no principled method to select a single robust partition at a suitable scale from the multiple partitions that PyGenStability produces. Our proposed method combines the Markov stability framework with a pre-trained machine learning model for scale selection to obtain one robust partition at a scale that is learned based on the graph structure. This automatic scale selection involves using a gradient boosting model pre-trained on hand-crafted and embedding-based network features from a labeled dataset of 10k benchmark networks. This model was trained to predicts the scale value that maximizes the similarity of the output partition to the planted partition of the benchmark network. Combining our scale selection algorithm with the PyGenStability algorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale community detection algorithm that returns one robust partition at a suitable scale without the need for any assumptions, input, or tweaking from the user. We compare the performance of PO against 29 algorithms and show that it outperforms 25 other algorithms by statistically meaningful margins. Our results facilitate choosing between community detection algorithms, among which PO stands out as the accurate, robust, and hyperparameter-free method.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technological Complexity Based on Japanese Patent Data</title>
<link>https://arxiv.org/abs/2504.11932</link>
<guid>https://arxiv.org/abs/2504.11932</guid>
<content:encoded><![CDATA[
arXiv:2504.11932v1 Announce Type: new 
Abstract: As international competition intensifies in technologies, nations need to identify key technologies to foster innovation. However, the identification is difficult because a technology is independent, therefore has complex nature. Here, this study aims to assess patent technological fields by applying Technological Complexity Index from a corporate perspective, addressing its underutilization in Japan despite its potential. By utilizing carefully processed patent data from fiscal years 1981 to 2010, we analyze the bipartite network which consists of 1,938 corporations and 35 or 124 technological fields. Our findings provide quantitative characteristics of ubiquity and sophistication for patent fields, the detailed technological trends that reflect the social context, and methodological stability for policymakers and researchers, contributing to targeted innovation strategies in Japan.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2504.11699</link>
<guid>https://arxiv.org/abs/2504.11699</guid>
<content:encoded><![CDATA[
arXiv:2504.11699v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in representation learning, a challenge further amplified in self-supervised settings. We propose H$^3$GNNs, an end-to-end self-supervised learning framework that harmonizes both structural properties through two key innovations: (i) Joint Structural Node Encoding. We embed nodes into a unified space combining linear and non-linear feature projections with K-hop structural representations via a Weighted Graph Convolution Network(WGCN). A cross-attention mechanism enhances awareness and adaptability to heterophily and homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive Architectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a teacher-student model, the student sees the masked input graph and predicts node features inferred by the teacher that sees the full input graph in the joint encoding space. To enhance learning difficulty, we introduce two novel node-predictive-difficulty-based masking strategies. Experiments on seven benchmarks (four heterophily datasets and three homophily datasets) confirm the effectiveness and efficiency of H$^3$GNNs across diverse graph types. Our H$^3$GNNs achieves overall state-of-the-art performance on the four heterophily datasets, while retaining on-par performance to previous state-of-the-art methods on the three homophily datasets.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Analysis of Mixer Activities in the Bitcoin Network</title>
<link>https://arxiv.org/abs/2504.11924</link>
<guid>https://arxiv.org/abs/2504.11924</guid>
<content:encoded><![CDATA[
arXiv:2504.11924v1 Announce Type: cross 
Abstract: Cryptocurrency users increasingly rely on obfuscation techniques such as mixers, swappers, and decentralised or no-KYC exchanges to protect their anonymity. However, at the same time, these services are exploited by criminals to conceal and launder illicit funds. Among obfuscation services, mixers remain one of the most challenging entities to tackle. This is because their owners are often unwilling to cooperate with Law Enforcement Agencies, and technically, they operate as 'black boxes'. To better understand their functionalities, this paper proposes an approach to analyse the operations of mixers by examining their address-transaction graphs and identifying topological similarities to uncover common patterns that can define the mixer's modus operandi. The approach utilises community detection algorithms to extract dense topological structures and clustering algorithms to group similar communities. The analysis is further enriched by incorporating data from external sources related to known Exchanges, in order to understand their role in mixer operations. The approach is applied to dissect the Blender.io mixer activities within the Bitcoin blockchain, revealing: i) consistent structural patterns across address-transaction graphs; ii) that Exchanges play a key role, following a well-established pattern, which raises several concerns about their AML/KYC policies. This paper represents an initial step toward dissecting and understanding the complex nature of mixer operations in cryptocurrency networks and extracting their modus operandi.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Patterns of Viral Content on WhatsApp</title>
<link>https://arxiv.org/abs/2407.08172</link>
<guid>https://arxiv.org/abs/2407.08172</guid>
<content:encoded><![CDATA[
arXiv:2407.08172v2 Announce Type: replace 
Abstract: This paper explores the nature and spread of viral WhatsApp content among everyday users in three diverse countries: India, Indonesia, and Colombia. By analyzing hundreds of viral messages collected with participants' consent from private WhatsApp groups, we provide one of the first cross-cultural categorizations of viral content on WhatsApp. Despite the differences in cultural and geographic settings, our findings reveal striking similarities in the types of groups users engage with and the viral content they receive, particularly in the prevalence of misinformation. Our comparative analysis shows that viral content often includes political and religious narratives, with misinformation frequently recirculated despite prior debunking by fact-checking organizations. These parallels suggest that closed messaging platforms like WhatsApp facilitate similar patterns of information dissemination across different cultural contexts. This work contributes to the broader understanding of global digital communication ecosystems and provides a foundation for future research on information flow and moderation strategies in private messaging platforms.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socio-cognitive Networks between Researchers: Investigating Scientific Dualities with the Group-Oriented Relational Hyperevent Model</title>
<link>https://arxiv.org/abs/2407.21067</link>
<guid>https://arxiv.org/abs/2407.21067</guid>
<content:encoded><![CDATA[
arXiv:2407.21067v2 Announce Type: replace 
Abstract: Understanding why researchers cite certain works remains a key question in the study of scientific networks. Prior research has identified factors such as relevance, group cohesion, and source crediting. However, the interplay between cognitive and social dimensions in citation behavior - often conceptualized as a socio-cognitive network - is frequently overlooked, particularly regarding the intermediary steps that lead to a citation. Since a citation first requires a work to be published by a set of authors, we examine how the structure of coauthorship networks influences citation patterns. To investigate this relationship, we analyze the citation and collaboration behavior of Chilean astronomers from 2013 to 2015 using the Group-Oriented Relational Hyperevent Model, which allows us to study coauthorship and citation networks in a joint framework. Our findings suggest that when selecting which works to cite, authors favor recent research and maintain cognitive continuity across cited works. At the same time, we observe that coherent groups - closely connected coauthors - tend to be co-cited more frequently in subsequent publications, reinforcing the interdependence of collaboration and citation networks.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining higher-order triadic interactions</title>
<link>https://arxiv.org/abs/2404.14997</link>
<guid>https://arxiv.org/abs/2404.14997</guid>
<content:encoded><![CDATA[
arXiv:2404.14997v2 Announce Type: replace-cross 
Abstract: Complex systems often involve higher-order interactions which require us to go beyond their description in terms of pairwise networks. Triadic interactions are a fundamental type of higher-order interaction that occurs when one node regulates the interaction between two other nodes. Triadic interactions are found in a large variety of biological systems, from neuron-glia interactions to gene-regulation and ecosystems. However, triadic interactions have so far been mostly neglected. In this article, we propose a theoretical model that demonstrates that triadic interactions can modulate the mutual information between the dynamical state of two linked nodes. Leveraging this result, we propose the Triadic Interaction Mining (TRIM) algorithm to mine triadic interactions from node metadata, and we apply this framework to gene expression data, finding new candidates for triadic interactions relevant for Acute Myeloid Leukemia. Our work reveals important aspects of higher-order triadic interactions that are often ignored, yet can transform our understanding of complex systems and be applied to a large variety of systems ranging from biology to the climate.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiReddit: Tracing Information and Attention Flows Between Online Platforms</title>
<link>https://arxiv.org/abs/2502.04942</link>
<guid>https://arxiv.org/abs/2502.04942</guid>
<content:encoded><![CDATA[
arXiv:2502.04942v2 Announce Type: replace-cross 
Abstract: The World Wide Web is a complex interconnected digital ecosystem, where information and attention flow between platforms and communities throughout the globe. These interactions co-construct how we understand the world, reflecting and shaping public discourse. Unfortunately, researchers often struggle to understand how information circulates and evolves across the web because platform-specific data is often siloed and restricted by linguistic barriers. To address this gap, we present a comprehensive, multilingual dataset capturing all Wikipedia mentions and links shared in posts and comments on Reddit 2020-2023, excluding those from private and NSFW subreddits. Each linked Wikipedia article is enriched with revision history, page view data, article ID, redirects, and Wikidata identifiers. Through a research agreement with Reddit, our dataset ensures user privacy while providing a query and ID mechanism that integrates with the Reddit and Wikipedia APIs. This enables extended analyses for researchers studying how information flows across platforms. For example, Reddit discussions use Wikipedia for deliberation and fact-checking which subsequently influences Wikipedia content, by driving traffic to articles or inspiring edits. By analyzing the relationship between information shared and discussed on these platforms, our dataset provides a foundation for examining the interplay between social media discourse and collaborative knowledge consumption and production.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Platforms with Atomic Transactions: Social Networks, Cryptocurrencies, and Democratic Federations</title>
<link>https://arxiv.org/abs/2502.11299</link>
<guid>https://arxiv.org/abs/2502.11299</guid>
<content:encoded><![CDATA[
arXiv:2502.11299v3 Announce Type: replace-cross 
Abstract: Grassroots platforms aim to offer an egalitarian alternative to global platforms. Whereas global platforms can have only a single instance, grassroots platforms can have multiple instances that emerge and operate independently of each other and of any global resource except the network, and can interoperate and coalesce into ever-larger instances once interconnected. Key grassroots platforms include grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations. Previously, grassroots platforms were defined formally and proven grassroots using unary distributed transition systems, in which each transition is carried out by a single agent. However, grassroots platforms cater for a more abstract specification using transactions carried out atomically by multiple agents, something that cannot be expressed by unary transition systems. As a result, their original specifications and proofs were unnecessarily cumbersome and opaque.
  We enhance the notion of a distributed transition system to include atomic transactions and revisit the notion of grassroots platforms within this new foundation; present crisp specifications of key grassroots platforms using atomic transactions: befriending and defriending for grassroots social networks, coin swaps for grassroots cryptocurrencies, and communities forming, joining, and leaving a federation for grassroots democratic federations; prove a general theorem that a platform specified by atomic transactions that are so-called interactive is grassroots; show that the atomic transactions used to specify all three platforms are interactive; and conclude that the platforms thus specified are indeed grassroots. We thus provide a crisp mathematical foundation for grassroots platforms and a solid and clear starting point from which their implementation can commence.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>