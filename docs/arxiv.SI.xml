<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.SI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.SI</link>


<item>
<title>Meso-scale structures in signed networks</title>
<link>https://arxiv.org/abs/2512.11281</link>
<guid>https://arxiv.org/abs/2512.11281</guid>
<content:encoded><![CDATA[
<div> Keywords: signed networks, social balance theory, unbalanced structures, meso-scale structures, computational methods  

<br /><br />Summary:  
This study addresses the limitations of social balance theory, which traditionally assumes that positive connections exist within groups and negative connections occur between groups in signed networks. The authors propose a new methodology that does not rely on this assumption, enabling the detection and characterization of both balanced and unbalanced meso-scale structures. They apply this methodology to 24 empirical networks spanning social-political, financial, and biological domains. Their analysis reveals that unbalanced meso-scale structures are common in real-world networks, even when there is substantial balance at the smaller micro-scale level of triangles. A key finding is that assortativity—a tendency for nodes to connect to others with similar properties—often exists regardless of whether interactions are positive or negative. Additionally, core-periphery structures, where a densely connected core is surrounded by a sparsely connected periphery, are typical in online social networks. The results emphasize the complexity of relational structures at the meso-scale and underscore the need for computational methods that do not assume specific structural patterns. Lastly, the study highlights the importance of independently evaluating social balance theory’s predictions at both micro- and meso-scale levels to better understand signed network organization. <div>
arXiv:2512.11281v1 Announce Type: new 
Abstract: Meso-scale structures in signed networks have been studied under the limiting assumption of the validity of social balance theory, which predicts positive connections within groups and negative connections between groups. Here, we propose and apply a methodology that overcomes this limitation and is able to find and characterize also the different possible unbalanced structures in signed networks. Applying our methodology to 24 empirical networks, from social-political, financial, and biological domains, we find that unbalanced meso-scale structures are prevalent in real-world networks, including cases with substantial balance at the micro-scale of triangles. In particular, we find that assortativity often prevails regardless of the interaction sign and that core-periphery structures are typical in online social networks. Our findings highlight the complexity of meso-scale relational structures, the importance of using computational methods that are a priori agnostic to specific patterns, and the importance of independently evaluating micro- and meso-scale predictions of social balance theory.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complementary Strengths: Combining Geometric and Topological Approaches for Community Detection</title>
<link>https://arxiv.org/abs/2512.11496</link>
<guid>https://arxiv.org/abs/2512.11496</guid>
<content:encoded><![CDATA[
<div> community detection, spectral embedding, topological data analysis, ToMATo, modularity<br /><br />Summary:<br /><br />1. The paper highlights that the optimal strategy for community detection in complex networks depends critically on the network’s structural properties rather than a universal approach.<br />2. Traditional graph-theoretic methods like Louvain optimize modularity but may miss finer geometric community structures.<br />3. Topological data analysis (TDA) methods, such as ToMATo, excel at detecting density-defined clusters in embedded data but can be sensitive to initial projection choices.<br />4. The authors propose a novel hybrid framework that integrates spectral embedding and TDA, using spectral methods to capture the network’s geometric skeleton and ToMATo to identify persistent clusters as density basins within this embedded landscape.<br />5. Experimental results on synthetic benchmark networks demonstrate robustness of this hybrid approach, performing comparably to Louvain on modular networks, thereby supporting the development of new hybrid algorithms that adapt their strategy based on network geometry instead of relying on one-size-fits-all methods. <div>
arXiv:2512.11496v1 Announce Type: new 
Abstract: The optimal strategy for community detection in complex networks is not universal, but depends critically on the network's underlying structural properties. Although popular graph-theoretic methods, such as Louvain, optimize for modularity, they can overlook nuanced, geometric community structures. Conversely, topological data analysis (TDA) methods such as ToMATo are powerful in identifying density-defined clusters in embedded data but can be sensitive to initial projection. We propose a unified framework that integrates both paradigms to take advantage of their complementary advantages. Our method uses spectral embedding to capture the network's geometric skeleton, creating a landscape where communities manifest as density basins. The ToMATo algorithm then provides a topologically-grounded and parameter-aware method to extract persistent clusters from this landscape. Our comprehensive analysis across synthetic benchmarks shows that this hybrid approach is highly robust: it performs on par with Louvain on modular networks. These results argue for a new class of hybrid algorithms that select strategies based on network geometry, moving beyond one-size-fits-all solutions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network Centrality Metrics Based on Unrestricted Paths, Walks and Cycles Compared to Standard Centrality Metrics</title>
<link>https://arxiv.org/abs/2512.11585</link>
<guid>https://arxiv.org/abs/2512.11585</guid>
<content:encoded><![CDATA[
<div> Keywords: network centrality, influence spreading, probabilistic measures, in-centrality, out-centrality<br /><br />Summary:<br /><br />1. Traditional centrality measures such as closeness, betweenness, and degree centrality have inherent limitations; they often rely solely on shortest paths or immediate neighborhoods, which may not capture the full dynamics of influence spreading in networks.<br /><br />2. Many existing metrics fail to reflect the physical or probabilistic nature of nodal centrality and network flow, overlooking phenomena like recurrent spreading.<br /><br />3. The authors propose new probabilistic metrics based on an influence spreading model that accounts for all feasible walks and cycles, thus providing a more comprehensive characterization of network structure.<br /><br />4. They introduce two novel measures: in-centrality, which quantifies how central a node is as a target of influence, and out-centrality, which measures how central a node is as a source of influence on others.<br /><br />5. A comparative analysis using scatter plots and Pearson correlation coefficients demonstrates that the influence spreading betweenness centrality captures the significance of alternative routes while showing similarity to standard betweenness centrality, enhancing the understanding of network flow and influence mechanisms. <div>
arXiv:2512.11585v1 Announce Type: new 
Abstract: A key issue with standard network measures of closeness and betweenness centrality is that they rely on the shortest paths between nodes within the network structure, whereas the degree centrality only reveals the immediate neighborhood of a node. Furthermore, many measures found in the literature do not accurately represent the physical or probabilistic characteristics of nodal centrality, network flow, and other salient properties. For example, recurrent spreading in a network is often overlooked by these metrics. Standard centrality measures have limitations, being optimal for one application but not for others. Here, we present new metrics based on our influence spreading model to characterize network structure for various network science applications. These probabilistic measures account for all feasible walks and cycles in the network. We compare our new metrics with the standard metrics in terms of the node rankings given by different centrality measures, by examining scatter plots, and by using the Pearson correlation coefficient. In the influence spreading model, we define the in-centrality measure to characterize how central a node is as a target of influence by other nodes and the out-centrality measure to characterize how central a node is as a source of influence on other nodes. Our results show that the influence spreading betweenness centrality reveals the importance of alternative routes while maintaining similarity to standard betweenness centrality.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Relational Model of Neighborhood Mobility: The Role of Amenities and Cultural Alignment</title>
<link>https://arxiv.org/abs/2512.11662</link>
<guid>https://arxiv.org/abs/2512.11662</guid>
<content:encoded><![CDATA[
<div> culture, amenities, urban mobility, segregation, neighborhoods<br /><br />Summary:<br /><br />1. This study investigates why some neighborhoods are strongly connected while others remain isolated, focusing beyond traditional factors like demographics, economics, and geography.<br /><br />2. It introduces a relational, cross-national model emphasizing the role of local culture and amenity mix alignment, creating a "soft infrastructure" that influences urban mobility through symbolic cues and functional features.<br /><br />3. The research uses extensive data: approximately 650 million Google Places reviews to analyze co-visitation patterns between U.S. ZIP codes and around 30 million Canadian residential change-of-address records to track mobility.<br /><br />4. Results demonstrate that neighborhoods with similar cultural styles and amenity compositions are significantly more connected in terms of movement and social interaction.<br /><br />5. These findings remain robust even when controlling for race, income, education, political views, housing costs, and physical distance, suggesting that shared cultural and material ecologies crucially shape urban cohesion and segregation beyond traditional spatial and demographic explanations. <div>
arXiv:2512.11662v1 Announce Type: new 
Abstract: Why are some neighborhoods strongly connected while others remain isolated? Although standard explanations focus on demographics, economics, and geography, movement across the city may also depend on cultural styles and amenity mix. This study proposes a relational, cross-national model in which local culture and amenity mix alignment creates a "soft infrastructure" of urban mobility, i.e., symbolic cues and functional features that shape expectations about the character of places. Using ~650 million Google Places reviews to measure co-visitation between U.S. ZIP codes and ~30 million Canadian change-of-address to track residential mobility, results show that neighborhoods with similar cultural styles and amenities are significantly more connected. These effects persist even after controlling for race, income, education, politics, housing costs, and distance. Urban cohesion and segregation depend not only on who lives where or how far apart neighborhoods are, but on the shared cultural and material ecologies that structure movement across the city.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities</title>
<link>https://arxiv.org/abs/2512.11178</link>
<guid>https://arxiv.org/abs/2512.11178</guid>
<content:encoded><![CDATA[
<div> Keywords: heterogeneous data fusion, smart cities, graph learning, cross-domain integration, urban analytics  

<br /><br />Summary:  
Modern cities rely heavily on data-driven approaches for decision-making in sectors like transportation, public safety, and environmental management. However, urban data are often heterogeneous, collected independently by local agencies in diverse formats and standards. This paper introduces a heterogeneous data pipeline designed for cross-domain data fusion involving time-varying and spatially varying time-series datasets from over 50 different sources. The core innovation lies in the data-learning module, which incorporates homophily principles from spatial datasets into graph-learning techniques, allowing the embedding of locality-specific information within models. The framework’s versatility and generalizability are demonstrated through five case studies covering datasets such as ride-share, traffic crashes, and crime reports from multiple cities. The results confirm that the framework achieves strong predictive accuracy and requires minimal adjustment when applied to new urban areas or problem domains. This scalable approach advances the development of data-informed urban systems by effectively addressing the challenges posed by data heterogeneity and multi-modality in national and local datasets. The research thereby contributes significant progress toward smarter and more integrated city analytics. <div>
arXiv:2512.11178v1 Announce Type: cross 
Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Pricing in Social Networks with Individual and Group Fairness Considerations</title>
<link>https://arxiv.org/abs/2512.11252</link>
<guid>https://arxiv.org/abs/2512.11252</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized pricing, fairness, graph neural networks, individual fairness, group fairness<br /><br />Summary:<br /><br />1. The paper addresses personalized pricing, where customers are charged different prices based on their individual features to increase retailer revenue, but this often causes fairness issues.<br />2. It highlights concerns at two levels: individual fairness, where customers may feel unfairly treated if charged higher prices, and group fairness, involving discrimination against protected groups such as those defined by race or gender.<br />3. Existing research treats individual and group fairness separately; this work bridges that gap by proposing a unified formulation of personalized pricing that integrates both fairness dimensions within social network structures.<br />4. The authors introduce FairPricing, a novel graph neural network (GNN)-based framework that leverages customer features and social network topology to learn personalized pricing policies.<br />5. FairPricing incorporates individual fairness by penalizing customer demand to capture perceived unfairness and mitigates group discrimination through adversarial debiasing and price regularization.<br />6. Unlike classical optimization methods requiring re-optimization after network changes, FairPricing’s learned policy generalizes to dynamic networks, enabling price assignment to customers as the network evolves.<br />7. Experimental results demonstrate that FairPricing successfully balances profitability with improvements in individual fairness perceptions and adherence to group fairness constraints. <div>
arXiv:2512.11252v1 Announce Type: cross 
Abstract: Personalized pricing assigns different prices to customers for the same product based on customer-specific features to improve retailer revenue. However, this practice often raises concerns about fairness at both the individual and group levels. At the individual level, a customer may perceive unfair treatment if he/she notices being charged a higher price than others. At the group level, pricing disparities can result in discrimination against certain protected groups, such as those defined by gender or race. Existing studies on fair pricing typically address individual and group fairness separately. This paper bridges the gap by introducing a new formulation of the personalized pricing problem that incorporates both dimensions of fairness in social network settings. To solve the problem, we propose FairPricing, a novel framework based on graph neural networks (GNNs) that learns a personalized pricing policy using customer features and network topology. In FairPricing, individual perceived unfairness is captured through a penalty on customer demand, and thus the profit objective, while group-level discrimination is mitigated using adversarial debiasing and a price regularization term. Unlike existing optimization-based personalized pricing, which requires re-optimization whenever the network updates, the pricing policy learned by FairPricing assigns personalized prices to all customers in an updated network based on their features and the new network structure, thereby generalizing to network changes. Extensive experimental results show that FairPricing achieves high profitability while improving individual fairness perceptions and satisfying group fairness requirements.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging</title>
<link>https://arxiv.org/abs/2512.11512</link>
<guid>https://arxiv.org/abs/2512.11512</guid>
<content:encoded><![CDATA[
<div> closeness centrality, distributed computation, pruning, multi-packet messaging, communication efficiency<br /><br />Summary:<br /><br />Identifying central nodes in large-scale complex networks using closeness centrality is essential but challenging to compute in a decentralized manner due to high communication costs. Existing distributed approximation methods, such as pruning, struggle to reduce the communication overhead effectively in large networks because they require exchanging numerous data packets. To address this, the paper introduces a novel enhancement to the distributed pruning technique by leveraging multi-packet messaging, allowing nodes to batch multiple data packets into larger consolidated messages. This batching significantly decreases the total number of messages exchanged, reducing communication overhead and minimizing data loss while maintaining the accuracy of closeness centrality estimates. Experimental evaluation shows that this multi-packet approach outperforms the original pruning method in both message efficiency and computation time, without degrading approximation quality. Although the method incurs a moderate increase in per-node memory usage and local processing overhead, these costs are outweighed by the marked improvements in communication efficiency. This advancement offers a more scalable and practical solution for decentralized computation of closeness centrality on very large and complex networks, representing a significant step forward in large-scale network analysis techniques. <div>
arXiv:2512.11512v1 Announce Type: cross 
Abstract: Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Online Behavior Change with Observational Studies: a Review</title>
<link>https://arxiv.org/abs/2310.19951</link>
<guid>https://arxiv.org/abs/2310.19951</guid>
<content:encoded><![CDATA[
<div> Keywords: online behavior change, sentiment analysis, digital platforms, behavior detection methodologies, theory-practice alignment<br /><br />Summary:<br /><br />This article reviews 148 research studies published from 2000 to 2023 that examine behavior change in digital environments. It provides a comprehensive categorization of behaviors studied, the methods used to detect behavior changes, the digital platforms referenced, and the theoretical frameworks that underpin these analyses. The findings show a predominant focus on detecting sentiment shifts as the main type of behavior change. Additionally, many studies rely heavily on platforms with API restrictions, limiting data access and scope. The review also highlights a significant gap in the integration of behavioral theories into empirical research, suggesting that many studies lack a strong theoretical foundation. To advance the understanding of online behavior change, the authors advocate for the development of new methodologies capable of capturing a broader spectrum of behavioral types beyond sentiment alone. They also recommend incorporating more diverse data sources to overcome platform limitations. Lastly, the article stresses the importance of aligning theoretical frameworks more closely with practical research to enhance the robustness and applicability of findings related to online behavior dynamics and societal progress. <div>
arXiv:2310.19951v3 Announce Type: replace-cross 
Abstract: Exploring online behavior change is imperative for societal progress in the context of 21st-century challenges. We analyze 148 articles (2000-2023) focusing on behavior change in the digital space and build a map that categorizes behaviors, behavior change detection methodologies, platforms of reference, and theoretical frameworks that characterize the analysis of online behavior change. Our findings reveal a focus on sentiment shifts, an emphasis on API-restricted platforms, and limited integration of theory. We call for methodologies able to capture a wider range of behavior types, diverse data sources, and stronger theory-practice alignment in the study of online behavior and its change.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Fake-News Detection with Node-Level Topological Features</title>
<link>https://arxiv.org/abs/2512.09974</link>
<guid>https://arxiv.org/abs/2512.09974</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, fake news detection, graph-theoretic metrics, degree centrality, local clustering coefficient<br /><br />Summary:<br />1. The article addresses the growing issue of misinformation and fake news, emphasizing the need for effective automated detection methods. <br />2. Previous methods combine content analysis, user preferences, and propagation structures, relying heavily on Graph Neural Networks (GNNs) for graph-level representation but overlook explicit topological information. <br />3. The authors propose a simple yet effective enhancement by appending two classic graph-theoretic metrics—degree centrality and local clustering coefficient—to each node’s BERT and profile embeddings to explicitly signal the importance of hub nodes and community roles. <br />4. This enhancement was tested on the UPFD Politifact subset dataset, resulting in a significant improvement in macro F1 score from 0.7753 to 0.8344 compared to the original baseline. <br />5. The study demonstrates the practical benefits of incorporating explicit topology features for fake-news detection and offers an interpretable and easy-to-reproduce framework that can be extended to other information diffusion tasks. <div>
arXiv:2512.09974v1 Announce Type: new 
Abstract: In recent years, the proliferation of misinformation and fake news has posed serious threats to individuals and society, spurring intense research into automated detection methods. Previous work showed that integrating content, user preferences, and propagation structure achieves strong performance, but leaves all graph-level representation learning entirely to the GNN, hiding any explicit topological cues. To close this gap, we introduce a lightweight enhancement: for each node, we append two classical graph-theoretic metrics, degree centrality and local clustering coefficient, to its original BERT and profile embeddings, thus explicitly flagging the roles of hub and community. In the UPFD Politifact subset, this simple modification boosts macro F1 from 0.7753 to 0.8344 over the original baseline. Our study not only demonstrates the practical value of explicit topology features in fake-news detection but also provides an interpretable, easily reproducible template for fusing graph metrics in other information-diffusion tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simulation Framework for Studying Recommendation-Network Co-evolution in Social Platforms</title>
<link>https://arxiv.org/abs/2512.10106</link>
<guid>https://arxiv.org/abs/2512.10106</guid>
<content:encoded><![CDATA[
<div> Keywords: recommendation systems, social networks, agent-based simulation, graph attention network, content diversity<br /><br />Summary:<br /><br />This work addresses the challenge of studying how recommendation systems impact the structure and dynamics of social networks, which is difficult to analyze on live platforms due to confounding factors and ethical concerns with controlled experiments. The authors develop an agent-based simulator that models content creation, social tie formation, and a graph attention network (GAT) recommender operating in a closed feedback loop. The simulation parameters are calibrated using real Mastodon data and validated out-of-sample against Bluesky data, achieving 4–6% error on structural metrics and 10–15% error on held-out temporal segments. Experiments with 18 different configurations of 100 agents demonstrate that the timing of activating the recommender significantly influences network outcomes: activating recommendations earlier (at time t=10) reduces network transitivity by 10% compared to later activation (t=40), while engagement differences remain under 8%. Delaying recommendation activation also increases content diversity by 9% and lowers network modularity by 4%. Larger scale simulations with up to 5,000 agents show these effects persist but become less pronounced. A Jacobian matrix analysis confirms the system’s local stability assuming agents show bounded reactance to recommendations. The authors provide their configuration files and scripts publicly to facilitate reproducibility. <div>
arXiv:2512.10106v1 Announce Type: new 
Abstract: Studying how recommendation systems reshape social networks is difficult on live platforms: confounds abound, and controlled experiments risk user harm. We present an agent-based simulator where content production, tie formation, and a graph attention network (GAT) recommender co-evolve in a closed loop. We calibrate parameters using Mastodon data and validate out-of-sample against Bluesky (4--6\% error on structural metrics; 10--15\% on held-out temporal splits). Across 18 configurations at 100 agents, we find that \emph{activation timing} affects outcomes: introducing recommendations at $t=10$ vs.\ $t=40$ decreases transitivity by 10\% while engagement differs by $<$8\%. Delaying activation increases content diversity by 9\% while reducing modularity by 4\%. Scaling experiments ($n$ up to 5,000) show the effect persists but attenuates. Jacobian analysis confirms local stability under bounded reactance parameters. We release configuration schemas and reproduction scripts.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Toxic Interaction Across User and Video Clusters in Social Video Platforms</title>
<link>https://arxiv.org/abs/2512.10233</link>
<guid>https://arxiv.org/abs/2512.10233</guid>
<content:encoded><![CDATA[
<div> Keywords: social video platforms, toxicity, user-video interaction, clustering, Bilibili<br /><br />Summary:<br /><br />This study investigates the dynamics of toxic interactions on the Chinese social video platform Bilibili by integrating video and user data into an interaction matrix. The researchers normalize and reduce the dimensionality of this matrix, then apply K-means clustering separately to users and videos to identify stable groups. This method links structural interaction patterns with content features, allowing a detailed analysis of behavioral and textual attributes across clusters. They find distinct stratification in user interaction styles, such as message length and comment ratio, with some user clusters showing longer, more comment-driven messages with lower toxicity. In contrast, differences in sentiment and toxicity across video clusters are weaker and inconsistent. However, videos with higher viewing volumes tend to attract more toxic expressions, suggesting that rapid growth periods require timely moderation interventions. The findings imply that platforms should tailor their moderation strategies: for high-exposure videos, prioritize quick response to toxic surges; for user groups with rational, engaged communication, foster mechanisms to sustain dialogue and encourage diverse topic engagement. Overall, the study offers a nuanced understanding of how toxicity clusters in a social video environment, emphasizing the importance of combining structural and content-based signals to inform platform governance. <div>
arXiv:2512.10233v1 Announce Type: new 
Abstract: Social video platforms shape how people access information, while recommendation systems can narrow exposure and increase the risk of toxic interaction. Previous research has often examined text or users in isolation, overlooking the structural context in which such toxic interactions occur. Without considering who interacts with whom and around what content, it is difficult to explain why negative expressions cluster within particular communities. To address this issue, this study focuses on the Chinese social video platform Bilibili, incorporating video-level information as the environment for user expression, modeling users and videos in an interaction matrix. After normalization and dimensionality reduction, we perform separate clustering on both sides of the video-user interaction matrix with K-means. Cluster assignments facilitate comparisons of user behavior, including message length, posting frequency, and source (barrage and comment), as well as textual features such as sentiment and toxicity, and video attributes defined by uploaders. Such a clustering approach integrates structural ties with content signals to identify stable groups of videos and users. We find clear stratification in interaction style (message length, comment ratio) across user clusters, while sentiment and toxicity differences are weak or inconsistent across video clusters. Across video clusters, viewing volume exhibits a clear hierarchy, with higher exposure groups concentrating more toxic expressions. For such a group, platforms should require timely intervention during periods of rapid growth. Across user clusters, comment ratio and message length form distinct hierarchies, and several clusters with longer and comment-oriented messages exhibit lower toxicity. For such groups, platforms should strengthen mechanisms that sustain rational dialogue and encourage engagement across topics.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Circulate and Recapture Dynamic of Fan Mobility in Agency-Affiliated VTuber Networks</title>
<link>https://arxiv.org/abs/2512.10240</link>
<guid>https://arxiv.org/abs/2512.10240</guid>
<content:encoded><![CDATA[
<div> Keywords: VTuber agencies, audience overlap, viewer trajectories, affiliation, live streaming

<br /><br />Summary:  
This study investigates how VTuber agencies, which manage groups of Virtual YouTubers (VTubers) on YouTube, influence viewer behavior and audience dynamics. First, it explores whether affiliation with an agency encourages fans to remain loyal to a single VTuber channel or to move flexibly within the agency's portfolio, potentially reducing viewer exit. Second, the research uses a large, multi-year dataset of VTuber live stream engagement to construct monthly audience overlap networks, applying a similarity measure that accounts for differences in audience size. Third, at the micro level, it tracks individual viewer retention, shifts in their primary VTuber ("oshi"), and periods of inactivity, while at the meso level it analyzes the structural properties of affiliation-specific subgraphs and visualizes transitions between viewer states. Fourth, findings reveal a pattern of "loose mobility," where fans stay active by reallocating attention within the same affiliation type, showing limited crossover between affiliated and independent VTuber groups. Fifth, network analysis shows that while global audience overlap converges, local networks within agency affiliations remain denser, and flow diagrams demonstrate "circulate and recapture" dynamics that stabilize fan participation without strict channel lock-in. The study offers a reusable framework connecting micro-level viewer behaviors to meso-level network structures, contributing to the understanding of creator labor, influencer marketing, and platform governance in video streaming ecosystems. <div>
arXiv:2512.10240v1 Announce Type: new 
Abstract: VTuber agencies -- multichannel networks (MCNs) that bundle Virtual YouTubers (VTubers) on YouTube -- curate portfolios of channels and coordinate programming, cross appearances, and branding in the live-streaming VTuber ecosystem. It remains unclear whether affiliation binds fans to a single channel or instead encourages movement within a portfolio that buffers exit, and how these micro level dynamics relate to meso level audience overlap. This study examines how affiliation shapes short horizon viewer trajectories and the organization of audience overlap networks by contrasting agency affiliated and independent VTubers. Using a large, multiyear, fan centered panel of VTuber live stream engagement on YouTube, we construct monthly audience overlap between creators with a similarity measure that is robust to audience size asymmetries. At the micro level, we track retention, changes in the primary creator watched (oshi), and inactivity; at the meso level, we compare structural properties of affiliation specific subgraphs and visualize viewer state transitions. The analysis identifies a pattern of loose mobility: fans tend to remain active while reallocating attention within the same affiliation type, with limited leakage across affiliation type. Network results indicate convergence in global overlap while local neighborhoods within affiliated subgraphs remain persistently denser. Flow diagrams reveal circulate and recapture dynamics that stabilize participation without relying on single channel lock in. We contribute a reusable measurement framework for VTuber live streaming that links micro level trajectories to meso level organization and informs research on creator labor, influencer marketing, and platform governance on video platforms. We do not claim causal effects; the observed regularities are consistent with proximity engineered by VTuber agencies and coordinated recapture.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Turnover and Promotion Outcomes: Evidence on the Optimal Hybrid-Work Frequency</title>
<link>https://arxiv.org/abs/2512.10328</link>
<guid>https://arxiv.org/abs/2512.10328</guid>
<content:encoded><![CDATA[
<div> Hybrid work, remote work frequency, turnover risk, promotion likelihood, employee outcomes<br /><br />Summary:<br /><br />This study investigates the impact of remote work frequency on employee career outcomes, specifically turnover risk and promotion likelihood, using data from a company with over one million employees. The findings reveal a nonlinear relationship where moderate remote work (approximately two days per week) optimizes outcomes by minimizing turnover and maximizing promotion. Initially, increased remote work reduces turnover and enhances promotion chances, but beyond a certain threshold, turnover rises and promotion declines. These effects vary by subgroup: male employees gain more promotion benefits from remote work compared to female employees, while support workers show minimal promotion gains and only slight turnover reductions at their optimal remote frequency. Organizational leaders face greater challenges with remote work, experiencing increased turnover risk and decreased promotion likelihood as remote days increase, unlike individual contributors. The study also highlights that variations in time-allocation patterns help explain how different remote work frequencies affect these career outcomes, emphasizing the complexity in balancing remote work policies to serve diverse employee roles and demographics effectively. <div>
arXiv:2512.10328v1 Announce Type: new 
Abstract: Hybrid work policy, especially return-to-office requirements, remains a globally salient topic as workers, companies, and governments continue to debate and disagree. Despite extensive discussions on the benefits and drawbacks of remote and hybrid arrangements, the optimal number of remote days that jointly considers multiple organizational outcomes has not been empirically established. Focusing on two critical career outcomes -- turnover risk and promotion -- we examine how remote work frequency shapes employee trajectories using large-scale observational activity data from a company with over one million employees. We find that increased remote-work frequency is associated with an initial decrease and then an increase in turnover, while promotion likelihood initially rises and then declines. Accordingly, we identify approximately two remote days per week as an optimal balance -- maximizing promotion, a positive outcome for employees, while minimizing turnover, which is undesirable for organizations and may indicate negative employee experiences. These patterns vary across subgroups defined by gender, role type, and leadership status. Several notable results emerge. First, male employees derive greater promotion benefits from remote work than female employees. Second, support workers (non-core business roles) do not experience promotion gains, and the reduction in turnover at their optimal remote-work frequency is marginal compared with employees in core business roles. Third, organizational leaders face greater challenges in remote settings than individual contributors: their turnover risk increases substantially at higher remote frequencies, and their likelihood of promotion decreases as remote frequency rises. We further show that time-allocation patterns partly explain how remote-work frequency influences these career outcomes.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kicking Politics: How Football Fan Communities Became Arenas for Political Influence</title>
<link>https://arxiv.org/abs/2512.10737</link>
<guid>https://arxiv.org/abs/2512.10737</guid>
<content:encoded><![CDATA[
<div> Keywords: Brexit Referendum, UK football fandom, political campaigns, Twitter, social network analysis<br /><br />Summary:<br /><br />1. The paper investigates how political campaigns engaged with UK football fan communities on Twitter during the period immediately following the 2016 Brexit Referendum through 2017. 2. Football fandom is highlighted as a space with strong collective identities and tribal behaviors, making it a potent environment for political influence. 3. The study combines social network analysis and content analysis to explore how political discourse became integrated into football-related conversations on Twitter. 4. A diverse set of actors—including political parties, media outlets, activist groups, and pseudonymous influencers—participated in mobilizing support, provoking reactions, and shaping public opinion within these fan communities. 5. The paper presents case studies such as hashtag hijacking, embedded activism, and the use of political "megaphones" to illustrate how political campaigns leveraged fan cultures to amplify their messages. Overall, the findings expose mechanisms through which political influence operates in spaces traditionally seen as non-political, and the authors suggest that these insights could inform the development of a broader analytical framework in future research. <div>
arXiv:2512.10737v1 Announce Type: new 
Abstract: This paper investigates how political campaigns engaged UK football fan communities on Twitter in the aftermath of the Brexit Referendum (2016-2017). Football fandom, with its strong collective identities and tribal behaviours, offers fertile ground for political influence. Combining social network and content analysis, we examine how political discourse became embedded in football conversations. We show that a wide range of actors -- including parties, media, activist groups, and pseudonymous influencers -- mobilised support, provoked reactions, and shaped opinion within these communities. Through case studies of hashtag hijacking, embedded activism, and political "megaphones", we illustrate how campaigns leveraged fan cultures to amplify political messages. Our findings highlight mechanisms of political influence in ostensibly non-political online spaces and point toward the development of a broader framework in future work.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echoes of Automation: How Bots Shaped Political Discourse in Brazil</title>
<link>https://arxiv.org/abs/2512.10749</link>
<guid>https://arxiv.org/abs/2512.10749</guid>
<content:encoded><![CDATA[
<div> Keywords: bots, political communication, sentiment analysis, Brazilian election, social media manipulation<br /><br />Summary:<br /><br />This study investigates the role and behavior of bots on social media during critical political periods in Brazil, specifically the 2018 presidential election and the lead-up to the 2022 contest. Drawing from an extensive dataset of over 315 million tweets between August 2018 and June 2022, the analysis reveals that bots heavily relied on retweets and replies, with a notable increase in reply activity following the 2018 election, indicating strategies aimed at conversational infiltration and message amplification. Sentiment analysis shows that bots exhibited a constrained emotional range, maintaining a more uniform and narrower tone compared to human users, whose sentiments fluctuated more with political developments. Through topic modeling, it is evident that bots focused predominantly on Bolsonaro-related content, repeating similar messages, whereas human users engaged in more diverse discussions spanning multiple candidates, civic issues, and personal commentary. The findings highlight the function of bots as amplifiers of limited political agendas and point to their potential impact in skewing public political discourse on social media platforms during election cycles. <div>
arXiv:2512.10749v1 Announce Type: new 
Abstract: In an era where social media platforms are central to political communication, the activity of bots raises pressing concerns about amplification, manipulation, and misinformation. Drawing on more than 315 million tweets posted from August 2018 to June 2022, we examine behavioural patterns, sentiment dynamics, and the thematic focus of bot- versus human-generated content spanning the 2018 Brazilian presidential election and the lead-up to the 2022 contest. Our analysis shows that bots relied disproportionately on retweets and replies, with reply activity spiking after the 2018 election, suggesting tactics of conversational infiltration and amplification. Sentiment analysis indicates that bots maintained a narrower emotional tone, in contrast to humans, whose sentiment fluctuated more strongly with political events. Topic modelling further reveals bots' repetitive, Bolsonaro-centric messaging, while human users engaged with a broader range of candidates, civic concerns, and personal reflections. These findings underscore bots' role as amplifiers of narrow agendas and their potential to distort online political discourse.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research</title>
<link>https://arxiv.org/abs/2512.10058</link>
<guid>https://arxiv.org/abs/2512.10058</guid>
<content:encoded><![CDATA[
<div> Keywords: AI safety, AI ethics, alignment, bibliometric analysis, cross-disciplinary collaboration<br /><br />Summary:<br /><br />1. The article addresses the growing urgency in AI research to develop aligned systems that are both harmless and ethically sound, noting a divergence between two main research tracks: AI safety and AI ethics.<br /><br />2. AI safety research focuses on scaled intelligence, risks related to deceptive or scheming AI behaviors, and existential threats, while AI ethics concentrates on present-day harms, social biases, and flaws in AI production processes.<br /><br />3. These two communities have evolved largely in isolation due to differing definitions of alignment, methodologies, institutional settings, and disciplinary backgrounds.<br /><br />4. The authors conducted a large-scale quantitative study analyzing 6,442 papers from major ML and NLP conferences (2020-2025), revealing that over 80% of collaborations occur exclusively within either the safety or ethics communities, and only about 5% of papers serve as bridges between them.<br /><br />5. The study shows that cross-community collaboration relies heavily on a small number of key actors (“brokers”), and removing these brokers increases segregation, highlighting institutional and conceptual divides.<br /><br />6. The authors argue for integrating technical safety work with normative ethics through shared benchmarks, cross-institutional venues, and mixed-method approaches to create AI systems that are robust, just, and aligned both technically and ethically. <div>
arXiv:2512.10058v1 Announce Type: cross 
Abstract: While much research in artificial intelligence (AI) has focused on scaling capabilities, the accelerating pace of development makes countervailing work on producing harmless, "aligned" systems increasingly urgent. Yet research on alignment has diverged along two largely parallel tracks: safety--centered on scaled intelligence, deceptive or scheming behaviors, and existential risk--and ethics--focused on present harms, the reproduction of social bias, and flaws in production pipelines. Although both communities warn of insufficient investment in alignment, they disagree on what alignment means or ought to mean. As a result, their efforts have evolved in relative isolation, shaped by distinct methodologies, institutional homes, and disciplinary genealogies.
  We present a large-scale, quantitative study showing the structural split between AI safety and AI ethics. Using a bibliometric and co-authorship network analysis of 6,442 papers from twelve major ML and NLP conferences (2020-2025), we find that over 80% of collaborations occur within either the safety or ethics communities, and cross-field connectivity is highly concentrated: roughly 5% of papers account for more than 85% of bridging links. Removing a small number of these brokers sharply increases segregation, indicating that cross-disciplinary exchange depends on a handful of actors rather than broad, distributed collaboration. These results show that the safety-ethics divide is not only conceptual but institutional, with implications for research agendas, policy, and venues. We argue that integrating technical safety work with normative ethics--via shared benchmarks, cross-institutional venues, and mixed-method methodologies--is essential for building AI systems that are both robust and just.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topology Identification and Inference over Graphs</title>
<link>https://arxiv.org/abs/2512.10183</link>
<guid>https://arxiv.org/abs/2512.10183</guid>
<content:encoded><![CDATA[
<div> Keywords: topology identification, graph inference, multidimensional relational data, dynamic processes, kernel methods  

<br /><br />Summary: This chapter provides a comprehensive overview of methods for identifying graph topology and performing statistical inference on multidimensional relational data, which is critical in domains such as brain networks, transportation, finance, power systems, and social networks. It first addresses approaches for undirected links by exploring correlation metrics, covariance selection techniques, and their connections to smooth signal priors. The chapter then shifts focus to directional, potentially causal relationships among nodes, highlighting the limitations of linear time-invariant models in capturing dynamic and nonlinear dependencies. To overcome these issues, it surveys a principled framework that employs carefully chosen kernels from a specific dictionary to model such complexities. Generalizations via structural equation modeling and vector autoregressions are discussed to incorporate attributes like low rank, sparsity, acyclicity, and smoothness for dynamic processes over potentially time-varying topologies. The framework supports both batch and online learning algorithms with proven convergence rates. Additionally, it is compatible with tensor formulations and decompositions, making it suitable for multidimensional network data and capable of leveraging high-order statistical information for more robust topology inference and process modeling. <div>
arXiv:2512.10183v1 Announce Type: cross 
Abstract: Topology identification and inference of processes evolving over graphs arise in timely applications involving brain, transportation, financial, power, as well as social and information networks. This chapter provides an overview of graph topology identification and statistical inference methods for multidimensional relational data. Approaches for undirected links connecting graph nodes are outlined, going all the way from correlation metrics to covariance selection, and revealing ties with smooth signal priors. To account for directional (possibly causal) relations among nodal variables and address the limitations of linear time-invariant models in handling dynamic as well as nonlinear dependencies, a principled framework is surveyed to capture these complexities through judiciously selected kernels from a prescribed dictionary. Generalizations are also described via structural equations and vector autoregressions that can exploit attributes such as low rank, sparsity, acyclicity, and smoothness to model dynamic processes over possibly time-evolving topologies. It is argued that this approach supports both batch and online learning algorithms with convergence rate guarantees, is amenable to tensor (that is, multi-way array) formulations as well as decompositions that are well-suited for multidimensional network data, and can seamlessly leverage high-order statistical information.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach</title>
<link>https://arxiv.org/abs/2512.10633</link>
<guid>https://arxiv.org/abs/2512.10633</guid>
<content:encoded><![CDATA[
<div> Illegal border crossings, Europe migration routes, machine learning, expert judgment, EU migration policy<br /><br />Summary:<br /><br />This paper introduces a mixed-methodology to forecast illegal border crossings across five major migratory routes in Europe with a one-year forecasting horizon. The approach combines machine learning techniques with qualitative insights from migration experts, incorporating a human-assessed covariate to enhance predictive accuracy. This integration addresses issues like sudden shifts in migration patterns and data limitations that challenge traditional models. The methodology is directly aligned with the forecasting requirements specified in the EU Pact on Migration and Asylum and supports the Asylum and Migration Management Regulation (AMMR). It is developed to deliver policy-relevant predictions that assist in strategic planning, early warning systems, and coordination mechanisms among EU Member States. By merging data-driven modeling with expert judgment, the methodology responds to academic recommendations and introduces a novel tool tailored for operational use within EU migration governance. The paper also includes validation of the approach using historical data to demonstrate its practicality and reliability for informing migration-related policy decisions. <div>
arXiv:2512.10633v1 Announce Type: cross 
Abstract: This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying displacement: a gentrification's consequence via persistent homology</title>
<link>https://arxiv.org/abs/2512.10753</link>
<guid>https://arxiv.org/abs/2512.10753</guid>
<content:encoded><![CDATA[
<div> Gentrification, displacement, topological data analysis, address change data, Madrid

<br /><br />Summary:  
1. Gentrification involves wealthier individuals moving into previously lower-income neighborhoods, resulting in rising living costs, cultural shifts, and displacement of long-term lower-income residents.  
2. Quantifying displacement is challenging due to the lack of information about residents’ motives for moving and the gradual nature of displacement processes occurring over long time spans.  
3. The authors propose a novel methodological tool that leverages publicly available address change data to analyze displacement without requiring direct motive data.  
4. This method constructs four cubical complexes that integrate both geographic and temporal dimensions of people’s movements and applies Topological Data Analysis (TDA) techniques to uncover patterns.  
5. A 20-year case study focusing on Madrid, Spain, demonstrates the effectiveness of this approach by revealing spatiotemporal patterns of population displacement and identifying specific neighborhoods and years affected—insights not apparent from raw data alone. <div>
arXiv:2512.10753v1 Announce Type: cross 
Abstract: Gentrification is the process by which wealthier individuals move into a previously lower-income neighbourhood. Among the effects of this multi-faceted phenomenon are rising living costs, cultural and social changes-where local traditions, businesses, and community networks are replaced or diluted by new, more affluent lifestyles-and population displacement, where long-term, lower-income residents are priced out by rising rents and property taxes. Despite its relevance, quantifying displacement presents difficulties stemming from lack of information on motives for relocation and from the fact that a long time-span must be analysed: displacement is a gradual process (leases end or conditions change at different times), impossible to capture in one data snapshot. We introduce a novel tool to overcome these difficulties. Using only publicly available address change data, we construct four cubical complexes which simultaneously incorporate geographical and temporal information of people moving, and then analyse them building on Topological Data Analysis tools. Finally, we demonstrate the potential of this method through a 20-year case study of Madrid, Spain. The results reveal its ability to capture population displacement and to identify the specific neighbourhoods and years affected--patterns that cannot be inferred from raw address change data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Potential Landscapes Reveal Spatiotemporal Structure in Urban Mobility: Hodge Decomposition and Principal Component Analysis of Tokyo Before and During COVID-19</title>
<link>https://arxiv.org/abs/2505.20929</link>
<guid>https://arxiv.org/abs/2505.20929</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, dimensionality reduction, combinatorial Hodge theory, principal component analysis, pandemic mobility patterns  

<br /><br />Summary:  
1. Understanding human mobility is crucial for addressing societal challenges such as epidemic control and urban transportation optimization.  
2. The increasing availability of dynamic origin-destination (OD) mobility data introduces complexity and volume that make direct interpretation of spatiotemporal patterns difficult.  
3. To tackle this, the study proposes a two-step dimensionality reduction framework that balances data simplification with preservation of location-specific flow information.  
4. In the first step, combinatorial Hodge theory is applied to OD matrices with timestamps to construct potential landscapes, retaining imbalanced trip information between locations.  
5. The second step uses principal component analysis (PCA) to represent the time series of these potential landscapes as linear combinations of a few static spatial components, with coefficients capturing temporal variation.  
6. This method successfully decouples spatial and temporal components, enabling clearer insights into mobility patterns.  
7. Applying the framework to mobility data during a pandemic reveals significant changes, including an overall drop in mobility and distinct differences between weekdays and holidays.  
8. The results validate the framework’s effectiveness in revealing complex human mobility behaviors, with potential applications in urban planning and public health policy. <div>
arXiv:2505.20929v4 Announce Type: replace 
Abstract: Understanding human mobility is vital to solving societal challenges, such as epidemic control and urban transportation optimization. Recent advancements in data collection now enable the exploration of dynamic mobility patterns in human flow. However, the vast volume and complexity of mobility data make it difficult to interpret spatiotemporal patterns directly, necessitating effective information reduction. The core challenge is to balance data simplification with information preservation: methods must retain location-specific information about human flows from origins to destinations while reducing the data to a comprehensible level. This study proposes a two-step dimensionality reduction framework: First, combinatorial Hodge theory is applied to the given origin--destination (OD) matrices with timestamps to construct a set of potential landscapes of human flow, preserving imbalanced trip information between locations. Second, principal component analysis (PCA) expresses the time series of potential landscapes as a linear combination of a few static spatial components, with their coefficients representing temporal variations. The framework systematically decouples the spatial and temporal components of the given data. By implementing this two-step reduction method, we reveal large weight variations during a pandemic, characterized by an overall decline in mobility and stark contrasts between weekdays and holidays. These findings demonstrate the effectiveness of our framework in uncovering complex mobility patterns and its potential to inform urban planning and public health interventions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graph Enrichment and Reasoning for Nobel Laureates</title>
<link>https://arxiv.org/abs/2512.09707</link>
<guid>https://arxiv.org/abs/2512.09707</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Graph, Nobel Prize, Named Entity Recognition, Social Network Analysis, GraphRAG Chatbot<br /><br />Summary: This project focuses on creating and analyzing a rich knowledge graph of Nobel Prize laureates by enriching existing datasets with biographical information from Wikipedia. The methodology employs advanced techniques including automatic data augmentation powered by large language models (LLMs) for Named Entity Recognition (NER) and Relation Extraction (RE) to extract relevant details accurately. It also utilizes social network analysis to reveal hidden patterns and relationships within the scientific community, highlighting influential individuals and central organizations. Additionally, the team developed a GraphRAG-based chatbot system equipped with a fine-tuned model that translates natural language queries into Cypher queries for intuitive interaction with the knowledge graph. Experimental findings show that the enhanced knowledge graph exhibits characteristics of a small-world network, facilitating effective identification of key figures in science. The chatbot attains competitive accuracy on a custom multiple-choice evaluation, demonstrating the practical benefits of integrating LLMs with structured knowledge bases for complex reasoning and querying tasks. The project further supports transparency and reproducibility by providing open access to data and source code via GitHub. <div>
arXiv:2512.09707v1 Announce Type: new 
Abstract: This project aims to construct and analyze a comprehensive knowledge graph of Nobel Prize and Laureates by enriching existing datasets with biographical information extracted from Wikipedia. Our approach integrates multiple advanced techniques, consisting of automatic data augmentation using LLMs for Named Entity Recognition (NER) and Relation Extraction (RE) tasks, and social network analysis to uncover hidden patterns within the scientific community. Furthermore, we also develop a GraphRAG-based chatbot system utilizing a fine-tuned model for Text2Cypher translation, enabling natural language querying over the knowledge graph. Experimental results demonstrate that the enriched graph possesses small-world network properties, identifying key influential figures and central organizations. The chatbot system achieves a competitive accuracy on a custom multiple-choice evaluation dataset, proving the effectiveness of combining LLMs with structured knowledge bases for complex reasoning tasks. Data and source code are available at: https://github.com/tlam25/network-of-awards-and-winners.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning</title>
<link>https://arxiv.org/abs/2512.09831</link>
<guid>https://arxiv.org/abs/2512.09831</guid>
<content:encoded><![CDATA[
<div> Beliefs, Geometric Framework, Cognitive Heterogeneity, Influence, Value Alignment  

<br /><br />Summary:  
This paper develops a novel geometric framework to model belief, motivation, and influence among cognitively diverse agents by representing each agent with a personalized value space, a vector space encoding their unique interpretive dimensions. It formalizes beliefs as structured vectors called abstract beings, whose transmission depends on linear interpretation maps; beliefs that fall into the null spaces of these maps fail to survive communication, providing a structural explanation for intelligibility, miscommunication, and belief extinction. The framework reveals how phenomena such as belief distortion, motivational drift, counterfactual evaluation, and limitations in mutual understanding emerge naturally from algebraic constraints inherent in the vector space representations. A key theoretical contribution is the "No-Null-Space Leadership Condition," which reconceptualizes leadership as a matter of representational reachability within these cognitive spaces instead of traditional notions of persuasion or authority. More broadly, the model explains how abstract beings—beliefs—can propagate, mutate, or fade across diverse cognitive geometries, thus unifying perspectives from conceptual spaces, social epistemology, and AI value alignment. This cognitive-geometric approach situates meaning preservation in structural compatibility, rather than shared information or rationality, offering a robust foundation for analyzing belief dynamics and the epistemic boundaries of influence in both human and artificial multi-agent systems. <div>
arXiv:2512.09831v1 Announce Type: cross 
Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Machine Learning to Identify Gendered Stereotypes and Body Image Concerns on Diet and Fitness Online Forums</title>
<link>https://arxiv.org/abs/2407.03551</link>
<guid>https://arxiv.org/abs/2407.03551</guid>
<content:encoded><![CDATA[
<div> Keywords: body image, muscle dysmorphia, Reddit forums, gender differences, emotional expression  

<br /><br />Summary:  
This study investigates body image concerns across 46 Reddit forums related to diet, fitness, and mental health, expanding beyond traditional focus on pro-anorexia communities. It highlights how different online spaces align with gender and body ideal dimensions, distinguishing between feminine-oriented forums promoting the thin ideal and muscular-ideal communities often connected to muscle dysmorphia ("bigorexia"). Feminine and thin-ideal communities express more negative emotions but receive caring and supportive comments, indicating a nurturing social response. In contrast, communities endorsing a muscular ideal show less negativity but attract aggressive compliments combining admiration with toxicity, regardless of gender orientation. Mental health discussions on these platforms tend to be more prevalent in feminine-leaning, thin ideal spaces. These gendered emotional dynamics reveal contrasting social interactions and levels of negativity across body image communities online. The findings suggest valuable insights for creating better moderation strategies on social media that encourage supportive communication and reduce harmful or toxic content exposure. By mapping these emotional and community support patterns, the study offers pathways to foster healthier online environments related to body image issues. <div>
arXiv:2407.03551v2 Announce Type: replace 
Abstract: The pervasive expectations about ideal body types in Western society can lead to body image concerns, dissatisfaction, and in extreme cases, eating disorders and other psychopathologies related to body image. While previous research has focused on online pro-anorexia communities glorifying the "thin ideal," less attention has been given to the broader spectrum of body image concerns or how emerging disorders like muscle dysmorphia ("bigorexia") present on online platforms. To address this gap, we analyze 46 Reddit forums related to diet, fitness, and mental health. We map these communities along gender and body ideal dimensions, revealing distinct patterns of emotional expression and community support. Feminine-oriented communities, especially those endorsing the thin ideal, express higher levels of negative emotions and receive caring comments in response. In contrast, muscular ideal communities display less negativity, regardless of gender orientation, but receive aggressive compliments in response, marked by admiration and toxicity. Mental health discussions align more with thin ideal, feminine-leaning spaces. By uncovering these gendered emotional dynamics, our findings can inform the development of moderation strategies that foster supportive interactions while reducing exposure to harmful content.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Independence Breeds Disruption but Limits Recognition</title>
<link>https://arxiv.org/abs/2504.09589</link>
<guid>https://arxiv.org/abs/2504.09589</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific disruption, knowledge independence, citations, innovation, research teams<br /><br />Summary:<br /><br />1. The study addresses two unresolved questions in scientific disruption research: why disruption has decreased despite growing knowledge, and why disruptive papers tend to receive fewer and delayed citations.  
2. A novel metric called "knowledge independence" is introduced, which measures the extent to which a paper relies on references that do not cite each other.  
3. Analysis of 114 million publications demonstrates that knowledge independence strongly predicts the disruptive nature of scientific work and explains why small, onsite, and fresh teams tend to produce more disruptive research.  
4. The observed long-term decline in knowledge independence parallels the decline in disruption and cannot be explained by null models, suggesting a mechanistic cause for this downward trend in scientific disruption.  
5. Further causal and simulation analyses reveal a persistent trade-off: while knowledge independence fosters disruption, it simultaneously limits the immediate recognition and citation impact of the work.  
Together, these findings establish knowledge independence as a key paper-level property that governs the dynamics of scientific innovation, providing a universal understanding that disruptive work, driven by independent knowledge bases, gains less immediate acclaim but fuels innovation over time. <div>
arXiv:2504.09589v2 Announce Type: replace-cross 
Abstract: Despite extensive research on scientific disruption, two questions remain: why disruption has declined amid growing knowledge, and why disruptive work receives fewer and delayed citations. One way to address these questions is to identify an intrinsic, paper-level property that reliably predicts disruption and explains both patterns. Here, we propose a novel measure, knowledge independence, capturing the extent to which a paper draws on references that do not cite one another. Analyzing 114 million publications, we find that knowledge independence strongly predicts disruption and mediates the disruptive advantage of small, onsite, and fresh teams. Its long-term decline, nonreproducible by null models, provides a mechanistic explanation for the parallel decline in disruption. Causal and simulation evidence further indicates that knowledge independence drives the persistent trade-off between disruption and impact. Taken together, these findings fill a critical gap in understanding scientific innovation, revealing a universal law: Knowledge independence breeds disruption but limits recognition.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finding core subgraphs of directed graphs via discrete Ricci curvature flow</title>
<link>https://arxiv.org/abs/2512.07899</link>
<guid>https://arxiv.org/abs/2512.07899</guid>
<content:encoded><![CDATA[
<div> Ricci curvature, curvature flow, directed graphs, strongly connected subgraphs, weakly connected graphs<br /><br />Summary: This paper introduces a novel definition of Ricci curvature and an associated curvature flow tailored specifically for directed graphs. Unlike previous research that focused predominantly on undirected graphs or required directed graphs to be strongly connected, this work expands the framework to handle weakly connected directed graphs by temporarily transforming them into strongly connected graphs through the addition of edges with very large artificial weights. Importantly, these artificially added edges do not affect the core subgraph detection because their extreme weights cause them to be discarded in the final step of the Ricci curvature flow. The curvature flow on strongly connected directed graphs guarantees a unique global solution, providing a solid theoretical foundation. The authors demonstrate the practical utility of their approach by applying it to detect strongly connected subgraphs within weakly connected directed graphs. Experimental results confirm that their method consistently outperforms traditional approaches, achieving superior performance on at least two out of three evaluation metrics for core detection. The work also contributes an openly accessible implementation, enabling further research and applications in the analysis of complex directed networks using geometric methods based on Ricci curvature. <div>
arXiv:2512.07899v1 Announce Type: new 
Abstract: Ricci curvature and its associated flow offer powerful geometric methods for analyzing complex networks. While existing research heavily focuses on applications for undirected graphs such as community detection and core extraction, there have been relatively less attention on directed graphs.
  In this paper, we introduce a definition of Ricci curvature and an accompanying curvature flow for directed graphs. Crucially, for strongly connected directed graphs, this flow admits a unique global solution. We then apply this flow to detect strongly connected subgraphs from weakly connected directed graphs. (A weakly connected graph is connected overall but not necessarily strongly connected). Unlike prior work requiring graphs to be strongly connected, our method loosens this requirement. We transform a weakly connected graph into a strongly connected one by adding edges with very large artificial weights. This modification does not compromise our core subgraph detection. Due to their extreme weight, these added edges are automatically discarded during the final iteration of the Ricci curvature flow.
  For core evaluation, our approach consistently surpasses traditional methods, achieving better results on at least two out of three key metrics. The implementation code is publicly available at https://github.com/12tangze12/Finding-core-subgraphs-on-directed-graphs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaffolding Reshapes Dialogic Engagement in Collaborative Problem Solving: Comparative Analysis of Two Approaches</title>
<link>https://arxiv.org/abs/2512.08045</link>
<guid>https://arxiv.org/abs/2512.08045</guid>
<content:encoded><![CDATA[
<div> Keywords: Collaborative Problem Solving, Scaffolding, Dialogic Engagement, Heterogeneous Interaction Network Analysis, Sequential Pattern Mining  

<br /><br />Summary:  
1. The study investigates how different scaffolding levels (maximal vs minimal) influence K-12 students' dialogic engagement and behaviors during different phases of Collaborative Problem Solving (CPS).  
2. Using Heterogeneous Interaction Network Analysis (HINA) and Sequential Pattern Mining (SPM), the research uncovers structural effects of scaffolding on CPS processes in real educational contexts.  
3. Students receiving maximal scaffolding exhibited higher dialogic engagement across more CPS phases but showed extensive scripting behaviors, indicating overscripting.  
4. Students with minimal scaffolding displayed more problem-solving behaviors and fewer scripting behaviors but tended to repeat certain behaviors across phases and moved more towards socializing behaviors.  
5. In both scaffolding conditions, problem-solving behaviors rarely transitioned into other problem-solving behaviors, suggesting limited progression.  
6. The findings highlight important considerations for designing scaffolds and teaching practices in CPS, emphasizing the need to balance support while avoiding overscripting.  
7. The study also demonstrates the complementary value of combining HINA and SPM methods to analyze students' learning processes during CPS effectively. <div>
arXiv:2512.08045v1 Announce Type: new 
Abstract: Supporting learners during Collaborative Problem Solving (CPS) is a necessity. Existing studies have compared scaffolds with maximal and minimal instructional support by studying their effects on learning and behaviour. However, our understanding of how such scaffolds could differently shape the distribution of individual dialogic engagement and behaviours across different CPS phases remains limited. This study applied Heterogeneous Interaction Network Analysis (HINA) and Sequential Pattern Mining (SPM) to uncover the structural effects of scaffolding on different phases of the CPS process among K-12 students in authentic educational settings. Students with a maximal scaffold demonstrated higher dialogic engagement across more phases than those with a minimal scaffold. However, they were extensively demonstrating scripting behaviours across the phases, evidencing the presence of overscripting. Although students with the minimal scaffold demonstrated more problem solving behaviours and fewer scripting behaviours across the phases, they repeated particular behaviours in multiple phases and progressed more to socialising behaviours. In both scaffold conditions, problem solving behaviours rarely progressed to other problem solving behaviours. The paper discusses the implications of these findings for scaffold design and teaching practice of CPS, and highlights the distinct yet complementary value of HINA and SPM approaches to investigate students' learning processes during CPS.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-aware PageRank via Edge Reweighting</title>
<link>https://arxiv.org/abs/2512.08055</link>
<guid>https://arxiv.org/abs/2512.08055</guid>
<content:encoded><![CDATA[
<div> Keywords: PageRank, fairness, link-analysis, group homophily, transition matrix<br /><br />Summary:<br /><br />This paper addresses the issue of incorporating group fairness into the PageRank algorithm, a widely-used link-analysis method for evaluating vertex importance in networks. The authors propose a novel approach that modifies the transition probabilities within the existing transition matrix rather than altering the network’s topology or restart vectors. They formulate fairness as the minimization of a fairness loss, defined as the difference between the original group-wise PageRank distribution and a desired target distribution. To account for group homophily—the tendency of nodes to connect more within the same group—a group-adapted fairness notion is introduced by using group-biased random walks with restarts tailored for each group. Because the fairness loss function is non-convex, an efficient projected gradient-descent algorithm is developed to find locally optimal edge weights that improve fairness. The paper distinguishes itself from prior work by strictly maintaining the original network edges, focusing solely on reweighting them to achieve fairer PageRank scores. Empirical evaluations demonstrate that these minimal adjustments to the transition matrix significantly enhance fairness, outperforming existing state-of-the-art methods while preserving the network’s structural integrity. <div>
arXiv:2512.08055v1 Announce Type: new 
Abstract: Link-analysis algorithms, such as PageRank, are instrumental in understanding the structural dynamics of networks by evaluating the importance of individual vertices based on their connectivity. Recently, with the rising importance of responsible AI, the question of fairness in link-analysis algorithms has gained traction. In this paper, we present a new approach for incorporating group fairness into the PageRank algorithm by reweighting the transition probabilities in the underlying transition matrix. We formulate the problem of achieving fair PageRank by seeking to minimize the fairness loss, which is the difference between the original group-wise PageRank distribution and a target PageRank distribution. We further define a group-adapted fairness notion, which accounts for group homophily by considering random walks with group-biased restart for each group. Since the fairness loss is non-convex, we propose an efficient projected gradient-descent method for computing locally-optimal edge weights. Unlike earlier approaches, we do not recommend adding new edges to the network, nor do we adjust the restart vector. Instead, we keep the topology of the underlying network unchanged and only modify the relative importance of existing edges. We empirically compare our approach with state-of-the-art baselines and demonstrate the efficacy of our method, where very small changes in the transition matrix lead to significant improvement in the fairness of the PageRank algorithm.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Framing Climate Change on YouTube: North-South Divides in Narratives and Public Engagement</title>
<link>https://arxiv.org/abs/2512.08183</link>
<guid>https://arxiv.org/abs/2512.08183</guid>
<content:encoded><![CDATA[
<div> Keywords: YouTube, climate change, Global North-South divide, sentiment analysis, international agreements<br /><br />Summary:<br /><br />This study investigates climate change debates on YouTube through the lens of the Global North-South divide, analyzing 758 climate-related videos and their comment sections. It applies topic modeling and sentiment analysis to uncover recurring discourse patterns that mirror those seen in international climate negotiations. The research identifies distinct thematic differences: videos from the Global North focus primarily on policies aimed at reducing carbon emissions, while those from the Global South emphasize developmental priorities alongside climate concerns. Despite these differences, both regions share a common recognition of the importance of emissions reduction and international climate agreements. Audience reactions show a stark contrast; comments on Global North videos are often characterized by criticism, conspiracy theories, and climate fatigue, whereas comments under Global South videos tend to be more supportive, constructive, and knowledge-driven. The study highlights how YouTube both reflects and shapes global climate politics, exposing the divide between official narratives and public sentiment. Bridging these divides on the platform could foster more inclusive and cooperative global climate action strategies. <div>
arXiv:2512.08183v1 Announce Type: new 
Abstract: Climate change debates have gained increasing visibility on social media, with YouTube emerging as one of the most influential platforms for political communication. Reaching billions of users worldwide, it functions both as a news outlet and as a space for public discourse. While existing studies of climate discourse on YouTube often adopt a global perspective, this study examines the platform through the lens of the Global North-South divide. We analyse a dataset of 758 climate-related videos and their comment sections, applying topic modelling and sentiment analysis to identify recurring discursive patterns. Through these patterns, we recognise parallels with respect to debates in international climate negotiations. The findings reveal notable differences. Videos from the Global North and Global South reflect real-world divides, with the North emphasising the need for policies to curb carbon emissions, while the South highlights developmental priorities. A key area of convergence between the regions lies in the shared recognition of the importance of emissions reduction and international agreements. Audience responses, however, diverge more sharply: comment sections under Global North videos are dominated by criticism, conspiracy, and climate fatigue, whereas those under Global South videos are generally more supportive, constructive, and knowledge-oriented. Overall, the study demonstrates how YouTube reflects and reshapes global climate politics, while also revealing the gap between curated narratives and public sentiment. Bridging these divides may contribute to more inclusive and cooperative approaches to climate action.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary perspective of large language models on shaping research insights into healthcare disparities</title>
<link>https://arxiv.org/abs/2512.08122</link>
<guid>https://arxiv.org/abs/2512.08122</guid>
<content:encoded><![CDATA[
<div> Healthcare disparities, large language models, ChatGPT, research themes, AI evolution<br /><br />Summary:<br /><br />1. Introduction: The paper explores the use of large language models (LLMs) as scientific assistants in the domain of healthcare disparities, focusing on how these models can help improve public access to relevant research information.<br /><br />2. Methods: It evaluates three prominent LLMs—ChatGPT, Copilot, and Gemini—by repeatedly querying them weekly with a consistent prompt about key research themes in healthcare disparities for one month.<br /><br />3. Analysis: The responses from these models were categorized and cross-validated against H-index metrics from Web of Science to ensure that the themes identified correspond with influential scientific work.<br /><br />4. Results: Findings indicate that the themes produced by the LLMs closely match actual scientific trends and impact, demonstrating their potential to assist users in understanding the landscape of healthcare disparities. Additionally, there were notable differences in how comprehensively and in-depth each model identified and classified the themes.<br /><br />5. Conclusion: The study proposes a framework that leverages the evolving outputs of multiple LLMs to highlight their utility in researching healthcare disparities, providing valuable insights for guiding future academic inquiry and enhancing public engagement efforts. <div>
arXiv:2512.08122v1 Announce Type: cross 
Abstract: Introduction. Advances in large language models (LLMs) offer a chance to act as scientific assistants, helping people grasp complex research areas. This study examines how LLMs evolve in healthcare disparities research, with attention to public access to relevant information. Methods. We studied three well-known LLMs: ChatGPT, Copilot, and Gemini. Each week, we asked them a consistent prompt about research themes in healthcare disparities and tracked how their answers changed over a one-month period. Analysis. The themes produced by the LLMs were categorized and cross-checked against H-index values from the Web of Science to verify relevance. This dual approach shows how the outputs of LLMs develop over time and how such progress could help researchers navigate trends. Results. The outputs aligned with actual scientific impact and trends in the field, indicating that LLMs can help people understand the healthcare disparities landscape. Time-series comparisons showed differences among the models in how broadly and deeply they identified and classified themes. Conclusion. The study offers a framework that uses the evolution of multiple LLMs to illuminate AI tools for studying healthcare disparities, informing future research and public engagement strategies.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WikIPedia: Unearthing a 20-Year History of IPv6 Client Addressing</title>
<link>https://arxiv.org/abs/2512.08808</link>
<guid>https://arxiv.org/abs/2512.08808</guid>
<content:encoded><![CDATA[
<div> IPv6, Wikimedia, IP addresses, EUI-64, adoption<br /><br />Summary:<br /><br />1. Wikimedia projects like Wikipedia serve as inadvertent time capsules capturing IPv6 addresses because anonymous edits are logged with the editor's IP instead of a username.<br />2. The study extracts and analyzes 19 million unique IPv6 client addresses from Wikimedia edits spanning 2003 to 2024.<br />3. These addresses provide valuable insights into how IPv6 adoption has evolved globally, especially relative to the countries associated with Wikimedia's language-specific sites.<br />4. The research investigates the regional prevalence of IPv6 usage and tracks trends in its adoption over more than two decades.<br />5. The analysis also explores the presence of EUI-64 addressing—a method embedding hardware identifiers in IPv6 addresses—to infer details about client device types such as desktops, laptops, and mobile phones.<br /><br />This work leverages a unique and extensive historical data set to deepen understanding of IPv6 deployment and addressing patterns, offering researchers a novel vantage point on Internet protocol evolution over time and across geographic regions. <div>
arXiv:2512.08808v1 Announce Type: cross 
Abstract: Due to their article editing policies, Wikimedia sites like Wikipedia have become inadvertent time capsules for IPv6 addresses. When Wikimedia users make edits without signing into an account, their IP addresses are used in lieu of a username. Wikimedia site dumps therefore provide researchers with over two decades worth of timestamped client IPv6 addresses to understand address assignments and how they have changed over time and space.
  In this work, we extract 19M unique IPv6 addresses from Wikimedia sites like Wikipedia that were used by editors from 2003 to 2024. We use these addresses to understand the prevalence of IPv6 in countries corresponding to Wikimedia site languages, how IPv6 adoption has grown over time, and the prevalence of EUI-64 addressing on client devices like desktops, laptops, and mobile phones.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rabble-Rousers in the New King's Court: Algorithmic Effects on Account Visibility in Pre-X Twitter</title>
<link>https://arxiv.org/abs/2512.06129</link>
<guid>https://arxiv.org/abs/2512.06129</guid>
<content:encoded><![CDATA[
<div> Algorithmic effects, right-leaning accounts, social media exposure, Elon Musk, verification status<br /><br />Summary: This paper analyzes how algorithmic curation on Twitter, after its change of ownership and before its rebranding as X, influenced the exposure of different accounts. It replicates earlier findings that right-leaning accounts receive more exposure in algorithmically curated feeds than in reverse-chronological ones. Importantly, the study finds that this advantage is less about political affiliation and more about behavioral aspects, such as posting provocative content and attracting attention from Elon Musk, the platform's owner and most central network figure. The research also highlights disparities in exposure related to verification status: legacy-verified accounts (including businesses and government officials) gained less exposure in the algorithmic feed compared to non-verified or Twitter Blue-verified accounts. These observations suggest that algorithmic incentives shape content reach by favoring behavior aligned with engagement-seeking strategies rather than political ideology. The findings raise critical questions about how such incentives impact online trust and safety, as algorithmic exposure may amplify divisive or agitating content, complicating efforts to ensure a healthy online environment. <div>
arXiv:2512.06129v1 Announce Type: new 
Abstract: Algorithmic effects on social media platforms have come under recent scrutiny, with several works reporting that right-leaning accounts tend to receive more exposure. In this paper, we expand upon this body of work using data collected from user feeds after Twitter's change of ownership but before its re-branding to X. We replicate findings from prior work regarding the increased exposure of right-leaning accounts to wider audiences in algorithmically curated compared to reverse-chronological feeds, and, crucially, we further unpack this effect to understand what correlated (and did not correlate) with these differences. Our results reveal that right-leaning accounts benefited not necessarily due to their political affiliation, but possibly because they behaved in ways associated with algorithmic rewards; namely, posting more agitating content and receiving attention from the platform's owner, Elon Musk, who was the most central network account. We also demonstrate that legacy-verified accounts, like businesses and government officials, received less exposure in the algorithmic feed compared to non-verified or Twitter Blue-verified accounts. We discuss implications of these findings for the intersection between behavioral incentives for algorithmic reach and online trust and safety.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The relationship between offline partisan geographical segregation and online partisan segregation</title>
<link>https://arxiv.org/abs/2512.07121</link>
<guid>https://arxiv.org/abs/2512.07121</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, echo chambers, partisan segregation, online networks, offline networks<br /><br />Summary:<br /><br />This study addresses the common belief that social media creates echo chambers by comparing online and offline partisan segregation among US voters. Researchers linked voter data from public files for 180 million individuals with their Twitter networks to analyze political homogeneity. They measured how isolated voters are from opposing partisans both geographically offline and within online social networks. Results show that while Twitter users tend to form politically homogeneous groups, the degree of partisan sorting is significantly less intense online than in real-world communities. Additionally, Democrats experience greater isolation than Republicans across both settings. Interestingly, the only exception to this trend is older Republicans, who show higher segregation online compared to offline. Overall, the findings suggest that offline environments exhibit stronger partisan segregation, challenging claims that social media is the primary driver of echo chambers. The study contributes important empirical evidence to political communication research and enhances understanding of homophily—the tendency to connect with similar others—in both online and offline political networks. <div>
arXiv:2512.07121v1 Announce Type: new 
Abstract: Social media is often blamed for the creation of echo chambers. However, these claims fail to consider the prevalence of offline echo chambers resulting from high levels of partisan segregation in the United States. Our article empirically assesses these online versus offline dynamics by linking a novel dataset of voters' offline partisan segregation extracted from publicly available voter files for 180 million US voters with their online network segregation on Twitter. We investigate offline and online partisan segregation using measures of geographical and network isolation of every matched voter-twitter user to their co-partisans online and offline. Our results show that while social media users tend to form politically homogeneous online networks, these levels of partisan sorting are significantly lower than those found in offline settings. Notably, Democrats are more isolated than Republicans in both settings, and only older Republicans exhibit higher online than offline segregation. Our results contribute to the emerging literature on political communication and the homophily of online networks, providing novel evidence on partisan sorting both online and offline.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction</title>
<link>https://arxiv.org/abs/2512.06356</link>
<guid>https://arxiv.org/abs/2512.06356</guid>
<content:encoded><![CDATA[
<div> Incomplete node features, feature propagation, graph neural networks, masked autoencoder, inductive tasks<br /><br />Summary:<br /><br />This paper addresses the challenges of incomplete node features in Graph Neural Networks (GNNs), which commonly occur in real-world applications and negatively impact model performance. It identifies three primary issues with traditional feature propagation (FP) methods: difficulty handling graphs with many disconnected components, susceptibility to over-smoothing of imputed features, and poor adaptability to inductive tasks due to feature distribution shifts. To overcome these, the authors propose DDFI, a method combining feature propagation with a graph-based Masked AutoEncoder (MAE) in a novel manner. Firstly, they introduce Co-Label Linking (CLL), an algorithm that randomly connects training nodes sharing the same label, improving performance for graphs with numerous connected components. Secondly, DDFI utilizes a two-step representation generation process at inference, where the MAE reconstructs features after FP imputation, reducing distribution shifts and enhancing feature diversity in inductive settings. Furthermore, the authors contribute a new real-world dataset, Sailing, containing naturally missing node features, for more realistic evaluation of imputation methods beyond synthetic masking. Experimental results on six public datasets plus Sailing demonstrate that DDFI outperforms state-of-the-art approaches under both transductive and inductive scenarios, validating its effectiveness and generalizability. <div>
arXiv:2512.06356v1 Announce Type: cross 
Abstract: Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Code vs. Context: STEM Students' Resistance to Non-STEM Coursework</title>
<link>https://arxiv.org/abs/2512.06529</link>
<guid>https://arxiv.org/abs/2512.06529</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive Switching Costs, Work Overload, Role Ambiguity, Affective Resistance, Long-Term Adoption  

<br /><br />Summary:  
This study investigates why engineering students resist mandated non-technical courses in STEM programs, focusing on underlying cognitive and identity-related factors rather than just workload. The researchers surveyed 212 undergraduate Computer Science and Engineering students and tested relationships using sequential OLS regression. They found Role Ambiguity—uncertainty about how non-technical courses fit into students' professional identities—to be the strongest predictor of Affective Resistance (beta = 0.47, p < 0.001), more so than Work Overload (beta = 0.20, p = 0.007) or Cognitive Switching Costs (beta = 0.14, p = 0.038). Affective Resistance, in turn, significantly decreased students’ Willingness to Engage (beta = -0.25, p < 0.001) with the courses. Willingness to Engage strongly predicted Long-Term Adoption of the skills learned (beta = 0.55, p < 0.001). These results imply that resistance arises primarily from the mismatch between non-technical content and students’ emerging professional identities, rather than from cognitive burden alone. The authors recommend curricular design that reduces role ambiguity by contextualizing humanities and social science content explicitly within engineering frameworks to enhance engagement and skill adoption. <div>
arXiv:2512.06529v1 Announce Type: cross 
Abstract: Many STEM programs now require students to take non-technical courses to develop the soft skills necessary for professional practice, yet engineering students frequently resist this requirement. While prior research often attributes this resistance to heavy workloads, little is known about its cognitive and identity-related mechanisms. This study fills this knowledge gap by examining the effects of Cognitive Switching Costs, Work Overload, and Role Ambiguity on students' Affective Resistance to non-STEM coursework, as well as the subsequent impact on their Willingness to Engage and Long-Term Adoption of skills. We collected survey data from 212 undergraduate Computer Science and Engineering students and tested directional relationships using sequential OLS regression. Role Ambiguity emerged as the strongest predictor of Affective Resistance (beta of 0.47, p less than 0.001), exceeding the effects of Work Overload (beta of 0.20, p equals 0.007) and Cognitive Switching Cost (beta of 0.14, p equals 0.038). In turn, Affective Resistance significantly reduced Willingness to Engage (beta of -0.25, p less than 0.001), while Willingness to Engage served as a strong predictor of Long-Term Adoption (beta of 0.55, p less than 0.001). These results indicate that student resistance is driven primarily by the incongruence between non-technical content and students' emergent professional identities, rather than by cognitive effort or workload alone. To improve outcomes, curricula should focus on reducing role ambiguity by placing humanities and social science material in clear engineering contexts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing Large-Scale Adversarial Activities Through Large-Scale Honey-Nets</title>
<link>https://arxiv.org/abs/2512.06557</link>
<guid>https://arxiv.org/abs/2512.06557</guid>
<content:encoded><![CDATA[
<div> Keywords: cyber threats, honeypot, network traffic, brute-force attacks, IoT security<br /><br />Summary: This paper presents a detailed longitudinal study of attacker behavior using HoneyTrap, an adaptive honeypot framework deployed across multiple geographically distributed nodes designed to mimic vulnerable services and safely capture malicious traffic. Over a 24-day observation period, the system recorded more than 60.3 million events, offering a rich dataset for analysis. To facilitate scalable analytics, raw JSON logs were converted into Apache Parquet format, which provided a 5.8 to 9.3 times compression improvement and 7.2 times faster query performance. Additionally, data enrichment techniques such as Autonomous System Number (ASN) tagging and salted SHA-256 pseudonymization were applied to enhance network intelligence and ensure privacy protection. The findings show that the majority of traffic targeted HTTP and HTTPS services (ports 80 and 443), with over 8 million connection attempts and daily peaks surpassing 1.7 million events. SSH (port 22) experienced intensive brute-force attacks, recording over 4.6 million attempts. Furthermore, less commonly targeted services such as Minecraft (port 25565) and SMB (port 445) also faced significant attacks, with Minecraft receiving roughly 118,000 daily connection attempts often occurring simultaneously with spikes on other ports. These insights contribute to understanding attacker strategies within critical infrastructure and IoT environments. <div>
arXiv:2512.06557v1 Announce Type: cross 
Abstract: The increasing sophistication of cyber threats demands novel approaches to characterize adversarial strategies, particularly those targeting critical infrastructure and IoT ecosystems. This paper presents a longitudinal analysis of attacker behavior using HoneyTrap, an adaptive honeypot framework deployed across geographically distributed nodes to emulate vulnerable services and safely capture malicious traffic. Over a 24 day observation window, more than 60.3 million events were collected. To enable scalable analytics, raw JSON logs were transformed into Apache Parquet, achieving 5.8 - 9.3x compression and 7.2x faster queries, while ASN enrichment and salted SHA-256 pseudonymization added network intelligence and privacy preservation.
  Our analysis reveals three key findings: (1) The majority of traffic targeted HTTP and HTTPS services (ports 80 and 443), with more than 8 million connection attempts and daily peaks exceeding 1.7 million events. (2) SSH (port 22) was frequently subject to brute-force attacks, with over 4.6 million attempts. (3) Less common services like Minecraft (25565) and SMB (445) were also targeted, with Minecraft receiving about 118,000 daily attempts that often coincided with spikes on other ports.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MASim: Multilingual Agent-Based Simulation for Social Science</title>
<link>https://arxiv.org/abs/2512.07195</link>
<guid>https://arxiv.org/abs/2512.07195</guid>
<content:encoded><![CDATA[
<div> Multilingual simulation, multi-agent systems, sociolinguistic profiles, public opinion modeling, information diffusion<br /><br />Summary:<br /><br />This paper introduces MASim, a novel multilingual agent-based simulation framework designed to model multi-turn interactions among generative agents with diverse sociolinguistic backgrounds. Unlike previous monolingual simulations, MASim captures cross-lingual interactions reflective of real-world social dynamics. The framework supports two primary analyses: first, global public opinion modeling that tracks how attitudes toward open-domain topics evolve across different languages and cultures; second, the study of media influence and information diffusion through autonomous news agents that generate dynamic content and influence user behavior. To enable realistic simulation scenarios, the authors create the MAPS benchmark, which integrates survey questions and demographic personas based on actual global population distributions. Through extensive experiments focused on calibration, sensitivity, consistency, and cultural case studies, MASim successfully reproduces sociocultural phenomena, demonstrating its capacity to model complex social interactions in multilingual contexts. The findings emphasize the critical role of multilingual simulations in computational social science, offering scalable and controlled setups for studying social behavior and language use in diverse populations. This work marks a significant advancement in simulating social and linguistic complexity using AI-driven agents. <div>
arXiv:2512.07195v1 Announce Type: cross 
Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Bias in Graph Hyperdimensional Computing</title>
<link>https://arxiv.org/abs/2512.07433</link>
<guid>https://arxiv.org/abs/2512.07433</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph hyperdimensional computing, fairness, demographic parity, bias mitigation, efficiency<br /><br />Summary:<br /><br />This paper addresses fairness issues in graph hyperdimensional computing (HDC), a brain-inspired approach using high-dimensional hypervectors for cognitive tasks on graph data. The study reveals that biases can arise and be amplified during hypervector encoding and similarity-based classification, leading to unfair treatment of different demographic groups. To tackle these challenges, the authors propose FairGHDC, a fairness-aware training framework that incorporates a bias correction term based on a gap-based demographic-parity regularizer. This correction is translated into a scalar fairness factor that adjusts the class hypervector update for the ground-truth label, enabling bias mitigation directly in hypervector space without altering the graph encoder or requiring backpropagation. Experimental validation on six benchmark datasets shows that FairGHDC effectively reduces demographic-parity and equal-opportunity disparities while preserving classification accuracy at a level comparable to standard and fairness-aware graph neural networks (GNNs). Additionally, FairGHDC maintains the computational benefits of HDC, achieving approximately a 10× speedup in training time on GPUs relative to GNN-based fairness approaches. Overall, the work advances fair and efficient graph representation learning by combining fairness constraints with the speed and robustness of hyperdimensional computing. <div>
arXiv:2512.07433v1 Announce Type: cross 
Abstract: Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\approx 10\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.07684</link>
<guid>https://arxiv.org/abs/2512.07684</guid>
<content:encoded><![CDATA[
<div> Keywords: online incivility, Graph Neural Network, toxicity detection, attention mechanism, Wikipedia community<br /><br />Summary:<br /><br />1. The paper addresses the problem of online incivility, which includes behaviors such as toxicity, aggression, and personal attacks, posing significant social and psychological challenges in digital communities.<br /><br />2. Existing moderation efforts and automated detection methods often lack accuracy and efficiency in identifying uncivil behavior.<br /><br />3. The authors propose a novel detection framework using Graph Neural Networks (GNNs), where each user comment is modeled as a node and edges represent textual similarity between comments, enabling joint learning from both the linguistic content and the relational structure of interactions.<br /><br />4. A dynamically adjusted attention mechanism is introduced to balance the influence of node features (textual content) and graph topology during the information aggregation process.<br /><br />5. Empirical tests on the English Wikipedia community demonstrate that the proposed GNN model surpasses 12 state-of-the-art Large Language Models (LLMs) in multiple performance metrics while being more computationally efficient.<br /><br />6. The study highlights the importance of incorporating structural context for effective behavioral prediction in online communities and points out the limitations of text-only LLM-based approaches.<br /><br />7. To foster further research and reproducibility, all relevant datasets and comparative results will be made publicly available in the authors’ repository. <div>
arXiv:2512.07684v1 Announce Type: cross 
Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ComGPT: Detecting Local Community Structure with Large Language Models</title>
<link>https://arxiv.org/abs/2408.06658</link>
<guid>https://arxiv.org/abs/2408.06658</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, community detection, seed expansion algorithm, ComGPT, graph encoding  

<br /><br />Summary:  
This paper addresses the limitations of Large Language Models (LLMs) like GPT-3.5-turbo in community detection tasks, noting their lack of specific domain knowledge and weaker grasp of community-related graph information. Traditional local community detection algorithms based on seed expansion often suffer from issues such as the seed-dependent problem, community diffusion, and the free rider effect. To overcome these challenges, the authors propose ComGPT, a GPT-guided seed expansion algorithm that iteratively selects candidate nodes using local modularity from the community's neighbors and leverages LLMs to choose the most appropriate node to add. To boost the LLMs' understanding of community structures, the study introduces ComIncident, a novel graph encoding technique that incorporates community knowledge specifically tailored for community detection. Additionally, the Node Selection Guide (NSG) prompt is designed to further enhance the LLMs’ comprehension of community characteristics during the node selection process. Experimental results demonstrate that ComGPT significantly outperforms baseline methods, validating the effectiveness of both the ComIncident encoding method and the NSG prompt in improving community detection performance. This work highlights the potential of combining LLMs with graph-specific techniques to tackle challenging graph reasoning tasks. <div>
arXiv:2408.06658v3 Announce Type: replace 
Abstract: Large Language Models (LLMs), like GPT-3.5-turbo, have demonstrated the ability to understand graph structures and have achieved excellent performance in various graph reasoning tasks, such as node classification. Despite their strong abilities in graph reasoning tasks, they lack specific domain knowledge and have a weaker understanding of community-related graph information, which hinders their capabilities in the community detection task. Moreover, local community detection algorithms based on seed expansion, referred to as seed expansion algorithms, often face several shortcomings, including the seed-dependent problem, community diffusion, and free rider effect. To use LLMs to overcome the above shortcomings, we explore a GPT-guided seed expansion algorithm named ComGPT. ComGPT iteratively selects potential nodes by local modularity from the detected community's neighbors, and subsequently employs LLMs to choose the node from these selected potential nodes to join the detected community. To improve LLMs' understanding of community-related graph information, we propose ComIncident, a graph encoding method that incorporates community knowledge and is designed for the community detection task. Additionally, we design the Node Selection Guide (NSG) prompt to enhance LLMs' understanding of community characteristics. Experimental results demonstrate that ComGPT outperforms the baselines, thereby confirming the effectiveness of the ComIncident and the NSG prompt.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can GNNs Learn Link Heuristics? A Concise Review and Evaluation of Link Prediction Methods</title>
<link>https://arxiv.org/abs/2411.14711</link>
<guid>https://arxiv.org/abs/2411.14711</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, link prediction, node embeddings, neighborhood aggregation, structural information<br /><br />Summary: This paper investigates the capabilities of Graph Neural Networks (GNNs) in learning different types of information crucial for link prediction, complemented by a concise review of prevailing link prediction techniques. It identifies a key limitation of GNNs: their inability to effectively capture structural information associated with the number of common neighbors between node pairs, which is primarily due to the set-based pooling mechanism used in neighborhood aggregation. Through extensive experiments, the study finds that incorporating trainable node embeddings significantly enhances the performance of GNN-based link prediction models. This enhancement is particularly pronounced in denser graphs, where nodes have more opportunities to participate in the neighborhood aggregation of others. The authors explain that in such graphs, the link states can be encoded more effectively into node embeddings, as nodes involved in multiple link samples aggregate more comprehensive information. The paper concludes by emphasizing that these insights not only clarify the fundamental limitations inherent in current link prediction methods but also provide valuable guidance for developing more robust and effective algorithms for future research in this area. <div>
arXiv:2411.14711v2 Announce Type: replace 
Abstract: This paper explores the ability of Graph Neural Networks (GNNs) in learning various forms of information for link prediction, alongside a brief review of existing link prediction methods. Our analysis reveals that GNNs cannot effectively learn structural information related to the number of common neighbors between two nodes, primarily due to the nature of set-based pooling of the neighborhood aggregation scheme. Also, our extensive experiments indicate that trainable node embeddings can improve the performance of GNN-based link prediction models. Importantly, we observe that the denser the graph, the greater such the improvement. We attribute this to the characteristics of node embeddings, where the link state of each link sample could be encoded into the embeddings of nodes that are involved in the neighborhood aggregation of the two nodes in that link sample. In denser graphs, every node could have more opportunities to attend the neighborhood aggregation of other nodes and encode states of more link samples to its embedding, thus learning better node embeddings for link prediction. Lastly, we demonstrate that the insights gained from our research carry important implications in identifying the limitations of existing link prediction methods, which could guide the future development of more robust algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Approximate Biclique Counting over Large Bipartite Graphs</title>
<link>https://arxiv.org/abs/2505.10471</link>
<guid>https://arxiv.org/abs/2505.10471</guid>
<content:encoded><![CDATA[
<div> Keywords: bipartite graphs, (p,q)-bicliques, approximation algorithms, dynamic programming, graph coloring  

<br /><br />Summary:  
Counting \((p,q)\)-bicliques within bipartite graphs is essential for applications such as recommendation systems and cohesive subgraph analysis, but exact counting is computationally expensive due to combinatorial explosion. The paper proposes a novel approximation method leveraging the concept of a \((p,q)\)-broom, a special spanning tree contained in the \((p,q)\)-biclique, which can be efficiently enumerated using graph coloring and dynamic programming. By utilizing intermediate dynamic programming results, the authors introduce a sampling algorithm that estimates the \((p,q)\)-biclique count from the \((p,q)\)-broom counts. The method is proved to provide unbiased estimates accompanied by theoretical error bounds, ensuring reliability in approximation. Experimental evaluation on nine real-world bipartite networks demonstrates that the approach achieves significant improvements compared to existing techniques, delivering up to 8 times reduction in estimation error and up to 50 times faster runtimes. This work thus offers a scalable, accurate, and efficient tool for large-scale \((p,q)\)-biclique counting, addressing practical challenges in graph analysis tasks where exact counts are often unnecessary. <div>
arXiv:2505.10471v2 Announce Type: replace 
Abstract: Counting $(p,q)$-bicliques in bipartite graphs is crucial for a variety of applications, from recommendation systems to cohesive subgraph analysis. Yet, it remains computationally challenging due to the combinatorial explosion to exactly count the $(p,q)$-bicliques. In many scenarios, e.g., graph kernel methods, however, exact counts are not strictly required. To design a scalable and high-quality approximate solution, we novelly resort to $(p,q)$-broom, a special spanning tree of the $(p,q)$-biclique, which can be counted via graph coloring and efficient dynamic programming. Based on the intermediate results of the dynamic programming, we propose an efficient sampling algorithm to derive the approximate $(p,q)$-biclique count from the $(p,q)$-broom counts. Theoretically, our method offers unbiased estimates with provable error guarantees. Empirically, our solution outperforms existing approximation techniques in both accuracy (up to 8$\times$ error reduction) and runtime (up to 50$\times$ speedup) on nine real-world bipartite networks, providing a scalable solution for large-scale $(p,q)$-biclique counting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProbeWalk: Fast Estimation of Biharmonic Distance on Graphs via Probe-Driven Random Walks</title>
<link>https://arxiv.org/abs/2512.05460</link>
<guid>https://arxiv.org/abs/2512.05460</guid>
<content:encoded><![CDATA[
<div> Biharmonic distance, graph metrics, algorithm complexity, relative-error guarantee, large-scale networks<br /><br />Summary:<br /><br />1. The biharmonic distance is an important graph metric used to measure dissimilarity between nodes by capturing both local and global graph structures, useful in areas such as network centrality, graph clustering, and machine learning.<br /><br />2. Efficient computation of pairwise biharmonic distances is critical but challenging due to high computational costs of existing algorithms.<br /><br />3. Current state-of-the-art methods achieve an absolute-error guarantee with time complexity O(L^5 / epsilon_abs^2), where L is the truncation length, which becomes prohibitively expensive for large L values frequently encountered in real-world networks.<br /><br />4. This work introduces a new algorithm leveraging probe-driven random walks to reduce the time complexity to O(L^3 / epsilon^2) while providing a relative-error guarantee, which is more appropriate given the wide range of biharmonic distance magnitudes.<br /><br />5. Experiments on large real-world networks demonstrate that the proposed method achieves 10 to 1000 times faster query times at comparable relative errors to existing strong baselines, and scales efficiently to graphs with tens of millions of nodes. <div>
arXiv:2512.05460v1 Announce Type: new 
Abstract: The biharmonic distance is a fundamental metric on graphs that measures the dissimilarity between two nodes, capturing both local and global structures. It has found applications across various fields, including network centrality, graph clustering, and machine learning. These applications typically require efficient evaluation of pairwise biharmonic distances. However, existing algorithms remain computationally expensive. The state-of-the-art method attains an absolute-error guarantee epsilon_abs with time complexity O(L^5 / epsilon_abs^2), where L denotes the truncation length. In this work, we improve the complexity to O(L^3 / epsilon^2) under a relative-error guarantee epsilon via probe-driven random walks. We provide a relative-error guarantee rather than an absolute-error guarantee because biharmonic distances vary by orders of magnitude across node pairs. Since L is often very large in real-world networks (for example, L >= 10^3), reducing the L-dependence from the fifth to the third power yields substantial gains. Extensive experiments on real-world networks show that our method delivers 10x-1000x per-query speedups at matched relative error over strong baselines and scales to graphs with tens of millions of nodes.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Power of Network Pluralism: Multi-Perspective Modeling of Heterogeneous Legal Document Networks</title>
<link>https://arxiv.org/abs/2512.05679</link>
<guid>https://arxiv.org/abs/2512.05679</guid>
<content:encoded><![CDATA[
<div> Keywords: Network Pluralism, epistemological pluralism, multi-perspectivity, multi-network analysis, legal systems<br /><br />Summary:<br /><br />1. The article introduces the concept of Network Pluralism, a framework in network science inspired by epistemological pluralism, which emphasizes considering multiple research perspectives simultaneously to achieve a more complete understanding of complex phenomena.<br /><br />2. It highlights that insights in research are inherently relative, shaped by assumptions, scopes, and methods, making it impossible for a single perspective to generate complete knowledge.<br /><br />3. To demonstrate Network Pluralism, the authors conduct an empirical analysis of complex legal systems by constructing a heterogeneous network space that incorporates references across documents from different branches of government, organizational hierarchies above, and detailed structures below the document level.<br /><br />4. Their multi-network approach reveals how complementing perspectives can contextualize high-level findings, how contrasting networks derived from the same data facilitates learning through differences, and how linking network metrics to specific perspectives enhances the transparency and robustness of network analysis.<br /><br />5. The paper also acknowledges the challenge of mapping domain-specific variation to network modeling choices and metric parameters, proposing their work as a blueprint to encourage the adoption of Network Pluralism in interdisciplinary, domain-driven network research. <div>
arXiv:2512.05679v1 Announce Type: new 
Abstract: Insights are relative - influenced by a range of factors such as assumptions, scopes, or methods that together define a research perspective. In normative and empirical fields alike, this insight has led to the conclusion that no single perspective can generate complete knowledge. As a response, epistemological pluralism mandates that researchers consider multiple perspectives simultaneously to obtain a holistic understanding of their phenomenon under study. Translating this mandate to network science, our work introduces Network Pluralism as a conceptual framework that leverages multi-perspectivity to yield more complete, meaningful, and robust results. We develop and demonstrate the benefits of this approach via a hands-on analysis of complex legal systems, constructing a network space from references across documents from different branches of government as well as including organizational hierarchy above and fine-grained structure below the document level. Leveraging the resulting heterogeneity in a multi-network analysis, we show how complementing perspectives can help contextualize otherwise high-level findings, how contrasting several networks derived from the same data enables researchers to learn by difference, and how relating metrics to perspectives may increase the transparency and robustness of network-analytical results. To analyze a space of networks as perspectives, researchers need to map dimensions of variation in a given domain to network-modeling decisions and network-metric parameters. While this remains a challenging and inherently interdisciplinary task, our work acts as a blueprint to facilitate the broader adoption of Network Pluralism in domain-driven network research.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Job Satisfaction Through the Lens of Social Media: Rural--Urban Patterns in the U.S</title>
<link>https://arxiv.org/abs/2512.05144</link>
<guid>https://arxiv.org/abs/2512.05144</guid>
<content:encoded><![CDATA[
<div> Keywords: job satisfaction, social media, rural-urban gap, labor market, subjective well-being<br /><br />Summary:<br /><br />1. The study introduces a new large-scale measure of U.S. job satisfaction derived from analyzing 2.6 billion georeferenced tweets using a fine-tuned large language model. <br />2. This social-media-based metric is linked to county-level labor market data spanning from 2013 to 2023, allowing for an extensive examination of regional job satisfaction trends.<br />3. Logistic regression analyses reveal that rural counties consistently exhibit lower job satisfaction sentiments compared to urban counties.<br />4. However, the rural-urban job satisfaction gap diminishes during periods of tight labor markets, indicating that employment conditions strongly influence subjective job quality perceptions.<br />5. Unlike the growing income disparities between rural and urban areas, perceived job quality tends to converge in low-unemployment contexts, suggesting that labor market slack, rather than income differences alone, plays a crucial role in spatial inequality related to work-related well-being. <div>
arXiv:2512.05144v1 Announce Type: cross 
Abstract: We analyze a novel large-scale social-media-based measure of U.S. job satisfaction, constructed by applying a fine-tuned large language model to 2.6 billion georeferenced tweets, and link it to county-level labor market conditions (2013-2023). Logistic regressions show that rural counties consistently report lower job satisfaction sentiment than urban ones, but this gap decreases under tight labor markets. In contrast to widening rural-urban income disparities, perceived job quality converges when unemployment is low, suggesting that labor market slack, not income alone, drives spatial inequality in subjective work-related well-being.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards agent-based-model informed neural networks</title>
<link>https://arxiv.org/abs/2512.05764</link>
<guid>https://arxiv.org/abs/2512.05764</guid>
<content:encoded><![CDATA[
<div> Agent-Based Models, Neural Networks, Graph Neural Networks, SIR Model, Macroeconomic Dynamics<br /><br />Summary:<br /><br />1. This article introduces a novel framework called Agent-Based-Model informed Neural Networks (ABM-NNs) designed to build neural networks that maintain consistency with agent-based models, particularly preserving structural constraints absent from typical neural differential equations.<br /><br />2. The framework addresses key limitations of standard neural differential equations in complex system modeling by integrating constraints such as mass conservation, network locality, and bounded rationality rather than relying on classical physical invariants like energy.<br /><br />3. ABM-NNs employ restricted graph neural networks combined with hierarchical decomposition to enable learning of interpretable and structure-preserving dynamics that capture the underlying system mechanisms.<br /><br />4. Validation is performed through three case studies: (i) recovering ground-truth parameters from short trajectories in a generalized Lotka-Volterra model with interventions, (ii) outperforming leading graph learning baselines such as GCN, GraphSAGE, and Graph Transformer in out-of-sample forecasting and noise robustness on a graph-based SIR contagion model, and (iii) modeling coupled GDP dynamics of the ten largest economies and demonstrating gradient-based counterfactual policy intervention analyses.<br /><br />5. The study demonstrates the potential of ABM-NNs to provide interpretable, robust, and policy-relevant modeling tools across multiple domains involving complex agent-based systems. <div>
arXiv:2512.05764v1 Announce Type: cross 
Abstract: In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka--Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finding coherent node groups in directed graphs</title>
<link>https://arxiv.org/abs/2310.02993</link>
<guid>https://arxiv.org/abs/2310.02993</guid>
<content:encoded><![CDATA[
<div> Keywords: directed network clustering, group coherence, NP-hard problem, approximation algorithm, heuristics  

<br /><br />Summary:  
This paper studies the problem of partitioning a directed network into an ordered sequence of coherent groups, where nodes have features used to measure group coherence. The problem generalizes classic clustering by incorporating penalties for forward and backward cross edges, with backward edge penalties introducing the significance of group order, linking it to segmentation problems. An iterative approach is considered, alternating between assigning nodes to groups given centroids and finding centroids given groups. Unlike clustering, the assignment step is NP-hard in general; however, exact solutions are possible when the underlying graph is a tree or for two groups. For the general scenario, the authors propose an approximation algorithm based on linear programming. Additionally, three heuristics are introduced: optimizing pairs of groups while fixing others, using a spanning tree to reduce complexity, and a greedy node-switching approach to minimize loss. Experiments demonstrate the practicality of the methods and their ability to yield interpretable groupings, validating the approach for coherent group discovery in directed networks with feature data and edge directionality considerations. <div>
arXiv:2310.02993v2 Announce Type: replace 
Abstract: Grouping the nodes of a graph into clusters is a standard technique for studying networks. We study a problem where we are given a directed network and are asked to partition the graph into a sequence of coherent groups. We assume that nodes in the network have features, and we measure the group coherence by comparing these features. Furthermore, we incorporate the cross edges by penalizing the forward cross edges and backward cross edges with different weights. If the weights are set to 0, then the problem is equivalent to clustering. However, if we penalize the backward edges, the order of discovered groups matters, and we can view our problem as a generalization of a classic segmentation problem.
  We consider a common iterative approach where we solve the groups given the centroids, and then find the centroids given the groups. We show that, unlike in clustering, the first subproblem is NP-hard. However, we show that we can solve the subproblem exactly if the underlying graph is a tree or if the number of groups is 2. For a general case, we propose an approximation algorithm based on linear programming.
  We propose 3 additional heuristics: (1) optimizing each pair of groups separately while keeping the remaining groups intact, (2) computing a spanning tree and then optimizing using only the edges in that, and (3) a greedy search moving nodes between the groups while optimizing the overall loss. We demonstrate with our experiments that the algorithms are practical and yield interpretable results.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Networks: Enumerating Maximal Community Patterns in $c$-Closed Graphs</title>
<link>https://arxiv.org/abs/2506.11437</link>
<guid>https://arxiv.org/abs/2506.11437</guid>
<content:encoded><![CDATA[
<div> Keywords: c-closed graphs, maximal blow-ups, polynomial enumeration, induced blow-ups, social networks<br /><br />Summary:<br /><br />This paper focuses on the model of $c$-closed graphs introduced by Jacob Fox, C. Seshadhri, Tim Roughgarden, Fan Wei, and Nicole Wein, which captures structural characteristics related to triadic closure observed in social networks. Unlike general graphs where enumerating maximal cliques is computationally expensive, $c$-closed graphs allow polynomial-time enumeration of maximal cliques and maximal complete bipartite subgraphs. The authors extend this framework by examining maximal blow-ups of arbitrary fixed finite graphs $H$ within $c$-closed graphs. They prove that for any fixed $H$, the number of maximal blow-ups in an $n$-vertex $c$-closed graph is polynomially bounded in $n$. Additionally, they investigate induced blow-ups, identifying precisely which graphs $H$ yield a polynomially bounded number of maximal induced blow-ups. Finally, the paper explores scenarios when $H$ is not fixed but varies across an infinite family of graphs, studying analogous enumeration questions. This work generalizes previous results related to simple building blocks (single vertices or edges) to more complex pattern graphs, deepening the understanding of structural enumeration in $c$-closed graphs and their algorithmic implications. <div>
arXiv:2506.11437v2 Announce Type: replace-cross 
Abstract: Jacob Fox, C. Seshadhri, Tim Roughgarden, Fan Wei, and Nicole Wein introduced the model of $c$-closed graphs--a distribution-free model motivated by triadic closure, one of the most pervasive structural signatures of social networks. While enumerating maximal cliques in general graphs can take exponential time, it is known that in $c$-closed graphs, maximal cliques and maximal complete bipartite subgraphs can always be enumerated in polynomial time. These structures correspond to blow-ups of simple patterns: a single vertex or a single edge, with some vertices required to form cliques. In this work, we explore a natural extension: we study maximal blow-ups of arbitrary finite graphs $H$ in $c$-closed graphs. We prove that for any fixed graph $H$, the number of maximal blow-ups of $H$ in an $n$-vertex $c$-closed graph is always bounded by a polynomial in $n$. We further investigate the case of induced blow-ups and provide a precise characterization of the graphs $H$ for which the number of maximal induced blow-ups is also polynomially bounded in $n$. Finally, we study the analogue questions when $H$ ranges over an infinite family of graphs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Articulation Point Identification in Time-Varying Undirected Networks</title>
<link>https://arxiv.org/abs/2512.04409</link>
<guid>https://arxiv.org/abs/2512.04409</guid>
<content:encoded><![CDATA[
<div> articulation points, time-varying networks, distributed algorithm, incremental update, biconnectivity<br /><br />Summary:<br /><br />1. The paper addresses the problem of identifying articulation points (APs) in dynamic, time-varying networks where topological changes occur frequently. 2. It proposes a fully distributed algorithm that efficiently updates the set of APs and monitors biconnectivity based on incremental changes rather than global recomputation. 3. The key innovation is an incremental update protocol that propagates information only from the nodes affected by edge additions or deletions, reducing communication overhead significantly. 4. The algorithm leverages a maximum consensus protocol to ensure convergence to the correct set of articulation points after any change in the network topology while maintaining privacy by preventing nodes from reconstructing the entire global topology. 5. The authors provide rigorous proofs demonstrating the correctness and eventual convergence of their approach and validate the method’s performance and practicality with experimental evaluations. <div>
arXiv:2512.04409v1 Announce Type: new 
Abstract: Identifying articulation points (APs) is fundamental to assessing the robustness of time-varying networks. In such dynamic environments, topological changes including edge additions and deletions can instantly alter the set of APs, demanding rapid and efficient re-assessment. This paper proposes a fully distributed algorithm for identifying APs and monitoring biconnectivity. Our core contribution is an incremental update protocol. Unlike static methods that require global re-initialization which incurs high communication overhead, our algorithm propagates information from the site of the change, updating only the affected nodes' state values. This approach, which builds upon a maximum consensus protocol, not only ensures convergence to the correct AP set following topological changes but also preserves network privacy by preventing nodes from reconstructing the global topology. We provide rigorous proofs of correctness for this eventual convergence and demonstrate its applicability and efficiency through experiments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When GenAI Meets Fake News: Understanding Image Cascade Dynamics on Reddit</title>
<link>https://arxiv.org/abs/2512.04639</link>
<guid>https://arxiv.org/abs/2512.04639</guid>
<content:encoded><![CDATA[
<div> AI-generated content, misinformation, visual content, Reddit, virality<br /><br />Summary: This article addresses the increasing prevalence of AI-generated content and misinformation on social networks, emphasizing the relatively understudied role of visual content in spreading misinformation. The study conducts the first large-scale analysis of how misinformation and AI-generated images propagate through repost cascades within five ideologically diverse Reddit communities. The research integrates multiple factors—including textual sentiment, visual attributes, and diffusion metrics such as time-to-first repost and community reach—to understand content virality. The proposed framework effectively predicts immediate post-level virality with an AUC of 0.83, showing strong performance in identifying which posts will quickly gain attention. Additionally, the model forecasts long-term cascade-level spread with an impressive AUC of 0.998, indicating near-perfect accuracy in predicting the overall extent of misinformation diffusion. These findings provide valuable insights that can aid platform moderators and policymakers in identifying and managing the dissemination of synthetic and misleading visual content online, contributing to efforts aimed at reducing the spread of misinformation across social media. <div>
arXiv:2512.04639v1 Announce Type: new 
Abstract: AI-generated content and misinformation are increasingly prevalent on social networks. While prior research primarily examined textual misinformation, fewer studies have focused on visual content's role in virality. In this work, we present the first large-scale analysis of how misinformation and AI-generated images propagate through repost cascades across five ideologically diverse Reddit communities. By integrating textual sentiment, visual attributes, and diffusion metrics (e.g., time-to-first repost, community reach), our framework accurately predicts both immediate post-level virality (AUC=0.83) and long-term cascade-level spread (AUC=0.998). These findings offer essential insights for moderating synthetic and misleading visual content online.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring YouTube's Political Communication Networks during the 2024 French Elections</title>
<link>https://arxiv.org/abs/2512.04971</link>
<guid>https://arxiv.org/abs/2512.04971</guid>
<content:encoded><![CDATA[
<div> Keywords: French elections, YouTube activity, political engagement, commenting communities, media bias<br /><br />Summary: The article analyzes the political climate in France during 2024 following the far-right National Rally's victory in the European elections, which led President Emmanuel Macron to dissolve the National Assembly and call legislative elections. It focuses on YouTube activity by 35 news media channels and 28 politicians’ and parties’ channels, covering 43.5k videos and 7.4 million comments from three months before the European elections until one week after the second round of the legislative elections. The study reveals that far-right and left-wing politician and party channels were significantly more active and attracted higher engagement—measured in views, likes, and comments—compared to other groups, with dense and highly clustered commenting communities. Approximately 7% of commenters engaged across different political orientations and were more active than those commenting within their own groups. News media channels showed a tendency to favor politically aligned guests, though centrist politicians appeared disproportionately often. The presence of politicians on news media channels was shown to increase the share of commenters active both on the media channel and on political channels, regardless of commenters’ political orientation, highlighting cross-platform engagement and interaction patterns. <div>
arXiv:2512.04971v1 Announce Type: new 
Abstract: In 2024, France was shaken by the far-right National Rally's victory in the European elections. In response to this unprecedented result, French President Emmanuel Macron dissolved the National Assembly, triggering legislative elections just two weeks later. A whirlwind campaign followed, partly on social media, as is now the norm, and concluded with the victory of a left-wing coalition. This article examines the YouTube activity of two key actors during this period, news media and politicians, and the commenting behavior they generated. We built a dataset of 35 news media channels, 28 politicians and parties channels, 43.5k videos posted from three months before the European elections to one week after the second round of the legislative elections, and 7.4M associated comments. We examined upload activity and engagement across political orientations and used network analysis methods to uncover the structure of their commenting communities. We also identified politicians' appearances on news media channels and assessed their impact on commenting user bases. Our findings show that, among politicians and parties channels, far-right and left-wing ones were significantly more active and received substantially higher engagement (views, likes, and comments) than other groups, with denser and more clustered commenting communities. About 7% of commenters commented across political orientations and were much more active than in-group commenters. News media channels tended to favor politically aligned guests, while centrist politicians were over-represented. Finally, politicians' presence in the videos of a specific news media channel increased the share of commenters who were active on this channel and political channels, regardless of their orientation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A 'Turtle Model' of Food System Transformations: Embracing Citizens' Diverse Values and Knowledge in Change Processes</title>
<link>https://arxiv.org/abs/2512.04384</link>
<guid>https://arxiv.org/abs/2512.04384</guid>
<content:encoded><![CDATA[
<div> Keywords: sustainable food systems, democratic food governance, wicked problems, quadruple helix innovation, citizen engagement<br /><br />Summary:<br /><br />This article addresses the complexities of transitioning to sustainable food systems through democratic food governance, emphasizing inclusivity and systemic change. It introduces a 'turtle model' theory of change that incorporates citizens' diverse values and knowledge to map multiple pathways for transformation. The study highlights cities as key innovation and governance hubs under the quadruple helix model, integrating government, industry, academia, and civil society to foster collaborative food system transformations. Dublin, Ireland, serves as a case study where citizens' food identities were mobilized to enhance ecological awareness and encourage sustainable seafood consumption. The democratic governance framework champions approaches like open science, transdisciplinarity, and active citizen participation to connect various food system stakeholders in shared dialogue and coordinated action. Overall, the article proposes systemic and democratic strategies to tackle wicked problems in food sustainability, leveraging urban innovation ecosystems and inclusive participation to drive meaningful, multi-actor change in food systems. <div>
arXiv:2512.04384v1 Announce Type: cross 
Abstract: We explore the challenges and opportunities of transitioning towards sustainable food systems through the lens of democratic food governance fostering inclusive and systemic transformation. Drawing on concepts of wicked problems and systems thinking, we propose a theory of change represented as a 'turtle model' that embraces the diversity of citizens' values and knowledge to highlight multiple avenues of transformation. As quadruple helix innovation and governance hubs, cities can be hotspots for food system transformations. We illustrate this for Dublin, Ireland, where local citizens' value-based food identities were galvanized to activate ecological awareness and promote sustainable seafood consumption. Within this democratic food governance framework, approaches such as open science, transdisciplinarity, and citizen engagement are fit-for-purpose to engage diverse food actors from government, industry, academia, and civil society in shared dialogue and action to transform food systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Dynamics Based on Reputation in Networked Populations with Game Transitions</title>
<link>https://arxiv.org/abs/2512.04671</link>
<guid>https://arxiv.org/abs/2512.04671</guid>
<content:encoded><![CDATA[
<div> Keywords: evolutionary dynamics, cooperation, reputation, game transitions, network topology

<br /><br />Summary:  
This paper investigates how the dynamic interplay between game transitions and reputation affects the evolution of cooperation within populations structured by networks. The environment is considered to be constantly changing due to both internal (endogenous) and external (exogenous) factors, influencing individuals' physical and psychological states and, consequently, the evolutionary dynamics of strategies. Game transitions depend not only on individuals’ behaviors but also on uncontrollable external factors. Reputation is modeled based on neighbors' cooperation levels and factored into a general fitness function that combines both payoff and reputation. Using the donation game framework, the study explores outcomes across various network topologies, reflecting diverse interaction patterns. Additionally, the authors incorporate a biased mutation mechanism to better understand its effect on strategy evolution. Extensive simulations reveal a significant increase in cooperation levels under these model conditions. The study also uncovers notable phenomena such as the unilateral increase in prosocial behavior value restricting the promotion of cooperation on square-lattice networks. Overall, the findings highlight the complex effects of environment-driven game changes and reputation dynamics on cooperative behavior evolution in structured populations. <div>
arXiv:2512.04671v1 Announce Type: cross 
Abstract: The environment undergoes perpetual changes that are influenced by a combination of endogenous and exogenous factors. Consequently, it exerts a substantial influence on an individual's physical and psychological state, directly or indirectly affecting the evolutionary dynamics of a population described by a network, which in turn can also alter the environment. Furthermore, the evolution of strategies, shaped by reputation, can diverge due to variations in multiple factors. To explore the potential consequences of the mentioned situations, this paper studies how game and reputation dynamics alter the evolution of cooperation. Concretely, game transitions are determined by individuals' behaviors and external uncontrollable factors. The cooperation level of its neighbors reflects individuals' reputation, and further, a general fitness function regarding payoff and reputation is provided. Within the context of the donation game, we investigate the relevant outcomes associated with the aforementioned evolutionary process, considering various topologies for distinct interactions. Additionally, a biased mutation is introduced to gain a deeper insight into the strategy evolution. We detect a substantial increase in the cooperation level through intensive simulations, and some important phenomena are observed, e.g., the unilateral increase of the value of prosocial behavior limits promotion in cooperative behavior in square-lattice networks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accumulated Local Effects and Graph Neural Networks for link prediction</title>
<link>https://arxiv.org/abs/2512.03061</link>
<guid>https://arxiv.org/abs/2512.03061</guid>
<content:encoded><![CDATA[
<div> Accumulated Local Effects, Graph Neural Networks, Link Prediction, Explainability, Approximate Method  

<br /><br />Summary:  
This article explores the adaptation of Accumulated Local Effects (ALE), a model-agnostic interpretability technique, for explaining the influence of node features in link prediction tasks using Graph Neural Networks (GNNs), specifically Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). A central challenge arises from the intricate interactions among nodes during the message passing process across GNN layers, which complicates the straightforward use of ALE. The paper highlights the computational inefficiency of modifying one node at a time for ALE calculations. To address this, the authors propose an approximate method designed to reduce computation time while maintaining reliable explanations. Experiments indicate that although the approximate method is computationally more efficient, the exact method produces more stable explanations, especially when smaller data subsets are involved. Nonetheless, the approximate method's explanations do not significantly differ from those of the exact method, demonstrating its practical value. Finally, the study investigates the impact of different parameters on the accuracy of ALE estimations for both the exact and approximate approaches, providing insights into optimizing explanation quality and computational resources in GNN link prediction tasks. <div>
arXiv:2512.03061v1 Announce Type: new 
Abstract: We investigate how Accumulated Local Effects (ALE), a model-agnostic explanation method, can be adapted to visualize the influence of node feature values in link prediction tasks using Graph Neural Networks (GNNs), specifically Graph Convolutional Networks and Graph Attention Networks. A key challenge addressed in this work is the complex interactions of nodes during message passing within GNN layers, complicating the direct application of ALE. Since a straightforward solution of modifying only one node at once substantially increases computation time, we propose an approximate method that mitigates this challenge. Our findings reveal that although the approximate method offers computational efficiency, the exact method yields more stable explanations, particularly when smaller data subsets are used. However, the explanations produced with the approximate method are not significantly different from the ones obtained with the exact method. Additionally, we analyze how varying parameters affect the accuracy of ALE estimation for both approaches.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Multimodal Graph-based Model for Geo-social Analysis</title>
<link>https://arxiv.org/abs/2512.03063</link>
<guid>https://arxiv.org/abs/2512.03063</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal analysis, graph-based model, geospatial context, disaster management, unsupervised learning<br /><br />Summary:<br /><br />This paper addresses the challenge of analyzing user-generated social media content enriched with geospatial information by proposing an unsupervised multimodal graph-based approach. Unlike previous models that process semantic and geographic data separately, the authors introduce two architectures: MonoGraph, which jointly encodes semantic and geographic modalities in a single graph, and MultiGraph, which models them independently before integrating via multi-head attention mechanisms. Their learning framework uses a composite loss function combining contrastive, coherence, and alignment objectives to ensure that learned embeddings form semantically meaningful and spatially compact clusters. The proposed methods are evaluated on four real-world disaster datasets, showing superior performance over existing baselines in terms of topic quality, spatial coherence, and interpretability. Furthermore, the framework's domain-independent design allows it to be easily adapted to various types of multimodal data and a broad spectrum of downstream tasks in different application areas. This work contributes a unified, end-to-end solution for joint semantic and spatial representation learning in multimodal social media analysis, particularly beneficial for disaster response and public opinion monitoring. <div>
arXiv:2512.03063v1 Announce Type: new 
Abstract: The systematic analysis of user-generated social media content, especially when enriched with geospatial context, plays a vital role in domains such as disaster management and public opinion monitoring. Although multimodal approaches have made significant progress, most existing models remain fragmented, processing each modality separately rather than integrating them into a unified end-to-end model. To address this, we propose an unsupervised, multimodal graph-based methodology that jointly embeds semantic and geographic information into a shared representation space. The proposed methodology comprises two architectural paradigms: a mono graph (MonoGrah) model that jointly encodes both modalities, and a multi graph (MultiGraph) model that separately models semantic and geographic relationships and subsequently integrates them through multi-head attention mechanisms. A composite loss, combining contrastive, coherence, and alignment objectives, guides the learning process to produce semantically coherent and spatially compact clusters. Experiments on four real-world disaster datasets demonstrate that our models consistently outperform existing baselines in topic quality, spatial coherence, and interpretability. Inherently domain-independent, the framework can be readily extended to diverse forms of multimodal data and a wide range of downstream analysis tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demographic Inference from Social Media Data with Multimodal Foundation Models: Strategies, Evaluation, and Benchmarking</title>
<link>https://arxiv.org/abs/2512.03064</link>
<guid>https://arxiv.org/abs/2512.03064</guid>
<content:encoded><![CDATA[
<div> Keywords: demographic inference, multimodal model, GPT-5, social media, fairness<br /><br />Summary:<br /><br />This study addresses the challenge of accurately inferring demographic attributes such as age, gender, and race from social media profiles. Existing methods often rely on a single data modality and are limited in scope, reducing their robustness and generalizability. Leveraging GPT-5, a cutting-edge multimodal foundation model, the research designs a progressive framework that incrementally integrates multiple information sources including usernames, profile descriptions, tweets, and profile images. The approach evaluates how each modality contributes to improving inference accuracy. Using a dataset of 263 publicly available X (formerly Twitter) users, the framework demonstrates consistent accuracy gains with the inclusion of both textual and visual cues. Specifically, GPT-5 achieves 90% accuracy for age prediction, 98% for gender, and 85% for race, outperforming prior models with comparable inputs. The study highlights the advantage of large multimodal models in capturing complex demographic signals with minimal task-specific training. Additionally, the research emphasizes transparency and interpretability in the multimodal reasoning process. Overall, these findings suggest that integrating multiple modalities via advanced foundation models can markedly enhance the fairness, scalability, and accuracy of demographic inference in social media analytics. <div>
arXiv:2512.03064v1 Announce Type: new 
Abstract: Demographic inference plays a crucial role in understanding the representativeness and equity of social media-based research. However, existing methods typically rely on a single modality, such as text, image, or network, and are limited to predicting one or two demographic attributes, constraining their generalizability and robustness across populations. This study leverages GPT-5, a state-of-the-art multimodal foundation model, to infer age, gender, and race from social media profiles. Using a dataset of 263 publicly available X (formerly Twitter) users, we design a progressive multimodal framework that incrementally incorporates usernames, profile descriptions, tweets, and profile images to examine how each information source contributes to inference accuracy. Results show a consistent improvement across all conditions, with the inclusion of textual and visual cues substantially enhancing performance. GPT-5 achieves an overall accuracy of 0.90 for age, 0.98 for gender, and 0.85 for race, outperforming existing models under equivalent inputs. These findings demonstrate the potential of large multimodal foundation models to capture complex, cross-modal demographic cues with minimal task-specific training. The study further highlights a transparent, interpretable approach to multimodal reasoning that advances the accuracy, fairness, and scalability of demographic inference in social data analytics.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Potential to Escape Filter Bubbles: A Behavior-Aware Measure via Contrastive Simulation</title>
<link>https://arxiv.org/abs/2512.03067</link>
<guid>https://arxiv.org/abs/2512.03067</guid>
<content:encoded><![CDATA[
<div> Keywords: recommendation systems, filter bubbles, Bubble Escape Potential, behavior-aware measure, preference modeling<br /><br />Summary:<br />1. Recommendation systems are vital for online platforms by accurately modeling user preferences to influence user exposure.<br />2. However, these systems can inadvertently reinforce existing user preferences, causing "filter bubbles" that restrict exposure to diverse information and may lead to negative consequences like group polarization.<br />3. Existing evaluation metrics mostly measure exposure diversity but fail to distinguish between the effects of algorithmic preference modeling and genuine information confinement caused by filter bubbles.<br />4. To address this, the paper introduces Bubble Escape Potential (BEP), a novel behavior-aware metric that quantifies how easily users can escape from filter bubbles by simulating different user behavioral tendencies and comparing the resulting exposure patterns.<br />5. This approach decouples the influence of preference modeling from filter bubble effects, enabling a more accurate diagnosis of bubble severity.<br />6. Extensive experiments across various recommendation models investigate the trade-off between predictive accuracy and bubble escape potential for different user groups.<br />7. The findings reveal a previously unquantified dilemma between preference accuracy and filter bubble formation.<br />8. Surprisingly, the study also finds that mild random recommendations are largely ineffective at mitigating filter bubbles, suggesting that more principled strategies are needed for bubble alleviation. <div>
arXiv:2512.03067v1 Announce Type: new 
Abstract: Nowadays, recommendation systems have become crucial to online platforms, shaping user exposure by accurate preference modeling. However, such an exposure strategy can also reinforce users' existing preferences, leading to a notorious phenomenon named filter bubbles. Given its negative effects, such as group polarization, increasing attention has been paid to exploring reasonable measures to filter bubbles. However, most existing evaluation metrics simply measure the diversity of user exposure, failing to distinguish between algorithmic preference modeling and actual information confinement. In view of this, we introduce Bubble Escape Potential (BEP), a behavior-aware measure that quantifies how easily users can escape from filter bubbles. Specifically, BEP leverages a contrastive simulation framework that assigns different behavioral tendencies (e.g., positive vs. negative) to synthetic users and compares the induced exposure patterns. This design enables decoupling the effect of filter bubbles and preference modeling, allowing for more precise diagnosis of bubble severity. We conduct extensive experiments across multiple recommendation models to examine the relationship between predictive accuracy and bubble escape potential across different groups. To the best of our knowledge, our empirical results are the first to quantitatively validate the dilemma between preference modeling and filter bubbles. What's more, we observe a counter-intuitive phenomenon that mild random recommendations are ineffective in alleviating filter bubbles, which can offer a principled foundation for further work in this direction.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximate Bayesian Inference on Mechanisms of Network Growth and Evolution</title>
<link>https://arxiv.org/abs/2512.03092</link>
<guid>https://arxiv.org/abs/2512.03092</guid>
<content:encoded><![CDATA[
<div> Keywords: mechanistic models, network growth, mixture-of-mechanisms, conditional density estimator, graph neural network<br /><br />Summary:<br />1. The paper introduces mechanistic models to explain network growth by defining generative rules based on domain knowledge or desired network motifs. <br />2. It highlights the importance of capturing multiple mechanisms that may simultaneously influence the formation of real-world networks and understanding their relative contributions.<br />3. The authors propose a novel event-wise mixture-of-mechanisms model that assigns mechanisms to each edge formation event rather than at the node level, enabling a more detailed and dynamic explanation of network evolution.<br />4. The model uses a conditional density estimator augmented with a graph neural network to perform inference, allowing for flexible mixture modeling of network-forming mechanisms.<br />5. Through an approximate Bayesian inference approach, the method provides valid estimates for the relative weights of different mechanisms and is demonstrated on various real-world networks to reveal the underlying formation processes. <div>
arXiv:2512.03092v1 Announce Type: new 
Abstract: Mechanistic models can provide an intuitive and interpretable explanation of network growth by specifying a set of generative rules. These rules can be defined by domain knowledge about real-world mechanisms governing network growth or may be designed to facilitate the appearance of certain network motifs. In the formation of real-world networks, multiple mechanisms may be simultaneously involved; it is then important to understand the relative contribution of each of these mechanisms. In this paper, we propose the use of a conditional density estimator, augmented with a graph neural network, to perform inference on a flexible mixture of network-forming mechanisms. This event-wise mixture-of-mechanisms model assigns mechanisms to each edge formation event rather than stipulating node-level mechanisms, thus allowing for an explanation of the network generation process, as well as the dynamic evolution of the network over time. We demonstrate that our approximate Bayesian approach yields valid inferences for the relative weights of the mechanisms in our model, and we utilize this method to investigate the mechanisms behind the formation of a variety of real-world networks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community Quality and Influence Maximization: An Empirical Study</title>
<link>https://arxiv.org/abs/2512.03095</link>
<guid>https://arxiv.org/abs/2512.03095</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence Maximization, Independent Cascade Model, Community Detection, Hierarchical Clustering, Information Diffusion

<br /><br />Summary:  
This paper investigates the impact of community quality on influence maximization in social networks, focusing on the Independent Cascade model. It extends a previously proposed community detection method called α-Hierarchical Clustering to guide seed node selection for influence spread. The study compares two approaches: one using α-Hierarchical Clustering to produce higher-quality communities, and another relying on standard Hierarchical Clustering that yields communities of lower quality. Both approaches employ the same seed selection criteria but differ in community quality. Extensive experiments on various real-world datasets reveal that using higher-quality community structures significantly enhances the spread of influence, especially when the propagation probability is low. The findings highlight the critical role of accurate community detection in improving the effectiveness of seed selection strategies for viral marketing, epidemiology, product recommendations, and other applications relying on influence diffusion. Overall, the research establishes that community quality is a key factor in optimizing influence maximization under the Independent Cascade framework in complex networks. <div>
arXiv:2512.03095v1 Announce Type: new 
Abstract: Influence maximization in social networks plays a vital role in applications such as viral marketing, epidemiology, product recommendation, opinion mining, and counter-terrorism. A common approach identifies seed nodes by first detecting disjoint communities and subsequently selecting representative nodes from these communities. However, whether the quality of detected communities consistently affects the spread of influence under the Independent Cascade model remains unclear. This paper addresses this question by extending a previously proposed disjoint community detection method, termed $\alpha$-Hierarchical Clustering, to the influence maximization problem under the Independent Cascade model. The proposed method is compared with an alternative approach that employs the same seed selection criteria but relies on communities of lower quality obtained through standard Hierarchical Clustering. The former is referred to as Hierarchical Clustering-based Influence Maximization, while the latter, which leverages higher-quality community structures to guide seed selection, is termed $\alpha$-Hierarchical Clustering-based Influence Maximization. Extensive experiments are performed on multiple real-world datasets to assess the effectiveness of both methods. The results demonstrate that higher-quality community structures substantially improve information diffusion under the Independent Cascade model, particularly when the propagation probability is low. These findings underscore the critical importance of community quality in guiding effective seed selection for influence maximization in complex networks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Public Sentiment Analysis of Traffic Management Policies in Knoxville: A Social Media Driven Study</title>
<link>https://arxiv.org/abs/2512.03103</link>
<guid>https://arxiv.org/abs/2512.03103</guid>
<content:encoded><![CDATA[
<div> Keywords: public sentiment, traffic management, social media analysis, sentiment analysis, topic modeling<br /><br />Summary:<br /><br />This study analyzes public sentiment regarding traffic management policies in Knoxville, Tennessee, using data from Twitter and Reddit collected between January 2022 and December 2023, totaling 7,906 posts. Sentiment analysis was conducted using VADER, and topic modeling was performed with Latent Dirichlet Allocation (LDA). The results show an overall negative sentiment toward traffic policies, with Twitter users expressing more negativity than Reddit users. Six key topics emerged from the modeling, with construction-related issues eliciting the most negative responses, while general traffic discussions tended to be more positive. Additionally, the research identified spatiotemporal patterns in sentiment, indicating that public opinion varied across different geographic areas and time periods within Knoxville. The findings underscore the value of social media as a real-time tool for monitoring public sentiment, offering transportation planners and policymakers insights to evaluate and adjust traffic management strategies effectively. This approach provides a timely and cost-effective complement to traditional surveys, helping to enhance community engagement and responsiveness in transportation planning. <div>
arXiv:2512.03103v1 Announce Type: new 
Abstract: This study presents a comprehensive analysis of public sentiment toward traffic management policies in Knoxville, Tennessee, utilizing social media data from Twitter and Reddit platforms. We collected and analyzed 7906 posts spanning January 2022 to December 2023, employing Valence Aware Dictionary and sEntiment Reasoner (VADER) for sentiment analysis and Latent Dirichlet Allocation (LDA) for topic modeling. Our findings reveal predominantly negative sentiment, with significant variations across platforms and topics. Twitter exhibited more negative sentiment compared to Reddit. Topic modeling identified six distinct themes, with construction-related topics showing the most negative sentiment while general traffic discussions were more positive. Spatiotemporal analysis revealed geographic and temporal patterns in sentiment expression. The research demonstrates social media's potential as a real-time public sentiment monitoring tool for transportation planning and policy evaluation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Associating Healthcare Teamwork with Patient Outcomes for Predictive Analysis</title>
<link>https://arxiv.org/abs/2512.03296</link>
<guid>https://arxiv.org/abs/2512.03296</guid>
<content:encoded><![CDATA[
<div> Collaboration, Healthcare Teams, Cancer Outcomes, Machine Learning, Electronic Health Records  

<br /><br />Summary:  
This paper addresses the influence of healthcare professionals' collaboration on cancer treatment outcomes, an area previously underexplored compared to clinical and demographic factors. It introduces an AI-driven method that models healthcare team interactions using electronic health record (EHR) data as networks. Machine learning techniques are then applied to these networks to detect predictive patterns related to patient survival. The models developed undergo cross-validation to ensure they are generalizable across different datasets and settings. Key network traits within these collaborations that correlate with improved patient outcomes are identified, providing interpretable insights into what aspects of teamwork matter most. Clinical experts and existing literature validate these identified collaboration features, confirming their practical relevance. The study contributes a workflow for utilizing digital traces of collaboration combined with AI to evaluate and enhance team-based healthcare delivery. Furthermore, this approach is adaptable to other complex collaborative domains beyond healthcare. Overall, the findings offer actionable insights to support data-driven interventions aimed at improving healthcare outcomes through better collaboration among providers. <div>
arXiv:2512.03296v1 Announce Type: new 
Abstract: Cancer treatment outcomes are influenced not only by clinical and demographic factors but also by the collaboration of healthcare teams. However, prior work has largely overlooked the potential role of human collaboration in shaping patient survival. This paper presents an applied AI approach to uncovering the impact of healthcare professionals' (HCPs) collaboration-captured through electronic health record (EHR) systems-on cancer patient outcomes. We model EHR-mediated HCP interactions as networks and apply machine learning techniques to detect predictive signals of patient survival embedded in these collaborations. Our models are cross validated to ensure generalizability, and we explain the predictions by identifying key network traits associated with improved outcomes. Importantly, clinical experts and literature validate the relevance of the identified crucial collaboration traits, reinforcing their potential for real-world applications. This work contributes to a practical workflow for leveraging digital traces of collaboration and AI to assess and improve team-based healthcare. The approach is potentially transferable to other domains involving complex collaboration and offers actionable insights to support data-informed interventions in healthcare delivery.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic Substitution: How Grokipedia's AI-Generated Encyclopedia Restructures Authority</title>
<link>https://arxiv.org/abs/2512.03337</link>
<guid>https://arxiv.org/abs/2512.03337</guid>
<content:encoded><![CDATA[
<div> Keywords: Wikipedia, Grokipedia, epistemic profiles, citation networks, AI-generated knowledge<br /><br />Summary:<br /><br />1. This study compares knowledge sourcing between Wikipedia, a decentralized human-curated encyclopedia, and Grokipedia, a generative AI-based encyclopedia, to investigate whether both rely on the same foundations of authority.  
2. The analysis is based on citation networks from 72 matched article pairs, encompassing nearly 60,000 sources, mapped using an 8-category epistemic classification to derive the "epistemic profiles" of each platform.  
3. Results show Wikipedia heavily depends on peer-reviewed "Academic & Scholarly" sources, while Grokipedia significantly increases reliance on "User-generated" and "Civic organization" sources.  
4. Grokipedia exhibits distinct epistemological sourcing patterns when addressing leisure topics (e.g., Sports, Entertainment) versus socially sensitive civic topics (e.g., Politics, Conflicts, Geography, Society).  
5. A unique "scaling-law for AI-generated knowledge sourcing" is identified, revealing a linear correlation between article length and citation density for Grokipedia, differing from human collective sourcing patterns.  
6. The findings suggest that Grokipedia does not simply automate knowledge production but restructures it, signaling important epistemological shifts.  
7. The study emphasizes the need for ongoing and deeper algorithm audits to understand evolving knowledge curation in AI-driven encyclopedias due to their significant societal role. <div>
arXiv:2512.03337v1 Announce Type: new 
Abstract: A quarter century ago, Wikipedia's decentralized, crowdsourced, and consensus-driven model replaced the centralized, expert-driven, and authority-based standard for encyclopedic knowledge curation. The emergence of generative AI encyclopedias, such as Grokipedia, possibly presents another potential shift in epistemic evolution. This study investigates whether AI- and human-curated encyclopedias rely on the same foundations of authority. We conducted a multi-scale comparative analysis of the citation networks from 72 matched article pairs, which cite a total of almost 60,000 sources. Using an 8-category epistemic classification, we mapped the "epistemic profiles" of the articles on each platform. Our findings reveal several quantitative and qualitative differences in how knowledge is sourced and encyclopedia claims are epistemologically justified. Grokipedia replaces Wikipedia's heavy reliance on peer-reviewed "Academic & Scholarly" work with a notable increase in "User-generated" and "Civic organization" sources. Comparative network analyses further show that Grokipedia employs very different epistemological profiles when sourcing leisure topics (such as Sports and Entertainment) and more societal sensitive civic topics (such as Politics & Conflicts, Geographical Entities, and General Knowledge & Society). Finally, we find a "scaling-law for AI-generated knowledge sourcing" that shows a linear relationship between article length and citation density, which is distinct from collective human reference sourcing. We conclude that this first implementation of an LLM-based encyclopedia does not merely automate knowledge production but restructures it. Given the notable changes and the important role of encyclopedias, we suggest the continuation and deepening of algorithm audits, such as the one presented here, in order to understand the ongoing epistemological shifts.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inaccessibility in Public Transit Networks</title>
<link>https://arxiv.org/abs/2512.03766</link>
<guid>https://arxiv.org/abs/2512.03766</guid>
<content:encoded><![CDATA[
<div> Accessibility, Public Transit, Network Analysis, Centrality, Socioeconomic Factors<br /><br />Summary:<br /><br />This article investigates the infrastructure-based accessibility of two major public transit systems: the London Underground and the New York City Subway. The authors construct network models where accessible stations are represented as nodes and edges signify adjacency along transit lines, enabling a structural analysis of the accessibility networks. Using network analysis tools, the study examines clustering patterns and the spatial distribution of accessible stations, revealing notable disparities within and between the two systems. Centrality measures are employed to identify key accessible hubs that play a critical role in transit accessibility. Furthermore, the research incorporates socioeconomic and tourism-related variables to evaluate how neighborhood wealth and popularity affect the presence and distribution of accessible stations. The findings demonstrate significant inequalities in accessibility that correlate with broader social and economic factors. Overall, the paper highlights the application of mathematical and network-theoretic approaches as powerful tools to better understand and potentially improve accessibility in modern transit infrastructures, addressing both infrastructure design and spatial equity issues. <div>
arXiv:2512.03766v1 Announce Type: new 
Abstract: The study of networks derived from infrastructure systems has received considerable attention, yet the accessibility of such systems, particularly within public transit networks, remains comparatively underexplored. Accessibility encompasses a broad range of considerations, from infrastructure-based features such as elevators and step-free access to spatial factors such as the geographic distribution of accessible stations. In this work, we investigate infrastructure-based accessibility in two major transit systems: the London Underground and the New York City Subway. We construct network models in which nodes represent accessible stations and edges represent adjacency along transit lines. Using tools from network analysis, we examine the structural properties of these accessibility networks, including clustering patterns and the spatial distribution of accessible nodes. We further employ centrality measures to identify stations that serve as major accessible hubs. Finally, we analyze socioeconomic and tourism-related variables to assess the influence of neighborhood wealth and popularity on the prevalence of accessible stations. Our findings highlight significant disparities in accessibility across both systems and demonstrate the utility of mathematical and network-theoretic methods in understanding and improving modern transit infrastructure.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSP: A Statistically-Principled Structural Polarization Measure</title>
<link>https://arxiv.org/abs/2512.03937</link>
<guid>https://arxiv.org/abs/2512.03937</guid>
<content:encoded><![CDATA[
<div> Keywords: polarization, social networks, diffusion, random walk, null model<br /><br />Summary:<br /><br />1. The article addresses the issue of polarization in social and information networks, which leads to echo chambers and political gridlock. 2. Existing polarization measures often misinterpret structural divisions by confusing them with random topological features, resulting in misleadingly high polarization scores on random networks. 3. The authors propose DSP (Diffusion-based Structural Polarization), a novel measure designed to correct these biases by removing reliance on predetermined 'influencers' and treating every node as a potential origin for diffusion processes. 4. They introduce a set of desirable properties for polarization measures, validated through reference network topologies with known structural characteristics, and show that DSP meets these criteria, producing near-zero scores on non-polarized networks and accurately detecting polarization in specifically structured networks. 5. Applying DSP to U.S. Congress datasets reveals a trend of increasing political polarization over recent years. By integrating a null model within its core, DSP offers a statistically grounded, reliable, and interpretable tool for diagnosing societal fragmentation in network data. <div>
arXiv:2512.03937v1 Announce Type: new 
Abstract: Social and information networks may become polarized, leading to echo chambers and political gridlock. Accurately measuring this phenomenon is a critical challenge. Existing measures often conflate genuine structural division with random topological features, yielding misleadingly high polarization scores on random networks, and failing to distinguish real-world networks from randomized null models. We introduce DSP, a Diffusion-based Structural Polarization measure designed from first principles to correct for such biases. DSP removes the arbitrary concept of 'influencers' used by the popular Random Walk Controversy (RWC) score, instead treating every node as a potential origin for a random walk. To validate our approach, we introduce a set of desirable properties for polarization measures, expressed through reference topologies with known structural properties. We show that DSP satisfies these desiderata, being near-zero for non-polarized structures such as cliques and random networks, while correctly capturing the expected polarization of reference topologies such as monochromatic-splittable networks. Our method applied to U.S. Congress datasets uncovers trends of increasing polarization in recent years. By integrating a null model into its core definition, DSP provides a reliable and interpretable diagnostic tool, highlighting the necessity of statistically-grounded metrics to analyze societal fragmentation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information</title>
<link>https://arxiv.org/abs/2512.03074</link>
<guid>https://arxiv.org/abs/2512.03074</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, fairness, sensitive attributes, regularization, bias mitigation<br /><br />Summary:<br /><br />1. This paper addresses fairness concerns in Graph Neural Networks (GNNs), which often perpetuate or amplify societal biases linked to sensitive attributes like race or gender present in node features and graph structure.<br /><br />2. Existing fairness-aware methods assume that sensitive attributes are fully available for all nodes during training, which is impractical due to privacy and data collection limitations.<br /><br />3. The authors propose a novel, model-agnostic fairness regularization framework that works when sensitive attributes are only partially available, making it applicable in more realistic scenarios.<br /><br />4. The framework integrates equal opportunity and statistical parity as differentiable regularization terms into the objective function to systematically mitigate bias.<br /><br />5. Empirical evaluations on five real-world benchmark datasets demonstrate that the proposed method effectively reduces bias on key fairness metrics while preserving competitive node classification accuracy, outperforming baseline models in the fairness-accuracy trade-off.<br /><br />6. The paper also commits to releasing datasets and source code publicly, providing resources for further research and application at https://github.com/mtavassoli/GNN-FC. <div>
arXiv:2512.03074v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating a Contact Matrix for Aged Care Settings in Australia: an agent-based model study</title>
<link>https://arxiv.org/abs/2512.03866</link>
<guid>https://arxiv.org/abs/2512.03866</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-based model, aged care, contact heterogeneity, airborne transmission, vaccination impact  

<br /><br />Summary:  
This study develops an agent-based model (ABM) to simulate interactions between staff and residents within a synthetic aged care facility, focusing on movement, task execution, and proximity-based contacts across different staff shifts and resident care levels. Contacts were defined using spatial thresholds of 1.5 m and 3 m along with cumulative duration, allowing detailed contact matrices to be generated. Simulation results revealed that residents requiring low and medium care had the highest frequency of contacts, especially with staff during morning and afternoon shifts, whereas high care residents and night staff experienced substantially fewer contacts. Poisson-based regression confirmed significant variation in contact rates by care level and shift. Temporal analysis showed clustering of high-risk contacts during structured daily routines such as communal and care activities. An integrated airborne transmission module, initiated with a single infectious staff member, indicated the highest infection risk occurred during high-contact shifts and predominantly affected medium care residents. Vaccination scenarios demonstrated reductions in predicted transmission by up to 68%, with the greatest benefits observed when both staff and residents were vaccinated. Overall, the study highlights the critical role of contact heterogeneity in aged care settings and demonstrates the value of ABMs for evaluating targeted infection control strategies in enclosed, high-risk environments. <div>
arXiv:2512.03866v1 Announce Type: cross 
Abstract: This study presents an agent-based model (ABM) developed to simulate staff and resident interactions within a synthetic aged care facility, capturing movement, task execution, and proximity-based contact events across three staff shifts and varying levels of resident care. Contacts were defined by spatial thresholds (1.5 m and 3 m) and cumulative duration, enabling the generation of detailed contact matrices. Simulation results showed that low and medium care residents experienced the highest frequency of interactions, particularly with staff on morning and afternoon shifts, while high care residents and night staff had substantially fewer contacts. Contact rates varied significantly by care level and shift, confirmed through Poisson-based regression modelling. Temporal analyses revealed clustering of high-risk contacts during structured daily routines, especially communal and care activities. An integrated airborne transmission module, seeded with a single infectious staff member, demonstrated that infection risk was highest during high-contact shifts and among medium care residents. Vaccination scenarios reduced predicted transmission by up to 68\%, with the greatest impact observed when both staff and residents were vaccinated. These findings highlight the importance of accounting for contact heterogeneity in aged care and demonstrate the utility of ABMs for evaluating targeted infection control strategies in high-risk, enclosed environments.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aggregating maximal cliques in real-world graphs</title>
<link>https://arxiv.org/abs/2512.03960</link>
<guid>https://arxiv.org/abs/2512.03960</guid>
<content:encoded><![CDATA[
<div> Keywords: maximal clique, $\rho$-dense aggregators, graph mining, sub-exponential size, bounded degeneracy<br /><br />Summary:<br /><br />1. The paper addresses the challenge of maximal clique enumeration in graphs, which is computationally difficult and often produces highly redundant outputs.  
2. It introduces the concept of $\rho$-dense aggregators, a novel approach that captures maximal clique structure by identifying a small set of clusters with edge density at least $\rho$ that together contain every maximal clique.  
3. The authors prove that for every $\rho < 1$, any graph has a $\rho$-dense aggregator of sub-exponential size $n^{O(\log_{1/\rho} n)}$, and provide an algorithm that constructs such aggregators.  
4. For graphs with bounded degeneracy—a common trait of real-world networks—the algorithm runs in near-linear time and produces aggregators of near-linear size.  
5. They also establish a matching lower bound on aggregator size, proving the theoretical tightness of their results.  
6. Empirical evaluation on real-world datasets shows the algorithm significantly outperforms state-of-the-art maximal clique enumeration methods, achieving median speedups over six times (and up to 300 times in extreme cases), while producing a more concise and informative structural summary. <div>
arXiv:2512.03960v1 Announce Type: cross 
Abstract: Maximal clique enumeration is a fundamental graph mining task, but its utility is often limited by computational intractability and highly redundant output. To address these challenges, we introduce \emph{$\rho$-dense aggregators}, a novel approach that succinctly captures maximal clique structure. Instead of listing all cliques, we identify a small collection of clusters with edge density at least $\rho$ that collectively contain every maximal clique.
  In contrast to maximal clique enumeration, we prove that for all $\rho < 1$, every graph admits a $\rho$-dense aggregator of \emph{sub-exponential} size, $n^{O(\log_{1/\rho}n)}$, and provide an algorithm achieving this bound. For graphs with bounded degeneracy, a typical characteristic of real-world networks, our algorithm runs in near-linear time and produces near-linear size aggregators. We also establish a matching lower bound on aggregator size, proving our results are essentially tight. In an empirical evaluation on real-world networks, we demonstrate significant practical benefits for the use of aggregators: our algorithm is consistently faster than the state-of-the-art clique enumeration algorithm, with median speedups over $6\times$ for $\rho=0.1$ (and over $300\times$ in an extreme case), while delivering a much more concise structural summary.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Inference of Parameters in Opinion Dynamics Models</title>
<link>https://arxiv.org/abs/2403.05358</link>
<guid>https://arxiv.org/abs/2403.05358</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-based models, variational inference, probabilistic generative ABMs, opinion dynamics, parameter estimation<br /><br />Summary:  
This article addresses the challenge of parameter estimation in agent-based models (ABMs), which are widely used to study social phenomena but often rely on expensive simulation-based heuristics. The authors propose a novel framework that employs variational inference to transform the parameter estimation task into a more efficient optimization problem, allowing direct solution through automatic differentiation. The methodology hinges on constructing probabilistic generative ABMs (PGABMs) by synthesizing probabilistic models directly from ABM rules. To handle categorical agent attributes, the approach uses the Gumbel-Softmax reparameterization, while stochastic variational inference is applied for estimating model parameters. The paper further investigates the balance between complexity and performance by comparing variational distributions of different forms, including normal distributions and normalizing flows. The method is validated on a bounded confidence opinion dynamics model with distinct agent roles (leaders and followers), achieving superior accuracy in estimating both macroscopic parameters (e.g., confidence intervals, backfire thresholds) and microscopic features (such as 200 categorical agent-level roles) compared to traditional simulation-based and MCMC methods. Overall, this technique facilitates the tuning and validation of ABMs with real-world data, enabling more data-driven insights into human social behavior. <div>
arXiv:2403.05358v2 Announce Type: replace-cross 
Abstract: Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly simulation-based heuristics. This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly.
  Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules. Then, we transform the inference process into an optimization problem suitable for automatic differentiation. In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation. Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows.
  We validate our method on a bounded confidence model with agent roles (leaders and followers). Our approach estimates both macroscopic (bounded confidence intervals and backfire thresholds) and microscopic ($200$ categorical, agent-level roles) more accurately than simulation-based and MCMC methods. Consequently, our technique enables experts to tune and validate their ABMs against real-world observations, thus providing insights into human behavior in social systems via data-driven analysis.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layered Division and Global Allocation for Community Detection in Multilayer Network</title>
<link>https://arxiv.org/abs/2512.02334</link>
<guid>https://arxiv.org/abs/2512.02334</guid>
<content:encoded><![CDATA[
<div> Community detection, multilayer networks, Transformer, modularity, unsupervised learning

<br /><br />Summary: This paper addresses the problem of community detection in multilayer networks (CDMN), which involves dividing entities connected by multiple types of relations into distinct communities. Existing neural network methods typically fuse layer-wise node representations into a global representation for community division, but these fused representations may overlook unique structural features of each layer. To overcome this, the authors propose a new paradigm called Layered Division and Global Allocation (LDGA). LDGA first performs community division separately within each layer and then combines these results into a global community allocation. The approach uses a multi-head Transformer encoder with each head capturing node structures specific to one network layer, integrated with a community-latent encoder to model community prototypes. A shared scorer generates soft assignments for each layer, while the global allocation step assigns final community labels based on the highest confidence across layers. The model is trained unsupervised using a loss function that incorporates differentiable multilayer modularity and a cluster balance regularizer. Extensive experiments on synthetic and real-world multilayer networks show that LDGA outperforms state-of-the-art methods by achieving higher detected community modularities. The authors provide the code, parameter settings, and datasets at the specified repository. <div>
arXiv:2512.02334v1 Announce Type: new 
Abstract: Community detection in multilayer networks (CDMN) is to divide a set of entities with multiple relation types into a few disjoint subsets, which has many applications in the Web, transportation, and sociology systems. Recent neural network-based solutions to the CDMN task adopt a kind of representation fusion and global division paradigm: Each node is first learned a kind of layer-wise representations which are then fused for global community division. However, even with contrastive or attentive fusion mechanisms, the fused global representations often lack the discriminative power to capture structural nuances unique to each layer. In this paper, we propose a novel paradigm for the CDMN task: Layered Division and Global Allocation (LDGA). The core idea is to first perform layer-wise group division, based on which global community allocation is next performed. Concretely, LDGA employs a multi-head Transformer as the backbone representation encoder, where each head is for encoding node structural characteristics in each network layer. We integrate the Transformer with a community-latent encoder to capture community prototypes in each layer. A shared scorer performs layered division by generating layer-wise soft assignments, while global allocation assigns each node the community label with highest confidence across all layers to form the final consensus partition. We design a loss function that couples differentiable multilayer modularity with a cluster balance regularizer to train our model in an unsupervised manner. Extensive experiments on synthetic and real-world multilayer networks demonstrate that our LDGA outperforms the state-of-the-art competitors in terms of higher detected community modularities. Our code with parameter settings and datasets are available at https://anonymous.4open.science/r/LDGA-552B/.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniCom: Towards a Unified and Cohesiveness-aware Framework for Community Search and Detection</title>
<link>https://arxiv.org/abs/2512.02460</link>
<guid>https://arxiv.org/abs/2512.02460</guid>
<content:encoded><![CDATA[
<div> Keywords: community search, community detection, knowledge transfer, universal graph learning, domain-aware specialization<br /><br />Summary:<br />1. The paper addresses limitations in current learning-based methods for community search and community detection by proposing a unified framework, UniCom, which handles both tasks simultaneously without requiring task- or dataset-specific retraining. <br />2. Existing approaches treat community search (finding the best community for a query) and community detection (partitioning the entire graph) as separate problems, which restricts generalization and applicability, especially under limited supervision. <br />3. UniCom introduces a Domain-aware Specialization (DAS) procedure that dynamically adapts to new graphs or tasks, eliminating the need for costly retraining while maintaining a compact model through a lightweight prompt-based design.<br />4. At its core, UniCom features a Universal Graph Learning (UGL) backbone that pre-trains on multiple source domains to capture transferable semantic and topological knowledge. Both DAS and UGL leverage local neighborhood signals and cohesive subgraph structures to provide consistent guidance.<br />5. The framework is extensively validated through experiments on 16 benchmark datasets and compared against 22 baselines, showing superior performance across both community search and detection tasks, especially in scenarios with scarce or no supervision, while also ensuring runtime efficiency. <div>
arXiv:2512.02460v1 Announce Type: new 
Abstract: Searching and detecting communities in real-world graphs underpins a wide range of applications. Despite the success achieved, current learning-based solutions regard community search, i.e., locating the best community for a given query, and community detection, i.e., partitioning the whole graph, as separate problems, necessitating task- and dataset-specific retraining. Such a strategy limits the applicability and generalization ability of the existing models. Additionally, these methods rely heavily on information from the target dataset, leading to suboptimal performance when supervision is limited or unavailable. To mitigate this limitation, we propose UniCom, a unified framework to solve both community search and detection tasks through knowledge transfer across multiple domains, thus alleviating the limitations of single-dataset learning. UniCom centers on a Domain-aware Specialization (DAS) procedure that adapts on the fly to unseen graphs or tasks, eliminating costly retraining while maintaining framework compactness with a lightweight prompt-based paradigm. This is empowered by a Universal Graph Learning (UGL) backbone, which distills transferable semantic and topological knowledge from multiple source domains via comprehensive pre-training. Both DAS and UGL are informed by local neighborhood signals and cohesive subgraph structures, providing consistent guidance throughout the framework. Extensive experiments on both tasks across 16 benchmark datasets and 22 baselines have been conducted to ensure a comprehensive and fair evaluation. UniCom consistently outperforms all state-of-the-art baselines across all tasks under settings with scarce or no supervision, while maintaining runtime efficiency.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying preferred routes of sharing information on social networks</title>
<link>https://arxiv.org/abs/2512.02483</link>
<guid>https://arxiv.org/abs/2512.02483</guid>
<content:encoded><![CDATA[
<div> Keywords: information dissemination, social networks, preferential models, hashtag propagation, Twitter  

<br /><br />Summary:  
This study investigates whether the spread of information on social networks is random or structured. Using real-world hashtag data, the authors find that hashtag dissemination is not random but follows specific patterns. They propose two preferential models—the global and local preferential selection models—to explain how news circulates on social media platforms. These models suggest that information flows through defined pathways in the network rather than spreading arbitrarily. Importantly, new information tends to propagate along the same routes previously used for similar content, indicating consistent information channels. The pathways through which information spreads can differ depending on the type of content being shared. To validate these findings, the authors analyze the spread of political hashtags on Twitter and observe that the dissemination paths align well with those predicted by their proposed models. Overall, the study provides evidence that information diffusion in social networks follows preferential structural patterns shaped by past propagation paths and content type, offering insights into the mechanisms behind viral news spreading online. <div>
arXiv:2512.02483v1 Announce Type: new 
Abstract: The spread of information has become faster and wider than ever with the advent of social network platforms. The question raised in this study is whether information dissemination in social networks is random or follows a discernible structure. Our results from real-world hashtag data suggest that the spread of hashtags is not random and follows specific patterns. This study proposes two preferential models to explore how news spreads on social media. Specifically, we examine global and local preferential selection models and demonstrate that information dissemination aligns with these patterns. According to these two models, information flows are distributed through specific paths on networks. This suggests that new information tends to propagate along the same paths as previous news, with the specific pathways varying depending on the type of content. Finally, an examination of the propagation of political hashtags on Twitter confirms the existence of these paths that also emerge from the two preferential models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding networks with the random walk first return time distribution</title>
<link>https://arxiv.org/abs/2512.02694</link>
<guid>https://arxiv.org/abs/2512.02694</guid>
<content:encoded><![CDATA[
<div> Keywords: First Return Time Distribution, node embedding, random walk, graph equivalence, network alignment<br /><br />Summary:  
The article proposes the first return time distribution (FRTD) of a random walk as a novel, interpretable node embedding method for complex networks. The FRTD assigns a probability mass function to each node, enabling the use of standard discrete distribution metrics to define distances between nodes. Firstly, the authors demonstrate that FRTDs contain strictly more information than eigenvalue spectra, though they are not fully sufficient for distinguishing all graph isomorphisms—placing FRTD equivalence between cospectrality and isomorphism in terms of graph characterization. Secondly, FRTD equivalence between nodes effectively captures structural similarity, allowing identification of nodes with shared roles or positions in the network. Thirdly, empirical experiments show that the FRTD embedding outperforms manually designed graph metrics in network alignment tasks, highlighting its practical utility. Finally, the authors find that random networks constructed to approximate the FRTD of a target network also tend to preserve other important structural features, indicating the FRTD’s ability to encapsulate key properties of complex networks. Collectively, these results position the FRTD as a mathematically principled and effective embedding technique for analyzing and comparing complex networks. <div>
arXiv:2512.02694v1 Announce Type: new 
Abstract: We propose the first return time distribution (FRTD) of a random walk as an interpretable and mathematically grounded node embedding. The FRTD assigns a probability mass function to each node, allowing us to define a distance between any pair of nodes using standard metrics for discrete distributions. We present several arguments to motivate the FRTD embedding. First, we show that FRTDs are strictly more informative than eigenvalue spectra, yet insufficient for complete graph identification, thus placing FRTD equivalence between cospectrality and isomorphism. Second, we argue that FRTD equivalence between nodes captures structural similarity. Third, we empirically demonstrate that the FRTD embedding outperforms manually designed graph metrics in network alignment tasks. Finally, we show that random networks that approximately match the FRTD of a desired target also preserve other salient features. Together these results demonstrate the FRTD as a simple and mathematically principled embedding for complex networks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic construction of typical networks in network models</title>
<link>https://arxiv.org/abs/2512.02254</link>
<guid>https://arxiv.org/abs/2512.02254</guid>
<content:encoded><![CDATA[
<div> Keywords: grand canonical ensembles, typical state, derandomization, network models, hyperbolic graphs<br /><br />Summary:<br /><br />1. The article addresses how to evaluate the extent to which a dataset is well represented by a particular model, especially relevant in network science and statistical physics. <br />2. It focuses on identifying the "most typical state" of a given statistical ensemble that best represents real-world data and presents conditions under which this state can be defined. <br />3. The study constructs a deterministic method that converges to this most typical state in the thermodynamic limit, involving innovative rounds of derandomization procedures, including those applied to point processes—a novel approach. <br />4. The authors demonstrate their construction using deterministic hyperbolic graphs as a network model and apply this to real-world networks, finding many closely resemble the typical network from the model. <br />5. While the primary application is in network models, the results are broadly applicable to any grand canonical ensembles and their random mixtures that meet certain smoothness or “niceness” conditions, thus extending the method’s relevance beyond network science. <div>
arXiv:2512.02254v1 Announce Type: cross 
Abstract: It is often desirable to assess how well a given dataset is described by a given model. In network science, for instance, one often wants to say that a given real-world network appears to come from a particular network model. In statistical physics, the corresponding problem is about how typical a given state, representing real-world data, is in a particular statistical ensemble. One way to address this problem is to measure the distance between the data and the most typical state in the ensemble. Here, we identify the conditions that allow us to define this most typical state. These conditions hold in a wide class of grand canonical ensembles and their random mixtures. Our main contribution is a deterministic construction of a state that converges to this most typical state in the thermodynamic limit. This construction involves rounds of derandomization procedures, some of which deal with derandomizing point processes, an uncharted territory. We illustrate the construction on one particular network model, deterministic hyperbolic graphs, and its application to real-world networks, many of which we find are close to the most typical network in the model. While our main focus is on network models, our results are very general and apply to any grand canonical ensembles and their random mixtures satisfying certain niceness requirements.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstructing Large Scale Production Networks</title>
<link>https://arxiv.org/abs/2512.02362</link>
<guid>https://arxiv.org/abs/2512.02362</guid>
<content:encoded><![CDATA[
<div> Keywords: firm-to-firm networks, gravity model, input-output flows, network reconstruction, weighted networks<br /><br />Summary:<br /><br />This paper proposes an algorithm to reconstruct extensive weighted firm-to-firm networks using firm size and sectoral input-output flow data. The method involves a four-step process starting with generating a probability matrix of connections between firms via an augmented gravity model embedded in a logistic function, where firm size serves as the mass. This model accounts for both firm sizes and sectoral flows, parameterizing link probabilities accordingly. Next, a directed but unweighted graph is sampled through Bernoulli draws based on the logistic-gravity probabilities. The graph is made aperiodic by adding self-loops and irreducible by linking strongly connected components with minimal distortion to sectoral flows. Subsequently, the unweighted network is transformed into a weighted one by solving a convex quadratic programming problem that minimizes the Euclidean norm of weights while preserving observed firm sizes and sectoral flows and controlling self-loop strengths. The algorithm operates with O(N²) complexity but can be approximated in O(N) time using sector-wise binning of firm sizes. Implemented for the full US production network—with more than 5 million firms and 100 million connections—the reconstructed network reflects realistic topological features such as fat-tailed degree distributions, mild clustering, and near-zero reciprocity. The authors provide open-source code to facilitate large-scale granular production network reconstruction from public data. <div>
arXiv:2512.02362v1 Announce Type: cross 
Abstract: This paper develops an algorithm to reconstruct large weighted firm-to-firm networks using information about the size of the firms and sectoral input-output flows. Our algorithm is based on a four-step procedure. We first generate a matrix of probabilities of connections between all firms in the economy using an augmented gravity model embedded in a logistic function that takes firm size as mass. The model is parameterized to allow for the probability of a link between two firms to depend not only on their sizes but also on flows across the sectors to which they belong. We then use a Bernoulli draw to construct a directed but unweighted random graph from the probability distribution generated by the logistic-gravity function. We make the graph aperiodic by adding self-loops and irreducible by adding links between Strongly Connected Components while limiting distortions to sectoral flows. We convert the unweighted network to a weighted network by solving a convex quadratic programming problem that minimizes the Euclidean norm of the weights. The solution preserves the observed firm sizes and sectoral flows within reasonable bounds, while limiting the strength of the self-loops. Computationally, the algorithm is O(N2) in the worst case, but it can be evaluated in O(N) via sector-wise binning of firm sizes, albeit with an approximation error. We implement the algorithm to reconstruct the full US production network with more than 5 million firms and 100 million buyer-seller connections. The reconstructed network exhibits topological properties consistent with small samples of the real US buyer-seller networks, including fat-tails in degree distribution, mild clustering, and near-zero reciprocity. We provide open-source code of the algorithm to enable researchers to reconstruct large-scale granular production networks from publicly available data.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Invisible Hand: Characterizing Generative AI Adoption and its Effects on An Online Freelancing Market</title>
<link>https://arxiv.org/abs/2512.02509</link>
<guid>https://arxiv.org/abs/2512.02509</guid>
<content:encoded><![CDATA[
<div> Generative AI, Freelancing Market, ChatGPT, Job Demand, Worker Engagement  
<br /><br />Summary:  
The study analyzes the impact of generative AI (GenAI) on freelancing platforms, focusing on data from Freelancer.com that includes over 1.8 million job postings and 3.8 million users. It documents the significant growth in freelance jobs and registrations following the COVID-19 pandemic and explores how GenAI technologies, particularly ChatGPT, have influenced this trend. The research identifies the emergence of new jobs related to GenAI adoption and highlights ChatGPT's leading role in the freelancing market. It investigates the skill requirements for ChatGPT-related jobs and the specific tasks freelancers are expected to perform. The findings reveal insights into how AI technologies reshape employment opportunities, skill demands, and user behaviors on freelancing platforms. Overall, the study provides a comprehensive profile of GenAI's effects on job demand, worker engagement, and the evolving dynamics of the freelancing market in the AI era. <div>
arXiv:2512.02509v1 Announce Type: cross 
Abstract: Since the COVID-19 pandemic, freelancing platforms have experienced significant growth in both worker registrations and job postings. However, the rise of generative AI (GenAI) technologies has raised questions about how it affect the job posting in freelancer market. Despite growing discussions, there is limited empirical research on the GenAI adoption and its effect on job demand and worker engagement. We present a large-scale analysis of Freelancer.com, utilizing over 1.8 million job posts and 3.8 million users. We investigate the emergence of jobs with the adoption of GenAI and identify leading position of ChatGPT in the freelancing market. With a focus on ChatGPT related jobs, we inspect their specific skill requirements, and the tasks that workers are asked to perform. Our findings provide insights into the evolving landscape of freelancing in the age of AI, offering a comprehensive profile of GenAI's effects on employment, skills, and user behaviors in freelancing market.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation</title>
<link>https://arxiv.org/abs/2512.02920</link>
<guid>https://arxiv.org/abs/2512.02920</guid>
<content:encoded><![CDATA[
<div> Traffic Accidents, Multimodal Learning, Satellite Imagery, Graph Neural Networks, Causal Analysis  

<br /><br />Summary:  
This study develops a large multimodal dataset combining road network data and high-resolution satellite images aligned with road graph nodes, covering six U.S. states. The dataset includes nine million official traffic accident records and one million satellite images, with each node annotated by regional weather statistics and road types, and each edge annotated with traffic volume data (Average Annual Daily Traffic). The research evaluates multimodal learning methods that merge visual data from satellite images with network embeddings, demonstrating improved traffic accident prediction performance. Integrating these data modalities yields an average AUROC of 90.1%, a 3.7% increase over models using only graph structure information. The study further employs a causal analysis using a matching estimator to identify critical factors influencing accidents, revealing that accident rates increase by 24% with higher precipitation, by 22% on higher-speed roads like motorways, and by 29% due to seasonal effects, after controlling for confounders. Ablation studies underscore the importance of satellite imagery features in achieving these prediction improvements, highlighting the value of incorporating environmental visual data alongside traditional road network information for traffic safety modeling. <div>
arXiv:2512.02920v1 Announce Type: cross 
Abstract: We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\%$, which is a $3.7\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\%$ under higher precipitation, by $22\%$ on higher-speed roads such as motorways, and by $29\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A review of mechanistic and data-driven models of terrorism and radicalization</title>
<link>https://arxiv.org/abs/1903.08485</link>
<guid>https://arxiv.org/abs/1903.08485</guid>
<content:encoded><![CDATA[
<div> Keywords: radicalization, terrorism, social media, mathematical modeling, de-radicalization

<br /><br />Summary: The article addresses the increasing spread of radical ideologies leading to terrorist attacks globally, emphasizing the importance of understanding the processes behind radicalization for both cultural insight and prevention strategies. It highlights demographic studies and interviews that reveal radicalization as a progressive process influenced by age, social context, and peer interaction. The role of social media as a catalyst for faster recruitment and escalation to fanaticism is underscored. Although a full sociological explanation remains incomplete, the paper reviews quantitative methods—such as statistical mechanics, applied mathematics, and data science—that help model and analyze radicalization dynamics. Key modeling approaches include compartmental population models, continuous time age-structured frameworks, social contagion models on networks, adversarial evolutionary games simulating terrorist and counter-terrorism interactions, and point processes for spatial-temporal clustering of attacks. Additionally, the article discusses the application of machine learning techniques to publicly available terrorism data. Finally, it considers how institutional interventions might be timed and targeted for optimal effectiveness in de-radicalization efforts, contributing to both theoretical understanding and practical counter-terrorism measures. <div>
arXiv:1903.08485v3 Announce Type: replace-cross 
Abstract: The rapid spread of radical ideologies in recent years has led to a worldwide string of terrorist attacks. Understanding how extremist tendencies germinate, develop, and drive individuals to action is important from a cultural standpoint, but also to help formulate response and prevention strategies. Demographic studies, interviews with radicalized subjects, analysis of terrorist databases, reveal that the path to radicalization occurs along progressive steps, where age, social context and peer-to-peer exchange of extremist ideas play major roles. Furthermore, the advent of social media has offered new channels of communication, facilitated recruitment, and hastened the leap from mild discontent to unbridled fanaticism. While a complete sociological understanding of the processes and circumstances that lead to full-fledged extremism is still lacking, quantitative approaches, using modeling and data analyses, can offer useful insight. We review some approaches from statistical mechanics, applied mathematics, data science, that can help describe and understand radicalization and terrorist activity. Specifically, we focus on compartment models of populations harboring extremist views, continuous time models for age-structured radical populations, radicalization as social contagion processes on lattices and social networks, adversarial evolutionary games coupling terrorists and counter-terrorism agents, and point processes to study the spatiotemporal clustering of terrorist events. We also present recent applications of machine learning methods on open-source terrorism databases. Finally, we discuss the role of institutional intervention and the stages at which de-radicalization strategies might be most effective.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concentration Within Distribution: Unmasking Bitcoin's Structural Centralization Through Network Science</title>
<link>https://arxiv.org/abs/2512.00437</link>
<guid>https://arxiv.org/abs/2512.00437</guid>
<content:encoded><![CDATA[
<div> Bitcoin User Network, blockchain, centrality measures, network structure, price volatility<br /><br />Summary:  
1. The study constructs the Bitcoin User Network (BUN) using raw blockchain data up to late 2025, enabling analysis of its mesoscopic properties and temporal changes.  
2. It examines the structure of connected components and directed assortativity through four variants of Newman's coefficient, utilizing custom algorithms and a dedicated database for implementation.  
3. The research introduces direction-sensitive centrality measures grounded in PageRank and HITS to characterize structural influence within the BUN, offering a global perspective on network importance.  
4. Findings reveal a persistently unequal and increasingly core-periphery structure, highlighting concentration within the network’s user distribution despite Bitcoin’s decentralized design.  
5. Complementing network analysis, the authors study Bitcoin’s price volatility with high-frequency market data, establishing a link between network structure and market dynamics.  
Overall, the results demonstrate that the emergent Bitcoin User Network evolves toward an asymmetric mesoscopic architecture dominated by a few large connected components that constitute the critical backbone of the system. <div>
arXiv:2512.00437v1 Announce Type: new 
Abstract: We construct the Bitcoin User Network (BUN) directly from raw blockchain data up to late 2025, which allows us to explore its mesoscopic properties and trace its temporal evolution. In particular, we analyze the structure of connected components and directed assortativity through the four variants of Newman's coefficient, implemented via custom algorithms and a dedicated database. Building on this, to characterize the distribution of structural influence, we introduce direction-sensitive centrality measures based on PageRank and HITS, which provide a complementary global analysis of the BUN and reveal a persistently unequal and increasingly core-periphery structure. In addition, we complement the structural analysis with a study of Bitcoin's price volatility using high-frequency market data. Overall, our results reveal a clear pattern of concentration within distribution: although the protocol is decentralized by design, the emergent user network evolves toward an asymmetric mesoscopic structure that indicates the existence of a few large-scale connected components that function as the critical backbone of the system.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Media Data Mining of Human Behaviour during Bushfire Evacuation</title>
<link>https://arxiv.org/abs/2512.01262</link>
<guid>https://arxiv.org/abs/2512.01262</guid>
<content:encoded><![CDATA[
<div> Keywords: bushfire evacuation, social media data, data mining, emergency communication, data quality<br /><br />Summary:<br /><br />This article addresses the limitations of traditional data collection methods for bushfire evacuation behavior, such as quantitative surveys and manual observations, which often fail to provide comprehensive insights. It highlights the potential of social media data mining to fill this gap by enabling the collection of large volumes of behavioral data that are low-cost, accurate, and enriched with location and contextual information. However, the inherent challenges of social media data—such as its scattered, incomplete, and informal nature—are thoroughly discussed. The paper presents a scoping review of recent advances in relevant data mining techniques aimed at overcoming these challenges and offers guidance on how social media data can be effectively utilized for bushfire evacuation research. Several promising future applications are envisioned, including evacuation model calibration and validation, emergency communication enhancement, personalized evacuation training, and improved allocation of resources for evacuation preparedness. The study also identifies several open problems that remain unresolved, specifically issues related to data quality, bias and representativeness, geolocation accuracy, contextual understanding, crisis-specific lexicon and semantics, and the interpretation of multimodal data. These insights aim to guide future research efforts in leveraging social media data for improved bushfire evacuation strategies. <div>
arXiv:2512.01262v1 Announce Type: new 
Abstract: Traditional data sources on bushfire evacuation behaviour, such as quantitative surveys and manual observations have severe limitations. Mining social media data related to bushfire evacuations promises to close this gap by allowing the collection and processing of a large amount of behavioural data, which are low-cost, accurate, possibly including location information and rich contextual information. However, social media data have many limitations, such as being scattered, incomplete, informal, etc. Together, these limitations represent several challenges to their usefulness to better understand bushfire evacuation. To overcome these challenges and provide guidance on which and how social media data can be used, this scoping review of the literature reports on recent advances in relevant data mining techniques. In addition, future applications and open problems are discussed. We envision future applications such as evacuation model calibration and validation, emergency communication, personalised evacuation training, and resource allocation for evacuation preparedness. We identify open problems such as data quality, bias and representativeness, geolocation accuracy, contextual understanding, crisis-specific lexicon and semantics, and multimodal data interpretation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Centrality and Importance Measures in Hypergraphs: Categorization and Empirical Insights</title>
<link>https://arxiv.org/abs/2512.00107</link>
<guid>https://arxiv.org/abs/2512.00107</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraph centrality, network science, topology-based, system dynamics, taxonomy  

<br /><br />Summary:  
This paper addresses the challenge of identifying central entities and interactions within complex networks, extending the focus from traditional graphs to hypergraphs that model higher-order interactions common in biological and social systems. Recognizing the fragmented nature of existing hypergraph centrality measures, the authors conduct the first comprehensive survey covering 39 distinct centrality metrics. They introduce a novel taxonomy that categorizes these measures into three classes: structural, which are based purely on network topology; functional, which consider the influence on system dynamics; and contextual, which incorporate external or domain-specific features. To complement this taxonomy, the paper presents an experimental comparison assessing both the empirical similarity among these measures and their computational efficiency. The findings highlight the relationships and differences between centrality metrics while providing insights into their practical applicability. Finally, the authors discuss diverse applications of these hypergraph centrality measures across fields and propose a coherent research roadmap that aims to unify and advance the study of centrality in hypergraphs, thereby guiding future investigations and methodological developments in this emerging area of network science. <div>
arXiv:2512.00107v1 Announce Type: cross 
Abstract: Identifying central entities and interactions is a fundamental problem in network science. While well-studied for graphs (pairwise relations), many biological and social systems exhibit higher-order interactions best modeled by hypergraphs. This has led to a proliferation of specialized hypergraph centrality measures, but the field remains fragmented and lacks a unifying framework. This paper addresses this gap by providing the first systematic survey of 39 distinct measures. We introduce a novel taxonomy classifying them as: (1) structural (topology-based), (2) functional (impact on system dynamics), or (3) contextual (incorporating external features). We also present an experimental assessment comparing their empirical similarity and computation time. Finally, we discuss applications, establishing a coherent roadmap for future research in this area.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.00545</link>
<guid>https://arxiv.org/abs/2512.00545</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence Maximization, Fairness, Reinforcement Learning, Deep Q-Learning, Social Networks  

<br /><br />Summary:  
The article addresses the Influence Maximization (IM) problem, which focuses on selecting seed nodes within a budget to maximize the spread of influence in social networks. It highlights the issue of structural inequalities in real-world networks that lead to biased IM outcomes favoring majority groups and marginalizing minorities. To tackle this, the authors introduce a fairness-aware IM method that incorporates a maximin fairness objective, ensuring equitable influence across all community groups regardless of protected attributes. The proposed method, DQ4FairIM, leverages reinforcement learning (RL), specifically deep Q-learning, combined with Structure2Vec network embeddings to model and solve the IM problem as a Markov Decision Process (MDP). Extensive experiments on both synthetic and real-world networks demonstrate that DQ4FairIM outperforms existing fairness-agnostic and fairness-aware baselines by achieving higher fairness levels while maintaining an optimal fairness-performance balance. Additionally, the method learns generalized seeding policies capable of adapting to different problem instances without requiring retraining, such as varying network sizes and seed budgets. This work contributes a novel, practical framework for equitable influence maximization that accounts for community-level fairness in networked systems. <div>
arXiv:2512.00545v1 Announce Type: cross 
Abstract: The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the disruptiveness of conceptual papers in the field of marketing</title>
<link>https://arxiv.org/abs/2308.14724</link>
<guid>https://arxiv.org/abs/2308.14724</guid>
<content:encoded><![CDATA[
<div> Keywords: conceptual articles, marketing research, citation analysis, disruption score, empirical research<br /><br />Summary:  
This article investigates the role and impact of conceptual articles in marketing research by comparing them against empirical studies using two network-based metrics: the number of citations and the disruption score. These measures assess not only how often articles are referenced but also their ability to challenge and reshape existing knowledge within the field. Leveraging a large language model, the authors classify a comprehensive dataset of marketing journal articles into conceptual and empirical categories. The analysis demonstrates that conceptual research not only garners more citations but also exhibits a higher disruptive influence, indicating that it plays a critical role in advancing marketing theory and practice. By providing a robust quantitative assessment of conceptual versus empirical contributions, the paper reinforces the value of theoretical development in marketing scholarship. Ultimately, this work enhances our understanding of how different types of marketing articles contribute to knowledge progression through developmental approaches, underscoring the importance of fostering conceptual scholarship alongside empirical investigations. <div>
arXiv:2308.14724v3 Announce Type: replace 
Abstract: Marketing scholars have underscored the importance of conceptual articles in providing theoretical foundations and new perspectives to the field. This paper supports the argument by employing two network-based measures -- the number of citations and the disruption score -- and comparing them for conceptual and empirical research. With the aid of a large language model, we classify conceptual and empirical articles published in a substantial set of marketing journals. The findings reveal that conceptual research is not only more frequently cited but also has a greater disruptive impact on the field of marketing than empirical research. Our paper contributes to the understanding of how marketing articles advance knowledge through developmental approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reinforcement Learning Method to Factual and Counterfactual Explanations for Session-based Recommendation</title>
<link>https://arxiv.org/abs/2504.13632</link>
<guid>https://arxiv.org/abs/2504.13632</guid>
<content:encoded><![CDATA[
<div> Keywords: Session-based Recommendation, Explainability, Factual and Counterfactual Explanations, Reinforcement Learning, Contrastive Learning  

<br /><br />Summary:  
This paper addresses the challenge of explaining session-based recommendation (SR) systems, which are often perceived as "black boxes" due to their complex decision-making processes. The authors propose FCESR, a novel explanation framework that provides both factual (sufficiency) and counterfactual (necessity) explanations for SR predictions. FCESR reformulates explanation generation as a combinatorial optimization problem and employs reinforcement learning to identify the minimal yet critical sequence of items that most influence the recommendations. Beyond explainability, the framework integrates these factual and counterfactual insights into a contrastive learning paradigm, using them as high-quality positive and negative samples to fine-tune SR models. This approach not only improves the interpretability of recommendations but also significantly enhances recommendation accuracy. The paper validates FCESR's effectiveness through extensive qualitative and quantitative experiments on multiple datasets and SR architectures. Results show that the method consistently boosts both the quality of explanations and the recommendation performance, paving the way for more transparent, interpretable, and trustworthy session-based recommendation systems. <div>
arXiv:2504.13632v2 Announce Type: replace 
Abstract: Session-based Recommendation (SR) systems have recently achieved considerable success, yet their complex, "black box" nature often obscures why certain recommendations are made. Existing explanation methods struggle to pinpoint truly influential factors, as they frequently depend on static user profiles or fail to grasp the intricate dynamics within user sessions. In response, we introduce FCESR (Factual and Counterfactual Explanations for Session-based Recommendation), a novel framework designed to illuminate SR model predictions by emphasizing both the sufficiency (factual) and necessity (counterfactual) of recommended items. By recasting explanation generation as a combinatorial optimization challenge and leveraging reinforcement learning, our method uncovers the minimal yet critical sequence of items influencing recommendations. Moreover, recognizing the intrinsic value of robust explanations, we innovatively utilize these factual and counterfactual insights within a contrastive learning paradigm, employing them as high-quality positive and negative samples to fine-tune and significantly enhance SR accuracy. Extensive qualitative and quantitative evaluations across diverse datasets and multiple SR architectures confirm that our framework not only boosts recommendation accuracy but also markedly elevates the quality and interpretability of explanations, thereby paving the way for more transparent and trustworthy recommendation systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Artificial Benchmark for Community Detection with Outliers and Overlapping Communities (ABCD+$o^2$)</title>
<link>https://arxiv.org/abs/2506.05486</link>
<guid>https://arxiv.org/abs/2506.05486</guid>
<content:encoded><![CDATA[
<div> Keywords: ABCD model, community detection, power-law distribution, overlapping communities, random graph model<br /><br />Summary:<br /><br />1. The paper discusses the Artificial Benchmark for Community Detection (ABCD) graph, which is a random graph model designed to incorporate community structure with power-law distributions governing both node degrees and community sizes. <br /><br />2. This model offers an alternative to the widely used LFR model by being computationally faster, easier to interpret, and more amenable to analytical study.<br /><br />3. The authors extend the ABCD model to include outliers, resulting in the ABCD+$o$ variant, which introduces nodes that do not necessarily belong to any community, adding realism to the benchmark graphs.<br /><br />4. Further, they propose a new variant called ABCD+$o^2$ that allows for overlapping communities, meaning that nodes can be members of multiple communities simultaneously, reflecting more complex network structures.<br /><br />5. These advancements improve the versatility and applicability of synthetic benchmark graphs for evaluating community detection algorithms under various realistic network scenarios. <div>
arXiv:2506.05486v2 Announce Type: replace 
Abstract: The Artificial Benchmark for Community Detection (ABCD) graph is a random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs similar to the well-known LFR model but it is faster, more interpretable, and can be investigated analytically. In this paper, we use the underlying ingredients of the ABCD model, and its generalization to include outliers (ABCD+$o$), and introduce another variant that allows for overlapping communities, ABCD+$o^2$.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Global Measures of Network Centralization: Axiomatic and Numerical Assessments</title>
<link>https://arxiv.org/abs/2511.21849</link>
<guid>https://arxiv.org/abs/2511.21849</guid>
<content:encoded><![CDATA[
<div> Keywords: network centralization, centrality measures, axiomatic framework, normalized metrics, real-world networks<br /><br />Summary:<br /><br />1. This article tackles the challenge of identifying network centralization measures that are both theoretically sound and empirically reliable across diverse network types.<br /><br />2. Eleven existing centralization measures are normalized and systematically evaluated using a dual approach: an axiomatic framework and numerical simulations.<br /><br />3. The axiomatic framework tests each measure against six fundamental postulates of centralization to ensure minimal theoretical consistency.<br /><br />4. Numerical simulations assess how these normalized measures behave across different random graph models, revealing substantial differences between measures despite their shared goal.<br /><br />5. Among the evaluated metrics, three stand out as particularly suitable: normalized betweenness centralization, normalized closeness centralization, and normalized degree centralization.<br /><br />6. The three measures capture complementary aspects of centralization—betweenness centralization emphasizes path-based dominance, closeness centralization reflects network accessibility and efficiency, and degree centralization measures the concentration of hubs.<br /><br />7. Application to a broad set of real-world networks demonstrates meaningful variations in hub organization that each metric highlights differently.<br /><br />8. Using these three measures jointly provides a sensitive and comprehensive assessment of network centralization, offering advantages over any single measure.<br /><br />9. The study’s framework clarifies conceptual differences among centralization measures and provides practical guidance for selecting reliable metrics in network analysis. <div>
arXiv:2511.21849v1 Announce Type: new 
Abstract: Network centralization, driven by hub nodes, impacts communication efficiency, structural integration, and dynamic processes such as diffusion and synchronization. Although numerous centralization measures exist, a major challenge lies in determining measures that are both theoretically sound and empirically reliable across different network contexts. To resolve this challenge, we normalize 11 measures of network centralization and assess them systematically using an axiomatic framework and numerical simulations. Our axiomatic assessment tests each measure against the six postulates of centralization, ensuring consistency with minimal theoretical requirements. In addition, our numerical assessment examines the behavior of normalized centralization measures over different random graphs. Our results indicate major differences among the measures, despite their common aim of quantifying centralization. Together, our assessments point to the relative suitability of three measures: normalized betweenness centralization, normalized closeness centralization, and normalized degree centralization. Applying these three measures to real-world networks from diverse domains reveals meaningful variation in the organization of the networks with respect to hubs. Normalized betweenness centralization highlights path-based dominance; normalized closeness centralization reflects accessibility and efficiency of reach; and normalized degree centralization captures degree-based hub concentration. When used jointly, the three measures demonstrate the required sensitivity to varying levels of centralization and provide complementary aspects of network centralization that no single measure can offer alone. Our dual evaluation framework clarifies conceptual differences among existing measures and offers practical guidance for selecting reliable centralization metrics.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WET -- Weighted Ensemble Transformer for Identifying Psychiatric Stressors Related to Suicide X (formerly Twitter)</title>
<link>https://arxiv.org/abs/2511.22082</link>
<guid>https://arxiv.org/abs/2511.22082</guid>
<content:encoded><![CDATA[
<div> Keywords: Suicide detection, Transformer, Social media, Psychological stressors, Emotional features<br /><br />Summary:<br />1. Suicide is a leading cause of death worldwide, especially among young people, with psychological stressors being key triggers for suicidal thoughts and behaviors.<br />2. Social media platforms like X (formerly Twitter) serve as critical spaces where individuals reveal emotional distress and suicide-related conditions, providing opportunities for early intervention.<br />3. Current detection methods mainly use raw text from posts and often overlook valuable emotional and contextual information found in user metadata.<br />4. The paper introduces a novel Weighted Ensemble Transformer (WET), a dual branch deep learning model that combines Transformer-based semantic representations with engineered feature vectors capturing sentiment, subjectivity, polarity, and engagement metrics.<br />5. The authors collected and annotated 125,754 English tweets focused on psychological stressors related to suicide and tested WET against traditional ML models and other deep learning baselines.<br />6. Results demonstrate that WET achieves state-of-the-art binary classification accuracy of 0.9901, showing that integrating semantic signals with auxiliary emotional and behavioral features significantly enhances suicidality detection performance. <div>
arXiv:2511.22082v1 Announce Type: new 
Abstract: Suicide remains one of the leading causes of death worldwide, particularly among young people, and psychological stressors are consistently identified as proximal drivers of suicidal ideation and behavior. In recent years, social media platforms such as X have become critical environments where individuals openly disclose emotional distress and conditions associated with suicidality, creating new opportunities for early detection and intervention. Existing approaches, however, predominantly rely on raw textual content and often neglect auxiliary emotional and contextual signals embedded in user metadata. To address this limitation, we propose a Weighted Ensemble Transformer (WET), a dual branch deep learning architecture designed to identify psychiatric stressors associated with suicide in X posts. Our model integrates semantic representations extracted through Transformer encoders with an engineered feature vector capturing sentiment, subjectivity, polarity, and user engagement characteristics. We collected, filtered, and annotated 125,754 English tweets for suicide-related psychological stressors and evaluated the proposed model under two configurations. Extensive comparative experiments against traditional machine learning methods, advanced recurrent networks, and transformer baselines demonstrate that WET achieves state-of-the-art performance, reaching 0.9901 accuracy in binary classification. These findings show that hybridizing deep semantic signals with auxiliary emotional and behavioral features substantially improves suicidality detection accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HW-GNN: Homophily-Aware Gaussian-Window Constrained Graph Spectral Network for Social Network Bot Detection</title>
<link>https://arxiv.org/abs/2511.22493</link>
<guid>https://arxiv.org/abs/2511.22493</guid>
<content:encoded><![CDATA[
<div> Social bots, Graph Neural Networks, spectral GNN, homophily, Gaussian window<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting social bots on online platforms, which are responsible for spreading misinformation and manipulation. It focuses on Graph Neural Networks (GNNs), especially spectral-based methods, which utilize structural and attribute features of social networks but face two main issues: their broad-spectrum fitting reduces emphasis on bot-specific spectral features, and they underutilize domain knowledge such as the relationship between homophily and frequency features. To overcome these limitations, the authors propose HW-GNN, a homophily-aware graph spectral network that incorporates Gaussian window constraints. This model introduces learnable Gaussian windows designed to selectively amplify bot-related spectral features, enhancing detection precision. Furthermore, HW-GNN integrates domain knowledge by linking homophily ratios to frequency characteristics within the Gaussian window optimization process, making the approach more adaptive to social network properties. Extensive experiments on multiple benchmark datasets show that HW-GNN outperforms existing spectral GNN methods, achieving an average 4.3% improvement in F1-score for social bot detection. The method also demonstrates strong compatibility as a plug-in module for existing spectral GNN frameworks, indicating its flexibility and practical utility in improving cybersecurity against social bots. <div>
arXiv:2511.22493v1 Announce Type: new 
Abstract: Social bots are increasingly polluting online platforms by spreading misinformation and engaging in coordinated manipulation, posing severe threats to cybersecurity. Graph Neural Networks (GNNs) have become mainstream for social bot detection due to their ability to integrate structural and attribute features, with spectral-based approaches demonstrating particular efficacy due to discriminative patterns in the spectral domain. However, current spectral GNN methods face two limitations: (1) their broad-spectrum fitting mechanisms degrade the focus on bot-specific spectral features, and (2) certain domain knowledge valuable for bot detection, e.g., low homophily correlates with high-frequency features, has not been fully incorporated into existing methods.
  To address these challenges, we propose HW-GNN, a novel homophily-aware graph spectral network with Gaussian window constraints. Our framework introduces two key innovations: (i) a Gaussian-window constrained spectral network that employs learnable Gaussian windows to highlight bot-related spectral features, and (ii) a homophily-aware adaptation mechanism that injects domain knowledge between homophily ratios and frequency features into the Gaussian window optimization process. Through extensive experimentation on multiple benchmark datasets, we demonstrate that HW-GNN achieves state-of-the-art bot detection performance, outperforming existing methods with an average improvement of 4.3% in F1-score, while exhibiting strong plug-in compatibility with existing spectral GNNs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community Detection in Multilayer Networks: Challenges, Opportunities and Applications</title>
<link>https://arxiv.org/abs/2511.23247</link>
<guid>https://arxiv.org/abs/2511.23247</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, multilayer networks, network analysis, systematic review, future directions<br /><br />Summary:  
This paper focuses on the field of community detection within multilayer networks—networks featuring multiple types of interactions. Despite the advancements in detecting communities in single-layer networks, multilayer networks present unique challenges and remain underexplored, offering significant potential for further research. The authors highlight the various types of multilayer networks that exist and review the current community detection techniques specifically designed or adapted for these complex structures. They emphasize the practical applications of these methods across different disciplines, demonstrating their relevance to real-world scenarios. The paper also addresses the core challenges faced by researchers, such as handling the increased complexity and integrating information across layers. Additionally, the authors stress the importance of a systematic review to consolidate knowledge, assess progress, and identify gaps in the current literature. Finally, the paper suggests promising directions for future work aimed at refining existing techniques and developing new approaches that enhance the effectiveness of community detection in multilayer settings. This work serves as both a comprehensive overview and a call to action for innovation in this dynamic and evolving area of network science. <div>
arXiv:2511.23247v1 Announce Type: new 
Abstract: Community detection is a fascinating and rapidly evolving field, but when it comes to analyzing networks with multiple types of interactions, referred to as multilayer networks, there is still a lot of untapped potential. Despite the wide array of methods developed to identify community structures in such networks, this area remains underexplored, leaving plenty of room for innovation. A systematic review of recent advancements is essential to understand where the field stands and where it is headed. While significant strides have been made across various disciplines, many questions remain unanswered, and new opportunities are waiting to be uncovered. In this paper, we explore the different types of multilayer networks, community detection techniques, and how they are applied in real world scenarios. We also dive into the key challenges researchers face and suggest potential directions for future work, aiming to refine community detection techniques and boost their effectiveness in multilayer networks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeXposure: A Dataset and Benchmarks for Inter-protocol Credit Exposure in Decentralized Financial Networks</title>
<link>https://arxiv.org/abs/2511.22314</link>
<guid>https://arxiv.org/abs/2511.22314</guid>
<content:encoded><![CDATA[
<div> Keywords: DeXposure dataset, inter-protocol credit exposure, DeFi networks, temporal graph neural networks, financial risk monitoring

<br /><br />Summary:  
The paper introduces the DeXposure dataset, a pioneering large-scale resource capturing inter-protocol credit exposure in decentralized financial (DeFi) networks, spanning 43.7 million entries across 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens from 2020 to 2025. It defines a novel metric called value-linked credit exposure, which infers financial dependency relationships between protocols based on changes in Total Value Locked (TVL). The authors develop a token-to-protocol model employing DefiLlama metadata to estimate this exposure from token stock dynamics reported by protocols. Leveraging this dataset, the study presents three machine learning benchmarks: (1) graph clustering for measuring global network structure and its evolution, (2) vector autoregression modeling of sector-level credit exposure during major market shocks such as Terra and FTX collapses, and (3) temporal graph neural networks for dynamic link prediction. Key observations include rapid growth in network volume, concentration towards key protocols, declining network density, and distinct patterns of shock propagation across lending, trading, and asset management sectors. The publicly released dataset and accompanying code are aimed at fostering further research and practical applications in machine learning, financial risk monitoring, policy analysis, and DeFi market modeling while contributing valuable benchmarks for advancing graph-based and time-series analysis techniques. <div>
arXiv:2511.22314v1 Announce Type: cross 
Abstract: We curate the DeXposure dataset, the first large-scale dataset for inter-protocol credit exposure in decentralized financial networks, covering global markets of 43.7 million entries across 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens, from 2020 to 2025. A new measure, value-linked credit exposure between protocols, is defined as the inferred financial dependency relationships derived from changes in Total Value Locked (TVL). We develop a token-to-protocol model using DefiLlama metadata to infer inter-protocol credit exposure from the token's stock dynamics, as reported by the protocols. Based on the curated dataset, we develop three benchmarks for machine learning research with financial applications: (1) graph clustering for global network measurement, tracking the structural evolution of credit exposure networks, (2) vector autoregression for sector-level credit exposure dynamics during major shocks (Terra and FTX), and (3) temporal graph neural networks for dynamic link prediction on temporal graphs. From the analysis, we observe (1) a rapid growth of network volume, (2) a trend of concentration to key protocols, (3) a decline of network density (the ratio of actual connections to possible connections), and (4) distinct shock propagation across sectors, such as lending platforms, trading exchanges, and asset management protocols. The DeXposure dataset and code have been released publicly. We envision they will help with research and practice in machine learning as well as financial risk monitoring, policy analysis, DeFi market modeling, amongst others. The dataset also contributes to machine learning research by offering benchmarks for graph clustering, vector autoregression, and temporal graph analysis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A lasso-alternative to Dijkstra's algorithm for identifying short paths in networks</title>
<link>https://arxiv.org/abs/2511.22745</link>
<guid>https://arxiv.org/abs/2511.22745</guid>
<content:encoded><![CDATA[
<div> Keywords: shortest path, lasso regression, LARS algorithm, bi-directional Dijkstra, ADMM<br /><br />Summary:<br /><br />1. This paper revisits the classical problem of finding the shortest path between two specified vertices in a graph and reframes it as an \(\ell_1\)-regularized regression problem, specifically utilizing the Least Absolute Shrinkage and Selection Operator (lasso).<br /><br />2. The authors explore a numerical implementation of this lasso formulation using the Least Angle Regression (LARS) algorithm and establish connections between this approach and the traditional bi-directional Dijkstra algorithm.<br /><br />3. One notable advantage of the lasso approach is its compatibility with the Alternating Direction Method of Multipliers (ADMM), which provides an effective mechanism to solve the shortest path problem while encouraging sparsity in the path solution.<br /><br />4. The formulation also enables relatively efficient updates in response to topological changes in the graph, which is beneficial for dynamic graphs where edges or vertices may change over time.<br /><br />5. Overall, the work proposes a novel perspective combining optimization and classical graph algorithms, offering a flexible framework that potentially enhances computational methods for shortest path problems, especially in environments where regularization and adaptivity are desirable. <div>
arXiv:2511.22745v1 Announce Type: cross 
Abstract: We revisit the problem of finding the shortest path between two selected vertices of a graph and formulate this as an $\ell_1$-regularized regression -- Least Absolute Shrinkage and Selection Operator (lasso). We draw connections between a numerical implementation of this lasso-formulation, using the so-called LARS algorithm, and a more established algorithm known as the bi-directional Dijkstra. Appealing features of our formulation include the applicability of the Alternating Direction of Multiplier Method (ADMM) to the problem to identify short paths, and a relatively efficient update to topological changes.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The impact of anticonformity on the diffusion of innovation -- insights from the q-voter model</title>
<link>https://arxiv.org/abs/2511.23061</link>
<guid>https://arxiv.org/abs/2511.23061</guid>
<content:encoded><![CDATA[
<div> Keywords: anticonformity, diffusion of innovation, q-voter model, agent-based modeling, hysteresis<br /><br />Summary:<br /><br />This article explores the role of anticonformity—deliberate opposition to group influence—as a constructive behavior in social dynamics, distinct from conformity and independence. Recent experimental and modeling studies suggest that anticonformity can strategically emerge when rewards are anticipated, helping depolarize groups and prevent social hysteresis. To further investigate its effects on innovation spread, the authors extend the q-voter model, which simulates innovation diffusion, by incorporating anticonformity alongside independence and conformity behaviors. Using a mean-field approximation, they analyze how these tendencies influence adoption rates. The findings reveal that anticonformists accelerate early adoption phases and enable successful diffusion where it might normally fail. The model identifies two stable adoption states separated by an unstable one, indicating hysteresis and the presence of a critical mass effect. Furthermore, increasing the level of independent behavior in the population lowers the anticonformity threshold required for widespread adoption. Overall, the study demonstrates that anticonformist behavior can facilitate innovation diffusion, underscoring its potential value for decision-makers and policy designs aimed at promoting the uptake of new ideas or technologies. <div>
arXiv:2511.23061v1 Announce Type: cross 
Abstract: Anticonformity, behaving in deliberate opposition to the group of influence, has long been recognized as a distinct social response, differing both from conformity and from independence. While often treated as a source of noise or contrarianism, anticonformity can play a constructive role in social dynamics by counterbalancing majority pressure and influencing collective outcomes. Recently, it was shown in laboratory experiments that evaluation may induce strategic anticonformity when rewards are anticipated. Moreover, using agent-based modeling, it has been demonstrated that anticonformity can depolarize highly polarized social groups and prevent social hysteresis. These findings encouraged us to extend the q-voter model with asymmetric independence, an agent-based model of the diffusion of innovation, by introducing anticonformity, so that agents can act independently, follow the group, or oppose it. Using a mean-field approximation (MFA), we investigate how these behavioral tendencies influence the diffusion of innovation. Our results show that anticonformists can accelerate early adoption and enable successful diffusion even in cases where diffusion would otherwise fail. The model exhibits two stable adoption levels separated by an unstable branch, giving rise to hysteresis and a critical mass effect. We also demonstrate that increasing independence lowers the threshold of anticonformity needed for widespread adoption. These results highlight how anticonformist behavior can facilitate innovation diffusion, with practical implications for decision-makers and policy design.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Improved and Generalised Analysis for Spectral Clustering</title>
<link>https://arxiv.org/abs/2511.23261</link>
<guid>https://arxiv.org/abs/2511.23261</guid>
<content:encoded><![CDATA[
<div> Spectral Clustering, Graph Partitioning, Eigenvalues, Hermitian Matrices, Directed Graphs<br /><br />Summary:<br /><br />This article revisits the theoretical performance of Spectral Clustering, a classical graph partitioning algorithm based on eigenvectors of matrix representations of graphs. The authors demonstrate that Spectral Clustering is effective when the smallest eigenvalues of the chosen matrix appear in well-separated groups, a condition often found in graphs with hierarchical cluster structures at multiple scales. This insight extends beyond previous theoretical frameworks which did not capture such multi-scale cluster regimes. Furthermore, the work generalizes the matrix representations used in Spectral Clustering to include Hermitian representations of directed graphs (digraphs). In this setting, the method can successfully identify partitions in digraphs where inter-cluster edges are predominantly oriented in the same direction. Such directed clustering has practical applications, such as analyzing trophic levels in ecological networks. Finally, the authors validate their theoretical findings through experiments on both synthetic and real-world datasets, showing that their characterizations accurately predict the performance of Spectral Clustering in these contexts. <div>
arXiv:2511.23261v1 Announce Type: cross 
Abstract: We revisit the theoretical performances of Spectral Clustering, a classical algorithm for graph partitioning that relies on the eigenvectors of a matrix representation of the graph. Informally, we show that Spectral Clustering works well as long as the smallest eigenvalues appear in groups well separated from the rest of the matrix representation's spectrum. This arises, for example, whenever there exists a hierarchy of clusters at different scales, a regime not captured by previous analyses. Our results are very general and can be applied beyond the traditional graph Laplacian. In particular, we study Hermitian representations of digraphs and show Spectral Clustering can recover partitions where edges between clusters are oriented mostly in the same direction. This has applications in, for example, the analysis of trophic levels in ecological networks. We demonstrate that our results accurately predict the performances of Spectral Clustering on synthetic and real-world data sets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Startup-VC Fund Matches with Structural Embeddings and Temporal Investment Data</title>
<link>https://arxiv.org/abs/2511.23364</link>
<guid>https://arxiv.org/abs/2511.23364</guid>
<content:encoded><![CDATA[
<div> Keywords: startup prediction, venture capital, binary classification, Node2Vec, LSTM<br /><br />Summary:<br /><br />This study introduces a novel method to predict the likelihood that a venture capital (VC) fund will invest in a specific startup. Unlike typical recommendation systems that rank multiple options, the approach is formulated as a binary classification problem focusing on each unique fund-startup pair. Startups are represented through a comprehensive feature integration that includes textual, numerical, and structural information. The structural context of startups within networks is captured using Node2Vec embeddings. To effectively combine these diverse features, a multihead attention mechanism is employed. The investment histories of VC funds are modeled as sequential data using LSTM networks, allowing the method to account for temporal dynamics in investment behavior. Experiments conducted on Japanese startup datasets show that this approach outperforms a static baseline model, confirming the importance of both structural network features and temporal investment patterns in improving prediction accuracy. The findings highlight the benefit of combining heterogeneous data sources and sequential modeling to better understand and predict fund-startup compatibility, providing a tailored solution for venture capital investment prediction tasks. <div>
arXiv:2511.23364v1 Announce Type: cross 
Abstract: This study proposes a method for predicting startup inclusion, estimating the probability that a venture capital fund will invest in a given startup. Unlike general recommendation systems, which typically rank multiple candidates, our approach formulates the problem as a binary classification task tailored to each fund-startup pair. Each startup is represented by integrating textual, numerical, and structural features, with Node2Vec capturing network context and multihead attention enabling feature fusion. Fund investment histories are encoded as LSTM based sequences of past investees.
  Experiments on Japanese startup data demonstrate that the proposed method achieves higher accuracy than a static baseline. The results indicate that incorporating structural features and modeling temporal investment dynamics are effective in capturing fund-startup compatibility.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Public sentiments on the fourth industrial revolution: An unsolicited public opinion poll from Twitter</title>
<link>https://arxiv.org/abs/2411.14230</link>
<guid>https://arxiv.org/abs/2411.14230</guid>
<content:encoded><![CDATA[
<div> Keywords: Fourth Industrial Revolution, public sentiment, natural language processing, echo chambers, generative AI

<br /><br />Summary:  
This paper establishes an empirical baseline of public sentiment toward Fourth Industrial Revolution (4IR) technologies across six European countries from 2006 to 2019, before the widespread adoption of generative AI. Using transformer-based natural language processing models, the study analyzes around 90,000 tweets and news articles to assess public discourse. Findings reveal a decline in neutral sentiment over time, with citizens polarizing into groups of enthusiasm and concern about technological change. This polarization varies by national context and technology domain. Approximately 6% of users are found within echo chambers defined by sentiment-aligned social networks, with privacy discussions showing the greatest vulnerability to these dynamics. The paper provides a methodologically rigorous reference point for future analyses of shifts in public opinion following the introduction of ChatGPT and other generative AI systems. These insights carry significant implications for policymakers aiming to align technological governance with social values amidst rapid advancements in AI technology. <div>
arXiv:2411.14230v2 Announce Type: replace-cross 
Abstract: This paper establishes an empirical baseline of public sentiment toward Fourth Industrial Revolution (4IR) technologies across six European countries during the period 2006--2019, prior to the widespread adoption of generative AI systems. Employing transformer-based natural language processing models on a corpus of approximately 90,000 tweets and news articles, I document a European public sphere increasingly divided in its assessment of technological change: neutral sentiment declined markedly over the study period as citizens sorted into camps of enthusiasm and concern, a pattern that manifests distinctively across national contexts and technology domains. Approximately 6\% of users inhabit echo chambers characterized by sentiment-aligned networks, with privacy discourse exhibiting the highest susceptibility to such dynamics. These findings provide a methodologically rigorous reference point for evaluating how the introduction of ChatGPT and subsequent generative AI systems has transformed public discourse on automation, employment, and technological change. The results carry implications for policymakers seeking to align technological governance with societal values in an era of rapid AI advancement.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reranking partisan animosity in algorithmic social media feeds alters affective polarization</title>
<link>https://arxiv.org/abs/2411.14652</link>
<guid>https://arxiv.org/abs/2411.14652</guid>
<content:encoded><![CDATA[
<div> AAPA, feed ranking algorithms, affective polarization, large language model, independent evaluation<br /><br />Summary:<br /><br />1. The study addresses the current limitation where social media platforms exclusively control research on feed ranking algorithms by introducing an independent method that reranks users' feeds in real-time without requiring platform cooperation. <br />2. Researchers conducted a preregistered 10-day field experiment with 1,256 participants on the social media platform X during the 2024 U.S. presidential campaign to test this approach. <br />3. Using a large language model, the experiment reranked posts containing antidemocratic attitudes and partisan animosity (AAPA) to either increase or decrease participants' exposure to such content. <br />4. Results showed that altering exposure to AAPA led to a measurable shift in out-party partisan animosity, specifically by two points on a 100-point feeling thermometer, indicating a causal relationship between AAPA content exposure and affective polarization. <br />5. The findings demonstrated no significant differences in this effect across political party lines. This study establishes a novel platform-independent tool to evaluate the impact of feed algorithms, enabling naturalistic and independent assessment of ranking interventions on social media. <div>
arXiv:2411.14652v2 Announce Type: replace-cross 
Abstract: Today, social media platforms hold sole power to study the effects of feed ranking algorithms. We developed a platform-independent method that reranks participants' feeds in real-time and used this method to conduct a preregistered 10-day field experiment with 1,256 participants on X during the 2024 U.S. presidential campaign. Our experiment used a large language model to rerank posts that expressed antidemocratic attitudes and partisan animosity (AAPA). Decreasing or increasing AAPA exposure shifted out-party partisan animosity by two points on a 100-point feeling thermometer, with no detectable differences across party lines, providing causal evidence that exposure to AAPA content alters affective polarization. This work establishes a method to study feed algorithms without requiring platform cooperation, enabling independent evaluation of ranking interventions in naturalistic settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linking Opinion Dynamics and Emotional Expression in Online Communities: A Case Study of COVID-19 Vaccination Discourse in Japan</title>
<link>https://arxiv.org/abs/2511.21078</link>
<guid>https://arxiv.org/abs/2511.21078</guid>
<content:encoded><![CDATA[
<div> COVID-19 vaccination, social media discourse, emotional expression, opinion shifts, community influence<br /><br />Summary:<br /><br />This study investigates the dynamic changes in emotions related to COVID-19 vaccination as expressed on social media. First, it analyzes the evolving collective emotions at a broad societal level, showing distinct emotional trends that correspond to the pace and progress of vaccination efforts. Second, it examines emotional differences across various communities, revealing that each community has unique emotional compositions that respond dynamically to shifts in the pandemic context. These community-level emotional changes suggest the significant influence communities exert on their members' vaccination opinions. Third, at the individual level, the study tracks users who change their opinions, identifying contrasting emotional trajectories between those adopting pro-vaccine stances and those moving toward anti-vaccine views. Fourth, the findings demonstrate that emotions play a crucial role in shaping and shifting users' vaccination opinions across all levels of analysis. Finally, this integrated approach—combining collective, community, and individual perspectives—provides comprehensive insight into the emotional underpinnings of opinion formation during a global health crisis, emphasizing the importance of understanding emotions to address vaccine hesitancy effectively. <div>
arXiv:2511.21078v1 Announce Type: new 
Abstract: Social media discourse on COVID-19 vaccination provides a valuable context for studying opinion formation, emotional expression, and social influence during a global crisis. While prior studies have examined emotional strategies within communities and the link between emotions and vaccine hesitancy, few have investigated dynamic emotion changes across collective, community, and individual levels. In this study, we address this gap by conducting an integrated analysis of the evolving collective emotions, community affiliations, and individual emotion changes associated with opinion shifts. Our results show that collective emotions exhibit distinct trends in response to vaccination progress. Emotional compositions differ across communities and respond dynamically to changing pandemic circumstances, potentially reflecting the communities' influence on users' opinions. At the individual level, users shifting to pro-vaccine opinions display markedly different emotional changes compared to those shifting toward anti-vaccine opinions. Together, these findings highlight the central role of emotions in shaping users' vaccination opinions.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multi-Order Block Structure in Higher-Order Networks</title>
<link>https://arxiv.org/abs/2511.21350</link>
<guid>https://arxiv.org/abs/2511.21350</guid>
<content:encoded><![CDATA[
<div> Keywords: higher-order networks, hypergraphs, stochastic block model, multi-order block structure, mesoscale organization  

<br /><br />Summary:  
1. Higher-order networks, which involve interactions among three or more entities, are naturally modeled as hypergraphs and are essential for representing complex real-world systems.  
2. Traditional stochastic block models (SBMs) provide a framework for describing mesoscale organization but face challenges when extended to hypergraphs due to a trade-off between expressive power and computational complexity.  
3. A simplified "single-order" model assumes a universal affinity pattern across all interaction orders, reducing complexity but potentially missing important order-dependent structural details.  
4. This work proposes a multi-order stochastic block model that allows different affinity patterns for distinct subsets of interaction orders, relaxing the universal assumption of the single-order model.  
5. The framework optimizes the partitioning of interaction orders to maximize out-of-sample hyperlink prediction accuracy, thereby better capturing the mesoscale structure.  
6. Analysis of diverse real-world hypergraphs reveals that multi-order block structures are widespread and that accounting for them improves prediction performance relative to single-order models.  
7. Moreover, the multi-order approach uncovers sharper and more interpretable mesoscale organization, highlighting the importance of order-dependent mechanisms in the mesoscale architecture of higher-order networks. <div>
arXiv:2511.21350v1 Announce Type: new 
Abstract: Higher-order networks, naturally described as hypergraphs, are essential for modeling real-world systems involving interactions among three or more entities. Stochastic block models offer a principled framework for characterizing mesoscale organization, yet their extension to hypergraphs involves a trade-off between expressive power and computational complexity. A recent simplification, a single-order model, mitigates this complexity by assuming a single affinity pattern governs interactions of all orders. This universal assumption, however, may overlook order-dependent structural details. Here, we propose a framework that relaxes this assumption by introducing a multi-order block structure, in which different affinity patterns govern distinct subsets of interaction orders. Our framework is based on a multi-order stochastic block model and searches for the optimal partition of the set of interaction orders that maximizes out-of-sample hyperlink prediction performance. Analyzing a diverse range of real-world networks, we find that multi-order block structures are prevalent. Accounting for them not only yields better predictive performance over the single-order model but also uncovers sharper, more interpretable mesoscale organization. Our findings reveal that order-dependent mechanisms are a key feature of the mesoscale organization of real-world higher-order networks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAGFN: A Text-Attributed Graph Dataset for Fake News Detection in the Age of LLMs</title>
<link>https://arxiv.org/abs/2511.21624</link>
<guid>https://arxiv.org/abs/2511.21624</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Outlier Detection, Fake News, Text-Attributed Graphs, Dataset  

<br /><br />Summary:  
The paper addresses the underexplored application of Large Language Models (LLMs) to graph outlier detection with a focus on fake news detection. It highlights a major challenge in this domain: the lack of large-scale, realistic, and well-annotated datasets suitable for benchmarking outlier detection algorithms. To overcome this limitation, the authors introduce TAGFN, a novel large-scale real-world dataset comprised of text-attributed graphs designed explicitly for outlier detection tasks associated with fake news. TAGFN serves as a comprehensive benchmark that facilitates rigorous evaluation of both traditional graph-based outlier detection methods and modern LLM-based approaches. Additionally, the dataset supports the fine-tuning of LLMs to enhance their misinformation detection capabilities, thus advancing trustworthy AI development. The dataset and accompanying code are made publicly available to promote widespread research and development efforts in this area. Overall, TAGFN is expected to be a valuable resource for the research community, enabling progress in robust graph-based outlier detection methods and empowering the detection of fake news through advanced machine learning techniques. <div>
arXiv:2511.21624v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently revolutionized machine learning on text-attributed graphs, but the application of LLMs to graph outlier detection, particularly in the context of fake news detection, remains significantly underexplored. One of the key challenges is the scarcity of large-scale, realistic, and well-annotated datasets that can serve as reliable benchmarks for outlier detection. To bridge this gap, we introduce TAGFN, a large-scale, real-world text-attributed graph dataset for outlier detection, specifically fake news detection. TAGFN enables rigorous evaluation of both traditional and LLM-based graph outlier detection methods. Furthermore, it facilitates the development of misinformation detection capabilities in LLMs through fine-tuning. We anticipate that TAGFN will be a valuable resource for the community, fostering progress in robust graph-based outlier detection and trustworthy AI. The dataset is publicly available at https://huggingface.co/datasets/kayzliu/TAGFN and our code is available at https://github.com/kayzliu/tagfn.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Network Dynamical Systems Approach to SDGs</title>
<link>https://arxiv.org/abs/2511.21091</link>
<guid>https://arxiv.org/abs/2511.21091</guid>
<content:encoded><![CDATA[
<div> Sustainable Development Goals, Networked Dynamical System, Lotka-Volterra model, Runge-Kutta 4, Quality Education<br /><br />Summary:<br /><br />1. The paper addresses the challenge of optimizing resource allocation across the United Nations' Sustainable Development Goals (SDGs) by identifying leverage points where investments yield maximum overall benefits. 2. It models the SDGs as a Networked Dynamical System (NDS), using empirical data from Our World in Data (2018) to build a weighted interaction network of 16 SDG indicators. 3. Principal Component Analysis (PCA) and multiple linear regression are applied to empirically determine the coupling weights among the indicators, capturing their interdependencies. 4. The study goes beyond static correlation by simulating the temporal evolution of SDG indicators via an extended Lotka-Volterra model, improving numerical simulation fidelity through the Runge-Kutta 4 (RK4) integration method. 5. In the case study focused on Mexico, the simulation identifies SDG 4 (Quality Education) as a critical driver with significant positive spillover effects across other development goals. 6. Sensitivity analysis is conducted to test the robustness of results, and the investigation reveals a power-law relationship between investment magnitude and system stability, providing insights into efficient development policy design. <div>
arXiv:2511.21091v1 Announce Type: cross 
Abstract: The United Nations' Sustainable Development Goals (SDGs) represent a complex, interdependent framework where progress in one area can synergistically promote or competitively inhibit progress in others. For policymakers in international development, a critical challenge is identifying "leverage points" - specific goals where limited resource allocation yields the maximum system-wide benefit. This study addresses this challenge by modeling the SDGs as a Networked Dynamical System (NDS). Using empirical data from Our World in Data (2018), we construct a weighted interaction network of 16 SDG indicators. We employ Principal Component Analysis (PCA) and multiple linear regression to derive coupling weights empirically. Unlike previous static analyses, we simulate the temporal evolution of development indicators using an extended Lotka-Volterra model. To ensure numerical stability and sophistication, we upgrade the simulation method from standard Euler integration to the Runge-Kutta 4 (RK4) method. Our simulation, applied to a case study of Mexico, reveals that SDG 4 (Quality Education) acts as a critical driver, suggesting that prioritizing education yields the most significant positive spillover effects across the development network. Furthermore, we perform sensitivity analysis and explore the power-law relationship between investment and stability.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Evolution-Based Models for Experimentation Under Interference</title>
<link>https://arxiv.org/abs/2511.21675</link>
<guid>https://arxiv.org/abs/2511.21675</guid>
<content:encoded><![CDATA[
<div> Keywords: causal effect estimation, network interference, exposure mapping, treatment randomization, spillover effects<br /><br /><br />Summary:<br />1. The paper addresses the challenge of estimating causal effects in networked systems where interventions on one unit can influence others through unobserved interaction pathways.<br />2. It argues that recovering the exact network structure is unnecessary; instead, characterizing how interactions affect outcome evolution suffices for identifying population-level causal effects.<br />3. The authors propose an evolution-based approach that examines changes in outcomes across observation rounds to compensate for missing network information.<br />4. Using an exposure-mapping framework, they provide an axiomatic characterization identifying when the empirical outcome distribution satisfies a low-dimensional recursive equation, linking this to a distributional form of difference-in-differences.<br />5. Instead of assuming parallel trends for individual units, the method relies on parallel evolution patterns across treatment scenarios to infer counterfactual trajectories.<br />6. Treatment randomization is emphasized for its dual role: eliminating confounding and implicitly sampling hidden interference channels to learn heterogeneous spillover effects consistently.<br />7. The approach is instantiated through causal message passing in dense networks and extended to more complex interference structures, such as influencer networks where few units dominate spillovers.<br />8. The paper concludes with a discussion on limitations, noting that strong temporal trends or endogenous interference may hinder causal identification using this framework. <div>
arXiv:2511.21675v1 Announce Type: cross 
Abstract: Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Illusions of Intimacy: How Emotional Dynamics Shape Human-AI Relationships</title>
<link>https://arxiv.org/abs/2505.11649</link>
<guid>https://arxiv.org/abs/2505.11649</guid>
<content:encoded><![CDATA[
<div> Keywords: AI companion chatbots, emotional bonding, intimacy formation, user affect, vulnerable users<br /><br />Summary:<br /><br />This study investigates how emotional bonding and intimacy develop in interactions between humans and AI companion chatbots, which are increasingly serving as always-available sources of empathy and support. Prior research predominantly used self-reports and interviews, leaving a gap in understanding real-time emotional dynamics during conversations. To address this, the authors analyzed over 17,000 user-shared chats with social chatbots collected from Reddit forums. The findings reveal that AI companions dynamically track and mimic users' affective states, effectively amplifying positive emotions during their interactions. This amplification occurs even when users share explicit or transgressive content, highlighting the chatbots’ role in engaging psychological processes related to intimacy and emotional bonding. Recognizing the implications of such interactions, especially for vulnerable users, the authors release an anonymized dataset of emotionally salient human-AI chat dialogues to aid future empirical research. Finally, the paper discusses the need for redesigning and governing social chatbots as high-risk systems, emphasizing safety concerns and the importance of ethical frameworks to protect users engaging in these emotionally charged interactions. <div>
arXiv:2505.11649v4 Announce Type: replace 
Abstract: AI companion chatbots, such as those offered by Replika and CharacterAI, increasingly function as always-available companions that provide empathy, validation, and support. While these systems appear to meet basic needs for connection, mounting safety concerns raise a deeper question: how do processes of emotional bonding and intimacy formation unfold in human-AI relationships? Prior research has relied largely on self-reports, interviews, or clinical assessments, leaving unclear how real-world emotional dynamics develop within ongoing human-AI conversations. We address this gap by analyzing over 17,000 user-shared chats with social chatbots from Reddit forums. We show that AI companions dynamically track and mimic user affect and amplify positive emotions, including when users share explicit or transgressive content. These dynamics suggest how chatbots can engage psychological processes involved in intimacy formation and emotional bonding. Finally, we release an anonymized dataset of emotionally salient human-AI companion dialogues to support future empirical work and discuss implications for redesigning and governing social chatbots as high-risk systems for vulnerable users.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Scale Community-Aware Network Generation</title>
<link>https://arxiv.org/abs/2511.19717</link>
<guid>https://arxiv.org/abs/2511.19717</guid>
<content:encoded><![CDATA[
<div> community detection, synthetic networks, RECCS, parallelization, scalability  

<br /><br />Summary:  
Community detection is a method used to uncover latent community structures within networks. Evaluating these algorithms is difficult due to the limited availability of labeled ground truth data in real-world networks. To overcome this, synthetic network generators are employed to produce networks with known ground truth community labels. RECCS is an existing algorithm that generates synthetic networks by preserving key characteristics of input clusters such as connectivity, minimum degree, and degree sequence distribution. This paper introduces two enhanced versions: RECCS+ and RECCS++. RECCS+ retains the original algorithm's fidelity while introducing parallelization through a process orchestrator and multithreading, enhancing performance. RECCS++ further builds on this by incorporating additional algorithmic optimizations for greater speed improvements. Experimental results show that RECCS+ achieves up to a 49x speedup, and RECCS++ up to a 139x speedup compared to the original RECCS. Although RECCS++ introduces a modest accuracy tradeoff, it enables scaling to extremely large networks, handling more than 100 million nodes and nearly 2 billion edges. These advancements make synthetic network generation for community detection evaluation significantly faster and scalable to much larger data sets. <div>
arXiv:2511.19717v1 Announce Type: new 
Abstract: Community detection, or network clustering, is used to identify latent community structure in networks. Due to the scarcity of labeled ground truth in real-world networks, evaluating these algorithms poses significant challenges. To address this, researchers use synthetic network generators that produce networks with ground-truth community labels. RECCS is one such algorithm that takes a network and its clustering as input and generates a synthetic network through a modular pipeline. Each generated ground truth cluster preserves key characteristics of the corresponding input cluster, including connectivity, minimum degree, and degree sequence distribution. The output consists of a synthetically generated network, and disjoint ground truth cluster labels for all nodes. In this paper, we present two enhanced versions: RECCS+ and RECCS++. RECCS+ maintains algorithmic fidelity to the original RECCS while introducing parallelization through an orchestrator that coordinates algorithmic components across multiple processes and employs multithreading. RECCS++ builds upon this foundation with additional algorithmic optimizations to achieve further speedup. Our experimental results demonstrate that RECCS+ and RECCS++ achieve speedups of up to 49x and 139x respectively on our benchmark datasets, with RECCS++'s additional performance gains involving a modest accuracy tradeoff. With this newfound performance, RECCS++ can now scale to networks with over 100 million nodes and nearly 2 billion edges.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modelling the Spread of Toxicity and Exploring its Mitigation on Online Social Networks</title>
<link>https://arxiv.org/abs/2511.20546</link>
<guid>https://arxiv.org/abs/2511.20546</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech, toxicity transformation, online social networks, peace-bots, temporal analysis  

<br /><br />Summary:  
This paper addresses the spread of hate speech and toxic content on online social networks like Twitter, Koo, and Gab, emphasizing the urgent need to understand and mitigate such harmful behavior. Unlike prior models that classify users strictly as hateful or not, this work conceptualizes users as transformers who modify incoming toxicity by amplifying, attenuating, or replicating it before forwarding. Temporal analysis reveals that toxicity is not conserved throughout the network, only a subset of users change their behavior over time, and behavior-changing users do not exhibit homophily. The authors propose a user-level model where each individual applies a "shift" to incoming toxicity, which varies based on the toxicity input and user category. Leveraging this model, they develop a network framework that accounts for time-varying user behavior to better capture toxicity dynamics. Building on these findings, the study proposes an intervention method using “peace-bots,” automated agents designed to reduce toxicity through strategic placement in networks. Experiments on both real-world and synthetic networks demonstrate that peace-bot interventions can effectively decrease toxic content, although their success significantly depends on network structure and bot deployment strategies. This work contributes a nuanced perspective on toxicity diffusion and practical mitigation techniques in digital environments. <div>
arXiv:2511.20546v1 Announce Type: new 
Abstract: Hate speech on online platforms has been credibly linked to multiple instances of real world violence. This calls for an urgent need to understand how toxic content spreads and how it might be mitigated on online social networks, and expectedly has been the topic of extensive research in recent times. Prior work has largely modelled hate through epidemic or spread activation based diffusion models, in which the users are often divided into two categories, hateful or not. In this work, users are treated as transformers of toxicity, based on how they respond to incoming toxicity. Compared with the incoming toxicity, users amplify, attenuate, or replicate (effectively, transform) the toxicity and send it forward. We do a temporal analysis of toxicity on Twitter, Koo and Gab and find that (a) toxicity is not conserved in the network; (b) only a subset of users change behaviour over time; and (c) there is no evidence of homophily among behaviour-changing users. In our model, each user transforms incoming toxicity by applying a "shift" to it prior to sending it forward. Based on this, we develop a network model of toxicity spread that incorporates time-varying behaviour of users. We find that the "shift" applied by a user is dependent on the input toxicity and the category. Based on this finding, we propose an intervention strategy for toxicity reduction. This is simulated by deploying peace-bots. Through experiments on both real-world and synthetic networks, we demonstrate that peace-bot interventions can reduce toxicity, though their effectiveness depends on network structure and placement strategy.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Semi-Supervised Node Classification with Self-Supervised Graph Clustering</title>
<link>https://arxiv.org/abs/2511.19976</link>
<guid>https://arxiv.org/abs/2511.19976</guid>
<content:encoded><![CDATA[
<div> Keywords: graph neural networks, semi-supervised node classification, graph clustering, self-supervised learning, message passing<br /><br />Summary: <br />1. This paper introduces NCGC, a unified framework combining self-supervised graph clustering with semi-supervised node classification to address limited label supervision in graph neural networks (GNNs).<br />2. The authors theoretically unify the optimization objectives of GNNs and spectral graph clustering, resulting in the development of soft orthogonal GNNs (SOGNs), which improve node representations through a refined message passing scheme tailored for both classification and clustering.<br />3. NCGC incorporates a novel self-supervised graph clustering module featuring two clustering objectives and the Sinkhorn-Knopp normalization technique, which converts cluster predictions into balanced soft pseudo-labels, effectively leveraging unlabeled nodes.<br />4. By jointly optimizing the supervised classification loss on labeled data and the self-supervised clustering loss on unlabeled data via a multi-task objective, NCGC fosters synergy between classification and clustering tasks, enhancing the model's expressive power.<br />5. Extensive experiments on seven real-world graphs demonstrate that NCGC consistently outperforms established GNN models and recent baselines across different GNN backbone architectures, highlighting its effectiveness in semi-supervised node classification. <div>
arXiv:2511.19976v1 Announce Type: cross 
Abstract: The emergence of graph neural networks (GNNs) has offered a powerful tool for semi-supervised node classification tasks. Subsequent studies have achieved further improvements through refining the message passing schemes in GNN models or exploiting various data augmentation techniques to mitigate limited supervision. In real graphs, nodes often tend to form tightly-knit communities/clusters, which embody abundant signals for compensating label scarcity in semi-supervised node classification but are not explored in prior methods.
  Inspired by this, this paper presents NCGC that integrates self-supervised graph clustering and semi-supervised classification into a unified framework. Firstly, we theoretically unify the optimization objectives of GNNs and spectral graph clustering, and based on that, develop soft orthogonal GNNs (SOGNs) that leverage a refined message passing paradigm to generate node representations for both classification and clustering. On top of that, NCGC includes a self-supervised graph clustering module that enables the training of SOGNs for learning representations of unlabeled nodes in a self-supervised manner. Particularly, this component comprises two non-trivial clustering objectives and a Sinkhorn-Knopp normalization that transforms predicted cluster assignments into balanced soft pseudo-labels. Through combining the foregoing clustering module with the classification model using a multi-task objective containing the supervised classification loss on labeled data and self-supervised clustering loss on unlabeled data, NCGC promotes synergy between them and achieves enhanced model capacity. Our extensive experiments showcase that the proposed NCGC framework consistently and considerably outperforms popular GNN models and recent baselines for semi-supervised node classification on seven real graphs, when working with various classic GNN backbones.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media</title>
<link>https://arxiv.org/abs/2511.20001</link>
<guid>https://arxiv.org/abs/2511.20001</guid>
<content:encoded><![CDATA[
<div> Mental health, cyberbullying, multiclass classification, MentalBERT, explainability<br /><br />Summary:<br /><br />This paper addresses the increasing prevalence of mental health challenges and cyberbullying in digital spaces by proposing a scalable and interpretable detection system. It introduces a unified multiclass classification framework capable of detecting ten distinct mental health and cyberbullying categories from social media platforms like Twitter and Reddit. The authors curate datasets and implement a "split-then-balance" pipeline that ensures training occurs on balanced data while evaluation is performed on a realistic, imbalanced test set, reflecting practical deployment scenarios. A thorough evaluation compares traditional lexical models, hybrid models, and end-to-end fine-tuned transformer-based models, highlighting that end-to-end fine-tuning is essential for optimal performance. Among the tested models, the domain-adapted MentalBERT achieves the best results with 0.92 accuracy and a Macro F1 score of 0.76, outperforming its generic version and zero-shot large language model baselines. Ethically, the system is positioned as a human-in-the-loop screening tool rather than a diagnostic instrument, emphasizing responsible use. To enhance interpretability, the authors introduce SHAPLLM, a hybrid explainability framework, and develop a prototype dashboard named "Social Media Screener" to integrate predictions and explanations into the moderator workflow. Finally, the work establishes a robust baseline and calls for future research focusing on multi-label, clinically-validated datasets at the nexus of online safety and computational mental health. <div>
arXiv:2511.20001v1 Announce Type: cross 
Abstract: Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous "split-then-balance" pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conducted a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAPLLM explainability framework and present a prototype dashboard ("Social Media Screener") designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realistic gossip in Trust Game on networks: the GODS model</title>
<link>https://arxiv.org/abs/2511.20248</link>
<guid>https://arxiv.org/abs/2511.20248</guid>
<content:encoded><![CDATA[
<div> Gossip, cooperation, reputation, Trust Game, agent-based model<br /><br />Summary:<br /><br />This study addresses the shortcomings in previous research on gossip by introducing an agent-based model that incorporates realistic gossip processes alongside different variants of the Trust Game. The authors highlight that many previous models unrealistically assume near-perfect information or broadcast-like gossip spread, which does not reflect real social dynamics. Their findings reveal that cooperators are disadvantaged under local gossip interactions, as they struggle to effectively distinguish defectors. Despite this, realistic gossip mechanisms lead to an overall increase in available resources, although they paradoxically tend to encourage more defection. Additionally, even when agents select partners through dynamic network interactions, there can be significant payoff inequalities between cooperators and defectors. Cooperators are thus faced with a strategic dilemma: either outcompete defectors or focus on maximizing collective growth. The study ultimately demonstrates that a combination of direct and indirect reciprocity, bolstered by reputation systems facilitated through gossip, significantly boosts cooperative efficiency by an order of magnitude. This nuanced approach offers deeper insights into the role of gossip in fostering cooperation and managing social exchanges in realistic settings. <div>
arXiv:2511.20248v1 Announce Type: cross 
Abstract: Gossip has been shown to be a relatively efficient solution to problems of cooperation in reputation-based systems of exchange, but many studies don't conceptualize gossiping in a realistic way, often assuming near-perfect information or broadcast-like dynamics of its spread. To solve this problem, we developed an agent-based model that pairs realistic gossip processes with different variants of Trust Game. The results show that cooperators suffer when local interactions govern spread of gossip, because they cannot discriminate against defectors. Realistic gossiping increases the overall amount of resources, but is more likely to promote defection. Moreover, even partner selection through dynamic networks can lead to high payoff inequalities among agent types. Cooperators face a choice between outcompeting defectors and overall growth. By blending direct and indirect reciprocity with reputations we show that gossiping increases the efficiency of cooperation by an order of magnitude.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limit Order Book Dynamics in Matching Markets:Microstructure, Spread, and Execution Slippage</title>
<link>https://arxiv.org/abs/2511.20606</link>
<guid>https://arxiv.org/abs/2511.20606</guid>
<content:encoded><![CDATA[
<div> Matching markets, liquidity constraints, limit order book, compensation inefficiency, preference state matrix<br /><br />Summary: Conventional matching market models presume monetary transfers can resolve utility differences among participants to clear the market efficiently. However, empirical evidence shows these transfers often do not suffice to bridge underlying structural preference gaps. This paper develops a novel market microstructure framework that conceptualizes matching decisions as analogous to limit order book dynamics featuring rigid bid-ask spreads. It models agent preferences with a latent preference state matrix, highlighting that the spread between an individual's internal ask price (their unconditional maximum utility) and the market’s best bid price (the reachable utility maximum) creates an inherent liquidity constraint. The authors prove a Threshold Impossibility Theorem which states that linear monetary compensation cannot eliminate these structural spreads unless it causes a fundamental categorical identity shift of the agents involved. A dynamic discrete choice execution model further illustrates that matches only happen when the market-to-book ratio surpasses a time-dependent liquidity threshold, mirroring order execution patterns under inventory pressure. Numerical simulations support persistent price slippage, stability in regional preference ordering, and frequent zero-spread high-tier executions. Overall, this framework provides a comprehensive microstructure explanation for observed matching failures, inefficiencies of compensation, and post-match regret commonly seen in illiquid, order-driven environments. <div>
arXiv:2511.20606v1 Announce Type: cross 
Abstract: Conventional models of matching markets assume that monetary transfers can clear markets by compensating for utility differentials. However, empirical patterns show that such transfers often fail to close structural preference gaps. This paper introduces a market microstructure framework that models matching decisions as a limit order book system with rigid bid ask spreads. Individual preferences are represented by a latent preference state matrix, where the spread between an agent's internal ask price (the unconditional maximum) and the market's best bid (the reachable maximum) creates a structural liquidity constraint. We establish a Threshold Impossibility Theorem showing that linear compensation cannot close these spreads unless it induces a categorical identity shift. A dynamic discrete choice execution model further demonstrates that matches occur only when the market to book ratio crosses a time decaying liquidity threshold, analogous to order execution under inventory pressure. Numerical experiments validate persistent slippage, regional invariance of preference orderings, and high tier zero spread executions. The model provides a unified microstructure explanation for matching failures, compensation inefficiency, and post match regret in illiquid order driven environments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multilevel network approach to revealing patterns of online political selective exposure</title>
<link>https://arxiv.org/abs/2408.03828</link>
<guid>https://arxiv.org/abs/2408.03828</guid>
<content:encoded><![CDATA[
<div> Selective exposure, political polarization, multi-scale community detection, Twitter, Brazilian Presidential Election<br /><br />Summary:<br /><br />1. The study addresses selective exposure, the tendency of individuals to seek information that aligns with their beliefs and avoid contradictory information, which fuels polarization and echo chambers.<br />2. Unlike traditional political analyses that focus on a simplistic left-right ideology scale, this research introduces a multilevel framework using multi-scale community detection to capture more nuanced political groupings.<br />3. The methodology combines survey data and Twitter/X user data from the 2022 Brazilian Presidential Election, constructing a bipartite network linking survey respondents to political influencers, which is then projected onto influencer nodes.<br />4. Applying multi-scale community detection reveals a hierarchical clustering of political influencers, showing that selective exposure is more complex than a binary left-right division.<br />5. Different community resolution levels correspond to varying selective exposure patterns and associations with a broad range of respondent attributes, including ideology, demographics, news consumption habits, and perceptions of incivility.<br />6. At finer community resolutions, multiple factors correlate with respondents' community overlap, whereas at coarser levels, ideological position predominates.<br />7. The research highlights the limitations of measuring selective exposure using only a single-level left-right approach, emphasizing the need for multilevel analyses to better understand online political behavior. <div>
arXiv:2408.03828v2 Announce Type: replace 
Abstract: Selective exposure, individuals' inclination to seek out information that supports their beliefs while avoiding information that contradicts them, plays an important role in the emergence of polarization and echo chambers. In the political domain, selective exposure is usually measured on a left-right ideology scale, ignoring finer details. To bridge the gap, this work introduces a multilevel analysis framework based on a multi-scale community detection approach. To test this approach, we combine survey and Twitter/X data collected during the 2022 Brazilian Presidential Election and investigate selective exposure patterns among survey respondents in their choices of whom to follow. We construct a bipartite network connecting survey respondents with political influencers and project it onto the influencer nodes. Applying multi-scale community detection to this projection uncovers a hierarchical clustering of political influencers. Different indices of selective exposure suggest that the characteristics of the influencer communities engaged by survey respondents vary with the level of community resolution. This finding indicates that online political selective exposure exhibits a more complex structure than a mere left-right dichotomy. Moreover, depending on the resolution level we consider, we find different associations between network indices of exposure patterns and 189 individual attributes of the survey respondents. For example, at finer levels, survey respondents' Community Overlap is associated with several factors, such as ideological position, demographics, news consumption frequency, and incivility perception. In comparison, only their ideological position is a significant factor at coarser levels. Our work demonstrates that measuring selective exposure at a single level, such as left and right, misses important information necessary to capture this phenomenon correctly.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiple Randomization Designs: Estimation and Inference with Interference</title>
<link>https://arxiv.org/abs/2112.13495</link>
<guid>https://arxiv.org/abs/2112.13495</guid>
<content:encoded><![CDATA[
<div> Keywords: experimental designs, randomized controlled trials, multiple populations, interference, statistical methods<br /><br />Summary:<br />1. The article introduces a new class of experimental designs aimed at addressing complexities in modern experiments involving multiple interacting populations, such as buyers and sellers or drivers and riders.<br />2. Traditional randomized controlled trials (RCTs), which assign treatment and control groups randomly within a single population, estimate treatment effects by comparing average outcomes between groups.<br />3. In settings where outcomes and treatments are indexed by multiple populations, interactions between these populations can cause interference or spillover effects, invalidating simple mean comparisons.<br />4. The proposed Multiple Randomization Designs allow for the study of interference effects that are otherwise unidentifiable in classical RCT frameworks.<br />5. The authors also develop new statistical methods tailored for analyzing data arising from these complex designs, enabling more accurate estimation of treatment effects in the presence of multi-population interference. <div>
arXiv:2112.13495v2 Announce Type: replace-cross 
Abstract: In this study we introduce a new class of experimental designs. In a classical randomized controlled trial (RCT), or A/B test, a randomly selected subset of a population of units (e.g., individuals, plots of land, or experiences) is assigned to a treatment (treatment A), and the remainder of the population is assigned to the control treatment (treatment B). The difference in average outcome by treatment group is an estimate of the average effect of the treatment. However, motivating our study, the setting for modern experiments is often different, with the outcomes and treatment assignments indexed by multiple populations. For example, outcomes may be indexed by buyers and sellers, by content creators and subscribers, by drivers and riders, or by travelers and airlines and travel agents, with treatments potentially varying across these indices. Spillovers or interference can arise from interactions between units across populations. For example, sellers' behavior may depend on buyers' treatment assignment, or vice versa. This can invalidate the simple comparison of means as an estimator for the average effect of the treatment in classical RCTs. We propose new experiment designs for settings in which multiple populations interact. We show how these designs allow us to study questions about interference that cannot be answered by classical randomized experiments. Finally, we develop new statistical methods for analyzing these Multiple Randomization Designs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PriME: Privacy-aware Membership profile Estimation in networks</title>
<link>https://arxiv.org/abs/2406.02794</link>
<guid>https://arxiv.org/abs/2406.02794</guid>
<content:encoded><![CDATA[
<div> Keywords: community membership estimation, differential privacy, degree corrected mixed membership stochastic block model, spectral clustering, edge flip mechanism  

<br /><br />Summary:  
This paper introduces a novel method for estimating the probabilities of vertices belonging to communities in networks generated by the Degree Corrected Mixed Membership Stochastic Block Model (DCMMSBM). Operating under the stringent framework of $\varepsilon$-edge local differential privacy, the approach ensures individual edge privacy while performing community detection. The authors propose an optimal private algorithm based on a symmetric edge flip mechanism, which perturbs edges to protect privacy yet retains sufficient structure for accurate inference. Spectral clustering is then used on the privatized data to estimate the vertex community memberships effectively. The paper provides a thorough theoretical analysis, detailing the estimation risk of their procedure and establishing its minimax optimality by presenting matching lower bounds on the risk under privacy constraints. To demonstrate practical viability, the authors validate their method through extensive numerical simulations and apply it to real-world network data, showing competitive performance despite privacy restrictions. This work significantly advances the state-of-the-art by balancing high-accuracy community membership estimation with rigorous privacy guarantees, addressing a critical challenge in analyzing sensitive network datasets. <div>
arXiv:2406.02794v2 Announce Type: replace-cross 
Abstract: This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy. Operating within the $\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships. We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints. To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data. This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetful by Design? A Critical Audit of YouTube's Search API for Academic Research</title>
<link>https://arxiv.org/abs/2506.11727</link>
<guid>https://arxiv.org/abs/2506.11727</guid>
<content:encoded><![CDATA[
<div> Keywords: YouTube Data API, search endpoint, video discoverability, research replicability, Digital Services Act<br /><br />Summary:<br /><br />This paper performs a critical analysis of YouTube Data API (v3) search functionality, widely used in academic research. Through weekly searches over six months with eleven distinct queries, the study identifies major limitations in completeness, representativeness, consistency, and bias of returned data. It finds considerable differences between ranking parameters like relevance and date: relevance-based searches often return many off-topic videos, reducing precision. A significant temporal decay effect is observed where video discoverability sharply declines 20-60 days post-publication despite videos remaining available on the platform, threatening systematic data collection over time. Search results lack consistency, with identical queries yielding differing video sets across time points, thereby compromising replicability of research. A case study focused on the European Parliament elections illustrates how these issues can skew research findings. The paper proposes mitigation strategies but concludes that the API’s search endpoint, possibly optimized for 'freshness' rather than comprehensive recall, is insufficient for robust academic research—particularly in the context of requirements set by the Digital Services Act. <div>
arXiv:2506.11727v3 Announce Type: replace-cross 
Abstract: This paper critically audits the search endpoint of YouTube's Data API (v3), a common tool for academic research. Through systematic weekly searches over six months using eleven queries, we identify major limitations regarding completeness, representativeness, consistency, and bias. Our findings reveal substantial differences between ranking parameters like relevance and date in terms of video recall and precision, with relevance often retrieving numerous off-topic videos. We also observe severe temporal decay in video discoverability: the number of retrievable videos for a given period drops dramatically within just 20-60 days of publication, even though these videos remain on the platform. This potentially undermines research designs that rely on systematic data collection. Furthermore, search results lack consistency, with identical queries yielding different video sets over time, compromising replicability. A case study on the European Parliament elections highlights how these issues impact research outcomes. While the paper offers several mitigation strategies, it concludes that the API's search function, potentially prioritizing 'freshness' over comprehensive retrieval, is not adequate for robust academic research, especially concerning Digital Services Act requirements.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Signals Reveal Hidden Connections: A Physics-Inspired Framework for Link Prediction via Personalized PageRank Signals</title>
<link>https://arxiv.org/abs/2511.17569</link>
<guid>https://arxiv.org/abs/2511.17569</guid>
<content:encoded><![CDATA[
<div> Keywords: link prediction, diffusion distance, Personalized PageRank, complex networks, network dynamics<br /><br />Summary:<br /><br />This paper addresses the challenge of link prediction in complex networks, aiming to identify missing or future connections crucial for understanding network evolution and function. It introduces a novel framework called Diffusion Distance with Personalized PageRank (D-PPR), which integrates static network topology with dynamic information flow by treating nodes as sources of signals that propagate through the network using Personalized PageRank vectors. The approach measures similarity between node pairs via diffusion distance governed by the graph Laplacian, linking local interactions to overall network dynamics. The authors benchmarked D-PPR systematically on synthetic networks (Barabási-Albert and LFR models) and seven large real-world networks from technological, biological, and social domains. Results show that D-PPR performs competitively against established local and global heuristics, particularly excelling in sparse and modular network structures. The study provides a physics-inspired, theoretically rigorous foundation for link prediction by demonstrating how embedding dynamic processes into structural similarity metrics deepens insights into network connectivity patterns. Ultimately, this work advances both methodology and theory by elucidating the interplay between network topology and dynamics, highlighting the benefits of combining static and dynamic perspectives for improved link prediction. <div>
arXiv:2511.17569v1 Announce Type: new 
Abstract: Link prediction in complex networks--identifying the missing or future connections--remains a cornerstone problem for understanding network evolution and function, yet existing methods struggle to balance computational efficiency with theoretical rigor across heterogeneous topologies. This work introduces a physically principled framework, Diffusion Distance with Personalized PageRank (D-PPR), which unifies static topology with dynamic information flow by modeling nodes as signal sources propagating through the network via Personalized PageRank (PPR) vectors. The method quantifies node-pair similarity through the graph Laplacian-governed diffusion distance between their topology-aware signal distributions, thereby bridging microscopic interactions with macroscopic network dynamics. Systematic benchmarking on synthetic (Barab\'asi-Albert, LFR) and seven large-scale real-world networks spanning technology, biology, and social domains demonstrates that D-PPR achieves highly competitive performance, yielding favorable results when compared to representative local and global heuristics, particularly in sparse and modular networks. These findings establish a rigorous foundation for physics-inspired link prediction by revealing that incorporating dynamical processes into structural similarity metrics enables deeper insights into network connectivity patterns, offering both methodological advances and new theoretical perspectives on the interplay between topology and dynamics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constructing Political Coordinates: Aggregating Over the Opposition for Diverse News Recommendation</title>
<link>https://arxiv.org/abs/2511.17574</link>
<guid>https://arxiv.org/abs/2511.17574</guid>
<content:encoded><![CDATA[
<div> Keywords: News Recommender Systems, Political Partisanship, Filter Bubbles, Constructed Political Coordinates, Collaborative Filtering<br /><br />Summary:<br /><br />1. The paper addresses the challenges posed by news recommender systems (NRSs) in democratic societies, focusing on how they can inadvertently reinforce political biases and contribute to filter bubbles. <br />2. It highlights that NRSs often confuse users' true interests with the partisan slant present in their reading history and the dominant biases in popular news coverage. <br />3. To mitigate this, the authors introduce Constructed Political Coordinates (CPC), a novel embedding space designed to represent users' political partisanship across topics relative to a broader population. <br />4. Using CPC, they develop a collaborative filtering (CF) recommendation approach that encourages introducing articles from users with opposing biases to the target user, promoting exposure to diverse political views. <br />5. Empirical comparison shows CPC-based methods foster greater bias diversity and align better with users' actual political tolerance, while traditional CF tends to exploit biases to increase engagement, potentially exacerbating polarization. <div>
arXiv:2511.17574v1 Announce Type: new 
Abstract: In the past two decades, open access to news and information has increased rapidly, empowering educated political growth within democratic societies. News recommender systems (NRSs) have shown to be useful in this process, minimizing political disengagement and information overload by providing individuals with articles on topics that matter to them. Unfortunately, NRSs often conflate underlying user interest with the partisan bias of the articles in their reading history and with the most popular biases present in the coverage of their favored topics. Over extended interaction, this can result in the formation of filter bubbles and the polarization of user partisanship. In this paper, we propose a novel embedding space called Constructed Political Coordinates (CPC), which models the political partisanship of users over a given topic-space, relative to a larger sample population. We apply a simple collaborative filtering (CF) framework using CPC-based correlation to recommend articles sourced from oppositional users, who have different biases from the user in question. We compare against classical CF methods and find that CPC-based methods promote pointed bias diversity and better match the true political tolerance of users, while classical methods implicitly exploit biases to maximize interaction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Review of Core-Periphery and Community Detection Paradigms</title>
<link>https://arxiv.org/abs/2511.17657</link>
<guid>https://arxiv.org/abs/2511.17657</guid>
<content:encoded><![CDATA[
<div> core-periphery structure, community detection, network science, meso-scale structures, evaluation metrics<br /><br />Summary:<br /><br />This article addresses meso-scale structures in network science, focusing on core-periphery (CP) structures and community detection. Communities feature dense connections within groups and sparse links between groups, whereas CP structures involve a densely connected core and a loosely linked periphery connected primarily to the core. Despite their importance, CP structure identification is challenging due to a lack of a universal definition and standardized detection methods, making the problem ill-posed. This ambiguity results in conceptual overlaps and inconsistent evaluation metrics, hindering methodological progress. The review offers a comprehensive and structured overview of foundational concepts related to CP structures alongside recent advances in detection techniques. It highlights key challenges faced in the identification and evaluation of CP structures. Furthermore, the article discusses the relationship between CP and community structures in networks, elaborating on their interplay and distinctions. Practical applications of CP detection approaches in diverse real-world networks are also examined, demonstrating the relevance and impact of these methodologies. Overall, the review aims to clarify the conceptual landscape, promote consistent evaluation, and stimulate further research developments in CP structure detection within network science. <div>
arXiv:2511.17657v1 Announce Type: new 
Abstract: Meso-scale structures, such as core-periphery (CP) and community structure, have attracted significant attention in modern network science. While communities are characterized by dense intra-group and sparse inter-group connections, CP structures consist of a densely interconnected core and a loosely connected periphery, where peripheral nodes are typically linked to the core. Despite growing interest, identifying CP structures remains an ill-posed problem, with no universally accepted definition or standardized detection methodology. This ambiguity has led to conceptual overlaps, inconsistent evaluation metrics and slowed methodological progress. In this review, we provide a structured overview of foundational concepts, recent advances, key challenges and comparative evaluations of CP detection approaches, along with a discussion of their interplay with community structure and applications in real-world networks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lossy communication constrains iterated learning</title>
<link>https://arxiv.org/abs/2511.18220</link>
<guid>https://arxiv.org/abs/2511.18220</guid>
<content:encoded><![CDATA[
<div> iterated learning, communication ability, information theory, evolutionary differences, lossy communication  

<br /><br />Summary:  
This article explores why humans excel at iterated learning, a process where knowledge is progressively expanded and refined across generations. Many theories propose that humans possess significant evolutionary discontinuities in communication or cognition compared to other species. The authors question whether large differences in ability are necessary or if small improvements in communication can lead to substantial gains in what populations can learn over multiple generations. Using a formal information theory model, they vary the amount of information individual learners can transmit and analyze its impact on iterated learning outcomes. They find that incremental increases in communication channel capacity can cause dramatic, nonlinear improvements in population learning performance. Additionally, they present a theoretical result demonstrating how constraints from lossy communication at the individual level limit collective learning potential. The findings suggest that relatively minor, quantitative enhancements in communication ability alone could explain large disparities in cumulative cultural learning across species. This challenges views requiring major evolutionary leaps to account for humans’ unique iterative learning capabilities. <div>
arXiv:2511.18220v1 Announce Type: new 
Abstract: Humans' distinctive role in the world can largely be attributed to our capacity for iterated learning, a process by which knowledge is expanded and refined over generations. A range of theories seek to explain why humans are so adept at iterated learning, many positing substantial evolutionary discontinuities in communication or cognition. Is it necessary to posit large differences in abilities between humans and other species, or could small differences in communication ability produce large differences in what a species can learn over generations? We investigate this question through a formal model based on information theory. We manipulate how much information individual learners can send each other and observe the effect on iterated learning performance. Incremental changes to the channel rate can lead to dramatic, non-linear changes to the eventual performance of the population. We complement this model with a theoretical result that describes how individual lossy communications constrain the global performance of iterated learning. Our results demonstrate that incremental, quantitative changes to communication abilities could be sufficient to explain large differences in what can be learned over many generations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perplexity-Homophily Index: Homophily through Diversity in Hypergraphs</title>
<link>https://arxiv.org/abs/2511.19170</link>
<guid>https://arxiv.org/abs/2511.19170</guid>
<content:encoded><![CDATA[
<div> hypergraphs, homophily, interaction perplexity, diversity gap, Perplexity-Homophily Index  

<br /><br />Summary:  
The paper addresses the challenge of modeling real-world complex systems as hypergraphs, which capture group interactions among multiple entities. It focuses on quantifying homophily, the tendency of similar entities to associate, within these higher-order networks. The authors introduce a hyperedge-centric framework where each group interaction is represented as a hyperedge. They define interaction perplexity as a measure of the effective number of distinct attributes present in a hyperedge. By comparing this observed perplexity with a degree-preserving random baseline, they derive the diversity gap, which quantifies how much more or less diverse an interaction is than expected by chance. Building on this, the study proposes the Perplexity-Homophily Index, a global homophily score computed by averaging the normalized diversity gaps across all hyperedges in a network. Experiments on both synthetic and real-world datasets demonstrate that this index effectively captures the full distribution of homophily in hypergraphs. Furthermore, the results show how homophilic (similarity-driven) and heterophilic (diversity-driven) tendencies vary depending on the size of the interactions involved. This framework provides a comprehensive tool for analyzing community formation and information flow in complex systems modeled as hypergraphs. <div>
arXiv:2511.19170v1 Announce Type: new 
Abstract: Real-world complex systems are often better modeled as hypergraphs, where edges represent group interactions involving multiple entities. Understanding and quantifying homophily (similarity-driven association) in such networks is essential for analyzing community formation and information flow. We propose a hyperedge-centric framework to quantify homophily in hypergraphs. Each interaction is represented as a hyperedge, and its interaction perplexity measures the effective number of distinct attributes it contains. Comparing this observed perplexity with a degree-preserving random baseline defines the diversity gap, which quantifies how diverse an interaction is than expected by chance. The global homophily score for a network, called Perplexity-Homophily Index, is computed by averaging the normalized diversity gap across all hyperedges. Experiments on synthetic and real-world datasets show that the proposed index captures the full distribution of homophily and reveals how homophilic and heterophilic tendencies vary with interaction size in hypergraphs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Yukawa Potential Centrality for Identification of Influential Spreaders in Complex Networks</title>
<link>https://arxiv.org/abs/2511.19300</link>
<guid>https://arxiv.org/abs/2511.19300</guid>
<content:encoded><![CDATA[
<div> Keywords: influential nodes, Yukawa Potential Centrality, complex networks, centrality measures, epidemic spreading

<br /><br />Summary:  
This study addresses the challenge of identifying influential nodes in complex networks, which is crucial for understanding information and contagion spread. Traditional centrality measures, especially gravity-based models, rely on pairwise interaction forces and a fixed radius of influence, which do not capture the heterogeneous and dynamic aspects of real networks. To improve upon these limitations, the authors propose a novel method called Yukawa Potential Centrality (YPC), inspired by the physical Yukawa potential but adapted to network topology. Unlike gravity models, YPC computes a scalar potential for each node instead of pairwise forces and dynamically adjusts the radius of influence based on local structural properties. This approach creates a physically interpretable connection between potential theory and network science while drastically reducing computational complexity from quadratic to near-linear time. The performance of YPC is validated on both synthetic and real-world social networks. The node rankings produced by YPC are compared with classical centrality indices and epidemic spreading models such as SI and SIS. Results show a strong positive correlation between YPC and the SIS model, demonstrating YPC’s effectiveness in isolating key spreaders, even in networks with irregular topologies. Overall, YPC offers a scalable, adaptive, and theoretically sound framework for analyzing influence in diverse networked systems. <div>
arXiv:2511.19300v1 Announce Type: new 
Abstract: Identifying influential nodes in complex networks is a fundamental challenge for understanding how information, influence, and contagion propagate through interconnected systems. Conventional centrality measures, particularly gravity-based models, often depend on pairwise interaction forces and a fixed radius of influence, which oversimplify the heterogeneous and dynamic nature of real networks. To overcome these limitations, this study proposes a novel non-interactive, action-based model, termed Yukawa Potential Centrality (YPC), which adapts the physical Yukawa potential to the topology of complex networks. Unlike gravity models, YPC computes a scalar potential for each node rather than pairwise forces, dynamically adjusting its radius of influence according to local structural properties. This formulation establishes a physically interpretable bridge between potential theory and network science, while significantly reducing computational complexity, from quadratic to near-linear time. The model is evaluated across both synthetic and real-world social networks, and its node rankings are compared with classical centrality indices and epidemic spreading models (SI and SIS). Experimental findings reveal that YPC exhibits a strong positive correlation with the SIS model and effectively isolates key spreaders, even within highly irregular topologies. These results demonstrate that YPC provides a scalable, adaptive, and theoretically grounded framework for influence analysis in social, biological, and communication networks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motivational Climate Effects on Communications, Emotional-Social States, and Performance in Collaborative Gaming Environment</title>
<link>https://arxiv.org/abs/2511.17513</link>
<guid>https://arxiv.org/abs/2511.17513</guid>
<content:encoded><![CDATA[
<div> Keywords: motivational climate, communication, emotions, collective efficacy, performance<br /><br />Summary:<br /><br />1. The study investigates how different motivational climates—positive-supportive (PS) versus neutral-unsupported (NU)—impact communication, emotional states, collective efficacy, and team performance in collaborative gaming.<br /><br />2. Forty participants with no gaming experience were assigned to 20 gender-matched teams of three, each including one confederate, divided evenly between the two motivational climates.<br /><br />3. Teams played three progressively difficult levels of the game Overcooked! 2, during which communication contents, emotional responses, collective efficacy, and performance were recorded and analyzed.<br /><br />4. Statistical methods, including mixed-design MANOVAs, ANOVAs, and chi-square tests, were used to assess the effects of motivational climate and task difficulty on communication patterns, emotions, collective efficacy, and performance.<br /><br />5. Results showed PS teams significantly outperformed NU teams at the lowest difficulty level, but the performance advantage lessened with increasing task complexity.<br /><br />6. Communication analysis revealed PS teams used more action-oriented, factual, and emotional/motivational talk, whereas NU teams expressed more uncertainty and off-task communication.<br /><br />7. Talk time increased for all teams as difficulty rose; however, PS teams retained more positive emotional profiles with higher excitement and happiness and lower anxiety, dejection, and anger than NU teams.<br /><br />8. PS teams also maintained consistently higher collective efficacy beliefs across all task difficulties.<br /><br />9. Overall, a positive motivational climate enhances team communication effectiveness, emotional resilience, and performance in demanding collaborative gaming environments. <div>
arXiv:2511.17513v1 Announce Type: cross 
Abstract: The study explores the effects of motivational climate on communication features, emotional states, collective efficacy, and performance in collaborative gaming environments. Forty participants with no prior gaming experience were randomly assigned to 20 gender-matched teams of three (including one confederate) across two motivational climates: positive-supportive (PS) or neutral-unsupported (NU) (10 teams per condition). Team members completed three progressively difficult levels of Overcooked! 2 during which communication contents, emotional responses, collective efficacy, and performance outcomes were observed and coded. Mixed-design MANOVAs and ANOVAs were employed to examine the effects of motivational climate and task difficulty on communication patterns, emotions, collective efficacy, and performance. Chi-square analyses were performed to test communication content differences between conditions. Results revealed that PS team members significantly outperformed NU teams at lower task difficulty level, but this advantage diminished as task complexity increased. Communication analysis revealed that PS team members utilized significantly more action-oriented, factual, and emotional/motivational statements, while NU team members used more statements of uncertainty and non-task-related communication. The percentage of the talk time increased with difficulty across both climate conditions. PS team members maintained more positive emotional profiles throughout, with higher excitement and happiness scores and lower anxiety, dejection, and anger compared to NU team members. Furthermore, PS team members reported consistently higher collective efficacy beliefs across all difficulty levels. These findings reveal that positive motivational climate enhances team communication effectiveness, emotional resilience, and performance outcomes in challenging collaborative environments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs</title>
<link>https://arxiv.org/abs/2511.17572</link>
<guid>https://arxiv.org/abs/2511.17572</guid>
<content:encoded><![CDATA[
<div> Alignment, Large Language Models, Epistemic Stance, Behavioral Patterns, Fact Deletion<br /><br />Summary:  
1. The paper investigates whether large language models (LLMs) aligned to specific online communities exhibit generalizable behavioral patterns that mirror those communities' attitudes when facing new uncertainties, or if they merely recall training data patterns.  
2. The authors introduce a novel testing framework called epistemic stance transfer, which involves targeted deletion of event-specific knowledge from the models. This is validated through multiple probing methods to ensure effective fact removal.  
3. After removing factual information, the study evaluates whether the aligned LLMs continue to reproduce community-specific response patterns, particularly under conditions of ignorance or uncertainty.  
4. Applying this framework to Russian–Ukrainian military discourse and U.S. partisan Twitter data, the results show that LLMs maintain stable, community-aligned behavioral responses despite aggressive fact removal.  
5. These findings suggest that alignment encodes structured and generalizable behaviors beyond simple memorization or surface mimicry of training data. This work provides a systematic approach to detecting persistent behavioral biases in LLMs, contributing to safer and more transparent model deployments. <div>
arXiv:2511.17572v1 Announce Type: cross 
Abstract: When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Concerns and ChatGPT: Exploring Online Discourse through the Lens of Information Practice on Reddit</title>
<link>https://arxiv.org/abs/2511.18268</link>
<guid>https://arxiv.org/abs/2511.18268</guid>
<content:encoded><![CDATA[
<div> Privacy, ChatGPT, Reddit, Risk Negotiation, AI Design  

<br /><br />Summary:  
This study investigates how Reddit users collectively address privacy concerns related to ChatGPT usage, especially given its widespread adoption for education, writing, and health advice. Data was collected from three key subreddits—r/ChatGPT, r/privacy, and r/OpenAI—over a period spanning November 2022 to May 2025, culminating in a dataset of 426 posts and 1,900 comments. Through qualitative thematic analysis guided by information practice theory, combined with BERTopic modeling to validate thematic coverage, the research uncovers dominant discourses such as risk signaling, norm-setting, and resignation among users regarding privacy threats. Importantly, the findings highlight adaptive behaviors including collective troubleshooting and advocacy for privacy-preserving alternatives. The study positions Reddit as a collective sense-making environment where users surface privacy risks, establish informal norms, and exchange strategies to mitigate these concerns. These insights contribute valuable knowledge for the design of AI systems and initiatives aimed at improving privacy literacy among users. The research underscores the role of online communities in shaping user responses to emerging technological risks and offers guidance for fostering safer and more transparent AI interactions. <div>
arXiv:2511.18268v1 Announce Type: cross 
Abstract: As millions of people use ChatGPT for tasks such as education, writing assistance, and health advice, concerns have grown about how personal prompts and data are stored and used. This study explores how Reddit users collectively negotiate and respond to these privacy concerns. Posts were collected from three major subreddits -- r/Chatgpt, r/privacy, and r/OpenAI -- between November 2022 and May 2025. An iterative keyword search followed by manual screening resulted in a final dataset of 426 posts and 1,900 comments. Using information practice as the theoretical lens, we conducted a qualitative thematic analysis to identify collective practices of risk negotiation, validated with BERTopic topic modeling to ensure thematic saturation. Findings revealed risk signaling, norm-setting, and resignation as dominant discourses, and collective troubleshooting and advocacy for privacy-preserving alternatives as key adaptive practices. Reddit functions as a site of collective sense-making where users surface risks, establish informal norms, and share strategies for mitigating privacy threats, offering insights for AI design and privacy literacy initiatives.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Effects of Remote Working on Scientific Collaboration and Impact</title>
<link>https://arxiv.org/abs/2511.18481</link>
<guid>https://arxiv.org/abs/2511.18481</guid>
<content:encoded><![CDATA[
<div> COVID-19 pandemic, academic collaboration, cross-border partnerships, citation impact, hybrid work models  

<br /><br />Summary:  
This study investigates how the COVID-19 pandemic shifted academic collaboration from predominantly in-person to remote interactions, analyzing research output before, during, and after the pandemic. Using extensive bibliometric data, the authors track changes in scientific collaboration networks and their impact over time. First, there was a significant geographic redistribution of collaborations, with cross-border partnerships increasing noticeably post-2020, highlighting a reduction in the previous limitations imposed by geographic proximity. Second, despite the broader and more diverse collaboration networks, citation impact declined, signaling potential negative consequences of the lack of spontaneous in-person interactions, which are crucial for fostering deep discussions and exchanging ideas that enhance research quality. The findings underscore that while remote and hybrid work models allow for more inclusive and geographically expansive collaboration, they may also challenge traditional mechanisms that drive high-quality scientific output. The study concludes by emphasizing that universities and research institutions need to thoughtfully balance remote and in-person engagement to maintain research excellence as hybrid academic models become more widespread. <div>
arXiv:2511.18481v1 Announce Type: cross 
Abstract: The COVID-19 pandemic shifted academic collaboration from in-person to remote interactions. This study explores, for the first time, the effects on scientific collaborations and impact of such a shift, comparing research output before, during, and after the pandemic. Using large-scale bibliometric data, we track the evolution of collaboration networks and the resulting impact of research over time. Our findings are twofold: first, the geographic distribution of collaborations significantly shifted, with a notable increase in cross-border partnerships after 2020, indicating a reduction in the constraints of geographic proximity. Second, despite the expansion of collaboration networks, there was a concerning decline in citation impact, suggesting that the absence of spontaneous in-person interactions-which traditionally foster deep discussions and idea exchange-negatively affected research quality. As hybrid work models in academia gain traction, this study highlights the need for universities and research organizations to carefully consider the balance between remote and in-person engagement.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs</title>
<link>https://arxiv.org/abs/2511.18783</link>
<guid>https://arxiv.org/abs/2511.18783</guid>
<content:encoded><![CDATA[
<div> Keywords: Hypergraph Neural Networks, Heterophily, Contrastive Learning, Hyperedge Representation, High-pass Filtering<br /><br />Summary:<br /><br />This paper introduces HONOR, an unsupervised hypergraph contrastive learning framework designed to work effectively on both homophilic and heterophilic hypergraphs, addressing limitations of existing methods that mainly assume homophily. The approach explicitly models heterophilic relationships by integrating two key mechanisms: a prompt-based hyperedge feature construction strategy that ensures global semantic consistency and reduces local noise, and an adaptive attention aggregation module that dynamically assesses node contributions to hyperedges. Additionally, HONOR leverages high-pass filtering to better exploit heterophilic connection patterns, leading to more discriminative and robust representations for nodes and hyperedges. The authors provide theoretical analysis demonstrating HONOR’s superior generalization ability and robustness compared to prior approaches. Empirical evaluations on multiple datasets with varying homophily and heterophily confirm that HONOR consistently outperforms state-of-the-art baselines, highlighting its effectiveness and broad applicability for modeling complex high-order relationships in hypergraphs. <div>
arXiv:2511.18783v1 Announce Type: cross 
Abstract: Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \textbf{HONOR}, a novel unsupervised \textbf{H}ypergraph c\textbf{ON}trastive learning framework suitable for both hom\textbf{O}philic and hete\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction</title>
<link>https://arxiv.org/abs/2511.18874</link>
<guid>https://arxiv.org/abs/2511.18874</guid>
<content:encoded><![CDATA[
<div> Multimodal trajectory prediction, map-free model, global context, hybrid attention, motion-intention alignment  

<br /><br />Summary:  
This paper addresses the challenge of predicting multiple plausible future vehicle trajectories without relying on costly and potentially outdated HD maps. It critiques both HD map-dependent and existing map-free models, noting that the former suffer from high acquisition costs and sensitivity to corrupted inputs while the latter lack global contextual understanding, causing misalignment with vehicle motion intentions. To overcome these limitations, the authors propose GContextFormer, a novel plug-and-play transformer-based encoder-decoder architecture. It features a Motion-Aware Encoder that constructs scene-level intention priors through scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations within a shared global context, reducing inter-mode suppression. The Hierarchical Interaction Decoder decomposes social interaction reasoning using dual-pathway cross-attention: one pathway for uniform geometric coverage of all agent-mode pairs and another that emphasizes crucial neighbor contexts via a gating mechanism, balancing coverage and focus. Evaluations on eight highway-ramp scenarios from the TOD-VT dataset demonstrate that GContextFormer surpasses state-of-the-art baselines, especially in high-curvature and transition zones, and exhibits improved robustness. Additionally, the architecture offers interpretability by distinguishing motion modes and modulating neighbor context contributions. Its modular design also supports extensibility to cross-domain multimodal reasoning tasks. <div>
arXiv:2511.18874v1 Announce Type: cross 
Abstract: Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trade-Off Between Multiplicity and Specificity in the Inter-layer Connectivity of non-identical Multilayer Networks</title>
<link>https://arxiv.org/abs/2511.19047</link>
<guid>https://arxiv.org/abs/2511.19047</guid>
<content:encoded><![CDATA[
<div> Keywords: multilayer networks, inter-layer connections, intra-layer synchronization, amplitude death, network topology<br /><br />Summary:<br /><br />This study explores the dynamics of multilayer networks (MLs) with symmetric inter-layer connections and multilayer networks with asymmetric inter-layer connections (MLas). Symmetric connections correspond to a one-to-one node mapping between layers, while asymmetric connections involve randomized one-to-many links preserving overall density. The research shows that one-to-one (symmetric) inter-layer coupling promotes intra-layer synchronization (ILS) more effectively than asymmetric coupling. The presence and nature of ILS in networks with random inter-layer connections strongly depend on how these randomizations affect intra-layer homomorphisms—permutations preserving network structure. The phenomenon of amplitude death (AD) occurs at lower coupling strength and frequency mismatch in symmetric MLs compared to asymmetric MLas. In symmetric MLs, AD depends on connection density and topology but is independent of network size, whereas in asymmetric MLas, AD is influenced by size, density, topology, and inter-layer mismatches. Both network types exhibit multistability, with the faster layer consistently showing remanent periodic phase-locked oscillations regardless of topology or connectivity. Additionally, the slower layer maintains remnant synchrony among nodes related by homomorphisms. The study concludes that symmetric inter-layer connections are advantageous for achieving ILS and maintaining persistent memory in mismatched node layers. Conversely, asymmetric connections better mitigate AD under low coupling and layer mismatch conditions. <div>
arXiv:2511.19047v1 Announce Type: cross 
Abstract: We study the coupled dynamics of multilayer networks with symmetric (MLs) and asymmetric (MLas) inter-layer connections. The symmetric inter-layer connections arise from a one-to-one correspondence between the nodes of different layers. In contrast, asymmetry results from the multiplicity of inter-layer connections, achieved by randomizing the links while preserving their overall density, thereby allowing one-to-many inter-layer connections. We investigate how different types of inter-layer coupling impact the dynamics of non-identical multilayer networks. We find that the specificity of one-to-one inter-layer connections facilitates intra-layer synchronization (ILS). In contrast, for networks with random inter-layer connectivity, ILS depends on how randomness affects intra-layer homomorphism (the set of permutations that preserve the network structure). Furthermore, amplitude death (AD) in MLs is observed at lower connectivity strength and frequency mismatch than the MLas. Moreover, AD in MLs depends on the density and topology, but does not depend on the size of the networks. On the other hand, AD in MLas is influenced by network size in addition to density, topology, and inter-layer mismatches. Moreover, both the MLs and MLas exhibit multi-stability, with the faster layer exhibiting a remanent periodic phase-locked oscillation, irrespective of the topology and inter-layer connectivity. In addition, remnant synchrony between nodes with homomorphic relationships is observed in the slower layer. Overall, we propose that symmetric inter-layer connections should be preferable for achieving intra-layer synchronization-regardless of global synchronization-and for sustaining permanent memory in multilayer networks with mismatched nodes across layers. However, to mitigate AD at low coupling values and layer mismatch, asymmetric inter-layer connectivity is more advantageous.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Does Bottom-up Beat Top-down in Hierarchical Community Detection?</title>
<link>https://arxiv.org/abs/2306.00833</link>
<guid>https://arxiv.org/abs/2306.00833</guid>
<content:encoded><![CDATA[
<div> Hierarchical clustering, networks, bottom-up algorithm, hierarchical stochastic block model, exact recovery<br /><br />Summary:<br /><br />1. This article focuses on hierarchical clustering of networks, aiming to find a tree representation of communities where each level reveals increasingly finer community structures.<br /><br />2. Two main approaches are discussed: divisive (top-down) algorithms that recursively split nodes until no further subdivision is needed, and agglomerative (bottom-up) algorithms that start with the smallest communities and merge them using linkage methods.<br /><br />3. The authors establish theoretical guarantees for recovering the hierarchical tree and community structure within a Hierarchical Stochastic Block Model using a bottom-up approach.<br /><br />4. The bottom-up algorithm is proven to achieve the information-theoretic threshold for exact recovery at intermediate hierarchy levels, expanding the feasible parameter space compared to top-down methods.<br /><br />5. Numerical experiments on synthetic and real-world networks demonstrate the superior performance of bottom-up algorithms over top-down ones, which may produce dendrogram inversions.<br /><br />6. These results enhance the understanding of hierarchical clustering techniques and suggest that bottom-up algorithms provide better recovery conditions and more consistent dendrogram structures in network analysis tasks. <div>
arXiv:2306.00833v3 Announce Type: replace 
Abstract: Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive (top-down) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative (bottom-up) algorithms first identify the smallest community structure and then repeatedly merge the communities using a linkage method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for top-down algorithms. This shows that bottom-up algorithms extend the feasible region for achieving exact recovery at intermediate levels. Numerical experiments on both synthetic and real data sets confirm the superiority of bottom-up algorithms over top-down algorithms. We also observe that top-down algorithms can produce dendrograms with inversions. These findings contribute to a better understanding of hierarchical clustering techniques and their applications in network analysis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Network-Based Measure of Cosponsorship Influence on Bill Passing in the United States House of Representatives</title>
<link>https://arxiv.org/abs/2406.19554</link>
<guid>https://arxiv.org/abs/2406.19554</guid>
<content:encoded><![CDATA[
<div> Keywords: influence, cosponsorship, US Congress, bill passage, network analysis  

<br /><br />Summary:  
This paper investigates the role of accrued influence among members of the US House of Representatives in determining the passage of legislative bills. The authors develop a novel measure of influence based on cosponsorship dynamics, where influence between two Congress members is recognized when they cosponsor a bill that achieves a defined threshold of legislative success. By constructing and analyzing a cosponsorship network, the study captures the relational patterns that signify influence in Congress. The research demonstrates that properties of this network serve as meaningful signals to explain why some bills pass while others do not. Importantly, the proposed influence measure outperforms traditional off-the-shelf centrality metrics in predicting bill outcomes. This suggests that standard network centrality measures may not fully capture the nuanced peer influences operative in congressional decision-making. The findings highlight the importance of cosponsor relationships and accumulated influence in shaping legislative success, offering insights into political behavior and legislative strategy within the US House. The study contributes to political network analysis by providing a tailored metric for influence that better aligns with legislative realities and outcomes. <div>
arXiv:2406.19554v2 Announce Type: replace 
Abstract: Each year, the United States Congress considers thousands of legislative proposals to select bills to present to the US President to sign into law. Naturally, the decision processes of members of Congress are subject to peer influence. In this paper, we examine the effect on bill passage of accrued influence between US Congress members in the US House of Representatives. We explore how the influence of a bill's cosponsors affects the bill's outcome (specifically, whether or not it passes in the House). We define a notion of influence by analyzing the structure of a network that we construct using cosponsorship dynamics. We award `influence' between a pair of Congress members when they cosponsor a bill that achieves some amount of legislative success. We find that properties of the bill cosponsorship network can be a useful signal to examine influence in Congress; they help explain why some bills pass and others fail. We compare our measure of influence to off-the-shelf centrality measures and conclude that our influence measure is more indicative of bill passage.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentivized Network Dynamics in Digital Job Recruitment</title>
<link>https://arxiv.org/abs/2410.09698</link>
<guid>https://arxiv.org/abs/2410.09698</guid>
<content:encoded><![CDATA[
<div> Keywords: Independent Halting Cascade, passive candidates, network diffusion, recruitment, agent-based model<br /><br />Summary:<br /><br />This article addresses the challenge of recruiting passive candidates in digital recruitment, those who are not actively seeking jobs but may consider compelling opportunities. Motivated by industry collaboration, the authors introduce the Independent Halting Cascade (IHC) model, an agent-based framework integrating network diffusion and potential halting via job applications. In the IHC, agents can recommend job vacancies to peers or apply themselves; incentives boost recommendations, thus activating passive candidates. The model unites concepts from social network diffusion, coordinated task completion, and labor economics by incorporating heterogeneous skills, job specifics, and network structures including homophily. Analytical boundaries are derived to distinguish between diffusion success and failure regimes. Simulations demonstrate that the IHC replicates empirical chain-length distributions observed in classic social experiments (Travers and Milgram; Dodds) with minimal calibration. Tests on synthetic (ER, BA, homophilic) and real networks (SMS, e-mail, Twitter) reveal that the IHC achieves comparable or higher recruitment success rates than direct-recommendation approaches while requiring fewer applicants. Overall, the IHC model captures essential mechanisms behind coordinated task completion and provides a theoretical and practical foundation for recruitment systems aiming to engage passive candidates effectively. <div>
arXiv:2410.09698v3 Announce Type: replace 
Abstract: Recruiting passive candidates, i.e., individuals not actively seeking jobs but open to compelling opportunities, remains one of the hardest challenges in digital recruitment. Motivated by a real collaboration with an industry partner, we introduce the Independent Halting Cascade (IHC) model: a simple but rich agent-based framework that couples network diffusion with the possibility of halting through job applications. Agents can either recommend vacancies to peers or apply themselves, and incentives increase the likelihood of recommendation, mobilizing otherwise passive candidates. The IHC bridges research on social network diffusion, coordinated task completion, and labor economics by modeling heterogeneous skills, job specificities, and network structures, including homophily. We derive analytical boundaries that characterize diffusion and failure regimes, and we show, through simulations, that the IHC reproduces the empirical chain-length distributions of Travers and Milgram, and of Dodds, with only coarse calibration. Across synthetic (ER, BA, homophilic) and real networks (SMS, e-mail, Twitter), the IHC achieves comparable or higher success rates than direct-recommendation baselines, while requiring fewer applicants. Our findings suggest that the IHC captures core mechanisms of coordinated task completion, offering both a theoretical contribution and a practical foundation for recruitment systems designed to reach and engage passive candidates.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in translation: using global fact-checks to measure multilingual misinformation prevalence, spread, and evolution</title>
<link>https://arxiv.org/abs/2310.18089</link>
<guid>https://arxiv.org/abs/2310.18089</guid>
<content:encoded><![CDATA[
<div> Misinformation, Multilingual, Fact-checking, Cross-lingual diffusion, Claim evolution<br /><br />Summary:<br /><br />This paper examines the prevalence and dynamics of multilingual misinformation by analyzing 264,487 fact-checks across 95 languages. It represents fact-checked claims using multilingual sentence embeddings and constructs a graph linking semantically similar claims to track their evolution and mutation over time and languages. Quantitative results show that while most misinformation claims are fact-checked only once, 10.26% (over 27,000 claims) undergo multiple fact-checks, indicating repeated efforts. Importantly, 32.26% of these repeated claims cross language barriers, revealing the cross-lingual diffusion of misinformation. However, the spread predominantly occurs within the same language or language family, showing strong assortativity. Fact-checkers generally require more time to verify claims that have crossed languages. Through analysis of connected components and shortest paths in the claim graph, the study finds that misinformation drifts over time and changes more significantly when crossing languages, which challenges static claim-matching methods. The findings emphasize the need for enhanced global collaboration among fact-checkers and highlight the essential role of localized verification to combat evolving misinformation effectively. <div>
arXiv:2310.18089v2 Announce Type: replace-cross 
Abstract: Misinformation and disinformation are growing threats in the digital age, affecting people across languages and borders. However, no research has investigated the prevalence of multilingual misinformation and quantified the extent to which misinformation diffuses across languages. This paper investigates the prevalence and dynamics of multilingual misinformation through an analysis of 264,487 fact-checks spanning 95 languages. To study the evolution of claims over time and mutations across languages, we represent fact-checks with multilingual sentence embeddings and build a graph where semantically similar claims are linked. We provide quantitative evidence of repeated fact-checking efforts and establish that claims diffuse across languages. Specifically, we find that while the majority of misinformation claims are only fact-checked once, 10.26%, corresponding to more than 27,000 claims, are checked multiple times. Using fact-checks as a proxy for the spread of misinformation, we find 32.26% of repeated claims cross linguistic boundaries, suggesting that some misinformation permeates language barriers. However, spreading patterns exhibit strong assortativity, with misinformation more likely to spread within the same language or language family. Next we show that fact-checkers take more time to fact-check claims that have crossed language barriers and model the temporal and cross-lingual evolution of claims. We analyze connected components and shortest paths connecting different versions of a claim finding that claims gradually drift over time and undergo greater alteration when traversing languages. Misinformation changes over time, reducing the effectiveness of static claim matching algorithms. The findings advocate for expanded information sharing between fact-checkers globally while underscoring the importance of localized verification.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Language Models Detect Misinformation in Scientific News Reporting?</title>
<link>https://arxiv.org/abs/2402.14268</link>
<guid>https://arxiv.org/abs/2402.14268</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific misinformation, large language models, SciNews dataset, prompt engineering, automated detection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of automatically detecting misinformation in scientific reporting within popular press articles, a problem highlighted during the COVID-19 pandemic due to differences in writing styles between scientific literature and media reports.<br />2. Traditional claim verification approaches require significant expert human effort to generate claims, which is impractical in real-world settings where explicit claims may not be available.<br />3. To overcome this, the authors introduce SciNews, a new labeled dataset containing 2,400 scientific news stories sourced from both trustworthy and untrustworthy outlets, paired with related scientific abstracts from the CORD-19 database. This dataset includes both human-written and LLM-generated news articles, reflecting emerging trends in content creation.<br />4. The authors identify various dimensions of scientific validity in news articles and propose integrating these aspects into the automated detection of false or misleading scientific information.<br />5. Several baseline detection architectures leveraging large language models such as GPT-3.5, GPT-4, and Llama2 (7B and 13B) are explored using advanced prompt engineering techniques including zero-shot, few-shot, and chain-of-thought prompting to effectively identify misinformation in scientific news reporting. <div>
arXiv:2402.14268v2 Announce Type: replace-cross 
Abstract: Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database. Our dataset includes both human-written and LLM-generated news articles, making it more comprehensive in terms of capturing the growing trend of using LLMs to generate popular press articles. Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation. We propose several baseline architectures using LLMs to automatically detect false representations of scientific findings in the popular press. For each of these architectures, we use several prompt engineering strategies including zero-shot, few-shot, and chain-of-thought prompting. We also test these architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B, Llama2-13B.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Shifting Landscape of Vaccine Discourse: Insights From a Decade of Pre- to Post-COVID-19 Vaccine Posts on Social Media</title>
<link>https://arxiv.org/abs/2511.16832</link>
<guid>https://arxiv.org/abs/2511.16832</guid>
<content:encoded><![CDATA[
<div> Keywords: vaccine discourse, social media, COVID-19, sentiment analysis, stereotype content model

<br /><br />Summary:  
This study examines English-language vaccine discourse on social media platform X (formerly Twitter) over a ten-year period spanning 2013 to 2022, encompassing seven years before and three years after the COVID-19 outbreak. By applying theories from social cognition and the stereotype content model in social psychology, the authors explore how vaccine-related narratives have evolved among English speakers online. A novel dataset of 18.7 million curated vaccine-related posts was created from an initial pool of 129 million, carefully filtered and preprocessed to capture changes across pre-pandemic and pandemic phases. The analysis reveals that the COVID-19 pandemic triggered intricate shifts in users' sentiments and language around vaccines. Notably, the usage of negative emotion words decreased during the pandemic, while words associated with surprise and trust increased. Early in the pandemic, vaccine discussions showed more warmth-focused language related to trustworthiness and competence, signaling a positive narrative shift. However, toward the pandemic's end, there was a resurgence of negative word usage, which the authors suggest may reflect rising vaccine hesitancy and skepticism within the online discourse. Overall, the study highlights dynamic changes in sentiment and emotional framing in vaccine conversations on social media across a significant timeframe. <div>
arXiv:2511.16832v1 Announce Type: new 
Abstract: In this work, we study English-language vaccine discourse in social media posts, specifically posts on X (formerly Twitter), in seven years before the COVID-19 outbreak (2013 to 2019) and three years after the outbreak was first reported (2020 to 2022). Drawing on theories from social cognition and the stereotype content model in Social Psychology, we analyze how English speakers talk about vaccines on social media to understand the evolving narrative around vaccines in social media posts. To do that, we first introduce a novel dataset comprising 18.7 million curated posts on vaccine discourse from 2013 to 2022. This extensive collection-filtered down from an initial 129 million posts through rigorous preprocessing-captures both pre-COVID and COVID-19 periods, offering valuable insights into the evolution of English-speaking X users' perceptions related to vaccines. Our analysis shows that the COVID-19 pandemic led to complex shifts in X users' sentiment and discourse around vaccines. We observe that negative emotion word usage decreased during the pandemic, with notable rises in usage of surprise, and trust related emotion words. Furthermore, vaccine-related language tended to use more warmth-focused words associated with trustworthiness, along with positive, competence-focused words during the early days of the pandemic, with a marked rise in negative word usage towards the end of the pandemic, possibly reflecting a growing vaccine hesitancy and skepticism.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Toxicity to Conformity: Adaptive user behavior to social norms in Telegram communities</title>
<link>https://arxiv.org/abs/2511.17333</link>
<guid>https://arxiv.org/abs/2511.17333</guid>
<content:encoded><![CDATA[
<div> toxicity, social media, community norms, conformity, user behavior<br /><br />Summary: This study investigates toxic and antisocial behavior on social media by focusing on the influence of local community norms. Utilizing six large-scale datasets with over 500 million Telegram messages collected from 2015 to 2024, the research examines toxic user behavior across various chats and languages. A novel methodological framework is introduced, employing a conformity index to classify user behaviors as conformist, anti-conformist, or independent. Findings reveal that the majority of users adapt their level of toxicity to align with the normative toxicity of the chat groups they participate in. These conformity patterns are consistent regardless of language or dataset, underscoring the strong effect of social influence in online environments. Additionally, increased user participation correlates with stronger conformity, indicating that exposure to community norms reinforces toxic behavior adaptation. The study highlights the importance of understanding toxic behavior in its social context and suggests that content moderation strategies should consider these local normative influences to be more effective. This holistic approach advances the comprehension of how toxic expressions are shaped by social environments on digital platforms. <div>
arXiv:2511.17333v1 Announce Type: new 
Abstract: Toxic and antisocial user behavior on social media platforms has received considerable scholarly attention due to its detrimental effects on society. This study takes a holistic perspective on the phenomenon of online toxicity by investigating the impact of local community norms on toxic expression. By using six large-scale datasets, comprising over 500 million Telegram messages collected between 2015 and 2024, we analyze toxic user behavior across multiple chats and languages. We introduce a methodological framework that models user adaptation through a conformity index, capturing conformist, anti-conformist, and independent behavioral tendencies. Our findings show that most users tend to conform to local normative environments, adjusting their toxicity to match the toxicity levels of the chats in which they participate. These patterns are consistent across datasets and languages, suggesting that community norms and social influence play a decisive role in shaping user behavior online. Furthermore, we demonstrate that exposure to these norms, in terms of increased user participation in chats, is associated with a stronger tendency toward conformity with the surrounding social contexts. Collectively, these findings contribute to a deeper understanding of toxic online behavior and highlight the importance of contextualized approaches to content moderation.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling memory in time-respecting paths on temporal networks</title>
<link>https://arxiv.org/abs/2511.17108</link>
<guid>https://arxiv.org/abs/2511.17108</guid>
<content:encoded><![CDATA[
<div> memory, temporal networks, time-respecting paths, human proximity, diffusion dynamics<br /><br />Summary:<br /><br />This study focuses on human close-range proximity interactions, which significantly influence spreading processes such as knowledge diffusion, norm adoption, and infectious disease transmission. The authors model these processes using time-respecting paths within temporal networks, capturing the sequence and timing of interactions. They introduce a novel framework to quantify memory effects in these time-respecting paths and apply it to multiple empirical datasets of human proximity collected from diverse environments. The analysis reveals strong memory effects that persist across different settings and parameter choices, with findings statistically significant compared to memoryless null models. Additionally, the paper presents a generative model designed to create synthetic temporal graphs that incorporate memory characteristics. Using this model, the authors demonstrate that memory in time-respecting paths tends to slow down the diffusion speed of spreading processes on temporal networks, thereby impacting the overall dynamics of these processes. The results highlight the importance of accounting for memory when studying and simulating spreading phenomena in networks shaped by human proximity and temporal structure. <div>
arXiv:2511.17108v1 Announce Type: cross 
Abstract: Human close-range proximity interactions are the key determinant for spreading processes like knowledge diffusion, norm adoption, and infectious disease transmission. These dynamical processes can be modeled with time-respecting paths on temporal networks. Here, we propose a framework to quantify memory in time-respecting paths and evaluate it on several empirical datasets encoding proximity between humans collected in different settings. Our results show strong memory effects, robust across settings, model parameters, and statistically significant when compared to memoryless null models. We further propose a generative model to create synthetic temporal graphs with memory and use it to show that memory in time-respecting paths decreases the diffusion speed, affecting the dynamics of spreading processes on temporal networks.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Easy-access online social media metrics can foster the identification of misinformation sharing users</title>
<link>https://arxiv.org/abs/2408.15186</link>
<guid>https://arxiv.org/abs/2408.15186</guid>
<content:encoded><![CDATA[
<div> Misinformation, social media, tweet frequency, account age, misinformation spreaders<br /><br />Summary:<br /><br />This study addresses the challenge of identifying social media users likely to spread misinformation, a time-consuming process when relying on traditional data collection methods. The authors propose a low-barrier method that leverages easily accessible social network metrics on X (formerly Twitter), specifically average daily tweet count and account age, to distinguish potential misinformation spreaders. Their analysis reveals a positive correlation between higher tweet frequency and the sharing of low factuality content, while older account age is negatively associated with misinformation sharing. Additionally, the study uncovers that the impact of the number of accounts followed and total tweets produced varies depending on the user's follower count, indicating that user influence modulates these effects. The findings suggest that social media platforms and researchers can use these readily available metrics as an initial filter to identify users more prone to sharing misinformation. This approach provides a practical, scalable tool to enhance misinformation detection and mitigation efforts on social networks, helping to combat the broader issue of false information dissemination in online environments. <div>
arXiv:2408.15186v3 Announce Type: replace 
Abstract: Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is time-consuming and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via Semantics Completion and Decomposition</title>
<link>https://arxiv.org/abs/2407.07026</link>
<guid>https://arxiv.org/abs/2407.07026</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal sentiment analysis, sentiment discrepancy, semantics completion, semantics decomposition, cross-attention<br /><br />Summary:<br /><br />1. The paper addresses the challenge of detecting sentiments in multimodal social media posts where the image and text may express contradictory sentiments, termed as sentiment discrepancy.  
2. Existing methods largely use single-branch fusion structures that focus on consistent sentiment and neglect or only implicitly model differing sentiments between image and text, which limits performance.  
3. The authors propose a novel semantics Completion and Decomposition (CoDe) network to explicitly handle sentiment discrepancy by leveraging both the shared and exclusive semantics of image and text modalities.  
4. The semantics completion module enriches image and text representations by incorporating the semantics of in-image text, effectively bridging the sentiment gap between modalities.  
5. The semantics decomposition module employs exclusive projection and contrastive learning to disentangle and explicitly capture discrepant sentiment information present in each modality.  
6. Finally, a cross-attention mechanism fuses the complemented and decomposed representations, combining consistent and discrepant sentiment cues for improved sentiment classification.  
7. Extensive experiments on four datasets demonstrate that the CoDe network outperforms existing approaches and that each module contributes to the overall effectiveness of the framework. <div>
arXiv:2407.07026v2 Announce Type: replace-cross 
Abstract: With the proliferation of social media posts in recent years, the need to detect sentiments in multimodal (image-text) content has grown rapidly. Since posts are user-generated, the image and text from the same post can express different or even contradictory sentiments, leading to potential \textbf{sentiment discrepancy}. However, existing works mainly adopt a single-branch fusion structure that primarily captures the consistent sentiment between image and text. The ignorance or implicit modeling of discrepant sentiment results in compromised unimodal encoding and limited performance. In this paper, we propose a semantics Completion and Decomposition (CoDe) network to resolve the above issue. In the semantics completion module, we complement image and text representations with the semantics of the in-image text, helping bridge the sentiment gap. In the semantics decomposition module, we decompose image and text representations with exclusive projection and contrastive learning, thereby explicitly capturing the discrepant sentiment between modalities. Finally, we fuse image and text representations by cross-attention and combine them with the learned discrepant sentiment for final classification. Extensive experiments on four datasets demonstrate the superiority of CoDe and the effectiveness of each proposed module.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the Complexities of Responsibly Sharing NSFW Content Online</title>
<link>https://arxiv.org/abs/2511.15726</link>
<guid>https://arxiv.org/abs/2511.15726</guid>
<content:encoded><![CDATA[
<div> Keywords: NSFW content, Reddit, non-consensual sharing, RoBERTa model, adult social platforms<br /><br />Summary:<br /><br />This paper investigates the management of Not Safe For Work (NSFW) content on Reddit, focusing on the top 15 NSFW-restricted subreddits by size. It highlights that Reddit is one of the few mainstream platforms permitting edge content, including NSFW material, a category growing also on other platforms like X. Users commonly leverage these NSFW subreddits as gateways to private or specialized adult platforms such as Telegram, Kik, and OnlyFans for extended interactions. The study reveals that image "trades" occur directly through payment methods like credit cards, PayPal, Bitcoin, or Venmo. A concerning finding is the presence of linguistic signals indicative of non-consensual content sharing within these communities. To address moderation challenges, the authors developed a RoBERTa-based classification model to detect non-consensual sharing, outperforming GPT-4 as well as traditional classifiers like logistic regression and random forest. This model thus provides a more effective tool for platforms seeking to responsibly moderate adult content. The researchers have made the source code and trained model publicly accessible for wider application and enhancement at their GitHub repository. <div>
arXiv:2511.15726v1 Announce Type: new 
Abstract: Reddit is in the minority of mainstream social platforms that permit posting content that may be considered to be at the edge of what is permissible, including so-called Not Safe For Work (NSFW) content. However, NSFW is becoming more common on mainstream platforms, with X now allowing such material. We examine the top 15 NSFW-restricted subreddits by size to explore the complexities of responsibly sharing adult content, aiming to balance ethical and legal considerations with monetization opportunities. We find that users often use NSFW subreddits as a social springboard, redirecting readers to private or specialized adult social platforms such as Telegram, Kik or OnlyFans for further interactions. They also directly negotiate image "trades" through credit cards or payment platforms such as PayPal, Bitcoin or Venmo. Disturbingly, we also find linguistic cues linked to non-consensual content sharing. To help platforms moderate such behavior, we trained a RoBERTa-based classification model, which outperforms GPT-4 and traditional classifiers such as logistic regression and random forest in identifying non-consensual content sharing, demonstrating superior performance in this specific task. The source code and trained model weights are publicly available at https://github.com/socsys/15NSFW Subreddits.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disagreement is Disappearing on U.S. Cable Debate Shows</title>
<link>https://arxiv.org/abs/2511.15774</link>
<guid>https://arxiv.org/abs/2511.15774</guid>
<content:encoded><![CDATA[
<div> Polarization, cable news, disagreement, partisanship, debate<br /><br />Summary:<br /><br />This study analyzes over 21,000 episodes from 24 major U.S. cable news opinion shows on Fox News, MSNBC, and CNN spanning 2010-2024, using a large-language-model classifier to label 2.13 million speaker turn-pairs for agreement and disagreement. First, it finds a significant decline in the amount of on-air disagreement, with debate proportions dropping by roughly one-third from 2017 to 2024, indicating a retreat from genuine discussion. Second, the analysis reveals a strong partisan asymmetry: conservatives on Fox News rarely receive pushback, liberals on MSNBC face little challenge, and CNN is trending toward a neutral midpoint, highlighting entrenched echo chambers on these networks. Third, highly polarizing topics such as abortion, gun rights, and immigration attract the least dissent, suggesting that these contentious issues further discourage cross-cutting debate. The research provides a new public corpus and an open-source stance detection pipeline, marking the first longitudinal evidence that prime-time cable news "debate" has transformed into partisan affirmation platforms. This shift undermines the media’s role in fostering pluralism by eroding cross-cutting societal cleavages, thereby amplifying affective political polarization in the United States. <div>
arXiv:2511.15774v1 Announce Type: new 
Abstract: Prime-time cable news programs are a highly influential part of the American media landscape, with top-rated opinion shows attracting millions of politically attentive viewers each night. In an era of intense political polarization, a critical question is whether these widely-watched "debate" shows foster genuine discussion or have devolved into partisan echo chambers that deepen societal divides. While these programs claim to air competing viewpoints, no large-scale evidence exists to quantify how often hosts and guests actually disagree. Measuring these exchanges is a significant challenge, as live broadcasts contain overlapping speakers, sarcasm, and billions of words of text. To address this gap, we construct the first speaker-resolved map of agreement and disagreement across U.S. cable opinion programming. Our study assembles over 21,000 episodes from 24 flagship shows on Fox News, MSNBC, and CNN from 2010-2024, segmenting them into host-guest turns and labeling 2.13 million turn-pairs using a high-fidelity large-language-model classifier. We present three findings: (1) the proportion of disagreement/debate on prime time shows a consistent downward trend, dropping by roughly one-third between 2017 and 2024; (2) on-air challenge is partisan and asymmetric--conservatives seldom face push-back on Fox, liberals seldom on MSNBC, with CNN declining toward the midpoint; and (3) polarizing issues such as abortion, gun rights, and immigration attract the least disagreement. The work contributes a public corpus, an open-source stance pipeline, and the first longitudinal evidence that televised "debate" is retreating from genuine discussion. By transforming into platforms for partisan affirmation, these shows erode the cross-cutting cleavages essential for a pluralistic society, thereby intensifying affective polarization.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Critical Adversarial Influence Blocking Maximization</title>
<link>https://arxiv.org/abs/2511.16068</link>
<guid>https://arxiv.org/abs/2511.16068</guid>
<content:encoded><![CDATA[
<div> Adversarial Influence Blocking, Time-Critical Independent Cascade, Submodularity, Bidirectional Influence Sampling, Approximation Guarantee

<br /><br />Summary:  
This paper addresses the Adversarial Influence Blocking Maximization (AIBM) problem by incorporating time constraints into influence propagation models, which is critical for scenarios like political campaigns or public emergencies. It introduces the Time-Critical Independent Cascade (TC-IC) model as an extension of the classical Independent Cascade model to incorporate these time factors. Further, it establishes the Time-Critical Adversarial Influence Blocking Maximization (TC-AIBM) framework to better manage influence blocking under time constraints. The authors provide a detailed problem formulation and rigorously prove the submodularity of the objective function under three different tie-breaking rules, a key property that was previously unexplored. To solve the TC-AIBM problem efficiently, the paper proposes the Bidirectional Influence Sampling (BIS) algorithm. Thanks to the submodularity, the BIS algorithm offers a strong approximation guarantee relative to the optimal solution. Extensive experiments conducted on four real-world datasets demonstrate the BIS algorithm's robustness across various negative seed sets, time constraints, and tie-breaking methods. Furthermore, BIS significantly outperforms existing state-of-the-art methods in both effectiveness and efficiency, achieving up to 1000 times speedup compared to the classic Greedy algorithm. <div>
arXiv:2511.16068v1 Announce Type: new 
Abstract: Adversarial Influence Blocking Maximization (AIBM) aims to select a set of positive seed nodes that propagate synchronously with the known negative seed nodes on the graph to counteract their negative influence. Currently, most AIBM studies are based on the classical Independent Cascade (IC) model, which omits the time factor and thus hinders their applications to time-critical scenarios like political campaigns or public emergencies. More importantly, existing AIBM studies have not investigated in-depth the submodularity of the objective function, resulting in their failure to provide a theoretical lower bound for the problem. To address these challenges, firstly, this paper proposes the Time-Critical Independent Cascade (TC-IC) model, which incorporates time constraints into the classical IC model. Secondly, the Time-Critical Adversarial Influence Blocking Maximization (TC-AIBM) is established to better handle time-critical scenarios. A detailed formulation of the problem is then presented, along with a theoretical proof of its submodularity under three different tie-breaking rules. Finally, a Bidirectional Influence Sampling (BIS) algorithm is proposed to solve the TC-AIBM problem. The submodularity of the objective function guarantees that the BIS can provide an approximation guarantee of
  to the optimal solution. Comprehensive experiments on four real-world datasets demonstrated that the proposed BIS algorithm exhibits excellent stability with various negative seeds, time constraints, and tie-breaking rules, outperforming state-of-the-art baselines. In addition, BIS improves efficiency by up to three orders of magnitude compared to the Greedy algorithm.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality</title>
<link>https://arxiv.org/abs/2511.16191</link>
<guid>https://arxiv.org/abs/2511.16191</guid>
<content:encoded><![CDATA[
<div> Keywords: Rumor detection, causal discovery, graph convolutional networks, Mamba sequence modeling, misinformation spread<br /><br />Summary: Rumor detection on social media is challenging due to complex propagation patterns and limited model interpretability. The proposed framework, CausalMamba, integrates Mamba-based sequence modeling with graph convolutional networks (GCNs) and differentiable causal discovery using NOTEARS. This integration enables the joint learning of temporal tweet sequences and reply graph structures while simultaneously uncovering latent causal graphs representing the spread dynamics. CausalMamba identifies influential nodes within rumor propagation chains, providing deeper insights into the misinformation diffusion process. Experiments conducted on the Twitter15 dataset demonstrate that CausalMamba achieves classification performance on par with state-of-the-art baselines. Beyond classification, the model supports counterfactual intervention analysis, allowing for simulated removal of key causal nodes to observe changes in graph connectivity. Qualitative evaluations reveal that eliminating top-ranked causal nodes significantly disrupts the propagation graphs, highlighting their critical role in rumor dynamics. Overall, CausalMamba offers a unified approach that combines rumor classification with influence analysis, advancing the interpretability and explainability of misinformation detection. This approach paves the way for more actionable and transparent tools in combating online rumor spread. <div>
arXiv:2511.16191v1 Announce Type: cross 
Abstract: Rumor detection on social media remains a challenging task due to the complex propagation dynamics and the limited interpretability of existing models. While recent neural architectures capture content and structural features, they often fail to reveal the underlying causal mechanisms of misinformation spread. We propose CausalMamba, a novel framework that integrates Mamba-based sequence modeling, graph convolutional networks (GCNs), and differentiable causal discovery via NOTEARS. CausalMamba learns joint representations of temporal tweet sequences and reply structures, while uncovering latent causal graphs to identify influential nodes within each propagation chain. Experiments on the Twitter15 dataset show that our model achieves competitive classification performance compared to strong baselines, and uniquely enables counterfactual intervention analysis. Qualitative results demonstrate that removing top-ranked causal nodes significantly alters graph connectivity, offering interpretable insights into rumor dynamics. Our framework provides a unified approach for rumor classification and influence analysis, paving the way for more explainable and actionable misinformation detection systems.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ART: A Graph-based Framework for Investigating Illicit Activity in Monero via Address-Ring-Transaction Structures</title>
<link>https://arxiv.org/abs/2511.16192</link>
<guid>https://arxiv.org/abs/2511.16192</guid>
<content:encoded><![CDATA[
<div> Keywords: Monero, cryptocurrency forensics, privacy, graph-based methodology, machine learning<br /><br />Summary:  
1. The paper addresses challenges in tracing illicit fund movements in privacy-centric cryptocurrencies, focusing on Monero due to its strong privacy and untraceability features.  
2. It highlights the difficulty law enforcement faces when analyzing criminal activities using Monero, emphasizing a need for new investigative strategies.  
3. The authors propose a novel graph-based methodology that constructs Address-Ring-Transaction graphs from already flagged Monero transactions linked to criminal activities.  
4. From these graphs, structural and temporal features are extracted to characterize transactional behaviors and patterns.  
5. These features are then utilized to train machine learning models aimed at detecting similar behavioral patterns indicative of criminal modus operandi.  
6. The approach represents an initial but important step toward developing analytical tools for law enforcement to support investigations within privacy-preserving blockchain ecosystems.  
7. The study contributes a case study demonstrating how combining graph analysis with machine learning can improve detection of illicit activities in Monero transactions.  
8. Overall, the work underscores the potential for advanced data science techniques to aid forensic efforts in cryptocurrencies with high privacy protections. <div>
arXiv:2511.16192v1 Announce Type: cross 
Abstract: As Law Enforcement Agencies advance in cryptocurrency forensics, criminal actors aiming to conceal illicit fund movements increasingly turn to "mixin" services or privacy-based cryptocurrencies. Monero stands out as a leading choice due to its strong privacy preserving and untraceability properties, making conventional blockchain analysis ineffective. Understanding the behavior and operational patterns of criminal actors within Monero is therefore challenging and it is essential to support future investigative strategies and disrupt illicit activities. In this work, we propose a case study in which we leverage a novel graph-based methodology to extract structural and temporal patterns from Monero transactions linked to already discovered criminal activities. By building Address-Ring-Transaction graphs from flagged transactions, we extract structural and temporal features and use them to train Machine Learning models capable of detecting similar behavioral patterns that could highlight criminal modus operandi. This represents a first partial step toward developing analytical tools that support investigative efforts in privacy-preserving blockchain ecosystems
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disparity-in-Differences: Extracting Hierarchical Backbones of Weighted Directed Networks</title>
<link>https://arxiv.org/abs/2511.16598</link>
<guid>https://arxiv.org/abs/2511.16598</guid>
<content:encoded><![CDATA[
<div> Keywords: disparity filter, hierarchical relations, weighted directed networks, backbone extraction, network dependency<br /><br />Summary:<br /><br />This paper addresses the challenge of extracting meaningful backbones from complex networks that exhibit heterogeneous and asymmetrical relations between nodes. Traditional disparity filters effectively identify important edges but do not account for node-to-node dependencies that may reflect hierarchical relationships. The authors propose an extended method called "disparity-in-differences" which captures asymmetric dependencies by measuring the disparity between normalized edge weight differences and their expected values. This synthetic relation highlights when one node depends more heavily on another, revealing underlying hierarchical structures. The method is evaluated across diverse real-world networks including a journal citation network, a U.S. airport network, the Enron email network, and a global trade network. Results demonstrate that the disparity-in-differences filter better uncovers hierarchical relations than the original disparity filter. Specifically, it aligns citation network structures with journal quality ratings, categorizes airports by hub size, identifies managerial levels in the email network, and exposes core-periphery country structures in world trade. This extended filter thus provides a powerful tool for enhanced backbone extraction that incorporates dependency patterns reflecting hierarchical organization in complex systems. <div>
arXiv:2511.16598v1 Announce Type: cross 
Abstract: Networks are useful representations for complex systems. Especially, heterogeneous and asymmetrical relations commonly found in complex systems can be converted to weighted directed edges between nodes. The disparity filter (Serrano et al., 2009) has successfully extracted backbones, sets of important edges, from empirical networks but is not designed to incorporate node-node dependency that may encode hierarchical relations. This paper proposes an extended disparity filter named "disparity-in-differences" that assigns a synthetic relation between two nodes if one depends relatively more on the other where the extent of asymmetric dependence is measured by the disparity between a normalized edge weight difference and an expected edge weight difference. For evaluation, the proposed method is applied to a journal citation network, a U.S. airport network, the Enron email network, and a world trade network. Compared to the disparity filter, the proposed approach better captures hierarchical relations that align well with journal quality ratings, airport hub categories by size, levels of management, and a core-periphery structure of countries, respectively.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks</title>
<link>https://arxiv.org/abs/2403.02011</link>
<guid>https://arxiv.org/abs/2403.02011</guid>
<content:encoded><![CDATA[
<div> Keywords: citizen science, sampling bias, plant-pollinator interactions, variational graph autoencoders, Hilbert-Schmidt Independence Criterion (HSIC)  

<br /><br />Summary:  
1. The article addresses the challenge of sampling bias in citizen science programs that collect data on plant-pollinator interactions.  
2. The authors propose a method to learn embeddings representing species interactions by modeling the probability of interaction between plants and pollinators.  
3. Their methodology adapts variational graph autoencoders specifically designed for bipartite graphs, suitable for encoding the relationships between two distinct groups (plants and pollinators).  
4. To reduce the impact of sampling bias, the approach integrates the Hilbert-Schmidt Independence Criterion (HSIC) to enforce independence from continuous variables linked to the sampling process, ensuring fairer representations of ecological data.  
5. The model’s performance is validated both through simulation studies that replicate key features of the sampling process and by application to real-world data from the Spipoll dataset, demonstrating effectiveness and practical applicability. <div>
arXiv:2403.02011v3 Announce Type: replace-cross 
Abstract: Citizen science monitoring programs can generate large amounts of valuable data, but are often affected by sampling bias. We focus on a citizen science initiative that records plant-pollinator interactions, with the goal of learning embeddings that summarize the observed interactions while accounting for such bias. In our approach, plant and pollinator species are embedded based on their probability of interaction. These embeddings are derived using an adaptation of variational graph autoencoders for bipartite graphs. To mitigate the influence of sampling bias, we incorporate the Hilbert-Schmidt Independence Criterion (HSIC) to ensure independence from continuous variables related to the sampling process. This allows us to integrate a fairness perspective, commonly explored in the social sciences, into the analysis of ecological data. We validate our method through a simulation study replicating key aspects of the sampling process and demonstrate its applicability and effectiveness using the Spipoll dataset.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random Abstract Cell Complexes</title>
<link>https://arxiv.org/abs/2406.01999</link>
<guid>https://arxiv.org/abs/2406.01999</guid>
<content:encoded><![CDATA[
<div> Keywords: random cell complexes, Erdős-Rényi model, cycle sampling, 2-dimensional cell complexes, importance sampling  

<br /><br />Summary:  
This article introduces a new probabilistic model for random abstract cell complexes (CCs), extending the Erdős-Rényi model from graphs and simplicial complexes to more general cell complexes. The model begins with sampling an Erdős-Rényi random graph, then incrementally adds higher-dimensional cells—specifically 2-cells—with specified probabilities. Since enumerating possible cells combinatorially explodes (e.g., 2-cells correspond to cycles or permutations in the graph), the authors develop an approximate sampling algorithm focused on two-dimensional complexes. Central to this is a spanning-tree-based method for efficiently sampling simple cycles, which also approximates the probability of occurrence of particular cycles. This method enables importance sampling to estimate various cycle-related graph statistics, such as the number of cycles of given lengths. By approximating these probabilities, the framework provides a practical way to control the expected count of sampled 2-cells. The work includes initial analysis of the properties of these random cell complexes generated from the model. Finally, the authors demonstrate applications including usage as null models in data analysis and for graph liftings to cell complexes. The algorithms for cycle sampling, cycle count estimation, and combined cell sampling are implemented and available in the Python package `py-raccoon`. <div>
arXiv:2406.01999v2 Announce Type: replace-cross 
Abstract: We define a model for random (abstract) cell complexes (CCs), similiar to the well-known Erd\H{o}s-R\'enyi model for graphs and its extensions for simplicial complexes. To build a random cell complex, we first draw from an Erd\H{o}s-R\'enyi graph, and consecutively augment the graph with cells for each dimension with a specified probability. As the number of possible cells increases combinatorially -- e.g., 2-cells can be represented as cycles, or permutations -- we derive an approximate sampling algorithm for this model limited to two-dimensional abstract cell complexes. As a basis for this algorithm, we first introduce a spanning-tree-based method that samples simple cycles and allows the efficient approximation of various properties, most notably the probability of occurence of a given cycle. This approximation is of independent interest as it enables the approximation of a wide variety of cycle-related graph statistics using importance sampling. We use this to approximate the number of cycles of a given length on a graph, allowing us to calculate the sampling probability to arrive at a desired expected number of sampled 2-cells. The probability approximation also trivially leads to a sampling algorithm for $2$-cells with a desired sampling probability. We provide some initial analysis into the properties of random CCs drawn from this model. We further showcase practical applications for our random CCs as null models, and in the context of (random) liftings of graphs to cell complexes. The cycle sampling, cycle count estimation, and combined cell sampling algorithms are available in the package `py-raccoon` on the Python Packaging Index.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion Dynamics Models for Sentiment Evolution in Weibo Blogs</title>
<link>https://arxiv.org/abs/2511.15303</link>
<guid>https://arxiv.org/abs/2511.15303</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment dynamics, emotional contagion, opinion formation, social networks, Weibo bloggers<br /><br />Summary:<br /><br />This study investigates how sentiment dynamics and emotional contagion evolve among followers of influential tech-focused bloggers on the Chinese social media platform Weibo over a six-month period. It highlights the importance of understanding these processes for influencers and marketers to optimize engagement, brand perception, and consumer behavior. By analyzing text-mined follower feedback, the researchers treat each blogger's audience as a "macro-agent" and discover that sentiment trajectories adhere to the principle of iterative averaging, a key mechanism in opinion formation models. The findings show strong alignment between observed sentiment evolution and modified French-DeGroot opinion-dynamics models that incorporate delayed perception and differentiate between expressed and private opinions. Additionally, the inferred influence structures among bloggers suggest interdependencies likely resulting from homophily, meaning emotionally similar users tend to subscribe to the same blogs and collectively influence community sentiment. The study thus advances understanding by combining sentiment analysis with dynamical systems theory and social network analysis, offering a richer explanation for how collective sentiment evolves within topic-focused online communities. This approach improves the capacity to model and predict audience reactions in social media settings. <div>
arXiv:2511.15303v1 Announce Type: new 
Abstract: Online social media platforms enable influencers to distribute content and quickly capture audience reactions, significantly shaping their promotional strategies and advertising agreements. Understanding how sentiment dynamics and emotional contagion unfold among followers is vital for influencers and marketers, as these processes shape engagement, brand perception, and purchasing behavior. While sentiment analysis tools effectively track sentiment fluctuations, dynamical models explaining their evolution remain limited, often neglecting network structures and interactions both among blogs and between their topic-focused follower groups. In this study, we tracked influential tech-focused Weibo bloggers over six months, quantifying follower sentiment from text-mined feedback. By treating each blogger's audience as a single "macro-agent", we find that sentiment trajectories follow the principle of iterative averaging -- a foundational mechanism in many dynamical models of opinion formation, a theoretical framework at the intersection of social network analysis and dynamical systems theory. The sentiment evolution aligns closely with opinion-dynamics models, particularly modified versions of the classical French-DeGroot model that incorporate delayed perception and distinguish between expressed and private opinions. The inferred influence structures reveal interdependencies among blogs that may arise from homophily, whereby emotionally similar users subscribe to the same blogs and collectively shape the shared sentiment expressed within these communities.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Equity in STEM: A Critical Analysis of NSF's Division for Equity and Excellence in STEM through Theoretical Lenses</title>
<link>https://arxiv.org/abs/2511.15217</link>
<guid>https://arxiv.org/abs/2511.15217</guid>
<content:encoded><![CDATA[
<div> Keywords: NSF, STEM equity, Critical Race Theory, racial hierarchies, diversity initiatives  

<br /><br />Summary: This paper critically examines the National Science Foundation's Division of Equity for Excellence in STEM, focusing on its mission to broaden participation among underrepresented groups. Despite supporting this objective, the study finds that current policies fall short of dismantling systemic barriers that persist in STEM fields. Employing frameworks such as Critical Race Theory and Mills's Racial Contract, the analysis reveals that some initiatives, though well-intentioned, inadvertently reinforce existing racial hierarchies through processes of commodification and exclusion. The research further critiques diversity efforts that prioritize competitiveness, arguing that these often fail to fully recognize the personhood and intellectual capabilities of marginalized students. Ultimately, the paper advocates for transformative reforms that extend beyond access and inclusion, urging a fundamental restructuring of STEM education environments. Such reforms would better align the National Science Foundation’s practices with its stated commitments to equity and inclusion, aiming to create genuinely supportive and affirming spaces for all students. <div>
arXiv:2511.15217v1 Announce Type: cross 
Abstract: This paper critically analyzes the National Science Foundation's Division of Equity for Excellence in STEM. While supporting its mission to broaden participation for underrepresented groups, the study finds current policies inadequate for dismantling systemic barriers. Using Critical Race Theory and Mills's Racial Contract, the analysis reveals how well-intentioned initiatives may reinforce racial hierarchies through commodification and exclusion. The research argues that diversity efforts focused on competitiveness often fail to affirm marginalized students' full personhood and intellectual capabilities. The paper concludes by advocating transformative reforms that move beyond access to fundamentally restructure STEM education environments, aligning NSF's practices with its equity and inclusion commitments.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided rewiring of social networks reduces polarization and accelerates collective action</title>
<link>https://arxiv.org/abs/2309.12141</link>
<guid>https://arxiv.org/abs/2309.12141</guid>
<content:encoded><![CDATA[
<div> Keywords: social polarization, agent-based models, link rewiring, cooperative consensus, network topology<br /><br />Summary:  
1. Sociopolitical polarization impedes collective cooperation on global social and ecological issues, which are collective action problems needing pro-mitigation norms.  
2. Previous agent-based modeling showed polarization naturally arises in structured social networks and that the rate at which polarized clusters dissolve limits the speed of consensus formation.  
3. This study investigates how guided link rewiring impacts depolarization dynamics across both synthetic and real-world social networks, including Facebook and Twitter topologies.  
4. Various heuristic rewiring algorithms were compared, representing random meetings, mutual acquaintance introductions, and community bridging, alongside topology-based link recommendation algorithms such as Who to Follow and node2vec.  
5. Findings show that heuristic algorithms outperform Who to Follow in fostering cooperative consensus, with homophilic rewiring effective only when opinion change is easy, whereas heterophilic rewiring works under broader conditions and can accelerate consensus formation by approximately 20% despite some backfiring interactions.  
6. Heterophilic rewiring significantly outperforms topology-based recommenders, while random rewiring performs consistently well, yielding higher steady-state cooperation than most complex algorithms tested.  
7. The variance in outcomes for topology-based recommenders highlights their instability across different network structures.  
8. Overall, the study identifies a nuanced interaction between network topology, rewiring strategies, and social depolarization, indicating potential for redesigning social networking technologies to promote social good. <div>
arXiv:2309.12141v2 Announce Type: replace-cross 
Abstract: Global social and ecological challenges represent collective action problems requiring rapid and sufficient cooperation with pro-mitigation norms. Sociopolitical polarization hinders such cooperation. Prior agent-based models showed polarization emerges naturally in structured social networks and polarized cluster dissolution rate limits consensus formation rate. Here we study how guided link rewiring affects depolarization dynamics across synthetic and empirical (Facebook, Twitter) network topologies. We compare heuristic rewiring algorithms representing random meetings, mutual acquaintance introductions, and community bridging, alongside topology-based link recommender algorithms (Who to Follow and node2vec). Our heuristic algorithms all outperform Who to Follow in generating cooperative consensus. Homophilic rewiring generates cooperative consensus when agents can easily change opinions. However, heterophilic rewiring achieves this over broader conditions and can accelerate cooperative consensus formation by ~20%, including where up to 33% of the population experiences backfiring interactions. Heterophilic rewiring also vastly outperforms topology-based recommender algorithms. Random rewiring performed consistently well, achieving higher steady-state cooperation than seven out of eight more complex algorithms. Large disparities in steady-state cooperation for topology-based recommender systems highlight their volatility across network structures. Overall, our work reveals a subtle interplay between topology, rewiring algorithm and social depolarization, suggesting strong potential for carefully redesigning social networking technologies for social good.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion formation at Ising social networks</title>
<link>https://arxiv.org/abs/2511.12786</link>
<guid>https://arxiv.org/abs/2511.12786</guid>
<content:encoded><![CDATA[
<div> Keywords: Ising social network, opinion formation, elite influence, phase transition, Monte Carlo process  

<br /><br />Summary:  
1. The study investigates opinion formation on an undirected Ising social network representing scientific collaborations, where each node corresponds to an individual with a binary opinion (up/red or down/blue).  
2. Some nodes hold fixed, opposing opinions considered as the "elite," whose influence spreads across the network, influencing the other nodes ("crowd") which are initially randomly assigned opinions with equal probability.  
3. The opinion update process is asynchronous and based on a Monte Carlo algorithm, where spins flip only if the weighted majority influence from neighboring nodes exceeds a certain conviction threshold.  
4. The influence amplitude of each node is self-consistently adjusted throughout the process, reflecting dynamic opinion strength.  
5. The system exhibits a phase transition driven by the increasing influence amplitude of the crowd spins, shifting dominance from the elite’s fixed opinions to the crowd's majority view, demonstrating that the elite’s ability to impose opinions can be overcome when crowd influence is sufficiently strong. <div>
arXiv:2511.12786v1 Announce Type: cross 
Abstract: We study the process of opinion formation in an Ising social network of scientific collaborations. The network is undirected. An Ising spin is associated with each network node being oriented up (red) or down (blue). Certain nodes carry fixed, opposite opinions whose influence propagates over the other spins, which are flipped according to the majority-influence opinion of neighbors of a given spin during the asynchronous Monte Carlo process. The amplitude influence of each spin is self-consistently adapted, and a flip occurs only if this majority influence exceeds a certain conviction threshold. All non-fixed spins are initially randomly distributed, with half of them oriented up and half down. Such a system can be viewed as a model of elite influence, coming from the fixed spins, on the opinions of the crowd of non-fixed spins. We show that a phase transition occurs as the amplitude influence of the crowd spins increases: the dominant opinion shifts from that of the elite nodes to a phase in which the crowd spins' opinion becomes dominant and the elite can no longer impose their views.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRITES: An integrative framework for investigating and assessing web-scraped HTTP-response datasets for research applications</title>
<link>https://arxiv.org/abs/2511.13773</link>
<guid>https://arxiv.org/abs/2511.13773</guid>
<content:encoded><![CDATA[
<div> Keywords: web-scraping, data quality, framework, PRITES, statistical analysis 

<br /><br />Summary: The proliferation of web-scraped datasets across various sectors has spurred discussions about their statistical quality and limitations. Existing literature on web-scraping is fragmented across disciplines like computer science and statistics, often leading to inconsistent definitions and quality assessments. To address these issues, the authors propose an integrated framework known as PRITES, which encompasses six stages: 'Plan', 'Retrieve', 'Investigate', 'Transform', 'Evaluate', and 'Summarise'. This framework is designed to help monitor quality factors during the web-scraping process and assess existing datasets' applicability. Each stage corresponds with technical and statistical challenges encountered in data collection and analysis. Additionally, the authors demonstrate the framework's utility through a case study involving the transformation of web-scraped data on retail prices of alcoholic beverages into analysis-ready formats for public health policy research. This application highlights how the PRITES framework can enhance the accuracy and comprehensiveness of reporting in studies that rely on web-scraped datasets, ultimately contributing to better-informed decision-making in research and practice. <div>
arXiv:2511.13773v1 Announce Type: cross 
Abstract: The ability to programmatically retrieve vast quantities of data from online sources has given rise to increasing usage of web-scraped datasets for various purposes across government, industry and academia. Contemporaneously, there has also been growing discussion about the statistical qualities and limitations of collecting from online data sources and analysing web-scraped datasets. However, literature on web-scraping is distributed across computer science, statistical methodology and application domains, with distinct and occasionally conflicting definitions of web-scraping and conceptualisations of web-scraped data quality. This work synthesises technical and statistical concepts, best practices and insights across these relevant disciplines to inform documentation during web-scraping processes, and quality assessment of the resultant web-scraped datasets.
  We propose an integrated framework to cover multiple processes during the creation of web-scraped datasets including 'Plan', 'Retrieve', 'Investigate', 'Transform', 'Evaluate' and 'Summarise' (PRITES). The framework groups related quality factors which should be monitored during the collection of new web-scraped data, and/or investigated when assessing potential applications of existing web-scraped datasets. We connect each stage to existing discussions of technical and statistical challenges in collecting and analysing web-scraped data. We then apply the framework to describe related work by the co-authors to adapt web-scraped retail prices for alcoholic beverages collected by an industry data partner into analysis-ready datasets for public health policy research. The case study illustrates how the framework supports accurate and comprehensive scientific reporting of studies using web-scraped datasets.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex-Weighted Convolutional Networks: Provable Expressiveness via Complex Diffusion</title>
<link>https://arxiv.org/abs/2511.13937</link>
<guid>https://arxiv.org/abs/2511.13937</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, complex weights, diffusion process, node-classification, CWCN

<br /><br />Summary: Graph Neural Networks (GNNs) are widely used but face challenges such as oversmoothing and subpar performance on heterophilic graphs. This paper introduces a new framework that utilizes complex weights on graph edges, allowing for a diffusion process that expands random walks into the complex domain. The authors demonstrate that this diffusion technique is highly expressive; with the right complex weights, any node-classification task can be addressed in the steady state of a complex random walk. Building on this foundational insight, the proposed Complex-Weighted Convolutional Network (CWCN) learns complex-weighted structures from the data itself. Additionally, it enriches the diffusion process with learnable matrices and nonlinear activations. CWCN is straightforward to implement and does not require extra hyperparameters beyond those of traditional GNNs, showcasing its practicality. The experimental results indicate that complex-weighted diffusion significantly enhances the expressiveness of GNNs. This framework not only addresses existing limitations but also fosters new theoretical and practical developments in GNN modeling, making it a substantial contribution to the field. <div>
arXiv:2511.13937v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across diverse applications, yet they remain limited by oversmoothing and poor performance on heterophilic graphs. To address these challenges, we introduce a novel framework that equips graphs with a complex-weighted structure, assigning each edge a complex number to drive a diffusion process that extends random walks into the complex domain. We prove that this diffusion is highly expressive: with appropriately chosen complex weights, any node-classification task can be solved in the steady state of a complex random walk. Building on this insight, we propose the Complex-Weighted Convolutional Network (CWCN), which learns suitable complex-weighted structures directly from data while enriching diffusion with learnable matrices and nonlinear activations. CWCN is simple to implement, requires no additional hyperparameters beyond those of standard GNNs, and achieves competitive performance on benchmark datasets. Our results demonstrate that complex-weighted diffusion provides a principled and general mechanism for enhancing GNN expressiveness, opening new avenues for models that are both theoretically grounded and practically effective.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data</title>
<link>https://arxiv.org/abs/2511.14098</link>
<guid>https://arxiv.org/abs/2511.14098</guid>
<content:encoded><![CDATA[
<div> Keywords: collaborative question-answering, hallucination, mean-field dynamics, randomized utility model, network of LLMs

<br /><br />Summary: This paper investigates how a network of interacting large language models (LLMs) can effectively conduct collaborative question-answering (CQA) to estimate a ground truth from a distributed set of documents. The study addresses the phenomenon of hallucination, where LLMs generate inaccurate information in the absence of direct evidence, which can propagate through an interconnected network, leading to a cascading effect where otherwise accurate models also begin to hallucinate. By leveraging concepts from mean-field dynamics (MFD) and a randomized utility model, the authors create a generative model for understanding the dynamics of these interactions. They introduce a latent state for the LLMs that indicates whether they are truthful, extending traditional models to explore how information diffuses across a directed network. The paper posits conditions for the existence and uniqueness of a fixed point in this model and analyzes its behavior in response to incentives provided to the LLMs. Finally, extensive experimental analysis is performed on a network of 100 open-source LLMs, considering factors such as data heterogeneity, node capability, network structure, and sensitivity to framing across various semi-synthetic datasets. <div>
arXiv:2511.14098v1 Announce Type: cross 
Abstract: In this paper, we model and analyze how a network of interacting LLMs performs collaborative question-answering (CQA) in order to estimate a ground truth given a distributed set of documents. This problem is interesting because LLMs often hallucinate when direct evidence to answer a question is lacking, and these effects become more pronounced in a network of interacting LLMs. The hallucination spreads, causing previously accurate LLMs to hallucinate. We study interacting LLMs and their hallucination by combining novel ideas of mean-field dynamics (MFD) from network science and the randomized utility model from economics to construct a useful generative model. We model the LLM with a latent state that indicates if it is truthful or not with respect to the ground truth, and extend a tractable analytical model considering an MFD to model the diffusion of information in a directed network of LLMs. To specify the probabilities that govern the dynamics of the MFD, we propose a randomized utility model. For a network of LLMs, where each LLM has two possible latent states, we posit sufficient conditions for the existence and uniqueness of a fixed point and analyze the behavior of the fixed point in terms of the incentive (e.g., test-time compute) given to individual LLMs. We experimentally study and analyze the behavior of a network of $100$ open-source LLMs with respect to data heterogeneity, node capability, network structure, and sensitivity to framing on multiple semi-synthetic datasets.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Trivial Edges: A Fractional Approach to Cohesive Subgraph Detection in Hypergraphs</title>
<link>https://arxiv.org/abs/2410.20350</link>
<guid>https://arxiv.org/abs/2410.20350</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, (k,g,p)-core model, cohesive subgraphs, pruning algorithms, subgraph detection<br /><br />Summary:  
This paper addresses the challenge of accurately identifying cohesive subgraphs within hypergraphs, which model complex relationships in areas such as social networks, transaction data, and recommendation systems. The existing (k,g)-core model assesses cohesiveness based on internal connections and co-occurrence but can be skewed by trivial hyperedges that artificially inflate cohesiveness metrics. To overcome this limitation, the authors introduce the $(k,g,p)$-core model, which integrates the relative importance of hyperedges, thereby offering a more precise detection of meaningful substructures. Two pruning algorithms are proposed: a Naïve algorithm and an Advanced pruning algorithm, both designed to efficiently filter out irrelevant components while preserving the integrity of detected cores. Experimental evaluation on real-world datasets demonstrates that the proposed methods significantly reduce the execution frequency of computationally expensive operations by 51.9%, highlighting the practical efficiency and effectiveness of the approach. Overall, this work contributes a refined core decomposition model coupled with optimized algorithms that improve subgraph cohesion detection and provide scalable solutions to complex hypergraph analysis problems. <div>
arXiv:2410.20350v2 Announce Type: replace 
Abstract: Hypergraphs serve as a powerful tool for modeling complex relationships across domains like social networks, transactions, and recommendation systems. The (k,g)-core model effectively identifies cohesive subgraphs by assessing internal connections and co-occurrence patterns, but it is susceptible to inflated cohesiveness due to trivial hyperedges. To address this, we propose the $(k,g,p)$-core model, which incorporates the relative importance of hyperedges for more accurate subgraph detection. We develop both Na\"ive and Advanced pruning algorithms, demonstrating through extensive experiments that our approach reduces the execution frequency of costly operations by 51.9% on real-world datasets.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying community evolution in temporal networks</title>
<link>https://arxiv.org/abs/2411.10632</link>
<guid>https://arxiv.org/abs/2411.10632</guid>
<content:encoded><![CDATA[
<div> Keywords: communities, temporal networks, Normalised Mutual Information, node variation, similarity measurement

<br /><br />Summary: The paper addresses the detection of communities in temporal networks and emphasizes the importance of understanding how these communities evolve over time. It critiques the traditional use of Normalised Mutual Information (NMI) which is effective only when nodes remain constant. To improve upon this, the authors propose two new measures: Union-Normalised Mutual Information (UNMI) and Intersection-Normalised Mutual Information (INMI). These extensions are designed to assess the similarity of community structures, even when the set of nodes changes, thus accommodating the dynamic nature of temporal networks. Experimental results indicate that UNMI and INMI effectively capture the evolving community structures in both synthetic and real temporal networks. The study not only introduces a novel similarity measurement method for network analysis but also enhances our understanding of community changes in complex temporal networks. Ultimately, the findings of this research contribute valuable insights into the dynamic behavior of communities, paving the way for further exploration in the field of network analysis. <div>
arXiv:2411.10632v3 Announce Type: replace 
Abstract: When we detect communities in temporal networks it is important to ask questions about how they change in time. Normalised Mutual Information (NMI) has been used to measure the similarity of communities when the nodes on a network do not change. We propose two extensions namely Union-Normalised Mutual Information (UNMI) and Intersection-Normalised Mutual Information (INMI). UNMI and INMI evaluate the similarity of community structure under the condition of node variation. Experiments show that these methods are effective in dealing with temporal networks with the changes in the set of nodes, and can capture the dynamic evolution of community structure in both synthetic and real temporal networks. This study not only provides a new similarity measurement method for network analysis but also helps to deepen the understanding of community change in complex temporal networks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTENN: A Spatiotemporal Graph Neural Framework for Community Discovery in Dynamic Social Networks</title>
<link>https://arxiv.org/abs/2501.12208</link>
<guid>https://arxiv.org/abs/2501.12208</guid>
<content:encoded><![CDATA[
<div> Keywords: community discovery, dynamic social networks, spatiotemporal, graph neural networks, embeddings

<br /><br />Summary: Community discovery is crucial for understanding dynamic social networks, but traditional algorithms often overlook complex spatial and temporal patterns. To overcome this, the authors introduce GTENN, a spatiotemporal graph neural framework designed for community discovery. GTENN combines spatial structure and temporal dynamics into a unified embedding process. Initially, Graph Convolutional Networks (GCNs) are utilized to capture latent spatial information and create expressive node representations for each time snapshot. Following this, Gated Recurrent Units (GRUs) model the temporal evolution of these node embeddings, effectively tracking changes and relationships over time. To cluster nodes and determine community affiliations, a Self-Organizing Map (SOM) is employed on the derived spatiotemporal embeddings. The efficacy of GTENN is demonstrated through experimental results from four types of dynamic networks, where it consistently outperforms traditional community discovery methods. Evaluation metrics such as purity, normalized mutual information, homogeneity, and completeness indicate GTENN's superior capability in identifying evolving community structures within dynamic social networks. This research highlights the importance of incorporating temporal and spatial dynamics in community detection algorithms. <div>
arXiv:2501.12208v2 Announce Type: replace 
Abstract: Community discovery is one of the key issues in the study of dynamic social networks. Traditional community discovery algorithms mainly focus on the formation and dissolution of links between nodes, and thus fail to capture richer spatial and temporal patterns underlying network evolution. To address this limitation, we propose GTENN, a spatiotemporal graph neural framework for community discovery in dynamic social networks. GTENN integrates spatial structure and temporal dynamics within a unified embedding architecture. First, Graph Convolutional Networks (GCN) are employed to aggregate latent spatial information and learn expressive node representations at each snapshot. Next, Gated Recurrent Units (GRU) are used to model temporal evolutions of node embeddings, effectively capturing node dynamism and relationship propagation across time. Finally, a Self-Organizing Map (SOM) is applied to the learned spatiotemporal embeddings to cluster nodes and infer their community affiliations. We conduct experiments on four types of dynamic networks, and the results show that GTENN consistently outperforms traditional community discovery algorithms in terms of purity, normalized mutual information, homogeneity, and completeness. These findings demonstrate the superior ability of GTENN to accurately uncover evolving community structures hidden in dynamic social networks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topic and Sentiment Trends in Semaglutide Discussions on X: Subpopulation-Based Longitudinal Analysis</title>
<link>https://arxiv.org/abs/2505.18432</link>
<guid>https://arxiv.org/abs/2505.18432</guid>
<content:encoded><![CDATA[
<div> Keywords: semaglutide, sentiment analysis, social media, user groups, pharmacovigilance

<br /><br />Summary: This study investigates the perception and discussion of semaglutide on social media platform X, focusing on user experiences influencing drug effectiveness. It analyzes 859,751 posts from July 2021 to April 2024, examining changes in sentiment and identifying key topics discussed. The overall mean sentiment was negative (-0.24), with all user groups showing declines over time. Main discussion topics included weight loss, side effects, costs, and the role of celebrities or politicians. Significant differences in sentiment were noted between user groups: organizational accounts had a mean sentiment of -0.04, while individuals reported -0.28, with this difference being statistically significant (P < .001). An interrupted time-series analysis revealed a notable sentiment drop from November 2022 to January 2023, correlating with regulatory announcements. Additionally, gender differences were evident as female users discussed celebrities and political figures more frequently (21%) than male users (17%), who showed more positive sentiment overall. The findings underscore the varied perceptions and discussions surrounding semaglutide among different user demographics, highlighting the implications for health communication and drug monitoring practices. <div>
arXiv:2505.18432v2 Announce Type: replace 
Abstract: Background: User experience strongly influences pharmaceutical drug effectiveness. Social media platforms like X have become major spaces where people share medication-related experiences, especially for widely marketed drugs such as semaglutide. Despite high activity online, how different user groups engage in semaglutide discussions remains unclear.
  Objective: This study examines how semaglutide is perceived and discussed across X user groups by analyzing (1) changes in sentiment over time and (2) key discussion topics.
  Methods: We collected 859,751 posts about semaglutide from July 2021 to April 2024, along with metadata. We performed sentiment analysis and topic modeling to evaluate patterns across user subpopulations and time periods.
  Results: The overall mean sentiment was -0.24, with all groups showing declines over time. Discussions focused on weight loss, side effects, costs, and celebrity or political influence. Organizational accounts expressed less negative sentiment (mean = -0.04) than individuals (mean = -0.28), a statistically significant difference (P < .001). An interrupted time-series analysis showed a sentiment drop between Nov 2022 and Jan 2023, coinciding with regulatory announcements. We also found gender differences: posts by female users contained more discussions of celebrities and politicians (21 percent) compared to male users (17 percent), while male users expressed more positive sentiment.
  Conclusions: This study highlights how diverse user groups perceive and discuss semaglutide. Although sentiment was broadly negative, important differences emerged across subpopulations. These findings have implications for health communication and pharmacovigilance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Graph Recommendation via Sparse Augmentation and Singular Adaptation</title>
<link>https://arxiv.org/abs/2511.11969</link>
<guid>https://arxiv.org/abs/2511.11969</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic recommendation, graph neural networks, fine-tuning, singular value decomposition, test-time augmentation

<br /><br />Summary: The paper discusses the significance of dynamic recommendation systems, which model user preferences based on historical interactions for personalized services. It highlights the effectiveness of pre-trained dynamic graph neural networks (GNNs) but identifies challenges in existing fine-tuning methods, particularly their high computational demands and issues arising from the long-tail distribution of node degrees, leading to insufficient representations for sparsely interacting nodes. To tackle these problems, the authors introduce GraphSASA, a novel method that enhances the fine-tuning process in dynamic recommendation systems. GraphSASA utilizes test-time augmentation to improve node representations by taking advantage of the similarities in node representation distributions attained during hierarchical graph aggregation. Furthermore, it implements singular value decomposition (SVD), allowing the original vector matrix to remain unchanged while directing fine-tuning efforts towards the derived singular value matrices. This approach significantly reduces the parameter burden during fine-tuning and enhances adaptability. Experimental results indicate that GraphSASA outperforms existing methods, achieving state-of-the-art results across three large-scale datasets, confirming its efficacy in improving dynamic recommendation performance. <div>
arXiv:2511.11969v1 Announce Type: new 
Abstract: Dynamic recommendation, focusing on modeling user preference from historical interactions and providing recommendations on current time, plays a key role in many personalized services. Recent works show that pre-trained dynamic graph neural networks (GNNs) can achieve excellent performance. However, existing methods by fine-tuning node representations at large scales demand significant computational resources. Additionally, the long-tail distribution of degrees leads to insufficient representations for nodes with sparse interactions, posing challenges for efficient fine-tuning. To address these issues, we introduce GraphSASA, a novel method for efficient fine-tuning in dynamic recommendation systems. GraphSASA employs test-time augmentation by leveraging the similarity of node representation distributions during hierarchical graph aggregation, which enhances node representations. Then it applies singular value decomposition, freezing the original vector matrix while focusing fine-tuning on the derived singular value matrices, which reduces the parameter burden of fine-tuning and improves the fine-tuning adaptability. Experimental results demonstrate that our method achieves state-of-the-art performance on three large-scale datasets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying and Minimizing Perception Gap in Social Networks</title>
<link>https://arxiv.org/abs/2511.12106</link>
<guid>https://arxiv.org/abs/2511.12106</guid>
<content:encoded><![CDATA[
<div> Keywords: perception gap index, majority illusion, spectral graph theory, community structure, link recommendation<br /><br />Summary: Social media's network structure can distort perceptions through phenomena such as the majority illusion and echo chambers. The authors introduce the perception gap index, a graph-based metric that measures the divergence between local and global opinions, generalizing the majority illusion to continuous opinion settings. Using spectral graph theory, they show that networks with higher connectivity are more robust against perception distortions. However, networks exhibiting strong community structures, modeled by stochastic block models, are more susceptible to these distortions. The study also addresses the optimization problem of minimizing the perception gap through link recommendation strategies under a fixed budget. They prove that no polynomial-time algorithm can achieve any bounded approximation ratio for this problem unless P = NP, highlighting its computational hardness. Despite this, the authors propose several efficient heuristic algorithms that perform near-optimally on real-world network data, offering practical solutions for reducing perception gaps in social networks. <div>
arXiv:2511.12106v1 Announce Type: new 
Abstract: Social media has transformed global communication, yet its network structure can systematically distort perceptions through effects like the majority illusion and echo chambers. We introduce the perception gap index, a graph-based measure that quantifies local-global opinion divergence, which can be viewed as a generalization of the majority illusion to continuous settings. Using techniques from spectral graph theory, we demonstrate that higher connectivity makes networks more resilient to perception distortion. Our analysis of stochastic block models, however, shows that pronounced community structure increases vulnerability. We also study the problem of minimizing the perception gap via link recommendation with a fixed budget. We prove that this problem does not admit a polynomial-time algorithm for any bounded approximation ratio, unless P = NP. However, we propose a collection of efficient heuristic methods that have been demonstrated to produce near-optimal solutions on real-world network data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Control Misinformation: a Closed-loop Approach for Misinformation Mitigation over Social Networks</title>
<link>https://arxiv.org/abs/2511.12393</link>
<guid>https://arxiv.org/abs/2511.12393</guid>
<content:encoded><![CDATA[
<div> Misinformation, recommender systems, sentiment analysis, control framework, user engagement  

<br /><br />Summary:  
Modern social networks use recommender systems that often prioritize user engagement over content accuracy, inadvertently amplifying misinformation. This paper introduces a control framework designed to reduce the spread of misinformation while sustaining user engagement by targeting content features commonly abused by false information, specifically extreme negative sentiment and novelty. The authors extend the Friedkin-Johnsen opinion dynamics model into a closed-loop system that simultaneously mitigates misinformation and maximizes engagement. Both model-free and model-based control approaches are tested, showing up to a 76% reduction in misinformation diffusion across various network structures. The simulation studies utilize the LIAR2 dataset, applying large language models to extract sentiment features. A key finding is that in networks with radical or extremist users, reducing misinformation can actually increase median engagement among the broader user base, indicating that content moderation can improve discourse quality for non-extremist participants. Overall, the proposed framework offers actionable strategies for platform operators to balance the trade-off between suppressing misinformation and maintaining healthy engagement levels, contributing to more trustworthy and engaging social media environments. <div>
arXiv:2511.12393v1 Announce Type: new 
Abstract: Modern social networks rely on recommender systems that inadvertently amplify misinformation by prioritizing engagement over content veracity. We present a control framework that mitigates misinformation spread while maintaining user engagement by penalizing content characteristics commonly exploited by false information, specifically, extreme negative sentiment and novelty. We extend the closed-loop Friedkin-Johnsen model to incorporate the mitigation of misinformation together with the maximization of user engagement. Both model-free and model-based control strategies demonstrate up to 76% reduction in misinformation propagation across diverse network configurations, validated through simulations using the LIAR2 dataset with sentiment features extracted via large language models. Analysis of engagement-misinformation trade-offs reveals that in networks with radical users, median engagement improves even as misinformation decreases, suggesting content moderation enhances discourse quality for non-extremist users. The framework provides practical guidance for platform operators in balancing misinformation suppression with engagement objectives.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designed to Spread: Generative Approaches to Enhance Information Diffusion</title>
<link>https://arxiv.org/abs/2511.12516</link>
<guid>https://arxiv.org/abs/2511.12516</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, information diffusion, content generation, reinforcement learning, influence indicator<br /><br />Summary:  
This paper addresses the transformative impact of social media on information access and social connections, emphasizing the importance of content expression in driving information diffusion. While existing research mainly focuses on network structures and identifying tipping points for viral spread, there is a lack of automated tools for generating content specifically optimized for virality within targeted audiences. To bridge this gap, the authors propose a novel task called DOCG (Diffusion Optimized Content Generation) and present an information enhancement algorithm designed to generate content that maximizes diffusion potential. A key component of their method is an influence indicator, which enables content-level diffusion assessment without relying on the underlying network topology, making it applicable in more general scenarios. The approach also includes an information editor that uses reinforcement learning to explore interpretable content editing strategies that maintain semantic integrity while tailoring content for better audience engagement. The editor leverages advanced generative models to produce both textual and visual content that is semantically faithful and audience-aware. Experiments conducted on real-world social media datasets along with a user study confirm that the proposed method significantly improves the effectiveness of content diffusion without losing the core meaning of the original content. This offers a practical and scalable solution for optimizing social media content for virality.<br /><br />Summary: <div>
arXiv:2511.12516v1 Announce Type: new 
Abstract: Social media has fundamentally transformed how people access information and form social connections, with content expression playing a critical role in driving information diffusion. While prior research has focused largely on network structures and tipping point identification, it provides limited tools for automatically generating content tailored for virality within a specific audience. To fill this gap, we propose the novel task of DOCG and introduce an information enhancement algorithm for generating content optimized for diffusion. Our method includes an influence indicator that enables content-level diffusion assessment without requiring access to network topology, and an information editor that employs reinforcement learning to explore interpretable editing strategies. The editor leverages generative models to produce semantically faithful, audience-aware textual or visual content. Experiments on real-world social media datasets and user study demonstrate that our approach significantly improves diffusion effectiveness while preserving the core semantics of the original content.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the filter bubble? Developing a research agenda for the protective filter bubble</title>
<link>https://arxiv.org/abs/2511.12873</link>
<guid>https://arxiv.org/abs/2511.12873</guid>
<content:encoded><![CDATA[
<div> Keywords: filter bubbles, echo chambers, misinformation, marginalized communities, press freedom

<br /><br />Summary: The concept of filter bubbles and echo chambers has become a focal point of interest among scholars, media organizations, and the public. Traditionally, filter bubbles are viewed negatively due to their ability to create information silos, amplify misinformation, and foster hatred and extremism. However, this commentary highlights that the protective benefits of filter bubbles, particularly for marginalized communities and individuals in countries with low press freedom, deserve further exploration. Despite the prevailing narrative that emphasizes the detrimental effects of filter bubbles, there is a significant gap in research regarding their potential to serve as digital safe spaces. These environments can provide support and solidarity, allowing underrepresented voices to connect and share experiences. The article calls for a re-evaluation of the filter bubble phenomenon, suggesting that future research should focus on understanding its dual nature—both harmful and beneficial. By doing so, scholars can gain a more nuanced perspective on how filter bubbles operate in various social contexts and their impact on diverse populations. This could ultimately contribute to a more balanced discourse around digital communication dynamics. <div>
arXiv:2511.12873v1 Announce Type: new 
Abstract: Filter bubbles and echo chambers have received global attention from scholars, media organizations, and the general public. Filter bubbles have primarily been regarded as intrinsically negative, and many studies have sought to minimize their influence. The detrimental influence of filter bubbles is well-studied. Filter bubbles may, for example, create information silos, amplify misinformation, and promote hatred and extremism. However, comparatively few studies have considered the other side of the filter bubble; its protective benefits, particularly to marginalized communities and those living in countries with low levels of press freedom. Through a review of the literature on digital safe spaces and protective filter bubbles, this commentary suggests that there may be a need to rethink the filter bubble, and it proposes several areas for future research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying points of interest taxonomies: mapping OpenStreetMap tags to the Foursquare category system</title>
<link>https://arxiv.org/abs/2511.13369</link>
<guid>https://arxiv.org/abs/2511.13369</guid>
<content:encoded><![CDATA[
<div> Keywords: Point of Interest, OpenStreetMap, Foursquare, taxonomy unification, urban analytics

<br /><br />Summary: The article addresses the challenges posed by the heterogeneity of Point of Interest (POI) taxonomies in integrating urban datasets and developing location-based services. OpenStreetMap (OSM) has a community-driven tagging system, while Foursquare (FS) features a curated hierarchical structure. To bridge the gap between these systems, the authors present a benchmark and mapping framework that aligns OSM tags with the FS taxonomy. This framework is openly available and enhances reproducibility and interoperability in urban analytics. The developed dataset includes an evaluation of text embedding and LLM-based alignment strategies, along with a pipeline for updating the mapping as OSM evolves. The methodology is structured around three key components: the creation of a manually curated benchmark to serve as a gold standard, evaluation of pretrained text embedding models for semantic alignment, and an LLM-based refinement stage to improve adaptability and robustness. This approach not only offers a solid reference resource but also serves as a practical tool for the community, with active applications in urban analytics, mobility studies, and smart city services, ultimately achieving scalable and reproducible taxonomy unification. <div>
arXiv:2511.13369v1 Announce Type: new 
Abstract: The heterogeneity of Point of Interest (POI) taxonomies is a persistent challenge for the integration of urban datasets and the development of location-based services. OpenStreetMap (OSM) adopts a flexible, community-driven tagging system, while Foursquare (FS) relies on a curated hierarchical structure. Here we present an openly available benchmark and mapping framework that aligns OSM tags with the FS taxonomy. This resource integrates the richness of community-driven OSM data with the hierarchical structure of FS, enabling reproducible and interoperable urban analytics. The dataset is complemented by an evaluation of embedding and LLM-based alignment strategies and a pipeline that supports scalable updates as OSM evolves. Together, these elements provide both a robust reference resource and a practical tool for the community. Our approach is structured around three components: the construction of a manually curated benchmark as a gold standard, the evaluation of pretrained text embedding models for semantic alignment between OSM tags and FS categories, and an LLM-based refinement stage that enhances robustness and adaptability. The proposed methodology provides a scalable and reproducible solution for taxonomy unification, with direct applications to urban analytics, mobility studies, and smart city services.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis</title>
<link>https://arxiv.org/abs/2511.12018</link>
<guid>https://arxiv.org/abs/2511.12018</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic safety, Post-Encroachment Time, multi-camera vision, real-time analysis, edge computing<br /><br />Summary:<br /><br />This paper introduces a novel multi-camera computer vision framework designed for real-time traffic safety assessment at signalized intersections, focusing on Post-Encroachment Time (PET) computation. The system was demonstrated at the intersection of H Street and Broadway in Chula Vista, California, utilizing four synchronized cameras to provide continuous visual coverage. Each frame is processed on NVIDIA Jetson AGX Xavier edge devices using YOLOv11 segmentation for accurate vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye view using homography matrices, enabling alignment across overlapping camera views. A new pixel-level PET algorithm is employed, which does not rely on fixed cells, allowing for fine-grained hazard visualization through dynamic heatmaps with a spatial resolution accurate to 3.3 square centimeters. All detected vehicle and PET data are timestamped and stored in an SQL database to support long-term monitoring. The framework achieves real-time throughput on edge devices with an average processing speed of 2.68 frames per second, producing an 800 x 800 pixel logarithmic heatmap. Results demonstrate the system’s capability to identify high-risk zones with sub-second precision. This study validates a decentralized, scalable, and replicable approach for high-resolution, real-time intersection safety evaluation suitable for intelligent transportation systems. <div>
arXiv:2511.12018v1 Announce Type: cross 
Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Aware Retrieval Augmentation for Dynamic Recommendation</title>
<link>https://arxiv.org/abs/2511.12495</link>
<guid>https://arxiv.org/abs/2511.12495</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic recommendation systems, graph neural networks, task-aware retrieval, temporal generalization, dynamic graphs<br /><br />Summary: Dynamic recommendation systems focus on providing personalized suggestions by modeling user-item interactions over time through temporal behavioral data. Recent advancements employ pre-trained dynamic graph neural networks (GNNs) to represent these interactions using temporal snapshot graphs. However, fine-tuning these models often suffers from generalization problems due to temporal discrepancies between pre-training and fine-tuning phases, hindering the capture of evolving user preferences. To overcome this, the paper presents TarDGR, a task-aware retrieval-augmented framework designed to improve generalization by combining task-aware modeling with retrieval augmentation. TarDGR introduces a Task-Aware Evaluation Mechanism that identifies semantically relevant historical subgraphs, which allows the creation of task-specific datasets without needing manual labels. It further proposes a Graph Transformer-based Task-Aware Model that integrates both semantic and structural encodings to evaluate subgraph relevance effectively. During inference, TarDGR retrieves and combines these task-aware subgraphs with the current query subgraph, enriching its representation and addressing temporal generalization challenges. Experimental results on multiple large-scale dynamic graph datasets show that TarDGR consistently outperforms existing state-of-the-art methods, demonstrating significant improvements in both accuracy and capability to generalize across time. <div>
arXiv:2511.12495v1 Announce Type: cross 
Abstract: Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation</title>
<link>https://arxiv.org/abs/2511.12922</link>
<guid>https://arxiv.org/abs/2511.12922</guid>
<content:encoded><![CDATA[
<div> Keywords: UniTok, large language model, tokenization, mixture-of-experts, recommender systems  

<br /><br />Summary: Large language model (LLM)-based recommender systems excel by addressing the gap between item and language spaces through item tokenization. Traditional methods necessitate separate models for each item domain, which restricts generalization and complicates the creation of a unified tokenization that maintains domain-specific nuances. The proposed UniTok framework overcomes these obstacles by employing a mixture-of-experts (MoE) architecture alongside several codebooks to transform items into discrete tokens. This approach aligns items from various domains into a common latent space using a shared encoder, directing them to domain-specific experts for capturing unique semantics while also utilizing a shared expert for common knowledge. Furthermore, a mutual information calibration mechanism is introduced to ensure balanced semantic representation across domains. Extensive experiments on diverse real-world datasets reveal that UniTok achieves significant enhancements of up to 51.89% over established benchmarks, demonstrates analytical soundness in its design and optimization, and maintains strong generalization capabilities across different domains without the need for per-domain retraining, a feat not achievable by current baselines. <div>
arXiv:2511.12922v1 Announce Type: cross 
Abstract: Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeding with Differentially Private Network Information</title>
<link>https://arxiv.org/abs/2305.16590</link>
<guid>https://arxiv.org/abs/2305.16590</guid>
<content:encoded><![CDATA[
<div> Keywords: differential privacy, influence maximization, PrEP distribution, contact tracing, sexual networks<br /><br />Summary:  
This study focuses on improving public health interventions like distributing preexposure prophylaxis (PrEP) for HIV prevention by identifying key individuals using seeding algorithms. Since reconstructing complete sexual activity networks is challenging due to privacy concerns, the authors consider influence samples obtained through contact tracing, which consists of observed sequences of sexual contacts without revealing the full network. The research addresses two primary challenges: safeguarding individual privacy in these influence samples and adapting seeding algorithms to work effectively with incomplete data. To this end, the authors investigate differential privacy guarantees in the context of influence maximization when data is gathered from randomly collected cascades. Building upon recent costly seeding methods, they propose new privacy-preserving algorithms that incorporate randomization either in the input data or the output results, providing formal bounds on the privacy loss incurred by each node. Theoretical analyses backed by simulations on both synthetic datasets and real-world sexual contact data demonstrate that algorithmic performance decreases gradually as the privacy budget becomes more restrictive. The findings highlight that centralized privacy models yield better privacy-utility trade-offs compared to local privacy approaches, offering practical routes for deploying privacy-aware interventions in sensitive networks. <div>
arXiv:2305.16590v5 Announce Type: replace 
Abstract: In public health interventions such as distributing preexposure prophylaxis (PrEP) for HIV prevention, decision makers often use seeding algorithms to identify key individuals who can amplify intervention impact. However, building a complete sexual activity network is typically infeasible due to privacy concerns. Instead, contact tracing can provide influence samples, observed sequences of sexual contacts, without full network reconstruction. This raises two challenges: protecting individual privacy in these samples and adapting seeding algorithms to incomplete data. We study differential privacy guarantees for influence maximization when the input consists of randomly collected cascades. Building on recent advances in costly seeding, we propose privacy-preserving algorithms that introduce randomization in data or outputs and bound the privacy loss of each node. Theoretical analysis and simulations on synthetic and real-world sexual contact data show that performance degrades gracefully as privacy budgets tighten, with central privacy regimes achieving better trade-offs than local ones.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuanTaxo: A Quantum Approach to Self-Supervised Taxonomy Expansion</title>
<link>https://arxiv.org/abs/2501.14011</link>
<guid>https://arxiv.org/abs/2501.14011</guid>
<content:encoded><![CDATA[
<div> Keywords: Taxonomy expansion, quantum-inspired framework, Hilbert space, hierarchical polysemy, embedding models<br /><br />Summary:<br /><br />1. The paper addresses the challenge of keeping taxonomies—hierarchical knowledge graphs used in web applications—relevant and up-to-date as web content rapidly expands. Manual taxonomy construction is labor-intensive, and existing taxonomies struggle to incorporate emerging information dynamically. <br /><br />2. Existing taxonomy expansion methods rely on classical word embeddings to represent entities, but these methods fall short in capturing hierarchical polysemy, where the meaning of an entity varies depending on its hierarchical position and context. <br /><br />3. To overcome these limitations, the authors propose QuanTaxo, a quantum-inspired approach that encodes entities in a Hilbert space and models interference effects between entities to produce richer, context-sensitive representations. <br /><br />4. The quantum framework enables better handling of context and hierarchical nuances within taxonomies, addressing the inherent polysemy that classical embeddings miss. <br /><br />5. Experimental results on five real-world benchmark datasets demonstrate QuanTaxo’s effectiveness, showing significant improvements over nine classical embedding-based baselines, with accuracy gains of 12.3%, Mean Reciprocal Rank (MRR) improvements of 11.2%, and 6.9% in Wu & Palmer metrics. <div>
arXiv:2501.14011v3 Announce Type: replace 
Abstract: A taxonomy is a hierarchical graph containing knowledge to provide valuable insights for various web applications. However, the manual construction of taxonomies requires significant human effort. As web content continues to expand at an unprecedented pace, existing taxonomies risk becoming outdated, struggling to incorporate new and emerging information effectively. As a consequence, there is a growing need for dynamic taxonomy expansion to keep them relevant and up-to-date. Existing taxonomy expansion methods often rely on classical word embeddings to represent entities. However, these embeddings fall short of capturing hierarchical polysemy, where an entity's meaning can vary based on its position in the hierarchy and its surrounding context. To address this challenge, we introduce QuanTaxo, a quantum-inspired framework for taxonomy expansion that encodes entities in a Hilbert space and models interference effects between them, yielding richer, context-sensitive representations. Comprehensive experiments on five real-world benchmark datasets show that QuanTaxo significantly outperforms classical embedding models, achieving substantial improvements of 12.3% in accuracy, 11.2% in Mean Reciprocal Rank (MRR), and 6.9% in Wu & Palmer (Wu&amp;P) metrics across nine classical embedding-based baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Clustering via Gradual Community Detection</title>
<link>https://arxiv.org/abs/2501.02036</link>
<guid>https://arxiv.org/abs/2501.02036</guid>
<content:encoded><![CDATA[
<div> Keywords: deep clustering, community detection, pseudo-label purity, self-supervision, neural networks<br /><br />Summary: Deep clustering aims to group data samples into homogeneous clusters but faces challenges due to insufficient supervision signals. This paper presents a novel clustering strategy based on gradual community detection. The method begins by dividing samples into several pseudo-communities, which are then merged over time to form actual clusters. This approach incorporates principles of cluster network analysis, allowing it to utilize global structural characteristics to improve the quality of cluster pseudo-labels. High pseudo-label purity is vital for enhancing self-supervised learning performance. The proposed strategy has been implemented using popular representation learning backbones and tested against benchmark image datasets. Experimental results indicate that this new approach significantly enhances state-of-the-art (SOTA) performance in deep clustering tasks. Additionally, the ablation study confirms that the community detection perspective effectively raises the purity of pseudo-labels, leading to better outcomes in self-supervised learning contexts. Overall, the paper demonstrates that leveraging community detection provides valuable insights into deep clustering, ultimately leading to improved results in clustering tasks. <div>
arXiv:2501.02036v2 Announce Type: replace-cross 
Abstract: Deep clustering is an essential task in modern artificial intelligence, aiming to partition a set of data samples into a given number of homogeneous groups (i.e., clusters). Recent studies have proposed increasingly advanced deep neural networks and training strategies for deep clustering, effectively improving performance. However, deep clustering generally remains challenging due to the inadequacy of supervision signals. Building upon the existing representation learning backbones, this paper proposes a novel clustering strategy of gradual community detection. It initializes clustering by partitioning samples into many pseudo-communities and then gradually expands clusters by community merging. Compared with the existing clustering strategies, community detection factors in the new perspective of cluster network analysis in the clustering process. The new perspective can effectively leverage global structural characteristics to enhance cluster pseudo-label purity, which is critical to the performance of self-supervision. We have implemented the proposed approach based on the popular backbones and evaluated its efficacy on benchmark image datasets. Our extensive experiments have shown that the proposed clustering strategy can effectively improve the SOTA performance. Our ablation study also demonstrates that the new network perspective can effectively improve community pseudo-label purity, resulting in improved self-supervision.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-team Formation System for Collaborative Crowdsourcing</title>
<link>https://arxiv.org/abs/2511.10926</link>
<guid>https://arxiv.org/abs/2511.10926</guid>
<content:encoded><![CDATA[
<div> Keywords: crowdsourcing, team formation, social network, heuristic optimization, Bayesian optimization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of forming teams for complex crowdsourcing tasks that require collaboration, emphasizing the importance of considering both worker compatibility and expertise.<br />2. It notes the constraints posed by budget limitations and the potential negative impact of excessively large team sizes on collaborative performance.<br />3. The authors propose a heuristic optimization algorithm that utilizes social network information, where historical collaboration is modeled as a network with edge weights representing explicit ratings of worker compatibility.<br />4. Through simulation experiments with synthetic data, Gaussian process regression is used to analyze the relationship between eight experimental parameters and evaluation metrics, facilitating a detailed understanding of the algorithm’s performance.<br />5. Experimental parameters were optimized sequentially using Bayesian optimization to generate data for regression.<br />6. Results show that evaluation values were notably low when team size limits, the social network’s degree mean, and task budgets were set low.<br />7. The proposed algorithm consistently outperformed a hill-climbing method under almost all tested conditions.<br />8. Optimal performance was observed when the simulated annealing temperature decrease rate was about 0.9, while attempts to smooth the objective function were ineffective.<br /><br />This study contributes a novel team formation approach leveraging social network data and advanced optimization techniques to enhance collaboration effectiveness in budget-constrained crowdsourcing environments. <div>
arXiv:2511.10926v1 Announce Type: new 
Abstract: For complex crowdsourcing tasks that require collaboration between multiple individuals, teams should be formed by considering both worker compatibility and expertise. Furthermore, the nature of crowdsourcing dictates the budget for tasks and workers' remuneration, and excessively large team sizes may reduce collaborative performance. To address these challenges, we propose a heuristic optimization algorithm that leverages social network information to simultaneously form teams with optimized worker compatibility for multiple tasks. In our approach, historical collaboration is represented as a social network, where the edge weights correspond to explicit ratings of worker compatibility. In a simulation experiment using synthetic data, we applied Gaussian process regression to examine the relationship between eight experimental parameters and evaluation values, thereby analyzing the output of the proposed algorithm. To generate the necessary data for regression, we ran the proposed algorithm with experimental parameters that were sequentially estimated using Bayesian optimization. Our experiments revealed that the evaluation values were extremely low when the team size limit, the degree mean of the social network, and the task budget were set to low values. The results also indicate that the proposed algorithm outperformed the hill-climbing method under almost all experimental conditions. In addition, the highest evaluation values were achieved when the simulated annealing temperature decrease rate was approximately 0.9, while smoothing the objective function proved ineffective.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural asymmetry as a fraud signature: detecting collusion with Heron's Information Coefficient</title>
<link>https://arxiv.org/abs/2511.10957</link>
<guid>https://arxiv.org/abs/2511.10957</guid>
<content:encoded><![CDATA[
<div> Keywords: fraud detection, public procurement, network analysis, Heron's Information Coefficient, collusion patterns<br /><br />Summary:<br /><br />1. The article addresses the challenge of fraud in public procurement, with a focus on Brazil’s Unified Health System, a large and decentralized network. <br /><br />2. It introduces Heron's Information Coefficient (HIC), a novel geometric measure designed to quantify deviations of subgraphs from the global structure within a network. <br /><br />3. Applied to over eight years of bidding data for medical supplies in Brazil, HIC effectively uncovers collusive patterns and structural shifts indicative of cartel-like coordinated behavior that traditional metrics may fail to detect. <br /><br />4. The Heron coefficient uniquely assesses the interaction between active and inactive subgraphs, offering insights beyond conventional robustness measures and highlighting subtle network dynamics tied to corruption. <br /><br />5. Synthetic experiments validate the strong performance of HIC across different corruption intensities and varying network sizes, suggesting its practical utility as a complementary tool for auditors and policymakers aiming to enhance procurement integrity without replacing legal or economic analyses. <div>
arXiv:2511.10957v1 Announce Type: new 
Abstract: Fraud in public procurement remains a persistent challenge, especially in large, decentralized systems like Brazil's Unified Health System. We introduce Heron's Information Coefficient (HIC), a geometric measure that quantifies how subgraphs deviate from the global structure of a network. Applied to over eight years of Brazilian bidding data for medical supplies, this measure highlights collusive patterns that standard indicators may overlook. Unlike conventional robustness metrics, the Heron coefficient focuses on the interaction between active and inactive subgraphs, revealing structural shifts that may signal coordinated behavior, such as cartel formation. Synthetic experiments support these findings, demonstrating strong detection performance across varying corruption intensities and network sizes. While our results do not replace legal or economic analyses, they offer an effective complementary tool for auditors and policymakers to monitor procurement integrity more effectively. This study demonstrates that simple geometric insight can reveal hidden dynamics in real-world networks better than other Information Theoretic metrics.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling and Control of Sustainable Transitions through Opinion-Behavior Coupling in Heterogeneous Networks</title>
<link>https://arxiv.org/abs/2511.11053</link>
<guid>https://arxiv.org/abs/2511.11053</guid>
<content:encoded><![CDATA[
<div> Keywords: sustainable behavior, opinion-adoption dynamics, multilayer network, electric vehicles, targeted interventions<br /><br />Summary:<br /><br />This paper presents a data-driven computational framework to model the spread of sustainable behaviors—in particular, the adoption of electric vehicles—within heterogeneous societies. The model uses multilayer networks where each node corresponds to a community defined by age group and mobility level, based on large-scale survey data from Northern Europe. It integrates three core mechanisms: behavioral contagion via social and informational diffusion, abandonment due to dissatisfaction, and feedback loops between opinions and adoption driven by social influence. By analyzing the system’s equilibrium points, the study identifies the conditions conducive to widespread adoption of sustainable technologies. Empirical calibration of the model creates synthetic populations and social similarity networks to simulate different intervention strategies. Two types of policy controls are explored: those targeting opinions through social network influences, and those aimed at improving user experience and mitigating dissatisfaction. Simulation results demonstrate that interventions addressing experience and dissatisfaction lead to more stable, long-term adoption compared to opinion-based policies. The findings provide actionable insights for designing effective sociotechnical transitions toward sustainability by leveraging targeted behavioral and social influence mechanisms. <div>
arXiv:2511.11053v1 Announce Type: new 
Abstract: Understanding how sustainable behaviors spread within heterogeneous societies requires the integration of behavioral data, social influence mechanisms, and structured approaches to control. In this paper, we propose a data-driven computational framework for coupled opinion-adoption dynamics in social systems. Each node in the multilayer network represents a community characterized by a specific age group and mobility level, derived from large-scale survey data on the predisposition to adopt electric vehicles in Northern Europe. The proposed model captures three mechanisms: behavioral contagion through social and informational diffusion, abandonment driven by dissatisfaction, and feedback between opinions and adoption levels through social influence. Analyzing the equilibrium points of the coupled system allows us to derive the conditions that enable large-scale adoption. We empirically calibrate the model using data to construct synthetic populations and social similarity networks, which we use to explore targeted interventions that promote sustainable transitions. Specifically, the analysis focuses on two types of control strategies: opinion-based policies, which act on the social network layer, and policies that aim to improve experience and reduce dissatisfaction. Simulation results show that the latter ensure more stable and long-term adoption, offering concrete insights for designing effective interventions in sociotechnical transitions toward sustainability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent space models for grouped multiplex networks</title>
<link>https://arxiv.org/abs/2511.11086</link>
<guid>https://arxiv.org/abs/2511.11086</guid>
<content:encoded><![CDATA[
<div> Keywords: multilayer networks, group-specific structure, latent space model, convex optimization, brain connectivity

<br /><br />Summary:  
The paper addresses the challenge of analyzing complex multilayer network datasets prevalent in fields such as neuroscience, social sciences, economics, and genetics. Traditional methods focus on common structures shared across all network layers or account for individual layer variations, but often overlook shared patterns within specific subsets of layers representing meaningful groups like treatment vs. control or patient subgroups. To fill this gap, the authors propose the GroupMultiNeSS model, designed to simultaneously extract shared, group-specific, and individual latent structures from networks defined on the same set of nodes. They establish the model's identifiability and develop a fitting approach based on convex optimization with a nuclear norm penalty. Theoretical guarantees for recovery of latent positions are provided under conditions of sufficient separation between the different latent subspaces. Extensive synthetic experiments demonstrate improved accuracy compared to existing models such as MultiNeSS, especially when group-level components are present. Additionally, application to a Parkinson's disease brain connectivity dataset shows that GroupMultiNeSS better highlights biologically relevant node-level differences between treatment and control groups, offering enhanced interpretability and potential for downstream tasks like hypothesis testing and visualization. <div>
arXiv:2511.11086v1 Announce Type: new 
Abstract: Complex multilayer network datasets have become ubiquitous in various applications, including neuroscience, social sciences, economics, and genetics. Notable examples include brain connectivity networks collected across multiple patients or trade networks between countries collected across multiple goods. Existing statistical approaches to such data typically focus on modeling the structure shared by all networks; some go further by accounting for individual, layer-specific variation. However, real-world multilayer networks often exhibit additional patterns shared only within certain subsets of layers, which can represent treatment and control groups, or patients grouped by a specific trait. Identifying these group-level structures can uncover systematic differences between groups of networks and influence many downstream tasks, such as testing and low-dimensional visualization. To address this gap, we introduce the GroupMultiNeSS model, which enables the simultaneous extraction of shared, group-specific, and individual latent structures from a sample of networks on a shared node set. For this model, we establish identifiability, develop a fitting procedure using convex optimization in combination with a nuclear norm penalty, and prove a guarantee of recovery for the latent positions as long as there is sufficient separation between the shared, group-specific, and individual latent subspaces. We compare the model with MultiNeSS and other models for multiplex networks in various synthetic scenarios and observe an apparent improvement in the modeling accuracy when the group component is accounted for. Experiment with the Parkinson's disease brain connectivity dataset demonstrates the superiority of GroupMultiNeSS in highlighting node-level insights on biological differences between the treatment and control patient groups.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An External Fairness Evaluation of LinkedIn Talent Search</title>
<link>https://arxiv.org/abs/2511.10752</link>
<guid>https://arxiv.org/abs/2511.10752</guid>
<content:encoded><![CDATA[
<div> Bias audit, LinkedIn Talent Search, ranking disparity, demographic inference, temporal stability<br /><br />Summary:  
This paper presents an independent third-party audit examining potential bias in LinkedIn's Talent Search ranking system, focusing specifically on gender and race attributes. The authors constructed a dataset by collecting extensive real-world Talent Search results across diverse occupational queries. They developed a robust labeling pipeline to infer users' demographic attributes to assess bias. Two exposure disparity metrics, deviation from group proportions and MinSkew, were employed to evaluate bias in the rankings. The analysis revealed under-representation of minority groups in top-ranked positions across many queries. The study investigated potential causes for these disparities and discussed why eliminating such bias, especially in early ranks, is challenging or sometimes impossible. Beyond static evaluations, the paper explored subgroup fairness over time, revealing disparities in exposure persistence and rank stability among demographic groups. Minority groups experienced greater volatility in their ranked positions over multiple days, which affects the likelihood of candidate selection. The findings were contextualized by comparing them to LinkedIn’s own published self-audits. Finally, the paper addresses methodological challenges of conducting black-box external evaluations, including limited system observability and inaccuracies in demographic inference. <div>
arXiv:2511.10752v1 Announce Type: cross 
Abstract: We conduct an independent, third-party audit for bias of LinkedIn's Talent Search ranking system, focusing on potential ranking bias across two attributes: gender and race. To do so, we first construct a dataset of rankings produced by the system, collecting extensive Talent Search results across a diverse set of occupational queries. We then develop a robust labeling pipeline that infers the two demographic attributes of interest for the returned users. To evaluate potential biases in the collected dataset of real-world rankings, we utilize two exposure disparity metrics: deviation from group proportions and MinSkew. Our analysis reveals an under-representation of minority groups in early ranks across many queries. We further examine potential causes of this disparity, and discuss why they may be difficult or, in some cases, impossible to fully eliminate among the early ranks of queries. Beyond static metrics, we also investigate the concept of subgroup fairness over time, highlighting temporal disparities in exposure and retention, which are often more difficult to audit for in practice. In employer recruiting platforms such as LinkedIn Talent Search, the persistence of a particular candidate over multiple days in the ranking can directly impact the probability that the given candidate is selected for opportunities. Our analysis reveals demographic disparities in this temporal stability, with some groups experiencing greater volatility in their ranked positions than others. We contextualize all our findings alongside LinkedIn's published self-audits of its Talent Search system and reflect on the methodological constraints of a black-box external evaluation, including limited observability and noisy demographic inference.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings</title>
<link>https://arxiv.org/abs/2511.10842</link>
<guid>https://arxiv.org/abs/2511.10842</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Graphs, Embeddings, Hyperbolic Space, Complex Space, Scalability<br /><br />Summary:<br /><br />1. Knowledge graphs serve as vital structures for representing complex relational data in diverse domains, but current embedding methods exhibit critical shortcomings in handling various relationship types at scale.<br />2. Euclidean embedding models have difficulties representing hierarchical data, vector space models struggle with asymmetric relations, and hyperbolic models are inadequate for symmetric relations.<br />3. The authors introduce HyperComplEx, a novel hybrid embedding framework that combines hyperbolic, complex, and Euclidean spaces through a learned attention mechanism to dynamically adapt to different relation types.<br />4. A relation-specific space weighting approach is used to select the most suitable geometry for each relation, supported by a multi-space consistency loss ensuring coherent joint predictions.<br />5. Extensive experiments on computer science knowledge graphs ranging from 1K to 10M papers demonstrate consistent improvements over state-of-the-art baselines such as TransE, RotatE, and ComplEx.<br />6. On the largest 10M-paper dataset, HyperComplEx achieves a mean reciprocal rank (MRR) of 0.612, which is a 4.8% relative improvement over the best baseline, while maintaining efficient training and fast inference performance (85 ms per triple).<br />7. The model exhibits near-linear scalability through adaptive dimension allocation and is accompanied by publicly released code and datasets to support reproducible research in scalable knowledge graph embeddings. <div>
arXiv:2511.10842v1 Announce Type: cross 
Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning</title>
<link>https://arxiv.org/abs/2511.11081</link>
<guid>https://arxiv.org/abs/2511.11081</guid>
<content:encoded><![CDATA[
<div> Heterogeneous Graph Neural Networks, Pre-computation, Label Leakage, Echoless Propagation, Partitioning

<br /><br />Summary: Heterogeneous Graph Neural Networks (HGNNs) are essential for learning on heterogeneous graphs but are often inefficient due to repetitive message passing during training. Pre-computation-based HGNNs improve efficiency by performing message passing just once in preprocessing, aggregating neighbor information into regular tensors to support mini-batch training. However, label-based pre-computation methods experience training label leakage or the "echo effect," where a node’s label information indirectly propagates back during multi-hop message passing, degrading performance. Existing solutions to this issue are either memory-intensive or incompatible with advanced message passing techniques. To overcome this, the paper introduces Echoless Label-based Pre-computation (Echoless-LP), which uses Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and ensures they gather label information only from neighbors in different partitions, thus preventing echo while remaining memory-efficient and broadly compatible. Additionally, the paper proposes an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to counteract information loss and distributional shifts caused by partitioning. Experimental results on public datasets reveal that Echoless-LP not only mitigates label leakage effectively but also achieves superior performance and memory efficiency compared to existing baseline methods. <div>
arXiv:2511.11081v1 Announce Type: cross 
Abstract: Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Optimization of Pairwise Polynomial Graph Spectral Functions via Subgraph Optimization</title>
<link>https://arxiv.org/abs/2511.11517</link>
<guid>https://arxiv.org/abs/2511.11517</guid>
<content:encoded><![CDATA[
<div> Distributed optimization, Laplacian spectrum, local subgraph problems, degree-regularization, learning-based proposer  

<br /><br />Summary:  
This work addresses the problem of distributed optimization for finite-degree polynomial Laplacian spectral objectives under a fixed graph topology and a global weight budget. Unlike prior approaches that focus on a few extreme eigenvalues, this study targets the collective behavior of the entire Laplacian spectrum. The authors reformulate the global cost into a bilinear form, enabling derivation of local subgraph problems whose gradient directions approximately align with the global descent direction via an SVD-based test on the matrix \(ZC\). This approach leads to an iterate-and-embed scheme operating over disjoint 1-hop neighborhoods, enforcing positivity and budget constraints by construction, and scaling efficiently to large geometric graphs. For objectives dependent on pairwise eigenvalue differences, a quadratic upper bound in the degree vector is obtained, motivating a warm start through degree-regularization. The warm start employs randomized gossip protocols to estimate the global average degree, accelerating local descent iterations while preserving decentralization and achieving about 95% of centralized optimization performance. Additionally, a learning-based proposer module is introduced to predict one-shot edge updates based on maximal 1-hop embeddings, yielding immediate objective improvements. Together, these components establish a modular, practical pipeline for decentralized, spectrum-aware weight tuning that respects constraints and is applicable to a broad class of whole-spectrum cost functions. <div>
arXiv:2511.11517v1 Announce Type: cross 
Abstract: We study distributed optimization of finite-degree polynomial Laplacian spectral objectives under fixed topology and a global weight budget, targeting the collective behavior of the entire spectrum rather than a few extremal eigenvalues. By re-formulating the global cost in a bilinear form, we derive local subgraph problems whose gradients approximately align with the global descent direction via an SVD-based test on the $ZC$ matrix. This leads to an iterate-and-embed scheme over disjoint 1-hop neighborhoods that preserves feasibility by construction (positivity and budget) and scales to large geometric graphs. For objectives that depend on pairwise eigenvalue differences $h(\lambda_i-\lambda_j)$, we obtain a quadratic upper bound in the degree vector, which motivates a ``warm-start'' by degree-regularization. The warm start uses randomized gossip to estimate global average degree, accelerating subsequent local descent while maintaining decentralization, and realizing $\sim95\%{}$ of the performance with respect to centralized optimization. We further introduce a learning-based proposer that predicts one-shot edge updates on maximal 1-hop embeddings, yielding immediate objective reductions. Together, these components form a practical, modular pipeline for spectrum-aware weight tuning that preserves constraints and applies across a broader class of whole-spectrum costs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protection Degree and Migration in the Stochastic SIRS Model: A Queueing System Perspective</title>
<link>https://arxiv.org/abs/2407.03159</link>
<guid>https://arxiv.org/abs/2407.03159</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, epidemic modeling, individual protection, migration, stochastic analysis  

<br /><br />Summary:  
This paper addresses the significant role of individual behaviors, especially self-protection and migration, in the spread of epidemics, which have often been overlooked in previous epidemic propagation models. Two models are proposed from individual and population perspectives to better capture these influences. The first model extends the classical SIRS framework by incorporating the individual protection degree as a stochastic variable, which effectively reduces the epidemic level. The second model adopts a population-level approach by constructing an open Markov queueing network to analyze the distribution of individuals among various epidemic states while accounting for migration dynamics to form an evolving population network. Stochastic methods are employed to rigorously analyze the behavior and outcomes of both models. Through a series of simulations, the study demonstrates infection probabilities, the count of individuals in each epidemic state, and their limiting distributions, providing valuable insights into how individual-level actions and mobility impact epidemic progression. The combined modeling approach offers a more comprehensive understanding of epidemic dynamics by integrating behavioral factors and migration effects, which can enhance epidemic control strategies. <div>
arXiv:2407.03159v2 Announce Type: replace 
Abstract: With the prevalence of COVID-19, the modeling of epidemic propagation and its analyses have played a significant role in controlling epidemics. However, individual behaviors, in particular the self-protection and migration, which have a strong influence on epidemic propagation, were always neglected in previous studies. In this paper, we mainly propose two models from the individual and population perspectives. In the first individual model, we introduce the individual protection degree that effectively suppresses the epidemic level as a stochastic variable to the SIRS model. In the alternative population model, an open Markov queueing network is constructed to investigate the individual number of each epidemic state, and we present an evolving population network via the migration of people. Besides, stochastic methods are applied to analyze both models. In various simulations, the infected probability, the number of individuals in each state and its limited distribution are demonstrated.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolving Network Modeling Driven by the Degree Increase and Decrease Mechanism</title>
<link>https://arxiv.org/abs/2407.08299</link>
<guid>https://arxiv.org/abs/2407.08299</guid>
<content:encoded><![CDATA[
<div> Keywords: evolving networks, preferential attachment, vertex degree, degree distribution, network growth and reduction  

<br /><br />Summary:  
This paper introduces a novel mechanism for evolving networks focusing on the dynamics of vertex degree, addressing limitations of traditional models like the Barabási-Albert (BA) scale-free network which emphasize only network growth. The authors construct a queueing system framework to describe both the increase and decrease of vertex degree, effectively capturing dynamic evolution including network reduction. The degree increase rate is modeled as a function positively correlated with the current degree of a vertex, thereby ensuring preferential attachment through a new perspective. Two functional forms of the degree increase rate are explored: one exhibiting a “long tail” behavior in degree distribution, and another that varies depending on parameter values, allowing flexibility in modeling different network behaviors. Theoretical degree distributions derived from the model are extensively compared with simulation results, demonstrating good agreement. Additionally, the model's applicability is tested on real network data, validating its effectiveness in capturing real-world network dynamics. Overall, this work provides a more comprehensive framework for understanding evolving networks by integrating both vertex degree increase and decrease mechanisms, expanding on existing preferential attachment theories. <div>
arXiv:2407.08299v2 Announce Type: replace 
Abstract: Ever since the Barab\'{a}si-Albert (BA) scale-free network has been proposed, network modeling has been studied intensively in light of the network growth and the preferential attachment (PA). However, numerous real systems are featured with a dynamic evolution including network reduction in addition to network growth. In this paper, we propose a novel mechanism for evolving networks from the perspective of vertex degree. We construct a queueing system to describe the increase and decrease of vertex degree, which drives the network evolution. In our mechanism, the degree increase rate is regarded as a function positively correlated to the degree of a vertex, ensuring the preferential attachment in a new way. Degree distributions are investigated under two expressions of the degree increase rate, one of which manifests a ``long tail'', and another one varies with different values of parameters. In simulations, we compare our theoretical distributions with simulation results and also apply them to real networks, which presents the validity and applicability of our model.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The evolution of cooperation in spatial public goods game with tolerant punishment based on reputation threshold</title>
<link>https://arxiv.org/abs/2412.17351</link>
<guid>https://arxiv.org/abs/2412.17351</guid>
<content:encoded><![CDATA[
<div> Keywords: reputation, punishment, public goods game, cooperation, spatial game<br /><br />Summary:<br />Reputation and punishment play crucial roles in regulating individual behavior within human societies, influencing the likelihood of imitation among individuals with good reputations and shaping how harmful behaviors are sanctioned. This study identifies a gap in traditional models that rely on pairwise interactions and standardized punishment rules, which fail to consider the varying reputations of groups. To address this, the authors introduce two major enhancements to a spatial public goods game: first, implementing a reputation threshold that restricts defection through punishment in low-reputation groups while permitting defection in high-reputation ones, thereby recognizing the social context of behavior; second, integrating both reputation and payoff into the calculation of individual fitness, ensuring that players with high reputations and payoffs are more likely to be imitated, in contrast to conventional approaches focusing solely on payoff. Through extensive simulations, the study demonstrates that higher reputation thresholds combined with stringent punishment significantly improve cooperation levels in the population. This novel mechanism sheds light on the persistent emergence of cooperation among individuals by acknowledging the importance of reputation dynamics alongside traditional evolutionary incentives. The findings have potential implications for designing better social and economic systems that promote cooperative behavior. <div>
arXiv:2412.17351v2 Announce Type: replace 
Abstract: Reputation and punishment are significant guidelines for regulating individual behavior in human society, and those with a good reputation are more likely to be imitated by others. In addition, society imposes varying degrees of punishment for behaviors that harm the interests of groups with different reputations. However, conventional pairwise interaction rules and the punishment mechanism overlook this aspect. Building on this observation, this paper enhances a spatial public goods game in two key ways: 1) We set a reputation threshold and use punishment to regulate the defection behavior of players in low-reputation groups while allowing defection behavior in high-reputation game groups. 2) Differently from pairwise interaction rules, we combine reputation and payoff as the fitness of individuals to ensure that players with both high payoff and reputation have a higher chance of being imitated. Through simulations, we find that a higher reputation threshold, combined with a stringent punishment environment, can substantially enhance the level of cooperation within the population. This mechanism provides deeper insight into the widespread phenomenon of cooperation that emerges among individuals.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impacts of Physical-Layer Information on Epidemic Spreading in Cyber-Physical Networked Systems</title>
<link>https://arxiv.org/abs/2503.06591</link>
<guid>https://arxiv.org/abs/2503.06591</guid>
<content:encoded><![CDATA[
<div> Information propagation, epidemic dynamics, physical-layer information, Microscopic Markov Chain Approach, 2-simplex interaction<br /><br />Summary:<br /><br />Since Granell et al. introduced a multiplex network model combining information and epidemic propagation, much research has focused on how information dissemination influences epidemic dynamics. This study introduces a novel information source termed physical-layer information, representing information acquired through physical interactions among individuals. Using the Microscopic Markov Chain Approach (MMCA), the authors derive the epidemic outbreak threshold and verify its accuracy through Monte Carlo (MC) simulations, which show consistent results. The research demonstrates that physical-layer information boosts the population’s awareness density and raises the infection threshold β_c, thereby reducing the overall infection density and suppressing epidemic spread. Additionally, when the density of 2-simplex information—representing higher-order group interactions—is sufficiently high, it acts similarly to pairwise interactions by significantly increasing awareness in the population, effectively preventing large-scale outbreaks. Moreover, the proposed model is applicable to cyber physical systems with comparable interaction mechanisms. The authors validate the model’s effectiveness by simulating it within a real grid system, highlighting its practical relevance and broad applicability in understanding and controlling epidemic spread influenced by complex information dynamics. <div>
arXiv:2503.06591v2 Announce Type: replace 
Abstract: Since Granell et al. proposed a multiplex network for information and epidemic propagation, researchers have explored how information propagation affects epidemic dynamics. However, the role of individuals acquiring information through physical interactions has received relatively less attention. In this work, we introduce a novel source of information: physical layer information, and derive the epidemic outbreak threshold using the Microscopic Markov Chain Approach (MMCA). Our simulation results indicate that the outbreak threshold derived from the MMCA is consistent with the Monte Carlo (MC) simulation results, thereby confirming the accuracy of the theoretical model. Furthermore, we find that the physical-layer information effectively increases the population's awareness density and the infection threshold $\beta_c$, while reducing the population's infection density, thereby suppressing the spreading of the epidemic. Another interesting finding is that when the density of 2-simplex information is relatively high, the 2-simplex plays a role similar to pairwise interaction, significantly enhancing the population's awareness density and effectively preventing large-scale epidemic outbreaks. In addition, our model works equally well for cyber physical systems with similar interaction mechanisms, while we simulate and validate it in a real grid system.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Technological Complexity Using Japanese Patents</title>
<link>https://arxiv.org/abs/2504.11932</link>
<guid>https://arxiv.org/abs/2504.11932</guid>
<content:encoded><![CDATA[
<div> Technology Identification, Corporate Level Analysis, Japanese Patents, Bipartite Network, Innovation Strategy<br /><br />Summary:<br /><br />As international technological competition grows, identifying key technologies for innovation becomes crucial but challenging due to the complexity and independence of technologies. The study proposes evaluating the Technology Complexity Index (TCI) at the corporate level, offering more detailed insights than conventional regional-level analyses. Utilizing Japanese patent data from fiscal years 1981 to 2010, the research constructs a bipartite network linking 1,939 corporations with technological fields classified into either 35 or 124 categories. This approach quantitatively characterizes each technology’s ubiquity and sophistication, revealing detailed technological trends aligned with broader societal contexts. The methodology proves stable even under finer technological classifications, ensuring robustness. Corporate-level analysis facilitates consistent comparisons across different regions and technologies, highlighting regional strengths and advantages in specific technological domains. Additionally, the study illustrates how corporate-level insights open up new opportunities for formulating targeted innovation strategies. Overall, this research advances the understanding of technological portfolios by offering fine-grained, quantitative methods that support decision-making at both corporate and regional scales, thereby aiding nations and organizations in fostering innovation through strategic technology identification. <div>
arXiv:2504.11932v5 Announce Type: replace 
Abstract: As international competition intensifies in technologies, nations need to identify key technologies to foster innovation. However, the identification is challenging due to the independent and inherently complex nature of technologies. Traditionally, analyses of technological portfolios have been limited to simple evaluations, indicating merely whether a technology is specialized. We propose evaluating TCI at the corporate level, which provides finer granularity and more detailed insights than conventional regional evaluations by using Japanese patent data spanning fiscal years 1981 to 2010. Specifically, we analyze a bipartite network composed of 1,939 corporations connected to technological fields categorized into either 35 or 124 classifications. Our findings quantitatively characterize the ubiquity and sophistication of each technological field, reveal detailed technological trends reflecting broader societal contexts, and demonstrate methodological stability even when employing finer technological classifications. Additionally, our corporate-level analysis allows consistent comparisons across different regions and technological fields, clarifying regional advantages in specific technologies. The corporate level analysis also reveals a new possibility for formulating an innovation strategy.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIS Epidemic Modelling on Homogeneous Networked System: General Recovering Process and Mean-Field Perspective</title>
<link>https://arxiv.org/abs/2505.12290</link>
<guid>https://arxiv.org/abs/2505.12290</guid>
<content:encoded><![CDATA[
<div> Disease Spread, Recovery Time, grp-SIS Model, Mean-Field Equations, Infection Dynamics<br /><br />Summary:<br /><br />This paper introduces the general recovering process susceptible-infected-susceptible (grp-SIS) model, an extension of the classic SIS model designed to incorporate arbitrary recovery time distributions for infected individuals. Unlike existing models that often assume exponential or memoryless recovery rates, this new approach allows for more realistic and diverse recovery time behaviors. The authors develop mean-field equations under the assumption of a homogeneous network structure to describe the disease dynamics. They provide explicit solutions for certain specified recovery time distributions, enabling deeper insights into how these distributions influence infection spread. The study further explores the probability density function (PDF) of infection times when the system reaches a steady state, emphasizing the critical role recovery time distributions play in shaping overall disease dynamics. Results indicate that non-exponential recovery significantly affects infection prevalence and persistence, suggesting the limitations of classical models. The paper concludes by proposing future research directions, including extending the model framework to incorporate arbitrary infection time distributions and applying the quasistationary method to better address discrepancies observed in numerical simulations. This work broadens the understanding of epidemic processes on complex networks by integrating more realistic recovery mechanisms. <div>
arXiv:2505.12290v2 Announce Type: replace 
Abstract: Although we have made progress in understanding disease spread in complex systems with non-Poissonian activity patterns, current models still fail to capture the full range of recovery time distributions. In this paper, we propose an extension of the classic susceptible-infected-susceptible (SIS) model, called the general recovering process SIS (grp-SIS) model. This model incorporates arbitrary recovery time distributions for infected nodes within the system. We derive the mean-field equations assuming a homogeneous network, provide solutions for specific recovery time distributions, and investigate the probability density function (PDF) for infection times in the system's steady state. Our findings show that recovery time distributions significantly affect disease dynamics, and we suggest several future research directions, including extending the model to arbitrary infection processes and using the quasistationary method to address deviations in numerical results.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</title>
<link>https://arxiv.org/abs/2505.12684</link>
<guid>https://arxiv.org/abs/2505.12684</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated graph learning, Graph foundation models, Knowledge entanglement, Domain-aware initialization, Adaptive domain-sensitive prompts<br /><br />Summary:<br /><br />1. Recent progress in graph machine learning highlights two emerging approaches: Federated graph learning (FGL) enables collaboration across multiple clients but struggles with data and task heterogeneity; Graph foundation models (GFM) generalize well across domains but are typically trained on single machines, missing cross-silo opportunities.<br />2. FedGFM is proposed as a novel decentralized training paradigm for GFMs that seeks to combine the strengths of FGL and GFM, addressing the limitations inherent in each.<br />3. A critical challenge identified is knowledge entanglement, where multi-domain knowledge becomes mixed in representations, which complicates downstream task adaptation.<br />4. To solve this, FedGFM+ introduces two core components: AncDAI, which uses global anchor-based domain-aware initialization by creating distinguishable domain-specific prototypes that guide pre-training initialization; and AdaDPP, a local adaptive domain-sensitive prompt pool where clients learn lightweight prompts that are pooled and selected during fine-tuning to enhance domain-specific adaptation.<br />5. Experimental results on eight diverse benchmarks across various domains and tasks demonstrate that FedGFM+ outperforms 20 competitive baselines covering supervised learning, federated graph learning, and federated GFM variants, confirming its effectiveness in reducing knowledge entanglement and improving downstream performance. <div>
arXiv:2505.12684v2 Announce Type: replace-cross 
Abstract: Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What did Elon change? A comprehensive analysis of Grokipedia</title>
<link>https://arxiv.org/abs/2511.09685</link>
<guid>https://arxiv.org/abs/2511.09685</guid>
<content:encoded><![CDATA[
<div> Keywords: Grokipedia, Wikipedia, article similarity, citation practices, content quality  

<br /><br />Summary:  
This paper presents the first comprehensive analysis of Grokipedia, an alternative online encyclopedia launched by Elon Musk in October 2025, comparing it extensively with English Wikipedia. The study finds that although Grokipedia's articles tend to be much longer than their Wikipedia counterparts, much of their content is highly derivative, drawing heavily from Wikipedia, regardless of licensing status. A key difference lies in citation practices: Grokipedia references a significantly higher number of sources considered unreliable or blacklisted by Wikipedia editors and external scholars, including controversial sites like Stormfront and Infowars. The authors analyze three subsets of articles—those about elected officials, controversial topics, and a random sample—to assess quality and similarity. Results show that the elected official and controversial topic articles display less similarity between Grokipedia and Wikipedia versions compared to other pages. The random subset reveals Grokipedia’s emphasis on rewriting the highest quality Wikipedia articles, with noticeable focus on topics such as biographies, politics, society, and history. To support further research, the authors publicly release a near-complete scrape of Grokipedia along with vector embeddings of its entire corpus. <div>
arXiv:2511.09685v1 Announce Type: new 
Abstract: Elon Musk released Grokipedia on 27 October 2025 to provide an alternative to Wikipedia, the crowdsourced online encyclopedia. In this paper, we provide the first comprehensive analysis of Grokipedia and compare it to a dump of Wikipedia, with a focus on article similarity and citation practices. Although Grokipedia articles are much longer than their corresponding English Wikipedia articles, we find that much of Grokipedia's content (including both articles with and without Creative Commons licenses) is highly derivative of Wikipedia. Nevertheless, citation practices between the sites differ greatly, with Grokipedia citing many more sources deemed "generally unreliable" or "blacklisted" by the English Wikipedia community and low quality by external scholars, including dozens of citations to sites like Stormfront and Infowars. We then analyze article subsets: one about elected officials, one about controversial topics, and one random subset for which we derive article quality and topic. We find that the elected official and controversial article subsets showed less similarity between their Wikipedia version and Grokipedia version than other pages. The random subset illustrates that Grokipedia focused rewriting the highest quality articles on Wikipedia, with a bias towards biographies, politics, society, and history. Finally, we publicly release our nearly-full scrape of Grokipedia, as well as embeddings of the entire Grokipedia corpus.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Misinformation Propagation in Social Networks using Large Language Models</title>
<link>https://arxiv.org/abs/2511.10384</link>
<guid>https://arxiv.org/abs/2511.10384</guid>
<content:encoded><![CDATA[
<div> Misinformation, Large Language Models, Personas, Information Fidelity, Misinformation Propagation

<br /><br />Summary:  
This article investigates misinformation spread on social media by modeling large language model (LLM) personas as synthetic agents that reflect user biases, ideological leanings, and trust heuristics. A novel auditor–node framework simulates the evolution of misinformation as news articles are sequentially rewritten by these persona-conditioned LLM nodes across networks. To track factual degradation, a question-answering-based auditor evaluates factual fidelity at each rewrite step, enabling claim-level analysis of misinformation drift. The authors introduce formal metrics: a misinformation index and misinformation propagation rate, which quantify the degree and speed of factual distortion over up to 30 sequential rewrites. Experiments with 21 distinct personas across 10 news domains reveal that personas aligned with identity and ideology tend to accelerate misinformation spread, with pronounced effects in politics, marketing, and technology sectors. In contrast, expert-driven personas help maintain factual accuracy and stability. Simulations further show that once misinformation begins, interactions among heterogeneous personas rapidly escalate distortions to propaganda levels. The study develops a taxonomy of misinformation severity, linking observed distortions to prevailing misinformation theories. Overall, the work highlights LLMs’ dual role as both emulators of human bias and effective auditors of information fidelity, providing an interpretable, empirically grounded framework for analyzing and mitigating misinformation diffusion in digital environments. <div>
arXiv:2511.10384v1 Announce Type: new 
Abstract: Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two Americas of Well-Being: Divergent Rural-Urban Patterns of Life Satisfaction and Happiness from 2.6 B Social Media Posts</title>
<link>https://arxiv.org/abs/2511.10542</link>
<guid>https://arxiv.org/abs/2511.10542</guid>
<content:encoded><![CDATA[
<div> Keywords: life satisfaction, happiness, rural-urban divide, social media, political effects

<br /><br />Summary:  
This study analyzes 2.6 billion geolocated social media posts from 2014 to 2022, applying a fine-tuned generative language model to create county-level indicators of life satisfaction and happiness across the United States. It identifies a rural-urban paradox where rural counties report higher life satisfaction (evaluative well-being), while urban counties show greater happiness (hedonic well-being). The research distinguishes these two subjective well-being dimensions, showing different spatial, political, and temporal patterns. Republican-leaning areas score higher on evaluative well-being, but political differences in happiness tend to diminish outside major metropolitan regions, suggesting context-dependent political influences. Temporal shocks prominently affect hedonic well-being, with happiness notably declining during the 2020-2022 period—likely reflecting the COVID-19 pandemic’s impact—while life satisfaction remains relatively steady. These findings hold across different model specifications (logistic and OLS) and align with established well-being theories. By leveraging large-scale, language-based data, the study offers a transparent and reproducible method that helps reconcile conflicting reports on rural-urban well-being disparities, supplementing traditional survey-based approaches. <div>
arXiv:2511.10542v1 Announce Type: new 
Abstract: Using 2.6 billion geolocated social-media posts (2014-2022) and a fine-tuned generative language model, we construct county-level indicators of life satisfaction and happiness for the United States. We document an apparent rural-urban paradox: rural counties express higher life satisfaction while urban counties exhibit greater happiness. We reconcile this by treating the two as distinct layers of subjective well-being, evaluative vs. hedonic, showing that each maps differently onto place, politics, and time. Republican-leaning areas appear more satisfied in evaluative terms, but partisan gaps in happiness largely flatten outside major metros, indicating context-dependent political effects. Temporal shocks dominate the hedonic layer: happiness falls sharply during 2020-2022, whereas life satisfaction moves more modestly. These patterns are robust across logistic and OLS specifications and align with well-being theory. Interpreted as associations for the population of social-media posts, the results show that large-scale, language-based indicators can resolve conflicting findings about the rural-urban divide by distinguishing the type of well-being expressed, offering a transparent, reproducible complement to traditional surveys.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual understanding boost in the WikiRace</title>
<link>https://arxiv.org/abs/2511.10585</link>
<guid>https://arxiv.org/abs/2511.10585</guid>
<content:encoded><![CDATA[
<div> Keywords: WikiRace, goal-directed search, semantic similarity, language models, navigation strategies<br /><br />Summary:<br /><br />1. The paper studies the WikiRace game, where players navigate between Wikipedia articles using hyperlinks, as a challenging example of goal-directed search in complex information networks.<br /><br />2. It evaluates multiple navigation strategies, comparing agents based on graph-theoretic measures (like betweenness centrality), semantic meaning derived from language model embeddings, and hybrid approaches combining both.<br /><br />3. Experiments conducted on a large subgraph of Wikipedia show that a greedy strategy guided purely by semantic similarity of article titles greatly outperforms other methods.<br /><br />4. This semantic similarity-driven greedy agent, when enhanced with a simple loop-avoidance mechanism, reaches a perfect success rate and completes navigation with efficiency roughly ten times better than structural or hybrid methods.<br /><br />5. The study concludes that relying solely on structural heuristics has critical limitations for goal-directed search, while large language models provide powerful, zero-shot semantic navigation capabilities that can transform searching in complex information spaces. <div>
arXiv:2511.10585v1 Announce Type: new 
Abstract: The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphSB: Boosting Imbalanced Node Classification on Graphs through Structural Balance</title>
<link>https://arxiv.org/abs/2511.10022</link>
<guid>https://arxiv.org/abs/2511.10022</guid>
<content:encoded><![CDATA[
<div> Imbalanced node classification, Graph Neural Networks, Structural Balance, minority-class nodes, relation diffusion  

<br /><br />Summary:  
Imbalanced node classification presents a significant challenge in graph learning, where majority-class nodes dominate and minority-class nodes are often underrepresented. Current methods mainly fall into two categories: data-level techniques that generate synthetic minority-class nodes, and algorithm-level strategies that optimize learning to prioritize minority classes. However, both approaches neglect the fundamental issue of imbalanced graph structure that leads to majority-class dominance. Addressing this gap, the paper introduces GraphSB (Graph Structural Balance), a novel framework that prioritizes structural balance before node synthesis. GraphSB includes a two-stage structure optimization process: first, Structure Enhancement adaptively constructs similarity-based edges to improve connectivity among minority-class nodes; second, Relation Diffusion captures higher-order dependencies and amplifies the influence of minority classes. This approach balances the structural distribution within the graph, enabling Graph Neural Networks to learn node representations more effectively. Extensive experiments show that GraphSB significantly outperforms state-of-the-art methods. Furthermore, Structural Balance can be easily integrated as a plug-and-play module into existing methods, boosting their accuracy by an average of 3.67%. This work provides both theoretical insights and practical tools to overcome the biases caused by imbalanced graph structures in node classification tasks. <div>
arXiv:2511.10022v1 Announce Type: cross 
Abstract: Imbalanced node classification is a critical challenge in graph learning, where most existing methods typically utilize Graph Neural Networks (GNNs) to learn node representations. These methods can be broadly categorized into the data-level and the algorithm-level. The former aims to synthesize minority-class nodes to mitigate quantity imbalance, while the latter tries to optimize the learning process to highlight minority classes. However, neither category addresses the inherently imbalanced graph structure, which is a fundamental factor that incurs majority-class dominance and minority-class assimilation in GNNs. Our theoretical analysis further supports this critical insight. Therefore, we propose GraphSB (Graph Structural Balance), a novel framework that incorporates Structural Balance as a key strategy to address the underlying imbalanced graph structure before node synthesis. Structural Balance performs a two-stage structure optimization: Structure Enhancement that adaptively builds similarity-based edges to strengthen connectivity of minority-class nodes, and Relation Diffusion that captures higher-order dependencies while amplifying signals from minority classes. Thus, GraphSB balances structural distribution before node synthesis, enabling more effective learning in GNNs. Extensive experiments demonstrate that GraphSB significantly outperforms the state-of-the-art methods. More importantly, the proposed Structural Balance can be seamlessly integrated into state-of-the-art methods as a simple plug-and-play module, increasing their accuracy by an average of 3.67\%.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reputation assimilation mechanism for sustaining cooperation</title>
<link>https://arxiv.org/abs/2511.10193</link>
<guid>https://arxiv.org/abs/2511.10193</guid>
<content:encoded><![CDATA[
<div> Keywords: reputation, cooperation, public goods game, social learning, adaptive synergy factor<br /><br />Summary:<br /><br />This study addresses the role of reputation not only as an individual attribute but as a reflection of the broader community's state, emphasizing its cumulative and socially transmissible aspects. The authors introduce a spatial public goods game model where an assimilated reputation metric simultaneously accounts for both an individual’s reputation and that of their neighbors. This approach captures the influence of local community reputation on cooperative behavior. Additionally, the model incorporates a reputation-dependent synergy factor that adjusts the group’s payoff quality based on the collective reputation, representing high or low group efficiency. Through extensive numerical simulations, the research explores how individual cooperation and extended reputation evolve together dynamically within the population. The findings indicate that the assimilated reputation mechanism fosters social learning by encouraging individuals to imitate those with higher reputations. Moreover, groups with higher reputations benefit from adaptive payoff multipliers, further promoting cooperative behavior. Ultimately, these mechanisms synergistically enhance the emergence and persistence of cooperation at the systemic level. The study highlights that considering reputation as both an individual and communal variable significantly changes the dynamics of cooperation, a perspective underexplored in earlier works. <div>
arXiv:2511.10193v1 Announce Type: cross 
Abstract: Keeping a high reputation, by contributing to common efforts, plays a key role in explaining the evolution of collective cooperation among unrelated agents in a complex society. Nevertheless, it is not necessarily an individual feature, but may also reflect the general state of a local community. Consequently, a person with a high reputation becomes attractive not just because we can expect cooperative acts with higher probability, but also because such a person is involved in a more efficient group venture. These observations highlight the cumulative and socially transmissible nature of reputation. Interestingly, these aspects were completely ignored by previous works. To reveal the possible consequences, we introduce a spatial public goods game in which we use an assimilated reputation simultaneously characterizing the individual and its neighbors' reputation. Furthermore, a reputation-dependent synergy factor is used to capture the high (or low) quality of a specific group. Through extensive numerical simulations, we investigate how cooperation and extended reputation co-evolve, thereby revealing the dynamic influence of the assimilated reputation mechanism on the emergence and persistence of cooperation. By fostering social learning from high-reputation individuals and granting payoff advantages to high-reputation groups via an adaptive multiplier, the assimilated reputation mechanism promotes cooperation, ultimately to the systemic level.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating the Ethics of Internet Measurement: Researchers' Perspectives from a Case Study in the EU</title>
<link>https://arxiv.org/abs/2511.10408</link>
<guid>https://arxiv.org/abs/2511.10408</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet measurement, ethical challenges, privacy, ethics review, responsible research<br /><br />Summary:<br /><br />This study investigates how Internet measurement researchers navigate ethical decisions in their everyday research practices. The authors conducted interviews with 16 researchers from a European Internet measurement group to gain insight into common ethical challenges faced in the field. Five main ethical challenges were identified: managing privacy and consent, preventing unintended harm, balancing transparency with security and accountability, dealing with uncertain ethical boundaries, and overcoming hurdles in the ethics review process. Researchers employ various mitigation strategies including conducting lab testing, rate limiting data collection, establishing clear communication channels, and seeking guidance from mentors and colleagues. The study also reveals that ethical requirements are inconsistent across institutions, jurisdictions, and conferences, complicating compliance. Moreover, ethics review boards often lack the technical expertise required to properly assess Internet measurement research projects. The authors emphasize the "invisible labor" involved in ethical decision-making and describe researchers’ ethics practices as a form of craft knowledge that is essential for maintaining responsible research standards in this community. Overall, the paper highlights the complexity and variability of ethical decision-making in Internet measurement research and underscores the need for better institutional support and understanding. <div>
arXiv:2511.10408v1 Announce Type: cross 
Abstract: Internet measurement research is essential for understanding, improving, and securing Internet infrastructure. However, its methods often involve large-scale data collection and user observation, raising complex ethical questions. While recent research has identified ethical challenges in Internet measurement research and laid out best practices, little is known about how researchers actually make ethical decisions in their research practice. To understand how these practices take shape day-to-day from the perspective of Internet measurement researchers, we interviewed 16 researchers from an Internet measurement research group in the EU. Through thematic analysis, we find that researchers deal with five main ethical challenges: privacy and consent issues, the possibility of unintended harm, balancing transparency with security and accountability, uncertain ethical boundaries, and hurdles in the ethics review process. Researchers address these by lab testing, rate limiting, setting up clear communication channels, and relying heavily on mentors and colleagues for guidance. Researchers express that ethical requirements vary across institutions, jurisdictions and conferences, and ethics review boards often lack the technical knowledge to evaluate Internet measurement research. We also highlight the invisible labor of Internet measurement researchers and describe their ethics practices as craft knowledge, both of which are crucial in upholding responsible research practices in the Internet measurement community.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overlap-aware meta-learning attention to enhance hypergraph neural networks for node classification</title>
<link>https://arxiv.org/abs/2503.07961</link>
<guid>https://arxiv.org/abs/2503.07961</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraph neural networks, attention mechanism, overlap-aware, meta-learning, multi-task learning<br /><br />Summary:<br /><br />This paper addresses limitations in existing hypergraph neural networks (HGNNs), which often rely on a single attention mechanism focusing on either structural or feature similarities, potentially restricting model performance. The authors propose OMA-HGNN, a novel framework that integrates both structural and feature similarities through a combined hypergraph attention mechanism. They introduce a strategy to account for varying node overlap levels by partitioning nodes into different tasks, reflecting heterogeneity in node overlap that prior models overlook. A multi-task Meta-Weight-Net (MWN) is developed to dynamically determine weighted factors corresponding to these node partitions, optimizing the balance of combined losses in the HGNN. The framework employs a joint training process where the internal MWN model is trained with the external HGNN model's losses, while the external model receives adaptive weighted factors from MWN, enhancing overall learning. Experimental validation on six real-world datasets for node classification tasks demonstrates superior node representation learning and consistent outperformance compared to nine state-of-the-art baseline methods, confirming the effectiveness of the proposed approach. This work highlights the benefits of combining attention mechanisms and incorporating node heterogeneity through meta-learning for advancing HGNN performance. <div>
arXiv:2503.07961v2 Announce Type: replace-cross 
Abstract: Although hypergraph neural networks (HGNNs) have emerged as a powerful framework for analyzing complex datasets, their practical performance often remains limited. On one hand, existing networks typically employ a single type of attention mechanism, focusing on either structural or feature similarities during message passing. On the other hand, assuming that all nodes in current hypergraph models have the same level of overlap may lead to suboptimal generalization. To overcome these limitations, we propose a novel framework, overlap-aware meta-learning attention for hypergraph neural networks (OMA-HGNN). First, we introduce a hypergraph attention mechanism that integrates both structural and feature similarities. Specifically, we linearly combine their respective losses with weighted factors for the HGNN model. Second, we partition nodes into different tasks based on their diverse overlap levels and develop a multi-task Meta-Weight-Net (MWN) to determine the corresponding weighted factors. Third, we jointly train the internal MWN model with the losses from the external HGNN model and train the external model with the weighted factors from the internal model. To evaluate the effectiveness of OMA-HGNN, we conducted experiments on six real-world datasets and benchmarked its perfor-mance against nine state-of-the-art methods for node classification. The results demonstrate that OMA-HGNN excels in learning superior node representations and outperforms these baselines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Prediction for Multi-Source Detection on a Network</title>
<link>https://arxiv.org/abs/2511.08867</link>
<guid>https://arxiv.org/abs/2511.08867</guid>
<content:encoded><![CDATA[
<div> Keywords: information detection, infection spread, networks, conformal prediction framework, recall guarantees

Summary:
The article presents a novel approach to the multi-source detection problem in networks, where the goal is to identify the set of source nodes responsible for information or infection spread. Existing methods often lack statistical guarantees or are limited in their scope, but the proposed conformal prediction framework offers reliable recall guarantees regardless of the underlying diffusion process. By introducing principled score functions and using a calibration set, this method constructs prediction sets with specified recall and coverage levels, making it applicable to various diffusion dynamics and computationally efficient for large graphs. Empirical results show that the approach achieves rigorous coverage and outperforms existing baselines in terms of accuracy and scalability. The code for the method is available online. 

<br /><br />Summary: <div>
arXiv:2511.08867v1 Announce Type: new 
Abstract: Detecting the origin of information or infection spread in networks is a fundamental challenge with applications in misinformation tracking, epidemiology, and beyond. We study the multi-source detection problem: given snapshot observations of node infection status on a graph, estimate the set of source nodes that initiated the propagation. Existing methods either lack statistical guarantees or are limited to specific diffusion models and assumptions. We propose a novel conformal prediction framework that provides statistically valid recall guarantees for source set detection, independent of the underlying diffusion process or data distribution. Our approach introduces principled score functions to quantify the alignment between predicted probabilities and true sources, and leverages a calibration set to construct prediction sets with user-specified recall and coverage levels. The method is applicable to both single- and multi-source scenarios, supports general network diffusion dynamics, and is computationally efficient for large graphs. Empirical results demonstrate that our method achieves rigorous coverage with competitive accuracy, outperforming existing baselines in both reliability and scalability.The code is available online.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Ricci-Foster Curvature Flow with GMM-Based Edge Pruning: A Novel Approach to Community Detection</title>
<link>https://arxiv.org/abs/2511.08919</link>
<guid>https://arxiv.org/abs/2511.08919</guid>
<content:encoded><![CDATA[
<div> Ricci flow, community detection, Foster curvature, effective resistance, Gaussian Mixture Model  

<br /><br />Summary:  
This article presents a novel method for community detection in complex networks based on Ricci flow applied to graphs. The method iteratively updates edge weights according to the Foster version of Ricci curvature, calculated using effective resistance distance between nodes, which involves pseudo-inverting the graph Laplacian matrix. This approach serves as an alternative to the Ollivier-Ricci geometric flow method and demonstrates significantly better computational efficiency. After iterations of Foster-Ricci flow highlight regions of varying curvature, a Gaussian Mixture Model (GMM) is used to classify edges into "strong" intra-community and "weak" inter-community groups. This classification enables systematic pruning of edges to isolate distinct communities effectively. The algorithm is benchmarked on synthetic networks generated from the Stochastic Block Model (SBM) and evaluated using the Adjusted Rand Index (ARI) for accuracy. Results confirm that the proposed Ricci-Foster flow combined with GMM clustering robustly recovers the planted community structure in SBM-generated graphs. Overall, this framework offers a principled, computationally efficient alternative to existing Ricci flow methods paired with spectral clustering, making it a promising tool for network analysis and community detection tasks. <div>
arXiv:2511.08919v1 Announce Type: new 
Abstract: Community detection in complex networks is a fundamental problem, open to new approaches in various scientific settings. We introduce a novel community detection method, based on Ricci flow on graphs. Our technique iteratively updates edge weights (their metric lengths) according to their (combinatorial) Foster version of Ricci curvature computed from effective resistance distance between the nodes. The latter computation is known to be done by pseudo-inverting the graph Laplacian matrix. At that, our approach is alternative to one based on Ollivier-Ricci geometric flow for community detection on graphs, significantly outperforming it in terms of computation time. In our proposed method, iterations of Foster-Ricci flow that highlight network regions of different curvature -- are followed by a Gaussian Mixture Model (GMM) separation heuristic. That allows to classify edges into ''strong'' (intra-community) and ''weak'' (inter-community) groups, followed by a systematic pruning of the former to isolate communities. We benchmark our algorithm on synthetic networks generated from the Stochastic Block Model (SBM), evaluating performance with the Adjusted Rand Index (ARI). Our results demonstrate that proposed framework robustly recovers the planted community structure of SBM-s, establishing Ricci-Foster Flow with GMM-clustering as a principled and computationally effective new tool for network analysis, tested against alternative Ricci-Ollivier flow coupled with spectral clustering.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spanning-Tree-Based Algorithm for Planar Graph Dismantling</title>
<link>https://arxiv.org/abs/2511.09132</link>
<guid>https://arxiv.org/abs/2511.09132</guid>
<content:encoded><![CDATA[
<div> Spanning-tree-skeleton, edge removals, connectivity, robustness analysis, planar graph<br />
Summary:<br />
This paper addresses the problem of dismantling planar graphs under an edge-budget constraint in spatially embedded networks. A novel spanning-tree-skeleton dual-path framework is proposed, which samples multiple uniform spanning trees to identify network backbones and selects between two paths based on the budget. The small-budget path estimates a dismantlable subgraph fraction using a logarithmic density feature, while the large-budget path predicts the optimal partition count using a slope-based model. Experiments on random planar graphs show near-linear runtime scaling, consistent reductions in the largest connected component ratio, and clear budget-fragmentation trends. The method offers an efficient and interpretable approach for analyzing the robustness of planar networks. <br /> <div>
arXiv:2511.09132v1 Announce Type: new 
Abstract: In spatially embedded networks such as transportation and power grids, understanding how edge removals affect connectivity is crucial for robustness analysis. This paper studies a planar graph dismantling problem under an edge-budget constraint. We propose a spanning-tree-skeleton dual-path framework that first samples multiple uniform spanning trees to capture network backbones and then adaptively selects between two complementary paths according to the budget. The small-budget path estimates a dismantlable subgraph fraction using a logarithmic density feature, while the large-budget path predicts the optimal partition count through a slope-based model. Experiments on random planar graphs demonstrate near-linear runtime scaling, consistent reductions in the largest connected component ratio, and clear budget-fragmentation trends. The method provides an interpretable and efficient approach for planar-network robustness analysis.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Phase Transition for Opinion Dynamics with Competing Biases</title>
<link>https://arxiv.org/abs/2511.09434</link>
<guid>https://arxiv.org/abs/2511.09434</guid>
<content:encoded><![CDATA[
<div> Keywords: opinion dynamics, nonlinear system, directed networks, phase transition, disruptive bias  

<br /><br />Summary:  
This article investigates the nonlinear dynamics of binary opinions within a population of agents connected by a directed network, where each agent is influenced by two competing forces: stubbornness and disruptive bias. Stubbornness represents an agent's inclination to maintain its initial opinion, while the disruptive bias models external influences pushing agents toward the opposite opinion. Agents update their opinions based on a nonlinear function involving neighbors' states and the bias parameter \( p \in [0,1] \). The authors focus on random directed graphs with fixed in- and out-degree distributions. They prove a phase transition phenomenon: if the disruptive bias \( p \) exceeds a critical threshold \( p_c \), the population rapidly converges to consensus on the disruptive opinion. If \( p < p_c \), the system reaches a metastable state wherein only a fraction \( q_\star(p) < 1 \) of agents adopt the new opinion for an extended period. The critical threshold \( p_c \) and the fraction \( q_\star(p) \) are explicitly characterized and depend solely on simple statistics of the degree sequences. The analysis employs a dual system composed of branching, coalescing, and dying particles, which mirrors the original dynamics and enables rigorous mathematical treatment. Overall, the results provide insight into how agent connectivity, stubbornness, and external bias interplay to produce tipping points in opinion evolution on networks. <div>
arXiv:2511.09434v1 Announce Type: new 
Abstract: We study a nonlinear dynamics of binary opinions in a population of agents connected by a directed network, influenced by two competing forces. On the one hand, agents are stubborn, i.e., have a tendency for one of the two opinions; on the other hand, there is a disruptive bias, $p\in[0,1]$, that drives the agents toward the other opinion. The disruptive bias models external factors, such as market innovations or social controllers, aiming to challenge the status quo, while agents' stubbornness reinforces the initial opinion making it harder for the external bias to drive the process toward change. Each agent updates its opinion according to a nonlinear function of the states of its neighbors and of the bias $p$. We consider the case of random directed graphs with prescribed in- and out-degree sequences and we prove that the dynamics exhibits a phase transition: when the disruptive bias $p$ is larger than a critical threshold $p_c$, the population converges in constant time to a consensus on the disruptive opinion. Conversely, when the bias $p$ is less than $p_c$, the system enters a metastable state in which only a fraction of agents $q_\star(p)<1$ will share the new opinion for a long time. We characterize $p_c$ and $q_\star(p)$ explicitly, showing that they only depend on few simple statistics of the degree sequences. Our analysis relies on a dual system of branching, coalescing, and dying particles, which we show exhibits equivalent behavior and allows a rigorous characterization of the system's dynamics. Our results characterize the interplay between the degree of the agents, their stubbornness, and the external bias, shedding light on the tipping points of opinion dynamics in networks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Scope Experts at Test: Generalizing Deeper Graph Neural Networks with Shallow Variants</title>
<link>https://arxiv.org/abs/2409.06998</link>
<guid>https://arxiv.org/abs/2409.06998</guid>
<content:encoded><![CDATA[
<div> Keywords: heterophilous graphs, graph neural networks, GNN depth, generalization, Mixture of scope experts (Moscat)  

<br /><br />Summary:  
1. Heterophilous graphs, characterized by connections between dissimilar nodes, challenge the effectiveness of graph neural networks (GNNs).  
2. Increasing GNN depth can broaden the receptive field, potentially capturing homophily from higher-order neighborhoods, but this often leads to performance degradation.  
3. Despite their greater expressivity, deeper state-of-the-art GNNs show only marginal improvement over shallow versions.  
4. The study reveals, through theoretical and empirical analysis, a shift in GNN generalization preferences for nodes with varying homophily levels as depth increases, creating disparities in how models of different depths generalize.  
5. To address this, the authors propose Moscat (Mixture of scope experts at test), a method that enhances generalization of deeper GNNs while retaining their expressivity.  
6. Experiments demonstrate that Moscat flexibly integrates with various GNN architectures across numerous datasets, significantly boosting accuracy.  
7. The code implementation of Moscat is publicly available for further research and application. <div>
arXiv:2409.06998v4 Announce Type: replace-cross 
Abstract: Heterophilous graphs, where dissimilar nodes tend to connect, pose a challenge for graph neural networks (GNNs). Increasing the GNN depth can expand the scope (i.e., receptive field), potentially finding homophily from the higher-order neighborhoods. However, GNNs suffer from performance degradation as depth increases. Despite having better expressivity, state-of-the-art deeper GNNs achieve only marginal improvements compared to their shallow variants. Through theoretical and empirical analysis, we systematically demonstrate a shift in GNN generalization preferences across nodes with different homophily levels as depth increases. This creates a disparity in generalization patterns between GNN models with varying depth. Based on these findings, we propose to improve deeper GNN generalization while maintaining high expressivity by Mixture of scope experts at test (Moscat). Experimental results show that Moscat works flexibly with various GNNs across a wide range of datasets while significantly improving accuracy. Our code is available at (https://github.com/Hydrapse/moscat).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Message Passing Experts with Routing Entropy Regularization for Node Classification</title>
<link>https://arxiv.org/abs/2502.08083</link>
<guid>https://arxiv.org/abs/2502.08083</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, heterogeneous structures, mixture of experts, adaptive representation learning, entropy regularization

Summary:
Graph neural networks (GNNs) have shown success in graph-based learning tasks but struggle with heterogeneous structures. To overcome this, GNNMoE is introduced, a framework that combines multiple expert networks for adaptive node representation learning. GNNMoE breaks down message passing into propagation and transformation components, incorporating a hybrid routing mechanism to guide the experts. With dynamic soft weighting and top-k routing adjustments through routing entropy regularization, GNNMoE can effectively adapt to diverse neighborhood contexts. The framework outperforms state-of-the-art methods in node classification tasks across various datasets while maintaining scalability and interpretability. GNNMoE offers a systematic approach to achieve personalized node representation learning, paving the way for fine-grained graph-based analysis. 

<br /><br />Summary: <div>
arXiv:2502.08083v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) have achieved significant progress in graph-based learning tasks, yet their performance often deteriorates when facing heterophilous structures where connected nodes differ substantially in features and labels. To address this limitation, we propose GNNMoE, a novel entropy-driven mixture of message-passing experts framework that enables node-level adaptive representation learning. GNNMoE decomposes message passing into propagation and transformation operations and integrates them through multiple expert networks guided by a hybrid routing mechanism. And a routing entropy regularization dynamically adjusts soft weighting and soft top-$k$ routing, allowing GNNMoE to flexibly adapt to diverse neighborhood contexts. Extensive experiments on twelve benchmark datasets demonstrate that GNNMoE consistently outperforms SOTA node classification methods, while maintaining scalability and interpretability. This work provides a unified and principled approach for achieving fine-grained, personalized node representation learning.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging the Power of AI and Social Interactions to Restore Trust in Public Polls</title>
<link>https://arxiv.org/abs/2511.07593</link>
<guid>https://arxiv.org/abs/2511.07593</guid>
<content:encoded><![CDATA[
<div> polling, crowdsourced data, social networks, credibility, social interactions

Summary:
The paper discusses the challenges of credibility in crowdsourced data collected on social networks and explores the use of social interactions to enhance reliability. Traditional polling methods have seen a decline in engagement, while social networks face credibility issues due to dishonest participants. The study uses AI-based graph analysis to detect ineligible participation in a polling task by analyzing social interactions within a network of honest and dishonest actors. By focusing on the structure of social interaction graphs without considering content, the study aims to restore credibility in data collection. Experiments on real-world datasets show promising results in detecting ineligibility across diverse participation patterns, achieving an accuracy of over 90% in some scenarios. The research highlights the potential of leveraging social interactions to improve the trustworthiness of crowdsourced data in social science research. 

<br /><br />Summary: <div>
arXiv:2511.07593v1 Announce Type: new 
Abstract: The emergence of crowdsourced data has significantly reshaped social science, enabling extensive exploration of collective human actions, viewpoints, and societal dynamics. However, ensuring safe, fair, and reliable participation remains a persistent challenge. Traditional polling methods have seen a notable decline in engagement over recent decades, raising concerns about the credibility of collected data. Meanwhile, social and peer-to-peer networks have become increasingly widespread, but data from these platforms can suffer from credibility issues due to fraudulent or ineligible participation. In this paper, we explore how social interactions can help restore credibility in crowdsourced data collected over social networks. We present an empirical study to detect ineligible participation in a polling task through AI-based graph analysis of social interactions among imperfect participants composed of honest and dishonest actors. Our approach focuses solely on the structure of social interaction graphs, without relying on the content being shared. We simulate different levels and types of dishonest behavior among participants who attempt to propagate the task within their social networks. We conduct experiments on real-world social network datasets, using different eligibility criteria and modeling diverse participation patterns. Although structural differences in social interaction graphs introduce some performance variability, our study achieves promising results in detecting ineligibility across diverse social and behavioral profiles, with accuracy exceeding 90% in some configurations.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiLoMix: Robust High- and Low-Frequency Graph Learning Framework for Mixing Address Association</title>
<link>https://arxiv.org/abs/2511.07759</link>
<guid>https://arxiv.org/abs/2511.07759</guid>
<content:encoded><![CDATA[
<div> Graph-based models, Mixing address association, Label noise, Label scarcity, Weak supervised learning <br />
Summary: <br />
HiLoMix is a novel graph-based learning framework designed for mixing address association in transaction networks. It addresses challenges of label noise and scarcity by constructing a Heterogeneous Attributed Mixing Interaction Graph (HAMIG) and applying frequency-aware graph contrastive learning. The framework utilizes weak supervised learning to handle noisy labels and trains high-pass and low-pass Graph Neural Networks (GNNs) using unsupervised contrastive signals and confidence-based supervision. A stacking framework is employed to fuse predictions from multiple models, enhancing generalization and robustness. Experimental results show that HiLoMix outperforms existing methods in accurately associating mixing addresses, making it an effective tool in detecting illicit transactions. <br /> <div>
arXiv:2511.07759v1 Announce Type: new 
Abstract: As mixing services are increasingly being exploited by malicious actors for illicit transactions, mixing address association has emerged as a critical research task. A range of approaches have been explored, with graph-based models standing out for their ability to capture structural patterns in transaction networks. However, these approaches face two main challenges: label noise and label scarcity, leading to suboptimal performance and limited generalization. To address these, we propose HiLoMix, a graph-based learning framework specifically designed for mixing address association. First, we construct the Heterogeneous Attributed Mixing Interaction Graph (HAMIG) to enrich the topological structure. Second, we introduce frequency-aware graph contrastive learning that captures complementary structural signals from high- and low-frequency graph views. Third, we employ weak supervised learning that assigns confidence-based weighting to noisy labels. Then, we jointly train high-pass and low-pass GNNs using both unsupervised contrastive signals and confidence-based supervision to learn robust node representations. Finally, we adopt a stacking framework to fuse predictions from multiple heterogeneous models, further improving generalization and robustness. Experimental results demonstrate that HiLoMix outperforms existing methods in mixing address association.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Value Expressions in Social Media Posts</title>
<link>https://arxiv.org/abs/2511.08453</link>
<guid>https://arxiv.org/abs/2511.08453</guid>
<content:encoded><![CDATA[
<div> Keywords: value alignment, sociotechnical systems, large-language models, Schwartz value system, social media data

Summary: 
The study focuses on the measurement of human values expressed in social media posts, a crucial aspect of value alignment in sociotechnical systems. Utilizing the Schwartz value system as a comprehensive framework, the researchers develop a method to measure value expressions at scale. Through the analysis of over 32,000 social media posts and annotations from human raters, low inter-rater agreement is observed. The study suggests that value expression is subjective and personalized, leading to the construction of a personalization architecture for classifying value expressions. Results show that a system accounting for individual differences produces more consistent predictions of value expressions compared to human raters. This research provides valuable insights and methods for understanding and measuring human values in the context of social media data.
<br /><br />Summary: <div>
arXiv:2511.08453v1 Announce Type: new 
Abstract: The value alignment of sociotechnical systems has become a central debate but progress in this direction requires the measurement of the expressions of values. While the rise of large-language models offer new possible opportunities for measuring expressions of human values (e.g., humility or equality) in social media data, there remain both conceptual and practical challenges in operationalizing value expression in social media posts: what value system and operationalization is most applicable, and how do we actually measure them? In this paper, we draw on the Schwartz value system as a broadly encompassing and theoretically grounded set of basic human values, and introduce a framework for measuring Schwartz value expressions in social media posts at scale. We collect 32,370 ground truth value expression annotations from N=1,079 people on 5,211 social media posts representative of real users' feeds. We observe low levels of inter-rater agreement between people, and low agreement between human raters and LLM-based methods. Drawing on theories of interpretivism - that different people will have different subjective experiences of the same situation - we argue that value expression is (partially) in the eye of the beholder. In response, we construct a personalization architecture for classifying value expressions. We find that a system that explicitly models these differences yields predicted value expressions that people agree with more than they agree with other people. These results contribute new methods and understanding for the measurement of human values in social media data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Optimization on Graph-Structured Data via Gaussian Processes with Spectral Representations</title>
<link>https://arxiv.org/abs/2511.07734</link>
<guid>https://arxiv.org/abs/2511.07734</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian optimization, graph-structured domains, Gaussian process surrogates, low-rank spectral representations, global search

Summary: 
This paper introduces a scalable framework for global optimization over graphs using Bayesian optimization. The framework utilizes low-rank spectral representations to build Gaussian process surrogates from sparse structural observations on graph-structured domains. By jointly inferring graph structure and node representations through learnable embeddings, the method enables efficient global search and principled uncertainty estimation even with limited data. The approach achieves faster convergence and improved optimization performance compared to existing methods, overcoming challenges posed by the discrete and combinatorial nature of graphs. Theoretical analysis establishes conditions for accurate recovery of underlying graph structure under different sampling regimes. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework in achieving faster convergence and improved optimization performance. <div>
arXiv:2511.07734v1 Announce Type: cross 
Abstract: Bayesian optimization (BO) is a powerful framework for optimizing expensive black-box objectives, yet extending it to graph-structured domains remains challenging due to the discrete and combinatorial nature of graphs. Existing approaches often rely on either full graph topology-impractical for large or partially observed graphs-or incremental exploration, which can lead to slow convergence. We introduce a scalable framework for global optimization over graphs that employs low-rank spectral representations to build Gaussian process (GP) surrogates from sparse structural observations. The method jointly infers graph structure and node representations through learnable embeddings, enabling efficient global search and principled uncertainty estimation even with limited data. We also provide theoretical analysis establishing conditions for accurate recovery of underlying graph structure under different sampling regimes. Experiments on synthetic and real-world datasets demonstrate that our approach achieves faster convergence and improved optimization performance compared to prior methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Political Text at Scale with Online Tensor LDA</title>
<link>https://arxiv.org/abs/2511.07809</link>
<guid>https://arxiv.org/abs/2511.07809</guid>
<content:encoded><![CDATA[
<div> Keywords: topic modeling, Tensor Latent Dirichlet Allocation, scalable, social media, #MeToo movement

Summary: 
This paper introduces a scalable topic modeling method called Tensor Latent Dirichlet Allocation (TLDA) that can handle billions of documents. The method guarantees identifiable and recoverable parameters, sample complexity, and computational and memory efficiency, outperforming previous parallelized Latent Dirichlet Allocation (LDA) methods. By providing an open-source, GPU-based implementation, TLDA allows for the analysis of very large text datasets, like over a billion documents. The scalability of TLDA enables social scientists to conduct large-scale studies on subjects like the #MeToo movement and social media conversations about election fraud in the 2020 presidential election. The method facilitates in-depth analysis and provides insights into real-time discourse on significant societal topics.<br /><br />Summary: <div>
arXiv:2511.07809v1 Announce Type: cross 
Abstract: This paper proposes a topic modeling method that scales linearly to billions of documents. We make three core contributions: i) we present a topic modeling method, Tensor Latent Dirichlet Allocation (TLDA), that has identifiable and recoverable parameter guarantees and sample complexity guarantees for large data; ii) we show that this method is computationally and memory efficient (achieving speeds over 3-4x those of prior parallelized Latent Dirichlet Allocation (LDA) methods), and that it scales linearly to text datasets with over a billion documents; iii) we provide an open-source, GPU-based implementation, of this method. This scaling enables previously prohibitive analyses, and we perform two real-world, large-scale new studies of interest to political scientists: we provide the first thorough analysis of the evolution of the #MeToo movement through the lens of over two years of Twitter conversation and a detailed study of social media conversations about election fraud in the 2020 presidential election. Thus this method provides social scientists with the ability to study very large corpora at scale and to answer important theoretically-relevant questions about salient issues in near real-time.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Weisfeiler-Lehman Kernels to Subgraphs</title>
<link>https://arxiv.org/abs/2412.02181</link>
<guid>https://arxiv.org/abs/2412.02181</guid>
<content:encoded><![CDATA[
<div> Subgraph representation learning; graph neural networks; Weisfeiler-Lehman kernel; structural information; neighborhood sampling <br />
<br />
Summary: <br />
Subgraph representation learning has become a popular method for solving real-world problems, but current graph neural networks struggle with capturing complex interactions within and between subgraphs. To address this, the WLKS approach introduces a Weisfeiler-Lehman kernel that is specialized for subgraphs by applying the WL algorithm on induced k-hop neighborhoods. By combining kernels across different k-hop levels, WLKS can capture richer structural information without the need for neighborhood sampling. In experiments across various datasets, WLKS outperformed existing approaches on five datasets while also reducing training time significantly, showcasing a balance between expressiveness and efficiency in subgraph representation learning. <div>
arXiv:2412.02181v3 Announce Type: replace-cross 
Abstract: Subgraph representation learning has been effective in solving various real-world problems. However, current graph neural networks (GNNs) produce suboptimal results for subgraph-level tasks due to their inability to capture complex interactions within and between subgraphs. To provide a more expressive and efficient alternative, we propose WLKS, a Weisfeiler-Lehman (WL) kernel generalized for subgraphs by applying the WL algorithm on induced $k$-hop neighborhoods. We combine kernels across different $k$-hop levels to capture richer structural information that is not fully encoded in existing models. Our approach can balance expressiveness and efficiency by eliminating the need for neighborhood sampling. In experiments on eight real-world and synthetic benchmarks, WLKS significantly outperforms leading approaches on five datasets while reducing training time, ranging from 0.01x to 0.25x compared to the state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effects of higher-order interactions and homophily on information access inequality</title>
<link>https://arxiv.org/abs/2506.00156</link>
<guid>https://arxiv.org/abs/2506.00156</guid>
<content:encoded><![CDATA[
<div> Keywords: socio-technical systems, hypergraphs, hyperedge homophily, nonlinear social contagion, information access <br />
Summary: 
The article introduces the $\texttt{H3}$ model for hypergraphs with hyperedge homophily, hyperedge size-dependent property, and tunable degree distribution. It also presents a model for nonlinear social contagion with asymmetric transmission between in-group and out-group nodes. The study shows that the interaction between social contagion dynamics and hyperedge homophily can significantly influence group-level differences in information access. The findings highlight the importance of considering hyperedge homophily in shaping interaction patterns and suggest targeted interventions at specific hyperedge sizes to reduce inequality in socio-technical systems. The research emphasizes the need for a higher-order perspective in designing socio-technical systems, focusing on dynamics-informed interventions embedded in platform architecture. <br /><br />Summary: <div>
arXiv:2506.00156v2 Announce Type: replace-cross 
Abstract: The spread of information through socio-technical systems determines which individuals are the first to gain access to opportunities and insights. Yet, the pathways through which information flows can be skewed, leading to systematic differences in access across social groups. These inequalities remain poorly characterized in settings involving nonlinear social contagion and higher-order interactions that exhibit homophily. We introduce a enerative model for hypergraphs with hyperedge homophily, a hyperedge size-dependent property, and tunable degree distribution, called the $\texttt{H3}$ model, along with a model for nonlinear social contagion that incorporates asymmetric transmission between in-group and out-group nodes. Using stochastic simulations of a social contagion process on hypergraphs from the $\texttt{H3}$ model and diverse empirical datasets, we show that the interaction between social contagion dynamics and hyperedge homophily -- an effect unique to higher-order networks due to its dependence on hyperedge size -- can critically shape group-level differences in information access. By emphasizing how hyperedge homophily shapes interaction patterns, our findings underscore the need to rethink socio-technical system design through a higher-order perspective and suggest that dynamics-informed, targeted interventions at specific hyperedge sizes, embedded in a platform architecture, offer a powerful lever for reducing inequality.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Food as Soft Power: Taiwanese Gastrodiplomacy on Social Media and Algorithmic Suppression</title>
<link>https://arxiv.org/abs/2511.05729</link>
<guid>https://arxiv.org/abs/2511.05729</guid>
<content:encoded><![CDATA[
<div> dataset, bubble tea, Instagram, Taiwanese cuisine, soft power
Summary: 
This study analyzes how Taiwanese gastrodiplomacy is represented on social media platforms, specifically focusing on bubble tea. The researchers collected data from Instagram posts over five years, revealing that bubble tea is a dominant symbol of Taiwanese cuisine on the platform. However, there is evidence of Instagram suppressing posts mentioning Taiwan, indicating potential vulnerabilities in Taiwan's gastrodiplomatic strategy. The study also observes a significant decrease in engagement following a political event in Taiwan. These findings highlight the role of digital platforms in facilitating or hindering gastrodiplomacy and cultural diplomacy, emphasizing the importance of algorithmic transparency. The study provides insights for nations looking to leverage soft power through food diplomacy and for researchers studying algorithmic suppression. 
<br /><br />Summary: <div>
arXiv:2511.05729v1 Announce Type: new 
Abstract: Social media platforms have become pivotal for projecting national identity and soft power in an increasingly digital world. This study examines the digital manifestation of Taiwanese gastrodiplomacy by focusing on bubble tea -- a culturally iconic beverage -- leveraging a dataset comprising 107,169 posts from the popular lifestyle social media platform Instagram. Including 315,279,227 engagements, 4,756,320 comments, and 8,097,260,651 views over five full years (2020--2024), we investigate how social media facilitates discussion about Taiwanese cuisine and contributes to Taiwan's digital soft power. Our analysis reveals that bubble tea consistently emerges as the dominant representation of Taiwanese cuisine across Meta's Instagram channels. However, this dominance also indicates vulnerability in gastrodiplomatic strategy compared to other countries. Additionally, we find evidence that Instagram suppresses bubble tea posts mentioning Taiwan by 1,200\% -- roughly a twelve--fold decrease in exposure -- relative to posts without such mentions. Crucially, we observe a significant drop in the number of posts, views, and engagement following Lai's inauguration in May 2024. This study ultimately contributes to understanding how digital platforms can enable or disable gastrodiplomacy, soft power, and cultural diplomacy while highlighting the need for greater algorithmic transparency. By noting Taiwan's bubble tea's digital engagement and footprint, critical insights are brought for nations seeking to leverage soft power through gastronomic means in a politicized digital era and researchers trying to better understand algorithmic suppression.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration of Enterprise Big Data Microservice Architecture Based on Domain-Driven Design (DDD)</title>
<link>https://arxiv.org/abs/2511.05880</link>
<guid>https://arxiv.org/abs/2511.05880</guid>
<content:encoded><![CDATA[
<div> Keywords: digitization, enterprise big data processing platform, microservice architecture, Domain Driven Design, scalability

Summary:
This article introduces a solution for enterprise big data processing platforms by utilizing a microservice architecture based on Domain Driven Design. Traditional monolithic architectures struggle to handle the growing complexity of business demands and data volume, resulting in limited scalability and efficiency. By decomposing the core business logic into independent microservices, the platform gains flexibility and scalability. The article also presents an automated data collection process utilizing microservices and a dynamic scheduling algorithm to efficiently allocate tasks to Docker nodes. Real-time monitoring ensures data collection accuracy and efficiency. Implementation and testing of the platform confirm significant enhancements in scalability, data quality, and collection efficiency.<br /><br />Summary: <div>
arXiv:2511.05880v1 Announce Type: new 
Abstract: With the rapid advancement of digitization and intelligence, enterprise big data processing platforms have become increasingly important in data management. However, traditional monolithic architectures, due to their high coupling, are unable to cope with increasingly complex demands in the face of business expansion and increased data volume, resulting in limited platform scalability and decreased data collection efficiency. This article proposes a solution for enterprise big data processing platform based on microservice architecture, based on the concept of Domain Driven Design (DDD). Through in-depth analysis of business requirements, the functional and non functional requirements of the platform in various scenarios were determined, and the DDD method was used to decompose the core business logic into independent microservice modules, enabling data collection, parsing, cleaning, and visualization functions to be independently developed, deployed, and upgraded, thereby improving the flexibility and scalability of the system. This article also designs an automated data collection process based on microservices and proposes an improved dynamic scheduling algorithm to efficiently allocate data collection tasks to Docker nodes, and monitor the collection progress and service status in real time to ensure the accuracy and efficiency of data collection. Through the implementation and testing of the platform, it has been verified that the enterprise big data processing platform based on microservice architecture has significantly improved scalability, data quality, and collection efficiency.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Construction and Evolutionary Analysis of a Game Model for Supply Chain Finance Funding Based on Blockchain Technology</title>
<link>https://arxiv.org/abs/2511.05891</link>
<guid>https://arxiv.org/abs/2511.05891</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain finance, blockchain technology, information asymmetry, credit transmission chains, evolutionary game theory

Summary:
Supply chain finance has addressed capital challenges in domestic enterprises by facilitating financing services based on core enterprise credit. However, challenges like information asymmetry and inefficient credit transmission hinder growth in China. This paper proposes a supply chain finance framework integrating blockchain technology to enhance efficiency. It outlines participating entities, their relationships, and constructs a financing game model using evolutionary game theory. The model analyzes equilibrium points and stability, exploring strategies adopted by small and medium-sized enterprises, key players, and financing entities. The impact of blockchain technology on supply chain finance transactions is also assessed, highlighting its influence on completing transactions successfully. <div>
arXiv:2511.05891v1 Announce Type: new 
Abstract: The current surge in supply chain finance has significantly alleviated the "capital challenges" faced by domestic related enterprises, enabling enterprises upstream and subsequent stages of the industrial chain to achieve effective circulation of financing services in the supply chain based on the credit of core enterprises. By gathering essential information from the heart of the supply chain, supply chain financing enables efficient resource distribution and aids all stakeholders in making well-informed choices. However, supply chain finance in China still faces numerous obstacles, such as information asymmetry and inefficient credit transmission chains, hindering its long-term development. This paper designs an operational framework for supply chain finance incorporating blockchain technology, clearly defines the participating entities, and analyzes their business relationships. Based upon evolutionary game theory, a supply chain finance financing game model incorporating blockchain technology is constructed. A comparative analysis of the model's equilibrium points and their stability is conducted. The choices of evolutionary equilibrium strategies adopted by small and medium-sized enterprises, key players, and financing entities within this framework are explored, and the influence of blockchain technology on the prerequisites for completing supply chain finance transactions is investigated.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research On CODP Localization Decision Model Of Automotive Supply Chain Based On Delayed Manufacturing Strategy</title>
<link>https://arxiv.org/abs/2511.05899</link>
<guid>https://arxiv.org/abs/2511.05899</guid>
<content:encoded><![CDATA[
<div> delayed response manufacturing, automotive manufacturing, production system, order response node configuration, cost control

Summary:
The traditional manufacturing model is insufficient to meet the current demands for personalized products and quick deliveries in the automotive industry. To address this, a production system based on delayed response manufacturing strategy is proposed. This system focuses on order response nodes configuration in the production process to optimize resource allocation and market response. The model considers structural cost changes, dynamic unit manufacturing cost variations, and intermediate inventory costs at different stages. It incorporates delivery time constraints to enhance practicality. By using function fitting, simulation analysis, and mathematical modeling tools, the study systematically describes total cost changes and validates the model through actual enterprise cases. The findings provide a decision-making framework for automobile manufacturers to achieve flexibility in production while controlling costs in response to changing demands. The research also offers practical insights for implementation strategies and system optimization in the future.<br /><br />Summary: <div>
arXiv:2511.05899v1 Announce Type: new 
Abstract: Under the market background of increasingly personalized product demand and compressed response cycle, the traditional manufacturing model with standardized mass production as the core has been difficult to meet the dual expectations of customers for differentiation and fast delivery. In order to improve the efficiency of resource allocation and market response, automobile manufacturers need to build a production system that takes into account cost and flexibility. Based on the delayed response manufacturing strategy, this study built an order response node configuration model suitable for automotive manufacturing scenarios, focusing on the positioning of order driven intervention points in the production process. The model comprehensively considers the structural cost changes brought by process adjustment, the dynamic characteristics of the changes of unit manufacturing cost and intermediate inventory cost at different stages with the location of nodes, and introduces delivery time constraints to embed time factors into the inventory decision logic to enhance the practicality of the model and the adaptation of realistic constraints. In terms of solution methods, this paper adopts function fitting and simulation analysis methods, combined with mathematical modeling tools, systematically describes the change trend of total cost, and verifies the rationality and effectiveness of the model structure and solution through actual enterprise cases. The research results provide a theoretical basis and decision support for automobile manufacturing enterprises to realize the synergy of flexible production and cost control in the environment of variable demand, and also provide an empirical reference for the implementation path and system optimization of subsequent relevant strategies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role and Mechanism of Deep Statistical Machine Learning In Biological Target Screening and Immune Microenvironment Regulation of Asthma</title>
<link>https://arxiv.org/abs/2511.05904</link>
<guid>https://arxiv.org/abs/2511.05904</guid>
<content:encoded><![CDATA[
<div> phosphodiesterase 4, phosphodiesterase 7, natural inhibitors, computer aided drug design, xanthine oxidase <br />
Summary:<br /> 
This study focused on screening natural inhibitors of phosphodiesterase 4 (PDE4) and phosphodiesterase 7 (PDE7) using a combination of computer-aided drug design (CADD) and deep learning methods. The potential inhibitors were verified through enzyme activity experiments and enzyme-linked immunoassays. Dual-target inhibitors for these enzymes are sought for treating inflammatory diseases with minimal adverse reactions. Additionally, the development of natural inhibitors for xanthine oxidase (XO) was explored using pharmacophore and molecular docking technologies. Sixteen potential natural inhibitors of PDE4/7 were identified and their binding stability was confirmed through molecular dynamics simulation. This study provides a foundation for establishing an efficient dual-target inhibitor screening system and discovering lead compounds for novel XO inhibitors. <br /> <div>
arXiv:2511.05904v1 Announce Type: new 
Abstract: As an important source of small molecule drugs, natural products show remarkable biological activities with their rich types and unique structures. However, due to the limited number of samples and structural complexity, the rapid discovery of lead compounds is limited. Therefore, in this study, natural inhibitors of phosphodiesterase 4 (PDE4) and Phosphodiesterase 7 (PDE7) were screened by combining computer aided drug design (CADD) technology and deep learning method, and their activities were verified by enzyme activity experiment and enzymo-linked immunoassay. These two enzymes have important application potential in the treatment of inflammatory diseases such as chronic obstructive pulmonary disease and asthma, but PDE4 inhibitors may cause adverse reactions, so it is particularly important to develop both effective and safe dual-target inhibitors. In addition, as a potential target of hyperuricemia, the development of natural inhibitors of xanthine oxidase (X0) is also of great value. We used pharmacophore technology for virtual screening, combined with molecular docking technology to improve accuracy, and finally selected 16 potential natural inhibitors of PDE4/7, and verified their binding stability through molecular dynamics simulation. The results of this study laid a foundation for establishing an efficient dual-target inhibitor screening system and exploring the lead compounds of novel X0 inhibitors.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing AI Manipulation Risks in Brazilian YouTube Climate Discourse</title>
<link>https://arxiv.org/abs/2511.06091</link>
<guid>https://arxiv.org/abs/2511.06091</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, YouTube, audience engagement, persuasive strategies, generative language models 

Summary: 
Climate change presents a global threat that requires evidence-based policies and a thorough understanding of public perception on platforms like YouTube. This study focuses on climate-related discourse on Brazilian YouTube to investigate the psychological traits driving audience engagement, their impact on content popularity, and their potential use in designing persuasive synthetic campaigns. The researchers also release a large dataset of Brazilian YouTube videos and user comments on climate change, including annotations of persuasive strategies, theory-of-mind categorizations, and content creator typologies. This dataset can support future research on digital climate communication and the ethical risks associated with algorithmically amplified narratives and generative media. <div>
arXiv:2511.06091v1 Announce Type: new 
Abstract: Climate change poses a global threat to public health, food security, and economic stability. Addressing it requires evidence-based policies and a nuanced understanding of how the threat is perceived by the public, particularly within visual social media, where narratives quickly evolve through voices of individuals, politicians, NGOs, and institutions. This study investigates climate-related discourse on YouTube within the Brazilian context, a geopolitically significant nation in global environmental negotiations. Through three case studies, we examine (1) which psychological content traits most effectively drive audience engagement, (2) the extent to which these traits influence content popularity, and (3) whether such insights can inform the design of persuasive synthetic campaigns--such as climate denialism--using recent generative language models. Another contribution of this work is the release of a large publicly available dataset of 226K Brazilian YouTube videos and 2.7M user comments on climate change. The dataset includes fine-grained annotations of persuasive strategies, theory-of-mind categorizations in user responses, and typologies of content creators. This resource can help support future research on digital climate communication and the ethical risk of algorithmically amplified narratives and generative media.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperEF 2.0: Spectral Hypergraph Coarsening via Krylov Subspace Expansion and Resistance-based Local Clustering</title>
<link>https://arxiv.org/abs/2511.06600</link>
<guid>https://arxiv.org/abs/2511.06600</guid>
<content:encoded><![CDATA[
<div> Framework, Spectral coarsening, Hypergraphs, Effective resistances, Clustering 

Summary: 
- HyperEF 2.0 is a framework for spectral coarsening and clustering of large-scale hypergraphs through hyperedge effective resistances.
- It leverages expanded Krylov subspace for improved approximation accuracy of effective resistances.
- A resistance-based local clustering scheme merges small isolated nodes into nearby clusters for balanced clusters with improved conductance.
- HyperEF 2.0 integrates resistance-based hyperedge weighting and community detection into a multilevel hypergraph partitioning tool.
- Extensive experiments on VLSI benchmarks show HyperEF 2.0 effectively coarsens hypergraphs, maintaining structural properties and delivering better solution quality. It outperforms state-of-the-art methods like HyperEF and HyperSF and achieves smaller cut sizes compared to hMETIS, SpecPart, MedPart, and KaHyPar. Additionally, HyperEF 2.0 achieves up to a 4.5x speedup over HyperSF, demonstrating superior efficiency and partitioning quality. 

<br /><br />Summary: <div>
arXiv:2511.06600v1 Announce Type: new 
Abstract: This paper introduces HyperEF 2.0, a scalable framework for spectral coarsening and clustering of large-scale hypergraphs through hyperedge effective resistances, aiming to decompose hypergraphs into multiple node clusters with a small number of inter-cluster hyperedges. Building on the recent HyperEF framework, our approach offers three primary contributions. Specifically, first, by leveraging the expanded Krylov subspace exploiting both clique and star expansions of hyperedges, we can significantly improve the approximation accuracy of effective resistances. Second, we propose a resistance-based local clustering scheme for merging small isolated nodes into nearby clusters, yielding more balanced clusters with substantially improved conductance. Third, the proposed HyperEF 2.0 enables the integration of resistance-based hyperedge weighting and community detection into a multilevel hypergraph partitioning tool, achieving state-of-the-art performance. Extensive experiments on real-world VLSI benchmarks show that HyperEF 2.0 can more effectively coarsen hypergraphs without compromising their structural properties, while delivering much better solution quality (e.g. conductance) than the state-of-the-art hypergraph coarsening methods, such as HyperEF and HyperSF. Moreover, compared to leading hypergraph partitioners such as hMETIS, SpecPart, MedPart, and KaHyPar, our framework consistently achieves smaller cut sizes. In terms of runtime, HyperEF 2.0 attains up to a 4.5x speedup over the latest flow-based local clustering algorithm, HyperSF, demonstrating both superior efficiency and partitioning quality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Centrality: Understanding Urban Street Network Typologies Through Intersection Patterns</title>
<link>https://arxiv.org/abs/2511.06747</link>
<guid>https://arxiv.org/abs/2511.06747</guid>
<content:encoded><![CDATA[
<div> Intersection classification, clustering algorithms, San Francisco Bay Area, road network patterns, typologies <br />
Summary: 
This study introduces a novel metric for classifying intersections in the San Francisco Bay Area based on angles formed, identifying grid, orthogonal, and organic city typologies. The research fills gaps in previous studies by focusing on detailed characterizations within a single large urban region and considering intersection-level geometric angles. The study's approach utilizes clustering algorithms in machine learning to classify intersections and differentiate cities based on street and intersection patterns. The typologies generated, grid, orthogonal, and organic, offer valuable insights for city planners and policymakers. These typologies can inform various strategies tailored to each city's road network complexities, including evacuation plans, traffic signage placements, and traffic signal control. <br /><br /> <div>
arXiv:2511.06747v1 Announce Type: new 
Abstract: The structure of road networks plays a pivotal role in shaping transportation dynamics. It also provides insights into how drivers experience city streets and helps uncover each urban environment's unique characteristics and challenges. Consequently, characterizing cities based on their road network patterns can facilitate the identification of similarities and differences, informing collaborative traffic management strategies, particularly at a regional scale. While previous studies have investigated global network patterns for cities, they have often overlooked detailed characterizations within a single large urban region. Additionally, most existing research uses metrics like degree, centrality, orientation, etc., and misses the nuances of street networks at the intersection level, specifically the geometric angles formed by links at intersections, which could offer a more refined feature for characterization. To address these gaps, this study examines over 100 cities in the San Francisco Bay Area. We introduce a novel metric for classifying intersections, distinguishing between different types of 3-way and 4-way intersections based on the angles formed at the intersections. Through the application of clustering algorithms in machine learning, we have identified three distinct typologies - grid, orthogonal, and organic cities - within the San Francisco Bay Area. We demonstrate the effectiveness of the metric in capturing the differences between cities based on street and intersection patterns. The typologies generated in this study could offer valuable support for city planners and policymakers in crafting a range of practical strategies tailored to the complexities of each city's road network, covering aspects such as evacuation plans, traffic signage placements, and traffic signal control.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CGLE: Class-label Graph Link Estimator for Link Prediction</title>
<link>https://arxiv.org/abs/2511.06982</link>
<guid>https://arxiv.org/abs/2511.06982</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Link Prediction, Class-label Graph Link Estimator, Semantic Information, Multi-Layer Perceptron

Summary:
Class-label Graph Link Estimator (CGLE) is introduced to enhance Graph Neural Network models by utilizing semantic information at the class level for link prediction tasks. CGLE constructs a class-conditioned link probability matrix based on ground-truth labels or pseudo-labels from clustering. This matrix is combined with structural link embeddings from a GNN and processed by a Multi-Layer Perceptron for prediction. CGLE's efficient preprocessing stage ensures no impact on the computational complexity of the underlying GNN model. Experimental results demonstrate significant performance improvements over strong baselines like NCN and NCNC, particularly achieving over 10% HR@100 improvement on homophilous datasets and over 4% MRR improvement on sparse heterophilous graphs. The integration of global semantic priors through CGLE provides a promising approach to link prediction without the need for complex model architectures.

Summary:<br /><br />Graph Neural Network, Link Prediction, Class-label Graph Link Estimator, Semantic Information, Multi-Layer Perceptron<br />CGLE enhances GNN models by incorporating class-level semantic information for link prediction tasks. It constructs a class-conditioned link probability matrix using ground-truth or pseudo-labels and combines it with structural embeddings for prediction. Despite its efficiency, CGLE outperforms baseline models like NCN and NCNC, achieving significant improvements on various datasets. This approach demonstrates the effectiveness of leveraging global semantic priors in link prediction tasks, presenting a compelling alternative to complex model architectures. <div>
arXiv:2511.06982v1 Announce Type: new 
Abstract: Link prediction is a pivotal task in graph mining with wide-ranging applications in social networks, recommendation systems, and knowledge graph completion. However, many leading Graph Neural Network (GNN) models often neglect the valuable semantic information aggregated at the class level. To address this limitation, this paper introduces CGLE (Class-label Graph Link Estimator), a novel framework designed to augment GNN-based link prediction models. CGLE operates by constructing a class-conditioned link probability matrix, where each entry represents the probability of a link forming between two node classes. This matrix is derived from either available ground-truth labels or from pseudo-labels obtained through clustering. The resulting class-based prior is then concatenated with the structural link embedding from a backbone GNN, and the combined representation is processed by a Multi-Layer Perceptron (MLP) for the final prediction. Crucially, CGLE's logic is encapsulated in an efficient preprocessing stage, leaving the computational complexity of the underlying GNN model unaffected. We validate our approach through extensive experiments on a broad suite of benchmark datasets, covering both homophilous and sparse heterophilous graphs. The results show that CGLE yields substantial performance gains over strong baselines such as NCN and NCNC, with improvements in HR@100 of over 10 percentage points on homophilous datasets like Pubmed and DBLP. On sparse heterophilous graphs, CGLE delivers an MRR improvement of over 4% on the Chameleon dataset. Our work underscores the efficacy of integrating global, data-driven semantic priors, presenting a compelling alternative to the pursuit of increasingly complex model architectures. Code to reproduce our findings is available at: https://github.com/data-iitd/cgle-icdm2025.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Past-aware game-theoretic centrality in complex contagion dynamics</title>
<link>https://arxiv.org/abs/2511.07157</link>
<guid>https://arxiv.org/abs/2511.07157</guid>
<content:encoded><![CDATA[
<div> centrality measures, game theory, collaboration, influence maximization, contagion dynamics  
Summary:  
- The paper introduces past-aware game-theoretic centrality, a method that considers the collaborative contribution of nodes in a network, including uncertain and certain collaborators.  
- It extends a general framework for computing standard game-theoretic centrality to the past-aware scenario.  
- A new heuristic is developed for influence maximization problems in complex contagion dynamics, focusing on processes that require reinforcement from multiple neighbors to spread.  
- An explicit formula for past-aware centrality score is derived, enabling scalable algorithms to identify the most influential nodes.  
- The proposed approach outperforms the standard greedy approach in both efficiency and solution quality for most cases.  

<br /><br />Summary: <div>
arXiv:2511.07157v1 Announce Type: new 
Abstract: In this paper, we introduce past-aware game-theoretic centrality, a class of centrality measures that captures the collaborative contribution of nodes in a network, accounting for both uncertain and certain collaborators. A general framework for computing standard game-theoretic centrality is extended to the past-aware case. As an application, we develop a new heuristic for different versions of the influence maximization problems in complex contagion dynamics, which models processes requiring reinforcement from multiple neighbors to spread. A computationally efficient explicit formula for the corresponding past-aware centrality score is derived, leading to scalable algorithms for identifying the most influential nodes, which in most cases outperform the standard greedy approach in both efficiency and solution quality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deception Decoder: Proposing a Human-Focused Framework for Identifying AI-Generated Content on Social Media</title>
<link>https://arxiv.org/abs/2511.05555</link>
<guid>https://arxiv.org/abs/2511.05555</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, misinformation, disinformation, Deception Decoder, social media <br />
<br />
Summary: 
Generative AI (GenAI) is a significant threat to the integrity of information in the public sphere, particularly on social media platforms. The focus of research has been on automated detection methods, but concerns about false positives and biases persist. This dissertation presents the Deception Decoder, a user-friendly framework to identify AI-generated misinformation and disinformation in text, image, and video. The framework was developed through a comparative synthesis of existing models and refined with a focus group session. Initial testing shows promising results, but further research is needed to ensure its effectiveness across user groups over time. The human-centered approach of the Deception Decoder offers a different perspective on combating the spread of misleading content online. <br /><br />Summary: <div>
arXiv:2511.05555v1 Announce Type: cross 
Abstract: Generative AI (GenAI) poses a substantial threat to the integrity of information within the contemporary public sphere, which increasingly relies on social media platforms as intermediaries for news consumption. At present, most research efforts are directed toward automated and machine learning-based detection methods, despite growing concerns regarding false positives, social and political biases, and susceptibility to circumvention. This dissertation instead adopts a human-centred approach. It proposes the Deception Decoder; a multimodal, systematic, and topological framework designed to support general users in identifying AI-generated misinformation and disinformation across text, image, and video. The framework was developed through a comparative synthesis of existing models, supplemented by a content analysis of GenAI-video, and refined through a small-scale focus group session. While initial testing indicates promising improvements, further research is required to confirm its generalisability across user groups, and sustained effectiveness over time.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network and Risk Analysis of Surety Bonds</title>
<link>https://arxiv.org/abs/2511.05691</link>
<guid>https://arxiv.org/abs/2511.05691</guid>
<content:encoded><![CDATA[
<div> Surety bonds, contractor network, risk propagation, Friedkin-Johnsen model, network effects <br />
Summary: <br />
- The study focuses on surety bonds in the context of a contractor network, where project failures can occur due to incomplete obligations propagating through the network.
- A network approach is taken, modeling the contractor network as a directed graph to understand risk propagation.
- By extending the Friedkin-Johnsen model and introducing a stochastic process, the study simulates principal failures across the network.
- The theoretical analysis suggests that considering network effects leads to an increase in both average risk and right-tail risk for the surety organization.
- Data from an insurance company validates the findings, showing a 2% higher exposure when network effects are taken into account. <br /> <div>
arXiv:2511.05691v1 Announce Type: cross 
Abstract: Surety bonds are financial agreements between a contractor (principal) and obligee (project owner) to complete a project. However, most large-scale projects involve multiple contractors, creating a network and introducing the possibility of incomplete obligations to propagate and result in project failures. Typical models for risk assessment assume independent failure probabilities within each contractor. However, we take a network approach, modeling the contractor network as a directed graph where nodes represent contractors and project owners and edges represent contractual obligations with associated financial records. To understand risk propagation throughout the contractor network, we extend the celebrated Friedkin-Johnsen model and introduce a stochastic process to simulate principal failures across the network. From a theoretical perspective, we show that under natural monotonicity conditions on the contractor network, incorporating network effects leads to increases in both the average risk and the tail probability mass of the loss distribution (i.e. larger right-tail risk) for the surety organization. We further use data from a partnering insurance company to validate our findings, estimating an approximately 2% higher exposure when accounting for network effects.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design and Implementation of Data Acquisition and Analysis System for Programming Debugging Process Based On VS Code Plug-In</title>
<link>https://arxiv.org/abs/2511.05825</link>
<guid>https://arxiv.org/abs/2511.05825</guid>
<content:encoded><![CDATA[
<div> debugging, programming, data acquisition, analysis, behavior<br />
<br />
Summary: This paper presents a data acquisition and analysis system for programming debugging process using a VS Code plug-in to enhance students' debugging ability training. The system captures real-time debugging behavior in the local editor, uploads data to a platform database for monitoring and feedback, and offers precise guidance to teachers. It introduces a debugging behavior analysis model based on abstract syntax tree, node annotation, sequence recognition, and cluster analysis to track the context of students' debugging process and identify key features accurately. The system supports multi-file and multi-task debugging scenarios, improving the capturing ability of debugging data and refining data analysis. Practical teaching tests verify the system's feasibility, stability, and effectiveness in supporting procedural evaluation in programming debugging teaching, offering a new direction for debugging behavior analysis research. <div>
arXiv:2511.05825v1 Announce Type: cross 
Abstract: In order to meet the needs of students' programming debugging ability training, this paper designs and implements a data acquisition and analysis system for programming debugging process based on VS Code plug-in, which aims to solve the limitation of traditional assessment methods that are difficult to fully evaluate students' debugging ability. The system supports a variety of programming languages, integrates debugging tasks and data acquisition functions, captures students' debugging behavior in the local editor in real time, and uploads the data to the platform database to realize the whole process monitoring and feedback, provides accurate debugging guidance for teachers, and improves the teaching effect. In terms of data analysis, the system proposed a debugging behavior analysis model based on abstract syntax tree, combined with node annotation, sequence recognition and cluster analysis and other technologies, to automatically track the context of students' debugging process and accurately identify key features in the debugging path. Through this tool, the system realizes the intelligent identification and labeling of the debugging direction and behavior pattern, and improves the refinement level of debugging data analysis. In this research system, a complex debugging scenario of multi-file and multi-task is introduced into the debugging problem design, which optimizes the multi-dimensional capturing ability of debugging data and lays a foundation for accurate debugging behavior analysis. Through several practical teaching tests, the feasibility and stability of the system are verified, which proves that it can effectively support procedural evaluation in programming debugging teaching, and provides a new direction for debugging behavior analysis research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss</title>
<link>https://arxiv.org/abs/2511.06402</link>
<guid>https://arxiv.org/abs/2511.06402</guid>
<content:encoded><![CDATA[
<div> Keywords: Sugar dating, social media, content detection, transformer-based framework, class imbalance<br />
Summary: <br />
The study focuses on detecting sugar dating-related content on social media, which raises concerns about intimate relationships' commercialization and normalization of transactional relationships. SugarTextNet, a transformer-based framework, is introduced to identify such posts by capturing subtle linguistic cues and addressing class imbalance. Context-Aware Focal Loss, a tailored loss function, enhances minority-class detection. Evaluation on a dataset of 3,067 Chinese social media posts shows superior performance compared to traditional and deep learning models. Ablation studies confirm the importance of each component in the framework. The research underscores the significance of domain-specific modeling for sensitive content detection and offers a robust solution for content moderation on mainstream social media platforms. <br /><br />Summary: <div>
arXiv:2511.06402v1 Announce Type: cross 
Abstract: Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms</title>
<link>https://arxiv.org/abs/2511.06448</link>
<guid>https://arxiv.org/abs/2511.06448</guid>
<content:encoded><![CDATA[
<div> Keywords: collective financial fraud, large language model agents, MultiAgentFraudBench, fraud scenarios, mitigation strategies

Summary: 
The study examines the risks of collective financial fraud in large multi-agent systems driven by large language model agents. It investigates how agents can collaborate in fraudulent activities, the amplification of risks through collaboration, and factors influencing fraud success. A benchmark called MultiAgentFraudBench is introduced to simulate realistic online fraud scenarios. Key factors impacting fraud success, such as interaction depth and activity level, are analyzed. Mitigation strategies proposed include incorporating content warnings in fraudulent communications, using LLMs to monitor and block potentially malicious agents, and promoting group resilience through societal-level information sharing. The study also observes that malicious agents can adapt to interventions. The research underscores the real-world threats posed by multi-agent financial fraud and suggests practical steps to address them. The code for the study is available on GitHub at https://github.com/zheng977/MutiAgent4Fraud. 

<br /><br />Summary: <div>
arXiv:2511.06448v1 Announce Type: cross 
Abstract: In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Dyadic Barrier: Rethinking Fairness in Link Prediction Beyond Demographic Parity</title>
<link>https://arxiv.org/abs/2511.06568</link>
<guid>https://arxiv.org/abs/2511.06568</guid>
<content:encoded><![CDATA[
<div> Keywords: link prediction, fairness, graph machine learning, bias mitigation, demographic parity

Summary:<br /><br />Link prediction is a crucial task in graph machine learning, but biases in predictions can worsen societal inequalities. Existing fairness measures based on demographic parity may overlook disparities across subgroups. Furthermore, demographic parity may not be suitable for ranking tasks like link prediction. In this study, the limitations of current fairness evaluations are formalized, and a new framework is proposed for a more comprehensive assessment. A post-processing method combined with separate link predictors is suggested as an effective way to mitigate bias and achieve optimal fairness-utility trade-offs. This approach improves fairness in link prediction tasks and sets new standards in addressing bias in machine learning models. <div>
arXiv:2511.06568v1 Announce Type: cross 
Abstract: Link prediction is a fundamental task in graph machine learning with applications, ranging from social recommendation to knowledge graph completion. Fairness in this setting is critical, as biased predictions can exacerbate societal inequalities. Prior work adopts a dyadic definition of fairness, enforcing fairness through demographic parity between intra-group and inter-group link predictions. However, we show that this dyadic framing can obscure underlying disparities across subgroups, allowing systemic biases to go undetected. Moreover, we argue that demographic parity does not meet desired properties for fairness assessment in ranking-based tasks such as link prediction. We formalize the limitations of existing fairness evaluations and propose a framework that enables a more expressive assessment. Additionally, we propose a lightweight post-processing method combined with decoupled link predictors that effectively mitigates bias and achieves state-of-the-art fairness-utility trade-offs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Collaborative Model for Improving Information Sharing among Cancer Care Groups using Software Engineering Principles</title>
<link>https://arxiv.org/abs/2511.06885</link>
<guid>https://arxiv.org/abs/2511.06885</guid>
<content:encoded><![CDATA[
<div> GitHub, cancer, information sharing, software engineering, care groups
Summary:
The article discusses the challenges in cancer treatment due to delays and miscommunication among care groups. It proposes a model for information sharing based on software engineering principles, inspired by GitHub's version control system. By involving all stakeholders, including patient caretakers and administrators, the model aims to reduce delays and improve coordination in cancer case management. Using AnyLogic simulation software, the model shows that principles from software engineering can be applied to enhance collaboration and information sharing. This approach can lead to early diagnosis, improve treatment outcomes, and increase the chances of patient survival in cancer care. <div>
arXiv:2511.06885v1 Announce Type: cross 
Abstract: Effective treatment of cancer requires early diagnosis which involves the patient's awareness of the early signs and symptoms, leading to a consultation with a health provider, who would then promptly refer the patient for confirmation of the diagnosis and thereafter treatment. However, this is not always the case because of delays arising from limited skilled manpower and health information management systems that are neither integrated nor organized in their design hence leading to information gap among care groups. Existing methods focus on using accumulated data to support decision making, enhancing the sharing of secondary data while others exclude some critical stakeholders like patient caretakers and administrators thus, leaving an information gap that creates delays and miscommunication during case management. We however notice some similarities between cancer treatment and software engineering information management especially when progress history needs to be maintained (versioning).
  We analyze the similarities and propose a model for information sharing among cancer care groups using the software engineering principles approach. We model for reducing delays and improving coordination among care groups in cancer case management. Model design was guided by software engineering principles adopted in GitHub version control system for bug fixing in open-source code projects. Any-Logic simulation software was used to mimic the model realism in a virtual environment. Results show that bug resolution principles from software engineering and GitHub version control system can be adopted to coordinate collaboration and information sharing among care groups in a cancer case management environment while involving all stakeholders to improve care treatment outcomes, ensure early diagnosis and increase patient's survival chances.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization</title>
<link>https://arxiv.org/abs/2511.06937</link>
<guid>https://arxiv.org/abs/2511.06937</guid>
<content:encoded><![CDATA[
<div> Diffusion models, ReFiT framework, Reinforcement learning, recommendation systems, collaborative filtering<br />
Summary:<br />
ReFiT introduces a new framework for recommender systems that combines diffusion models with Reinforcement Learning (RL) fine-tuning. Unlike previous RL approaches, ReFiT uses a task-aligned design to optimize recommendation quality. By formulating the denoising trajectory as a Markov decision process (MDP) and integrating a collaborative signal-aware reward function, ReFiT maximizes exact log-likelihood of user-item interactions. Experimental results on real-world datasets show that ReFiT outperforms competitors by up to 36.3% in sequential recommendation, exhibits linear complexity in user/item numbers, and generalizes well across different recommendation scenarios. The framework's source code and datasets are publicly available for further research. <br /> <div>
arXiv:2511.06937v1 Announce Type: cross 
Abstract: Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Policy Effects through Opinion Dynamics and Network Sampling</title>
<link>https://arxiv.org/abs/2501.08150</link>
<guid>https://arxiv.org/abs/2501.08150</guid>
<content:encoded><![CDATA[
<div> social dynamics, policy effectiveness, network topology, opinion change, policymaker decisions

Summary: 
This paper explores how social dynamics influence public opinion towards new policies and how policymakers can quantify these effects. By considering different scenarios of policy revelation within a population network, the study examines how opinions evolve through discussions and debates. The research evaluates the impact of network topology and social interactions on policy beliefs, using the Wasserstein distance as a measure. Through numerical analyses on both simulated and real-life network datasets, the paper aims to provide policymakers with a quantitative framework for assessing policy effectiveness amidst complex network structures and resource constraints. <div>
arXiv:2501.08150v2 Announce Type: replace 
Abstract: In the process of enacting or introducing a new policy, policymakers frequently consider the population's responses. These considerations are critical for effective governance. There are numerous methods to gauge the ground sentiment from a subset of the population; examples include surveys or listening to various feedback channels. Many conventional approaches implicitly assume that opinions are static; however, in reality, the population will discuss and debate these new policies among themselves, and reform new opinions in the process. In this paper, we pose the following questions: Can we quantify the effect of these social dynamics on the broader opinion towards a new policy? Given some information about the relationship network that underlies the population, how does overall opinion change post-discussion? We investigate three different settings in which the policy is revealed: respondents who do not know each other, groups of respondents who all know each other, and respondents chosen randomly. By controlling who the policy is revealed to, we control the degree of discussion among the population. We quantify how these factors affect the changes in policy beliefs via the Wasserstein distance between the empirically observed data post-discussion and its distribution pre-discussion. We also provide several numerical analyses based on generated network and real-life network datasets. Our work aims to address the challenges associated with network topology and social interactions, and provide policymakers with a quantitative lens to assess policy effectiveness in the face of resource constraints and network complexities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Cross-type Homophily of Heterogeneous Graphs: Understanding and Unleashing</title>
<link>https://arxiv.org/abs/2501.14600</link>
<guid>https://arxiv.org/abs/2501.14600</guid>
<content:encoded><![CDATA[
<div> Homophily, network science, graph neural networks, heterogeneous graphs, Cross-Type Homophily Ratio<br />
Summary:<br />
The study focuses on homophily in heterogeneous graphs and its impact on graph neural networks. Traditional homophily metrics are inadequate for heterogeneous graphs due to diverse node types. The Cross-Type Homophily Ratio (CHR) is introduced to measure homophily based on similarity across different node types. Cross-Type Homophily-guided Graph Editing (CTHGE) optimizes cross-type connectivity in heterogeneous graph neural networks (HGNNs) using CHR. Experiments on five datasets validate CTHGE's effectiveness, showing over 25% improvement in HGNN performance on node classification tasks. This approach provides valuable insights into the role of cross-type homophily in learning from heterogeneous graphs.<br /> <div>
arXiv:2501.14600v2 Announce Type: replace 
Abstract: Homophily, the tendency of similar nodes to connect, is a fundamental phenomenon in network science and a critical factor in the performance of graph neural networks (GNNs). While existing studies primarily explore homophily in homogeneous graphs, where nodes share the same type, real-world networks are often more accurately modeled as heterogeneous graphs (HGs) with diverse node types and intricate cross-type interactions. This structural diversity complicates the analysis of homophily, as traditional homophily metrics fail to account for distinct label spaces across node types. To address this limitation, we introduce the Cross-Type Homophily Ratio (CHR), a novel metric that quantifies homophily based on the similarity of target information across different node types. Additionally, we propose Cross-Type Homophily-guided Graph Editing (CTHGE), a novel method for improving heterogeneous graph neural networks (HGNNs) performance by optimizing cross-type connectivity using Cross-Type Homophily Ratio. Extensive experiments on five HG datasets with nine HGNNs validate the effectiveness of CTHGE, which delivers a maximum relative performance improvement of over 25% for HGNNs on node classification tasks, offering a fresh perspective on cross-type homophily in HGs learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindVote: When AI Meets the Wild West of Social Media Opinion</title>
<link>https://arxiv.org/abs/2505.14422</link>
<guid>https://arxiv.org/abs/2505.14422</guid>
<content:encoded><![CDATA[
<div> Benchmark, Large Language Models, Public Opinion, Social Media, MindVote

Summary:
The article introduces MindVote, a benchmark for predicting public opinion distribution using social media discourse. It addresses the limitations of current benchmarks that rely on structured surveys by incorporating the dynamic, context-rich nature of social media environments. MindVote is built from naturalistic polls from Reddit and Weibo, covering 23 topics with annotations for platform, topical, and temporal context. The benchmark evaluates 15 Large Language Models (LLMs) for their ability to predict public opinion in authentic social media settings. By shifting the evaluation paradigm from surveys to social media data, MindVote provides a more ecologically valid framework for testing LLMs and advancing the development of socially intelligent AI systems. <div>
arXiv:2505.14422v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used as scalable tools for pilot testing, predicting public opinion distributions before deploying costly surveys. To serve as effective pilot testing tools, the performance of these LLMs is typically benchmarked against their ability to reproduce the outcomes of past structured surveys. This evaluation paradigm, however, is misaligned with the dynamic, context-rich social media environments where public opinion is increasingly formed and expressed. By design, surveys strip away the social, cultural, and temporal context that shapes public opinion, and LLM benchmarks built on this paradigm inherit these critical limitations. To bridge this gap, we introduce MindVote, the first benchmark for public opinion distribution prediction grounded in authentic social media discourse. MindVote is constructed from 3,918 naturalistic polls sourced from Reddit and Weibo, spanning 23 topics and enriched with detailed annotations for platform, topical, and temporal context. Using this benchmark, we conduct a comprehensive evaluation of 15 LLMs. MindVote provides a robust, ecologically valid framework to move beyond survey-based evaluations and advance the development of more socially intelligent AI systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Prosocial Behavior Theory in LLM Agents under Policy-Induced Inequities</title>
<link>https://arxiv.org/abs/2505.15857</link>
<guid>https://arxiv.org/abs/2505.15857</guid>
<content:encoded><![CDATA[
<div> Simulation framework, prosocial behavior, large language models, social conditions, fairness-based punishment

Summary:
ProSim is introduced as a simulation framework to model prosocial behavior in Large Language Model (LLM) agents in various social contexts. The study conducted three progressive studies to evaluate prosocial alignment. Firstly, LLM agents exhibit human-like prosocial behavior and respond to normative policy interventions. Secondly, agents engage in fairness-based third-party punishment and respond to variations in inequity magnitude and enforcement cost. Thirdly, policy-induced inequities suppress prosocial behavior and contribute to norm erosion in social networks. This research enhances understanding of how institutional dynamics influence the development, decline, and spread of prosocial norms in societies driven by autonomous agents.<br /><br />Summary: <div>
arXiv:2505.15857v2 Announce Type: replace 
Abstract: As large language models (LLMs) increasingly operate as autonomous agents in social contexts, evaluating their capacity for prosocial behavior is both theoretically and practically critical. However, existing research has primarily relied on static, economically framed paradigms, lacking models that capture the dynamic evolution of prosociality and its sensitivity to structural inequities. To address these gaps, we introduce ProSim, a simulation framework for modeling the prosocial behavior in LLM agents across diverse social conditions. We conduct three progressive studies to assess prosocial alignment. First, we demonstrate that LLM agents can exhibit human-like prosocial behavior across a broad range of real-world scenarios and adapt to normative policy interventions. Second, we find that agents engage in fairness-based third-party punishment and respond systematically to variations in inequity magnitude and enforcement cost. Third, we show that policy-induced inequities suppress prosocial behavior, propagate norm erosion through social networks. These findings advance prosocial behavior theory by elucidating how institutional dynamics shape the emergence, decay, and diffusion of prosocial norms in agent-driven societies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Curse of Shared Knowledge: Recursive Belief Reasoning in a Coordination Game with Imperfect Information</title>
<link>https://arxiv.org/abs/2008.08849</link>
<guid>https://arxiv.org/abs/2008.08849</guid>
<content:encoded><![CDATA[
<div> Common Knowledge, Shared Knowledge, Coordination, Belief Attribution, Experiment<br />
Summary:<br />
- Common knowledge is crucial for successful group coordination.
- Humans often rely on attributing beliefs and intentions to infer shared knowledge.
- However, attributing shared knowledge is limited in depth, leading to coordination failures.
- Three experiments with 802 participants examined the distinction between common knowledge and nth-order shared knowledge.
- The results reveal that participants struggle to differentiate between common knowledge and shared knowledge, often prioritizing coordination even at shallow depths of shared knowledge despite significant penalties. <div>
arXiv:2008.08849v2 Announce Type: replace-cross 
Abstract: Common knowledge is a necessary condition for safe group coordination. When common knowledge can not be obtained, humans routinely use their ability to attribute beliefs and intentions in order to infer what is known. But such shared knowledge attributions are limited in depth and therefore prone to coordination failures, because any finite-order knowledge attribution allows for an even higher order attribution that may change what is known by whom. In three separate experiments we investigate to which degree human participants (N=802) are able to recognize the difference between common knowledge and nth-order shared knowledge. We use a new two-person coordination game with imperfect information that is able to cast the recursive game structure and higher-order uncertainties into a simple, everyday-like setting. Our results show that participants have a very hard time accepting the fact that common knowledge is not reducible to shared knowledge. Instead, participants try to coordinate even at the shallowest depths of shared knowledge and in spite of huge payoff penalties.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ginzburg--Landau Functionals in the Large-Graph Limit</title>
<link>https://arxiv.org/abs/2408.00422</link>
<guid>https://arxiv.org/abs/2408.00422</guid>
<content:encoded><![CDATA[
<div> Keywords: Ginzburg-Landau functionals, large-graph limits, graph clustering, graphons, total-variation functional

Summary: 
Ginzburg-Landau (GL) functionals on graphs, a relaxation of graph-cut functionals, are studied in the context of large-graph limits. By viewing graphs as nonlocal kernels, the authors analyze GL functionals on sequences of growing graphs converging to graphons. The graph GL functional converges to a continuous and nonlocal graphon GL functional. Relationships between sharp-interface limits of the functionals and a nonlocal total-variation functional are explored. The limiting GL functional is expressed using Young measures, providing a probabilistic interpretation in the large-graph limit. Additionally, examples of GL minimizers for various graphon families are determined to aid in understanding the graphon GL functional.

<br /><br />Summary: Ginzburg-Landau functionals on graphs, a relaxation of graph-cut functionals, are studied in the context of large-graph limits. By viewing graphs as nonlocal kernels, the authors analyze GL functionals on sequences of growing graphs converging to graphons. The graph GL functional converges to a continuous and nonlocal graphon GL functional. Relationships between sharp-interface limits of the functionals and a nonlocal total-variation functional are explored. The limiting GL functional is expressed using Young measures, providing a probabilistic interpretation in the large-graph limit. Additionally, examples of GL minimizers for various graphon families are determined to aid in understanding the graphon GL functional. <div>
arXiv:2408.00422v2 Announce Type: replace-cross 
Abstract: Ginzburg--Landau (GL) functionals on graphs, which are relaxations of graph-cut functionals on graphs, have yielded a variety of insights in image segmentation and graph clustering. In this paper, we study large-graph limits of GL functionals by taking a functional-analytic view of graphs as nonlocal kernels. For a graph $W_n$ with $n$ nodes, the corresponding graph GL functional $\GL^{W_n}_\ep$ is an energy for functions on $W_n$. We minimize GL functionals on sequences of growing graphs that converge to functions called graphons. For such sequences of graphs, we show that the graph GL functional $\Gamma$-converges to a continuous and nonlocal functional that we call the \emph{graphon GL functional}. We also investigate the sharp-interface limits of the graph GL and graphon GL functionals, and we relate these limits to a nonlocal total-variation (TV) functional. We express the limiting GL functional in terms of Young measures and thereby obtain a probabilistic interpretation of the variational problem in the large-graph limit. Finally, to develop intuition about the graphon GL functional, we determine the GL minimizer for several example families of graphons.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Misinformation Vulnerabilities With Agent Personas</title>
<link>https://arxiv.org/abs/2511.04697</link>
<guid>https://arxiv.org/abs/2511.04697</guid>
<content:encoded><![CDATA[
<div> Keywords: Disinformation campaigns, Agent-based simulation, Large Language Models, Information responses, Mental schemas

Summary:
Large Language Models (LLMs) are used in an agent-based simulation to model responses to misinformation in different populations. Agent personas from various professions and mental schemas are created to evaluate reactions to news headlines. The study shows that LLM-generated agents closely align with ground-truth labels and human predictions, supporting their use in analyzing information responses. Mental schemas have a greater impact on how agents interpret misinformation compared to professional backgrounds. This research validates the use of LLMs as agents in analyzing trust, polarization, and susceptibility to deceptive content in complex social systems.<br /><br />Summary: Large Language Models are employed in an agent-based simulation to study responses to misinformation across various populations. The study validates the use of LLMs as proxies for analyzing information responses and highlights the influence of mental schemas on misinformation interpretation, surpassing the impact of professional backgrounds. This research lays the foundation for further exploration of trust, polarization, and susceptibility to deceptive content in intricate social systems. <div>
arXiv:2511.04697v1 Announce Type: new 
Abstract: Disinformation campaigns can distort public perception and destabilize institutions. Understanding how different populations respond to information is crucial for designing effective interventions, yet real-world experimentation is impractical and ethically challenging. To address this, we develop an agent-based simulation using Large Language Models (LLMs) to model responses to misinformation. We construct agent personas spanning five professions and three mental schemas, and evaluate their reactions to news headlines. Our findings show that LLM-generated agents align closely with ground-truth labels and human predictions, supporting their use as proxies for studying information responses. We also find that mental schemas, more than professional background, influence how agents interpret misinformation. This work provides a validation of LLMs to be used as agents in an agent-based model of an information network for analyzing trust, polarization, and susceptibility to deceptive content in complex social systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication-Constrained Private Decentralized Online Personalized Mean Estimation</title>
<link>https://arxiv.org/abs/2511.04702</link>
<guid>https://arxiv.org/abs/2511.04702</guid>
<content:encoded><![CDATA[
<div> consensus-based algorithm, privacy constraint, collaborative personalized mean estimation, differential privacy, communication restriction

Summary:
The article addresses the problem of collaborative personalized mean estimation under a privacy constraint in an environment where agents receive data from unknown distributions. A consensus-based algorithm is proposed to protect data privacy while achieving convergence. The theoretical analysis shows that collaboration leads to faster convergence compared to a fully local approach. This advantage is observed under an oracle decision rule and with specific constraints on privacy level and connectivity between agents. The study highlights the benefits of private collaboration in an online setting with communication restrictions. Numerical results support the theoretical guarantee of faster-than-local convergence provided by the proposed algorithm. <div>
arXiv:2511.04702v1 Announce Type: new 
Abstract: We consider the problem of communication-constrained collaborative personalized mean estimation under a privacy constraint in an environment of several agents continuously receiving data according to arbitrary unknown agent-specific distributions. A consensus-based algorithm is studied under the framework of differential privacy in order to protect each agent's data. We give a theoretical convergence analysis of the proposed consensus-based algorithm for any bounded unknown distributions on the agents' data, showing that collaboration provides faster convergence than a fully local approach where agents do not share data, under an oracle decision rule and under some restrictions on the privacy level and the agents' connectivity, which illustrates the benefit of private collaboration in an online setting under a communication restriction on the agents. The theoretical faster-than-local convergence guarantee is backed up by several numerical results.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NCSAC: Effective Neural Community Search via Attribute-augmented Conductance</title>
<link>https://arxiv.org/abs/2511.04712</link>
<guid>https://arxiv.org/abs/2511.04712</guid>
<content:encoded><![CDATA[
<div> Attribute-augmented conductance, neural community search, deep learning, community detection, graph optimization <br />
Summary: <br />
- The paper introduces NCSAC, a novel approach that integrates deep learning with rule-based constraints to enhance community search.
- NCSAC combines structural proximity, attribute similarity, and reinforcement learning to identify locally dense communities.
- It proposes the concept of attribute-augmented conductance to refine candidate communities effectively.
- Extensive experiments on real-world graphs demonstrate the superiority of NCSAC in terms of accuracy, efficiency, and scalability.
- The proposed solution outperforms state-of-the-art methods, achieving significant F1-score improvements ranging from 5.3% to 42.4%. <br /> <div>
arXiv:2511.04712v1 Announce Type: new 
Abstract: Identifying locally dense communities closely connected to the user-initiated query node is crucial for a wide range of applications. Existing approaches either solely depend on rule-based constraints or exclusively utilize deep learning technologies to identify target communities. Therefore, an important question is proposed: can deep learning be integrated with rule-based constraints to elevate the quality of community search? In this paper, we affirmatively address this question by introducing a novel approach called Neural Community Search via Attribute-augmented Conductance, abbreviated as NCSAC. Specifically, NCSAC first proposes a novel concept of attribute-augmented conductance, which harmoniously blends the (internal and external) structural proximity and the attribute similarity. Then, NCSAC extracts a coarse candidate community of satisfactory quality using the proposed attribute-augmented conductance. Subsequently, NCSAC frames the community search as a graph optimization task, refining the candidate community through sophisticated reinforcement learning techniques, thereby producing high-quality results. Extensive experiments on six real-world graphs and ten competitors demonstrate the superiority of our solutions in terms of accuracy, efficiency, and scalability. Notably, the proposed solution outperforms state-of-the-art methods, achieving an impressive F1-score improvement ranging from 5.3\% to 42.4\%. For reproducibility purposes, the source code is available at https://github.com/longlonglin/ncsac.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zoo of Centralities: Encyclopedia of Node Metrics in Complex Networks</title>
<link>https://arxiv.org/abs/2511.05122</link>
<guid>https://arxiv.org/abs/2511.05122</guid>
<content:encoded><![CDATA[
<div> Keywords: Centrality, Network Science, Complex Systems, Centrality Measures, Centrality Zoo <br />
Summary: 
Centrality is a crucial concept in network science, used to understand the structure and dynamics of various complex systems. However, the lack of a universal definition has led to the development of numerous centrality measures, creating a "zoo" of metrics each capturing node importance in different ways. This diversity has resulted in challenges such as discoverability, redundancy, naming conflicts, validation, and accessibility. To address these issues, a catalog of over 400 centrality measures has been compiled, with detailed descriptions and references to original sources. The Centrality Zoo website offers an interactive platform for exploring, comparing, and implementing these measures. This effort represents a systematic and comprehensive approach to organizing and presenting centrality measures, providing a valuable resource for researchers in the field.<br /><br />Summary: <div>
arXiv:2511.05122v1 Announce Type: new 
Abstract: Centrality is a fundamental concept in network science, providing critical insights into the structure and dynamics of complex systems such as social, transportation, biological and financial networks. Despite its extensive use, there is no universally accepted definition of centrality, leading to the development of a vast array of distinct centrality measures. These measures have grown so numerous that they resemble a 'zoo', each representing a unique approach to capturing node importance within a network. However, the increasing number of metrics being developed has led to several challenges, including issues of discoverability, redundancy, naming conflicts, validation and accessibility. This work aims to address these challenges by providing a comprehensive catalog of over 400 centrality measures, along with clear descriptions and references to original sources. While not exhaustive, this compilation represents the most extensive and systematic effort to date in organizing and presenting centrality measures. We also encourage readers to explore and contribute to the Centrality Zoo website at https://centralityzoo.github.io/, which provides an interactive platform for discovering, comparing and implementing centrality measures.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cooperation Under Network-Constrained Communication</title>
<link>https://arxiv.org/abs/2511.05290</link>
<guid>https://arxiv.org/abs/2511.05290</guid>
<content:encoded><![CDATA[
<div> communication, cooperation, distributed games, network topology, Prisoner's Dilemma

Summary:
In this paper, the focus is on studying cooperation in distributed games with network-constrained communication. A sufficient condition for cooperative equilibrium is derived based on the network topology delaying communication between agents. Each player deploys agents at various locations, engaging in local Prisoner's Dilemma interactions. The derived condition hinges on the network diameter and the number of locations, exploring scenarios of instantaneous, delayed, and proportionally delayed communication. The study also delves into scale-free communication networks where the network diameter grows sub-linearly with the number of locations. These analyses provide insights into how communication latency and network design influence the emergence of distributed cooperation. <div>
arXiv:2511.05290v1 Announce Type: cross 
Abstract: In this paper, we study cooperation in distributed games under network-constrained communication. Building on the framework of Monderer and Tennenholtz (1999), we derive a sufficient condition for cooperative equilibrium in settings where communication between agents is delayed by the underlying network topology. Each player deploys an agent at every location, and local interactions follow a Prisoner's Dilemma structure. We derive a sufficient condition that depends on the network diameter and the number of locations, and analyze extreme cases of instantaneous, delayed, and proportionally delayed communication. We also discuss the asymptotic case of scale-free communication networks, in which the network diameter grows sub-linearly in the number of locations. These insights clarify how communication latency and network design jointly determine the emergence of distributed cooperation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Listening Between the Lines: Decoding Podcast Narratives with Language Modeling</title>
<link>https://arxiv.org/abs/2511.05310</link>
<guid>https://arxiv.org/abs/2511.05310</guid>
<content:encoded><![CDATA[
<div> Podcasts, narrative frames, automated analysis, BERT model, discourse trends <br />
<br />
Summary: This paper explores the use of narrative frames in analyzing podcast data to understand how they influence public opinion. The conversational nature of podcasts poses a challenge for automated analysis, as existing models struggle to capture subtle narrative cues. To address this, the study develops a fine-tuned BERT model that links narrative frames to specific entities mentioned in conversations, enhancing the analysis of podcast narratives at scale. The novel frame-labeling methodology aligns closely with human judgment for messy, conversational data. The analysis reveals a systematic relationship between discussed topics and how they are presented, providing a robust framework for studying influence in digital media. This research contributes to a better understanding of how podcasts shape contemporary discourse and offers a new approach to analyzing narrative structures in digital media.<br /> <div>
arXiv:2511.05310v1 Announce Type: cross 
Abstract: Podcasts have become a central arena for shaping public opinion, making them a vital source for understanding contemporary discourse. Their typically unscripted, multi-themed, and conversational style offers a rich but complex form of data. To analyze how podcasts persuade and inform, we must examine their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant challenge for automated analysis. We show that existing large language models, typically trained on more structured text such as news articles, struggle to capture the subtle cues that human listeners rely on to identify narrative frames. As a result, current approaches fall short of accurately analyzing podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that explicitly links narrative frames to specific entities mentioned in the conversation, effectively grounding the abstract frame in concrete details. Our approach then uses these granular frame labels and correlates them with high-level topics to reveal broader discourse trends. The primary contributions of this paper are: (i) a novel frame-labeling methodology that more closely aligns with human judgment for messy, conversational data, and (ii) a new analysis that uncovers the systematic relationship between what is being discussed (the topic) and how it is being presented (the frame), offering a more robust framework for studying influence in digital media.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting narrative signals from public discourse: a network-based approach</title>
<link>https://arxiv.org/abs/2411.00702</link>
<guid>https://arxiv.org/abs/2411.00702</guid>
<content:encoded><![CDATA[
<div> extract, analyze, political narratives, digital textual corpora, graph-based formalism <br />
Summary: 
The article introduces a graph-based formalism and machine-guided method for extracting and analyzing political narratives from digital textual corpora using Abstract Meaning Representation (AMR). It emphasizes the importance of narratives in understanding societal issues such as polarization and misinformation. The proposed method involves extracting the meaning of sentences in the corpus using AMR and filtering graph representations for actors, relationships, events, and perspectivization. These core narrative signals are then reconstructed to reveal underlying political narratives through a combination of distant and close reading. The approach is demonstrated through a case study on State of the European Union addresses from 2010 to 2023. The method aims to provide researchers with a systematic way to analyze and surface political narratives from public discourse. <br /><br /> <div>
arXiv:2411.00702v2 Announce Type: replace-cross 
Abstract: Narratives are key interpretative devices by which humans make sense of political reality. As the significance of narratives for understanding current societal issues such as polarization and misinformation becomes increasingly evident, there is a growing demand for methods that support their empirical analysis. To this end, we propose a graph-based formalism and machine-guided method for extracting, representing, and analyzing selected narrative signals from digital textual corpora, based on Abstract Meaning Representation (AMR). The formalism and method introduced here specifically cater to the study of political narratives that figure in texts from digital media such as archived political speeches, social media posts, transcripts of parliamentary debates, and political manifestos on party websites. We approach the study of such political narratives as a problem of information retrieval: starting from a textual corpus, we first extract a graph-like representation of the meaning of each sentence in the corpus using AMR. Drawing on transferable concepts from narratology, we then apply a set of heuristics to filter these graphs for representations of 1) actors and their relationships, 2) the events in which these actors figure, and 3) traces of the perspectivization of these events. We approach these references to actors, events, and instances of perspectivization as core narrative signals that allude to larger political narratives. By systematically analyzing and re-assembling these signals into networks that guide the researcher to the relevant parts of the text, the underlying narratives can be reconstructed through a combination of distant and close reading. A case study of State of the European Union addresses (2010 -- 2023) demonstrates how the formalism can be used to inductively surface signals of political narratives from public discourse.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Equitable AI: Evaluating Cultural Expressiveness in LLMs for Latin American Contexts</title>
<link>https://arxiv.org/abs/2511.04090</link>
<guid>https://arxiv.org/abs/2511.04090</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Latin America, Bias, Dataset, Cultural Awareness

Summary:
This paper examines the bias in Artificial Intelligence (AI) systems towards economically advanced regions, which marginalize economically developing regions like Latin America due to imbalanced datasets. The dominance of English over Spanish, Portuguese, and indigenous languages in AI systems perpetuates biases and frames Latin American perspectives through a Western lens. To address this issue, the authors introduce a culturally aware dataset rooted in Latin American history and socio-political contexts to challenge Eurocentric models. Evaluating six language models on questions testing cultural context awareness, they find disparities in capturing Latin American perspectives and significant sentiment misalignment. Fine-tuning Mistral-7B with the culturally aware dataset improves its cultural expressiveness by 42.9%, advancing equitable AI development. The authors advocate for equitable AI development by prioritizing datasets that reflect Latin American history, indigenous knowledge, and diverse languages, while emphasizing community-centered approaches to amplify marginalized voices.

Summary: <div>
arXiv:2511.04090v1 Announce Type: new 
Abstract: Artificial intelligence (AI) systems often reflect biases from economically advanced regions, marginalizing contexts in economically developing regions like Latin America due to imbalanced datasets. This paper examines AI representations of diverse Latin American contexts, revealing disparities between data from economically advanced and developing regions. We highlight how the dominance of English over Spanish, Portuguese, and indigenous languages such as Quechua and Nahuatl perpetuates biases, framing Latin American perspectives through a Western lens. To address this, we introduce a culturally aware dataset rooted in Latin American history and socio-political contexts, challenging Eurocentric models. We evaluate six language models on questions testing cultural context awareness, using a novel Cultural Expressiveness metric, statistical tests, and linguistic analyses. Our findings show that some models better capture Latin American perspectives, while others exhibit significant sentiment misalignment (p < 0.001). Fine-tuning Mistral-7B with our dataset improves its cultural expressiveness by 42.9%, advancing equitable AI development. We advocate for equitable AI by prioritizing datasets that reflect Latin American history, indigenous knowledge, and diverse languages, while emphasizing community-centered approaches to amplify marginalized voices.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Launch-Day Diffusion: Tracking Hacker News Impact on GitHub Stars for AI Tools</title>
<link>https://arxiv.org/abs/2511.04453</link>
<guid>https://arxiv.org/abs/2511.04453</guid>
<content:encoded><![CDATA[
<div> APIs, GitHub stars, Hacker News, machine learning models, open-source projects
<br />
Summary: 
Social news platforms, particularly Hacker News, play a crucial role in the launch of open-source projects, but quantifying their immediate impact can be challenging. A reproducible demonstration system has been developed to track the relationship between Hacker News exposure and GitHub star growth for AI and LLM tools. Analyzing 138 repository launches from 2024-2025, the study shows significant effects: repositories typically gain 121 stars within 24 hours, 189 stars within 48 hours, and 289 stars within a week of being featured on Hacker News. Machine learning models and non-linear approaches are employed to identify key predictors of viral growth, highlighting the importance of posting timing. Contrary to expectations, the "Show HN" tag does not offer a statistical advantage after accounting for other factors. The study's automated framework allows for quick data collection, model training, and visualization generation, making it easily reproducible and extendable to other platforms. <div>
arXiv:2511.04453v1 Announce Type: new 
Abstract: Social news platforms have become key launch outlets for open-source projects, especially Hacker News (HN), though quantifying their immediate impact remains challenging. This paper presents a reproducible demonstration system that tracks how HN exposure translates into GitHub star growth for AI and LLM tools. Built entirely on public APIs, our pipeline analyzes 138 repository launches from 2024-2025 and reveals substantial launch effects: repositories gain an average of 121 stars within 24 hours, 189 stars within 48 hours, and 289 stars within a week of HN exposure. Through machine learning models (Elastic Net) and non-linear approaches (Gradient Boosting), we identify key predictors of viral growth. Posting timing appears as key factor--launching at optimal hours can mean hundreds of additional stars--while the "Show HN" tag shows no statistical advantage after controlling for other factors. The demonstration completes in under five minutes on standard hardware, automatically collecting data, training models, and generating visualizations through single-file scripts. This makes our findings immediately reproducible and the framework easily be extended to other platforms, providing both researchers and developers with actionable insights into launch dynamics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLM-based agents for social science research: insights from citation network simulations</title>
<link>https://arxiv.org/abs/2511.03758</link>
<guid>https://arxiv.org/abs/2511.03758</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, CiteAgent framework, citation networks, LLM-SE, LLM-LE

Summary: The study focuses on exploring the social attributes of Large Language Models (LLMs) in social simulation using the CiteAgent framework. The framework generates citation networks based on human behavior simulation with LLM-based agents, replicating real-world citation network phenomena like power-law distribution and citational distortion. Two research paradigms, LLM-SE (LLM-based Survey Experiment) and LLM-LE (LLM-based Laboratory Experiment), are introduced to analyze citation network phenomena and challenge existing theories. The study extends traditional science of science studies through idealized social experiments, providing insights for real-world academic environments. Through rigorous analyses and realistic simulations, the potential of LLMs in advancing science of science research in social science is demonstrated. <br /><br />Summary: The study introduces the CiteAgent framework for social simulation using LLMs, replicating real-world citation network phenomena. It establishes research paradigms LLM-SE and LLM-LE to analyze and challenge existing theories, extending the scope of science of science studies. <div>
arXiv:2511.03758v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) demonstrates their potential to encapsulate the logic and patterns inherent in human behavior simulation by leveraging extensive web data pre-training. However, the boundaries of LLM capabilities in social simulation remain unclear. To further explore the social attributes of LLMs, we introduce the CiteAgent framework, designed to generate citation networks based on human-behavior simulation with LLM-based agents. CiteAgent successfully captures predominant phenomena in real-world citation networks, including power-law distribution, citational distortion, and shrinking diameter. Building on this realistic simulation, we establish two LLM-based research paradigms in social science: LLM-SE (LLM-based Survey Experiment) and LLM-LE (LLM-based Laboratory Experiment). These paradigms facilitate rigorous analyses of citation network phenomena, allowing us to validate and challenge existing theories. Additionally, we extend the research scope of traditional science of science studies through idealized social experiments, with the simulation experiment results providing valuable insights for real-world academic environments. Our work demonstrates the potential of LLMs for advancing science of science research in social science.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistics of leaves in growing random trees</title>
<link>https://arxiv.org/abs/2511.04085</link>
<guid>https://arxiv.org/abs/2511.04085</guid>
<content:encoded><![CDATA[
<div> leaves, vertex, degree, distribution, trees
Summary:
- The study focuses on the role of leaves in graph structure, particularly in sparsely connected settings where they are prevalent.
- It introduces the concept of leaf degree, the number of leaves a vertex is connected to, and analyzes the associated leaf degree distribution.
- The research investigates the leaf degree distribution of random recursive trees (RRTs) and trees grown using a leaf-based preferential attachment mechanism.
- In RRTs, the leaf degree distribution decays factorially, contrasting with the purely geometric degree distribution.
- A one-parameter leaf-based growth model is proposed where new vertices attach to existing vertices based on their leaf degree, leading to different types of leaf degree distributions based on the attachment rate parameter a. 
<br /><br />Summary: <div>
arXiv:2511.04085v1 Announce Type: cross 
Abstract: Leaves, i.e., vertices of degree one, can play a significant role in graph structure, especially in sparsely connected settings in which leaves often constitute the largest fraction of vertices. We consider a leaf-based counterpart of the degree, namely, the leaf degree -- the number of leaves a vertex is connected to -- and the associated leaf degree distribution, analogous to the degree distribution. We determine the leaf degree distribution of random recursive trees (RRTs) and trees grown via a leaf-based preferential attachment mechanism that we introduce. The RRT leaf degree distribution decays factorially, in contrast with its purely geometric degree distribution. In the one-parameter leaf-based growth model, each new vertex attaches to an existing vertex with rate $\ell$ + a, where $\ell$ is the leaf degree of the existing vertex, and a > 0. The leaf degree distribution has a powerlaw tail when 0 < a < 1 and an exponential tail (with algebraic prefactor) for a > 1. The critical case of a = 1 has a leaf degree distribution with stretched exponential tail. We compute a variety of additional characteristics in these models and conjecture asymptotic equivalence of degree and leaf degree powerlaw tail exponent in the scale free regime. We highlight several avenues of possible extension for future studies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Turing Test Reveals Systematic Differences Between Human and AI Language</title>
<link>https://arxiv.org/abs/2511.04195</link>
<guid>https://arxiv.org/abs/2511.04195</guid>
<content:encoded><![CDATA[
<div> validation, language models, human-likeness, calibration, text generation

Summary:<br /><br />
The paper introduces a computational Turing test to assess the realism of Large Language Models (LLMs) in generating human-like text. It combines aggregate metrics and linguistic features to evaluate LLMs' performance in approximating human language within specific datasets. The study compares nine LLMs using various calibration strategies on social media platforms. The findings challenge existing assumptions, showing that even calibrated LLMs are distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models perform poorly compared to base models, and increasing model size does not improve human-likeness. The research highlights a trade-off between optimizing for human-likeness and maintaining semantic fidelity. These results provide a valuable framework for validating and calibrating LLM simulations while cautioning about their current limitations in capturing human communication. <div>
arXiv:2511.04195v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used in the social sciences to simulate human behavior, based on the assumption that they can generate realistic, human-like text. Yet this assumption remains largely untested. Existing validation efforts rely heavily on human-judgment-based evaluations -- testing whether humans can distinguish AI from human output -- despite evidence that such judgments are blunt and unreliable. As a result, the field lacks robust tools for assessing the realism of LLM-generated text or for calibrating models to real-world data. This paper makes two contributions. First, we introduce a computational Turing test: a validation framework that integrates aggregate metrics (BERT-based detectability and semantic similarity) with interpretable linguistic features (stylistic markers and topical patterns) to assess how closely LLMs approximate human language within a given dataset. Second, we systematically compare nine open-weight LLMs across five calibration strategies -- including fine-tuning, stylistic prompting, and context retrieval -- benchmarking their ability to reproduce user interactions on X (formerly Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the literature. Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models underperform their base counterparts, and scaling up model size does not enhance human-likeness. Crucially, we identify a trade-off: optimizing for human-likeness often comes at the cost of semantic fidelity, and vice versa. These results provide a much-needed scalable framework for validation and calibration in LLM simulations -- and offer a cautionary note about their current limitations in capturing human communication.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Graph Transformers Across Diverse Graphs and Tasks via Pre-training</title>
<link>https://arxiv.org/abs/2407.03953</link>
<guid>https://arxiv.org/abs/2407.03953</guid>
<content:encoded><![CDATA[
<div> Transformer-based, PGT, graph pre-training, inductive ability, scalability <br />
<br />
Summary: 
The article introduces a scalable transformer-based graph pre-training framework called PGT (Pre-trained Graph Transformer) aiming to develop a general graph pre-trained model with inductive ability for web-scale graphs. The framework utilizes a masked autoencoder architecture with two pre-training tasks: reconstructing node features and local structures. Unlike traditional approaches, they propose a novel strategy that uses the decoder for feature augmentation. Tested on the ogbn-papers100M dataset and Tencent's online game data, the framework showcases state-of-the-art performance, scalability, and efficiency. It can pre-train on real-world graphs with millions of nodes and billions of edges and generalize effectively across diverse downstream tasks. The framework's ability to make predictions for unseen new nodes and graphs highlights its potential for industrial applications with large-scale graph data. <br /> <div>
arXiv:2407.03953v4 Announce Type: replace-cross 
Abstract: Graph pre-training has been concentrated on graph-level tasks involving small graphs (e.g., molecular graphs) or learning node representations on a fixed graph. Extending graph pre-trained models to web-scale graphs with billions of nodes in industrial scenarios, while avoiding negative transfer across graphs or tasks, remains a challenge. We aim to develop a general graph pre-trained model with inductive ability that can make predictions for unseen new nodes and even new graphs. In this work, we introduce a scalable transformer-based graph pre-training framework called PGT (Pre-trained Graph Transformer). Based on the masked autoencoder architecture, we design two pre-training tasks: one for reconstructing node features and the other for reconstructing local structures. Unlike the original autoencoder architecture where the pre-trained decoder is discarded, we propose a novel strategy that utilizes the decoder for feature augmentation. Our framework, tested on the publicly available ogbn-papers100M dataset with 111 million nodes and 1.6 billion edges, achieves state-of-the-art performance, showcasing scalability and efficiency. We have deployed our framework on Tencent's online game data, confirming its capability to pre-train on real-world graphs with over 540 million nodes and 12 billion edges and to generalize effectively across diverse static and dynamic downstream tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who is the root in a syntactic dependency structure?</title>
<link>https://arxiv.org/abs/2501.15188</link>
<guid>https://arxiv.org/abs/2501.15188</guid>
<content:encoded><![CDATA[
<div> Keywords: syntactic structure, unsupervised methods, root vertex, centrality scores, network science

Summary:
The article discusses the challenges in determining the syntactic structure of sentences, specifically in guessing the correct direction of edges in a syntactic dependency tree. The authors propose using an ensemble of centrality scores, incorporating both spatial and non-spatial factors, to identify the root vertex in the structure. They hypothesize that the root vertex is a crucial and central element in the dependency tree, which is supported by their findings that root vertices tend to have high centrality. Novel scoring methods that consider the position of a vertex and its neighbors perform best in identifying the root. The study contributes to a better understanding of the concept of rootness in syntactic structures from a network science perspective. Overall, the research provides theoretical and empirical foundations for a universal notion of rootness in syntactic dependency trees. 

<br /><br />Summary: <div>
arXiv:2501.15188v3 Announce Type: replace-cross 
Abstract: The syntactic structure of a sentence can be described as a tree that indicates the syntactic relationships between words. In spite of significant progress in unsupervised methods that retrieve the syntactic structure of sentences, guessing the right direction of edges is still a challenge. As in a syntactic dependency structure edges are oriented away from the root, the challenge of guessing the right direction can be reduced to finding an undirected tree and the root. The limited performance of current unsupervised methods demonstrates the lack of a proper understanding of what a root vertex is from first principles. We consider an ensemble of centrality scores, some that only take into account the free tree (non-spatial scores) and others that take into account the position of vertices (spatial scores). We test the hypothesis that the root vertex is an important or central vertex of the syntactic dependency structure. We confirm the hypothesis in the sense that root vertices tend to have high centrality and that vertices of high centrality tend to be roots. The best performance in guessing the root is achieved by novel scores that only take into account the position of a vertex and that of its neighbours. We provide theoretical and empirical foundations towards a universal notion of rootness from a network science perspective.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Establishing Trust in Crowdsourced Data</title>
<link>https://arxiv.org/abs/2511.03016</link>
<guid>https://arxiv.org/abs/2511.03016</guid>
<content:encoded><![CDATA[
<div> Trust management practices for crowdsourced data are examined across various platforms, including Volunteered Geographic Information, Wiki Ecosystems, and Social Media. Strengths identified include automated moderation and community validation, while limitations involve data influx and elite dominance. Proposed solutions include AI tools, transparent reputation metrics, decentralised moderation, structured engagement, and a "soft power" strategy. Keywords: crowdsourced data, trust management, automated moderation, community validation, AI tools.<br /><br />Summary: This study explores trust management practices in crowdsourced data platforms. It identifies strengths like automated moderation and community validation, as well as limitations such as rapid data influx and elite dominance. Proposed solutions involve using AI tools, transparent reputation metrics, decentralised moderation, structured community engagement, and a "soft power" strategy to improve data reliability and distribute decision-making authority equitably. <div>
arXiv:2511.03016v1 Announce Type: new 
Abstract: Crowdsourced data supports real-time decision-making but faces challenges like misinformation, errors, and contributor power concentration. This study systematically examines trust management practices across platforms categorised as Volunteered Geographic Information, Wiki Ecosystems, Social Media, Mobile Crowdsensing, and Specialised Review and Environmental Crowdsourcing. Identified strengths include automated moderation and community validation, while limitations involve rapid data influx, niche oversight gaps, opaque trust metrics, and elite dominance. Proposed solutions incorporate advanced AI tools, transparent reputation metrics, decentralised moderation, structured community engagement, and a ``soft power'' strategy, aiming to equitably distribute decision-making authority and enhance overall data reliability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Citations: Measuring Idea-level Knowledge Diffusion from Research to Journalism and Policy-making</title>
<link>https://arxiv.org/abs/2511.03378</link>
<guid>https://arxiv.org/abs/2511.03378</guid>
<content:encoded><![CDATA[
<div> Keywords: social science knowledge, diffusion, media effects theories, cross-domain, contextualized meanings<br />
Summary:<br />
- The study introduces a text-based approach to measure the diffusion of social science knowledge between research, journalism, and policy-making domains.<br />
- Focuses on media effects theories in communication science, analyzing 72,703 documents from 2000-2019 mentioning these ideas.<br />
- Varies significantly in diffusion patterns and dynamics among different ideas, some diffusing across domains while others do not.<br />
- Compares contextualized meanings across domains, finding larger distances between research and policy compared to research and journalism.<br />
- Observes semantic convergence over time, particularly for practically oriented ideas, showing ideas shifting roles across domains from theories in research to applied use in policy.<br /> <div>
arXiv:2511.03378v1 Announce Type: new 
Abstract: Despite the importance of social science knowledge for various stakeholders, measuring its diffusion into different domains remains a challenge. This study uses a novel text-based approach to measure the idea-level diffusion of social science knowledge from the research domain to the journalism and policy-making domains. By doing so, we expand the detection of knowledge diffusion beyond the measurements of direct references. Our study focuses on media effects theories as key research ideas in the field of communication science. Using 72,703 documents (2000-2019) from three domains (i.e., research, journalism, and policy-making) that mention these ideas, we count the mentions of these ideas in each domain, estimate their domain-specific contexts, and track and compare differences across domains and over time. Overall, we find that diffusion patterns and dynamics vary considerably between ideas, with some ideas diffusing between other domains, while others do not. Based on the embedding regression approach, we compare contextualized meanings across domains and find that the distances between research and policy are typically larger than between research and journalism. We also find that ideas largely shift roles across domains - from being the theories themselves in research to sense-making in news to applied, administrative use in policy. Over time, we observe semantic convergence mainly for ideas that are practically oriented. Our results characterize the cross-domain diffusion patterns and dynamics of social science knowledge at the idea level, and we discuss the implications for measuring knowledge diffusion beyond citations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A local eigenvector centrality</title>
<link>https://arxiv.org/abs/2511.03608</link>
<guid>https://arxiv.org/abs/2511.03608</guid>
<content:encoded><![CDATA[
<div> local eigenvector centrality, global connectivity, prominent community structures, network analysis, centrality assessment

Summary:
local eigenvector centrality is introduced as a new measure that combines both local and global connectivity, incorporating prominent eigengaps and the associated eigenspectrum to detect centrality that reflects community structures. In contact networks with clear community structures, it identifies distinct distributions compared to eigenvector centrality applied on isolated communities and PageRank. Discrepancies between the measures highlight nodes not conforming to local structures. References to PageRank help mitigate localisation effects of eigenvector measures. For networks without well-defined communities like city road networks, local eigenvector centrality identifies both local hubs and globally connected nodes, demonstrating its ability to capture both local and global significance in network analysis. <div>
arXiv:2511.03608v1 Announce Type: new 
Abstract: Eigenvector centrality is an established measure of global connectivity, from which the importance and influence of nodes can be inferred. We introduce a local eigenvector centrality that incorporates both local and global connectivity. This new measure references prominent eigengaps and combines their associated eigenspectrum, via the Euclidean norm, to detect centrality that reflects the influence of prominent community structures. In contact networks, with clearly defined community structures, local eigenvector centrality is shown to identify similar but distinct distributions to eigenvector centrality applied on each community in isolation and PageRank. Discrepancies between the two eigenvector measures highlight nodes and communities that do not conform to their defined local structures, e.g. nodes with more connections outside of their defined community than within it. While reference to PageRank's centrality assessment enables a mitigation strategy for localisation effects inherent in eigenvector-based measures. In networks without clearly defined communities, such as city road networks, local eigenvector centrality is shown to identify both locally prominent and globally connected hubs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models</title>
<link>https://arxiv.org/abs/2511.03251</link>
<guid>https://arxiv.org/abs/2511.03251</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Mixture-of-Experts, prompt-based learning, generalization, scalability

Summary:
GMoPE (Graph Mixture of Prompt-Experts) is a novel framework that combines the Mixture-of-Experts architecture with prompt-based learning for graphs. It addresses challenges such as negative transfer, scalability issues, and high adaptation costs by allowing experts to specialize in distinct subdomains and dynamically contribute to predictions. A soft orthogonality constraint across prompt vectors promotes diversity and prevents expert collapse, while a prompt-only fine-tuning strategy reduces spatiotemporal complexity during transfer. The framework outperforms existing baselines and achieves performance comparable to full parameter fine-tuning, requiring only a fraction of the adaptation overhead. GMoPE provides a scalable and efficient solution for advancing generalizable graph foundation models. 

<br /><br />Summary: GMoPE integrates Mixture-of-Experts with prompt-based learning for graphs to improve generalization, scalability, and adaptation costs. Experts specialize in distinct subdomains, with a soft orthogonality constraint promoting diversity and balance. A prompt-only fine-tuning strategy reduces complexity during transfer, leading to superior performance compared to baselines and similar results to full parameter fine-tuning with lower overhead. GMoPE offers a principled and scalable framework for enhancing generalizable and efficient graph foundation models. <div>
arXiv:2511.03251v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on task-specific benchmarks, yet their ability to generalize across diverse domains and tasks remains limited. Existing approaches often struggle with negative transfer, scalability issues, and high adaptation costs. To address these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture with prompt-based learning for graphs. GMoPE leverages expert-specific prompt vectors and structure-aware MoE routing to enable each expert to specialize in distinct subdomains and dynamically contribute to predictions. To promote diversity and prevent expert collapse, we introduce a soft orthogonality constraint across prompt vectors, encouraging expert specialization and facilitating a more balanced expert utilization. Additionally, we adopt a prompt-only fine-tuning strategy that significantly reduces spatiotemporal complexity during transfer. We validate GMoPE through extensive experiments under various pretraining strategies and multiple downstream tasks. Results show that GMoPE consistently outperforms state-of-the-art baselines and achieves performance comparable to full parameter fine-tuning-while requiring only a fraction of the adaptation overhead. Our work provides a principled and scalable framework for advancing generalizable and efficient graph foundation models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots</title>
<link>https://arxiv.org/abs/2511.03286</link>
<guid>https://arxiv.org/abs/2511.03286</guid>
<content:encoded><![CDATA[
<div> Keywords: global digital platforms, multiagent transition systems, centralised, decentralised, federated, grassroots

Summary:<br /><br />Global digital platforms, serving billions of people, can be classified into four classes based on the cardinality of essential agents in atomic transactions-based multiagent transition systems: Centralised (one server), Decentralised (finite bootstrap nodes), Federated (infinite servers), and Grassroots (universal agents). The article presents a formal framework for analyzing these platforms, using a global social network as an example to demonstrate the different classes and their specifications. The text also explores other global platforms like currencies, sharing economy apps, and AI. Furthermore, it discusses how grassroots platforms, which encompass all agents as essential, form a distinct class within this classification framework. This mathematical approach provides a structured way to categorize and analyze various global platforms, both existing and potential, based on their multiagent atomic transactions and essential agent configurations. <br />Summary: <div>
arXiv:2511.03286v1 Announce Type: cross 
Abstract: Global digital platforms are software systems designed to serve entire populations, with some already serving billions of people. We propose atomic transactions-based multiagent transition systems and protocols as a formal framework to study them; introduce essential agents -- minimal sets of agents the removal of which makes communication impossible; and show that the cardinality of essential agents partitions all global platforms into four classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite $>1$ (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we provide centralised, decentralised, federated, and grassroots specifications via multiagent atomic transactions, and prove they satisfy basic correctness properties. We discuss informally additional global platforms -- currencies, ``sharing economy'' apps, AI, and more. While this may be the first characterisation of centralised, decentralised, and federated global platforms, grassroots platforms have been formally defined previously, but using different notions. Here, we prove that their original definition implies that all agents are essential, placing grassroots platforms in a distinct class within the broader formal context that includes all global platforms. This work provides the first mathematical framework for classifying any global platform -- existing or imagined -- by providing a multiagent atomic-transactions specification of it and determining the cardinality of the minimal set of essential agents in the ensuing multiagent protocol. It thus
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond</title>
<link>https://arxiv.org/abs/2511.03434</link>
<guid>https://arxiv.org/abs/2511.03434</guid>
<content:encoded><![CDATA[
<div> Trust Models, Inter-Agent Protocols, LLM-powered AI agents, Protocol Design, Comparative Study<br />
Summary:<br />
The article examines trust models in inter-agent protocol design as the "agentic web" continues to grow with AI agents transacting autonomously. Various protocols such as Google's A2A and Ethereum's ERC-8004 have crystallized the shift towards protocol-driven trust. The study evaluates trust models including Brief, Claim, Proof, Stake, Reputation, and Constraint, noting their assumptions, vulnerabilities, and trade-offs. It highlights fragilities specific to LLM-powered agents and emphasizes the need for proof and stake mechanisms to gate high-impact actions. Recommendations include hybrid trust models to mitigate reputation gaming and ensure safe and scalable agent economies. The article also evaluates protocols under metrics like security, privacy, latency/cost, and social robustness, providing actionable design guidelines for interoperable systems. <br />Summary: <div>
arXiv:2511.03434v1 Announce Type: cross 
Abstract: As the "agentic web" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 "Trustless Agents," yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges</title>
<link>https://arxiv.org/abs/2403.04468</link>
<guid>https://arxiv.org/abs/2403.04468</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, imbalance, noise, privacy, out-of-distribution scenarios

Summary:
This comprehensive survey focuses on addressing real-world challenges faced by Graph Neural Networks (GNNs). These challenges include data distribution imbalance, noisy data, privacy concerns, and generalization for out-of-distribution scenarios. The paper reviews existing GNN models and their solutions to these issues, emphasizing the need to enhance the reliability and robustness of GNN models in practical scenarios. By dissecting how various strategies address the challenges, such as data balancing techniques, noise reduction methods, privacy-preserving mechanisms, and strategies for handling out-of-distribution scenarios, the survey provides insights into the advancements in the field. Lastly, it discusses promising directions and future perspectives for research in improving GNN models for real-world applications. 

<br /><br />Summary: <div>
arXiv:2403.04468v2 Announce Type: replace-cross 
Abstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Model for Human Mobility Generation in Natural Disasters</title>
<link>https://arxiv.org/abs/2511.01928</link>
<guid>https://arxiv.org/abs/2511.01928</guid>
<content:encoded><![CDATA[
<div> physics-informed prompt, physics-guided alignment, cross-disaster generalization, cross-city generalization, meta-learning framework 
Summary: 
The article introduces a model called UniDisMob for generating human mobility patterns in natural disaster scenarios. The model aims to be universal, capable of adapting to new disaster types and different cities. Two main challenges addressed are the diversity of disaster types and the heterogeneity among cities. To tackle these challenges, the model incorporates physics-informed prompt and physics-guided alignment to capture common patterns in mobility changes after various disasters. Additionally, a meta-learning framework is utilized to extract universal patterns across multiple cities and account for city-specific features. Experimental results across different cities and disaster scenarios show that the proposed method outperforms existing approaches by more than 13% on average. <div>
arXiv:2511.01928v1 Announce Type: new 
Abstract: Human mobility generation in disaster scenarios plays a vital role in resource allocation, emergency response, and rescue coordination. During disasters such as wildfires and hurricanes, human mobility patterns often deviate from their normal states, which makes the task more challenging. However, existing works usually rely on limited data from a single city or specific disaster, significantly restricting the model's generalization capability in new scenarios. In fact, disasters are highly sudden and unpredictable, and any city may encounter new types of disasters without prior experience. Therefore, we aim to develop a one-for-all model for mobility generation that can generalize to new disaster scenarios. However, building a universal framework faces two key challenges: 1) the diversity of disaster types and 2) the heterogeneity among different cities. In this work, we propose a unified model for human mobility generation in natural disasters (named UniDisMob). To enable cross-disaster generalization, we design physics-informed prompt and physics-guided alignment that leverage the underlying common patterns in mobility changes after different disasters to guide the generation process. To achieve cross-city generalization, we introduce a meta-learning framework that extracts universal patterns across multiple cities through shared parameters and captures city-specific features via private parameters. Extensive experiments across multiple cities and disaster scenarios demonstrate that our method significantly outperforms state-of-the-art baselines, achieving an average performance improvement exceeding 13%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community Notes are Vulnerable to Rater Bias and Manipulation</title>
<link>https://arxiv.org/abs/2511.02615</link>
<guid>https://arxiv.org/abs/2511.02615</guid>
<content:encoded><![CDATA[
<div> algorithm, bias, manipulation, moderation, reliability

Summary: 
The article evaluates the Community Notes algorithm used for crowdsourced moderation on social media platforms. It identifies challenges such as rater bias and manipulation that can impact the effectiveness of the system. The algorithm was found to suppress helpful notes and be sensitive to biases like polarization and in-group preferences. It was also discovered that a small percentage of bad raters could strategically censor reliable information by targeting helpful notes. These findings raise concerns about the reliability and trustworthiness of community-driven moderation systems. The study emphasizes the need for improved mechanisms to safeguard the integrity of crowdsourced fact-checking. 

<br /><br />Summary: <div>
arXiv:2511.02615v1 Announce Type: new 
Abstract: Social media platforms increasingly rely on crowdsourced moderation systems like Community Notes to combat misinformation at scale. However, these systems face challenges from rater bias and potential manipulation, which may undermine their effectiveness. Here we systematically evaluate the Community Notes algorithm using simulated data that models realistic rater and note behaviors, quantifying error rates in publishing helpful versus unhelpful notes. We find that the algorithm suppresses a substantial fraction of genuinely helpful notes and is highly sensitive to rater biases, including polarization and in-group preferences. Moreover, a small minority (5--20\%) of bad raters can strategically suppress targeted helpful notes, effectively censoring reliable information. These findings suggest that while community-driven moderation may offer scalability, its vulnerability to bias and manipulation raises concerns about reliability and trustworthiness, highlighting the need for improved mechanisms to safeguard the integrity of crowdsourced fact-checking.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedback dynamics in Politics: The interplay between sentiment and engagement</title>
<link>https://arxiv.org/abs/2511.02663</link>
<guid>https://arxiv.org/abs/2511.02663</guid>
<content:encoded><![CDATA[
<div> adaptation, political communication, sentiment dynamics, social media, feedback mechanisms <br />
Summary:<br />
- The study analyzes how politicians adjust the sentiment of their messages based on public engagement in political communication on social media platforms. 
- Over 1.5 million tweets from Members of Parliament in the UK, Spain, and Greece in 2021 were used to identify sentiment dynamics through a linear model. 
- Results show a closed-loop behavior, where engagement with positive and negative messages influences the sentiment of future posts. 
- Opposition members tend to be more responsive to negative engagement, while government officials are more influenced by positive signals. 
- The findings provide a quantitative understanding of behavioral adaptation in online politics, demonstrating how feedback mechanisms can shed light on the self-reinforcing dynamics seen in social media discourse. <br /> <div>
arXiv:2511.02663v1 Announce Type: new 
Abstract: We investigate feedback mechanisms in political communication by testing whether politicians adapt the sentiment of their messages in response to public engagement. Using over 1.5 million tweets from Members of Parliament in the United Kingdom, Spain, and Greece during 2021, we identify sentiment dynamics through a simple yet interpretable linear model. The analysis reveals a closed-loop behavior: engagement with positive and negative messages influences the sentiment of subsequent posts. Moreover, the learned coefficients highlight systematic differences across political roles: opposition members are more reactive to negative engagement, whereas government officials respond more to positive signals. These results provide a quantitative, control-oriented view of behavioral adaptation in online politics, showing how feedback principles can explain the self-reinforcing dynamics that emerge in social media discourse.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance</title>
<link>https://arxiv.org/abs/2412.00621</link>
<guid>https://arxiv.org/abs/2412.00621</guid>
<content:encoded><![CDATA[
<div> vulnerabilities, Large Language Models, scam detection, adversarial examples, dataset

Summary:
Large Language Models (LLMs) face vulnerabilities when detecting scams, particularly when dealing with adversarial scam messages. A comprehensive dataset was created with detailed labels for scam messages, including both original and adversarial examples. The dataset expanded traditional scam detection categories to include more specific scam types. Adversarial examples exploited LLM vulnerabilities, resulting in a high misclassification rate. The performance of LLMs on these adversarial scam messages was evaluated, and strategies to enhance their robustness were proposed.<br /><br />Summary: <div>
arXiv:2412.00621v1 Announce Type: cross 
Abstract: Can we trust Large Language Models (LLMs) to accurately predict scam? This paper investigates the vulnerabilities of LLMs when facing adversarial scam messages for the task of scam detection. We addressed this issue by creating a comprehensive dataset with fine-grained labels of scam messages, including both original and adversarial scam messages. The dataset extended traditional binary classes for the scam detection task into more nuanced scam types. Our analysis showed how adversarial examples took advantage of vulnerabilities of a LLM, leading to high misclassification rate. We evaluated the performance of LLMs on these adversarial scam messages and proposed strategies to improve their robustness.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Story and essential meaning dynamics in Bangladesh's July 2024 Student-People's Uprising</title>
<link>https://arxiv.org/abs/2511.01865</link>
<guid>https://arxiv.org/abs/2511.01865</guid>
<content:encoded><![CDATA[
<div> YouTube comments, news coverage, emotional dynamics, public perception, political unrest

Summary:
During the July 2024 Student-People's Uprising in Bangladesh, analysis of over 50,000 YouTube comments revealed a dominance of negative sentiment correlating with the number of protest deaths. Comments reflected a landscape of power, aggression, and danger, with touches of hope and moral conviction. Topic discourse evolved, shifting between political conflict, media flow, student violence, social resistance, and digital movement. Sentiment shifts indicated an increase in happiness after the second internet blackout, with more positive words like 'victory' and 'peace' being used. The allotaxonometric analysis showed a shift from protest to justice, indicating a progression in public perception during the movement. <div>
arXiv:2511.01865v1 Announce Type: cross 
Abstract: News media serves a crucial role in disseminating information and shaping public perception, especially during periods of political unrest. Using over 50,0000 YouTube comments on news coverage from July 16 to August 6, 2024, we investigate the emotional dynamics and evolving discourse of public perception during the July 2024 Student-People's Uprising in Bangladesh. Through integrated analyses of sentiment, emotion, topic, lexical discourse, timeline progression, sentiment shifts, and allotaxonometry, we show how negative sentiment dominated during the movement. We find a negative correlation between comment happiness and number of protest deaths $(r = -0.45,\p = 0.00)$. Using an ousiometer to measure essential meaning, we find public responses reflect a landscape of power, aggression, and danger, alongside persistent expressions of hope, moral conviction, and empowerment through goodnesses. Topic discourse progressed during the movement, with peaks in `Political Conflict', `Media Flow', and `Student Violence' during crisis surges, while topics like `Social Resistance' and `Digital Movement' persisted amid repression. Sentiment shifts reveal that after the second internet blackout, average happiness increased, driven by the more frequent use of positive words such as `victory', `peace' and `freedom' and a decrease in negative terms such as `death' and `lies'. Finally, through allotaxonometric analysis, we observe a clear shift from protest to justice.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expertise and confidence explain how social influence evolves along intellective tasks</title>
<link>https://arxiv.org/abs/2011.07168</link>
<guid>https://arxiv.org/abs/2011.07168</guid>
<content:encoded><![CDATA[
<div> Keywords: interpersonal influence, expertise, social confidence, cognitive dynamical model, deep neural networks

Summary:
Interpersonal influence in small groups undertaking intellective tasks is influenced by individuals' expertise and social confidence. Low performers tend to underestimate the expertise of high-performing teammates. Three hypotheses are introduced and supported by empirical and theoretical evidence. A cognitive dynamical model based on transactive memory systems, social comparison, and confidence heuristics describes how individuals adjust interpersonal influence over time. The model accurately predicts individuals' influence and provides analytical results for identically performing individuals. A novel approach using deep neural networks on a text embedding model accurately predicts individuals' influence based on message contents, times, and correctness. Experiments show the effectiveness of the proposed models compared to traditional methods. The neural networks model is the most accurate, while the dynamical model is the most interpretable for predicting influence. <br /><br />Summary: <div>
arXiv:2011.07168v2 Announce Type: replace 
Abstract: Discovering the antecedents of individuals' influence in collaborative environments is an important, practical, and challenging problem. In this paper, we study interpersonal influence in small groups of individuals who collectively execute a sequence of intellective tasks. We observe that along an issue sequence with feedback, individuals with higher expertise and social confidence are accorded higher interpersonal influence. We also observe that low-performing individuals tend to underestimate their high-performing teammate's expertise. Based on these observations, we introduce three hypotheses and present empirical and theoretical support for their validity. We report empirical evidence on longstanding theories of transactive memory systems, social comparison, and confidence heuristics on the origins of social influence. We propose a cognitive dynamical model inspired by these theories to describe the process by which individuals adjust interpersonal influences over time. We demonstrate the model's accuracy in predicting individuals' influence and provide analytical results on its asymptotic behavior for the case with identically performing individuals. Lastly, we propose a novel approach using deep neural networks on a pre-trained text embedding model for predicting the influence of individuals. Using message contents, message times, and individual correctness collected during tasks, we are able to accurately predict individuals' self-reported influence over time. Extensive experiments verify the accuracy of the proposed models compared to baselines such as structural balance and reflected appraisal model. While the neural networks model is the most accurate, the dynamical model is the most interpretable for influence prediction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Behavioural Analysis of Credulous Twitter Users</title>
<link>https://arxiv.org/abs/2101.10782</link>
<guid>https://arxiv.org/abs/2101.10782</guid>
<content:encoded><![CDATA[
<div> Keywords: Twitter, social media, fake news, bots, user behavior

Summary:
Using Twitter data, this study focuses on identifying "credulous" users who follow bots and are susceptible to spreading false information. By analyzing features of user behavior, the study successfully identifies credulous users using lightweight features. The research highlights differences in interaction patterns between credulous and non-credulous users, with credulous users amplifying content posted by bots. Detecting these credulous users is crucial in combating the dissemination of spam, propaganda, and unreliable information on social media platforms. The study emphasizes the importance of understanding user behavior to address the issue of false news spreading rapidly through social media.<br /><br />Summary: <div>
arXiv:2101.10782v2 Announce Type: replace 
Abstract: Thanks to platforms such as Twitter and Facebook, people can know facts and events that otherwise would have been silenced. However, social media significantly contribute also to fast spreading biased and false news while targeting specific segments of the population. We have seen how false information can be spread using automated accounts, known as bots. Using Twitter as a benchmark, we investigate behavioural attitudes of so called `credulous' users, i.e., genuine accounts following many bots. Leveraging our previous work, where supervised learning is successfully applied to single out credulous users, we improve the classification task with a detailed features' analysis and provide evidence that simple and lightweight features are crucial to detect such users. Furthermore, we study the differences in the way credulous and not credulous users interact with bots and discover that credulous users tend to amplify more the content posted by bots and argue that their detection can be instrumental to get useful information on possible dissemination of spam content, propaganda, and, in general, little or no reliable information.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Agnostic Modeling of Source Reliability on Wikipedia</title>
<link>https://arxiv.org/abs/2410.18803</link>
<guid>https://arxiv.org/abs/2410.18803</guid>
<content:encoded><![CDATA[
<div> Keywords: domain reliability, Wikipedia, language-agnostic model, credibility, verification

Summary: 
- The article introduces a language-agnostic model that assesses the reliability of web domains as sources in Wikipedia across multiple language editions.
- The model evaluates domain reliability within articles on various controversial topics like Climate Change, COVID-19, History, Media, and Biology.
- Crafted features expressing domain usage across articles help predict domain reliability.
- The model achieves an F1 Macro score of around 0.80 for high-resource languages and 0.65 for mid-resource languages.
- The time the domain remains present in articles (permanence) emerges as a highly predictive feature.
- Maintaining consistent model performance across languages of varying resource levels is a challenge.
- Adapting models from higher-resource languages can enhance performance.
- The findings can support Wikipedia editors in verifying citations and provide insights for other user-generated content communities.

Summary:<br /><br />The article introduces a language-agnostic model for assessing domain reliability in Wikipedia across multiple language editions, evaluating it on articles spanning various controversial topics and achieving high predictive accuracy, influenced significantly by the permanence of domain presence. The challenge of maintaining model performance across languages of differing resource levels is noted, with potential improvements through adapting models from richer language datasets, offering valuable support to Wikipedia editors and other user-generated content platforms in verifying information sources. <div>
arXiv:2410.18803v3 Announce Type: replace 
Abstract: Over the last few years, verifying the credibility of information sources has become a fundamental need to combat disinformation. Here, we present a language-agnostic model designed to assess the reliability of web domains as sources in references across multiple language editions of Wikipedia. Utilizing editing activity data, the model evaluates domain reliability within different articles of varying controversiality, such as Climate Change, COVID-19, History, Media, and Biology topics. Crafting features that express domain usage across articles, the model effectively predicts domain reliability, achieving an F1 Macro score of approximately 0.80 for English and other high-resource languages. For mid-resource languages, we achieve 0.65, while the performance of low-resource languages varies. In all cases, the time the domain remains present in the articles (which we dub as permanence) is one of the most predictive features. We highlight the challenge of maintaining consistent model performance across languages of varying resource levels and demonstrate that adapting models from higher-resource languages can improve performance. We believe these findings can assist Wikipedia editors in their ongoing efforts to verify citations and may offer useful insights for other user-generated content communities.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Small Acts Scale: Ethical Thresholds in Network Diffusion</title>
<link>https://arxiv.org/abs/2511.00329</link>
<guid>https://arxiv.org/abs/2511.00329</guid>
<content:encoded><![CDATA[
<div> Keywords: ethical evaluation, networked environments, social graph, platform design, responsibility<br />
Summary:<br />
This article introduces a model that considers the diffusion of public acts in networked, platform-mediated environments, moving beyond dyadic evaluations. The model examines how an initiating act spreads across a social graph based on factors such as exposure, salience, compliance, and depth. It identifies a network multiplier that quantifies the impact of these factors on the diffusion of acts. By analyzing common platform design levers like reach, ranking, and friction, the model shows how these levers can affect downstream responsibility in various scenarios, such as pandemic mitigation and platform amplification of norms. The model also highlights a threshold that distinguishes subcritical, critical, and supercritical regimes in the diffusion process. Overall, this research provides insights into how platform design can influence the spread of actions and the associated ethical implications. <br /><br /> <div>
arXiv:2511.00329v1 Announce Type: new 
Abstract: Much ethical evaluation treats actions dyadically: one agent acts on one recipient. In networked, platform-mediated environments, this lens misses how public acts diffuse. We introduce a minimal message-passing model in which an initiating act with baseline valence w spreads across a social graph with exposure b, per-hop salience $alpha$, compliance $q$, and depth (horizon) d. The model yields a closed-form \emph{network multiplier} relative to the dyadic baseline and identifies a threshold at r=b.alpha.q=1 separating subcritical (saturating), critical (linear), and supercritical (geometric) regimes. We show how common platform design levers -- reach and fan-out (affecting b), ranking and context (affecting alpha), share mechanics and friction (affecting q), and time-bounds (affecting d) -- systematically change expected downstream responsibility Applications include pandemic mitigation and vaccination externalities, as well as platform amplification of prosocial and harmful norms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-centrality: A Network Centrality Measure Based on Minimum Energy Control for Laplacian Dynamics</title>
<link>https://arxiv.org/abs/2511.00339</link>
<guid>https://arxiv.org/abs/2511.00339</guid>
<content:encoded><![CDATA[
<div> centrality, network, dynamics, optimal control theory, U-centrality <br />
Summary: <br />
The article introduces a dynamic, task-aware centrality framework based on optimal control theory. Traditional centrality measures often do not consider network dynamics but this new U-centrality measure takes into account both system dynamics and specific operational objectives. By formulating a control problem on minimum energy control of average opinion using Laplacian dynamics, U-centrality quantifies a node's ability to unify agents' state. It combines aspects of degree centrality and current-flow closeness centrality, offering a versatile tool for network analysis in dynamic environments. U-centrality interpolates between known centrality measures and reveals a node's importance over different time scales, making it a valuable addition to the field of network analysis. <div>
arXiv:2511.00339v1 Announce Type: new 
Abstract: Network centrality is a foundational concept for quantifying the importance of nodes within a network. Many traditional centrality measures--such as degree and betweenness centrality--are purely structural and often overlook the dynamics that unfold across the network. However, the notion of a node's importance is inherently context-dependent and must reflect both the system's dynamics and the specific objectives guiding its operation. Motivated by this perspective, we propose a dynamic, task-aware centrality framework rooted in optimal control theory. By formulating a problem on minimum energy control of average opinion based on Laplacian dynamics and focusing on the variance of terminal state, we introduce a novel centrality measure--termed U-centrality--that quantifies a node's ability to unify the agents' state. We demonstrate that U-centrality interpolates between known measures: it aligns with degree centrality in the short-time horizon and converges to a new centrality over longer time scales which is closely related to current-flow closeness centrality. This work bridges structural and dynamical approaches to centrality, offering a principled, versatile tool for network analysis in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion Dynamics: A Comprehensive Overview</title>
<link>https://arxiv.org/abs/2511.00401</link>
<guid>https://arxiv.org/abs/2511.00401</guid>
<content:encoded><![CDATA[
<div> Opinion dynamics, social interactions, interdisciplinary, unified framework, models<br />
<br />
Summary: <br />
- This survey reviews models of opinion dynamics across disciplines in a unified framework, categorizing them based on properties. <br />
- It analyzes convergence properties, viral marketing implications, and user characteristics in these models. <br />
- Examining final configuration and convergence time, the study delves into consensus vs polarized outcomes. <br />
- Algorithmic, complexity, and combinatorial results are reviewed in the context of viral marketing strategies. <br />
- Node characteristics like stubbornness and activeness are explored for their impact on diffusion outcomes. <br /> <div>
arXiv:2511.00401v1 Announce Type: new 
Abstract: Opinion dynamics, the evolution of individuals through social interactions, is an important area of research with applications ranging from politics to marketing. Due to its interdisciplinary relevance, studies of opinion dynamics remain fragmented across computer science, mathematics, the social sciences, and physics, and often lack shared frameworks. This survey bridges these gaps by reviewing well-known models of opinion dynamics within a unified framework and categorizing them into distinct classes based on their properties. Furthermore, the key findings on these models are covered in three parts: convergence properties, viral marketing, and user characteristics. We first analyze the final configuration (consensus vs polarized) and convergence time for each model. We then review the main algorithmic, complexity, and combinatorial results in the context of viral marketing. Finally, we explore how node characteristics, such as stubbornness, activeness, or neutrality, shape diffusion outcomes. By unifying terminology, methods, and challenges across disciplines, this paper aims to foster cross-disciplinary collaboration and accelerate progress in understanding and harnessing opinion dynamics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Framework Based on Graph Cellular Automata for Similarity Evaluation in Urban Spatial Networks</title>
<link>https://arxiv.org/abs/2511.00768</link>
<guid>https://arxiv.org/abs/2511.00768</guid>
<content:encoded><![CDATA[
<div> framework, graph cellular automata, similarity evaluation, urban spatial networks, network resonance

Summary:

The study introduces GCA-Sim, a framework based on graph cellular automata for assessing similarity in urban spatial networks. It utilizes multiple submodels to measure similarity by analyzing divergence between value distributions during an information evolution process. The research identifies network resonance, where certain propagation rules amplify differences in network signals. Through an enhanced differentiable logic-gate network, various submodels inducing network resonance are learned. Evaluation on city-level and district-level road networks demonstrates superior performance compared to existing methods. The framework reveals that planning-led street networks exhibit less internal homogeneity than organically developed ones. Different morphological categories contribute equally to similarity evaluation. Additionally, the study observes that degree, a basic topological signal, aligns more with land value and related variables over iterations. <div>
arXiv:2511.00768v1 Announce Type: new 
Abstract: Measuring similarity in urban spatial networks is key to understanding cities as complex systems. Yet most existing methods are not tailored for spatial networks and struggle to differentiate them effectively. We propose GCA-Sim, a similarity-evaluation framework based on graph cellular automata. Each submodel measures similarity by the divergence between value distributions recorded at multiple stages of an information evolution process. We find that some propagation rules magnify differences among network signals; we call this "network resonance." With an improved differentiable logic-gate network, we learn several submodels that induce network resonance. We evaluate similarity through clustering performance on fifty city-level and fifty district-level road networks. The submodels in this framework outperform existing methods, with Silhouette scores above 0.9. Using the best submodel, we further observe that planning-led street networks are less internally homogeneous than organically grown ones; morphological categories from different domains contribute with comparable importance; and degree, as a basic topological signal, becomes increasingly aligned with land value and related variables over iterations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deciphering Scientific Collaboration in Biomedical LLM Research: Dynamics, Institutional Participation, and Resource Disparities</title>
<link>https://arxiv.org/abs/2511.00818</link>
<guid>https://arxiv.org/abs/2511.00818</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, biomedical research, collaboration diversity, network analysis, research performance

Summary: 
1. Large language models (LLMs) are reshaping scientific collaboration in biomedical research, with a focus on the evolving diversity of collaborations over time.
2. Collaboration diversity has increased steadily, with a decrease in Computer Science and Artificial Intelligence authors, indicating lower technical barriers for biomedical investigators.
3. Central institutions such as Stanford University and Harvard Medical School, along with bridging disciplines like Medicine and Computer Science, anchor collaboration networks in LLM-related biomedical research.
4. Biomedical research resources are strongly linked to research performance, with high-performing resource-constrained institutions collaborating more with top institutions.
5. The study highlights a complex landscape where democratizing trends coexist with a resource-driven hierarchy, emphasizing the critical role of strategic collaboration in the field.

<br /><br />Summary: <div>
arXiv:2511.00818v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly transforming biomedical discovery and clinical innovation, yet their impact extends far beyond algorithmic revolution-LLMs are restructuring how scientific collaboration occurs, who participates, and how resources shape innovation. Despite this profound transformation, how this rapid technological shift is reshaping the structure and equity of scientific collaboration in biomedical LLM research remains largely unknown. By analyzing 5,674 LLM-related biomedical publications from PubMed, we examine how collaboration diversity evolves over time, identify institutions and disciplines that anchor and bridge collaboration networks, and assess how resource disparities underpin research performance. We find that collaboration diversity has grown steadily, with a decreasing share of Computer Science and Artificial Intelligence authors, suggesting that LLMs are lowering technical barriers for biomedical investigators. Network analysis reveals central institutions, including Stanford University and Harvard Medical School, and bridging disciplines such as Medicine and Computer Science that anchor collaborations in this field. Furthermore, biomedical research resources are strongly linked to research performance, with high-performing resource-constrained institutions exhibiting larger collaboration volume with the top 1% most connected institutions in the network. Together, these findings reveal a complex landscape, where democratizing trends coexist with a persistent, resource-driven hierarchy, highlighting the critical role of strategic collaboration in this evolving field.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Single-Tokenomics: How Farcaster's Pluralistic Incentives Reshape Social Networking</title>
<link>https://arxiv.org/abs/2511.00827</link>
<guid>https://arxiv.org/abs/2511.00827</guid>
<content:encoded><![CDATA[
<div> token-based rewards, platform dynamics, user behaviors, tokenomics design, social interactions  

Summary:  
This paper presents empirical analysis on the impact of diverse token-based reward mechanisms on platform dynamics and user behaviors using data from the Farcaster blockchain-based social network. The study found that different tokenomics designs influence participation rates and wealth concentration patterns. Inter-community tipping mitigates echo chambers among non-following pairs. However, token rewards may not always enhance content quality and can lead to asymmetric network growth. While token rewards boost content creation and follower acquisition, they may also encourage strategic optimization. The findings shed light on the challenges of aligning economic incentives with authentic social value in social platforms integrating cryptocurrencies. <div>
arXiv:2511.00827v1 Announce Type: new 
Abstract: This paper presents the first empirical analysis of how diverse token-based reward mechanisms impact platform dynamics and user behaviors. For this, we gather a unique, large-scale dataset from Farcaster. This blockchain-based, decentralized social network incorporates multiple incentive mechanisms spanning platform-native rewards, third-party token programs, and peer-to-peer tipping. Our dataset captures token transactions and social interactions from 574,829 wallet-linked users, representing 64.25% of the platform's user base. Our socioeconomic analyses reveal how different tokenomics design shape varying participation rates (7.6%--70%) and wealth concentration patterns (Gini 0.72--0.94), whereas inter-community tipping (51--75% of all tips) is 1.3--2x more frequent among non-following pairs, thereby mitigating echo chambers. Our causal analyses further uncover several critical trade-offs: (1) while most token rewards boost content creation, they often fail to enhance -- sometimes undermining -- content quality; (2) token rewards increase follower acquisition but show neutral or negative effects on outbound following, suggesting potential asymmetric network growth; (3) repeated algorithmic rewards demonstrate strong cumulative effects that may encourage strategic optimization. Our findings advance understanding of cryptocurrency integration in social platforms and highlight challenges in aligning economic incentives with authentic social value.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Employee Verification Mechanisms Alter Cultural Signals in Employer Reviews?</title>
<link>https://arxiv.org/abs/2511.01086</link>
<guid>https://arxiv.org/abs/2511.01086</guid>
<content:encoded><![CDATA[
<div> Keywords: online reviews, Glassdoor, Blind, verification, workplace culture

Summary:
Online reviews play a significant role in shaping perceptions of products and workplaces, with employer reviews combining narratives and ratings that reflect organizational culture. Platforms like Glassdoor allow for fully anonymous reviews, while Blind requires verification of employment. Research indicates that verified reviews may be more trustworthy, but verification can also impact authenticity when expectations are not met. Using the Competing Values Framework and the CultureBERT model, a study analyzed over 300k ratings on both platforms. Blind reviews tend to emphasize clan and hierarchy, while Glassdoor reviews skew positive and highlight clan and market cultures. Verification alone does not eliminate bias but alters how culture is portrayed. Job seekers using different platforms may receive conflicting signals about workplace culture, influencing their application decisions and job fit. <br /><br />Summary: Online reviews on Glassdoor and Blind, with and without verification, offer insights into workplace culture but may present different perspectives to job seekers, impacting their decision-making process. <div>
arXiv:2511.01086v1 Announce Type: new 
Abstract: Online reviews shape impressions across products and workplaces. Employer reviews combine narratives and ratings that reflect culture. Glassdoor permits fully anonymous posts; Blind requires employment verification while preserving anonymity. We ask how verification changes reviews. Evidence suggests verified reviews can be more trustworthy, yet verification can also erode authenticity when expectations are unmet. We use the Competing Values Framework (clan, adhocracy, hierarchy, market) and the CultureBERT model by Koch and Pasch, 2023 to over 300k ratings. We find that Blind reviews emphasize clan and hierarchy while Glassdoor skews positive and highlights clan and market. Verification on its own does not remove bias but shifts how culture is represented. Job seekers using different platforms receive systematically different signals about workplace culture, affecting application decisions and job-matching.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEEP: A Discourse Evolution Engine for Predictions about Social Movements</title>
<link>https://arxiv.org/abs/2511.01142</link>
<guid>https://arxiv.org/abs/2511.01142</guid>
<content:encoded><![CDATA[
<div> transformer-based, multivariate time series, Discourse Evolution Engine, social movements, prediction
<br />
SMART's DEEP tool uses transformer-based technology to predict the volume of future articles/posts and the emotions expressed in social movements related to the UN's Sustainable Development Goals. Developed by a multidisciplinary team, the tool provides probabilistic forecasts with uncertainty estimates, aiding editorial planning and strategic decision-making. DEEP is evaluated using a case study of the #MeToo movement, analyzing a longitudinal dataset of Reddit posts and news articles from September 2024 to June 2025. The dataset, comprising 433K Reddit posts and 121K news articles, will be publicly released for research upon publication of the paper.
<br /><br />Summary: SMART's DEEP tool, utilizing transformer-based technology, predicts future article/post volume and emotions in social movements supporting the UN's SDGs. Developed by a diverse team, the tool provides probabilistic forecasts aiding decision-making. Evaluated with a case study of #MeToo movement, a dataset of 433K Reddit posts and 121K news articles from 2024 to 2025 is analyzed and will be publicly released for research. <div>
arXiv:2511.01142v1 Announce Type: new 
Abstract: Numerous social movements (SMs) around the world help support the UN's Sustainable Development Goals (SDGs). Understanding how key events shape SMs is key to the achievement of the SDGs. We have developed SMART (Social Media Analysis & Reasoning Tool) to track social movements related to the SDGs. SMART was designed by a multidisciplinary team of AI researchers, journalists, communications scholars and legal experts. This paper describes SMART's transformer-based multivariate time series Discourse Evolution Engine for Predictions about Social Movements (DEEP) to predict the volume of future articles/posts and the emotions expressed. DEEP outputs probabilistic forecasts with uncertainty estimates, providing critical support for editorial planning and strategic decision-making. We evaluate DEEP with a case study of the #MeToo movement by creating a novel longitudinal dataset (433K Reddit posts and 121K news articles) from September 2024 to June 2025 that will be publicly released for research purposes upon publication of this paper.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Influence-aware Causal Autoencoder Network for Node Importance Ranking in Complex Networks</title>
<link>https://arxiv.org/abs/2511.01228</link>
<guid>https://arxiv.org/abs/2511.01228</guid>
<content:encoded><![CDATA[
<div> node importance ranking, graph data analysis, causal representation learning, node embeddings, generalization capability <br />
Summary: <br />
The paper proposes the Influence-aware Causal Autoencoder Network (ICAN), a novel framework for node importance ranking in graph data analysis. ICAN leverages causal representation learning to extract robust node embeddings that are causally related to node importance. By training exclusively on synthetic networks, ICAN eliminates the need to rely on the topology of target networks, improving generalization capability across diverse real-world graphs. The framework introduces an influence-aware causal representation learning module within an autoencoder architecture and utilizes a causal ranking loss and unified optimization framework to jointly optimize reconstruction and ranking objectives. Experimental results on benchmark datasets demonstrate that ICAN outperforms state-of-the-art baselines in terms of both ranking accuracy and generalization capability. <div>
arXiv:2511.01228v1 Announce Type: new 
Abstract: Node importance ranking is a fundamental problem in graph data analysis. Existing approaches typically rely on node features derived from either traditional centrality measures or advanced graph representation learning methods, which depend directly on the target network's topology. However, this reliance on structural information raises privacy concerns and often leads to poor generalization across different networks. In this work, we address a key question: Can we design a node importance ranking model trained exclusively on synthetic networks that is effectively appliable to real-world networks, eliminating the need to rely on the topology of target networks and improving both practicality and generalizability? We answer this question affirmatively by proposing the Influence-aware Causal Autoencoder Network (ICAN), a novel framework that leverages causal representation learning to get robust, invariant node embeddings for cross-network ranking tasks. Firstly, ICAN introduces an influence-aware causal representation learning module within an autoencoder architecture to extract node embeddings that are causally related to node importance. Moreover, we introduce a causal ranking loss and design a unified optimization framework that jointly optimizes the reconstruction and ranking objectives, enabling mutual reinforcement between node representation learning and ranking optimization. This design allows ICAN, trained on synthetic networks, to generalize effectively across diverse real-world graphs. Extensive experiments on multiple benchmark datasets demonstrate that ICAN consistently outperforms state-of-the-art baselines in terms of both ranking accuracy and generalization capability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polarization and echo chambers in Reddit's political discourse</title>
<link>https://arxiv.org/abs/2510.27467</link>
<guid>https://arxiv.org/abs/2510.27467</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, polarization, echo chambers, political debate, social media

Summary:
The study examines political debate on Reddit during the 2016 US presidential election, challenging the assumption that the platform is less polarized than others. By analyzing ideologically distinct communities based on user interactions and news consumption, the researchers identify Democratic, Conservative, and Banned subreddit clusters. They find clear polarization, intensified cross-group engagement during election periods, alignment of Banned and Conservative content, and reduced linguistic diversity within groups. The study characterizes Reddit as a polarized environment with echo chambers, emphasizing the importance of network validation for identifying behavioral and interaction patterns on social media platforms. <br /><br />Summary: The study on Reddit during the 2016 US presidential election reveals polarization, increased cross-group engagement during elections, alignment of certain subreddit clusters, and reduced linguistic diversity within groups, depicting Reddit as a polarized environment with echo chambers that necessitates network validation. <div>
arXiv:2510.27467v1 Announce Type: cross 
Abstract: Political debate nowadays takes place mainly on online social media, with election periods amplifying ideological engagement. Reddit is generally considered more resistant to polarization and echo chamber effects than platforms like Twitter or Facebook. Here, we challenge this assumption through a case study across the 2016 US presidential election. We use statistical validation techniques to extract ideologically distinct communities of subreddits, in terms of their contributing user base and news consumption, which we use to analyze the dynamics of political debate. We thus reveal clear polarization in both interaction-based and topic-based communities, with clusters of Democratic, Conservative, and Banned subreddits. Election periods intensify cross-group engagement, align Banned and Conservative content, and reduce linguistic diversity within groups. Overall we characterize Reddit as a polarized environment marked by the presence of echo chambers, highlighting network validation as a key method for identifying behavioral and interaction patterns on online social media.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Z-Dip: a validated generalization of the Dip Test</title>
<link>https://arxiv.org/abs/2511.01705</link>
<guid>https://arxiv.org/abs/2511.01705</guid>
<content:encoded><![CDATA[
<div> Detection, multimodality, empirical distributions, statistics, data analysis  
Summary:  
- Multimodality detection in empirical distributions is crucial in various fields for analyzing data patterns.  
- Hartigan's Dip Test is a traditional method for testing multimodality but is limited by sample size and requires lookup tables.  
- The Z-Dip method, a standardized extension of the Dip Test, eliminates sample size dependence by comparing observed Dip values to simulated null distributions.  
- A universal decision threshold for Z-Dip is calibrated through simulation and bootstrap resampling, offering a consistent criterion for multimodality detection.  
- A downsampling-based approach is proposed to reduce residual sample-size effects in large datasets.  
<br /><br />Summary: <div>
arXiv:2511.01705v1 Announce Type: cross 
Abstract: Detecting multimodality in empirical distributions is a fundamental problem in statistics and data analysis, with applications ranging from clustering to social science. Hartigan's Dip Test is a classical nonparametric procedure for testing unimodality versus multimodality, but its interpretation is hindered by strong dependence on sample size and the need for lookup tables. We introduce the Z-Dip, a standardized extension of the Dip Test that removes sample-size dependence by comparing observed Dip values to simulated null distributions. We calibrate a universal decision threshold for Z-Dip via simulation and bootstrap resampling, providing a unified criterion for multimodality detection. In the final section, we also propose a downsampling-based approach to further mitigate residual sample-size effects in very large datasets. Lookup tables and software implementations are made available for efficient use in practice.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Community Detection Over Streaming Bipartite Networks (Technical Report)</title>
<link>https://arxiv.org/abs/2411.01424</link>
<guid>https://arxiv.org/abs/2411.01424</guid>
<content:encoded><![CDATA[
<div> Keywords: bipartite graph, community detection, streaming networks, butterfly motif, structural cohesiveness

Summary: 
Community Detection over Streaming Bipartite Network (CD-SBN) is proposed to identify dense subgraphs in bipartite streaming graphs. The study focuses on the butterfly motif in the bipartite graph and defines a novel community structure called $(k,r,\sigma)$-bitruss. The problem aims to retrieve communities with user-specific query keywords and high structural cohesiveness. Efficient algorithms are developed to handle snapshot and continuous CD-SBN queries by employing pruning strategies and a hierarchical synopsis. The approach's performance is demonstrated through experiments on real and synthetic streaming bipartite networks. The CD-SBN approach addresses the challenge of identifying qualified communities in bipartite streaming networks, offering a valuable solution for various real-world applications. 

<br /><br />Summary: <div>
arXiv:2411.01424v2 Announce Type: replace 
Abstract: The streaming bipartite graph is widely used to model the dynamic relationship between two types of entities in various real-world applications, including movie recommendations, location-based services, and online shopping. Since it contains abundant information, discovering the dense subgraph with high structural cohesiveness (i.e., community detection) in the bipartite streaming graph is becoming a valuable problem. Inspired by this, in this paper, we study the structure of the community on the butterfly motif in the bipartite graph. We propose a novel problem, named Community Detection over Streaming Bipartite Network (CD-SBN), which aims to retrieve qualified communities with user-specific query keywords and high structural cohesiveness at snapshot and continuous scenarios. In particular, we formulate the user relationship score in the weighted bipartite network via the butterfly pattern and define a novel $(k,r,\sigma)$-bitruss as the community structure. To efficiently tackle the CD-SBN problem, we design effective pruning strategies to rule out false alarms of $(k,r,\sigma)$-bitruss and propose a hierarchical synopsis to facilitate the CD-SBN processing. We develop efficient algorithms to answer snapshot and continuous CD-SBN queries by traversing the synopsis and applying pruning strategies. With extensive experiments, we demonstrate the performance of our CD-SBN approach on real/synthetic streaming bipartite networks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering the Sociodemographic Fabric of Reddit</title>
<link>https://arxiv.org/abs/2502.05049</link>
<guid>https://arxiv.org/abs/2502.05049</guid>
<content:encoded><![CDATA[
<div> Keywords: sociodemographic composition, online platforms, Reddit, Naive Bayes, user self-disclosures

Summary: 
- The study focuses on understanding the sociodemographic makeup of online platforms, specifically Reddit, using a framework that relies on user-provided data.
- Over 850,000 user self-declarations of age, gender, and partisan affiliation are used to train models for sociodemographic inference, showcasing the effectiveness of simple probabilistic models like Naive Bayes.
- The approach outperforms more complex alternatives, improving classification performance by up to 19% in ROC AUC and maintaining quantification error below 15%.
- The models are well-calibrated and interpretable, allowing for uncertainty estimation and analysis of subreddit-level feature importance.
- The study advocates for a more ethical and transparent approach to computational social science, emphasizing the importance of grounding sociodemographic analysis in user-provided data rather than researcher assumptions. 

<br /><br />Summary: <div>
arXiv:2502.05049v2 Announce Type: replace 
Abstract: Understanding the sociodemographic composition of online platforms is essential for accurately interpreting digital behavior and its societal implications. Yet, current methods often lack the transparency and reliability required, risking misrepresenting social identities and distorting our understanding of digital society. Here, we introduce a principled framework for sociodemographic inference on Reddit that leverages over 850,000 user self-declarations of age, gender, and partisan affiliation. By training models on sparse user activity signals from this extensive, self-disclosed dataset, we demonstrate that simple probabilistic models, such as Naive Bayes, outperform more complex embedding-based alternatives. Our approach improves classification performance over the state of the art by up to 19% in ROC AUC and maintains quantification error below 15%. The models produce well-calibrated and interpretable outputs, enabling uncertainty estimation and subreddit-level feature importance analysis. More broadly, this work advocates for a shift toward more ethical and transparent computational social science by grounding sociodemographic analysis in user-provided data rather than researcher assumptions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating hashtag dynamics with networked groups of generative agents</title>
<link>https://arxiv.org/abs/2510.26832</link>
<guid>https://arxiv.org/abs/2510.26832</guid>
<content:encoded><![CDATA[
<div> Keywords: Networked environments, narrative interactions, Large Language Models, agent-based modeling, social rewards 

Summary:
The study explores how group communication around narrative media affects belief formation and consensus or polarization. Using a framework of Large Language Model (LLM) agents, the researchers simulated narrative interactions to understand how they evolve in networked communication settings. Benchmarking the simulations against human behaviors in an experiment and Twitter hashtags, the study found that LLMs can replicate human-like coherence in controlled environments but struggle with more complex or politically sensitive narratives. This suggests that incorporating background knowledge and social context in narrative generation may require structured prompting. The results highlight the importance of understanding how information embedded in narratives influences beliefs and behavior in networked environments, shedding light on the mechanisms behind belief formation and group dynamics. Overall, the study underscores the need for careful consideration when using LLMs in narrative communication settings to ensure accurate representation of social interactions. 

<br /><br />Summary: <div>
arXiv:2510.26832v1 Announce Type: new 
Abstract: Networked environments shape how information embedded in narratives influences individual and group beliefs and behavior. This raises key questions about how group communication around narrative media impacts belief formation and how such mechanisms contribute to the emergence of consensus or polarization. Language data from generative agents offer insight into how naturalistic forms of narrative interactions (such as hashtag generation) evolve in response to social rewards within networked communication settings. To investigate this, we developed an agent-based modeling and simulation framework composed of networks of interacting Large Language Model (LLM) agents. We benchmarked the simulations of four state-of-the-art LLMs against human group behaviors observed in a prior network experiment (Study 1) and against naturally occurring hashtags from Twitter (Study 2). Quantitative metrics of network coherence (e.g., entropy of a group's responses) reveal that while LLMs can approximate human-like coherence in sanitized domains (Study 1's experimental data), effective integration of background knowledge and social context in more complex or politically sensitive narratives likely requires careful and structured prompting.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Online Sports Fan Communities Becoming More Offensive? A Quantitative Review of Topics, Trends, and Toxicity of r/PremierLeague</title>
<link>https://arxiv.org/abs/2510.27003</link>
<guid>https://arxiv.org/abs/2510.27003</guid>
<content:encoded><![CDATA[
<div> community, sports fans, Premier League, online, toxicity

Summary:
The study focuses on the online community of sports fans, particularly those of the Premier League, analyzing over 1.1 million comments on the r/PremierLeague subreddit from 2013-2022. The research delves into sentiment, topics, and toxicity within these discussions, noting a rise in negative sentiment and toxicity over time. The subreddit has become a platform for users to discuss societal issues such as racism, the COVID-19 pandemic, and political tensions, reflecting the broader conversations happening within the fan community. The surge in popularity of online sports communities has led to more diverse discussions but has also raised concerns about the increase in negative interactions. This study provides insight into the landscape of online discussions among sports fans and highlights the need for further understanding and monitoring of online fan communities. 

<br /><br />Summary: <div>
arXiv:2510.27003v1 Announce Type: new 
Abstract: Online communities for sports fans have surged in popularity, with Reddit's r/PremierLeague emerging as a focal point for fans of one of the globe's most celebrated sports leagues. This boom has helped the Premier League make significant inroads into the US market, increasing viewership and sparking greater interest in its matches. Despite the league's broad appeal, there's still a notable gap in understanding its online fan community. Therefore, we analyzed a substantial dataset of over 1.1 million comments posted from 2013-2022 on r/PremierLeague. Our study delves into the sentiment, topics, and toxicity of these discussions, tracking trends over time, aiming to map out the conversation landscape. The rapid expansion has brought more diverse discussions, but also a worrying rise in negative sentiment and toxicity. Additionally, the subreddit has become a venue for users to voice frustrations about broader societal issues like racism, the COVID-19 pandemic, and political tensions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disrupting Networks: Amplifying Social Dissensus via Opinion Perturbation and Large Language Models</title>
<link>https://arxiv.org/abs/2510.27152</link>
<guid>https://arxiv.org/abs/2510.27152</guid>
<content:encoded><![CDATA[
<div> social networks, targeted content injection, disruption, Friedkin-Johnsen model, reinforcement learning

Summary:
Using the Friedkin-Johnsen model, the study explores disrupting social networks through targeted content injection. Simple variants of the model are ineffective in perturbing the network, but extensions show promise in achieving disruption. By altering individual opinions, disruption can be maximized. A reinforcement learning framework is developed to fine-tune a Large Language Model (LLM) for generating disruption-oriented text. Experiments on both synthetic and real-world data demonstrate that tuned LLMs can approach theoretical disruption limits. These findings have implications for content moderation, adversarial information campaigns, and regulation of generative models. 

<br /><br />Summary: <div>
arXiv:2510.27152v1 Announce Type: new 
Abstract: We study how targeted content injection can strategically disrupt social networks. Using the Friedkin-Johnsen (FJ) model, we utilize a measure of social dissensus and show that (i) simple FJ variants cannot significantly perturb the network, (ii) extending the model enables valid graph structures where disruption at equilibrium exceeds the initial state, and (iii) altering an individual's inherent opinion can maximize disruption. Building on these insights, we design a reinforcement learning framework to fine-tune a Large Language Model (LLM) for generating disruption-oriented text. Experiments on synthetic and real-world data confirm that tuned LLMs can approach theoretical disruption limits. Our findings raise important considerations for content moderation, adversarial information campaigns, and generative model regulation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aware Optimal Intervention for Rumor Dynamics on Networks: Node-Level, Time-Varying, and Resource-Constrained</title>
<link>https://arxiv.org/abs/2510.27165</link>
<guid>https://arxiv.org/abs/2510.27165</guid>
<content:encoded><![CDATA[
<div> Framework, Rumor propagation, Social networks, Optimal intervention, Misinformation management<br />
Summary: <br />
The article introduces a new framework for addressing rumor propagation in social networks. Traditional static, centrality-based approaches are ineffective in combating the spread of misinformation. The proposed framework utilizes a node-level, time-varying optimal intervention strategy to allocate resources based on the evolving diffusion state of the network. By solving a resource-constrained optimal control problem tied to the network structure, the framework successfully reduces both the infection peak and cumulative infection area compared to uniform and static centrality-based allocations. The approach follows a stage-aware law, focusing early resources on influential hubs to prevent rapid spread and later resources on peripheral nodes to eliminate residual transmission. This integrated approach combines global efficiency with adaptability, providing a scalable and interpretable method for managing misinformation and crisis response. <div>
arXiv:2510.27165v1 Announce Type: new 
Abstract: Rumor propagation in social networks undermines social stability and public trust, calling for interventions that are both effective and resource-efficient. We develop a node-level, time-varying optimal intervention framework that allocates limited resources according to the evolving diffusion state. Unlike static, centrality-based heuristics, our approach derives control weights by solving a resource-constrained optimal control problem tightly coupled to the network structure. Across synthetic and real-world networks, the method consistently lowers both the infection peak and the cumulative infection area relative to uniform and centrality-based static allocations. Moreover, it reveals a stage-aware law: early resources prioritize influential hubs to curb rapid spread, whereas later resources shift to peripheral nodes to eliminate residual transmission. By integrating global efficiency with fine-grained adaptability, the framework offers a scalable and interpretable paradigm for misinformation management and crisis response.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meritocracy versus Matthew-effect: Two underlying network formation mechanisms of online social platforms</title>
<link>https://arxiv.org/abs/2510.27339</link>
<guid>https://arxiv.org/abs/2510.27339</guid>
<content:encoded><![CDATA[
<div> Keyword: online social networks, network formation mechanisms, social power distribution, content creators, empirical data <br />
Summary: 
The article discusses the evolving landscape of online social networks, particularly the shift from connection-based platforms like Facebook to content-based platforms such as TikTok and Instagram. It highlights the significant inequality in social power distribution observed in content-based platforms compared to traditional ones. The authors propose two network formation mechanisms - meritocracy-based and Matthew-effect-based - to explain the distinct patterns in social power distribution. The models are validated through theoretical and numerical analysis, showcasing their ability to replicate essential statistical features and match empirical data. The study also suggests a hybrid model for platforms like academic collaboration networks, which exhibit characteristics between traditional and emerging social networks. Understanding the formation mechanisms of online social networks offers valuable insights into the evolution of content ecosystems and the behavior of content creators on these platforms. <br /><br />Summary: <div>
arXiv:2510.27339v1 Announce Type: new 
Abstract: With the rapid development of the internet industry, online social networks have come to play an increasingly significant role in everyday life. In recent years, content-based emerging platforms such as TikTok, Instagram, and Bilibili have diverged fundamentally in their underlying logic from traditional connection-based social platforms like Facebook and LinkedIn. Empirical data on follower counts and follower-count-based rankings reveal that the distribution of social power varies significantly across different types of platforms, with content-based platforms exhibiting notably greater inequality. Here we propose two fundamental network formation mechanisms: a meritocracy-based model and a Matthew-effect-based model, designed to capture the formation logic underlying traditional and emerging social networks, respectively. Through theoretical and numerical analysis, we demonstrate that both models replicate salient statistical features of social networks including scale-free and small-world property, while also closely match empirical patterns on the relationship between in-degrees and in-degree rankings, thereby capturing the distinctive distributions of social power in respective platforms. Moreover, networks such as academic collaboration networks, where the distribution of social power usually lies between that of traditional and emerging platorms, can be interpreted through a hybrid of the two proposed mechanisms. Deconstructing the formation mechanisms of online social networks offers valuable insights into the evolution of the content ecosystems and the behavioral patterns of content creators on online social platforms.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Communities: A Mixed-Methods and Community-Driven Evaluation of Cultural Sensitivity in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2510.27361</link>
<guid>https://arxiv.org/abs/2510.27361</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image models, cultural sensitivity, misrepresentation, evaluation methodology, minority groups<br />
Summary:<br />
The paper explores the issue of cultural sensitivity in text-to-image (T2I) models, which often reflect Western cultural norms and perpetuate misrepresentation of minority groups. Through a mixed-methods community-based evaluation methodology developed in collaboration with individuals from 19 countries, the study uncovers the complexities of assessing cultural sensitivity in T2I models. Quantitative scores and qualitative inquiries reveal both convergence and disagreement within and across communities, shedding light on the downstream consequences of misrepresentation. The research highlights how training data influenced by unequal power relations distort representations and emphasizes the need for extensive assessments with high resource requirements. The paper offers actionable recommendations for stakeholders and suggests pathways to investigate the sources, mechanisms, and impacts of cultural (mis)representation in T2I models.<br /> 
Summary: <div>
arXiv:2510.27361v1 Announce Type: new 
Abstract: Evidence shows that text-to-image (T2I) models disproportionately reflect Western cultural norms, amplifying misrepresentation and harms to minority groups. However, evaluating cultural sensitivity is inherently complex due to its fluid and multifaceted nature. This paper draws on a state-of-the-art review and co-creation workshops involving 59 individuals from 19 different countries. We developed and validated a mixed-methods community-based evaluation methodology to assess cultural sensitivity in T2I models, which embraces first-person methods. Quantitative scores and qualitative inquiries expose convergence and disagreement within and across communities, illuminate the downstream consequences of misrepresentation, and trace how training data shaped by unequal power relations distort depictions. Extensive assessments are constrained by high resource requirements and the dynamic nature of culture, a tension we alleviate through a context-based and iterative methodology. The paper provides actionable recommendations for stakeholders, highlighting pathways to investigate the sources, mechanisms, and impacts of cultural (mis)representation in T2I models.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Demographics: Behavioural Segmentation and Spatial Analytics to Enhance Visitor Experience at The British Museum</title>
<link>https://arxiv.org/abs/2510.27542</link>
<guid>https://arxiv.org/abs/2510.27542</guid>
<content:encoded><![CDATA[
<div> visitor behaviour, data science methods, British Museum, satisfaction, visitor types

Summary:
This study analyzes visitor behavior at The British Museum using data science methods such as audio guide usage logs and TripAdvisor reviews. Through the analysis of 42,000 visitor journeys and over 50,000 reviews, the study identifies key drivers of satisfaction, segments visitors into four types (Committed Trekkers, Leisurely Explorers, Targeted Visitors, and Speedy Samplers), examines tour engagement, models spatial navigation, and investigates room popularity. The analysis reveals high drop-off rates in tour usage, with accessibility and proximity playing a significant role in shaping visitor paths more than thematic organization. Room popularity is found to be more influenced by physical accessibility than curatorial content. The study proposes practical strategies for improving engagement and flow, providing a scalable framework for visitor-centered, data-informed museum planning. <br /><br />Summary: <div>
arXiv:2510.27542v1 Announce Type: new 
Abstract: This study explores visitor behaviour at The British Museum using data science methods applied to novel sources, including audio guide usage logs and TripAdvisor reviews. Analysing 42,000 visitor journeys and over 50,000 reviews, we identify key drivers of satisfaction, segment visitors by behavioural patterns, examine tour engagement, model spatial navigation, and investigate room popularity. Behavioural clustering uncovered four distinct visitor types: Committed Trekkers, Leisurely Explorers, Targeted Visitors, and Speedy Samplers, each characterised by different levels of engagement and movement. Tour usage analysis revealed high drop-off rates and variation in completion rates across different language groups. Spatial flow modelling revealed that accessibility and proximity, particularly aversion to stairs, shaped visitor paths more than thematic organisation. Room popularity was more strongly predicted by physical accessibility than curatorial content. We propose practical strategies for improving engagement and flow, offering a scalable framework for visitor-centred, data-informed museum planning.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Community Detection on Model Explanation Graphs for Explainable AI</title>
<link>https://arxiv.org/abs/2510.27655</link>
<guid>https://arxiv.org/abs/2510.27655</guid>
<content:encoded><![CDATA[
<div> Feature-attribution methods, SHAP, LIME, Modules of Influence, model explanation graph, community detection<br />
<br />
Summary:
Modules of Influence (MoI) is a new framework for explaining individual predictions by identifying sets of features that work together to affect outcomes. The framework constructs a model explanation graph from per-instance attributions and applies community detection to find feature modules that jointly influence predictions. By quantifying the relationship between these modules and bias, redundancy, and causality patterns, MoI enhances model debugging and uncovers correlated feature groups. The framework also localizes bias exposure to specific modules, improving the understanding of model predictions. Stability and synergy metrics, a reference implementation, and evaluation protocols have been released to benchmark module discovery in explainable artificial intelligence (XAI). Overall, MoI offers a comprehensive approach to understanding and interpreting the behavior of machine learning models. <br /><br /> <div>
arXiv:2510.27655v1 Announce Type: new 
Abstract: Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions but often miss higher-order structure: sets of features that act in concert. We propose Modules of Influence (MoI), a framework that (i) constructs a model explanation graph from per-instance attributions, (ii) applies community detection to find feature modules that jointly affect predictions, and (iii) quantifies how these modules relate to bias, redundancy, and causality patterns. Across synthetic and real datasets, MoI uncovers correlated feature groups, improves model debugging via module-level ablations, and localizes bias exposure to specific modules. We release stability and synergy metrics, a reference implementation, and evaluation protocols to benchmark module discovery in XAI.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Similar Are Grokipedia and Wikipedia? A Multi-Dimensional Textual and Structural Comparison</title>
<link>https://arxiv.org/abs/2510.26899</link>
<guid>https://arxiv.org/abs/2510.26899</guid>
<content:encoded><![CDATA[
<div> Keywords: Grokipedia, Wikipedia, AI-generated, biases, encyclopedic content

Summary: 
The study compares 382 article pairs between Grokipedia and Wikipedia to examine the similarities and differences in form and content. Results show strong semantic and stylistic alignment between the two platforms, with Grokipedia producing longer but less diverse articles. Additionally, Grokipedia has fewer references per word and variable structural depth compared to Wikipedia, indicating a divergence in editorial norms. The findings suggest that AI-generated content mirrors Wikipedia's scope of information but favors narrative expansion over citation-based verification. This raises concerns around transparency, provenance, and knowledge governance in the age of automated text generation. Ultimately, while Grokipedia aims to provide "truthful" entries, the study highlights the challenges in escaping biases and limitations inherent in human-edited platforms like Wikipedia.

Summary: <div>
arXiv:2510.26899v1 Announce Type: cross 
Abstract: The launch of Grokipedia, an AI-generated encyclopedia developed by Elon Musk's xAI, was presented as a response to perceived ideological and structural biases in Wikipedia, aiming to produce "truthful" entries via the large language model Grok. Yet whether an AI-driven alternative can escape the biases and limitations of human-edited platforms remains unclear. This study undertakes a large-scale computational comparison of 382 matched article pairs between Grokipedia and Wikipedia. Using metrics across lexical richness, readability, structural organization, reference density, and semantic similarity, we assess how closely the two platforms align in form and substance. The results show that while Grokipedia exhibits strong semantic and stylistic alignment with Wikipedia, it typically produces longer but less lexically diverse articles, with fewer references per word and more variable structural depth. These findings suggest that AI-generated encyclopedic content currently mirrors Wikipedia's informational scope but diverges in editorial norms, favoring narrative expansion over citation-based verification. The implications highlight new tensions around transparency, provenance, and the governance of knowledge in an era of automated text generation.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions</title>
<link>https://arxiv.org/abs/2510.27195</link>
<guid>https://arxiv.org/abs/2510.27195</guid>
<content:encoded><![CDATA[
<div> AI systems; social intelligence; deception detection; Multimodal Large Language Models; MIVA<br />
<br />
Summary: 
The article discusses the challenge of automatic deception detection in dynamic, multi-party conversations and the potential of Multimodal Large Language Models (MLLMs) in addressing this issue. A new task, Multimodal Interactive Veracity Assessment (MIVA), is introduced along with a dataset derived from the social deduction game Werewolf. Despite the impressive capabilities of MLLMs, such as GPT-4o, in visual and textual understanding, they struggle to reliably distinguish truth from falsehood. The analysis of failure modes suggests that these models have difficulty grounding language in visual social cues and may be overly conservative in their alignment. This highlights the need for novel approaches to enhancing the perceptiveness and trustworthiness of AI systems. <div>
arXiv:2510.27195v1 Announce Type: cross 
Abstract: As AI systems become increasingly integrated into human lives, endowing them with robust social intelligence has emerged as a critical frontier. A key aspect of this intelligence is discerning truth from deception, a ubiquitous element of human interaction that is conveyed through a complex interplay of verbal language and non-verbal visual cues. However, automatic deception detection in dynamic, multi-party conversations remains a significant challenge. The recent rise of powerful Multimodal Large Language Models (MLLMs), with their impressive abilities in visual and textual understanding, makes them natural candidates for this task. Consequently, their capabilities in this crucial domain are mostly unquantified. To address this gap, we introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and present a novel multimodal dataset derived from the social deduction game Werewolf. This dataset provides synchronized video, text, with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating state-of-the-art MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to ground language in visual social cues effectively and may be overly conservative in their alignment, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping Regional Disparities in Discounted Grocery Products</title>
<link>https://arxiv.org/abs/2510.27493</link>
<guid>https://arxiv.org/abs/2510.27493</guid>
<content:encoded><![CDATA[
<div> sale, retail, food waste, geographic context, sustainability

Summary: 
- Food waste contributes significantly to greenhouse gas emissions, with the retail sector playing a crucial role in mediating product flows.
- The study analyzes data from Denmark's largest retail group to understand the variations in near-expiry product sales across different geographic contexts.
- By employing a dual-clustering approach, the research identifies regional retail store clusters and their spatial separation using shortest-path distances along the street network.
- The study reveals that stores in rural areas are more likely to put meat and dairy products on sale compared to metropolitan regions, while metropolitan areas tend to favor convenience products with balanced nutritional profiles but less favorable environmental impacts.
- The findings emphasize the need for region-specific sustainability strategies to tackle food waste and reduce greenhouse gas emissions. 

<br /><br />Summary: <div>
arXiv:2510.27493v1 Announce Type: cross 
Abstract: Food waste represents a major challenge to global climate resilience, accounting for almost 10\% of annual greenhouse gas emissions. The retail sector is a critical player, mediating product flows between producers and consumers, where supply chain inefficiencies can shape which items are put on sale. Yet how these dynamics vary across geographic contexts remains largely unexplored. Here, we analyze data from Denmark's largest retail group on near-expiry products put on sale. We uncover the geospatial variations using a dual-clustering approach. We identify multi-scale spatial relationships in retail organization by correlating store clustering -- measured using shortest-path distances along the street network -- with product clustering based on promotion co-occurrence patterns. Using a bipartite network approach, we identify three regional store clusters, and use percolation thresholds to corroborate the scale of their spatial separation. We find that stores in rural communities put meat and dairy products on sale up to 2.2 times more frequently than metropolitan areas. In contrast, we find that metropolitan and capital regions lean toward convenience products, which have more balanced nutritional profiles but less favorable environmental impacts. By linking geographic context to retail inventory, we provide evidence that reducing food waste requires interventions tailored to local retail dynamics, highlighting the importance of region-specific sustainability strategies.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representing Classical Compositions through Implication-Realization Temporal-Gestalt Graphs</title>
<link>https://arxiv.org/abs/2510.27530</link>
<guid>https://arxiv.org/abs/2510.27530</guid>
<content:encoded><![CDATA[
<div> Graph-based approach, Implication-Realization model, Temporal Gestalt theory, cognitive music analysis, computational musicology <br />
Summary: This study introduces a graph-based computational approach to analyze musical compositions incorporating the Implication-Realization model and Temporal Gestalt theory. By segmenting melodies into perceptual units and annotating them with I-R patterns, the approach captures structural and cognitive aspects of music perception. Dynamic Time Warping and k-nearest neighbors graphs are used to model relationships between segments. Nodes in the graphs are labeled with melodic expectancy values to encode tension and resolution. The Weisfeiler-Lehman graph kernel shows significant distinctions in intra- and inter-graph structures, while multidimensional scaling confirms structural similarity at the graph level reflects perceptual similarity at the segment level. Graph2vec embeddings and clustering demonstrate capturing stylistic and structural features beyond composer identity. Overall, this graph-based method provides a structured and cognitively informed framework for computational music analysis, offering a deeper understanding of musical structure and style through listener perception. <br /><br /> <div>
arXiv:2510.27530v1 Announce Type: cross 
Abstract: Understanding the structural and cognitive underpinnings of musical compositions remains a key challenge in music theory and computational musicology. While traditional methods focus on harmony and rhythm, cognitive models such as the Implication-Realization (I-R) model and Temporal Gestalt theory offer insight into how listeners perceive and anticipate musical structure. This study presents a graph-based computational approach that operationalizes these models by segmenting melodies into perceptual units and annotating them with I-R patterns. These segments are compared using Dynamic Time Warping and organized into k-nearest neighbors graphs to model intra- and inter-segment relationships.
  Each segment is represented as a node in the graph, and nodes are further labeled with melodic expectancy values derived from Schellenberg's two-factor I-R model-quantifying pitch proximity and pitch reversal at the segment level. This labeling enables the graphs to encode both structural and cognitive information, reflecting how listeners experience musical tension and resolution.
  To evaluate the expressiveness of these graphs, we apply the Weisfeiler-Lehman graph kernel to measure similarity between and within compositions. Results reveal statistically significant distinctions between intra- and inter-graph structures. Segment-level analysis via multidimensional scaling confirms that structural similarity at the graph level reflects perceptual similarity at the segment level. Graph2vec embeddings and clustering demonstrate that these representations capture stylistic and structural features that extend beyond composer identity.
  These findings highlight the potential of graph-based methods as a structured, cognitively informed framework for computational music analysis, enabling a more nuanced understanding of musical structure and style through the lens of listener perception.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sybil-Resistant Service Discovery for Agent Economies</title>
<link>https://arxiv.org/abs/2510.27554</link>
<guid>https://arxiv.org/abs/2510.27554</guid>
<content:encoded><![CDATA[
<div> HTTP services, cryptocurrency payments, TraceRank, reputation propagation, semantic search 
Summary: 
x402 introduces a system that allows HTTP services to accept cryptocurrency payments and uses TraceRank, a reputation-weighted ranking algorithm, to determine the reliability of services. Payment transactions serve as endorsements, influencing the reputation of services based on transaction value and recency. This approach promotes services preferred by high-reputation users over those with high transaction volume. By combining TraceRank with semantic search, the system provides high-quality results in response to natural language queries. The reputation propagation mechanism helps in resisting Sybil attacks, ensuring that spam services with low-reputation payers rank lower than legitimate services with high-reputation payers. The system aims to create a search method for x402 enabled services that is unbiased and outperforms purely volume-based or semantic methods. 
<br /><br />Summary: <div>
arXiv:2510.27554v1 Announce Type: cross 
Abstract: x402 enables Hypertext Transfer Protocol (HTTP) services like application programming interfaces (APIs), data feeds, and inference providers to accept cryptocurrency payments for access. As agents increasingly consume these services, discovery becomes critical: which swap interface should an agent trust? Which data provider is the most reliable? We introduce TraceRank, a reputation-weighted ranking algorithm where payment transactions serve as endorsements. TraceRank seeds addresses with precomputed reputation metrics and propagates reputation through payment flows weighted by transaction value and temporal recency. Applied to x402's payment graph, this surfaces services preferred by high-reputation users rather than those with high transaction volume. Our system combines TraceRank with semantic search to respond to natural language queries with high quality results. We argue that reputation propagation resists Sybil attacks by making spam services with many low-reputation payers rank below legitimate services with few high-reputation payers. Ultimately, we aim to construct a search method for x402 enabled services that avoids infrastructure bias and has better performance than purely volume based or semantic methods.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social learning moderates the tradeoffs between efficiency, stability, and equity in group foraging</title>
<link>https://arxiv.org/abs/2510.27683</link>
<guid>https://arxiv.org/abs/2510.27683</guid>
<content:encoded><![CDATA[
<div> social learning, collective search, information sharing, area-restricted search, foraging

Summary:
The study explores how social learning impacts collective search behavior in foraging scenarios. The model combines social learning with area-restricted search and considers three behavioral modes: exploration, exploitation, and targeted walk. A parameter, $\rho$, regulates the balance between exploration and exploitation at the group level. Results show a trade-off between group efficiency, temporal variability, and agent equity in resource distribution based on $\rho$. Optimal group efficiency occurs at intermediate $\rho$ values balancing exploration and exploitation. At high $\rho$, agent equality is high, but efficiency decreases. Introducing negative rewards demonstrates how social learning can mitigate risk. <div>
arXiv:2510.27683v1 Announce Type: cross 
Abstract: Social learning shapes collective search by influencing how individuals use peer information. Empirical and computational studies show that optimal information sharing that is neither too localized nor too diffuse, can enhance resource detection and coordination. Building on these insights, we develop a randomized search model that integrates social learning with area-restricted search (ARS) to investigate how communication distance affects collective foraging. The model includes three behavioral modes: exploration, exploitation, and targeted walk, which are governed by a single parameter, $\rho$, that balances exploration and exploitation at the group level. We quantify how $\rho$ influences group efficiency ($\eta$), temporal variability/burstiness ($B$), and agent variability/equity in resource distribution ($\sigma$), revealing a clear trade-off among these outcomes. When $\rho \to 0$, agents explore independently, maximizing collective exploration. As $\rho$ increases, individuals preferentially exploit patches discovered by others: $\eta$ first rises and then declines, while $B$ shows the opposite trend. Group efficiency is optimized at interior $\rho$ values that balance exploration and exploitation. At the largest $\rho$, equality among agents is highest, but efficiency declines and burstiness is maximized too. Finally, by introducing negative rewards, we examine how social learning mitigates risk.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flexible Modeling of Information Diffusion on Networks with Statistical Guarantees</title>
<link>https://arxiv.org/abs/2411.09100</link>
<guid>https://arxiv.org/abs/2411.09100</guid>
<content:encoded><![CDATA[
<div> likelihood-based approach, edge weights, General Linear Threshold model, Influence Maximization, information diffusion paths<br />
Summary:<br />
The paper focuses on the estimation of edge weights in information spread models through networks, specifically under the General Linear Threshold (GLT) model. It addresses the need for accurate estimators of edge weights in probabilistic diffusion models and establishes the asymptotic properties of the estimator for the GLT model. The study includes deriving conditions for edge weight identifiability, finite sample error bounds, and asymptotic normality of the estimator. Additionally, the paper explores the application of the GLT model in the context of the Influence Maximization (IM) problem, optimizing node selections to maximize information spread. Through experiments on synthetic and real-world networks, the GLT model and estimation framework show improved spread estimation, node activation prediction, and quality of IM problem solutions. <div>
arXiv:2411.09100v2 Announce Type: replace 
Abstract: Modeling information spread through a network is one of the key problems of network analysis, with applications in a wide array of areas such as marketing and public health. Most approaches assume that the spread is governed by some probabilistic diffusion model, often parameterized by the strength of connections between network members (edge weights), highlighting the need for methods that can accurately estimate them. Multiple prior works suggest such estimators for particular diffusion models; however, most of them lack a rigorous statistical analysis that would establish the asymptotic properties of the estimator and allow for uncertainty quantification. In this paper, we develop a likelihood-based approach to estimate edge weights from the observed information diffusion paths under the proposed General Linear Threshold (GLT) model, a broad class of discrete-time information diffusion models that includes both the well-known linear threshold (LT) and independent cascade (IC) models. We first derive necessary and sufficient conditions that make the edge weights identifiable under this model. Then, we derive a finite sample error bound for the estimator and demonstrate that it is asymptotically normal under mild conditions. We conclude by studying the GLT model in the context of the Influence Maximization (IM) problem, that is, the task of selecting a subset of $k$ nodes to start the diffusion, so that the average information spread is maximized. Extensive experiments on synthetic and real-world networks demonstrate that the flexibility of the proposed class of GLT models, coupled with the proposed estimation and inference framework for its parameters, can significantly improve estimation of spread from a given subset of nodes, prediction of node activation, and the quality of the IM problem solutions.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Media Coverage of War Victims: Journalistic Biases in Reporting on Israel and Gaza</title>
<link>https://arxiv.org/abs/2510.06453</link>
<guid>https://arxiv.org/abs/2510.06453</guid>
<content:encoded><![CDATA[
<div> media bias, asymmetrical warfare, Israeli victims, Palestinian victims, journalistic biases

Summary: 
Western media coverage of the war against Gaza in 2023 showed three systematic biases. Firstly, Israeli victims were portrayed as individual human beings, while Palestinian victims were depicted as undifferentiated collectives. Secondly, a false balance was created by equating Israeli and Palestinian suffering, even when there were no new events involving Israeli victims. Lastly, when reporting on the number of victims, journalists cast doubt on the credibility of information regarding Palestinian suffering, undermining readers' trust. These biases were prevalent in Western outlets like The New York Times, BBC, and CNN but were minimal in Al Jazeera's coverage of the conflict. This analysis sheds light on the role of media bias in shaping perceptions of asymmetrical warfare and highlights the need for more objective reporting in conflict-ridden regions. 

<br /><br />Summary: <div>
arXiv:2510.06453v2 Announce Type: replace 
Abstract: October 7th 2023 marked the start of a war against Gaza, which is considered one of the most devastating wars in modern history and has led to a stark attitudinal divide within and between countries. To investigate the role of media bias in reporting on this asymmetrical warfare, we analyzed over 14,000 news articles published during the first year of war in three Western (The New York Times, BBC, CNN) and one non-Western English-language outlets (Al Jazeera English). Exploring the media narratives concerning Israeli and Palestinian victims experiencing hardship, we found three systematic biases in Western media. 1) Compared to Palestinian victims, represented mainly as undifferentiated collectives, Israeli victims were more likely to be portrayed as identifiable individual human beings. 2) Despite the striking difference in all forms of hardship (casualties, displacement, etc.), Western journalists created a false balance, equating Israeli and Palestinian suffering, by persistently referring back to the 7th of October massacre, even in the absence of new events involving Israeli victims. 3) When reporting on numbers of Palestinian (vs. Israeli) victims, journalists used language that casts doubt about the credibility of the information and the reputation of the source providing it, thereby selectively undermining the reader's trust in the information regarding Palestinian suffering. Together, our analysis reveals a series of systematic journalistic biases in high-profile Western media that are absent or greatly reduced in Al Jazeera.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flex-GAD : Flexible Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2510.25809</link>
<guid>https://arxiv.org/abs/2510.25809</guid>
<content:encoded><![CDATA[
<div> Framework, unsupervised, anomaly detection, attributed networks, node level<br />
Summary:<br />
- Flex-GAD is an unsupervised framework designed for detecting anomalous nodes in attributed networks, incorporating two encoders to capture structural and attribute information.<br />
- It integrates a community-based GCN encoder to model intra-community and inter-community relationships, ensuring structural consistency, and a standard attribute encoder for descriptive features.<br />
- The framework utilizes a self-attention-based representation fusion module to combine diverse representations, allowing adaptive weighting for effective integration of information from different encoders.<br />
- Flex-GAD outperforms the previous best-performing method, GAD-NR, by achieving an average AUC improvement of 7.98% across seven real-world attributed graphs, showcasing its effectiveness and flexibility.<br />
- Additionally, Flex-GAD significantly reduces training time, running 102x faster per epoch than Anomaly DAE and 3x faster per epoch than GAD-NR on average, making it a powerful and efficient tool for graph anomaly detection. <br /> 
Summary: <div>
arXiv:2510.25809v1 Announce Type: new 
Abstract: Detecting anomalous nodes in attributed networks, where each node is associated with both structural connections and descriptive attributes, is essential for identifying fraud, misinformation, and suspicious behavior in domains such as social networks, academic citation graphs, and e-commerce platforms. We propose Flex-GAD, a novel unsupervised framework for graph anomaly detection at the node level. Flex-GAD integrates two encoders to capture complementary aspects of graph data. The framework incorporates a novel community-based GCN encoder to model intra-community and inter-community information into node embeddings, thereby ensuring structural consistency, along with a standard attribute encoder. These diverse representations are fused using a self-attention-based representation fusion module, which enables adaptive weighting and effective integration of the encoded information. This fusion mechanism allows automatic emphasis of the most relevant node representation across different encoders. We evaluate Flex-GAD on seven real-world attributed graphs with varying sizes, node degrees, and attribute homogeneity. Flex-GAD achieves an average AUC improvement of 7.98% over the previously best-performing method, GAD-NR, demonstrating its effectiveness and flexibility across diverse graph structures. Moreover, it significantly reduces training time, running 102x faster per epoch than Anomaly DAE and 3x faster per epoch than GAD-NR on average across seven benchmark datasets.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signed Graph Unlearning</title>
<link>https://arxiv.org/abs/2510.26092</link>
<guid>https://arxiv.org/abs/2510.26092</guid>
<content:encoded><![CDATA[
<div> privacy-preserving mechanisms, signed networks, graph unlearning, SGU, model performance<br />
<br />
Summary: 
The article discusses the importance of privacy-preserving mechanisms in signed networks on social media platforms. It introduces SGU (Signed Graph Unlearning), a framework specifically designed for signed networks. SGU incorporates a new partition paradigm and algorithm that preserve edge sign information during partitioning, ensuring structural balance across partitions. Existing graph unlearning methods designed for unsigned networks do not consider edge sign information, leading to structural imbalance and decreased model performance in signed networks. SGU outperforms baselines in both model performance and unlearning efficiency, making it a state-of-the-art solution for managing sensitive and dynamic user interactions in signed networks. <div>
arXiv:2510.26092v1 Announce Type: new 
Abstract: The proliferation of signed networks in contemporary social media platforms necessitates robust privacy-preserving mechanisms. Graph unlearning, which aims to eliminate the influence of specific data points from trained models without full retraining, becomes particularly critical in these scenarios where user interactions are sensitive and dynamic. Existing graph unlearning methodologies are exclusively designed for unsigned networks and fail to account for the unique structural properties of signed graphs. Their naive application to signed networks neglects edge sign information, leading to structural imbalance across subgraphs and consequently degrading both model performance and unlearning efficiency. This paper proposes SGU (Signed Graph Unlearning), a graph unlearning framework specifically for signed networks. SGU incorporates a new graph unlearning partition paradigm and a novel signed network partition algorithm that preserve edge sign information during partitioning and ensure structural balance across partitions. Compared with baselines, SGU achieves state-of-the-art results in both model performance and unlearning efficiency.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating and Experimenting with Social Media Mobilization Using LLM Agents</title>
<link>https://arxiv.org/abs/2510.26494</link>
<guid>https://arxiv.org/abs/2510.26494</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, political mobilization, agent-based simulation, voter turnout, peer influence

Summary: 
An agent-based simulation framework was developed to study the impact of mobilization messages on voter turnout in online social networks. The framework integrates real U.S. Census demographic distributions, authentic Twitter network topology, and large language model (LLM) agents with varying political sophistication. Simulated agents interact over realistic social network structures, receiving personalized feeds and updating their engagement behaviors and voting intentions. The simulation replicates patterns observed in field experiments, showing stronger mobilization effects under social message treatments and peer spillovers. This framework allows for testing counterfactual designs and sensitivity analyses in political mobilization research, bridging the gap between field experiments and computational modeling. The code and data for the simulation are available on GitHub for reproducibility and further research. 

<br /><br />Summary: <div>
arXiv:2510.26494v1 Announce Type: new 
Abstract: Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\texttt{GPT-4.1}, \texttt{GPT-4.1-Mini}, or \texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis</title>
<link>https://arxiv.org/abs/2510.26172</link>
<guid>https://arxiv.org/abs/2510.26172</guid>
<content:encoded><![CDATA[
<div> Keywords: social media mining, large language models, heterogeneous data, visualization, multi-modal integration

Summary:<br />
The article discusses the challenges of analyzing heterogeneous data from social media platforms and introduces SIA (Social Insight Agents), an automated system that leverages large language models to analyze raw inputs such as text, network, and behavioral data. SIA connects different data types through coordinated agent flows guided by a taxonomy linking insight types with appropriate mining and visualization techniques. The system includes a data coordinator to unify tabular, textual, and network data for coherent analysis. SIA's interactive interface allows users to track, validate, and refine the agent's reasoning, supporting adaptability and trustworthiness. Expert-centered case studies and quantitative evaluations demonstrate that SIA effectively uncovers diverse insights from social media while facilitating collaboration between humans and agents in complex analytical tasks. <div>
arXiv:2510.26172v1 Announce Type: cross 
Abstract: Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Distributed Link Predictions Among Peers in Online Classrooms Using Federated Learning</title>
<link>https://arxiv.org/abs/2504.10456</link>
<guid>https://arxiv.org/abs/2504.10456</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Learning Networks, Federated Learning, Privacy-preserving, Model personalization, Explainable AI<br />
Summary: 
This study proposes a framework that integrates Federated Learning with Social Learning Networks to predict future interactions among students in multiple classrooms. By leveraging Federated Learning, the approach allows collaborative model training while preserving data privacy. Model personalization techniques are utilized to adapt the model to individual classroom characteristics. The results demonstrate the effectiveness of the approach in capturing both shared and classroom-specific representations of student interactions. Additionally, explainable AI techniques are employed to interpret model predictions, identifying key factors influencing link formation across classrooms. This collaborative and distributed machine learning framework provides insights into the drivers of social learning interactions in a privacy-preserving manner. <div>
arXiv:2504.10456v2 Announce Type: replace 
Abstract: Social interactions among classroom peers, represented as social learning networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN analysis has recently garnered attention, most existing approaches rely on centralized training, where data is aggregated and processed on a local/cloud server with direct access to raw data. However, in real-world educational settings, such direct access across multiple classrooms is often restricted due to privacy concerns. Furthermore, training models on isolated classroom data prevents the identification of common interaction patterns that exist across multiple classrooms, thereby limiting model performance. To address these challenges, we propose one of the first frameworks that integrates Federated Learning (FL), a distributed and collaborative machine learning (ML) paradigm, with SLNs derived from students' interactions in multiple classrooms' online forums to predict future link formations (i.e., interactions) among students. By leveraging FL, our approach enables collaborative model training across multiple classrooms while preserving data privacy, as it eliminates the need for raw data centralization. Recognizing that each classroom may exhibit unique student interaction dynamics, we further employ model personalization techniques to adapt the FL model to individual classroom characteristics. Our results demonstrate the effectiveness of our approach in capturing both shared and classroom-specific representations of student interactions in SLNs. Additionally, we utilize explainable AI (XAI) techniques to interpret model predictions, identifying key factors that influence link formation across different classrooms. These insights unveil the drivers of social learning interactions within a privacy-preserving, collaborative, and distributed ML framework -- an aspect that has not been explored before.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merit Network Telescope: Processing and Initial Insights from Nearly 20 Years of Darknet Traffic for Cybersecurity Research</title>
<link>https://arxiv.org/abs/2510.25050</link>
<guid>https://arxiv.org/abs/2510.25050</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet traffic, network telescope, threat activity, scanning behavior, denial-of-service campaigns

Summary:
An initial longitudinal analysis of unsolicited Internet traffic collected between 2005 and 2025 by a network telescope operated by Merit Network reveals global threat activity through scanning and backscatter traffic. The dataset allows insights into large-scale probing behavior, data outages, and ongoing denial-of-service campaigns. A coarse-to-fine methodology is used for processing the extensive archive, extracting general insights through a metadata sub-pipeline and detailed analysis with a packet header sub-pipeline. Long-term trends and recurring traffic spikes are observed, some attributed to Internet-wide scanning events and others linked to DoS activities. General observations from 2006-2024 are presented, with a focused analysis of traffic characteristics in 2024. <div>
arXiv:2510.25050v1 Announce Type: new 
Abstract: This paper presents an initial longitudinal analysis of unsolicited Internet traffic collected between 2005 and 2025 by one of the largest and most persistent network telescopes in the United States, operated by Merit Network. The dataset provides a unique view into global threat activity as observed through scanning and backscatter traffic, key indicators of large-scale probing behavior, data outages, and ongoing denial-of-service (DoS) campaigns. To process this extensive archive, coarse-to-fine methodology is adopted in which general insights are first extracted through a resource-efficient metadata sub-pipeline, followed by a more detailed packet header sub-pipeline for finer-grained analysis. The methodology establishes two sub-pipelines to enable scalable processing of nearly two decades of telescope data and supports multi-level exploration of traffic dynamics. Initial insights highlight long-term trends and recurring traffic spikes, some attributable to Internet-wide scanning events and others likely linked to DoS activities.We present general observations spanning 2006-2024, with a focused analysis of traffic characteristics during 2024.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMM-Fact: A Multimodal, Multi-Domain Fact-Checking Dataset with Multi-Level Retrieval Difficulty</title>
<link>https://arxiv.org/abs/2510.25120</link>
<guid>https://arxiv.org/abs/2510.25120</guid>
<content:encoded><![CDATA[
<div> multimodal, fact-checking, benchmark, misinformation, evidence  
Summary:  
The article introduces MMM-Fact, a new benchmark for fact-checking that addresses the limitations of existing resources. MMM-Fact includes 125,449 fact-checked statements from 1995 to 2025, covering multiple domains and providing multimodal evidence such as text, images, videos, and tables. Each statement is tagged with a retrieval-difficulty tier to support fairness in evaluation. The dataset uses a three-class veracity scheme (true/false/not enough information) and enables tasks in veracity prediction, explainable fact-checking, evidence aggregation, and longitudinal analysis. Baseline models using mainstream LLMs show that MMM-Fact is more challenging than previous benchmarks, especially as evidence complexity increases. Overall, MMM-Fact offers a realistic and scalable benchmark for transparent and reliable fact-checking that goes beyond simple evidence-based reasoning.  
Summary: <div>
arXiv:2510.25120v1 Announce Type: new 
Abstract: Misinformation and disinformation demand fact checking that goes beyond simple evidence-based reasoning. Existing benchmarks fall short: they are largely single modality (text-only), span short time horizons, use shallow evidence, cover domains unevenly, and often omit full articles -- obscuring models' real-world capability. We present MMM-Fact, a large-scale benchmark of 125,449 fact-checked statements (1995--2025) across multiple domains, each paired with the full fact-check article and multimodal evidence (text, images, videos, tables) from four fact-checking sites and one news outlet. To reflect verification effort, each statement is tagged with a retrieval-difficulty tier -- Basic (1--5 sources), Intermediate (6--10), and Advanced (>10) -- supporting fairness-aware evaluation for multi-step, cross-modal reasoning. The dataset adopts a three-class veracity scheme (true/false/not enough information) and enables tasks in veracity prediction, explainable fact-checking, complex evidence aggregation, and longitudinal analysis. Baselines with mainstream LLMs show MMM-Fact is markedly harder than prior resources, with performance degrading as evidence complexity rises. MMM-Fact offers a realistic, scalable benchmark for transparent, reliable, multimodal fact-checking.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Emotional Co-occurrence Patterns Revealed by Network Analysis of Social Media</title>
<link>https://arxiv.org/abs/2510.25204</link>
<guid>https://arxiv.org/abs/2510.25204</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion network, social media, crisis events, network theory, emotion co-occurrence

Summary: 
- The study explores emotion interactions as an emotion network in social media during crisis events and normal times, using a network theory-based computational approach.
- Large-scale Japanese social media data from the past decade covering earthquakes and COVID-19 vaccination periods is analyzed.
- Emotion links, represented by link strength, particularly those associated with Tension, are found to be significantly strengthened during earthquake and pre-vaccination periods.
- The stability of emotion network structure across different situations and over time at the population level is revealed.
- The research challenges the context-based assumption of emotion co-occurrence and provides deeper insights into the intrinsic structure of emotions. 
<br /><br />Summary: <div>
arXiv:2510.25204v1 Announce Type: new 
Abstract: Examining emotion interactions as an emotion network in social media offers key insights into human psychology, yet few studies have explored how fluctuations in such emotion network evolve during crises and normal times. This study proposes a novel computational approach grounded in network theory, leveraging large-scale Japanese social media data spanning varied crisis events (earthquakes and COVID-19 vaccination) and non-crisis periods over the past decade. Our analysis identifies and evaluates links between emotions through the co-occurrence of emotion-related concepts (words), revealing a stable structure of emotion network across situations and over time at the population level. We find that some emotion links (represented as link strength) such as emotion links associated with Tension are significantly strengthened during earthquake and pre-vaccination periods. However, the rank of emotion links remains highly intact. These findings challenge the assumption that emotion co-occurrence is context-based and offer a deeper understanding of emotions' intrinsic structure. Moreover, our network-based framework offers a systematic, scalable method for analyzing emotion co-occurrence dynamics, opening new avenues for psychological research using large-scale textual data.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Correlation in Graphs by Counting Bounded Degree Motifs</title>
<link>https://arxiv.org/abs/2510.25289</link>
<guid>https://arxiv.org/abs/2510.25289</guid>
<content:encoded><![CDATA[
<div> graphs, correlation, hypothesis testing, motifs, networks

Summary:
- The paper focuses on detecting correlation between two Erdős-Rényi graphs, treating it as a hypothesis testing problem.
- A polynomial-time test is developed using bounded degree motifs, proving its effectiveness for any constant correlation coefficient when the edge connecting probability is at least n^-2/3.
- Results show that the test works for any constant correlation, overcoming previous limitations.
- The use of bounded degree motifs, common in real networks, makes the proposed test both intuitive and scalable.
- Validation on synthetic and real co-citation networks confirms the effectiveness of this approach in capturing correlation signals. 

<br /><br />Summary: <div>
arXiv:2510.25289v1 Announce Type: new 
Abstract: Correlation analysis is a fundamental step for extracting meaningful insights from complex datasets. In this paper, we investigate the problem of detecting correlation between two Erd\H{o}s-R\'enyi graphs $G(n,p)$, formulated as a hypothesis testing problem: under the null hypothesis, the two graphs are independent, while under the alternative hypothesis, they are correlated. We develop a polynomial-time test by counting bounded degree motifs and prove its effectiveness for any constant correlation coefficient $\rho$ when the edge connecting probability satisfies $p\ge n^{-2/3}$. Our results overcome the limitation requiring $\rho \ge \sqrt{\alpha}$, where $\alpha\approx 0.338$ is the Otter's constant, extending it to any constant $\rho$. Methodologically, bounded degree motifs -- ubiquitous in real networks -- make the proposed statistic both natural and scalable. We also validate our method on synthetic and real co-citation networks, further confirming that this simple motif family effectively captures correlation signals and exhibits strong empirical performance.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YTLive: A Dataset of Real-World YouTube Live Streaming Sessions</title>
<link>https://arxiv.org/abs/2510.24769</link>
<guid>https://arxiv.org/abs/2510.24769</guid>
<content:encoded><![CDATA[
<div> Keywords: YouTube Live, live streaming, viewer behavior, dataset, temporal viewing patterns

Summary:
The article introduces the YTLive dataset, a public dataset focused on YouTube Live, containing over 507,000 records from 12,156 live streams. The dataset tracks concurrent viewer counts at five-minute intervals, providing insights into viewer behavior in real-time. An initial analysis of the dataset reveals that viewer counts are higher and more stable on weekends, particularly in the afternoon. Shorter streams attract larger and more consistent audiences, while longer streams exhibit slower growth and greater variability. These findings have implications for adaptive streaming, resource allocation, and Quality of Experience (QoE) modeling in live streaming platforms. YTLive serves as a valuable resource for researchers and industry professionals looking to study and improve live streaming systems. The dataset is available on GitHub for further analysis and research purposes. 

<br /><br />Summary: <div>
arXiv:2510.24769v1 Announce Type: cross 
Abstract: Live streaming plays a major role in today's digital platforms, supporting entertainment, education, social media, etc. However, research in this field is limited by the lack of large, publicly available datasets that capture real-time viewer behavior at scale. To address this gap, we introduce YTLive, a public dataset focused on YouTube Live. Collected through the YouTube Researcher Program over May and June 2024, YTLive includes more than 507000 records from 12156 live streams, tracking concurrent viewer counts at five-minute intervals along with precise broadcast durations. We describe the dataset design and collection process and present an initial analysis of temporal viewing patterns. Results show that viewer counts are higher and more stable on weekends, especially during afternoon hours. Shorter streams attract larger and more consistent audiences, while longer streams tend to grow slowly and exhibit greater variability. These insights have direct implications for adaptive streaming, resource allocation, and Quality of Experience (QoE) modeling. YTLive offers a timely, open resource to support reproducible research and system-level innovation in live streaming. The dataset is publicly available at github.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction</title>
<link>https://arxiv.org/abs/2510.25348</link>
<guid>https://arxiv.org/abs/2510.25348</guid>
<content:encoded><![CDATA[
<div> Keywords: information cascade prediction, social networks, e-commerce dataset, temporal modeling, state-of-the-art performance

Summary:
- The article addresses the limitations of current information cascade popularity prediction works.
- It introduces a time-ordered splitting strategy to avoid temporal leakage in evaluation.
- A new dataset, Taoke, is presented with rich promoter/product attributes and ground-truth purchase conversions.
- The CasTemp framework is developed, utilizing temporal walks, neighbor selection, and GRU-based encoding with time-aware attention for efficient cascade dynamics modeling.
- CasTemp achieves state-of-the-art performance on four datasets with a significant speedup, particularly excelling in predicting second-stage popularity conversions. 

<br /><br />Summary: <div>
arXiv:2510.25348v1 Announce Type: cross 
Abstract: Information cascade popularity prediction is a key problem in analyzing content diffusion in social networks. However, current related works suffer from three critical limitations: (1) temporal leakage in current evaluation--random cascade-based splits allow models to access future information, yielding unrealistic results; (2) feature-poor datasets that lack downstream conversion signals (e.g., likes, comments, or purchases), which limits more practical applications; (3) computational inefficiency of complex graph-based methods that require days of training for marginal gains. We systematically address these challenges from three perspectives: task setup, dataset construction, and model design. First, we propose a time-ordered splitting strategy that chronologically partitions data into consecutive windows, ensuring models are evaluated on genuine forecasting tasks without future information leakage. Second, we introduce Taoke, a large-scale e-commerce cascade dataset featuring rich promoter/product attributes and ground-truth purchase conversions--capturing the complete diffusion lifecycle from promotion to monetization. Third, we develop CasTemp, a lightweight framework that efficiently models cascade dynamics through temporal walks, Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based encoding with time-aware attention. Under leak-free evaluation, CasTemp achieves state-of-the-art performance across four datasets with orders-of-magnitude speedup. Notably, it excels at predicting second-stage popularity conversions--a practical task critical for real-world applications.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph clustering using Ricci curvature: an edge transport perspective</title>
<link>https://arxiv.org/abs/2412.15695</link>
<guid>https://arxiv.org/abs/2412.15695</guid>
<content:encoded><![CDATA[
<div> Ricci flow; hypergraphs; probability measures; community detection; weighting<br />
<br />
Summary: 
The paper introduces a novel method for extending Ricci flow to hypergraphs by defining probability measures on edges and transporting them on the line expansion. This new approach results in a new weighting on edges that improves community detection. A comparison with a similar Ricci flow defined on clique expansion shows enhanced sensitivity to hypergraph structure, particularly with large hyperedges. The two methods, when used together, create a powerful and interpretable framework for community detection in hypergraphs. <div>
arXiv:2412.15695v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce a novel method for extending Ricci flow to hypergraphs by defining probability measures on the edges and transporting them on the line expansion. This approach yields a new weighting on the edges, which proves particularly effective for community detection. We extensively compare this method with a similar notion of Ricci flow defined on the clique expansion, demonstrating its enhanced sensitivity to the hypergraph structure, especially in the presence of large hyperedges. The two methods are complementary and together form a powerful and highly interpretable framework for community detection in hypergraphs.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global YouTube Trending Dataset (2022-2025): Three Years of Platform-Curated, Cross-National Trends in Digital Culture</title>
<link>https://arxiv.org/abs/2510.23645</link>
<guid>https://arxiv.org/abs/2510.23645</guid>
<content:encoded><![CDATA[
<div> Keywords: YouTube, Trending, dataset, video, global attention

Summary: 
The article discusses the retirement of YouTube's public "Trending" pages on July 1, 2025, and the implications of this change on video discovery. A three-year archival dataset of YouTube Trending videos from July 1, 2022, to June 30, 2025, is presented, providing a valuable resource for studying global attention and cultural salience. The dataset includes 446,971 snapshots from 104 countries, encompassing 78.4 million video entries with associated metadata. Each record contains core identifiers and content metadata, allowing for in-depth analysis of digital culture and platform governance. The article outlines the data collection methodology, schema design, coverage, and descriptive statistics for both global and U.S. trending videos. Ethical safeguards were also implemented throughout the data collection process. This dataset offers a unique opportunity to study temporal dynamics in content popularity and platform influence on a global scale.<br /><br />Summary: <div>
arXiv:2510.23645v1 Announce Type: new 
Abstract: On July 1, 2025, YouTube retired its decade-long public "Trending" pages, ending platform-curated, non-personalized video discovery. The Trending list had long served as a vital lens into algorithmic influence, cultural diffusion, and crisis communication globally, offering a rare "ground-truth" reference to study global attention and cultural salience. We present a three-year archival dataset of YouTube Trending videos, collected from July 1, 2022, to June 30, 2025, with four daily snapshots for each of the 104 countries. The dataset includes 446,971 snapshots, each capturing up to 200 trending videos, encompassing 78.4 million video entries (726,627 unique videos) and associated metadata. Each record includes core identifiers (snapshot time, country, rank) and content metadata (video ID, channel ID, title, description, tags, publication date, category, channel name, language, live status, views, and comments). Unlike previous datasets with limited geographic scope or short timeframes, our non-personalized data provides exceptional cross-national and longitudinal coverage for studying digital culture, platform governance, and temporal dynamics in content popularity. We document the data collection methodology, schema design, coverage, descriptive statistics for both global and U.S. trending videos, and the ethical safeguards implemented throughout.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hamming Graph Metrics: A Multi-Scale Framework for Structural Redundancy and Uniqueness in Graphs</title>
<link>https://arxiv.org/abs/2510.23646</link>
<guid>https://arxiv.org/abs/2510.23646</guid>
<content:encoded><![CDATA[
<div> reachability tensor, graph centrality, Hamming Graph Metrics, multi-scale connectivity patterns, structural uniqueness

Summary: The paper introduces Hamming Graph Metrics (HGM), a framework to quantify the structural uniqueness of multi-scale connectivity patterns in graphs. HGM represents a graph using an exact-$k$ reachability tensor $\mathcal{B}G$, capturing shortest-path distances. The framework ensures permutation invariance and defines a true metric, the tensor Hamming distance, on labeled graphs. It offers Lipschitz stability to edge perturbations. The paper develops per-scale spectral analysis, summary statistics for structural dissimilarity, graph-to-graph comparison using the metric, and various analytic properties such as extremal characterizations and stability bounds. This framework enables a deeper understanding of network resilience and function by capturing the unique connectivity patterns present in graphs.<br /><br /> <div>
arXiv:2510.23646v1 Announce Type: new 
Abstract: Traditional graph centrality measures effectively quantify node importance but fail to capture the structural uniqueness of multi-scale connectivity patterns -- critical for understanding network resilience and function. This paper introduces \emph{Hamming Graph Metrics (HGM)}, a framework that represents a graph by its exact-$k$ reachability tensor $\mathcal{B}G\in{0,1}^{N\times N\times D}$ with slices $(\mathcal{B}G){:,:,1}=A$ and, for $k\ge 2$, $(\mathcal{B}G){:,:,k}=\mathbf{1}!\left[\sum{t=1}^{k} A^t>0\right]-\mathbf{1}!\left[\sum_{t=1}^{k-1} A^t>0\right]$ (shortest-path distance exactly $k$). Guarantees. (i) \emph{Permutation invariance}: $d_{\mathrm{HGM}}(\pi(G),\pi(H))=d_{\mathrm{HGM}}(G,H)$ for all vertex relabelings $\pi$; (ii) the \emph{tensor Hamming distance} $d_{\mathrm{HGM}}(G,H):=|,\mathcal{B}G-\mathcal{B}H,|{1}=\sum{i,j,k}\mathbf{1}!\big[(\mathcal{B}G){ijk}\neq(\mathcal{B}H){ijk}\big]$ is a \emph{true metric} on labeled graphs; and (iii) \emph{Lipschitz stability} to edge perturbations with explicit degree-dependent constants (see Graph-to-Graph Comparison'' $\to$ Tensor Hamming metric''; ``Stability to edge perturbations''; Appendix A). We develop: (1) \emph{per-scale spectral analysis} via classical MDS on double-centered Hamming matrices $D^{(k)}$, yielding spectral coordinates and explained variances; (2) \emph{summary statistics} for node-wise and graph-level structural dissimilarity; (3) \emph{graph-to-graph comparison} via the metric above; and (4) \emph{analytic properties} including extremal characterizations, multi-scale limits, and stability bounds.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoGBot: Relationship-Oblivious Graph-based Neural Network with Contextual Knowledge for Bot Detection</title>
<link>https://arxiv.org/abs/2510.23648</link>
<guid>https://arxiv.org/abs/2510.23648</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, BERT, semantic embeddings, max pooling, user-level representations
Summary:
- The article discusses the challenge of detecting automated accounts on platforms like Twitter.
- Existing methods combine text, metadata, and user relationship info in graph-based frameworks.
- Many models rely heavily on explicit user-user relationship data, limiting their applicability.
- The proposed multimodal framework integrates textual features with user metadata and employs graph-based reasoning without needing follower-following data.
- Transformer-based models extract deep embeddings from tweets, aggregated using max pooling for user-level representations, combined with behavioral features, and passed through a GraphSAGE model for local and global behavior patterns.
<br /><br />Summary: <div>
arXiv:2510.23648v1 Announce Type: new 
Abstract: Detecting automated accounts (bots) among genuine users on platforms like Twitter remains a challenging task due to the evolving behaviors and adaptive strategies of such accounts. While recent methods have achieved strong detection performance by combining text, metadata, and user relationship information within graph-based frameworks, many of these models heavily depend on explicit user-user relationship data. This reliance limits their applicability in scenarios where such information is unavailable. To address this limitation, we propose a novel multimodal framework that integrates detailed textual features with enriched user metadata while employing graph-based reasoning without requiring follower-following data. Our method uses transformer-based models (e.g., BERT) to extract deep semantic embeddings from tweets, which are aggregated using max pooling to form comprehensive user-level representations. These are further combined with auxiliary behavioral features and passed through a GraphSAGE model to capture both local and global patterns in user behavior. Experimental results on the Cresci-15, Cresci-17, and PAN 2019 datasets demonstrate the robustness of our approach, achieving accuracies of 99.8%, 99.1%, and 96.8%, respectively, and highlighting its effectiveness against increasingly sophisticated bot strategies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JiuTian Chuanliu: A Large Spatiotemporal Model for General-purpose Dynamic Urban Sensing</title>
<link>https://arxiv.org/abs/2510.23662</link>
<guid>https://arxiv.org/abs/2510.23662</guid>
<content:encoded><![CDATA[
<div> Keywords: urban sensing, human mobility data, spatiotemporal model, self-supervised learning, dynamic graph

Summary: 
The paper introduces the General-purpose and Dynamic Human Mobility Embedding (GDHME) framework for urban sensing using large-scale human mobility data collected from base stations. The framework employs self-supervised learning to capture the latent semantics behind mobility behavior and supports various urban sensing tasks. In the first stage, GDHME treats people and regions as nodes in a dynamic graph, capturing evolving node representations in continuous time. An autoregressive self-supervised task guides the learning of general-purpose node embeddings. In the second stage, these representations are utilized for various tasks. Offline experiments demonstrate the framework's ability to automatically learn valuable node features from vast data. The framework was used to deploy the JiuTian ChuanLiu Big Model at the 2023 China Mobile Worldwide Partner Conference. <div>
arXiv:2510.23662v1 Announce Type: new 
Abstract: As a window for urban sensing, human mobility contains rich spatiotemporal information that reflects both residents' behavior preferences and the functions of urban areas. The analysis of human mobility has attracted the attention of many researchers. However, existing methods often address specific tasks from a particular perspective, leading to insufficient modeling of human mobility and limited applicability of the learned knowledge in various downstream applications. To address these challenges, this paper proposes to push massive amounts of human mobility data into a spatiotemporal model, discover latent semantics behind mobility behavior and support various urban sensing tasks. Specifically, a large-scale and widely covering human mobility data is collected through the ubiquitous base station system and a framework named General-purpose and Dynamic Human Mobility Embedding (GDHME) for urban sensing is introduced. The framework follows the self-supervised learning idea and contains two major stages. In stage 1, GDHME treats people and regions as nodes within a dynamic graph, unifying human mobility data as people-region-time interactions. An encoder operating in continuous-time dynamically computes evolving node representations, capturing dynamic states for both people and regions. Moreover, an autoregressive self-supervised task is specially designed to guide the learning of the general-purpose node embeddings. In stage 2, these representations are utilized to support various tasks. To evaluate the effectiveness of our GDHME framework, we further construct a multi-task urban sensing benchmark. Offline experiments demonstrate GDHME's ability to automatically learn valuable node features from vast amounts of data. Furthermore, our framework is used to deploy the JiuTian ChuanLiu Big Model, a system that has been presented at the 2023 China Mobile Worldwide Partner Conference.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting sub-populations in online health communities: A mixed-methods exploration of breastfeeding messages in BabyCenter Birth Clubs</title>
<link>https://arxiv.org/abs/2510.23692</link>
<guid>https://arxiv.org/abs/2510.23692</guid>
<content:encoded><![CDATA[
<div> stress, parental, breastfeeding, anxiety, topics

Summary:
- The study examines parental stress through analyzing BabyCenter birth club users' posts.
- Parents seek advice and share experiences in various venues to alleviate stress.
- The analysis covers 5.43 million posts from 331,843 BabyCenter users.
- Anxiety-related terms in posts steadily increased from April 2017 to January 2024.
- Breastfeeding topics, particularly on sleep and work/daycare, were dominant among users compared to other birth club content. 

<br /><br />Summary: <div>
arXiv:2510.23692v1 Announce Type: new 
Abstract: Parental stress is a nationwide health crisis according to the U.S. Surgeon General's 2024 advisory. To allay stress, expecting parents seek advice and share experiences in a variety of venues, from in-person birth education classes and parenting groups to virtual communities, for example, BabyCenter, a moderated online forum community with over 4 million members in the United States alone. In this study, we aim to understand how parents talk about pregnancy, birth, and parenting by analyzing 5.43M posts and comments from the April 2017--January 2024 cohort of 331,843 BabyCenter "birth club" users (that is, users who participate in due date forums or "birth clubs" based on their babies' due dates). Using BERTopic to locate breastfeeding threads and LDA to summarize themes, we compare documents in breastfeeding threads to all other birth-club content. Analyzing time series of word rank, we find that posts and comments containing anxiety-related terms increased steadily from April 2017 to January 2024. We used an ensemble of topic models to identify dominant breastfeeding topics within birth clubs, and then explored trends among all user content versus those who posted in threads related to breastfeeding topics. We conducted Latent Dirichlet Allocation (LDA) topic modeling to identify the most common topics in the full population, as well as within the subset breastfeeding population. We find that the topic of sleep dominates in content generated by the breastfeeding population, as well anxiety-related and work/daycare topics that are not predominant in the full BabyCenter birth club dataset.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the influence of social media feedback on traveler's future trip-planning behavior: A multi-model machine learning approach</title>
<link>https://arxiv.org/abs/2510.24077</link>
<guid>https://arxiv.org/abs/2510.24077</guid>
<content:encoded><![CDATA[
<div> social return, social media, predictive model, Indian tourists, future trip planning

Summary:<br />
This paper explores how the social return, measured through social media responses, of recent trips influences the future travel decisions of Indian tourists. A multi-model framework was developed to create a predictive machine learning model that considers social media usage, trip-related factors, and future trip-planning behavior. Data collected from a survey of Indian tourists was cleaned and analyzed using oversampling and Monte Carlo cross-validation techniques. The results indicate a 75% accuracy in predicting the impact of social return on future trip plans. The findings have significant implications for the domestic tourism sector in India and suggest directions for future research on social media, destination marketing, smart tourism, and heritage tourism.<br />Summary: <div>
arXiv:2510.24077v1 Announce Type: new 
Abstract: With the surge of domestic tourism in India and the influence of social media on young tourists, this paper aims to address the research question on how "social return" - responses received on social media sharing - of recent trip details can influence decision-making for short-term future travels. The paper develops a multi-model framework to build a predictive machine learning model that establishes a relationship between a traveler's social return, various social media usage, trip-related factors, and her future trip-planning behavior. The primary data was collected via a survey from Indian tourists. After data cleaning, the imbalance in the data was addressed using a robust oversampling method, and the reliability of the predictive model was ensured by applying a Monte Carlo cross-validation technique. The results suggest at least 75% overall accuracy in predicting the influence of social return on changing the future trip plan. Moreover, the model fit results provide crucial practical implications for the domestic tourism sector in India with future research directions concerning social media, destination marketing, smart tourism, heritage tourism, etc.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social Simulation</title>
<link>https://arxiv.org/abs/2510.24251</link>
<guid>https://arxiv.org/abs/2510.24251</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, social graphs, Graphia, reinforcement learning, dynamic graph generation

Summary:
Graphia is a framework that utilizes social graph data as supervision for training large language models (LLMs) through reinforcement learning. It focuses on predicting interactions and generating edges in social networks. The framework is evaluated in two settings: Transductive Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG), showing significant improvements in alignment metrics for destination selection, edge generation, and network properties replication. Graphia also supports counterfactual simulation to simulate behavioral shifts under different incentives. The results demonstrate that social graphs can effectively guide LLM-based simulations, bridging the gap between individual agent behaviors and network dynamics. The code for Graphia is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2510.24251v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in simulating human-like social behaviors. Social graphs provide high-quality supervision signals that encode both local interactions and global network structure, yet they remain underutilized for LLM training. To address this gap, we propose Graphia, the first general LLM-based social graph simulation framework that leverages graph data as supervision for LLM post-training via reinforcement learning. With GNN-based structural rewards, Graphia trains specialized agents to predict whom to interact with (destination selection) and how to interact (edge generation), followed by designed graph generation pipelines. We evaluate Graphia under two settings: Transductive Dynamic Graph Generation (TDGG), a micro-level task with our proposed node-wise interaction alignment metrics; and Inductive Dynamic Graph Generation (IDGG), a macro-level task with our proposed metrics for aligning emergent network properties. On three real-world networks, Graphia improves micro-level alignment by 6.1% in the composite destination selection score, 12% in edge classification accuracy, and 27.9% in edge content BERTScore over the strongest baseline. For macro-level alignment, it achieves 41.11% higher structural similarity and 32.98% better replication of social phenomena such as power laws and echo chambers. Graphia also supports counterfactual simulation, generating plausible behavioral shifts under platform incentives. Our results show that social graphs can serve as high-quality supervision signals for LLM post-training, closing the gap between agent behaviors and network dynamics for LLM-based simulation. Code is available at https://github.com/Ji-Cather/Graphia.git.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rewarding Engagement and Personalization in Popularity-Based Rankings Amplifies Extremism and Polarization</title>
<link>https://arxiv.org/abs/2510.24354</link>
<guid>https://arxiv.org/abs/2510.24354</guid>
<content:encoded><![CDATA[
<div> Ranking algorithms, extremism, polarization, online platforms, mechanism <br />
Summary: 
- The article explores how ranking algorithms on online platforms can amplify extremism and polarization.
- Users are more likely to engage with items displayed higher in the ranking, leading to a preference for like-minded content.
- Individuals with more extreme views tend to be more active on these platforms.
- Popular items receive higher rankings, further reinforcing extremist content consumption.
- The study formalizes this mechanism in a dynamical model, validated through simulations and experiments with human participants. <br /><br />Summary: <div>
arXiv:2510.24354v1 Announce Type: new 
Abstract: Despite extensive research, the mechanisms through which online platforms shape extremism and polarization remain poorly understood. We identify and test a mechanism, grounded in empirical evidence, that explains how ranking algorithms can amplify both phenomena. This mechanism is based on well-documented assumptions: (i) users exhibit position bias and tend to prefer items displayed higher in the ranking, (ii) users prefer like-minded content, (iii) users with more extreme views are more likely to engage actively, and (iv) ranking algorithms are popularity-based, assigning higher positions to items that attract more clicks. Under these conditions, when platforms additionally reward \emph{active} engagement and implement \emph{personalized} rankings, users are inevitably driven toward more extremist and polarized news consumption. We formalize this mechanism in a dynamical model, which we evaluate by means of simulations and interactive experiments with hundreds of human participants, where the rankings are updated dynamically in response to user activity.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Importance of Overlapping Network Nodes in Influence Spreading</title>
<link>https://arxiv.org/abs/2510.24360</link>
<guid>https://arxiv.org/abs/2510.24360</guid>
<content:encoded><![CDATA[
<div> circle structures, influence spreading, overlapping nodes, network analysis, complex contagion

Summary:
- The study focuses on analyzing networks with circle structures and the role of overlapping nodes in influence spreading processes.
- Three node metrics are used to quantify the roles of nodes: In-Centrality, Out-Centrality, and Betweenness Centrality.
- Overlapping nodes consistently exhibit greater influence than non-overlapping nodes at each stage of the spreading process.
- The criteria used to define circles shape the overlapping effects, highlighting the strategic importance of overlapping nodes in spreading dynamics.
- Largest circles not only reflect node-level attributes but also topological importance, distinguishing between local attribute-driven circles and global community structures.

<br /><br />Summary: <div>
arXiv:2510.24360v1 Announce Type: new 
Abstract: In complex networks there are overlapping substructures or "circles" that consist of nodes belonging to multiple cohesive subgroups. Yet the role of these overlapping nodes in influence spreading processes remains underexplored. In the present study, we analyse networks with circle structures using a probabilistic influence spreading model for processes of simple and complex contagion. We quantify the roles of nodes using three metrics, i.e., In-Centrality, Out-Centrality, and Betweenness Centrality that represent the susceptibility, spreading power, and mediatory role of nodes, respectively, and find that at each stage of the spreading process the overlapping nodes consistently exhibit greater influence than the non-overlapping ones. Furthermore, we observe that the criteria to define circles shape the overlapping effects. When we restrict our analysis to only largest circles, we find that circles reflect not only node-level attributes but also of topological importance. These findings clarify the distinction between local attribute-driven circles and global community structures, thus highlighting the strategic importanc of overlapping nodes in spreading dynamics. This provides foundation for future research on overlapping nodes in both circles and communities.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Emergent Topological Properties in Socio-Economic Networks through Learning Heterogeneity</title>
<link>https://arxiv.org/abs/2510.24107</link>
<guid>https://arxiv.org/abs/2510.24107</guid>
<content:encoded><![CDATA[
<div> Keywords: individualized learning rates, structural dynamics, emergent phenomena, network architecture, socioeconomic networks 

Summary: 
This paper introduces a dual-learning framework that considers individualized learning rates for agents and a network rewiring rate, reflecting real-world cognitive diversity and structural adaptability. Through a simulation model based on the Prisoner's Dilemma and Quantal Response Equilibrium, the study examines how variations in learning rates impact the formation of large-scale network structures. Lower and more evenly distributed learning rates tend to promote scale-free networks, while higher or more diverse learning rates result in core-periphery topologies. Analysis of topological metrics such as scale-free exponents, Estrada heterogeneity, and assortativity demonstrates that the speed and variability of learning play a crucial role in shaping system rationality and network architecture. This research provides a comprehensive framework for exploring how individual learnability and structural adaptability influence the development of diverse topologies in socioeconomic networks, offering valuable insights into adaptive behavior, systemic organization, and resilience. 

<br /><br />Summary: <div>
arXiv:2510.24107v1 Announce Type: cross 
Abstract: Understanding how individual learning behavior and structural dynamics interact is essential to modeling emergent phenomena in socioeconomic networks. While bounded rationality and network adaptation have been widely studied, the role of heterogeneous learning rates both at the agent and network levels remains under explored. This paper introduces a dual-learning framework that integrates individualized learning rates for agents and a rewiring rate for the network, reflecting real-world cognitive diversity and structural adaptability.
  Using a simulation model based on the Prisoner's Dilemma and Quantal Response Equilibrium, we analyze how variations in these learning rates affect the emergence of large-scale network structures. Results show that lower and more homogeneously distributed learning rates promote scale-free networks, while higher or more heterogeneously distributed learning rates lead to the emergence of core-periphery topologies. Key topological metrics including scale-free exponents, Estrada heterogeneity, and assortativity reveal that both the speed and variability of learning critically shape system rationality and network architecture. This work provides a unified framework for examining how individual learnability and structural adaptability drive the formation of socioeconomic networks with diverse topologies, offering new insights into adaptive behavior, systemic organization, and resilience.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research</title>
<link>https://arxiv.org/abs/2510.24337</link>
<guid>https://arxiv.org/abs/2510.24337</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Large Language Models, content analysis, communication research, automation, best practices

Summary: 
Generative Large Language Models (gLLMs) like ChatGPT are proving to be valuable tools in communication research, surpassing human coders in coding tasks and offering efficiency and cost-effectiveness. However, integrating gLLMs into quantitative content analysis poses several challenges that researchers must address. These include developing a suitable codebook, creating effective prompts, selecting the appropriate model, fine-tuning parameters, refining iteratively, and validating reliability. Optional steps for enhancing performance may also be necessary. This paper seeks to bridge the gap by providing a best-practice guide to help communication researchers navigate these challenges and ensure the quality standards of validity, reliability, reproducibility, and research ethics are met. The goal is to make gLLM-based content analysis more accessible to a wider audience within the communication research realm. 

<br /><br />Summary: <div>
arXiv:2510.24337v1 Announce Type: cross 
Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly being used in communication research for content analysis. Studies show that gLLMs can outperform both crowd workers and trained coders, such as research assistants, on various coding tasks relevant to communication science, often at a fraction of the time and cost. Additionally, gLLMs can decode implicit meanings and contextual information, be instructed using natural language, deployed with only basic programming skills, and require little to no annotated data beyond a validation dataset - constituting a paradigm shift in automated content analysis. Despite their potential, the integration of gLLMs into the methodological toolkit of communication research remains underdeveloped. In gLLM-assisted quantitative content analysis, researchers must address at least seven critical challenges that impact result quality: (1) codebook development, (2) prompt engineering, (3) model selection, (4) parameter tuning, (5) iterative refinement, (6) validation of the model's reliability, and optionally, (7) performance enhancement. This paper synthesizes emerging research on gLLM-assisted quantitative content analysis and proposes a comprehensive best-practice guide to navigate these challenges. Our goal is to make gLLM-based content analysis more accessible to a broader range of communication researchers and ensure adherence to established disciplinary quality standards of validity, reliability, reproducibility, and research ethics.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pair Approximation Meets Reality: Diffusion of Innovation in Organizational Networks within the biased-independence q-Voter Model</title>
<link>https://arxiv.org/abs/2510.24447</link>
<guid>https://arxiv.org/abs/2510.24447</guid>
<content:encoded><![CDATA[
<div> Keywords: collective adaptation, biased-independence $q$-voter model, agent-based modeling, social influence, organizational networks

Summary: 
The study introduces the biased-independence $q$-voter model, a generalization of the $q$-voter model, to analyze collective adaptation processes influenced by individual decisions and social factors. The model incorporates conformity and independent choice parameters, with the engagement parameter breaking the symmetry between two options. The pair approximation (PA) for an asymmetric $q$-voter model is developed and validated on empirical organizational networks. The results demonstrate discontinuous phase transitions and irreversible hysteresis in adoption dynamics, reflecting path-dependent behaviors. Surprisingly, the PA performs well in predicting outcomes on empirical networks, showcasing its potential as a computationally efficient tool for exploring decision-making processes and collective actions. <div>
arXiv:2510.24447v1 Announce Type: cross 
Abstract: Collective adaptation, whether in innovation adoption, pro-environmental or organizational change, emerges from the interplay between individual decisions and social influence. Agent-based modeling provides a useful tool for studying such processes. Here, we introduce the biased-independence $q$-voter model, a generalization of the $q$-voter model with independence, one of the most popular agent-based models of opinion dynamics. In our model, individuals choose between two options, adopt or not adopt, under the competing influences of conformity and independent choice. Independent choice between two options is determined by an engagement parameter, inspired by earlier agent-based model of eco-innovation diffusion. When the engagement parameter equals $0.5$, the model reduces to the original $q$-voter model with independence; values different from $0.5$ break the symmetry between the two options. To place our study in a broader context, we briefly review asymmetric versions of the $q$-voter model proposed to date. The novelty of this work goes beyond introducing a generalized model: we develop the pair approximation (PA) for an asymmetric $q$-voter model and, for the first time, validate it on empirical organizational networks. Our results show that the interplay of social influence, independence, and option preference generates discontinuous phase transitions and irreversible hysteresis, reflecting path-dependent adoption dynamics. Surprisingly, the PA agrees well with Monte Carlo simulations on some empirical networks, even small ones, highlighting its potential as a computationally efficient bridge between individual decision-making and collective actions.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNN-based Anchor Embedding for Efficient Exact Subgraph Matching</title>
<link>https://arxiv.org/abs/2502.00031</link>
<guid>https://arxiv.org/abs/2502.00031</guid>
<content:encoded><![CDATA[
<div> Keywords: subgraph matching, deep learning, graph neural network, exact matching, query efficiency

Summary:
The article introduces a novel approach called the Graph Neural Network-based Anchor Embedding Framework (GNN-AE) for exact subgraph matching queries. Unlike traditional methods that rely on online creation of auxiliary structures for each query, GNN-AE indexes small feature subgraphs in the data graph offline. This approach utilizes GNNs to efficiently retrieve high-quality candidates for graph isomorphism tests. By leveraging anchored subgraphs and anchored paths, the exact subgraph matching problem is transformed into a search problem in the embedding space. Additionally, a parallel matching growth algorithm and a cost-based DFS query planning method are developed to enhance query efficiency. Experimental results on real-world and synthetic datasets demonstrate the superiority of GNN-AE over baseline methods, achieving up to 1-2 orders of magnitude improvement in efficiency, particularly surpassing exploration-based baselines.<br /><br />Summary: <div>
arXiv:2502.00031v5 Announce Type: replace 
Abstract: Subgraph matching query is a fundamental problem in graph data management and has a variety of real-world applications. Several recent works utilize deep learning (DL) techniques to process subgraph matching queries. Most of them find approximate subgraph matching results without accuracy guarantees. Unlike these DL-based inexact subgraph matching methods, we propose a learning-based exact subgraph matching framework, called \textit{graph neural network (GNN)-based anchor embedding framework} (GNN-AE). In contrast to traditional exact subgraph matching methods that rely on creating auxiliary summary structures online for each specific query, our method indexes small feature subgraphs in the data graph offline and uses GNNs to perform graph isomorphism tests for these indexed feature subgraphs to efficiently obtain high-quality candidates. To make a tradeoff between query efficiency and index storage cost, we use two types of feature subgraphs, namely anchored subgraphs and anchored paths. Based on the proposed techniques, we transform the exact subgraph matching problem into a search problem in the embedding space. Furthermore, to efficiently retrieve all matches, we develop a parallel matching growth algorithm and design a cost-based DFS query planning method to further improve the matching growth algorithm. Extensive experiments on 6 real-world and 3 synthetic datasets indicate that GNN-AE is more efficient than the baselines, especially outperforming the exploration-based baseline methods by up to 1--2 orders of magnitude.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heaven &amp; Hell II: Scale Laws and Robustness in One-Step Heaven-Hell Consensus</title>
<link>https://arxiv.org/abs/2510.21950</link>
<guid>https://arxiv.org/abs/2510.21950</guid>
<content:encoded><![CDATA[
<div> Conservation-law, scale laws, tie policies, tight bounds, asynchronous updates <br />
Summary: The study focuses on Heaven-Hell dynamics, a model for network consensus, particularly examining systems with a single uniform hub. They establish a one-step convergence threshold based on the per-node inbound hub weight, providing robustness to tie-breaking policies, node-specific tolerances, targeted seeding, multiple hubs, and asynchronous updates. The research introduces scale laws and operational refinements to ensure the threshold's reliability across various scenarios. Key contributions include a conservation-law perspective, parameterized tie policies, tighter pointwise bounds, one-pass fairness for asynchronous updates, and sufficient conditions for seeded convergence. All proofs are formalized in Coq, with experiments on different network topologies confirming the accuracy and effectiveness of the proposed refinements. <br /><br />Summary: <div>
arXiv:2510.21950v1 Announce Type: new 
Abstract: We study Heaven-Hell dynamics, a model for network consensus. A known result establishes an exact one-step convergence threshold for systems with a single uniform hub: the per-node inbound hub weight W suffices if and only if W >= maxrest, the maximum non-hub inbound mass. We develop scale laws and operational refinements that make this threshold robust to tie-breaking policies, node-specific tolerances, targeted seeding, multiple hubs, and asynchronous updates. Our contributions include a conservation-law perspective, parameterized tie policies, tighter pointwise bounds improving on classical worst-case guarantees, one-pass fairness for asynchronous updates, and sufficient conditions for seeded convergence. All proofs are mechanized in Coq, with experiments on rings, grids, scale-free graphs, and heterogeneous weighted graphs validating tightness and gap closures
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Social Division to Cohesion with AI Message Suggestions in Online Chat Groups</title>
<link>https://arxiv.org/abs/2510.21984</link>
<guid>https://arxiv.org/abs/2510.21984</guid>
<content:encoded><![CDATA[
<div> experiment, online communication, large language models, social cohesion, AI assistance
Summary: 
An online experiment with 557 participants explored the impact of AI-driven messaging assistance on social cohesion in online discussions. The study found that subtle linguistic style shifts mediated by AI assistance can influence the collective structures of discussion groups. Personalized AI assistance led users to segregate into like-minded groups, while group-focused assistance incorporating group members' viewpoints promoted cohesion through more receptive exchanges. The findings highlight the potential for AI-mediated communication to support social cohesion in diverse groups, emphasizing the importance of thoughtful personalization design. <div>
arXiv:2510.21984v1 Announce Type: new 
Abstract: Social cohesion is difficult to sustain in societies marked by opinion diversity, particularly in online communication. As large language model (LLM)-driven messaging assistance becomes increasingly embedded in these contexts, it raises critical questions about its societal impact. We present an online experiment with 557 participants who engaged in multi-round discussions on politically controversial topics while freely reconfiguring their discussion groups. In some conditions, participants received real-time message suggestions generated by an LLM, either personalized to the individual or adapted to their group context. We find that subtle shifts in linguistic style during communication, mediated by AI assistance, can scale up to reshape collective structures. While individual-focused assistance leads users to segregate into like-minded groups, relational assistance that incorporates group members' stances enhances cohesion through more receptive exchanges. These findings demonstrate that AI-mediated communication can support social cohesion in diverse groups, but outcomes critically depend on how personalization is designed.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Platform Short-Video Diplomacy: Topic and Sentiment Analysis of China-US Relations on Douyin and TikTok</title>
<link>https://arxiv.org/abs/2510.22415</link>
<guid>https://arxiv.org/abs/2510.22415</guid>
<content:encoded><![CDATA[
<div> Keywords: China-U.S. relations, social media, sentiment analysis, regulation, regional factors

Summary:
This study explores discussions about China-U.S. relations on the social media platforms Douyin and TikTok, owned by ByteDance, in China and the US. Through analyzing 4,040 videos and 338,209 user comments, key themes such as economic strength, technological interdependence, cultural values, and responses to global challenges were identified. Emotional differences between the two countries were evident in these discussions. The Chinese government's new regulation requiring social media accounts to disclose geolocation information led to an investigation of changes in sentiment towards the US in mainland China. By linking socioeconomic factors such as GDP per capita, minority index, and internet penetration rate with online discussions, this study sheds light on how regional and economic factors influence Chinese views of the US. These findings are valuable for understanding China-U.S. relations and informing policy decisions.<br /><br />Summary: <div>
arXiv:2510.22415v1 Announce Type: new 
Abstract: We examine discussions surrounding China-U.S. relations on the Chinese and American social media platforms \textit{Douyin} and \textit{TikTok}. Both platforms, owned by \textit{ByteDance}, operate under different regulatory and cultural environments, providing a unique perspective for analyzing China-U.S. public discourse. This study analyzed 4,040 videos and 338,209 user comments to assess the public discussions and sentiments on social media regarding China-U.S. relations. Through topic clustering and sentiment analysis, we identified key themes, including economic strength, technological and industrial interdependence, cultural cognition and value pursuits, and responses to global challenges. There are significant emotional differences between China and the US on various themes. Since April 2022, the Chinese government has implemented a new regulation requiring all social media accounts to disclose their provincial-level geolocation information. Utilizing this publicly available data, along with factors such as GDP per capita, minority index, and internet penetration rate, we investigate the changes in sentiment towards the U.S. in mainland China. This study links socioeconomic indicators with online discussions, deeply analyzing how regional and economic factors influence Chinese comments on their views of the US, providing important insights for China-U.S. relationship research and policy making.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Discrete-time Model of Information Diffusion on Social Networks Considering Users Behavior</title>
<link>https://arxiv.org/abs/2510.22501</link>
<guid>https://arxiv.org/abs/2510.22501</guid>
<content:encoded><![CDATA[
<div> SDIR model, online social networks, user behavior, mean-field approximation, edge-deletion problem
<br />
Summary:
In this paper, the SDIR model is introduced as an extension of the classic SIR epidemic framework for online social networks, explicitly capturing user behavior. The model includes a new state, D, representing delayed information spread. Dynamical equations are derived using mean-field approximation, and convergence and stability conditions are studied. An approximation algorithm for the edge-deletion problem is proposed to minimize information diffusion impact by identifying approximate solutions. The SDIR model offers a comprehensive framework for analyzing information spread dynamics in online social networks, considering user interactions and delay effects. <div>
arXiv:2510.22501v1 Announce Type: new 
Abstract: In this paper, we introduce the SDIR (Susceptible-Delayable-Infected-Recovered) model, an extension of the classical SIR epidemic framework, to provide a more explicit characterization of user behavior in online social networks. The newly merged state D (delayable) represents users who have received the information but delayed its spreading and may eventually choose not to share it at all. Based on the mean-field approximation method, we derive the dynamical equations of the model and investigate its convergence and stability conditions. Under these conditions, we further propose an approximation algorithm for the edge-deletion problem, aiming to minimize the influence of information diffusion by identifying approximate solutions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence of Network Topology and Vaccination Strategies on HPV Dynamics: A Simulation Study Using the SeCoNet Growth Model</title>
<link>https://arxiv.org/abs/2510.22644</link>
<guid>https://arxiv.org/abs/2510.22644</guid>
<content:encoded><![CDATA[
<div> Age-based; ring-based; centrality-based; vaccination strategies; human papillomavirus

Summary:
Degree, betweenness, and percolation centrality-based vaccination strategies are most effective in reducing peak incidence of human papillomavirus. Ring vaccination is most successful in lowering cumulative incidence among females. Network topology, including average degree, power-law exponent, average shortest path length, and clustering, significantly influences vaccination effectiveness. Higher average degree hinders vaccination outcomes, while a higher power-law exponent, longer average shortest path length, and stronger clustering improve results. Incorporating network structure into HPV vaccination program design is crucial for achieving optimal outcomes. 

<br /><br />Summary: <div>
arXiv:2510.22644v1 Announce Type: new 
Abstract: This study examines how contact network topology influences the effectiveness of vaccination programs in the context of human papillomavirus (HPV) transmission. Using the SeCoNet sexual contact network growth model, we evaluate age based, ring based, and several centrality based vaccination strategies across the overall, male, and female cohorts, focusing on peak incidence, timing of peak prevalence, and cumulative incidence. The simulations show that degree, betweenness, and percolation centrality based strategies are generally the most effective, while ring vaccination achieves the greatest reduction in cumulative incidence among females. Network topology also plays a critical role: higher average degree reduces vaccination effectiveness, whereas higher power-law exponent, longer average shortest path length, and stronger clustering improve vaccination outcomes. The results highlight the importance of incorporating network structure into the design of HPV vaccination programs.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Search in Attributed Networks using Dominance Relationships and Random Walks</title>
<link>https://arxiv.org/abs/2510.22850</link>
<guid>https://arxiv.org/abs/2510.22850</guid>
<content:encoded><![CDATA[
<div> algorithm, community search, attributed networks, structural connectivity, attribute similarity

Summary:
- The paper introduces a novel algorithm that combines hop-based and random-walk-based methods to find high-quality communities in attributed networks.
- It uses the domination score to measure the influence of nodes based on their attributes and incorporates $k$-core extraction for strong structural cohesion within communities.
- By considering both network structure and node attributes, the algorithm identifies cohesive communities with meaningful attribute similarities.
- The algorithm was tested on real-world datasets and proved effective in efficiently identifying well-connected communities.
- It is suitable for applications such as social network analysis and recommendation systems. 

<br /><br />Summary: <div>
arXiv:2510.22850v1 Announce Type: new 
Abstract: Community search in attributed networks poses a dual challenge: balancing structural connectivity -- the network's topological properties -- and attribute similarity -- the shared characteristics of nodes. This paper introduces a novel algorithm that integrates hop-based and random-walk-based methods to identify high-quality communities, effectively addressing this balance. Our approach employs the concept of the domination score to quantify the influence of nodes based on their attributes, followed by $k$-core extraction to ensure strong structural cohesion within the communities. By considering both the network structure and node attributes, the algorithm identifies communities that are not only well-connected, but also share meaningful attribute similarities. We evaluated the algorithm on large real-world datasets, demonstrating its ability to efficiently identify cohesive communities, making it suitable for applications such as social network analysis and recommendation systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Political Discourse with Sentence-BERT and BERTopic</title>
<link>https://arxiv.org/abs/2510.22904</link>
<guid>https://arxiv.org/abs/2510.22904</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, political discourse, topic evolution, Moral Foundations Theory, Twitter activity<br />
Summary: <br />
This study explores the impact of social media on political discourse by analyzing Twitter activity during the 117th U.S. Congress. The research introduces a unique framework that combines BERTopic-based topic modeling with Moral Foundations Theory to track the evolution and moral dimensions of political topics over time. The study finds that while overarching themes remain stable, more specific topics tend to fade quickly, limiting their long-term influence. The analysis also shows that moral values, particularly Care and Loyalty, play a crucial role in the persistence of topics, with partisan differences leading to varied moral framing strategies. Overall, this work offers a scalable and interpretable approach to understanding how moral values drive topic evolution on social media, contributing to the field of social network analysis and computational political discourse. <br /> <div>
arXiv:2510.22904v1 Announce Type: new 
Abstract: Social media has reshaped political discourse, offering politicians a platform for direct engagement while reinforcing polarization and ideological divides. This study introduces a novel topic evolution framework that integrates BERTopic-based topic modeling with Moral Foundations Theory (MFT) to analyze the longevity and moral dimensions of political topics in Twitter activity during the 117th U.S. Congress. We propose a methodology for tracking dynamic topic shifts over time and measuring their association with moral values and quantifying topic persistence. Our findings reveal that while overarching themes remain stable, granular topics tend to dissolve rapidly, limiting their long-term influence. Moreover, moral foundations play a critical role in topic longevity, with Care and Loyalty dominating durable topics, while partisan differences manifest in distinct moral framing strategies. This work contributes to the field of social network analysis and computational political discourse by offering a scalable, interpretable approach to understanding moral-driven topic evolution on social media.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies</title>
<link>https://arxiv.org/abs/2510.22017</link>
<guid>https://arxiv.org/abs/2510.22017</guid>
<content:encoded><![CDATA[
<div> trust-aware RL algorithm, resource allocation, institutional trust, fairness, humanitarian engineering

Summary: 
- The study focuses on the development of a trust-aware RL algorithm for resource allocation in communities, with a focus on humanitarian engineering.
- The research explores how incorporating institutional trust into RL algorithms can lead to more successful policies, especially when organizational goals are uncertain.
- Results show that more conservative trust estimates result in increased fairness and average community trust, albeit at the cost of organization success.
- An intervention strategy is proposed to prevent unfair outcomes by implementing a quota system to ensure the organization serves enough community members, leading to improved fairness and trust in some cases, but at the expense of organization success.
- The work highlights the significance of institutional trust in algorithm design and implementation, and uncovers a trade-off between organization success and community well-being. 

<br /><br />Summary: <div>
arXiv:2510.22017v1 Announce Type: cross 
Abstract: Many governmental bodies are adopting AI policies for decision-making. In particular, Reinforcement Learning has been used to design policies that citizens would be expected to follow if implemented. Much RL work assumes that citizens follow these policies, and evaluate them with this in mind. However, we know from prior work that without institutional trust, citizens will not follow policies put in place by governments. In this work, we develop a trust-aware RL algorithm for resource allocation in communities. We consider the case of humanitarian engineering, where the organization is aiming to distribute some technology or resource to community members. We use a Deep Deterministic Policy Gradient approach to learn a resource allocation that fits the needs of the organization. Then, we simulate resource allocation according to the learned policy, and model the changes in institutional trust of community members. We investigate how this incorporation of institutional trust affects outcomes, and ask how effectively an organization can learn policies if trust values are private. We find that incorporating trust into RL algorithms can lead to more successful policies, specifically when the organization's goals are less certain. We find more conservative trust estimates lead to increased fairness and average community trust, though organization success suffers. Finally, we explore a strategy to prevent unfair outcomes to communities. We implement a quota system by an external entity which decreases the organization's utility when it does not serve enough community members. We find this intervention can improve fairness and trust among communities in some cases, while decreasing the success of the organization. This work underscores the importance of institutional trust in algorithm design and implementation, and identifies a tension between organization success and community well-being.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Larger holes as narrower degree distributions in complex networks</title>
<link>https://arxiv.org/abs/2510.22720</link>
<guid>https://arxiv.org/abs/2510.22720</guid>
<content:encoded><![CDATA[
<div> loops, degree distributions, scale-free networks, connectivity, random networks  
Summary: 
The study explores the relationship between degree distributions and loop lengths in various networks, including scale-free networks, Erdos-Renyi random graphs, and regular networks. It is observed that networks with narrower degree distributions tend to have longer shortest loops, which is considered a universal property in random networks. This suggests that enhancing longer loops, specifically of O(log N) length, can improve the robustness of connectivity in networks. The findings indicate that increasing loop lengths can help decrease the variance of degree distributions and enhance connectivity resilience, particularly in scale-free networks containing shorter loops like triangles. This research contributes to understanding the structural properties of networks and how loop enhancement can impact network connectivity. <div>
arXiv:2510.22720v1 Announce Type: cross 
Abstract: Although the analysis of loops is not so much because of the complications, it has already been found that heuristically enhancing loops decreases the variance of degree distributions for improving the robustness of connectivity. While many real scale-free networks are known to contain shorter loops such as triangles, it remains to investigate the distributions of longer loops in more wide class of networks. We find a relation between narrower degree distributions and longer loops in investigating the lengths of the shortest loops in various networks with continuously changing degree distributions, including three typical types of scale-free networks, classical Erd\"os-R\'enyi random graphs, and regular networks. In particular, we show that narrower degree distributions contain longer shortest loops, as a universal property in a wide class of random networks. We suggest that the robustness of connectivity is enhanced by constructing long loops of O(log N).
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassmanian Interpolation of Low-Pass Graph Filters: Theory and Applications</title>
<link>https://arxiv.org/abs/2510.23235</link>
<guid>https://arxiv.org/abs/2510.23235</guid>
<content:encoded><![CDATA[
<div> Low-pass graph filters, signal processing, parametric graph families, Riemannian interpolation, normal coordinates<br />
Summary:<br />
The article proposes a new algorithm for interpolating low-pass graph filters using Riemannian interpolation on the Grassmann manifold, reducing computational costs for parametric graph families. An error bound estimate for subspace interpolation is derived, and two potential applications are discussed. Firstly, the temporal evolution of node features can be translated to evolving graph topology by adjusting network homophily. Secondly, a dot product graph family can be induced from a static graph to enhance message passing schemes for node classification through filter interpolation. <div>
arXiv:2510.23235v1 Announce Type: cross 
Abstract: Low-pass graph filters are fundamental for signal processing on graphs and other non-Euclidean domains. However, the computation of such filters for parametric graph families can be prohibitively expensive as computation of the corresponding low-frequency subspaces, requires the repeated solution of an eigenvalue problem. We suggest a novel algorithm of low-pass graph filter interpolation based on Riemannian interpolation in normal coordinates on the Grassmann manifold. We derive an error bound estimate for the subspace interpolation and suggest two possible applications for induced parametric graph families. First, we argue that the temporal evolution of the node features may be translated to the evolving graph topology via a similarity correction to adjust the homophily degree of the network. Second, we suggest a dot product graph family induced by a given static graph which allows to infer improved message passing scheme for node classification facilitated by the filter interpolation.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger together? The homophily trap in networks</title>
<link>https://arxiv.org/abs/2412.20158</link>
<guid>https://arxiv.org/abs/2412.20158</guid>
<content:encoded><![CDATA[
<div> homophily, diversity, inequalities, minority groups, network structure
Summary:
Homophily, the tendency to connect with similar others, can have both positive and negative effects on social networks. While it can foster a sense of belonging and shared values, it can also hinder diversity and exacerbate inequalities. In this study, the concept of homophily traps is introduced, where increased homophily among minorities can lead to reduced structural opportunities within a network. The research reveals that homophily traps occur when a minority group's size is below 25% of the network, resulting in lower visibility and opportunities for the minority. This highlights the importance of group size in determining the impact of homophily on structural outcomes in networks. By understanding these dynamics, we gain insights into the underlying processes that contribute to group inequality in social networks.<br /><br />Summary: <div>
arXiv:2412.20158v2 Announce Type: replace 
Abstract: While homophily -- the tendency to link with similar others -- may nurture a sense of belonging and shared values, it can also hinder diversity and widen inequalities. Here, we unravel this trade-off analytically, revealing homophily traps for minority groups: scenarios where increased homophilic interaction among minorities negatively affects their structural opportunities within a network. We demonstrate that homophily traps arise when minority size falls below 25% of a network, at which point homophily comes at the expense of lower structural visibility for the minority group. Our work reveals that social groups require a critical size to benefit from homophily without incurring structural costs, providing insights into core processes underlying the emergence of group inequality in networks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Dynamics on Signed Graphs and Graphons</title>
<link>https://arxiv.org/abs/2505.04472</link>
<guid>https://arxiv.org/abs/2505.04472</guid>
<content:encoded><![CDATA[
<div> Keywords: graphon theory, opinion dynamics, negative interactions, repelling model, opposing model

Summary:
In this paper, the authors utilize graphon theory to analyze opinion dynamics on large undirected networks. They focus on models that incorporate negative interactions among individuals, allowing for opinions to diverge. By considering repelling and opposing models of negative interactions, defined on signed graphons, the authors establish the existence and uniqueness of solutions to the initial value problem. The study demonstrates that the graphon dynamics serve as a reliable approximation for dynamics on large graphs converging to a graphon. The findings are applicable to large random graphs sampled according to a graphon (W-random graphs), supported by a novel convergence result under broad assumptions. <div>
arXiv:2505.04472v2 Announce Type: replace 
Abstract: In this paper, we make use of graphon theory to study opinion dynamics on large undirected networks. The opinion dynamics models that we take into consideration allow for negative interactions between the individuals, whose opinions can thus grow apart. We consider both the repelling and the opposing models of negative interactions, which have been studied in the literature. We define the repelling and the opposing dynamics on signed graphons and we show that their initial value problem solutions exist and are unique. We then show that, in a suitable sense, the graphon dynamics is a good approximation of the dynamics on large graphs that converge to a graphon. This result applies to large random graphs that are sampled according to a graphon (W-random graphs), for which we provide a new convergence result under very general assumptions.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Research Interest Similarity with Transition Probabilities</title>
<link>https://arxiv.org/abs/2409.18240</link>
<guid>https://arxiv.org/abs/2409.18240</guid>
<content:encoded><![CDATA[
<div> Keywords: paper similarity, author similarity, citation network, random walkers, classification tasks

Summary: 
The study introduces a novel family of measures for paper and author similarity based on the likelihood of papers being retrieved during literature searches following citations. This browsing process is modeled as a walk in a citation network, utilizing transition probabilities of random walkers. The proposed measures are continuous, symmetric, and applicable to any citation network. Validation tests compared these measures against existing alternatives, demonstrating the superiority of the basic transition probability measure in classifying papers and predicting future co-authors across various analysis scales. The study also explores leveraging publication-level data to approximate the research interest similarity among individual scientists. A Python package accompanying the paper implements all tested metrics.


<br /><br />Summary: <div>
arXiv:2409.18240v2 Announce Type: replace-cross 
Abstract: We introduce a family of paper and author similarity measures based on the concept that papers are more similar if they are more likely to be retrieved during a literature search following backward and forward citations. Since this browsing process resembles a walk in a citation network, we operationalize the concept using the transition probability (TP) of random walkers. The proposed measures are continuous, symmetric, and can be implemented on any citation network. We conduct validation tests of the TP concept and other extant alternatives to gauge which metric can classify papers and predict future co-authors most consistently across different scales of analysis (co-authorships, journals, and disciplines). Our results show that the proposed basic TP measure outperforms alternative metrics such as personalized PageRank and the Node2vec machine-learning technique in classification tasks at various scales. Additionally, we discuss how publication-level data can be leveraged to approximate the research interest similarity of individual scientists. This paper is accompanied by a Python package that implements all the tested metrics.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Approximate-Master-Equation Formulation of the Watts Threshold Model on Hypergraphs</title>
<link>https://arxiv.org/abs/2503.04020</link>
<guid>https://arxiv.org/abs/2503.04020</guid>
<content:encoded><![CDATA[
<div> Keywords: behavioral dynamics, opinion dynamics, social networks, hypergraphs, Watts threshold model <br />
Summary: 
The article explores behavioral and opinion dynamics on social networks, considering interactions in groups rather than just pairs. It extends the Watts threshold model (WTM) from graphs to hypergraphs, allowing interactions between any number of individuals. Using approximate master equations (AMEs), the model is accurately represented in continuous time. The high-dimensional system is reduced to three coupled differential equations for computational efficiency. A cascade condition is derived to predict large spreading events. The model is applied to empirical networks, a French primary school contact network, and a computer-science coauthorship hypergraph, showing accuracy in simulation. Future research incorporating structural correlations is suggested for improved real-world network modeling. <br /><br />Summary: <div>
arXiv:2503.04020v3 Announce Type: replace-cross 
Abstract: In traditional models of behavioral or opinion dynamics on social networks, researchers suppose that all interactions occur between pairs of individuals. However, in reality, social interactions also occur in groups of three or more individuals. A common way to incorporate such polyadic interactions is to study dynamical processes on hypergraphs. In a hypergraph, interactions can occur between any number of the individuals in a network. The Watts threshold model (WTM) is a well-known model of a simplistic social spreading process. Very recently, Chen et al. extended the WTM from dyadic networks (i.e., graphs) to polyadic networks (i.e., hypergraphs). In the present paper, we extend their discrete-time model to continuous time using approximate master equations (AMEs). By using AMEs, we are able to model the system with very high accuracy. We then reduce the high-dimensional AME system to a system of three coupled differential equations without any detectable loss of accuracy. This much lower-dimensional system is more computationally efficient to solve numerically and is also easier to interpret. We linearize the reduced AME system and calculate a cascade condition, which allows us to determine when a large spreading event occurs. We then apply our model to a social contact network of a French primary school and to a hypergraph of computer-science coauthorships. We find that the AME system is accurate in modelling the polyadic WTM on these empirical networks; however, we expect that future work that incorporates structural correlations between nearby nodes and groups into the model for the dynamics will lead to more accurate theory for real-world networks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Simulations with Large Language Model Risk Utopian Illusion</title>
<link>https://arxiv.org/abs/2510.21180</link>
<guid>https://arxiv.org/abs/2510.21180</guid>
<content:encoded><![CDATA[
<div> Keywords: simulation, human behavior, large language models (LLMs), social cognitive biases, social desirability bias

Summary: 
The study explores how large language models (LLMs) simulate human behavior in social contexts. Through chatroom-style interactions, the researchers analyzed LLMs across five linguistic dimensions to identify social cognitive biases. The findings reveal that LLMs do not accurately replicate authentic human behavior, exhibiting biases like social role bias, primacy effect, and positivity bias. These biases lead to the creation of "Utopian" societies that lack the complexity and variability of real human interactions. The study highlights the importance of developing socially grounded LLMs that can capture the diverse nature of human social behavior. This systematic framework provides a valuable tool for assessing the behavior of LLMs in social simulations and emphasizes the need for more nuanced representations of human behavior in artificial intelligence models. 

<br /><br />Summary: <div>
arXiv:2510.21180v1 Announce Type: cross 
Abstract: Reliable simulation of human behavior is essential for explaining, predicting, and intervening in our society. Recent advances in large language models (LLMs) have shown promise in emulating human behaviors, interactions, and decision-making, offering a powerful new lens for social science studies. However, the extent to which LLMs diverge from authentic human behavior in social contexts remains underexplored, posing risks of misinterpretation in scientific studies and unintended consequences in real-world applications. Here, we introduce a systematic framework for analyzing LLMs' behavior in social simulation. Our approach simulates multi-agent interactions through chatroom-style conversations and analyzes them across five linguistic dimensions, providing a simple yet effective method to examine emergent social cognitive biases. We conduct extensive experiments involving eight representative LLMs across three families. Our findings reveal that LLMs do not faithfully reproduce genuine human behavior but instead reflect overly idealized versions of it, shaped by the social desirability bias. In particular, LLMs show social role bias, primacy effect, and positivity bias, resulting in "Utopian" societies that lack the complexity and variability of real human interactions. These findings call for more socially grounded LLMs that capture the diversity of human social behavior.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shift Bribery over Social Networks</title>
<link>https://arxiv.org/abs/2510.21200</link>
<guid>https://arxiv.org/abs/2510.21200</guid>
<content:encoded><![CDATA[
<div> bribery, social influence, networks, complexity landscape, shift strategy<br />
Summary:<br />
In shift bribery over networks, a briber aims to influence voters in a social network to promote their preferred candidate by paying them to change their rankings. This study considers the impact of social influence within the network, where bribed voters can influence their neighbors, amplifying persuasion effects. The problem complexity is shown to be challenging, being NP-complete and W[2]-hard under various parameters. However, some positive results include algorithms for specific network structures such as complete graphs, path graphs, transitive tournaments, and cluster graphs. Additionally, fixed-parameter tractable algorithms are developed based on treewidth and cluster vertex deletion number parameters. Overall, this research provides a comprehensive analysis of the complexity landscape of shift bribery in social networks, offering insights into the feasibility and challenges of influencing voters through network propagation. <br /> <div>
arXiv:2510.21200v1 Announce Type: cross 
Abstract: In shift bribery, a briber seeks to promote his preferred candidate by paying voters to raise their ranking. Classical models of shift bribery assume voters act independently, overlooking the role of social influence. However, in reality, individuals are social beings and are often represented as part of a social network, where bribed voters may influence their neighbors, thereby amplifying the effect of persuasion. We study Shift bribery over Networks, where voters are modeled as nodes in a directed weighted graph, and arcs represent social influence between them. In this setting, bribery is not confined to directly targeted voters its effects can propagate through the network, influencing neighbors and amplifying persuasion. Given a budget and individual cost functions for shifting each voter's preference toward a designated candidate, the goal is to determine whether a shift strategy exists within budget that ensures the preferred candidate wins after both direct and network-propagated influence takes effect. We show that the problem is NP-Complete even with two candidates and unit costs, and W[2]-hard when parameterized by budget or maximum degree. On the positive side, we design polynomial-time algorithms for complete graphs under plurality and majority rules and path graphs for uniform edge weights, linear-time algorithms for transitive tournaments for two candidates, linear cost functions and uniform arc weights, and pseudo-polynomial algorithms for cluster graphs. We further prove the existence of fixed-parameter tractable algorithms with treewidth as parameter for two candidates, linear cost functions and uniform arc weights and pseudo-FPT with cluster vertex deletion number for two candidates and uniform arc weights. Together, these results give a detailed complexity landscape for shift bribery in social networks.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World-POI: Global Point-of-Interest Data Enriched from Foursquare and OpenStreetMap as Tabular and Graph Data</title>
<link>https://arxiv.org/abs/2510.21342</link>
<guid>https://arxiv.org/abs/2510.21342</guid>
<content:encoded><![CDATA[
<div> dataset, points of interest, Foursquare, OpenStreetMap, metadata
Summary: 
This data paper presents a methodology for integrating Foursquare and OpenStreetMap datasets to create a comprehensive dataset of points of interest (POIs). The Foursquare dataset provides a baseline of commercial POIs, while the OpenStreetMap dataset offers enriched metadata. By combining these datasets, a 1 TB dataset is created, with filtered releases available for practical use. Record linkage is achieved through name similarity scores and spatial distances, allowing for the identification of high-confidence matches representing real businesses. A graph-based representation of POIs is constructed using this filtered dataset, enabling advanced spatial analyses and a variety of downstream applications. <div>
arXiv:2510.21342v1 Announce Type: cross 
Abstract: Recently, Foursquare released a global dataset with more than 100 million points of interest (POIs), each representing a real-world business on its platform. However, many entries lack complete metadata such as addresses or categories, and some correspond to non-existent or fictional locations. In contrast, OpenStreetMap (OSM) offers a rich, user-contributed POI dataset with detailed and frequently updated metadata, though it does not formally verify whether a POI represents an actual business. In this data paper, we present a methodology that integrates the strengths of both datasets: Foursquare as a comprehensive baseline of commercial POIs and OSM as a source of enriched metadata. The combined dataset totals approximately 1 TB. While this full version is not publicly released, we provide filtered releases with adjustable thresholds that reduce storage needs and make the data practical to download and use across domains. We also provide step-by-step instructions to reproduce the full 631 GB build. Record linkage is achieved by computing name similarity scores and spatial distances between Foursquare and OSM POIs. These measures identify and retain high-confidence matches that correspond to real businesses in Foursquare, have representations in OSM, and show strong name similarity. Finally, we use this filtered dataset to construct a graph-based representation of POIs enriched with attributes from both sources, enabling advanced spatial analyses and a range of downstream applications.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Socio-Technical Topology-Aware Adaptive Threat Detection in Software Supply Chains</title>
<link>https://arxiv.org/abs/2510.21452</link>
<guid>https://arxiv.org/abs/2510.21452</guid>
<content:encoded><![CDATA[
<div> SSCs, software supply chains, threat detection, socio-technical models, adaptive.

Summary:
Software supply chains (SSCs) are complex systems that are vulnerable to attacks, necessitating targeted and adaptive threat detection strategies. Current approaches mainly focus on technical aspects, but understanding socio-technical dynamics is crucial for effective threat detection. By analyzing the XZ Utils attack, where malicious actors exploited social trust within the project, the importance of monitoring both technical and social data for identifying suspicious behavior is highlighted. The proposed research vision involves using socio-technical models to support adaptive threat detection in SSCs, leveraging trends in data to inform targeted vulnerability assessment. Key challenges include techniques for developer and software analysis, decentralized adaptation, and the establishment of a test bed for SSC security research. This holistic approach aims to enhance the security of software supply chains by integrating technical and social considerations in threat detection mechanisms.<br /><br />Summary: <div>
arXiv:2510.21452v1 Announce Type: cross 
Abstract: Software supply chains (SSCs) are complex systems composed of dynamic, heterogeneous technical and social components which collectively achieve the production and maintenance of software artefacts. Attacks on SSCs are increasing, yet pervasive vulnerability analysis is challenging due to their complexity. Therefore, threat detection must be targeted, to account for the large and dynamic structure, and adaptive, to account for its change and diversity. While current work focuses on technical approaches for monitoring supply chain dependencies and establishing component controls, approaches which inform threat detection through understanding the socio-technical dynamics are lacking. We outline a position and research vision to develop and investigate the use of socio-technical models to support adaptive threat detection of SSCs. We motivate this approach through an analysis of the XZ Utils attack whereby malicious actors undermined the maintainers' trust via the project's GitHub and mailing lists. We highlight that monitoring technical and social data can identify trends which indicate suspicious behaviour to then inform targeted and intensive vulnerability assessment. We identify challenges and research directions to achieve this vision considering techniques for developer and software analysis, decentralised adaptation and the need for a test bed for software supply chain security research.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Another Hour on TikTok: ID sampling to obtain a complete slice of TikTok</title>
<link>https://arxiv.org/abs/2504.13279</link>
<guid>https://arxiv.org/abs/2504.13279</guid>
<content:encoded><![CDATA[
<div> TikTok, platform, posts, metadata, statistics
Summary:
- The study focuses on analyzing TikTok, a popular platform with significant global impact, utilizing a method to extract a representative sample of posts.
- The method enables collection of metadata, video media, and comments from a vast portion of TikTok, providing crucial statistics about the platform.
- An estimated 269 million posts were produced on the observed day, highlighting the platform's extensive content creation.
- Approximately 18% of videos on TikTok feature children, reflecting a significant presence of younger users on the platform.
- Notably, at least 0.5% of posts contain artificial intelligence-generated content, indicating the integration of AI technology into TikTok's content creation. 

<br /><br />Summary: <div>
arXiv:2504.13279v4 Announce Type: replace 
Abstract: TikTok is now a massive platform, and has a deep impact on global events. Despite preliminary studies, issues remain in determining fundamental characteristics of the platform. We develop a method to extract a representative sample of >99% of posts from a given time range on TikTok, and use it to collect all posts from a full hour on the platform, alongside all posts from a single minute from each hour of a day. Through this, we obtain post metadata, video media, and comments from a close-to-complete slice of TikTok, and report the critical statistics of the platform. Notably, we estimate a total of 269 million posts produced on the day we looked at, that 18% of videos on the platform feature children, and that at least 0.5% of posts contain artificial intelligence-generated content.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-confidence opinion models with random-time interactions</title>
<link>https://arxiv.org/abs/2409.15148</link>
<guid>https://arxiv.org/abs/2409.15148</guid>
<content:encoded><![CDATA[
<div> random-time interactions, bounded-confidence model, renewal processes, interevent-time distributions, transient dynamics

Summary:
Random-time interactions in bounded-confidence models on networks are studied in this paper, allowing agents to interact at random times using renewal processes to determine event times following arbitrary interevent-time distributions (ITDs). Connections between random-time and deterministic-time interaction BCMs are established, with a focus on the impact of ITDs on transient dynamics. Markovian ITDs result in consistent statistical properties, while non-Markovian ITDs show dependence on the type of ITD even with the same mean. Numerical analysis of models with various ITDs on different networks reveals differences in expected order-parameter values and convergence times for transient and steady-state dynamics. This research provides insights into the effects of random-time interactions in opinion dynamics models and highlights the importance of considering randomness in social interactions. 

<br /><br />Summary: <div>
arXiv:2409.15148v2 Announce Type: replace-cross 
Abstract: In models of opinion dynamics, agents interact with each other and can change their opinions as a result of those interactions. One type of opinion model is a bounded-confidence model (BCM), in which opinions take continuous values and interacting agents compromise their opinions with each other if their opinions are sufficiently similar. In studies of BCMs, researchers typically assume that interactions between agents occur at deterministic times. This assumption neglects an inherent element of randomness in social interactions, and it is desirable to account for it. In this paper, we study BCMs on networks and allow agents to interact at random times. To incorporate random-time interactions, we use renewal processes to determine social-interaction event times, which can follow arbitrary interevent-time distributions (ITDs). We establish connections between these random-time-interaction BCMs and deterministic-time-interaction BCMs. We analyze the quantitative impact of ITDs on the transient dynamics of BCMs and derive approximate master equations for the time-dependent expectations of the BCM dynamics. We find that BCMs with Markovian ITDs have consistent statistical properties (in particular, they have the same expected time-dependent opinions) when the ITDs have the same mean but that the statistical properties of BCMs with non-Markovian ITDs depend on the type of ITD even when the ITDs have the same mean. Additionally, we numerically examine the transient and steady-state dynamics of our models with various ITDs on different networks and compare their expected order-parameter values and expected convergence times.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A QUBO Framework for Team Formation</title>
<link>https://arxiv.org/abs/2503.23209</link>
<guid>https://arxiv.org/abs/2503.23209</guid>
<content:encoded><![CDATA[
<div> team formation problem, experts, skills, cost functions, QUBO

Summary:
The article introduces the unified TeamFormation formulation, addressing the team formation problem by optimizing the coverage of required skills while minimizing expert costs. Three variants of TeamFormation are formulated using QUBO, with different cost functions considered. Two general-purpose solution methods are evaluated, demonstrating that QUBO-based solutions are at least as effective as established baselines. Additionally, the article highlights the use of graph neural networks in QUBO-based solutions, enabling the learning of expert and skill representations for transfer learning. This approach allows for efficient application of node embeddings from one problem instance to another, showcasing the potential for enhanced problem-solving capabilities in team formation scenarios. <div>
arXiv:2503.23209v2 Announce Type: replace-cross 
Abstract: The team formation problem assumes a set of experts and a task, where each expert has a set of skills and the task requires some skills. The objective is to find a set of experts that maximizes coverage of the required skills while simultaneously minimizing the costs associated with the experts. Different definitions of cost have traditionally led to distinct problem formulations and algorithmic solutions. We introduce the unified TeamFormation formulation that captures all cost definitions for team formation problems that balance task coverage and expert cost. Specifically, we formulate three TeamFormation variants with different cost functions using quadratic unconstrained binary optimization (QUBO), and we evaluate two distinct general-purpose solution methods. We show that solutions based on the QUBO formulations of TeamFormation problems are at least as good as those produced by established baselines. Furthermore, we show that QUBO-based solutions leveraging graph neural networks can effectively learn representations of experts and skills to enable transfer learning, allowing node embeddings from one problem instance to be efficiently applied to another.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Risks of Industry Influence in Tech Research</title>
<link>https://arxiv.org/abs/2510.19894</link>
<guid>https://arxiv.org/abs/2510.19894</guid>
<content:encoded><![CDATA[
<div> Keywords: information technologies, public health, technology policy, industry influence, scientific research

Summary: 
Emerging information technologies have a significant impact on public health, political institutions, social dynamics, and the environment. It is crucial to understand these effects to shape evidence-based technology policies that minimize harm and maximize benefits. However, the data necessary for scientific progress in this field are controlled by the same industry that may be subject to regulation, raising concerns about potential industry influence on the scientific record. Technology companies also play a major role in funding research in this area, further complicating the issue. The unique challenges faced by science in technology research necessitate strengthening existing safeguards and developing new ones to ensure the integrity of scientific understanding. Addressing these challenges is essential for maintaining the credibility and objectivity of research in the technology sector. 

<br /><br />Summary: <div>
arXiv:2510.19894v1 Announce Type: new 
Abstract: Emerging information technologies like social media, search engines, and AI can have a broad impact on public health, political institutions, social dynamics, and the natural world. It is critical to develop a scientific understanding of these impacts to inform evidence-based technology policy that minimizes harm and maximizes benefits. Unlike most other global-scale scientific challenges, however, the data necessary for scientific progress are generated and controlled by the same industry that might be subject to evidence-based regulation. Moreover, technology companies historically have been, and continue to be, a major source of funding for this field. These asymmetries in information and funding raise significant concerns about the potential for undue industry influence on the scientific record. In this Perspective, we explore how technology companies can influence our scientific understanding of their products. We argue that science faces unique challenges in the context of technology research that will require strengthening existing safeguards and constructing wholly new ones.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Feature Importance for Online Content Moderation</title>
<link>https://arxiv.org/abs/2510.19882</link>
<guid>https://arxiv.org/abs/2510.19882</guid>
<content:encoded><![CDATA[
<div> predictive performance, user behaviour, moderation intervention, feature selection, user characteristics
Summary: 
- The study aims to understand how user characteristics relate to different responses to moderation interventions. 
- 753 features were analyzed to predict changes in user behavior on Reddit after a major moderation intervention. 
- A greedy feature selection approach was used to identify the most informative features for predicting changes in user activity, toxicity, and participation diversity. 
- Certain features were found to be consistently predictive across all tasks, while others were task-specific or of limited use. 
- Changes in user activity and toxicity were easier to predict than changes in participation diversity, indicating the complexity of post-moderation user behavior. 
- The results suggest the importance of tailoring moderation strategies to both user traits and the specific goals of the intervention. 
- This research lays the groundwork for developing accurate systems to predict user reactions to moderation interventions. 
- Effective moderation strategies should take into account the diversity of user responses and adjust accordingly. 
- The findings highlight the need for personalized and adaptive moderation approaches in online communities. 
Summary: <div>
arXiv:2510.19882v1 Announce Type: cross 
Abstract: Accurately estimating how users respond to moderation interventions is paramount for developing effective and user-centred moderation strategies. However, this requires a clear understanding of which user characteristics are associated with different behavioural responses, which is the goal of this work. We investigate the informativeness of 753 socio-behavioural, linguistic, relational, and psychological features, in predicting the behavioural changes of 16.8K users affected by a major moderation intervention on Reddit. To reach this goal, we frame the problem in terms of "quantification", a task well-suited to estimating shifts in aggregate user behaviour. We then apply a greedy feature selection strategy with the double goal of (i) identifying the features that are most predictive of changes in user activity, toxicity, and participation diversity, and (ii) estimating their importance. Our results allow identifying a small set of features that are consistently informative across all tasks, and determining that many others are either task-specific or of limited utility altogether. We also find that predictive performance varies according to the task, with changes in activity and toxicity being easier to estimate than changes in diversity. Overall, our results pave the way for the development of accurate systems that predict user reactions to moderation interventions. Furthermore, our findings highlight the complexity of post-moderation user behaviour, and indicate that effective moderation should be tailored not only to user traits but also to the specific objective of the intervention.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drug-disease networks and drug repurposing</title>
<link>https://arxiv.org/abs/2510.19948</link>
<guid>https://arxiv.org/abs/2510.19948</guid>
<content:encoded><![CDATA[
<div> prediction, drug-disease associations, network analysis, graph embedding, machine learning <br />
Summary: <br />Repurposing existing drugs for new diseases is a cost-effective strategy, but the vast number of potential combinations makes it challenging to identify viable options. This study introduces a novel drug-disease network using a variety of data sources and analysis techniques. By applying network-based link prediction methods, the researchers were able to successfully identify potential drug-disease combinations. The methods utilized, particularly those based on graph embedding and network modeling, demonstrated impressive prediction accuracy, outperforming previous approaches. Cross-validation tests showed strong performance, with area under the ROC curve exceeding 0.95 and average precision significantly better than chance. This research highlights the value of utilizing computational methods and network analysis to facilitate the repurposing of drugs for new medical indications, providing a promising approach for identifying potential treatment options efficiently. <br /> <div>
arXiv:2510.19948v1 Announce Type: cross 
Abstract: Repurposing existing drugs to treat new diseases is a cost-effective alternative to de novo drug development, but there are millions of potential drug-disease combinations to be considered with only a small fraction being viable. In silico predictions of drug-disease associations can be invaluable for reducing the size of the search space. In this work we present a novel network of drugs and the diseases they treat, compiled using a combination of existing textual and machine-readable databases, natural-language processing tools, and hand curation, and analyze it using network-based link prediction methods to identify potential drug-disease combinations. We measure the efficacy of these methods using cross-validation tests and find that several methods, particularly those based on graph embedding and network model fitting, achieve impressive prediction performance, significantly better than previous approaches, with area under the ROC curve above 0.95 and average precision almost a thousand times better than chance.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Classic GNNs Strong Baselines Across Varying Homophily: A Smoothness-Generalization Perspective</title>
<link>https://arxiv.org/abs/2412.09805</link>
<guid>https://arxiv.org/abs/2412.09805</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, GNNs, homophily, message passing, smoothness-generalization dilemma<br />
Summary:<br />
The article introduces the Inceptive Graph Neural Network (IGNN) to address the smoothness-generalization dilemma in GNNs. The dilemma arises due to the trade-off between smoothness and generalization when increasing message passing hops in homophilic neighborhoods. IGNN's design principles enable distinct hop-wise generalization while improving overall generalization with adaptive smoothness. Benchmarking against 30 baselines shows IGNN's superiority and universality in homophilic GNN variants. The study sheds light on the theoretical challenges of GNN universality across varying homophily levels and provides insights for designing effective architectures to enhance GNN performance. <div>
arXiv:2412.09805v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have achieved great success but are often considered to be challenged by varying levels of homophily in graphs. Recent empirical studies have surprisingly shown that homophilic GNNs can perform well across datasets of different homophily levels with proper hyperparameter tuning, but the underlying theory and effective architectures remain unclear. To advance GNN universality across varying homophily, we theoretically revisit GNN message passing and uncover a novel smoothness-generalization dilemma, where increasing hops inevitably enhances smoothness at the cost of generalization. This dilemma hinders learning in higher-order homophilic neighborhoods and all heterophilic ones, where generalization is critical due to complex neighborhood class distributions that are sensitive to shifts induced by noise and sparsity. To address this, we introduce the Inceptive Graph Neural Network (IGNN) built on three simple yet effective design principles, which alleviate the dilemma by enabling distinct hop-wise generalization alongside improved overall generalization with adaptive smoothness. Benchmarking against 30 baselines demonstrates IGNN's superiority and reveals notable universality in certain homophilic GNN variants. Our code and datasets are available at https://github.com/galogm/IGNN.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refugees of the Digital Space: Platform Migration from TikTok to RedNote</title>
<link>https://arxiv.org/abs/2510.18894</link>
<guid>https://arxiv.org/abs/2510.18894</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, RedNote, cross-cultural platform migration, algorithmic governance, digital migrants. <br />
Summary: <br />
This study examines the impact of the nationwide TikTok ban in the U.S. in 2025, leading to American users migrating to RedNote. The research explores how these "TikTok Refugees" adapt to a new platform environment under algorithmic governance. The study analyzes temporal posting patterns, influence dynamics, thematic preferences, and sentiment-weighted topic expressions across different migration phases. High-influence users engage in culturally resonant or commercially strategic content, while political discourse is a point of transnational engagement. Emotionally, high-influence users display more positive affect in culturally connective topics, while low-influence users show stronger emotional intensity in personal narratives. The findings suggest that platform migration is influenced by both structural affordances and users' capacities to adapt and maintain visibility, contributing to the literature on platform society, affective publics, and user agency in transnational digital environments. <br /> <div>
arXiv:2510.18894v1 Announce Type: new 
Abstract: In January 2025, the U.S. government enacted a nationwide ban on TikTok, prompting a wave of American users -- self-identified as ``TikTok Refugees'' -- to migrate to alternative platforms, particularly the Chinese social media app RedNote (Xiaohongshu). This paper examines how these digital migrants navigate cross-cultural platform environments and develop adaptive communicative strategies under algorithmic governance. Drawing on a multi-method framework, the study analyzes temporal posting patterns, influence dynamics, thematic preferences, and sentiment-weighted topic expressions across three distinct migration phases: Pre-Ban, Refugee Surge, and Stabilization.
  An entropy-weighted influence score was used to classify users into high- and low-influence groups, enabling comparative analysis of content strategies. Findings reveal that while dominant topics remained relatively stable over time (e.g., self-expression, lifestyle, and creativity), high-influence users were more likely to engage in culturally resonant or commercially strategic content. Additionally, political discourse was not avoided, but selectively activated as a point of transnational engagement.
  Emotionally, high-influence users tended to express more positive affect in culturally connective topics, while low-influence users showed stronger emotional intensity in personal narratives. These findings suggest that cross-cultural platform migration is shaped not only by structural affordances but also by users' differential capacities to adapt, perform, and maintain visibility. The study contributes to literature on platform society, affective publics, and user agency in transnational digital environments.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Patience in Online Grocery Shopping</title>
<link>https://arxiv.org/abs/2510.19066</link>
<guid>https://arxiv.org/abs/2510.19066</guid>
<content:encoded><![CDATA[
<div> customer patience, online grocery shopping, urban traffic congestion, sustainability, delivery optimization 

Summary: 
This study investigates the impact of customer patience on reducing traffic congestion and emissions in urban delivery systems for online grocery shopping. The research highlights a convex relationship between customer patience and traffic congestion, with allowing just five additional minutes in delivery time resulting in a significant reduction in daily delivery mileage and CO2 emissions. By analyzing two large-scale datasets from Dubai, the study confirms theoretical predictions and demonstrates that modest increases in customer patience can lead to substantial gains in traffic reduction and sustainability. However, beyond ten minutes of added patience, the marginal benefits diminish significantly. This research emphasizes the importance of balancing individual convenience with societal welfare in urban delivery systems, providing a scalable strategy to address the environmental impact of online grocery shopping. <div>
arXiv:2510.19066v1 Announce Type: new 
Abstract: Since the COVID-19 pandemic, online grocery shopping has rapidly reshaped consumer behavior worldwide, fueled by ever-faster delivery promises aimed at maximizing convenience. Yet, this growth has also substantially increased urban traffic congestion, emissions, and pollution. Despite extensive research on urban delivery optimization, little is known about the trade-off between individual convenience and these societal costs. In this study, we investigate the value of marginal extensions in delivery times, termed customer patience, in mitigating the traffic burden caused by grocery deliveries. We first conceptualize the problem and present a mathematical model that highlights a convex relationship between patience and traffic congestion. The theoretical predictions are confirmed by an extensive, network-science based analysis leveraging two large-scale datasets encompassing over 8 million grocery orders in Dubai. Our findings reveal that allowing just five additional minutes in delivery time reduces daily delivery mileage by approximately 30 percent and life-cycle CO2 emissions by 20 percent. Beyond ten minutes of added patience, however, marginal benefits diminish significantly. These results highlight that modest increases in consumer patience can deliver substantial gains in traffic reduction and sustainability, offering a scalable strategy to balance individual convenience with societal welfare in urban delivery systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniqueRank: Identifying Important and Difficult-to-Replace Nodes in Attributed Graphs</title>
<link>https://arxiv.org/abs/2510.19113</link>
<guid>https://arxiv.org/abs/2510.19113</guid>
<content:encoded><![CDATA[
<div> Markov Chain, Node Ranking, Structural Importance, UniqueRank, Attribute Uniqueness  
Summary:  
UniqueRank is a new node-ranking method that considers both structural importance and attribute uniqueness in networks. This approach aims to identify nodes that are difficult to replace, thus playing a significant role in maintaining network efficiency. UniqueRank distinguishes important nodes with distinct attributes from their neighbors in symmetric networks. In real-world networks such as terrorist, social, and supply chain networks, top UniqueRank nodes prove to be more crucial as their removal leads to larger efficiency reductions compared to nodes ranked by other methods. The versatility of UniqueRank is highlighted in its ability to identify essential atoms with unique chemical environments in biomolecular structures. This research fills a gap in existing ranking methods by considering the replaceability of nodes, ultimately enhancing the understanding of network dynamics and identifying truly critical nodes in various applications.  
<br /><br />Summary: <div>
arXiv:2510.19113v1 Announce Type: new 
Abstract: Node-ranking methods that focus on structural importance are widely used in a variety of applications, from ranking webpages in search engines to identifying key molecules in biomolecular networks. In real social, supply chain, and terrorist networks, one definition of importance considers the impact on information flow or network productivity when a given node is removed. In practice, however, a nearby node may be able to replace another node upon removal, allowing the network to continue functioning as before. This replaceability is an aspect that existing ranking methods do not consider. To address this, we introduce UniqueRank, a Markov-Chain-based approach that captures attribute uniqueness in addition to structural importance, making top-ranked nodes harder to replace. We find that UniqueRank identifies important nodes with dissimilar attributes from its neighbors in simple symmetric networks with known ground truth. Further, on real terrorist, social, and supply chain networks, we demonstrate that removing and attempting to replace top UniqueRank nodes often yields larger efficiency reductions than removing and attempting to replace top nodes ranked by competing methods. Finally, we show UniqueRank's versatility by demonstrating its potential to identify structurally critical atoms with unique chemical environments in biomolecular structures.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief propagation for finite networks using a symmetry-breaking source node</title>
<link>https://arxiv.org/abs/2510.19231</link>
<guid>https://arxiv.org/abs/2510.19231</guid>
<content:encoded><![CDATA[
<div> Belief Propagation, message-passing algorithm, graphical models, statistical physics, finite systems <br />
Summary: Belief Propagation (BP) is widely used for inference in graphical models and statistical physics. However, it often provides inaccurate estimates in finite systems, especially in sparse networks. By fixing the state of a single well-connected node, accuracy can be improved without additional computational cost. This approach works well for percolation and Ising models, capturing finite-size effects in various networks, particularly tree-like ones. This method breaks global symmetry, enhancing inference accuracy in sparse networks with few loops. <div>
arXiv:2510.19231v1 Announce Type: new 
Abstract: Belief Propagation (BP) is an efficient message-passing algorithm widely used for inference in graphical models and for solving various problems in statistical physics. However, BP often yields inaccurate estimates of order parameters and their susceptibilities in finite systems, particularly in sparse networks with few loops. Here, we show for both percolation and Ising models that fixing the state of a single well-connected "source" node to break global symmetry substantially improves inference accuracy and captures finite-size effects across a broad range of networks, especially tree-like ones, at no additional computational cost.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Newborn to Impact: Bias-Aware Citation Prediction</title>
<link>https://arxiv.org/abs/2510.19246</link>
<guid>https://arxiv.org/abs/2510.19246</guid>
<content:encoded><![CDATA[
<div> Framework, Citation Prediction, Bias-Aware Learning, Multi-Agent Feature Extraction, Graph Representation Learning

Summary:
The article introduces a Bias-Aware Citation Prediction Framework to address gaps in the modeling of implicit scientific impact factors and bias-aware learning for accurate citation predictions of newly published papers. The framework combines multi-agent feature extraction and robust graph representation learning to derive fine-grained signals from metadata and external resources, even without citation signals. Robust mechanisms like a two-stage forward process, GroupDRO for optimizing worst-case group risk, and a regularization head for controllable factor analysis are incorporated. Experimental results on real datasets demonstrate the model's effectiveness, achieving a 13% reduction in error metrics and a 5.5% improvement in the ranking metric over baseline methods.<br /><br />Summary: <div>
arXiv:2510.19246v1 Announce Type: new 
Abstract: As a key to accessing research impact, citation dynamics underpins research evaluation, scholarly recommendation, and the study of knowledge diffusion. Citation prediction is particularly critical for newborn papers, where early assessment must be performed without citation signals and under highly long-tailed distributions. We identify two key research gaps: (i) insufficient modeling of implicit factors of scientific impact, leading to reliance on coarse proxies; and (ii) a lack of bias-aware learning that can deliver stable predictions on lowly cited papers. We address these gaps by proposing a Bias-Aware Citation Prediction Framework, which combines multi-agent feature extraction with robust graph representation learning. First, a multi-agent x graph co-learning module derives fine-grained, interpretable signals, such as reproducibility, collaboration network, and text quality, from metadata and external resources, and fuses them with heterogeneous-network embeddings to provide rich supervision even in the absence of early citation signals. Second, we incorporate a set of robust mechanisms: a two-stage forward process that routes explicit factors through an intermediate exposure estimate, GroupDRO to optimize worst-case group risk across environments, and a regularization head that performs what-if analyses on controllable factors under monotonicity and smoothness constraints. Comprehensive experiments on two real-world datasets demonstrate the effectiveness of our proposed model. Specifically, our model achieves around a 13% reduction in error metrics (MALE and RMSLE) and a notable 5.5% improvement in the ranking metric (NDCG) over the baseline methods.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfair Mistakes on Social Media: How Demographic Characteristics influence Authorship Attribution</title>
<link>https://arxiv.org/abs/2510.19708</link>
<guid>https://arxiv.org/abs/2510.19708</guid>
<content:encoded><![CDATA[
<div> gender, native language, age, authorship attribution, bias <br />
<br />Summary: 
The paper investigates the fairness of authorship attribution techniques with regards to gender, native language, and age. Three different fairness evaluations are conducted. Firstly, the study looks at how the distribution of demographic characteristics impacts classifier performance. Secondly, it examines whether a user's demographics affect the likelihood of misclassification. The findings indicate no bias across demographic groups in the closed-world setting. However, when the true author is not an option, errors tend to attribute authorship to individuals with similar demographic characteristics. This suggests a potential bias in certain error scenarios that can lead to false attributions. The study underscores the importance of considering fairness beyond overall classifier performance, as errors can reveal underlying biases in the authorship attribution process. <br /> <div>
arXiv:2510.19708v1 Announce Type: new 
Abstract: Authorship attribution techniques are increasingly being used in online contexts such as sock puppet detection, malicious account linking, and cross-platform account linking. Yet, it is unknown whether these models perform equitably across different demographic groups. Bias in such techniques could lead to false accusations, account banning, and privacy violations disproportionately impacting users from certain demographics. In this paper, we systematically audit authorship attribution for bias with respect to gender, native language, and age. We evaluate fairness in 3 ways. First, we evaluate how the proportion of users with a certain demographic characteristic impacts the overall classifier performance. Second, we evaluate if a user's demographic characteristics influence the probability that their texts are misclassified. Our analysis indicates that authorship attribution does not demonstrate bias across demographic groups in the closed-world setting. Third, we evaluate the types of errors that occur when the true author is removed from the suspect set, thereby forcing the classifier to choose an incorrect author. Unlike the first two settings, this analysis demonstrates a tendency to attribute authorship to users who share the same demographic characteristic as the true author. Crucially, these errors do not only include texts that deviate from a user's usual style, but also those that are very close to the author's average. Our results highlight that though a model may appear fair in the closed-world setting for a performant classifier, this does not guarantee fairness when errors are inevitable.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Substitution to Complement? Uncovering the Evolving Interplay between Ride-hailing Services and Public Transit</title>
<link>https://arxiv.org/abs/2510.19745</link>
<guid>https://arxiv.org/abs/2510.19745</guid>
<content:encoded><![CDATA[
<div> ride-hailing services, transportation network companies, TNC, public transit, Shanghai <br />
Summary: <br />
The study investigates the relationship between transportation network companies (TNCs) and public transit (PT) in Shanghai. Data from 96,716 ride-hailing vehicles in September 2022 was analyzed to classify TNC-PT relationships as first-mile complementary, last-mile complementary, substitutive, or independent. The study found comparable ratios of complementary and substitutive trips, challenging previous assumptions. A machine learning method revealed nonlinear effects of factors such as distance to metro stations and bus stop density on trip relationships. Distinct effects were observed for metro hubs and single-line stations on first- or last-mile complementary ratios, with an inverted U-shaped pattern in the relationship between distance to single-line stations and trip ratios. The findings suggest a complex interplay between TNCs and public transit in urban transportation systems. <br />   <br />Summary: <div>
arXiv:2510.19745v1 Announce Type: new 
Abstract: The literature on transportation network companies (TNCs), also known as ride-hailing services, has often characterized these service providers as predominantly substitutive to public transit (PT). However, as TNC markets expand and mature, the complementary and substitutive relationships with PT may shift. To explore whether such a transformation is occurring, this study collected travel data from 96,716 ride-hailing vehicles during September 2022 in Shanghai, a city characterized by an increasingly saturated TNC market. An enhanced data-driven framework is proposed to classify TNC-PT relationships into four types: first-mile complementary, last-mile complementary, substitutive, and independent. Our findings indicate comparable ratios of complementary trips (9.22%) and substitutive trips (9.06%), contrasting sharply with the findings of prior studies. Furthermore, to examine the nonlinear impact of various influential factors on these ratios, a machine learning method integrating categorical boosting (CatBoost) and Shapley additive explanations (SHAP) is proposed. The results show significant nonlinear effects in some variables, including the distance to the nearest metro station and the density of bus stops. Moreover, metro hubs and regular single-line stations exhibit distinct effects on first- or last-mile complementary ratios. These ratios' relation to the distance to single-line stations shows an inverted U-shaped pattern, with effects rising sharply within 1.5 km, remaining at the peak between 1.5 and 3 km, and then declining as the distance increases to about 15 km.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sync or Sink: Bounds on Algorithmic Collective Action with Noise and Multiple Groups</title>
<link>https://arxiv.org/abs/2510.18933</link>
<guid>https://arxiv.org/abs/2510.18933</guid>
<content:encoded><![CDATA[
<div> algorithmic systems, collective action, coordination challenges, multiple collectives, noise<br />
Summary:<br />
- The article discusses the potential growth of collective action against algorithmic systems and the impact of coordination challenges within collectives.<br />
- It highlights the lack of formal analysis on how coordination challenges affect collective outcomes and the interaction between multiple groups.<br />
- The authors aim to guarantee the success of collective action in the presence of coordination noise and multiple collectives.<br />
- They view data generated as originating from multiple data distributions and derive bounds on collective action success.<br />
- Experiments show that high levels of noise can significantly reduce collective success rates, emphasizing the importance of understanding strategic dynamics in algorithmic systems. <div>
arXiv:2510.18933v1 Announce Type: cross 
Abstract: Collective action against algorithmic systems, which enables groups to promote their own interests, is poised to grow. Hence, there will be growth in the size and the number of distinct collectives. Currently, there is no formal analysis of how coordination challenges within a collective can impact downstream outcomes, or how multiple collectives may affect each other's success. In this work, we aim to provide guarantees on the success of collective action in the presence of both coordination noise and multiple groups. Our insight is that data generated by either multiple collectives or by coordination noise can be viewed as originating from multiple data distributions. Using this framing, we derive bounds on the success of collective action. We conduct experiments to study the effects of noise on collective action. We find that sufficiently high levels of noise can reduce the success of collective action. In certain scenarios, large noise can sink a collective success rate from $100\%$ to just under $60\%$. We identify potential trade-offs between collective size and coordination noise; for example, a collective that is twice as big but with four times more noise experiencing worse outcomes than the smaller, more coordinated one. This work highlights the importance of understanding nuanced dynamics of strategic behavior in algorithmic systems.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Peer Influence Probabilities with Linear Contextual Bandits</title>
<link>https://arxiv.org/abs/2510.19119</link>
<guid>https://arxiv.org/abs/2510.19119</guid>
<content:encoded><![CDATA[
<div> learning, peer influence probabilities, contextual linear bandit framework, regret minimization, uncertainty-guided exploration<br />
Summary:<br />
The study focuses on estimating peer influence probabilities in networked environments where users share recommendations. The research highlights the contextual nature of these probabilities and their impact on information diffusion processes. Traditional methods struggle to accurately learn these probabilities, leading to a trade-off between regret minimization and estimation error. The proposed uncertainty-guided exploration algorithm aims to address this trade-off by tuning a parameter to achieve optimal rate pairs. Experiments on semi-synthetic network datasets demonstrate the superiority of this method over static approaches and contextual bandits that overlook the trade-off. The findings offer insights into enhancing viral marketing strategies and understanding information diffusion mechanisms in networked environments. <div>
arXiv:2510.19119v1 Announce Type: cross 
Abstract: In networked environments, users frequently share recommendations about content, products, services, and courses of action with others. The extent to which such recommendations are successful and adopted is highly contextual, dependent on the characteristics of the sender, recipient, their relationship, the recommended item, and the medium, which makes peer influence probabilities highly heterogeneous. Accurate estimation of these probabilities is key to understanding information diffusion processes and to improving the effectiveness of viral marketing strategies. However, learning these probabilities from data is challenging; static data may capture correlations between peer recommendations and peer actions but fails to reveal influence relationships. Online learning algorithms can learn these probabilities from interventions but either waste resources by learning from random exploration or optimize for rewards, thus favoring exploration of the space with higher influence probabilities. In this work, we study learning peer influence probabilities under a contextual linear bandit framework. We show that a fundamental trade-off can arise between regret minimization and estimation error, characterize all achievable rate pairs, and propose an uncertainty-guided exploration algorithm that, by tuning a parameter, attains any pair within this trade-off. Our experiments on semi-synthetic network datasets show the advantages of our method over static methods and contextual bandits that ignore this trade-off.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties</title>
<link>https://arxiv.org/abs/2510.19299</link>
<guid>https://arxiv.org/abs/2510.19299</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, social dynamics, online behavior, multi-agent simulation, behavioral reward functions

Summary: 
The study explores the ability of large language model (LLM) agents to exhibit complex social dynamics akin to human online behavior. A multi-agent LLM simulation framework is presented, where agents interact, evaluate each other, and adapt behaviors using in-context learning. Behavioral reward functions are designed to capture key drivers of online engagement, such as social interaction and emotional support. Through experiments, it is observed that coached LLM agents develop stable interaction patterns and form social ties similar to real online communities. The framework provides insights into how network structures and group formations arise from individual decision-making. By combining behavioral rewards with adaptation, the study offers a platform to study collective dynamics in LLM populations and understand the extent to which artificial agents can replicate human-like social behavior. <div>
arXiv:2510.19299v1 Announce Type: cross 
Abstract: Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? We present a multi-agent LLM simulation framework in which agents repeatedly interact, evaluate one another, and adapt their behavior through in-context learning accelerated by a coaching signal. To model human social behavior, we design behavioral reward functions that capture core drivers of online engagement, including social interaction, information seeking, self-presentation, coordination, and emotional support. These rewards align agent objectives with empirically observed user motivations, enabling the study of how network structures and group formations emerge from individual decision-making. Our experiments show that coached LLM agents develop stable interaction patterns and form emergent social ties, yielding network structures that mirror properties of real online communities. By combining behavioral rewards with in-context adaptation, our framework establishes a principled testbed for investigating collective dynamics in LLM populations and reveals how artificial agents may approximate or diverge from human-like social behavior.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning when to rank: Estimation of partial rankings from sparse, noisy comparisons</title>
<link>https://arxiv.org/abs/2501.02505</link>
<guid>https://arxiv.org/abs/2501.02505</guid>
<content:encoded><![CDATA[
<div> Bayesian, Nonparametric, Partial Ranking, Sparse Data, Inference<br />
Summary: 
The study focuses on developing a nonparametric Bayesian method for learning partial rankings based on pairwise comparisons of items. Traditional ranking methods often assign unique ranks or scores to each item, even when there is limited or noisy data. The proposed method aims to distinguish among the ranks of different items only when there is sufficient evidence available in the data. An agglomerative algorithm is developed for Maximum A Posteriori (MAP) inference of partial rankings. The method is tested on various real and synthetic network datasets, revealing that it provides a more concise summary of the data compared to traditional ranking methods, especially in scenarios with sparse observations. <div>
arXiv:2501.02505v3 Announce Type: replace-cross 
Abstract: Ranking items based on pairwise comparisons is common, from using match outcomes to rank sports teams to using purchase or survey data to rank consumer products. Statistical inference-based methods such as the Bradley-Terry model, which extract rankings based on an underlying generative model, have emerged as flexible and powerful tools to tackle ranking in empirical data. In situations with limited and/or noisy comparisons, it is often challenging to confidently distinguish the performance of different items based on the evidence available in the data. However, most inference-based ranking methods choose to assign each item to a unique rank or score, suggesting a meaningful distinction when there is none. Here, we develop a principled nonparametric Bayesian method, adaptable to any statistical ranking method, for learning partial rankings (rankings with ties) that distinguishes among the ranks of different items only when there is sufficient evidence available in the data. We develop a fast agglomerative algorithm to perform Maximum A Posteriori (MAP) inference of partial rankings under our framework and examine the performance of our method on a variety of real and synthetic network datasets, finding that it frequently gives a more parsimonious summary of the data than traditional ranking, particularly when observations are sparse.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiplex Networks Provide Structural Pathways for Social Contagion in Rural Social Networks</title>
<link>https://arxiv.org/abs/2510.18280</link>
<guid>https://arxiv.org/abs/2510.18280</guid>
<content:encoded><![CDATA[
<div> social networks, multiplex relationships, network torque, behavioral diffusion, network-based interventions 

Summary: 
This study examines the impact of different layers of relationships within human social networks on the spread of behavioral contagion. Using data from 110 rural Honduran communities, the researchers introduce the concept of network torque to measure the contribution of specific network layers to critical contagion pathways. Close friendships were found to play a significant role in enabling non-overlapping diffusion pathways, leading to increased adoption of health practices at the village level. The study highlights the importance of non-redundant pathways facilitated by specific relationship types in amplifying behavioral change, such as correct knowledge about infant feeding and attitudes towards fathers' involvement in postpartum care. Overall, the findings suggest that non-overlapping multiplex social ties are crucial for social contagion and social coherence in traditional social systems. 

<br /><br />Summary: <div>
arXiv:2510.18280v1 Announce Type: new 
Abstract: Human social networks are inherently multiplex, comprising overlapping layers of relationships. Different layers may have distinct structural properties and interpersonal dynamics, but also may interact to form complex interdependent pathways for social contagion. This poses a fundamental problem in understanding behavioral diffusion and in devising effective network-based interventions. Here, we introduce a new conceptualization of how much each network layer contributes to critical contagion pathways and quantify it using a novel metric, network torque. We exploit data regarding sociocentric maps of 110 rural Honduran communities using a battery of 11 name generators and an experiment involving an exogenous intervention. Using a novel statistical framework, we assess the extent to which specific network layers alter global connectivity and support the spread of three experimentally introduced health practices. The results show that specific relationship types - such as close friendships - particularly enable non-overlapping diffusion pathways, amplifying behavioral change at the village level. For instance, non-redundant pathways enabled by closest friends can increase the adoption of correct knowledge about feeding newborns inappropriate chupones and enhance attitudes regarding fathers' involvement in postpartum care. Non-overlapping multiplex social ties are relevant to social contagion and social coherence in traditionally organized social systems.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior</title>
<link>https://arxiv.org/abs/2510.18155</link>
<guid>https://arxiv.org/abs/2510.18155</guid>
<content:encoded><![CDATA[
<div> Keywords: Consumer decisions, Multi-agent simulation, Large language model, Marketing strategies, Social dynamics 

Summary: 
The article introduces a new framework for simulating consumer decision-making using large language model-powered multi-agent simulations. This framework allows generative agents to interact, express reasoning, form habits, and make purchasing decisions without predefined rules. In a scenario involving price-discount marketing, the system provides actionable outcomes for testing strategies and uncovers emergent social patterns. This approach offers marketers a scalable and low-risk tool for testing strategies before implementation, reducing the need for post-event evaluations and minimizing the risk of underperforming campaigns. <div>
arXiv:2510.18155v1 Announce Type: cross 
Abstract: Simulating consumer decision-making is vital for designing and evaluating marketing strategies before costly real- world deployment. However, post-event analyses and rule-based agent-based models (ABMs) struggle to capture the complexity of human behavior and social interaction. We introduce an LLM-powered multi-agent simulation framework that models consumer decisions and social dynamics. Building on recent advances in large language model simulation in a sandbox envi- ronment, our framework enables generative agents to interact, express internal reasoning, form habits, and make purchasing decisions without predefined rules. In a price-discount marketing scenario, the system delivers actionable strategy-testing outcomes and reveals emergent social patterns beyond the reach of con- ventional methods. This approach offers marketers a scalable, low-risk tool for pre-implementation testing, reducing reliance on time-intensive post-event evaluations and lowering the risk of underperforming campaigns.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Censorship Chokepoints: New Battlegrounds for Regional Surveillance, Censorship and Influence on the Internet</title>
<link>https://arxiv.org/abs/2510.18394</link>
<guid>https://arxiv.org/abs/2510.18394</guid>
<content:encoded><![CDATA[
<div> keywords: Internet, censorship, chokepoints, filtering, surveillance
Summary: 
The article discusses the evolving landscape of Internet censorship and the emergence of new sophisticated techniques that impede access to information. Traditional censorship methods, such as client-based, server-based, or network-based filtering, have been supplemented by modern techniques that transcend location boundaries. The concept of chokepoints is introduced to identify bottlenecks in the content production or delivery cycle, where new forms of large-scale client-side surveillance and filtering mechanisms are utilized. These chokepoints serve as barriers to information access, highlighting the need for a new understanding of contemporary censorship practices. The research emphasizes the importance of recognizing and addressing these chokepoints to ensure unrestricted access to information on the Internet. <div>
arXiv:2510.18394v1 Announce Type: cross 
Abstract: Undoubtedly, the Internet has become one of the most important conduits to information for the general public. Nonetheless, Internet access can be and has been limited systematically or blocked completely during political events in numerous countries and regions by various censorship mechanisms. Depending on where the core filtering component is situated, censorship techniques have been classified as client-based, server-based, or network-based. However, as the Internet evolves rapidly, new and sophisticated censorship techniques have emerged, which involve techniques that cut across locations and involve new forms of hurdles to information access. We argue that modern censorship can be better understood through a new lens that we term chokepoints, which identifies bottlenecks in the content production or delivery cycle where efficient new forms of large-scale client-side surveillance and filtering mechanisms have emerged.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoveOD: Synthesizing Origin-Destination Commute Distribution from U.S. Census Data</title>
<link>https://arxiv.org/abs/2510.18858</link>
<guid>https://arxiv.org/abs/2510.18858</guid>
<content:encoded><![CDATA[
<div> Keywords: commuter, origin-destination, transportation, synthetic trips, open data <br />
Summary: <br />
The article introduces MOVEOD, an open-source pipeline that synthesizes public data to create high-resolution origin-destination tables for transportation applications. By combining various open data sources such as ACS, LODES, OSM, and building footprints, MOVEOD generates commuter OD flows with spatial and temporal details for any county in the United States. The pipeline uses constrained sampling and integer-programming methods to ensure accuracy in OD data by matching commuter totals, aligning workplace destinations, and calibrating travel durations. The framework is demonstrated in Hamilton County, Tennessee, producing approximately 150,000 synthetic trips that are used in vehicle-routing algorithms. MOVEOD is an automated system that can be easily applied across the US with just a county and year input and can be adapted for other countries with similar census datasets. The source code and a user-friendly interface are available for public use. <br /> <div>
arXiv:2510.18858v1 Announce Type: cross 
Abstract: High-resolution origin-destination (OD) tables are essential for a wide spectrum of transportation applications, from modeling traffic and signal timing optimization to congestion pricing and vehicle routing. However, outside a handful of data rich cities, such data is rarely available. We introduce MOVEOD, an open-source pipeline that synthesizes public data into commuter OD flows with fine-grained spatial and temporal departure times for any county in the United States. MOVEOD combines five open data sources: American Community Survey (ACS) departure time and travel time distributions, Longitudinal Employer-Household Dynamics (LODES) residence-to-workplace flows, county geometries, road network information from OpenStreetMap (OSM), and building footprints from OSM and Microsoft, into a single OD dataset. We use a constrained sampling and integer-programming method to reconcile the OD dataset with data from ACS and LODES. Our approach involves: (1) matching commuter totals per origin zone, (2) aligning workplace destinations with employment distributions, and (3) calibrating travel durations to ACS-reported commute times. This ensures the OD data accurately reflects commuting patterns. We demonstrate the framework on Hamilton County, Tennessee, where we generate roughly 150,000 synthetic trips in minutes, which we feed into a benchmark suite of classical and learning-based vehicle-routing algorithms. The MOVEOD pipeline is an end-to-end automated system, enabling users to easily apply it across the United States by giving only a county and a year; and it can be adapted to other countries with comparable census datasets. The source code and a lightweight browser interface are publicly available.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis</title>
<link>https://arxiv.org/abs/2311.00164</link>
<guid>https://arxiv.org/abs/2311.00164</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, GraphSAGE, traffic accident analysis, deep-learning methods, road network connections<br />
Summary:<br />
- Constructed a large-scale dataset of 9 million traffic accident records from official reports in the US, along with road networks and traffic volume data.
- Evaluated existing deep-learning methods for accident prediction and found that GraphSAGE can accurately predict accidents with less than 22% mean absolute error.
- Achieved over 87% AUROC in predicting whether an accident will occur or not on road networks.
- Used multitask learning to handle cross-state variabilities and transfer learning to combine traffic volume with accident prediction.
- Highlighted the importance of road graph-structural features in predicting accidents accurately. <br /> <div>
arXiv:2311.00164v3 Announce Type: replace 
Abstract: We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not with over 87% AUROC, averaged over states. We achieve these results by using multitask learning to account for cross-state variabilities (e.g., availability of accident labels) and transfer learning to combine traffic volume with accident prediction. Ablation studies highlight the importance of road graph-structural features, amongst other features. Lastly, we discuss the implications of the analysis and develop a package for easily using our new dataset.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Voting: Resilience to Abstention and Sybils</title>
<link>https://arxiv.org/abs/2001.05271</link>
<guid>https://arxiv.org/abs/2001.05271</guid>
<content:encoded><![CDATA[
<div> Keywords: sybil votes, social choice, status quo enforcing, voting rules, verified participation

Summary:
In the presence of sybil (fake or duplicate) votes and voter abstention, voting rules may fail to accurately represent the will of society. This study introduces status quo enforcing (QUE) voting rules that add virtual votes in support of the status quo to address this issue. The framework of Reality-aware Social Choice is utilized, with a focus on the tradeoff between safety and liveness in maintaining or changing the status quo. The voting rules are shown to be optimal in several domains, offering a balance between resilience to sybils and responsiveness to verified participation. The study provides a quantitative tool for designers to measure the benefit of increased participation and verified identities, offering insights into the conditions under which mechanisms can effectively address challenges in social choice. <br /><br />Summary: <div>
arXiv:2001.05271v4 Announce Type: replace-cross 
Abstract: Voting rules may implement the will of the society when all eligible voters vote, and only them. However, they may fail to do so when sybil (fake or duplicate) votes are present and when only some honest (non sybil) voters actively participate. As, unfortunately, sometimes this is the case, our aim here is to address social choice in the presence of sybils and voter abstention. %
To do so, we build upon the framework of Reality-aware Social Choice: we assume the status quo as an ever-present distinguished alternative, and study \emph{status quo Enforcing (QUE) voting rules}, which add virtual votes in support of the status quo. We characterize the tradeoff between \emph{safety} and \emph{liveness} (the ability of active honest voters to maintain/change the status quo, respectively) in several domains, and show that the voting rules are often optimal. \revision{Our characterization identifies the exact conditions under which mechanisms remain both resilient to sybils and responsive to verified participation, offering a quantitative tool for designers to measure the benefit of increased participation and verified identities.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DarkGram: A Large-Scale Analysis of Cybercriminal Activity Channels on Telegram</title>
<link>https://arxiv.org/abs/2409.14596</link>
<guid>https://arxiv.org/abs/2409.14596</guid>
<content:encoded><![CDATA[
<div> Keywords: cybercriminal activity channels, DarkGram framework, malicious content, phishing attacks, malware

Summary: 

A large-scale analysis of 339 cybercriminal activity channels (CACs) was conducted, revealing the distribution of malicious and unethical content to over 23.8 million users. The DarkGram framework, with 96% accuracy, automatically identifies malicious posts shared on these channels. Content shared on the CACs includes compromised credentials, pirated software, and malware-infected files. Subscribers are engaged through promotions and giveaways, increasing sales of premium cybercriminal content. However, links in these channels often contain phishing attacks and executable files are bundled with malware. The channels can quickly migrate to evade scrutiny, posing risks to their subscribers. By utilizing DarkGram, 196 channels were taken down in three months. Urgent coordinated efforts are needed to combat the growing threats presented by these channels. The dataset and DarkGram framework are open-sourced to aid in this effort. 

<br /><br />Summary: <div>
arXiv:2409.14596v3 Announce Type: replace-cross 
Abstract: We present the first large-scale analysis of 339 cybercriminal activity channels (CACs). Followed by over 23.8 million users, these channels share a wide array of malicious and unethical content with their subscribers, including compromised credentials, pirated software and media, social media manipulation tools, and blackhat hacking resources such as malware, exploit kits, and social engineering scams. To evaluate these channels, we developed DarkGram, a BERT-based framework that automatically identifies malicious posts from the CACs with an accuracy of 96%. Using DarkGram, we conducted a quantitative analysis of 53,605 posts shared on these channels between February and May 2024, revealing key characteristics of the content. While much of this content is distributed for free, channel administrators frequently employ strategies such as promotions and giveaways to engage users and boost the sales of premium cybercriminal content. Interestingly, these channels sometimes pose significant risks to their own subscribers. Notably, 28.1% of the links shared in these channels contained phishing attacks, and 38% of executable files were bundled with malware. Analyzing how subscribers consume and positively react to the shared content paints a dangerous picture of the perpetuation of cybercriminal content at scale. We also found that the CACs can evade scrutiny or platform takedowns by quickly migrating to new channels with minimal subscriber loss, highlighting the resilience of this ecosystem. To counteract this, we utilized DarkGram to detect emerging channels and reported malicious content to Telegram and affected organizations. This resulted in the takedown of 196 channels over three months. Our findings underscore the urgent need for coordinated efforts to combat the growing threats posed by these channels. To aid this effort, we open-source our dataset and the DarkGram framework.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CayleyPy RL: Pathfinding and Reinforcement Learning on Cayley Graphs</title>
<link>https://arxiv.org/abs/2502.18663</link>
<guid>https://arxiv.org/abs/2502.18663</guid>
<content:encoded><![CDATA[
<div> Keywords: Pathfinding, Artificial Intelligence, Cayley graphs, Machine learning, Mathematical applications

Summary: 
This paper focuses on developing efficient artificial intelligence-based approaches for pathfinding on large Cayley graphs. The authors propose a novel combination of reinforcement learning and diffusion distance approach, benchmarking various choices for key building blocks. The methods were compared against the classical computer algebra system GAP, showing superior performance. Mathematical applications include examining the Cayley graph of the symmetric group and supporting a conjecture on its diameter. The study provides bounds on the diameter and presents conjectures on the central limit phenomenon, spectrum distribution, and sorting networks. To encourage collaboration, challenges are created on the Kaggle platform for improving and benchmarking pathfinding approaches on Cayley graphs. <div>
arXiv:2502.18663v2 Announce Type: replace-cross 
Abstract: This paper is the second in a series of studies on developing efficient artificial intelligence-based approaches to pathfinding on extremely large graphs (e.g. $10^{70}$ nodes) with a focus on Cayley graphs and mathematical applications. The open-source CayleyPy project is a central component of our research. The present paper proposes a novel combination of a reinforcement learning approach with a more direct diffusion distance approach from the first paper. Our analysis includes benchmarking various choices for the key building blocks of the approach: architectures of the neural network, generators for the random walks and beam search pathfinding. We compared these methods against the classical computer algebra system GAP, demonstrating that they "overcome the GAP" for the considered examples. As a particular mathematical application we examine the Cayley graph of the symmetric group with cyclic shift and transposition generators. We provide strong support for the OEIS-A186783 conjecture that the diameter is equal to n(n-1)/2 by machine learning and mathematical methods. We identify the conjectured longest element and generate its decomposition of the desired length. We prove a diameter lower bound of n(n-1)/2-n/2 and an upper bound of n(n-1)/2+ 3n by presenting the algorithm with given complexity. We also present several conjectures motivated by numerical experiments, including observations on the central limit phenomenon (with growth approximated by a Gumbel distribution), the uniform distribution for the spectrum of the graph, and a numerical study of sorting networks. To stimulate crowdsourcing activity, we create challenges on the Kaggle platform and invite contributions to improve and benchmark approaches on Cayley graph pathfinding and other tasks.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proactive and Fair Epidemic Resource Allocation Through an Integrated Supply Chain Framework: Insights from a COVID-19 Study</title>
<link>https://arxiv.org/abs/2510.16969</link>
<guid>https://arxiv.org/abs/2510.16969</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic forecasting, vaccine distribution, optimization framework, regional disparities, behavioral responses

Summary: 
This article presents a novel epidemiological-optimization framework that integrates epidemic forecasting with vaccine distribution and logistics planning. By incorporating spatio-temporally varying infection rates and regional vulnerability indicators, the model supports data-driven decision-making across spatial scales. Two scalable heuristic decomposition algorithms are designed to address computational complexity. Validated using COVID-19 data in the U.S., the model shows the potential to prevent millions of infections and thousands of deaths within a six-month period while improving vaccine accessibility in underserved regions. By prioritizing fairness and considering regional disparities, the framework demonstrates improved efficiency and long-term public health outcomes compared to traditional approaches. Policymakers can utilize this model as a scalable tool to enhance preparedness and ensure a more effective and equitable response to epidemics.<br /><br />Summary: <div>
arXiv:2510.16969v1 Announce Type: new 
Abstract: Timely and effective decision-making is critical during epidemics to reduce preventable infections and deaths. This demands integrated models that jointly capture disease dynamics, vaccine distribution, regional disparities, and behavioral responses. However, most existing approaches decouple epidemic forecasting from logistics planning, hindering adaptive and regionally responsive interventions. We propose a novel epidemiological-optimization framework that jointly models epidemic progression and a multiscale vaccine supply chain. The model incorporates spatio-temporally varying effective infection rates to reflect regional policy and behavioral dynamics. It supports coordinated, data-driven decision-making across spatial scales through two formulations: a multi-objective Gini-based model and a knapsack-based model that leverages regional vulnerability indicators for tractability and improved mitigation. To address computational complexity, we design two scalable heuristic decomposition algorithms inspired by the Benders decomposition. The model is validated using COVID-19 data in the U.S.. We introduce SARIMA-based forecasting as a novel approach for validating epidemic-optimization models under data limitations. The results show that our approach can prevent more than 2 million infections and 30,000 deaths in just six months while significantly improving the accessibility of vaccines in underserved regions. Our framework demonstrates that integrating fairness and epidemic dynamics with vaccine logistics leads to superior outcomes compared to traditional myopic policies. Fairness improves overall efficiency in the long term by prioritizing the most vulnerable populations, leading to better long-term public health outcomes. The model offers policymakers a scalable and operationally relevant tool to strengthen preparedness and ensure a more effective and equitable response to epidemics.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperSearch: Prediction of New Hyperedges through Unconstrained yet Efficient Search</title>
<link>https://arxiv.org/abs/2510.17153</link>
<guid>https://arxiv.org/abs/2510.17153</guid>
<content:encoded><![CDATA[
<div> algorithm, hyperedge prediction, hypergraphs, search-based, scoring function

Summary:
HyperSearch is a novel search-based algorithm for hyperedge prediction in complex systems represented as hypergraphs. It addresses the challenge of efficiently evaluating candidate sets by incorporating an empirically grounded scoring function and an efficient search mechanism. The algorithm prunes the search space using an anti-monotonic upper bound of the scoring function, ensuring discarded candidates are never better than kept ones. In experiments on real-world hypergraphs across different domains, HyperSearch outperforms existing baselines in accurately predicting new hyperedges. Its approach improves accuracy by leveraging real-world observations and theoretical guarantees for efficient search. HyperSearch's performance highlights its effectiveness in identifying missing or potential higher-order interactions within complex systems. <div>
arXiv:2510.17153v1 Announce Type: new 
Abstract: Higher-order interactions (HOIs) in complex systems, such as scientific collaborations, multi-protein complexes, and multi-user communications, are commonly modeled as hypergraphs, where each hyperedge (i.e., a subset of nodes) represents an HOI among the nodes. Given a hypergraph, hyperedge prediction aims to identify hyperedges that are either missing or likely to form in the future, and it has broad applications, including recommending interest-based social groups, predicting collaborations, and uncovering functional complexes in biological systems. However, the vast search space of hyperedge candidates (i.e., all possible subsets of nodes) poses a significant computational challenge, making naive exhaustive search infeasible. As a result, existing approaches rely on either heuristic sampling to obtain constrained candidate sets or ungrounded assumptions on hypergraph structure to select promising hyperedges.
  In this work, we propose HyperSearch, a search-based algorithm for hyperedge prediction that efficiently evaluates unconstrained candidate sets, by incorporating two key components: (1) an empirically grounded scoring function derived from observations in real-world hypergraphs and (2) an efficient search mechanism, where we derive and use an anti-monotonic upper bound of the original scoring function (which is not antimonotonic) to prune the search space. This pruning comes with theoretical guarantees, ensuring that discarded candidates are never better than the kept ones w.r.t. the original scoring function. In extensive experiments on 10 real-world hypergraphs across five domains, HyperSearch consistently outperforms state-of-the-art baselines, achieving higher accuracy in predicting new (i.e., not in the training set) hyperedges.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Maximization in Social Networks by Modifying Internal Opinions</title>
<link>https://arxiv.org/abs/2510.17226</link>
<guid>https://arxiv.org/abs/2510.17226</guid>
<content:encoded><![CDATA[
<div> maximizing overall opinion, social networks, computational efficiency, sampling-based algorithms, public opinion governance

Summary:
The paper addresses the important issue of maximizing overall opinion in social networks by strategically modifying the internal opinions of key nodes. Traditional matrix inversion methods are computationally expensive, leading the authors to propose two efficient sampling-based algorithms. A deterministic asynchronous algorithm is developed to precisely identify the optimal set of nodes through asynchronous update operations and progressive refinement. Extensive experiments on real-world datasets demonstrate the superior performance of the proposed methods compared to baseline approaches. Notably, the asynchronous algorithm stands out for its exceptional efficiency and accuracy even in networks with tens of millions of nodes.<br /><br />Summary: <div>
arXiv:2510.17226v1 Announce Type: new 
Abstract: Public opinion governance in social networks is critical for public health campaigns, political elections, and commercial marketing. In this paper, we addresse the problem of maximizing overall opinion in social networks by strategically modifying the internal opinions of key nodes. Traditional matrix inversion methods suffer from prohibitively high computational costs, prompting us to propose two efficient sampling-based algorithms. Furthermore, we develop a deterministic asynchronous algorithm that exactly identifies the optimal set of nodes through asynchronous update operations and progressive refinement, ensuring both efficiency and precision. Extensive experiments on real-world datasets demonstrate that our methods outperform baseline approaches. Notably, our asynchronous algorithm delivers exceptional efficiency and accuracy across all scenarios, even in networks with tens of millions of nodes.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Neuroticism Paradox: How Emotional Instability Fuels Collective Feelings</title>
<link>https://arxiv.org/abs/2510.16046</link>
<guid>https://arxiv.org/abs/2510.16046</guid>
<content:encoded><![CDATA[
<div> Keywords: collective emotions, emotional leadership, neuroticism, conscientiousness, emotional contagion

Summary:<br /><br />Collective emotions in organizations, communities, and societies are influenced by certain individuals, identified in this study as emotionally unstable individuals high in neuroticism and low in conscientiousness. Contrary to conventional wisdom, extraversion did not have a significant impact on emotional leadership. The study revealed a "Neuroticism Paradox," showing that emotional volatility, not stability, drives emotional contagion. The emotions spread at a rate comparable to measles and were sustained by high clustering. The increase in emotional variance over time challenges homeostasis theories and suggests entropy-driven dynamics. The proposed Affective Epidemiology framework suggests that network position and volatility, rather than personality stability, play a crucial role in governing collective emotions and emotional leadership in human systems. <div>
arXiv:2510.16046v1 Announce Type: cross 
Abstract: Collective emotions shape organizations, communities, and societies, yet the traits that determine who drives them remain unknown. Conventional wisdom holds that stable, extraverted individuals act as emotional leaders, calming and coordinating the feelings of others. Here we challenge this view by analyzing a 30.5-month longitudinal dataset of daily emotions from 38 co-located professionals (733,534 records). Using Granger-causality network reconstruction, we find that emotionally unstable individuals -- those high in neuroticism (r = 0.478, p = 0.002) and low in conscientiousness (r = -0.512, p = 0.001) -- are the true "emotional super-spreaders," while extraversion shows no effect (r = 0.238, p = 0.150). This "Neuroticism Paradox" reveals that emotional volatility, not stability, drives contagion. Emotions propagate with a reproduction rate (R_0 = 15.58) comparable to measles, yet the system avoids collapse through high clustering (C = 0.705) that creates "emotional quarantine zones." Emotional variance increased 22.9% over time, contradicting homeostasis theories and revealing entropy-driven dynamics. We propose an Affective Epidemiology framework showing that collective emotions are governed by network position and volatility rather than personality stability -- transforming how we understand emotional leadership in human systems.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shifting 'AI Policy' Preprints and Citation Trends in the U.S., U.K and E.U., and South Korea (2015-2024)</title>
<link>https://arxiv.org/abs/2510.16477</link>
<guid>https://arxiv.org/abs/2510.16477</guid>
<content:encoded><![CDATA[
<div> preprints, AI Policy, citations, global disruptions, normalization <br />
Summary: <br />
This study examines the increase in citations of preprints in the field of AI Policy over the past decade, with a particular focus on regions such as the U.S., U.K. & E.U., and South Korea. The study reveals a significant rise in preprint citations, from five percent to forty percent, across these regions. The analysis also compares the impact of COVID-19 and the release of ChatGPT on preprint citations globally. The study discusses the driving factors behind the normalization of preprints in AI Policy literature and the risks associated with this trend. The findings suggest that preprint citation patterns are following the broader trend in computer science research. <div>
arXiv:2510.16477v1 Announce Type: cross 
Abstract: This study of literature focusing on 'AI Policy' over the past decade, found that citations of preprints, publications on platforms such as arXiv, have increased from five percent to forty percent across three major regions: the U.S., U.K. & E.U., and South Korea. We compare regional responses of preprint citations across the global disruptions of COVID-19 and the release of ChatGPT. We discuss driving factors and risks of preprint normalization, which follows the trend in computer science.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network analysis reveals news press landscape and asymmetric user polarization</title>
<link>https://arxiv.org/abs/2408.07900</link>
<guid>https://arxiv.org/abs/2408.07900</guid>
<content:encoded><![CDATA[
<div> echo chambers, polarization, online news platforms, affective interaction, Naver News

Summary:
The study examines the structural and affective polarization among user groups on Naver News during the 2022 Korean presidential election. Analyzing a dataset of articles and user comments, the research identifies two opposing political leanings with significant bias and polarization. Echo chambers are found within co-commenting networks, indicating the reinforcement of opinions within like-minded groups. The study also uncovers asymmetric affective interaction patterns between the polarized groups, highlighting distinct communication strategies based on political affiliations. Through network analysis of a large-scale comment dataset, the research offers insights into the nuanced nature of user polarization on online news platforms. <div>
arXiv:2408.07900v2 Announce Type: replace 
Abstract: Unlike traditional media, online news platforms allow users to consume content that suits their tastes and to facilitate interactions with other people. However, as more personalized consumption of information and interaction with like-minded users increase, ideological bias can inadvertently increase and contribute to the formation of echo chambers, reinforcing the polarization of opinions. Although the structural characteristics of polarization among different ideological groups in online spaces have been extensively studied, research into how these groups emotionally interact with each other has not been as thoroughly explored. From this perspective, we investigate both structural and affective polarization between news media user groups on Naver News, South Korea's largest online news portal, during the period of 2022 Korean presidential election. By utilizing the dataset comprising 333,014 articles and over 36 million user comments, we uncover two distinct groups of users characterized by opposing political leanings and reveal significant bias and polarization among them. Additionally, we reveal the existence of echo chambers within co-commenting networks and investigate the asymmetric affective interaction patterns between the two polarized groups. Classification task of news media articles based on the distinct comment response patterns support the notion that different political groups may employ distinct communication strategies. Our approach based on network analysis on large-scale comment dataset offers novel insights into characteristics of user polarization in the online news platforms and the nuanced interaction nature between user groups.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Dynamic Modeling via Neural ODEs for Popularity Trajectory Prediction</title>
<link>https://arxiv.org/abs/2410.18742</link>
<guid>https://arxiv.org/abs/2410.18742</guid>
<content:encoded><![CDATA[
<div> neural ordinary differential equations, popularity trajectory prediction, information cascades, dynamic properties, real-world dynamics <br />
Summary:
The paper introduces a novel approach, NODEPT, for popularity trajectory prediction in information cascades. Unlike existing methods, NODEPT focuses on forecasting the continuous evolution of popularity over time, capturing dynamic properties like change rates and growth patterns. The method utilizes neural ordinary differential equations to model the underlying diffusion system's dynamics. It includes an encoder for learning cascade representations, an ODE-based generative module to capture system dynamics, and a decoder for predicting future popularity trajectory. Experimental results on real-world datasets validate the superiority and rationality of NODEPT, showcasing its effectiveness in predicting popularity trajectories accurately and efficiently. The approach extends beyond traditional discrete popularity prediction methods, offering a more comprehensive and insightful understanding of information cascade dynamics and future popularity trends. <br /> <div>
arXiv:2410.18742v3 Announce Type: replace 
Abstract: Popularity prediction for information cascades has significant applications across various domains, including opinion monitoring and advertising recommendations. While most existing methods consider this as a discrete problem, popularity actually evolves continuously, exhibiting rich dynamic properties such as change rates and growth patterns. In this paper, we argue that popularity trajectory prediction is more practical, as it aims to forecast the entire trajectory of how popularity unfolds over arbitrary future time. This approach offers insights into both instantaneous popularity and the underlying dynamic properties. However, traditional methods for popularity trajectory prediction primarily rely on specific diffusion mechanism assumptions, which may not align well with real-world dynamics and compromise their performance. To address these limitations, we propose NODEPT, a novel approach based on neural ordinary differential equations (ODEs) for popularity trajectory prediction. NODEPT models the continuous dynamics of the underlying diffusion system using neural ODEs. We first employ an encoder to initialize the latent state representations of information cascades, consisting of two representation learning modules that capture the co-evolution structural characteristics and temporal patterns of cascades from different perspectives. More importantly, we then introduce an ODE-based generative module that learns the dynamics of the diffusion system in the latent space. Finally, a decoder transforms the latent state into the prediction of the future popularity trajectory. Our experimental results on three real-world datasets demonstrate the superiority and rationality of the proposed NODEPT method.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moving towards informative and actionable social media research</title>
<link>https://arxiv.org/abs/2505.09254</link>
<guid>https://arxiv.org/abs/2505.09254</guid>
<content:encoded><![CDATA[
<div> social media, societal impacts, causal effects, observational studies, randomized controlled trials
<br />
Summary: 
Social media's impact on society is a topic of ongoing concern, with issues ranging from mental health to democratic disruption. Research on the causal effects of social media has produced conflicting results, with observational studies showing concerning associations and randomized controlled trials often yielding small or null results. The complexity of social media as a system presents challenges for inferring causality at societal scales, leading to debate over the severity of its impacts. To address these challenges, a new approach is proposed that combines the strengths of both observational and experimental methods while acknowledging the limitations of each. Drawing on insights from disciplines like climate science and epidemiology, this proposed approach seeks to provide a more comprehensive understanding of social media's societal effects. 
<br /> 
Summary: <div>
arXiv:2505.09254v2 Announce Type: replace 
Abstract: Social media is nearly ubiquitous in modern life, raising concerns about its societal impacts-from mental health and polarization to violence and democratic disruption. Yet research on its causal effects remains inconclusive: observational studies often find concerning associations, while randomized controlled trials (RCTs) tend to yield small, conflicting, or null results. Literature summaries tend to causally prioritize findings from RCTs, often arguing that concerns about social media are overstated. However, like observational studies, RCTs rely on assumptions that can easily be violated in the context of social media, especially regarding societal outcomes at scale. Here, we enumerate and examine the features of social media as a complex system that challenge our ability to infer causality at societal scales. Drawing on insight from disciplines that have faced similar challenges, like climate-science or epidemiology, we propose a path forward that combines the strength of observational and experimental approaches while acknowledging the limitations of each.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-based user embedding for competing events on social media</title>
<link>https://arxiv.org/abs/2308.14806</link>
<guid>https://arxiv.org/abs/2308.14806</guid>
<content:encoded><![CDATA[
<div> Keywords: social media analysis, user embedding, URL domain, COVID-19 infodemic, Twitter users

Summary: 
In this study, the researchers focused on understanding social divide and polarization through social media analysis in computational social science. They developed a method for embedding users based on a URL domain co-occurrence network, which effectively captures the complex characteristics of social media users involved in competing events. The performance of this method was assessed using binary classification tasks and datasets related to COVID-19 infodemic topics on Twitter. The results showed that domain-based embeddings outperformed user embeddings generated from the retweet network and language-based methods, while also reducing computation time. The findings indicate that domain-based embedding is an accessible and effective approach for characterizing social media users in competitive events such as political campaigns and public health crises.<br /><br />Summary: <div>
arXiv:2308.14806v3 Announce Type: replace-cross 
Abstract: Social divide and polarization have become significant societal issues. To understand the mechanisms behind these phenomena, social media analysis offers research opportunities in computational social science, where developing effective user embedding methods is essential for subsequent analysis. Traditionally, researchers have used predefined network-based user features (e.g., network size, degree, and centrality measures). However, because such measures may not capture the complex characteristics of social media users, in our study we developed a method for embedding users based on a URL domain co-occurrence network. This approach effectively represents social media users involved in competing events such as political campaigns and public health crises. We assessed the method's performance using binary classification tasks and datasets that covered topics associated with the COVID-19 infodemic, such as QAnon, Biden, and Ivermectin, among Twitter users. Our results revealed that user embeddings generated directly from the retweet network and/or based on language performed below expectations, whereas our domain-based embeddings outperformed those methods while reducing computation time. Therefore, domain-based embedding offers an accessible and effective method for characterizing social media users in competing events.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving interdisciplinary contributions to global societal challenges: A 50-year overview</title>
<link>https://arxiv.org/abs/2410.20619</link>
<guid>https://arxiv.org/abs/2410.20619</guid>
<content:encoded><![CDATA[
<div> societal challenges, interdisciplinary collaboration, Sustainable Development Goals, bibliometric data, research fields <br />
Summary:<br />
The study examines the impact of interdisciplinary collaborations on addressing the Sustainable Development Goals (SDGs) set by the United Nations. It analyzes data from 19 research fields from 1970 to 2022, revealing changes in the involvement of different disciplines in tackling specific SDGs over time. The study shows an increasing interconnectedness between fields since the 2000s, aligning with the UN's push for interdisciplinary approaches to global challenges. The findings offer valuable insights for policymakers and practitioners as they assess past achievements and plan for the upcoming SDG target deadline in five years. This research fills a critical gap in quantitative evidence on the role of cross-disciplinary efforts in addressing societal challenges and highlights the evolving relevance of specific disciplines in achieving sustainable development goals. <br /> <div>
arXiv:2410.20619v2 Announce Type: replace-cross 
Abstract: Addressing global societal challenges necessitates insights and expertise that transcend the boundaries of individual disciplines. In recent decades, interdisciplinary collaboration has been recognised as a vital driver of innovation and effective problem-solving, with the potential to profoundly influence policy and practice worldwide. However, quantitative evidence remains limited regarding how cross-disciplinary efforts contribute to societal challenges, as well as the evolving roles and relevance of specific disciplines in addressing these issues. To fill this gap, this study examines the long-term evolution of interdisciplinary contributions to the United Nations' Sustainable Development Goals (SDGs), drawing on extensive bibliometric data from OpenAlex. By analysing publication and citation trends across 19 research fields from 1970 to 2022, we reveal how the relative presence of different disciplines in addressing particular SDGs has shifted over time. Our results also provide unique evidence of the increasing interconnection between fields since the 2000s, coinciding with the United Nations' initiative to tackle global societal challenges through interdisciplinary efforts. These insights will benefit policymakers and practitioners as they reflect on past progress and plan for future action, particularly with the SDG target deadline approaching in the next five years.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Graph Anomaly Detection via Test-Time Training with Homophily-Guided Self-Supervision</title>
<link>https://arxiv.org/abs/2502.14293</link>
<guid>https://arxiv.org/abs/2502.14293</guid>
<content:encoded><![CDATA[
<div> unsupervised learning, graph anomaly detection, cross-domain, self-supervision, attention

Summary:
GADT3 is a novel framework for Graph Anomaly Detection (GAD) that addresses the challenges of cross-domain detection where labeled anomalies are scarce. It combines supervised and self-supervised learning during training and leverages self-supervised learning at test time to adapt to new domains. Key innovations include an effective self-supervision scheme, dynamic edge importance weighting, domain-specific encoders for heterogeneous features, and class-aware regularization for imbalance. Experimental results across multiple cross-domain settings show GADT3 outperforms existing approaches with average improvements of over 8.2% in AUROC and AUPRC. This framework demonstrates the effectiveness of combining supervised and self-supervised learning for GAD and highlights the importance of addressing distribution shifts and heterogeneous feature spaces in cross-domain anomaly detection.<br /><br />Summary: <div>
arXiv:2502.14293v2 Announce Type: replace-cross 
Abstract: Graph Anomaly Detection (GAD) has demonstrated great effectiveness in identifying unusual patterns within graph-structured data. However, while labeled anomalies are often scarce in emerging applications, existing supervised GAD approaches are either ineffective or not applicable when moved across graph domains due to distribution shifts and heterogeneous feature spaces. To address these challenges, we present GADT3, a novel test-time training framework for cross-domain GAD. GADT3 combines supervised and self-supervised learning during training while adapting to a new domain during test time using only self-supervised learning by leveraging a homophily-based affinity score that captures domain-invariant properties of anomalies. Our framework introduces four key innovations to cross-domain GAD: an effective self-supervision scheme, an attention-based mechanism that dynamically learns edge importance weights during message passing, domain-specific encoders for handling heterogeneous features, and class-aware regularization to address imbalance. Experiments across multiple cross-domain settings demonstrate that GADT3 significantly outperforms existing approaches, achieving average improvements of over 8.2\% in AUROC and AUPRC compared to the best competing model.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media</title>
<link>https://arxiv.org/abs/2504.12325</link>
<guid>https://arxiv.org/abs/2504.12325</guid>
<content:encoded><![CDATA[
<div> taxonomy, social media, large language models, factual claims, evaluation

Summary:
LLMTaxo is a framework that utilizes large language models to automatically create hierarchical taxonomies of factual claims from social media content. The framework generates topics at different levels of granularity to reduce redundancy and enhance information accessibility. The paper introduces new taxonomy evaluation metrics to assess the effectiveness of the constructed taxonomies. Evaluations on various datasets show that GPT-4o mini model outperforms others in producing clear, coherent, and comprehensive taxonomies. LLMTaxo's flexibility and minimal need for manual intervention make it suitable for a range of applications across different domains. <div>
arXiv:2504.12325v2 Announce Type: replace-cross 
Abstract: With the rapid expansion of content on social media platforms, analyzing and comprehending online discourse has become increasingly complex. This paper introduces LLMTaxo, a novel framework leveraging large language models for the automated construction of taxonomies of factual claims from social media by generating topics at multiple levels of granularity. The resulting hierarchical structure significantly reduces redundancy and improves information accessibility. We also propose dedicated taxonomy evaluation metrics to enable comprehensive assessment. Evaluations conducted on three diverse datasets demonstrate LLMTaxo's effectiveness in producing clear, coherent, and comprehensive taxonomies. Among the evaluated models, GPT-4o mini consistently outperforms others across most metrics. The framework's flexibility and low reliance on manual intervention underscore its potential for broad applicability.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalizable Rhetorical Strategy Annotation Model Using LLM-based Debate Simulation and Labelling</title>
<link>https://arxiv.org/abs/2510.15081</link>
<guid>https://arxiv.org/abs/2510.15081</guid>
<content:encoded><![CDATA[
<div> Keywords: rhetorical strategies, persuasive communication, large language models, transformer-based classifiers, U.S. Presidential debates

Summary:
This study introduces a novel framework that utilizes large language models (LLMs) to automatically generate and label synthetic debate data based on a four-part rhetorical typology. The framework aims to address the limitations of human annotation in analyzing rhetorical strategies, offering a scalable and consistent alternative. By fine-tuning transformer-based classifiers on this labeled dataset, the model demonstrates high performance and generalization across various topical domains. The study showcases two applications of the fine-tuned model, including improved prediction of persuasiveness by incorporating rhetorical strategy labels and analyzing temporal and partisan shifts in rhetorical strategies in U.S. Presidential debates from 1960 to 2020. The findings reveal a notable increase in the use of affective arguments over cognitive ones in U.S. Presidential debates over the years.<br /><br />Summary: <div>
arXiv:2510.15081v1 Announce Type: cross 
Abstract: Rhetorical strategies are central to persuasive communication, from political discourse and marketing to legal argumentation. However, analysis of rhetorical strategies has been limited by reliance on human annotation, which is costly, inconsistent, difficult to scale. Their associated datasets are often limited to specific topics and strategies, posing challenges for robust model development. We propose a novel framework that leverages large language models (LLMs) to automatically generate and label synthetic debate data based on a four-part rhetorical typology (causal, empirical, emotional, moral). We fine-tune transformer-based classifiers on this LLM-labeled dataset and validate its performance against human-labeled data on this dataset and on multiple external corpora. Our model achieves high performance and strong generalization across topical domains. We illustrate two applications with the fine-tuned model: (1) the improvement in persuasiveness prediction from incorporating rhetorical strategy labels, and (2) analyzing temporal and partisan shifts in rhetorical strategies in U.S. Presidential debates (1960-2020), revealing increased use of affective over cognitive argument in U.S. Presidential debates.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis</title>
<link>https://arxiv.org/abs/2510.15125</link>
<guid>https://arxiv.org/abs/2510.15125</guid>
<content:encoded><![CDATA[
<div> Topic taxonomy, social media, political discourse, large language model, political ads <br />
Summary: 
The article introduces a framework for generating a topic taxonomy from unlabeled social media content, specifically analyzing Meta political ads from the 2024 U.S. Presidential election. The framework combines clustering and prompt-based labeling using large language models to reveal discourse structures and annotate topics with moral framing dimensions. The analysis shows that voting and immigration ads dominate spending and impressions, while abortion and election-integrity ads have significant reach. Funding patterns are polarized, with economic appeals driven by conservative PACs and abortion messaging split between pro- and anti-rights coalitions. The framing of ads varies, with abortion messages emphasizing liberty/oppression and economic ads blending care/harm, fairness/cheating, and liberty/oppression narratives. Topic salience reveals correlations between moral foundations and issues, and demographic targeting is evident in the messaging. This framework provides scalable and interpretable ways to analyze political messaging on social media, aiding in understanding emerging narratives, polarization dynamics, and moral underpinnings of digital political communication. <br /><br />Summary: <div>
arXiv:2510.15125v1 Announce Type: cross 
Abstract: Social media platforms play a pivotal role in shaping political discourse, but analyzing their vast and rapidly evolving content remains a major challenge. We introduce an end-to-end framework for automatically generating an interpretable topic taxonomy from an unlabeled corpus. By combining unsupervised clustering with prompt-based labeling, our method leverages large language models (LLMs) to iteratively construct a taxonomy without requiring seed sets or domain expertise. We apply this framework to a large corpus of Meta (previously known as Facebook) political ads from the month ahead of the 2024 U.S. Presidential election. Our approach uncovers latent discourse structures, synthesizes semantically rich topic labels, and annotates topics with moral framing dimensions. We show quantitative and qualitative analyses to demonstrate the effectiveness of our framework. Our findings reveal that voting and immigration ads dominate overall spending and impressions, while abortion and election-integrity achieve disproportionate reach. Funding patterns are equally polarized: economic appeals are driven mainly by conservative PACs, abortion messaging splits between pro- and anti-rights coalitions, and crime-and-justice campaigns are fragmented across local committees. The framing of these appeals also diverges--abortion ads emphasize liberty/oppression rhetoric, while economic messaging blends care/harm, fairness/cheating, and liberty/oppression narratives. Topic salience further reveals strong correlations between moral foundations and issues. Demographic targeting also emerges. This work supports scalable, interpretable analysis of political messaging on social media, enabling researchers, policymakers, and the public to better understand emerging narratives, polarization dynamics, and the moral underpinnings of digital political communication.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Murals to Memes: A Theory of Aesthetic Asymmetry in Political Mobilization</title>
<link>https://arxiv.org/abs/2510.15256</link>
<guid>https://arxiv.org/abs/2510.15256</guid>
<content:encoded><![CDATA[
<div> Keywords: left-wing movements, right-wing movements, participatory art forms, digital culture, aesthetic asymmetry
Summary:
The article explores the historical integration of participatory art forms in left-wing movements and the prioritization of strategic communication and digital memes in right-wing movements. It introduces the concept of aesthetic asymmetry, attributing the difference to organizational ecosystems, moral frameworks, material supports, and historical traditions. Left-wing movements use art to build community and hope, while the contemporary right exploits art to mobilize divisive emotions like humor and resentment. The aesthetic logic aligns with each pole's strategic goals. The article provides a prescriptive model for artistic action, emphasizing emotional, narrative, and formatting strategies for effective mobilization. Understanding this asymmetry is crucial for analyzing political communication and designing cultural interventions for significant social change. 
<br /><br />Summary: <div>
arXiv:2510.15256v1 Announce Type: cross 
Abstract: Why have left-wing movements historically integrated participatory art forms (such as murals and protest songs) into their praxis, while right-wing movements have prioritized strategic communication and, more recently, the digital culture of memes? This article introduces the concept of aesthetic asymmetry to explain this divergence in political action. We argue that the asymmetry is not coincidental but the result of four interconnected structural factors: the organizational ecosystem, the moral and emotional framework, the material supports, and the historical tradition of each political spectrum. While the left tends to use art in a constitutive manner to forge community, solidarity, and hope, the contemporary right tends to use it instrumentally to mobilize polarizing affects such as humor and resentment. Drawing on comparative literature from the Theatre of the Oppressed to analyses of alt-right meme wars, we nuance this distinction and show how the aesthetic logic of each pole aligns with its strategic objectives. The article culminates in a prescriptive model for artistic action, synthesizing keys to effective mobilization into emotional, narrative, and formatting strategies. Understanding this asymmetry is crucial for analyzing political communication and for designing cultural interventions capable of generating profound social change.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VERA-MH Concept Paper</title>
<link>https://arxiv.org/abs/2510.15297</link>
<guid>https://arxiv.org/abs/2510.15297</guid>
<content:encoded><![CDATA[
<div> Keywords: VERA-MH, AI chatbots, mental health, suicide risk, automated evaluation

Summary: 
VERA-MH is a new automated evaluation tool for AI chatbots used in mental health contexts, with a focus on assessing suicide risk. Developed by clinicians and experts, VERA-MH utilizes user-agent and judge-agent AI models to simulate conversations between users and chatbots, scoring them based on a rubric informed by best practices in suicide risk management. The tool is currently undergoing rigorous validation to ensure realistic user-agent behavior and accurate scoring by the judge-agent. Preliminary evaluations have been conducted on existing AI chatbots, with further refinement and validation planned. Feedback from the community is being sought on both technical and clinical aspects of the evaluation process. <br /><br />Summary: <div>
arXiv:2510.15297v1 Announce Type: cross 
Abstract: We introduce VERA-MH (Validation of Ethical and Responsible AI in Mental Health), an automated evaluation of the safety of AI chatbots used in mental health contexts, with an initial focus on suicide risk.
  Practicing clinicians and academic experts developed a rubric informed by best practices for suicide risk management for the evaluation. To fully automate the process, we used two ancillary AI agents. A user-agent model simulates users engaging in a mental health-based conversation with the chatbot under evaluation. The user-agent role-plays specific personas with pre-defined risk levels and other features. Simulated conversations are then passed to a judge-agent who scores them based on the rubric. The final evaluation of the chatbot being tested is obtained by aggregating the scoring of each conversation.
  VERA-MH is actively under development and undergoing rigorous validation by mental health clinicians to ensure user-agents realistically act as patients and that the judge-agent accurately scores the AI chatbot. To date we have conducted preliminary evaluation of GPT-5, Claude Opus and Claude Sonnet using initial versions of the VERA-MH rubric and used the findings for further design development. Next steps will include more robust clinical validation and iteration, as well as refining actionable scoring. We are seeking feedback from the community on both the technical and clinical aspects of our evaluation.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Proactive Defense Against Cyber Cognitive Attacks</title>
<link>https://arxiv.org/abs/2510.15801</link>
<guid>https://arxiv.org/abs/2510.15801</guid>
<content:encoded><![CDATA[
<div> AI-driven disinformation, synthetic media, cognitive attacks, predictive methodology, proactive defense strategies
<br />
Summary:
This paper introduces a novel predictive methodology for forecasting disruptive innovations (DIs) and their malicious uses in cognitive attacks. It addresses the gaps in current studies by identifying trends in adversarial tactics and proposing proactive defense strategies. Cyber cognitive attacks exploit psychological biases to manipulate decision-making processes, with emerging technologies like AI-driven disinformation and synthetic media increasing the scale and sophistication of these threats. The paper highlights the need for proactive defense measures to anticipate future DIs and mitigate their malicious use in cognitive attacks. By understanding the evolving landscape of adversarial tactics, organizations can better prepare and defend against cyber cognitive attacks. <div>
arXiv:2510.15801v1 Announce Type: cross 
Abstract: Cyber cognitive attacks leverage disruptive innovations (DIs) to exploit psychological biases and manipulate decision-making processes. Emerging technologies, such as AI-driven disinformation and synthetic media, have accelerated the scale and sophistication of these threats. Prior studies primarily categorize current cognitive attack tactics, lacking predictive mechanisms to anticipate future DIs and their malicious use in cognitive attacks. This paper addresses these gaps by introducing a novel predictive methodology for forecasting the emergence of DIs and their malicious uses in cognitive attacks. We identify trends in adversarial tactics and propose proactive defense strategies.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Engagement Effectiveness of Cyber Cognitive Attacks: A Behavioral Metric for Disinformation Campaigns</title>
<link>https://arxiv.org/abs/2510.15805</link>
<guid>https://arxiv.org/abs/2510.15805</guid>
<content:encoded><![CDATA[
<div> Keywords: disinformation, cognitive attacks, engagement effectiveness, social media platforms, malicious influence<br />
<br />
Summary: 
This paper introduces a novel framework for measuring the engagement effectiveness of cognitive attacks in the context of cybersecurity defense strategies. The proposed weighted interaction metric takes into account both the type and volume of user engagement as compared to the number of attacker-generated transmissions. By applying this model to real-world disinformation campaigns on social media platforms, the metric is shown to not only capture reach but also the behavioral depth of user engagement. The findings offer valuable insights into the behavioral dynamics of cognitive warfare and provide actionable tools for researchers and practitioners to assess and combat the spread of malicious influence online. <div>
arXiv:2510.15805v1 Announce Type: cross 
Abstract: As disinformation-driven cognitive attacks become increasingly sophisticated, the ability to quantify their impact is essential for advancing cybersecurity defense strategies. This paper presents a novel framework for measuring the engagement effectiveness of cognitive attacks by introducing a weighted interaction metric that accounts for both the type and volume of user engagement relative to the number of attacker-generated transmissions. Applying this model to real-world disinformation campaigns across social media platforms, we demonstrate how the metric captures not just reach but the behavioral depth of user engagement. Our findings provide new insights into the behavioral dynamics of cognitive warfare and offer actionable tools for researchers and practitioners seeking to assess and counter the spread of malicious influence online.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is missing from this picture? Persistent homology and mixup barcodes as a means of investigating negative embedding space</title>
<link>https://arxiv.org/abs/2510.14327</link>
<guid>https://arxiv.org/abs/2510.14327</guid>
<content:encoded><![CDATA[
<div> Keywords: informetrics, scientometrics, topological data analysis, persistent homology, mixup barcodes

Summary: 
Recent work in informetrics and scientometrics has led to the development of new metrics that address biases in citation metrics. This study proposes a novel approach using topological data analysis, specifically persistent homology and mixup barcodes, to analyze the negative space among document embeddings generated by topic models. By using top2vec to embed documents and topics in n-dimensional space, the study aims to identify holes in the embedding distribution to determine missing research context or innovation space. The analysis focuses on unobserved publications that represent research published before or after the training data used for top2vec. The study investigates how these unobserved publications integrate research topics on the periphery. This metric has potential applications in understanding the disruptiveness of research and quantifying conceptual novelty.  
<br /><br />Summary: <div>
arXiv:2510.14327v1 Announce Type: new 
Abstract: Recent work in the information sciences, especially informetrics and scientometrics, has made substantial contributions to the development of new metrics that eschew the intrinsic biases of citation metrics. This work has tended to employ either network scientific (topological) approaches to quantifying the disruptiveness of peer-reviewed research, or topic modeling approaches to quantifying conceptual novelty. We propose a combination of these approaches, investigating the prospect of topological data analysis (TDA), specifically persistent homology and mixup barcodes, as a means of understanding the negative space among document embeddings generated by topic models. Using top2vec, we embed documents and topics in n-dimensional space, we use persistent homology to identify holes in the embedding distribution, and then use mixup barcodes to determine which holes are being filled by a set of unobserved publications. In this case, the unobserved publications represent research that was published before or after the data used to train top2vec. We investigate the extent that negative embedding space represents missing context (older research) versus innovation space (newer research), and the extend that the documents that occupy this space represents integrations of the research topics on the periphery. Potential applications for this metric are discussed.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media</title>
<link>https://arxiv.org/abs/2510.14889</link>
<guid>https://arxiv.org/abs/2510.14889</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, suicidal ideation, peer interactions, DeBERTa-v3 model, early detection <br />
Summary: 
This study focuses on detecting suicidal ideation (SI) on social media through implicit signals. Many individuals do not explicitly disclose their distress, making early detection challenging. The researchers developed a computational framework using a user's posting history and interactions with peers to predict SI. By identifying top neighbors of a user and analyzing their interactions, they integrated signals in a DeBERTa-v3 model. In a Reddit study, this approach improved early and implicit SI detection by 15%. The findings suggest that peer interactions provide valuable predictive signals for detecting indirect and masked expressions of risk online. This research has implications for designing early detection systems that can capture implicit signs of SI in online environments. 
<br /><br />Summary: <div>
arXiv:2510.14889v1 Announce Type: new 
Abstract: On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection</title>
<link>https://arxiv.org/abs/2510.13837</link>
<guid>https://arxiv.org/abs/2510.13837</guid>
<content:encoded><![CDATA[
<div> Keywords: Hate speech detection, cultural awareness, data sparsity, label propagation, classification performance<br />
Summary:<br />
- The study addresses challenges in hate speech detection, such as biased training labels and varying interpretations of hate across cultures.
- A culture-aware framework is proposed to construct hate subspaces based on individuals' cultural attributes.
- The model tackles data sparsity by considering combinations of cultural attributes and utilizes label propagation to handle cultural entanglement and ambiguous labeling.
- The approach results in individual hate subspaces that improve classification performance.
- Experimental results demonstrate the method outperforms existing techniques by an average of 1.05% across all metrics.<br /> 

Summary: <div>
arXiv:2510.13837v1 Announce Type: cross 
Abstract: Hate speech detection has been extensively studied, yet existing methods often overlook a real-world complexity: training labels are biased, and interpretations of what is considered hate vary across individuals with different cultural backgrounds. We first analyze these challenges, including data sparsity, cultural entanglement, and ambiguous labeling. To address them, we propose a culture-aware framework that constructs individuals' hate subspaces. To alleviate data sparsity, we model combinations of cultural attributes. For cultural entanglement and ambiguous labels, we use label propagation to capture distinctive features of each combination. Finally, individual hate subspaces, which in turn can further enhance classification performance. Experiments show our method outperforms state-of-the-art by 1.05\% on average across all metrics.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Behavior in Crowdfunding: Insights from a Large-Scale Online Experiment</title>
<link>https://arxiv.org/abs/2510.14872</link>
<guid>https://arxiv.org/abs/2510.14872</guid>
<content:encoded><![CDATA[
<div> Keywords: crowdfunding, strategic behavior, risk aversion, mutual insurance, online experiment

Summary:
The study examines strategic behavior in crowdfunding through a large-scale online experiment, focusing on risk aversion and mutual insurance. Participants exhibited distinct behavior in crowdfunding compared to voting, showing a preference for following private signals. Higher signal accuracy decreased risk aversion but increased reliance on mutual insurance. However, increasing the participation threshold amplified risk aversion, possibly indicating cognitive constraints. Mutual insurance supported participation but may hinder information aggregation, especially with higher signal accuracy. The results confirm the impact of informational incentives on crowdfunding and highlight behavioral deviations that challenge standard models. These findings offer valuable insights for platform design and mechanism refinement in crowdfunding. 

<br /><br />Summary: <div>
arXiv:2510.14872v1 Announce Type: cross 
Abstract: This study examines strategic behavior in crowdfunding using a large-scale online experiment. Building on the model of Arieli et. al 2023, we test predictions about risk aversion (i.e., opting out despite seeing a positive private signal) and mutual insurance (i.e., opting in despite seeing a negative private signal) in a static, single-shot crowdfunding game, focusing on informational incentives rather than dynamic effects. Our results validate key theoretical predictions: crowdfunding mechanisms induce distinct strategic behaviors compared to voting, where participants are more likely to follow private signals (odds ratio: 0.139, $p < 0.001$). Additionally, the study demonstrates that higher signal accuracy (85\% vs. 55\%) decreases risk aversion (odds ratio: 0.414, $p = 0.024$) but increases reliance on mutual insurance (odds ratio: 2.532, $p = 0.026$). However, contrary to theory, increasing the required participation threshold (50\% to 80\%) amplifies risk aversion (odds ratio: 3.251, $p = 0.005$), which, pending further investigation, may indicate cognitive constraints.
  Furthermore, we show that while mutual insurance supports participation, it may hinder information aggregation, particularly as signal accuracy increases. These findings advance crowdfunding theory by confirming the impact of informational incentives and identifying behavioral deviations that challenge standard models, offering insights for platform design and mechanism refinement.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ideology and polarization set the agenda on social media</title>
<link>https://arxiv.org/abs/2412.05176</link>
<guid>https://arxiv.org/abs/2412.05176</guid>
<content:encoded><![CDATA[
<div> Twitter, social media, online discourse, ideological alignment, polarization 

Summary: 
The study examines large-scale Twitter data from global debates on Climate Change, COVID-19, and the Russo-Ukrainian War to analyze the structural dynamics of engagement. It finds that discussions are driven by shared ideological alignment rather than specific categories of actors. Users tend to form polarized communities based on their ideological stance, which transcends individual topics and reflects broader patterns of ideological divides. The influence of individual actors within these communities is secondary to the reinforcing effects of selective exposure and shared narratives. Overall, the results emphasize the central role of ideological alignment in shaping online discourse and information spread in polarized environments. <div>
arXiv:2412.05176v2 Announce Type: replace 
Abstract: The abundance of information on social media has reshaped public discussions, shifting attention to the mechanisms that drive online discourse. This study analyzes large-scale Twitter (now X) data from three global debates--Climate Change, COVID-19, and the Russo-Ukrainian War--to investigate the structural dynamics of engagement. Our findings reveal that discussions are not primarily shaped by specific categories of actors, such as media or activists, but by shared ideological alignment. Users consistently form polarized communities, where their ideological stance in one debate predicts their positions in others. This polarization transcends individual topics, reflecting a broader pattern of ideological divides. Furthermore, the influence of individual actors within these communities appears secondary to the reinforcing effects of selective exposure and shared narratives. Overall, our results underscore that ideological alignment, rather than actor prominence, plays a central role in structuring online discourse and shaping the spread of information in polarized environments.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the gender attrition gap in academic psychology</title>
<link>https://arxiv.org/abs/2510.13273</link>
<guid>https://arxiv.org/abs/2510.13273</guid>
<content:encoded><![CDATA[
<div> Keywords: women, social science disciplines, career trajectories, attrition rates, academic performance <br />
Summary: Women are underrepresented in senior positions in social science disciplines, including psychology, despite comprising the majority of researchers. A study of over 78,000 psychology researchers found that women had higher attrition rates than men, particularly in the early years post-first publication. Early-career retention was strongly correlated with academic performance, specifically first-authored publications. Even after accounting for various factors, such as collaboration networks and institutional environment, women were more likely to leave academia, highlighting persistent barriers in their career progression. The study suggests that addressing the retention of female researchers at early career stages is crucial for promoting long-term gender equity in psychology and other social science disciplines. <br /><br /> <div>
arXiv:2510.13273v1 Announce Type: new 
Abstract: Although more women than men enter social science disciplines, they are underrepresented at senior levels. To investigate this leaky pipeline, this study analyzed the career trajectories of 78,216 psychology researchers using large-scale bibliometric data. Despite overall constituting over 60\% of these researchers, women experienced consistently higher attrition rates than men, particularly in the early years following their first publication. Academic performance, particularly first-authored publications, was strongly associated with early-career retention -- more so than collaboration networks or institutional environment. After controlling for gender differences in publication-, collaboration-, and institution-level factors, women remained more likely to leave academia, especially in early-career stages, pointing to persistent barriers that hinder women's academic careers. These findings suggest that in psychology and potentially other social science disciplines, the core challenge lies in retention rather than recruitment, underscoring the need for targeted, early-career interventions to promote long-term gender equity.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price-Pareto growth model of networks with community structure</title>
<link>https://arxiv.org/abs/2510.13392</link>
<guid>https://arxiv.org/abs/2510.13392</guid>
<content:encoded><![CDATA[
<div> Keywords: degree sequences, real-world networks, citations, scientific impact, community structure<br />
Summary:<br />
The article introduces a new analytical framework for modeling degree sequences in individual communities within real-world networks, specifically focusing on citations to papers in different fields. Inspired by Price's model and the 3DSI generalization, the framework considers citations as being gained accidentally and preferentially. It takes into account the varied growth ratios, average reference list lengths, and preferential citing tendencies across different scientific disciplines. By extending the 3DSI model to heterogeneous networks with a community structure, new analytical formulas for citation number inequality and preferentiality measures are devised. The distribution of citations in a community is found to tend towards a Pareto type II distribution, with analytical formulas provided for estimating its parameters and Gini's index. The model is validated using real citation networks. <div>
arXiv:2510.13392v1 Announce Type: cross 
Abstract: We introduce a new analytical framework for modelling degree sequences in individual communities of real-world networks, e.g., citations to papers in different fields. Our work is inspired by Price's model and its recent generalisation called 3DSI (three dimensions of scientific impact), which assumes that citations are gained partly accidentally, and to some extent preferentially. Our generalisation is motivated by existing research indicating significant differences between how various scientific disciplines grow, namely, minding different growth ratios, average reference list lengths, and preferential citing tendencies. Extending the 3DSI model to heterogeneous networks with a community structure allows us to devise new analytical formulas for, e.g., citation number inequality and preferentiality measures. We show that the distribution of citations in a community tends to a Pareto type II distribution. We also present analytical formulas for estimating its parameters and Gini's index. The new model is validated on real citation networks.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-asymptotic goodness-of-fit tests and model selection in valued stochastic blockmodels</title>
<link>https://arxiv.org/abs/2510.13636</link>
<guid>https://arxiv.org/abs/2510.13636</guid>
<content:encoded><![CDATA[
<div> SBM, stochastic blockmodel, dyad sampling, goodness-of-fit testing, Markov bases moves<br />
Summary:<br />
A valued stochastic blockmodel (SBM) framework is proposed for analyzing networked data with varying dyad sampling schemes. The paper focuses on testing the goodness-of-fit of non-Bernoulli SBMs, providing explicit Markov bases moves for generating reference distributions. Goodness-of-fit statistics are defined for model evaluation, specifically for labeled SBMs and censored-edge models. The asymptotic behavior of these statistics is studied, with simulations verifying power and Type 1 error rates. The method is applied to ecological networks, offering insights on selecting the number of blocks. The data analysis leads to novel conclusions on block selection for species in host-parasite interactions, differing from existing literature. The findings enhance understanding of how node block membership influences network formation in SBMs. <div>
arXiv:2510.13636v1 Announce Type: cross 
Abstract: A valued stochastic blockmodel (SBM) is a general way to view networked data in which nodes are grouped into blocks and links between them are measured by counts or labels. This family allows for varying dyad sampling schemes, thereby including the classical, Poisson, and labeled SBMs, as well as those in which some edge observations are censored. This paper addresses the question of testing goodness-of-fit of such non-Bernoulli SBMs, focusing in particular on finite-sample tests. We derive explicit Markov bases moves necessary to generate samples from reference distributions and define goodness-of-fit statistics for determining model fit, comparable to those in the literature for related model families.
  For the labeled SBM, which includes in particular the censored-edge model, we study the asymptotic behavior of said statistics. One of the main purposes of testing goodness-of-fit of an SBM is to determine whether block membership of the nodes influences network formation. Power and Type 1 error rates are verified on simulated data. Additionally, we discuss the use of asymptotic results in selecting the number of blocks under the latent-block modeling assumption. The method derived for Poisson SBM is applied to ecological networks of host-parasite interactions. Our data analysis conclusions differ in selecting the number of blocks for the species from previous results in the literature.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T3former: Temporal Graph Classification with Topological Machine Learning</title>
<link>https://arxiv.org/abs/2510.13789</link>
<guid>https://arxiv.org/abs/2510.13789</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal graph classification, Topological Temporal Transformer, Descriptor-Attention mechanism, Spectral descriptors, Cross-modal fusion

Summary:
T3former is introduced as a Topological Temporal Transformer for temporal graph classification, leveraging sliding-window topological and spectral descriptors with a specialized Descriptor-Attention mechanism. This approach preserves fine-grained temporal information, enhances robustness, and facilitates cross-modal fusion without rigid discretization. T3former achieves state-of-the-art performance on various benchmarks, including dynamic social networks, brain functional connectivity datasets, and traffic networks. It overcomes limitations of existing methods by addressing oversmoothing and oversquashing issues, allowing for better capture of complex temporal structures. The theoretical guarantees offered by T3former ensure stability under temporal and structural perturbations. Overall, the combination of topological and spectral insights in T3former demonstrates a significant advancement in the field of temporal graph learning. 

<br /><br />Summary: <br />T3former introduces a Topological Temporal Transformer for temporal graph classification, utilizing sliding-window topological and spectral descriptors with a specialized Descriptor-Attention mechanism. This approach enhances robustness, enables cross-modal fusion without rigid discretization, and achieves state-of-the-art performance on various benchmarks. T3former addresses limitations of existing methods by overcoming oversmoothing and oversquashing issues, allowing for better capture of complex temporal structures. The theoretical guarantees provided ensure stability under temporal and structural perturbations, highlighting the significance of combining topological and spectral insights in advancing temporal graph learning. <div>
arXiv:2510.13789v1 Announce Type: cross 
Abstract: Temporal graph classification plays a critical role in applications such as cybersecurity, brain connectivity analysis, social dynamics, and traffic monitoring. Despite its significance, this problem remains underexplored compared to temporal link prediction or node forecasting. Existing methods often rely on snapshot-based or recurrent architectures that either lose fine-grained temporal information or struggle with long-range dependencies. Moreover, local message-passing approaches suffer from oversmoothing and oversquashing, limiting their ability to capture complex temporal structures.
  We introduce T3former, a novel Topological Temporal Transformer that leverages sliding-window topological and spectral descriptors as first-class tokens, integrated via a specialized Descriptor-Attention mechanism. This design preserves temporal fidelity, enhances robustness, and enables principled cross-modal fusion without rigid discretization. T3former achieves state-of-the-art performance across multiple benchmarks, including dynamic social networks, brain functional connectivity datasets, and traffic networks. It also offers theoretical guarantees of stability under temporal and structural perturbations. Our results highlight the power of combining topological and spectral insights for advancing the frontier of temporal graph learning.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Ensemble-Based Semi-Supervised Learning for Illicit Account Detection in Ethereum DeFi Transactions</title>
<link>https://arxiv.org/abs/2412.02408</link>
<guid>https://arxiv.org/abs/2412.02408</guid>
<content:encoded><![CDATA[
<div> Keywords: smart contracts, decentralized finance, illicit account detection, Ethereum blockchain, SLEID framework

Summary: 
Smart contracts have enabled the rapid growth of Decentralized Finance (DeFi) on the Ethereum blockchain, but this expansion comes with security risks, including illicit accounts engaging in fraud. To address these challenges, the SLEID framework, a Self-Learning Ensemble-based Illicit account Detection system, has been proposed. SLEID uses an Isolation Forest model for outlier detection and a self-training mechanism to improve detection accuracy by generating pseudo-labels for unlabeled accounts. Experiments on Ethereum transactions show that SLEID outperforms traditional baselines, achieving higher precision, accuracy, and F1 scores, especially for detecting illicit accounts. The framework reduces the reliance on labeled data, making it a robust solution for safeguarding the DeFi ecosystem. <div>
arXiv:2412.02408v2 Announce Type: replace 
Abstract: The advent of smart contracts has enabled the rapid rise of Decentralized Finance (DeFi) on the Ethereum blockchain, offering substantial rewards in financial innovation and inclusivity. This growth, however, is accompanied by significant security risks such as illicit accounts engaged in fraud. Effective detection is further limited by the scarcity of labeled data and the evolving tactics of malicious accounts. To address these challenges with a robust solution for safeguarding the DeFi ecosystem, we propose $\textbf{SLEID}$, a $\textbf{S}$elf-$\textbf{L}$earning $\textbf{E}$nsemble-based $\textbf{I}$llicit account $\textbf{D}$etection framework. SLEID uses an Isolation Forest model for initial outlier detection and a self-training mechanism to iteratively generate pseudo-labels for unlabeled accounts, enhancing detection accuracy. Experiments on 6,903,860 Ethereum transactions with extensive DeFi interaction coverage demonstrate that SLEID significantly outperforms supervised and semi-supervised baselines with $\textbf{+2.56}$ percentage-point precision, comparable recall, and $\textbf{+0.90}$ percentage-point F1 -- particularly for the minority illicit class -- alongside $\textbf{+3.74}$ percentage-points higher accuracy and improvements in PR-AUC, while substantially reducing reliance on labeled data.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metrics for Assessing Inclusivity and Empowerment of People for Supporting the Design of Inclusive Product Lifecycles</title>
<link>https://arxiv.org/abs/2410.17287</link>
<guid>https://arxiv.org/abs/2410.17287</guid>
<content:encoded><![CDATA[
<div> Keywords: inclusive design, empowerment, stakeholders, lifecycle processes, sustainability

Summary: 
In this study, the focus is on designing an inclusive product lifecycle that empowers stakeholders through structured inclusion. The aim is to shift the focus from empowering users through products to empowering a wider range of stakeholders for sustainable development. By analyzing real-life case studies, a framework is applied to identify dimensions of empowerment and inclusivity and propose measurable metrics with strong causal connections. These metrics support fair stakeholder development and serve as a foundation for sustainability, dignity, well-being, and inclusivity in design processes. This study contributes to design science by expanding the understanding of inclusive design and emphasizing stakeholder inclusion in lifecycle phases. <br /><br />Summary: <div>
arXiv:2410.17287v2 Announce Type: replace-cross 
Abstract: The design of an inclusive product lifecycle is important for empowering stakeholders through their meaningful inclusion in lifecycle processes. To achieve this, the inclusion of stakeholders must be structured in a way that supports their empowerment. Inclusivity addresses the lifecycle context to improve how diverse stakeholders are included across phases, supporting their empowerment. This study aims to build on current inclusive design approaches, which often focus on empowering users through the use of products. It proposes inclusive lifecycle processes as a way to empower a wider range of stakeholders for sustainable development. We apply a novel framework to real-life case studies from the literature to identify the dimensions of empowerment and inclusivity. By analysing the relationships between these dimensions, we propose specific metrics that show strong causal connections. These metrics allow measurable outcomes to support the fair development of stakeholders. Measuring inclusivity and empowerment can serve as a foundation for supporting sustainability, dignity, well-being, and fair stakeholder development, which are explored further in Part 2 and Part 3 of this study. This study contributes to design science by expanding the understanding of inclusive design through stakeholder inclusion in lifecycle phases and by shifting the focus from product to process design.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Quadratic Penalty Method for a Class of Graph Clustering Problems</title>
<link>https://arxiv.org/abs/2501.02187</link>
<guid>https://arxiv.org/abs/2501.02187</guid>
<content:encoded><![CDATA[
<div> Keywords: community-based graph clustering, semi-assignment problems, sparse-constrained optimization, quadratic penalty method, real-world network datasets

Summary: 
Community-based graph clustering is a popular method in analyzing complex social networks. This clustering technique groups vertices based on their connections, forming densely connected subgraphs. The clustering problem is formulated as semi-assignment problems with block properties in the objective function. The problems are reformulated as sparse-constrained optimization models and relaxed to continuous optimization models. The quadratic penalty method and quadratic penalty regularized method are applied to solve these problems efficiently. Numerical experiments show both methods effectively solve graph clustering tasks for synthetic and real-world network datasets. The quadratic penalty regularized method is more efficient for small-scale problems, while the quadratic penalty method is better suited for large-scale cases.<br /><br />Summary: <div>
arXiv:2501.02187v2 Announce Type: replace-cross 
Abstract: Community-based graph clustering is one of the most popular topics in the analysis of complex social networks. This type of clustering involves grouping vertices that are considered to share more connections, whereas vertices in different groups share fewer connections. A successful clustering result forms densely connected induced subgraphs. This paper studies a specific form of graph clustering problems that can be formulated as semi-assignment problems, where the objective function exhibits block properties. We reformulate these problems as sparse-constrained optimization problems and relax them to continuous optimization models. We then apply the quadratic penalty method and the quadratic penalty regularized method to the relaxation problem, respectively. Extensive numerical experiments demonstrate that both methods effectively solve graph clustering tasks for both synthetic and real-world network datasets. For small-scale problems, the quadratic penalty regularized method demonstrates greater efficiency, whereas the quadratic penalty method proves more suitable for large-scale cases.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Hypergraph Using Large Language Models</title>
<link>https://arxiv.org/abs/2510.11728</link>
<guid>https://arxiv.org/abs/2510.11728</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, clustering, neural networks, computer vision, large language models

Summary: 
The paper discusses the scarcity of real-world hypergraph datasets, limiting the development of advanced hypergraph learning algorithms. To address this issue, the authors propose HyperLLM, a novel hypergraph generator driven by large language models (LLMs). HyperLLM simulates the formation and evolution of hypergraphs through multi-agent collaboration, integrating prompts and feedback mechanisms to ensure the generated hypergraphs reflect real-world patterns. Extensive experiments show that HyperLLM outperforms existing methods in capturing structural and temporal hypergraph patterns with minimal statistical priors. The study suggests that leveraging LLMs for hypergraph modeling presents a promising new approach. <div>
arXiv:2510.11728v1 Announce Type: new 
Abstract: Due to the advantages of hypergraphs in modeling high-order relationships in complex systems, they have been applied to higher-order clustering, hypergraph neural networks and computer vision. These applications rely heavily on access to high-quality, large-scale real-world hypergraph data. Yet, compared to traditional pairwise graphs, real hypergraph datasets remain scarce in both scale and diversity. This shortage significantly limits the development and evaluation of advanced hypergraph learning algorithms. Therefore, how to quickly generate large-scale hypergraphs that conform to the characteristics of real networks is a crucial task that has not received sufficient attention. Motivated by recent advances in large language models (LLMs), particularly their capabilities in semantic reasoning, structured generation, and simulating human behavior, we investigate whether LLMs can facilitate hypergraph generation from a fundamentally new perspective. We introduce HyperLLM, a novel LLM-driven hypergraph generator that simulates the formation and evolution of hypergraphs through a multi-agent collaboration. The framework integrates prompts and structural feedback mechanisms to ensure that the generated hypergraphs reflect key real-world patterns. Extensive experiments across diverse datasets demonstrate that HyperLLM achieves superior fidelity to structural and temporal hypergraph patterns, while requiring minimal statistical priors. Our findings suggest that LLM-based frameworks offer a promising new direction for hypergraph modeling.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed</title>
<link>https://arxiv.org/abs/2510.11739</link>
<guid>https://arxiv.org/abs/2510.11739</guid>
<content:encoded><![CDATA[
<div> machine learning, deep learning, celebrity profiling, Urdu language, demographic prediction

Summary:
This study explores the use of modern machine learning and deep learning techniques to predict celebrity demographics based on follower data in Urdu, a low-resource language. The study collected and preprocessed a dataset of short Urdu tweets from followers of subcontinent celebrities. Various algorithms, including Logistic Regression, Support Vector Machines, Random Forests, Convolutional Neural Networks, and Long Short-Term Memory networks, were trained and compared for gender, age, profession, and fame prediction. The best performance was achieved in gender prediction with a cRank of 0.65 and an accuracy of 0.65, showcasing the effectiveness of leveraging follower-based linguistic features for demographic prediction in Urdu. The study demonstrates the potential for using machine learning and neural approaches in analyzing social media data in low-resource languages like Urdu. 

<br /><br />Summary: <div>
arXiv:2510.11739v1 Announce Type: new 
Abstract: Social media has become an essential part of the digital age, serving as a platform for communication, interaction, and information sharing. Celebrities are among the most active users and often reveal aspects of their personal and professional lives through online posts. Platforms such as Twitter provide an opportunity to analyze language and behavior for understanding demographic and social patterns. Since followers frequently share linguistic traits and interests with the celebrities they follow, textual data from followers can be used to predict celebrity demographics. However, most existing research in this field has focused on English and other high-resource languages, leaving Urdu largely unexplored.
  This study applies modern machine learning and deep learning techniques to the problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from followers of subcontinent celebrities was collected and preprocessed. Multiple algorithms were trained and compared, including Logistic Regression, Support Vector Machines, Random Forests, Convolutional Neural Networks, and Long Short-Term Memory networks. The models were evaluated using accuracy, precision, recall, F1-score, and cumulative rank (cRank). The best performance was achieved for gender prediction with a cRank of 0.65 and an accuracy of 0.65, followed by moderate results for age, profession, and fame prediction. These results demonstrate that follower-based linguistic features can be effectively leveraged using machine learning and neural approaches for demographic prediction in Urdu, a low-resource language.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of wartime discourse on Telegram: A comparative study of Ukrainian and Russian policymakers' communication before and after Russia's full-scale invasion of Ukraine</title>
<link>https://arxiv.org/abs/2510.11746</link>
<guid>https://arxiv.org/abs/2510.11746</guid>
<content:encoded><![CDATA[
<div> Telegram, political communication, Russo-Ukrainian war, policymakers, social media <br />
<br />
Summary: This study analyzes political communication on Telegram by Ukrainian and Russian policymakers during the Russo-Ukrainian war. Following Russia's invasion in 2022, there was a significant increase in Telegram activity, with Ukrainian policymakers focusing initially on war-related topics that later declined. In contrast, Russian policymakers avoided war-related discussions, instead emphasizing unrelated topics to distract the public. Differences in communication strategies were observed between large and small parties, as well as individual policymakers. The study sheds light on how policymakers adapt to wartime communication challenges and provides insights into online political discourse dynamics during times of war. <div>
arXiv:2510.11746v1 Announce Type: new 
Abstract: This study examines elite-driven political communication on Telegram during the ongoing Russo-Ukrainian war, the first large-scale European war in the social media era. Using a unique dataset of Telegram public posts from Ukrainian and Russian policymakers (2019-2024), we analyze changes in communication volume, thematic content, and actor engagement following Russia's 2022 full-scale invasion. Our findings show a sharp increase in Telegram activity after the invasion, particularly among ruling-party policymakers. Ukrainian policymakers initially focused on war-related topics, but this emphasis declined over time In contrast, Russian policymakers largely avoided war-related discussions, instead emphasizing unrelated topics, such as Western crises, to distract public attention. We also identify differences in communication strategies between large and small parties, as well as individual policymakers. Our findings shed light on how policymakers adapt to wartime communication challenges and offer critical insights into the dynamics of online political discourse during times of war.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-aware Propagation Generation with Large Language Models for Fake News Detection</title>
<link>https://arxiv.org/abs/2510.12125</link>
<guid>https://arxiv.org/abs/2510.12125</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, social media, propagation-based methods, large language models, structural patterns

Summary: 
The paper discusses the challenges posed by the spread of fake news on social media and proposes a novel framework, StruSP, for improving fake news detection. By integrating synthetic propagation with real-world structural patterns, StruSP enhances the realism and effectiveness of fake news detection. The framework utilizes a bidirectional evolutionary propagation (BEP) learning strategy to align large language models (LLMs) with the structural dynamics of real propagation. Experimental results on three public datasets demonstrate the superior performance of StruSP in detecting fake news in various scenarios. The BEP strategy enables LLMs to generate more realistic and diverse propagation, resulting in improved detection accuracy. Overall, StruSP offers a promising approach to address the significant challenges posed by the spread of fake news on social media platforms. 

<br /><br />Summary: <div>
arXiv:2510.12125v1 Announce Type: new 
Abstract: The spread of fake news on social media poses a serious threat to public trust and societal stability. While propagation-based methods improve fake news detection by modeling how information spreads, they often suffer from incomplete propagation data. Recent work leverages large language models (LLMs) to generate synthetic propagation, but typically overlooks the structural patterns of real-world discussions. In this paper, we propose a novel structure-aware synthetic propagation enhanced detection (StruSP) framework to fully capture structural dynamics from real propagation. It enables LLMs to generate realistic and structurally consistent propagation for better detection. StruSP explicitly aligns synthetic propagation with real-world propagation in both semantic and structural dimensions. Besides, we also design a new bidirectional evolutionary propagation (BEP) learning strategy to better align LLMs with structural patterns of propagation in the real world via structure-aware hybrid sampling and masked propagation modeling objective. Experiments on three public datasets demonstrate that StruSP significantly improves fake news detection performance in various practical detection scenarios. Further analysis indicates that BEP enables the LLM to generate more realistic and diverse propagation semantically and structurally.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrisisNews: A Dataset Mapping Two Decades of News Articles on Online Problematic Behavior at Scale</title>
<link>https://arxiv.org/abs/2510.12243</link>
<guid>https://arxiv.org/abs/2510.12243</guid>
<content:encoded><![CDATA[
<div> Keywords: social media crises, problematic behaviors, mitigation strategies, stakeholder roles, news coverage

Summary: 
This article explores social media crises, defined as patterns of problematic behaviors on social media that escalate into larger-scale harms. The research examines 93,250 news articles over the past two decades to classify stakeholder roles, behavior types, and outcomes related to social media crises. By taking an event-focused perspective, the study aims to provide a more nuanced understanding of how these crises evolve beyond just analyzing user-generated content. The findings highlight the importance of developing proactive measures and strategies to mitigate social media crises and create safer online environments. By analyzing global news coverage, the research seeks to inform the design of platforms to foster trust and safety in the increasingly digital world.<br /><br />Summary: <div>
arXiv:2510.12243v1 Announce Type: new 
Abstract: As social media adoption grows globally, online problematic behaviors increasingly escalate into large-scale crises, requiring an evolving set of mitigation strategies. While HCI research often analyzes problematic behaviors with pieces of user-generated content as the unit of analysis, less attention has been given to event-focused perspectives that track how discrete events evolve. In this paper, we examine 'social media crises': discrete patterns of problematic behaviors originating and evolving within social media that cause larger-scale harms. Using global news coverage, we present a dataset of 93,250 news articles covering social media-endemic crises from the past 20 years. We analyze a representative subset to classify stakeholder roles, behavior types, and outcomes, uncovering patterns that inform more nuanced classification of social media crises beyond content-based descriptions. By adopting a wider perspective, this research seeks to inform the design of safer platforms, enabling proactive measures to mitigate crises and foster more trustworthy online environments.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOUFLON: Multi-group Modularity-based Fairness-aware Community Detection</title>
<link>https://arxiv.org/abs/2510.12348</link>
<guid>https://arxiv.org/abs/2510.12348</guid>
<content:encoded><![CDATA[
<div> Keywords: MOUFLON, fairness-aware, modularity-based community detection, proportional balance fairness metric, structural biases

Summary: 
MOUFLON is a new fairness-aware, modularity-based community detection method that allows for adjusting the balance between partition quality and fairness outcomes. The method introduces a novel proportional balance fairness metric that provides consistent scores across different network settings. The evaluation of MOUFLON on synthetic and real network datasets examines the trade-off between modularity and fairness in resulting communities, considering network characteristics such as size, density, and group distribution. The study also investigates scenarios with clustered homogeneous groups to understand the impact of structural biases on fairness outcomes. By incorporating fairness constraints into community detection, the research highlights the importance of addressing biases in social network analysis methods and provides valuable insights for designing and benchmarking fairness-aware algorithms.<br /><br />Summary: <div>
arXiv:2510.12348v1 Announce Type: new 
Abstract: In this paper, we propose MOUFLON, a fairness-aware, modularity-based community detection method that allows adjusting the importance of partition quality over fairness outcomes. MOUFLON uses a novel proportional balance fairness metric, providing consistent and comparable fairness scores across multi-group and imbalanced network settings. We evaluate our method under both synthetic and real network datasets, focusing on performance and the trade-off between modularity and fairness in the resulting communities, along with the impact of network characteristics such as size, density, and group distribution. As structural biases can lead to strong alignment between demographic groups and network structure, we also examine scenarios with highly clustered homogeneous groups, to understand how such structures influence fairness outcomes. Our findings showcase the effects of incorporating fairness constraints into modularity-based community detection, and highlight key considerations for designing and benchmarking fairness-aware social network analysis methods.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timeliness, Consensus, and Composition of the Crowd: Community Notes on X</title>
<link>https://arxiv.org/abs/2510.12559</link>
<guid>https://arxiv.org/abs/2510.12559</guid>
<content:encoded><![CDATA[
<div> Efficiency, Crowdsourcing, Moderation, Consensus, Timeliness
<br />
Summary: 
The study analyzes X's Community Notes, a crowdsourced moderation system. It reveals that a small group of top contributors dominate the system, with significant participation inequality (Gini Coefficient = 0.68). Consensus formation is rare, with only 11.5% of notes reaching agreement. Most posts receive conflicting classifications, with around 68% annotated as "Note Not Needed." Surprisingly, such posts are more likely to result in published notes. The average publication delay is 65.7 hours, and longer delays decrease the likelihood of consensus. The system is characterized by persistent dissensus and is repurposed for debate rather than moderation. The study suggests design strategies to enhance equity, faster consensus, and reliability in community-based moderation.
<br /> <div>
arXiv:2510.12559v1 Announce Type: new 
Abstract: This study presents the first large-scale quantitative analysis of the efficiency of X's Community Notes, a crowdsourced moderation system for identifying and contextualising potentially misleading content. Drawing on over 1.8 million notes, we examine three key dimensions of crowdsourced moderation: participation inequality, consensus formation, and timeliness. Despite the system's goal of collective moderation, we find substantial concentration effect, with the top 10% of contributors producing 58% of all notes (Gini Coefficient = 0.68). The observed consensus is rare-only 11.5% of notes reach agreement on publication, while 69% of posts receive conflicting classifications. A majority of noted posts (approximately 68%) are annotated as "Note Not Needed", reflecting the repurposing of the platform for debate rather than moderation. We found that such posts are paradoxically more likely to yield published notes (OR = 3.12). Temporal analyses show that the notes, on average, are published 65.7 hours after the original post, with longer delays significantly reducing the likelihood of consensus. These results portray Community Notes as a stratified, deliberative system dominated by a small contributor elite, marked by persistent dissensus, and constrained by timeliness. We conclude this study by outlining design strategies to promote equity, faster consensus, and epistemic reliability in community-based moderation.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Diameter of (Threshold) Geometric Inhomogeneous Random Graphs</title>
<link>https://arxiv.org/abs/2510.12543</link>
<guid>https://arxiv.org/abs/2510.12543</guid>
<content:encoded><![CDATA[
<div> GIRG, diameter, runtime, distributed protocols, real-world networks <br />
Summary: 
- The article proves that the diameter of threshold GIRG is in the order of log(n), impacting the runtime of distributed protocols that rely on diameter for efficiency.
- GIRG model shares empirical properties with real-world networks, making it a useful tool for analyzing algorithm performance.
- Previous knowledge of the diameter was limited to one-dimensional cases; this study extends the understanding to higher dimensions using novel methods.
- The proof utilizes Peierls-type argument and a renormalization scheme, avoiding complex topological arguments.
- The lower bound is established through a simple ad-hoc construction, providing insights into the structure of GIRG graphs. <br /> 

Summary: <div>
arXiv:2510.12543v1 Announce Type: cross 
Abstract: We prove that the diameter of threshold (zero temperature) Geometric Inhomogeneous Random Graphs (GIRG) is $\Theta(\log n)$. This has strong implications for the runtime of many distributed protocols on those graphs, which often have runtimes bounded as a function of the diameter.
  The GIRG model exhibits many properties empirically found in real-world networks, and the runtime of various practical algorithms has empirically been found to scale in the same way for GIRG and for real-world networks, in particular related to computing distances, diameter, clustering, cliques and chromatic numbers. Thus the GIRG model is a promising candidate for deriving insight about the performance of algorithms in real-world instances.
  The diameter was previously only known in the one-dimensional case, and the proof relied very heavily on dimension one. Our proof employs a similar Peierls-type argument alongside a novel renormalization scheme. Moreover, instead of using topological arguments (which become complicated in high dimensions) in establishing the connectivity of certain boundaries, we employ some comparatively recent and clearer graph-theoretic machinery. The lower bound is proven via a simple ad-hoc construction.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings</title>
<link>https://arxiv.org/abs/2510.12555</link>
<guid>https://arxiv.org/abs/2510.12555</guid>
<content:encoded><![CDATA[
<div> evolution, intelligence, multi-agent reinforcement learning, inclusive fitness, social dynamics <br />
<br />
Summary: Inspired by evolution, a novel multi-agent reinforcement learning framework is proposed, where each agent has a unique genotype linked to inclusive fitness-based reward functions. The framework mirrors natural selection dynamics, leading to the emergence of complex social behaviors resembling biological principles like Hamilton's rule. The study shows cooperative and competitive interactions in network games, indicating a spectrum of cooperation levels based on genetic similarity. This gene-based reward structure fosters non-team-based social dynamics, allowing mutual cooperation among some agents while adversarial behavior between others. The framework also suggests the potential for an arms race of strategies akin to biological evolution, leading to more socially intelligent agents. This approach lays the groundwork for the development of advanced and adaptive multi-agent systems. <br /> <div>
arXiv:2510.12555v1 Announce Type: cross 
Abstract: The competitive and cooperative forces of natural selection have driven the evolution of intelligence for millions of years, culminating in nature's vast biodiversity and the complexity of human minds. Inspired by this process, we propose a novel multi-agent reinforcement learning framework where each agent is assigned a genotype and where reward functions are modelled after the concept of inclusive fitness. An agent's genetic material may be shared with other agents, and our inclusive reward function naturally accounts for this. We study the resulting social dynamics in two types of network games with prisoner's dilemmas and find that our results align with well-established principles from biology, such as Hamilton's rule. Furthermore, we outline how this framework can extend to more open-ended environments with spatial and temporal structure, finite resources, and evolving populations. We hypothesize the emergence of an arms race of strategies, where each new strategy is a gradual improvement over earlier adaptations of other agents, effectively producing a multi-agent autocurriculum analogous to biological evolution. In contrast to the binary team-based structures prevalent in earlier research, our gene-based reward structure introduces a spectrum of cooperation ranging from full adversity to full cooperativeness based on genetic similarity, enabling unique non team-based social dynamics. For example, one agent having a mutual cooperative relationship with two other agents, while the two other agents behave adversarially towards each other. We argue that incorporating inclusive fitness in agents provides a foundation for the emergence of more strategically advanced and socially intelligent agents.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation</title>
<link>https://arxiv.org/abs/2504.16946</link>
<guid>https://arxiv.org/abs/2504.16946</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative agents, urban mobility, transportation system, agent profiles, computational efficiency<br />
Summary:<br />
The paper introduces MobileCity, a simulation platform focusing on modeling realistic urban mobility efficiently. It overcomes limitations in existing methods by incorporating a comprehensive transportation system with various transport modes and constructing agent profiles based on questionnaire data. To enhance scalability, agents in MobileCity select actions from a pre-generated space and utilize local memory models. Comparative evaluations involving 4,000 agents show that MobileCity outperforms baselines in generating realistic urban behaviors while maintaining computational efficiency. The platform's practical applications include predicting movement patterns and analyzing demographic trends in transportation preferences. The code for MobileCity is publicly accessible on GitHub at https://github.com/Tony-Yip/MobileCity. <br /> <div>
arXiv:2504.16946v3 Announce Type: replace 
Abstract: Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices, rely heavily on static agent profiles leading to behavioral homogenization, and inherit prohibitive computational costs. To address these limitations, we present MobileCity, a lightweight simulation platform designed to model realistic urban mobility with high computational efficiency. We introduce a comprehensive transportation system with multiple transport modes, and collect questionnaire data from respondents to construct agent profiles. To enable scalable simulation, agents perform action selection within a pre-generated action space and uses local models for efficient agent memory generation. Through extensive micro and macro-level evaluations on 4,000 agents, we demonstrate that MobileCity generates more realistic urban behaviors than baselines while maintaining computational efficiency. We further explore practical applications such as predicting movement patterns and analyzing demographic trends in transportation preferences. Our code is publicly available at https://github.com/Tony-Yip/MobileCity.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Relationship between Space-Time Accessibility and Leisure Activity Participation</title>
<link>https://arxiv.org/abs/2510.10307</link>
<guid>https://arxiv.org/abs/2510.10307</guid>
<content:encoded><![CDATA[
<div> space-time accessibility, leisure activities, capability approach, urban life, transport modes

Summary:
The study introduces a space-time accessibility (SPA) metric based on the capability approach to understand how accessibility influences participation in leisure activities in urban areas. Using GPS data from residents in the Paris region, the study assesses how SPA impacts total travel time and leisure participation diversity. Spatial analysis shows that individuals tend to choose destinations aligned with their SPA-defined opportunity sets, emphasizing the metric's validity. Structural equation modeling reveals that SPA directly enhances leisure diversity while reducing travel time, which in turn is linked to lower diversity. The findings underscore the importance of person-centered, capability-informed accessibility metrics in addressing inequalities in urban mobility and guiding transport planning efforts to enhance real freedoms for diverse population groups.<br /><br />Summary: <div>
arXiv:2510.10307v1 Announce Type: new 
Abstract: Understanding how accessibility shapes participation in leisure activities is central to promoting inclusive and vibrant urban life. Conventional accessibility measures often focus on potential access from fixed home locations, overlooking the constraints and opportunities embedded in daily routines. In this study, we introduce a space-time accessibility (SPA) metric rooted in the capability approach, capturing feasible leisure opportunities between home and work given a certain time budget, individual transport modes, and urban infrastructure. Using high-resolution GPS data from 2,415 residents in the Paris region, we assess how SPA influences total travel time and leisure participation, measured as the diversity of leisure activity locations. Spatial patterns show that most individuals-especially active transport users-choose destinations aligned with their SPA-defined opportunity sets, underscoring the metric's validity in capturing capability sets. Structural equation modeling reveals that SPA directly fosters leisure diversity but also reduces travel time, which in turn is associated with lower diversity. These findings highlight the value of person-centered, capability-informed accessibility metrics for understanding inequalities in urban mobility and informing transport planning strategies that expand real freedoms to participate in social life across diverse population groups.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving Core Structures of Social Networks via Information Guided Multi-Step Graph Pruning</title>
<link>https://arxiv.org/abs/2510.10499</link>
<guid>https://arxiv.org/abs/2510.10499</guid>
<content:encoded><![CDATA[
<div> pruning, social networks, information theory, network simplification, gradient boosting <br />
Summary:<br />
This study introduces a multi-step network pruning framework that utilizes information theory to identify the structural backbone of dense and overlapping social networks. The proposed method, IGPrune, iteratively removes edges based on their contribution to task-relevant mutual information, preserving essential interaction patterns. Inspired by gradient boosting, IGPrune enables efficient optimization to unveil semantically meaningful connections. Experiments on social and biological networks demonstrate that IGPrune retains critical structural and functional patterns, leading to interpretable network backbones. The approach shows promise in supporting scientific discovery and offering actionable insights in real-world networks. <div>
arXiv:2510.10499v1 Announce Type: new 
Abstract: Social networks often contain dense and overlapping connections that obscure their essential interaction patterns, making analysis and interpretation challenging. Identifying the structural backbone of such networks is crucial for understanding community organization, information flow, and functional relationships. This study introduces a multi-step network pruning framework that leverages principles from information theory to balance structural complexity and task-relevant information. The framework iteratively evaluates and removes edges from the graph based on their contribution to task-relevant mutual information, producing a trajectory of network simplification that preserves most of the inherent semantics. Motivated by gradient boosting, we propose IGPrune, which enables efficient, differentiable optimization to progressively uncover semantically meaningful connections. Extensive experiments on social and biological networks show that IGPrune retains critical structural and functional patterns. Beyond quantitative performance, the pruned networks reveal interpretable backbones, highlighting the method's potential to support scientific discovery and actionable insights in real-world networks.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioBench: Modeling Human Behavior in Sociological Surveys with Large Language Models</title>
<link>https://arxiv.org/abs/2510.11131</link>
<guid>https://arxiv.org/abs/2510.11131</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, social attitudes, survey data, demographic attributes

Summary: Large language models (LLMs) have shown promise in simulating human social behaviors, but there is a lack of comprehensive benchmarks to evaluate their alignment with real-world social attitudes. The SocioBench benchmark, derived from standardized survey data from the International Social Survey Programme, aggregates over 480,000 respondent records from 30 countries across various sociological domains and demographic attributes. Experiments indicate that LLMs only achieve 30-40% accuracy when simulating individuals in survey scenarios, with significant variations across domains and demographic groups. Current LLMs face limitations in survey contexts, including insufficient individual-level data coverage, lack of scenario diversity, and absence of group-level modeling.<br /><br />Summary: <div>
arXiv:2510.11131v1 Announce Type: new 
Abstract: Large language models (LLMs) show strong potential for simulating human social behaviors and interactions, yet lack large-scale, systematically constructed benchmarks for evaluating their alignment with real-world social attitudes. To bridge this gap, we introduce SocioBench-a comprehensive benchmark derived from the annually collected, standardized survey data of the International Social Survey Programme (ISSP). The benchmark aggregates over 480,000 real respondent records from more than 30 countries, spanning 10 sociological domains and over 40 demographic attributes. Our experiments indicate that LLMs achieve only 30-40% accuracy when simulating individuals in complex survey scenarios, with statistically significant differences across domains and demographic subgroups. These findings highlight several limitations of current LLMs in survey scenarios, including insufficient individual-level data coverage, inadequate scenario diversity, and missing group-level modeling.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation</title>
<link>https://arxiv.org/abs/2510.11423</link>
<guid>https://arxiv.org/abs/2510.11423</guid>
<content:encoded><![CDATA[
<div> misinformation governance, Community Notes, health-related, large language models, CrowdNotes+

Summary:<br /><br />Community Notes is a crowd-sourced system on X, aimed at addressing misinformation. However, a study on health-related notes revealed a delay in response time, prompting the proposal of CrowdNotes+. This framework leverages large language models to enhance the accuracy and speed of fact-checking. CrowdNotes+ includes evidence-grounded note augmentation and utility-guided note automation, coupled with a hierarchical evaluation process. The framework was implemented through HealthNotes, a dataset of annotated health notes. Experiments with fifteen LLMs uncovered an issue in the current evaluation system and demonstrated the efficacy of the hierarchical evaluation and LLM-augmented generation in improving factual precision and evidence utility. This hybrid human-AI governance model shows promise in enhancing the reliability and timeliness of crowd-sourced fact-checking. <div>
arXiv:2510.11423v1 Announce Type: new 
Abstract: Community Notes, the crowd-sourced misinformation governance system on X (formerly Twitter), enables users to flag misleading posts, attach contextual notes, and vote on their helpfulness. However, our analysis of 30.8K health-related notes reveals significant latency, with a median delay of 17.6 hours before the first note receives a helpfulness status. To improve responsiveness during real-world misinformation surges, we propose CrowdNotes+, a unified framework that leverages large language models (LLMs) to augment Community Notes for faster and more reliable health misinformation governance. CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note augmentation and (2) utility-guided note automation, along with a hierarchical three-step evaluation that progressively assesses relevance, correctness, and helpfulness. We instantiate the framework through HealthNotes, a benchmark of 1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness judge. Experiments on fifteen LLMs reveal an overlooked loophole in current helpfulness evaluation, where stylistic fluency is mistaken for factual accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented generation jointly enhance factual precision and evidence utility. These results point toward a hybrid human-AI governance model that improves both the rigor and timeliness of crowd-sourced fact-checking.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Networks Multiscale Entropy Analysis</title>
<link>https://arxiv.org/abs/2510.11524</link>
<guid>https://arxiv.org/abs/2510.11524</guid>
<content:encoded><![CDATA[
<div> multiscale entropy, network predictability, structural complexity, compression-based entropy, hierarchical patterns

Summary:
- The study introduces a multiscale entropy framework for analyzing complex networks, incorporating spectral graph reduction to capture complexity across multiple scales.
- Application of the framework to various real-world networks reveals consistent entropy profiles across different domains, highlighting three structural regimes (stable, increasing, and hybrid) that align with specific behaviors.
- Multiscale entropy proves superior to single-scale models in determining network predictability, emphasizing the importance of considering structural information across scales for a more comprehensive understanding of network complexity.
- The results position multiscale entropy as a powerful and scalable tool for characterizing, classifying, and assessing complex network structures. 

<br /><br />Summary: <div>
arXiv:2510.11524v1 Announce Type: new 
Abstract: Understanding the structural complexity and predictability of complex networks is a central challenge in network science. Although recent studies have revealed a relationship between compression-based entropy and link prediction performance, existing methods focus on single-scale representations. This approach often overlooks the rich hierarchical patterns that can exist in real-world networks. In this study, we introduce a multiscale entropy framework that extends previous entropy-based approaches by applying spectral graph reduction. This allows us to quantify how structural entropy evolves as the network is gradually coarsened, capturing complexity across multiple scales. We apply our framework to real-world networks across biological, economic, social, technological, and transportation domains. The results uncover consistent entropy profiles across network families, revealing three structural regimes$\unicode{x2013}$stable, increasing, and hybrid$\unicode{x2013}$that align with domain-specific behaviors. Compared to single-scale models, multiscale entropy significantly improves our ability to determine network predictability. This shows that considering structural information across scales provides a more complete characterization of network complexity. Together, these results position multiscale entropy as a powerful and scalable tool for characterizing, classifying, and assessing the structure of complex networks.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Population synthesis with geographic coordinates</title>
<link>https://arxiv.org/abs/2510.09669</link>
<guid>https://arxiv.org/abs/2510.09669</guid>
<content:encoded><![CDATA[
<div> population synthesis algorithm, Normalizing Flows, Variational Autoencoder, spatial accuracy, privacy preservation

Summary: 
This article proposes a novel population synthesis algorithm that utilizes Normalizing Flows (NF) and Variational Autoencoder (VAE) to generate synthetic populations with explicit coordinates. By mapping spatial coordinates into a more regular latent space, the algorithm can account for the unique characteristics of latitude and longitude, including empty spaces and uneven densities. The approach also incorporates non-spatial features to learn the joint distribution and exploit spatial autocorrelations. The method is tested on 121 datasets from diverse geographies, demonstrating superior performance compared to existing benchmarks in terms of spatial accuracy and practical utility. This advancement in generating geolocated synthetic populations at fine spatial resolution opens up new possibilities for applications such as emergency response planning, epidemic modeling, and transportation studies. Privacy preservation measures are also integrated into the evaluation framework to ensure the security of generated data. <div>
arXiv:2510.09669v1 Announce Type: cross 
Abstract: It is increasingly important to generate synthetic populations with explicit coordinates rather than coarse geographic areas, yet no established methods exist to achieve this. One reason is that latitude and longitude differ from other continuous variables, exhibiting large empty spaces and highly uneven densities. To address this, we propose a population synthesis algorithm that first maps spatial coordinates into a more regular latent space using Normalizing Flows (NF), and then combines them with other features in a Variational Autoencoder (VAE) to generate synthetic populations. This approach also learns the joint distribution between spatial and non-spatial features, exploiting spatial autocorrelations. We demonstrate the method by generating synthetic homes with the same statistical properties of real homes in 121 datasets, corresponding to diverse geographies. We further propose an evaluation framework that measures both spatial accuracy and practical utility, while ensuring privacy preservation. Our results show that the NF+VAE architecture outperforms popular benchmarks, including copula-based methods and uniform allocation within geographic areas. The ability to generate geolocated synthetic populations at fine spatial resolution opens the door to applications requiring detailed geography, from household responses to floods, to epidemic spread, evacuation planning, and transport modeling.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Blotto Networks and Voter Models to Simulate Voter Behavior in Response to Competitive Election Spending</title>
<link>https://arxiv.org/abs/2510.09697</link>
<guid>https://arxiv.org/abs/2510.09697</guid>
<content:encoded><![CDATA[
<div> Keywords: Voter Model, propaganda, dynamic population, Blotto Game, graph theory

Summary: 
The study combines the Voter Model and the Blotto Game to understand the impact of propaganda on a dynamic, interconnected population. By incorporating graph theory and game theory aspects, the research aims to analyze the behavior of voters or consumers under the influence of competing propaganda campaigns. The Voter-Blotto Game framework allows for a deeper understanding of the most effective spending strategy in information wars between two opposing parties. Factors such as the components of the graph influencing the value in the eyes of competing players will be examined. Ultimately, the study seeks to shed light on how propaganda influences decision-making processes and shapes outcomes in competitive environments. 

<br /><br />Summary: <div>
arXiv:2510.09697v1 Announce Type: cross 
Abstract: In the past, the Voter Model has been explicitly used to model the impact of propaganda on a dynamic, interconnected population, and certain factors have been identified that influence the behavior of voters when under outside influence. The Blotto Game has also been explicitly used to study information wars between two opposing parties, whether in regards to a political issue or advertising war. Both the graph theory behind the Voter Model and the game theory aspects of the Blotto Game are relevant to the behavior of voters or consumers when they are under the influence of competing propaganda campaigns, and for this reason both are useful to understand the most effective spending strategy. In this project, we seek to combine the two problems into a Voter-Blotto Game and examine what components of the graph most effect its value in the eyes of the competing players.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observer-Based Source Localization in Tree Infection Networks via Laplace Transforms</title>
<link>https://arxiv.org/abs/2510.09828</link>
<guid>https://arxiv.org/abs/2510.09828</guid>
<content:encoded><![CDATA[
<div> localize, infection source, tree-structured network, observer-based, scale-invariant <br />
Summary:
The article addresses the challenge of identifying the source of infection in a tree-structured network using a reduced set of observers. It focuses on the susceptible-infected outbreak model, where the infection spreads through random time delays between neighboring nodes. By analyzing the joint Laplace transform of observer infection times, the study evaluates the identifiability of the infection source. The researchers propose scale-invariant least-squares estimators to accurately localize the source based on edge-delay probability distributions. The effectiveness of these estimators is demonstrated through experiments on synthetic trees and a river network. The article also highlights the complexity of source localization on networks with cycles, pointing out potential issues with standard spanning-tree reductions in such scenarios. <div>
arXiv:2510.09828v1 Announce Type: cross 
Abstract: We address the problem of localizing the source of infection in an undirected, tree-structured network under a susceptible-infected outbreak model. The infection propagates with independent random time increments (i.e., edge-delays) between neighboring nodes, while only the infection times of a subset of nodes can be observed. We show that a reduced set of observers may be sufficient, in the statistical sense, to localize the source and characterize its identifiability via the joint Laplace transform of the observers' infection times. Using the explicit form of these transforms in terms of the edge-delay probability distributions, we propose scale-invariant least-squares estimators of the source. We evaluate their performance on synthetic trees and on a river network, demonstrating accurate localization under diverse edge-delay models. To conclude, we highlight overlooked technical challenges for observer-based source localization on networks with cycles, where standard spanning-tree reductions may be ill-posed.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference-driven Knowledge Distillation for Few-shot Node Classification</title>
<link>https://arxiv.org/abs/2510.10116</link>
<guid>https://arxiv.org/abs/2510.10116</guid>
<content:encoded><![CDATA[
<div> Graph neural networks; text-attributed graphs; knowledge distillation; few-shot learning; node classification <br />
Summary: <br />
This study introduces a Preference-Driven Knowledge Distillation (PKD) framework to enhance few-shot node classification in text-attributed graphs. By combining Large Language Models (LLMs) and Graph Neural Networks (GNNs), the framework effectively distills knowledge from LLMs to GNNs through a node selector mechanism. Moreover, a GNN selector is developed to customize knowledge distillation based on the local topologies of nodes, improving the accuracy of node classification. The proposed framework is validated through extensive experiments on real-world TAGs, demonstrating its efficacy in addressing the challenges of training GNNs and handling complex node topologies. <div>
arXiv:2510.10116v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) can efficiently process text-attributed graphs (TAGs) due to their message-passing mechanisms, but their training heavily relies on the human-annotated labels. Moreover, the complex and diverse local topologies of nodes of real-world TAGs make it challenging for a single mechanism to handle. Large language models (LLMs) perform well in zero-/few-shot learning on TAGs but suffer from a scalability challenge. Therefore, we propose a preference-driven knowledge distillation (PKD) framework to synergize the complementary strengths of LLMs and various GNNs for few-shot node classification. Specifically, we develop a GNN-preference-driven node selector that effectively promotes prediction distillation from LLMs to teacher GNNs. To further tackle nodes' intricate local topologies, we develop a node-preference-driven GNN selector that identifies the most suitable teacher GNN for each node, thereby facilitating tailored knowledge distillation from teacher GNNs to the student GNN. Extensive experiments validate the efficacy of our proposed framework in few-shot node classification on real-world TAGs.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations</title>
<link>https://arxiv.org/abs/2510.10864</link>
<guid>https://arxiv.org/abs/2510.10864</guid>
<content:encoded><![CDATA[
<div> Graph heterophily, spectral filters, adaptive filtering, graph embeddings, GNNs <br />
<br />
Summary: 
The relationship between graph heterophily and spectral filters is complex, with varying optimal filter responses across frequency components. The traditional approach of using fixed filters may not be suitable, prompting the need for adaptive filtering to maintain expressive graph embeddings. The impact of heterophily degree on GNN performance is not strictly correlated with the average frequency response, emphasizing the importance of adaptive graph filters to ensure good generalization. A new GNN method, called [METHOD NAME], is introduced, which leverages information across the heterophily spectrum and combines representations through adaptive mixing. [METHOD NAME] achieves significant improvements in accuracy compared to existing baselines for both homophilic and heterophilic graphs. <div>
arXiv:2510.10864v1 Announce Type: cross 
Abstract: Graph heterophily, where connected nodes have different labels, has attracted significant interest recently. Most existing works adopt a simplified approach - using low-pass filters for homophilic graphs and high-pass filters for heterophilic graphs. However, we discover that the relationship between graph heterophily and spectral filters is more complex - the optimal filter response varies across frequency components and does not follow a strict monotonic correlation with heterophily degree. This finding challenges conventional fixed filter designs and suggests the need for adaptive filtering to preserve expressiveness in graph embeddings. Formally, natural questions arise: Given a heterophilic graph G, how and to what extent will the varying heterophily degree of G affect the performance of GNNs? How can we design adaptive filters to fit those varying heterophilic connections? Our theoretical analysis reveals that the average frequency response of GNNs and graph heterophily degree do not follow a strict monotonic correlation, necessitating adaptive graph filters to guarantee good generalization performance. Hence, we propose [METHOD NAME], a simple yet powerful GNN, which extracts information across the heterophily spectrum and combines salient representations through adaptive mixing. [METHOD NAME]'s superior performance achieves up to 9.2% accuracy improvement over leading baselines across homophilic and heterophilic graphs.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events</title>
<link>https://arxiv.org/abs/2504.18837</link>
<guid>https://arxiv.org/abs/2504.18837</guid>
<content:encoded><![CDATA[
<div> sentiment analysis, social media, climate change events, machine learning models, data collection<br />
Summary:<br />
This survey explores sentiment analysis methods for social media content related to climate-induced events like wildfires and floods. It covers various approaches, from lexicon-based to large language models, and discusses challenges such as data collection and ethical considerations. The study highlights the importance of understanding public perception through sentiment analysis to inform policy decisions and improve emergency responses. It addresses open problems like misinformation detection and model alignment with human values. The research aims to guide researchers and practitioners in effectively analyzing sentiment during extreme weather events in the climate crisis era. <div>
arXiv:2504.18837v4 Announce Type: replace 
Abstract: Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake News in Social Networks</title>
<link>https://arxiv.org/abs/1708.06233</link>
<guid>https://arxiv.org/abs/1708.06233</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent reinforcement learning, fake news, social networks, human behavior, fake-news attack

Summary: 
In this study, the researchers introduce multi-agent reinforcement learning as a novel approach to understanding the dissemination of fake news in social networks. They investigate how human behavior in social networks influences the spread of fake news, particularly in populations exposed to fake news regularly. The study reveals that fake-news attacks are more successful when targeting highly connected individuals and those with weaker private information. Additionally, spreading disinformation across multiple agents is more effective than concentrating it on a few agents. The research also suggests that fake news spreads less effectively in balanced networks compared to clustered networks. A human-subject experiment conducted to validate the model's predictions supports the effectiveness of the proposed approach in analyzing fake news propagation in social networks.<br /><br />Summary: <div>
arXiv:1708.06233v2 Announce Type: replace-cross 
Abstract: We propose multi-agent reinforcement learning as a new method for modeling fake news in social networks. This method allows us to model human behavior in social networks both in unaccustomed populations and in populations that have adapted to the presence of fake news. In particular the latter is challenging for existing methods. We find that a fake-news attack is more effective if it targets highly connected people and people with weaker private information. Attacks are more effective when the disinformation is spread across several agents than when the disinformation is concentrated with more intensity on fewer agents. Furthermore, fake news spread less well in balanced networks than in clustered networks. We test a part of our findings in a human-subject experiment. The experimental evidence provides support for the predictions from the model, suggesting that the model is suitable to analyze the spread of fake news in social networks.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework</title>
<link>https://arxiv.org/abs/2310.07086</link>
<guid>https://arxiv.org/abs/2310.07086</guid>
<content:encoded><![CDATA[
<div> framework, NLP, social media, user feedback, transit surveys
Summary:
The paper presents a novel NLP-based framework that leverages social media data from Twitter to analyze user feedback on service issues, eliminating the need for costly transit surveys. The framework uses few-shot learning for tweet classification and sentiment analysis to identify and measure sentiments related to safety, reliability, and maintenance of the NYC subway system. The study validated the framework's effectiveness through manual label validation and comparison with agency-run customer surveys, demonstrating its ability to accurately gauge user feedback and identify areas for targeted improvement. The innovative approach highlights the potential of utilizing social media data for understanding user perceptions and planning transit system enhancements. 

Summary: <div>
arXiv:2310.07086v2 Announce Type: replace-cross 
Abstract: Traditional methods of collecting user feedback through transit surveys are often time-consuming, resource intensive, and costly. In this paper, we propose a novel NLP-based framework that harnesses the vast, abundant, and inexpensive data available on social media platforms like Twitter to understand users' perceptions of various service issues. Twitter, being a microblogging platform, hosts a wealth of real-time user-generated content that often includes valuable feedback and opinions on various products, services, and experiences. The proposed framework streamlines the process of gathering and analyzing user feedback without the need for costly and time-consuming user feedback surveys using two techniques. First, it utilizes few-shot learning for tweet classification within predefined categories, allowing effective identification of the issues described in tweets. It then employs a lexicon-based sentiment analysis model to assess the intensity and polarity of the tweet sentiments, distinguishing between positive, negative, and neutral tweets. The effectiveness of the framework was validated on a subset of manually labeled Twitter data and was applied to the NYC subway system as a case study. The framework accurately classifies tweets into predefined categories related to safety, reliability, and maintenance of the subway system and effectively measured sentiment intensities within each category. The general findings were corroborated through a comparison with an agency-run customer survey conducted in the same year. The findings highlight the effectiveness of the proposed framework in gauging user feedback through inexpensive social media data to understand the pain points of the transit system and plan for targeted improvements.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Codiscovering graphical structure and functional relationships within data: A Gaussian Process framework for connecting the dots</title>
<link>https://arxiv.org/abs/2311.17007</link>
<guid>https://arxiv.org/abs/2311.17007</guid>
<content:encoded><![CDATA[
<div> Function approximation, hypergraph, data-driven discovery, interpretable Gaussian Process, computational knowledge<br />
Summary:<br />
This article introduces three levels of complexity in function approximation, ranging from approximating unknown functions given input/output data to discovering the structure of a hypergraph. The focus is on Type 3 problems, where the structure of the hypergraph is unknown, requiring the data-driven discovery of both the structure and unknown functions. A novel interpretable Gaussian Process (GP) framework is proposed for solving Type 3 problems efficiently without the need for randomization of data or knowledge of its sampling. The framework leverages the nonlinear ANOVA capabilities of GPs as a sensing mechanism, allowing for polynomial complexity in contrast to super-exponential complexity in causal inference methods. This approach provides a platform for organizing, communicating, and processing computational knowledge effectively. <div>
arXiv:2311.17007v2 Announce Type: replace-cross 
Abstract: Most problems within and beyond the scientific domain can be framed into one of the following three levels of complexity of function approximation. Type 1: Approximate an unknown function given input/output data. Type 2: Consider a collection of variables and functions, some of which are unknown, indexed by the nodes and hyperedges of a hypergraph (a generalized graph where edges can connect more than two vertices). Given partial observations of the variables of the hypergraph (satisfying the functional dependencies imposed by its structure), approximate all the unobserved variables and unknown functions. Type 3: Expanding on Type 2, if the hypergraph structure itself is unknown, use partial observations of the variables of the hypergraph to discover its structure and approximate its unknown functions. These hypergraphs offer a natural platform for organizing, communicating, and processing computational knowledge. While most scientific problems can be framed as the data-driven discovery of unknown functions in a computational hypergraph whose structure is known (Type 2), many require the data-driven discovery of the structure (connectivity) of the hypergraph itself (Type 3). We introduce an interpretable Gaussian Process (GP) framework for such (Type 3) problems that does not require randomization of the data, access to or control over its sampling, or sparsity of the unknown functions in a known or learned basis. Its polynomial complexity, which contrasts sharply with the super-exponential complexity of causal inference methods, is enabled by the nonlinear ANOVA capabilities of GPs used as a sensing mechanism.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractional stochastic model of citation dynamics with memory and volatility</title>
<link>https://arxiv.org/abs/2503.03011</link>
<guid>https://arxiv.org/abs/2503.03011</guid>
<content:encoded><![CDATA[
<div> latent attention, citation dynamics, stochastic model, memory effects, collective attention

Summary: 
This study addresses the challenge of understanding citation dynamics in networks and the science of science. It introduces a novel discovery that the variance of the logarithm of citation counts per unit time follows a power law with respect to time. A stochastic model is developed, incorporating memory-driven processes with cumulative advantage in the form of fractional Brownian motion characterized by a Hurst parameter. It is found that antipersistent fluctuations in attention lead to log-normal citation distributions, while persistent dynamics result in heavy-tailed power laws, resolving the log-normal–power-law discrepancy. Empirical analysis of arXiv e-prints suggests that the latent attention process displays antipersistent behavior. By linking memory effects and stochastic fluctuations in attention to broader network dynamics, this study offers a unified framework for understanding the evolution of collective attention in science and other attention-driven processes. <div>
arXiv:2503.03011v2 Announce Type: replace-cross 
Abstract: Understanding the statistical laws governing citation dynamics remains a fundamental challenge in network theory and the science of science. Citation networks typically exhibit in-degree distributions well approximated by log-normal distributions yet also display power-law behaviour in the high-citation regime -- an apparent contradiction lacking a unified explanation. Here we identify a previously unrecognised phenomenon: the variance of the logarithm of citation counts per unit time follows a power law with respect to time ($t$) since publication, scaling as $t^{H}$, with $H$ constant. This discovery introduces a new challenge while simultaneously offering a crucial clue to resolving this discrepancy. We develop a stochastic model in which latent attention to publications evolves through a memory-driven process with cumulative advantage, modelled as fractional Brownian motion with Hurst parameter $H$ and volatility. We show that antipersistent fluctuations in attention ($H < 1/2$) yield log-normal citation distributions, whereas persistent attention dynamics ($H > 1/2$) favour heavy-tailed power laws, thus resolving the log-normal--power-law contradiction. Numerical simulations confirm both the $t^{H}$ law and the transition between regimes. Empirical analysis of arXiv e-prints indicates that the latent attention process is intrinsically antipersistent ($H \approx 0.13$). By linking memory effects and stochastic fluctuations in attention to broader network dynamics, our findings provide a unifying framework for understanding the evolution of collective attention in science and other attention-driven processes.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web Crawler Restrictions, AI Training Datasets \&amp;amp; Political Biases</title>
<link>https://arxiv.org/abs/2510.09031</link>
<guid>https://arxiv.org/abs/2510.09031</guid>
<content:encoded><![CDATA[
<div> restrictions, AI crawlers, content type, data composition, training data

Summary:<br /><br />Large language models rely on web-scraped text for training, but content creators are increasingly blocking AI crawlers. A study on the top one million websites since 2023 shows a rise in restrictions, especially in news outlets with high factual reporting. Websites with neutral political positions tend to impose stronger restrictions, while hyperpartisan sites and those with low factual reporting restrict access to AI crawlers less. These patterns may lead to training datasets being skewed towards low-quality or polarized content, which could impact the capabilities of models used by AI-as-a-Service providers. The findings highlight the importance of understanding the impact of crawler restrictions on training data composition and the potential consequences for the performance of large language models. <div>
arXiv:2510.09031v1 Announce Type: new 
Abstract: Large language models rely on web-scraped text for training; concurrently, content creators are increasingly blocking AI crawlers to retain control over their data. We analyze crawler restrictions across the top one million most-visited websites since 2023 and examine their potential downstream effects on training data composition. Our analysis reveals growing restrictions, with blocking patterns varying by website popularity and content type. A quarter of the top thousand websites restrict AI crawlers, decreasing to one-tenth across the broader top million. Content type matters significantly: 34.2% of news outlets disallow OpenAI's GPTBot, rising to 55% for outlets with high factual reporting. Additionally, outlets with neutral political positions impose the strongest restrictions (58%), whereas hyperpartisan websites and those with low factual reporting impose fewer restrictions -only 4.1% of right-leaning outlets block access to OpenAI. Our findings suggest that heterogeneous blocking patterns may skew training datasets toward low-quality or polarized content, potentially affecting the capabilities of models served by prominent AI-as-a-Service providers.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Platform Narrative Prediction: Leveraging Platform-Invariant Discourse Networks</title>
<link>https://arxiv.org/abs/2510.09464</link>
<guid>https://arxiv.org/abs/2510.09464</guid>
<content:encoded><![CDATA[
<div> network proximity, cross-platform prediction, discourse networks, information diffusion, social media


Summary:
This study introduces a novel approach to predicting cross-platform information diffusion by utilizing network proximity. By constructing platform-invariant discourse networks that connect users based on shared narrative engagement, the researchers were able to identify patterns in how content spreads across different platforms. The approach outperformed traditional diffusion models and baselines, requiring only a small percentage of active users to make accurate predictions. The framework was validated using data from the 2024 U.S. election, successfully identifying emerging narratives with a high degree of accuracy. The findings suggest that by leveraging cross-platform neighbor proximity, predictive signals can be gleaned without direct cross-platform influence, offering potential for proactive intervention in the spread of misinformation and rumors online.  <div>
arXiv:2510.09464v1 Announce Type: new 
Abstract: Online narratives spread unevenly across platforms, with content emerging on one site often appearing on others, hours, days or weeks later. Existing cross-platform information diffusion models often treat platforms as isolated systems, disregarding cross-platform activity that might make these patterns more predictable. In this work, we frame cross-platform prediction as a network proximity problem: rather than tracking individual users across platforms or relying on brittle signals like shared URLs or hashtags, we construct platform-invariant discourse networks that link users through shared narrative engagement. We show that cross-platform neighbor proximity provides a strong predictive signal: adoption patterns follow discourse network structure even without direct cross-platform influence. Our highly-scalable approach substantially outperforms diffusion models and other baselines while requiring less than 3% of active users to make predictions. We also validate our framework through retrospective deployment. We sequentially process a datastream of 5.7M social media posts occurred during the 2024 U.S. election, to simulate real-time collection from four platforms (X, TikTok, Truth Social, and Telegram): our framework successfully identified emerging narratives, including crises-related rumors, yielding over 94% AUC with sufficient lead time to support proactive intervention.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Birdwatch to Community Notes, from Twitter to X: four years of community-based content moderation</title>
<link>https://arxiv.org/abs/2510.09585</link>
<guid>https://arxiv.org/abs/2510.09585</guid>
<content:encoded><![CDATA[
<div> Keywords: Community Notes, crowdsourced content moderation, social media platforms, dataset, literature review

Summary:
Community Notes, previously known as Birdwatch, was launched by X (formerly Twitter) in January 2021 as a large-scale crowdsourced content moderation initiative. This Resource paper offers a systematic review of the literature on Community Notes and provides a curated dataset and source code for future research on the topic. The dataset includes Notes and Ratings data from the program's first four years, with language detection and analysis of discussion topics in English-language Notes. Monthly interaction networks among Contributors were also constructed. The resources presented in this paper aim to enhance understanding of the dynamics and effectiveness of Community Notes, as the model gains traction across various social media platforms. Overall, this paper serves as a valuable foundation for advancing research on the Community Notes system. 

<br /><br />Summary: Community Notes, previously Birdwatch, launched by X (formerly Twitter) in 2021 for content moderation. Resource paper provides literature review, dataset, source code for research. Analysis of Notes data, identification of discussion topics, and construction of Contributor interaction networks. Aims to enhance understanding of Community Notes dynamics and effectiveness. Valuable foundation for future research. <div>
arXiv:2510.09585v1 Announce Type: new 
Abstract: Community Notes (formerly known as Birdwatch) is the first large-scale crowdsourced content moderation initiative that was launched by X (formerly known as Twitter) in January 2021. As the Community Notes model gains momentum across other social media platforms, there is a growing need to assess its underlying dynamics and effectiveness. This Resource paper provides (a) a systematic review of the literature on Community Notes, and (b) a major curated dataset and accompanying source code to support future research on Community Notes. We parsed Notes and Ratings data from the first four years of the program and conducted language detection across all Notes. Focusing on English-language Notes, we extracted embedded URLs and identified discussion topics in each Note. Additionally, we constructed monthly interaction networks among the Contributors. Together with the literature review, these resources offer a robust foundation for advancing research on the Community Notes system.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiplexons: Limits of Multiplex Networks</title>
<link>https://arxiv.org/abs/2510.08639</link>
<guid>https://arxiv.org/abs/2510.08639</guid>
<content:encoded><![CDATA[
<div> Keywords: multiplex networks, limit theory, dense networks, graphons, clustering coefficients

Summary: 
Multiplex networks are complex systems that consist of multiple layers of interactions between nodes. This article introduces a framework for studying properties of large multiplex networks by developing a limit theory analogous to graphons for dense graphs. The framework allows for deriving limiting analogues of common multiplex features such as degree distributions and clustering coefficients. Various examples, including correlated versions of existing graph models and dynamic networks, are provided to illustrate the applications of the theory. The article also discusses the relationship between multiplex networks and decorated graphs, showing how convergence results can be obtained from the limit theory of decorated graphs. The authors propose several future directions for further developing the multiplex limit theory. <br /><br />Summary: <div>
arXiv:2510.08639v1 Announce Type: cross 
Abstract: In a multiplex network, a set of nodes is connected by different types of interactions, each represented as a separate layer within the network. Multiplexes have emerged as a key instrument for modeling large-scale complex systems, due to the widespread coexistence of diverse interactions in social, industrial, and biological domains. This motivates the development of a rigorous and readily applicable framework for studying properties of large multiplex networks. In this article, we provide a self-contained introduction to the limit theory of dense multiplex networks, analogous to the theory of graphons (limit theory of dense graphs). As applications, we derive limiting analogues of commonly used multiplex features, such as degree distributions and clustering coefficients. We also present a range of illustrative examples, including correlated versions of Erd\H{o}s-R\'enyi and inhomogeneous random graph models and dynamic networks. Finally, we discuss how multiplex networks fit within the broader framework of decorated graphs, and how the convergence results can be recovered from the limit theory of decorated graphs. Several future directions are outlined for further developing the multiplex limit theory.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed High-order Graph Dynamics Identification Learning for Predicting Complex Networks Long-term Dynamics</title>
<link>https://arxiv.org/abs/2510.09082</link>
<guid>https://arxiv.org/abs/2510.09082</guid>
<content:encoded><![CDATA[
<div> dynamic network, complex systems, higher-order relations, prediction, interpretability

Summary: 
The paper introduces a method for predicting the long-term dynamic evolution of complex networks. Traditional graph machine learning methods struggle to capture non-pairwise relationships in complex networks, so the proposed approach incorporates dynamic hypergraph learning to model higher-order relations. To improve prediction accuracy and interpretability, a dual-driven dynamic prediction module is introduced. This module utilizes Koopman operator theory to transform nonlinear dynamical equations into linear systems for solving while ensuring adherence to physical laws through the physical information neural differential equation method. Experimental results demonstrate the method's effectiveness in predicting complex network dynamics accurately and over the long term. <div>
arXiv:2510.09082v1 Announce Type: cross 
Abstract: Learning complex network dynamics is fundamental to understanding, modelling and controlling real-world complex systems. There are two main problems in the task of predicting the dynamic evolution of complex networks: on the one hand, existing methods usually use simple graphs to describe the relationships in complex networks; however, this approach can only capture pairwise relationships, while there may be rich non-pairwise structured relationships in the network. First-order GNNs have difficulty in capturing dynamic non-pairwise relationships. On the other hand, theoretical prediction models lack accuracy and data-driven prediction models lack interpretability. To address the above problems, this paper proposes a higher-order network dynamics identification method for long-term dynamic prediction of complex networks. Firstly, to address the problem that traditional graph machine learning can only deal with pairwise relations, dynamic hypergraph learning is introduced to capture the higher-order non-pairwise relations among complex networks and improve the accuracy of complex network modelling. Then, a dual-driven dynamic prediction module for physical data is proposed. The Koopman operator theory is introduced to transform the nonlinear dynamical differential equations for the dynamic evolution of complex networks into linear systems for solving. Meanwhile, the physical information neural differential equation method is utilised to ensure that the dynamic evolution conforms to the physical laws. The dual-drive dynamic prediction module ensures both accuracy and interpretability of the prediction. Validated on public datasets and self-built industrial chain network datasets, the experimental results show that the method in this paper has good prediction accuracy and long-term prediction performance.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do Temporal Graph Learning Models Learn?</title>
<link>https://arxiv.org/abs/2510.09416</link>
<guid>https://arxiv.org/abs/2510.09416</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal graphs, graph representation learning, evaluation protocols, structural characteristics, edge formation mechanisms

Summary:
Temporal graph learning models have shown strong performance in capturing certain attributes of temporal graphs, but there are concerns about the reliability of benchmark results and the competitiveness of simple heuristics. In this study, seven models were evaluated on their ability to capture eight fundamental attributes related to the link structure of temporal graphs. The models performed well in capturing some attributes such as density and temporal patterns like recency, but they struggled to reproduce other attributes related to edge formation mechanisms such as homophily. These findings highlight important limitations in current temporal graph learning models and suggest the need for more interpretability-driven evaluations in this research area. The results of this study provide practical insights for the application of temporal graph learning models and emphasize the importance of understanding which underlying graph properties contribute to the models' predictions.<br /><br />Summary: <div>
arXiv:2510.09416v1 Announce Type: cross 
Abstract: Learning on temporal graphs has become a central topic in graph representation learning, with numerous benchmarks indicating the strong performance of state-of-the-art models. However, recent work has raised concerns about the reliability of benchmark results, noting issues with commonly used evaluation protocols and the surprising competitiveness of simple heuristics. This contrast raises the question of which properties of the underlying graphs temporal graph learning models actually use to form their predictions. We address this by systematically evaluating seven models on their ability to capture eight fundamental attributes related to the link structure of temporal graphs. These include structural characteristics such as density, temporal patterns such as recency, and edge formation mechanisms such as homophily. Using both synthetic and real-world datasets, we analyze how well models learn these attributes. Our findings reveal a mixed picture: models capture some attributes well but fail to reproduce others. With this, we expose important limitations. Overall, we believe that our results provide practical insights for the application of temporal graph learning models, and motivate more interpretability-driven evaluations in temporal graph learning research.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Sanctions on decentralised Privacy Tools: A Case Study of Tornado Cash</title>
<link>https://arxiv.org/abs/2510.09443</link>
<guid>https://arxiv.org/abs/2510.09443</guid>
<content:encoded><![CDATA[
<div> sanctions, Tornado Cash, transaction privacy, blockchain, regulatory interventions

Summary: 
The paper examines the impact of sanctions on Tornado Cash, a protocol aimed at enhancing transaction privacy. After the U.S. Department of the Treasury imposed sanctions on Tornado Cash in August 2022, there was a significant decrease in transaction volume, user diversity, and overall protocol utilization across Ethereum, BNB Smart Chain, and Polygon blockchains. Despite the lifting and eventual removal of sanctions by the U.S. Office of Foreign Assets Control in March 2025, activity only partially recovered, highlighting the challenges of enforcing regulatory measures in decentralized environments. The case of Tornado Cash demonstrates how regulatory interventions can influence decentralized protocols and raises questions about the broader implications for such platforms in the future. 

<br /><br />Summary: <div>
arXiv:2510.09443v1 Announce Type: cross 
Abstract: This paper investigates the impact of sanctions on Tornado Cash, a smart contract protocol designed to enhance transaction privacy. Following the U.S. Department of the Treasury's sanctions against Tornado Cash in August 2022, platform activity declined sharply. We document a significant and sustained reduction in transaction volume, user diversity, and overall protocol utilization after the sanctions were imposed. Our analysis draws on transaction data from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We further examine developments following the partial lifting and eventual removal of sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025. Although activity partially recovered, the rebound remained limited. The Tornado Cash case illustrates how regulatory interventions can affect decentralized protocols, while also highlighting the challenges of fully enforcing such measures in decentralized environments.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Learning Augmented Social Recommendations</title>
<link>https://arxiv.org/abs/2502.15695</link>
<guid>https://arxiv.org/abs/2502.15695</guid>
<content:encoded><![CDATA[
<div> social-relation graph, recommender systems, cold users, dual-view denoising, mutual distillation

Summary: 
Recommender systems are crucial for content platforms, but traditional models struggle with cold users. To address this, leveraging the social-relation graph can enrich interest representations. A dual-view denoising strategy using low-rank SVD helps to clean noise in the social graph, and contrastive learning aligns the original and reconstructed graphs. "Mutual distillation" technique isolates and maximizes the utility of social/behavior interests and specific interests, reducing inconsistency. Experimental results show the method's effectiveness, especially for cold users. This work offers a new approach for enhancing recommender systems in the future. The implementation is available on GitHub at https://github.com/WANGLin0126/CLSRec. <div>
arXiv:2502.15695v3 Announce Type: replace-cross 
Abstract: Recommender systems are essential for modern content platforms, yet traditional behavior-based models often struggle with cold users who have limited interaction data. Engaging these users is crucial for platform growth. To bridge this gap, we propose leveraging the social-relation graph to enrich interest representations from behavior-based models. However, extracting value from social graphs is challenging due to relation noise and cross-domain inconsistency. To address the noise propagation and obtain accurate social interest, we employ a dual-view denoising strategy, employing low-rank SVD to the user-item interaction matrix for a denoised social graph and contrastive learning to align the original and reconstructed social graphs. Addressing the interest inconsistency between social and behavioral interests, we adopt a "mutual distillation" technique to isolate the original interests into aligned social/behavior interests and social/behavior specific interests, maximizing the utility of both. Experimental results on widely adopted industry datasets verify the method's effectiveness, particularly for cold users, offering a fresh perspective for future research. The implementation can be accessed at https://github.com/WANGLin0126/CLSRec.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024</title>
<link>https://arxiv.org/abs/2510.07821</link>
<guid>https://arxiv.org/abs/2510.07821</guid>
<content:encoded><![CDATA[
<div> Keywords: data science, methodology, artificial intelligence, election issues, opinion mining

Summary: 
This paper compares two data science methodologies to analyze voter choice in the 2024 presidential election using AI techniques. By examining user comments on YouTube videos from right and left-leaning journals, the study found that immigration and democracy were the most discussed election issues, followed by identity politics, while inflation was less prominent. The results challenge the importance of inflation as an election issue and suggest that opinion mining of online user data may provide more insights than traditional surveys. This research highlights the value of analyzing raw user data to understand voter preferences and election outcomes.<br /><br />Summary: <div>
arXiv:2510.07821v1 Announce Type: new 
Abstract: This paper aims to explore two competing data science methodologies to attempt answering the question, "Which issues contributed most to voters' choice in the 2024 presidential election?" The methodologies involve novel empirical evidence driven by artificial intelligence (AI) techniques. By using two distinct methods based on natural language processing and clustering analysis to mine over eight thousand user comments on election-related YouTube videos from one right leaning journal, Wall Street Journal, and one left leaning journal, New York Times, during pre-election week, we quantify the frequency of selected issue areas among user comments to infer which issues were most salient to potential voters in the seven days preceding the November 5th election. Empirically, we primarily demonstrate that immigration and democracy were the most frequently and consistently invoked issues in user comments on the analyzed YouTube videos, followed by the issue of identity politics, while inflation was significantly less frequently referenced. These results corroborate certain findings of post-election surveys but also refute the supposed importance of inflation as an election issue. This indicates that variations on opinion mining, with their analysis of raw user data online, can be more revealing than polling and surveys for analyzing election outcomes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Really Need SFT? Prompt-as-Policy over Knowledge Graphs for Cold-start Next POI Recommendation</title>
<link>https://arxiv.org/abs/2510.08012</link>
<guid>https://arxiv.org/abs/2510.08012</guid>
<content:encoded><![CDATA[
<div> Keywords: POI recommendation, large language models, knowledge graphs, reinforcement learning, contextual bandit optimization

Summary:
Prompt-as-Policy over knowledge graphs is a novel framework for improving point-of-interest (POI) recommendation in smart urban services. It addresses the challenge of cold-start conditions by dynamically constructing prompts through contextual bandit optimization. The framework treats prompt construction as a learnable policy, optimizing which relational evidences to include, the number of evidence per candidate, and their organization within prompts. By utilizing a knowledge graph to discover candidates and mine relational paths, evidence cards are generated to summarize rationales for each candidate POI. The frozen large language model then generates recommendations from the discovered candidate set based on the policy-optimized prompts. Experimental results on real-world datasets show that Prompt-as-Policy outperforms existing baselines, achieving significant improvements in recommendation accuracy for inactive users without requiring model fine-tuning. <div>
arXiv:2510.08012v1 Announce Type: new 
Abstract: Next point-of-interest (POI) recommendation is crucial for smart urban services such as tourism, dining, and transportation, yet most approaches struggle under cold-start conditions where user-POI interactions are sparse. Recent efforts leveraging large language models (LLMs) address this challenge through either supervised fine-tuning (SFT) or in-context learning (ICL). However, SFT demands costly annotations and fails to generalize to inactive users, while static prompts in ICL cannot adapt to diverse user contexts. To overcome these limitations, we propose Prompt-as-Policy over knowledge graphs, a reinforcement-guided prompting framework that learns to construct prompts dynamically through contextual bandit optimization. Our method treats prompt construction as a learnable policy that adaptively determines (i) which relational evidences to include, (ii) the number of evidence per candidate, and (iii) their organization and ordering within prompts. More specifically, we construct a knowledge graph (KG) to discover candidates and mine relational paths, which are transformed into evidence cards that summarize rationales for each candidate POI. The frozen LLM then acts as a reasoning engine, generating recommendations from the KG-discovered candidate set based on the policy-optimized prompts. Experiments on three real-world datasets demonstrate that Prompt-as-Policy consistently outperforms state-of-the-art baselines, achieving average 7.7\% relative improvements in Acc@1 for inactive users, while maintaining competitive performance on active users, without requiring model fine-tuning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric opinion exchange polarizes in every dimension</title>
<link>https://arxiv.org/abs/2510.08190</link>
<guid>https://arxiv.org/abs/2510.08190</guid>
<content:encoded><![CDATA[
<div> Polarization, Opinion exchange, Agent opinions, Update rule, Antipodal groups <br />
<br />Summary: 
This article introduces a model of opinion exchange where agent opinions on multiple topics are tracked concurrently. The opinions are represented as vectors on a unit sphere and are updated based on overall correlation. The model assumes biased assimilation, bringing similar opinions closer and opposing ones apart. For two topics, the model induces polarization, but it was unclear for higher dimensions. This work resolves the question for dimensions greater than or equal to three by analyzing model dynamics and utilizing random process theory. <div>
arXiv:2510.08190v1 Announce Type: new 
Abstract: A recent line of work studies models of opinion exchange where agent opinions about $d$ topics are tracked simultaneously. The opinions are represented as vectors on the unit $(d-1)$-sphere, and the update rule is based on the overall correlation between the relevant vectors. The update rule reflects the assumption of biased assimilation, i.e., a pair of opinions is brought closer together if their correlation is positive and further apart if the correlation is negative.
  This model seems to induce the polarization of opinions into two antipodal groups. This is in contrast to many other known models which tend to achieve consensus. The polarization property has been recently proved for $d=2$, but the general case of $d \ge 3$ remained open. In this work, we settle the general case, using a more detailed understanding of the model dynamics and tools from the theory of random processes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.08481</link>
<guid>https://arxiv.org/abs/2510.08481</guid>
<content:encoded><![CDATA[
<div> Hashtag trends, viral prediction, BuzzProphet, LLMs, social media  
Summary:  
BuzzProphet is a new framework for predicting hashtag popularity that combines the strengths of large language models (LLMs) and traditional regression models. It instructs LLMs to analyze a hashtag's virality, audience reach, and timing. By incorporating these insights into input features, BuzzProphet improves prediction accuracy and produces human-readable rationales for its forecasts. The framework outperforms traditional regressors, reducing RMSE by up to 2.8% and increasing correlation by 30% on a benchmark dataset of 7,532 hashtags. By leveraging LLMs for contextual reasoning rather than direct prediction, BuzzProphet offers a more interpretable and deployable solution for forecasting social media trends. <div>
arXiv:2510.08481v1 Announce Type: new 
Abstract: Hashtag trends ignite campaigns, shift public opinion, and steer millions of dollars in advertising spend, yet forecasting which tag goes viral is elusive. Classical regressors digest surface features but ignore context, while large language models (LLMs) excel at contextual reasoning but misestimate numbers. We present BuzzProphet, a reasoning-augmented hashtag popularity prediction framework that (1) instructs an LLM to articulate a hashtag's topical virality, audience reach, and timing advantage; (2) utilizes these popularity-oriented rationales to enrich the input features; and (3) regresses on these inputs. To facilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated from social media. Across diverse regressor-LLM combinations, BuzzProphet reduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while producing human-readable rationales. Results demonstrate that using LLMs as context reasoners rather than numeric predictors injects domain insight into tabular models, yielding an interpretable and deployable solution for social media trend forecasting.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments</title>
<link>https://arxiv.org/abs/2510.07359</link>
<guid>https://arxiv.org/abs/2510.07359</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, social media, urban environments, perception, opinion<br />
<br />
Summary: 
This study explores the impact of social media on urban sentiment analysis, focusing on the perceptions and opinions of residents in Beijing. By utilizing a dataset of street view images and social media posts, the researchers develop a reaction index to analyze sentiment trends in the Second Ring of Beijing. The findings reveal a shift towards more positive sentiment in perception but more extreme changes in opinion. Disparities between perception and opinion sentiments are identified, with significant relationships to elements such as building density and pedestrian presence. The study also highlights the impact of the pandemic on sentiment reactions and provides insights for environmental management and urban renewal strategies. <div>
arXiv:2510.07359v1 Announce Type: cross 
Abstract: The ascension of social media platforms has transformed our understanding of urban environments, giving rise to nuanced variations in sentiment reaction embedded within human perception and opinion, and challenging existing multidimensional sentiment analysis approaches in urban studies. This study presents novel methodologies for identifying and elucidating sentiment inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent Street view images to measure perceptions, and 984,024 Weibo social media text posts to measure opinions. A reaction index is developed, integrating object detection and natural language processing techniques to classify sentiment in Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is analysed and visualized using regression analysis, image segmentation, and word frequency based on land-use distribution to discern underlying factors. The perception affective reaction trend map reveals a shift toward more evenly distributed positive sentiment, while the opinion affective reaction trend map shows more extreme changes. Our mismatch map indicates significant disparities between the sentiments of human perception and opinion of urban areas over the years. Changes in sentiment reactions have significant relationships with elements such as dense buildings and pedestrian presence. Our inconsistent maps present perception and opinion sentiments before and after the pandemic and offer potential explanations and directions for environmental management, in formulating strategies for urban renewal.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynBenchmark: Customizable Ground Truths to Benchmark Community Detection and Tracking in Temporal Networks</title>
<link>https://arxiv.org/abs/2510.06245</link>
<guid>https://arxiv.org/abs/2510.06245</guid>
<content:encoded><![CDATA[
<div> Graph models, network dynamics, community detection algorithms, evolving community structures, temporal networks<br />
<br />
Summary: 
The article introduces a new community-centered model for generating evolving community structures in graphs. This model allows communities to grow, shrink, merge, split, appear, or disappear over time. It also creates a temporal network where nodes can appear, disappear, or move between communities. The benchmark includes tools to track community evolution and compare algorithm results with ground truth data. Three methods were tested using this benchmark to evaluate their performance in tracking nodes' cluster membership and detecting community evolution. Python libraries, drawing utilities, and validation metrics are provided to facilitate comparison between the ground truth and algorithm results. Overall, the benchmark offers a comprehensive tool for evaluating community detection algorithms in dynamic network environments. <br /><br /> <div>
arXiv:2510.06245v1 Announce Type: new 
Abstract: Graph models help understand network dynamics and evolution. Creating graphs with controlled topology and embedded partitions is a common strategy for evaluating community detection algorithms. However, existing benchmarks often overlook the need to track the evolution of communities in real-world networks. To address this, a new community-centered model is proposed to generate customizable evolving community structures where communities can grow, shrink, merge, split, appear or disappear. This benchmark also generates the underlying temporal network, where nodes can appear, disappear, or move between communities. The benchmark has been used to test three methods, measuring their performance in tracking nodes' cluster membership and detecting community evolution. Python libraries, drawing utilities, and validation metrics are provided to compare ground truth with algorithm results for detecting dynamic communities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Discourses on Childbirth and Parenthood in Popular Social Media Platforms Across China, Japan, and South Korea</title>
<link>https://arxiv.org/abs/2510.06788</link>
<guid>https://arxiv.org/abs/2510.06788</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, fertility desires, online discourse, family formation, low fertility rates 

Summary: 
The study examines online discussions surrounding childbirth and parenthood on popular video sharing platforms in China, South Korea, and Japan, regions known for their low fertility rates. By analyzing over 200,000 comments on 668 short videos, the research identifies key themes and sentiments expressed by users. The comments primarily focus on childrearing costs, the utility of children, and individualism, with differing sentiments across countries. Douyin comments display strong anti-natalist views, while Japanese and Korean comments are more neutral. The study also explores the impact of video characteristics, such as stances and account types, as well as regional socioeconomic indicators like GDP and urbanization, on the discourse. Overall, the research sheds light on the spread of family values online and provides valuable insights into the factors influencing fertility desires in these regions. 

<br /><br />Summary: <div>
arXiv:2510.06788v1 Announce Type: new 
Abstract: Social media use has been shown to be associated with low fertility desires. However, we know little about the discourses surrounding childbirth and parenthood that people consume online. We analyze 219,127 comments on 668 short videos related to reproduction and parenthood from Douyin and Tiktok in China, South Korea, and Japan, a region famous for its extremely low fertility level, to examine the topics and sentiment expressed online. BERTopic model is used to assist thematic analysis, and a large language model QWen is applied to label sentiment. We find that comments focus on childrearing costs in all countries, utility of children, particularly in Japan and South Korea, and individualism, primarily in China. Comments from Douyin exhibit the strongest anti-natalist sentiments, while the Japanese and Korean comments are more neutral. Short video characteristics, such as their stances or account type, significantly influence the responses, alongside regional socioeconomic indicators, including GDP, urbanization, and population sex ratio. This work provides one of the first comprehensive analyses of online discourses on family formation via popular algorithm-fed video sharing platforms in regions experiencing low fertility rates, making a valuable contribution to our understanding of the spread of family values online.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visualization of Interpersonal Communication using Indoor Positioning Technology with UWB Tags</title>
<link>https://arxiv.org/abs/2510.06797</link>
<guid>https://arxiv.org/abs/2510.06797</guid>
<content:encoded><![CDATA[
<div> Keywords: social gathering, UWB indoor positioning system, interpersonal communication, network analysis, community evolution

Summary: 
The study utilized a UWB indoor positioning system to track attendee movements and interactions during a social gathering on a university campus. Network and community analyses were conducted to study attendee interactions and the evolution of communities over time. By varying distance thresholds for defining contact, the study examined how it affected network structure and community analysis outcomes. The temporal evolution of communities identified through community analysis corresponded with visually observed groupings of participants. The study provides insights into understanding interpersonal communication patterns and community dynamics at social events through the use of technology, highlighting the potential of UWB indoor positioning systems for studying social interactions. The findings contribute to the growing body of research on social network analysis and community dynamics in real-world settings.<br /><br />Summary: <div>
arXiv:2510.06797v1 Announce Type: new 
Abstract: In conjunction with a social gathering held on a university campus, the movement of attendees were tracked within the venue for approximately two hours using a UWB indoor positioning system, in order to visualize their interpersonal communication. Network and community analyses were performed on attendee interaction data, and the evolution of communities over time was further investigated through repeated community analysis at different time points. Furthermore, recognizing the influence of distance thresholds on defining contact, we discussed how varying these thresholds affected the resulting network structure and community analysis outcomes. This study confirmed that the temporal evolution of communities identified through community analysis broadly corresponded with the visually observed groupings of participants using the UWB indoor positioning system.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit</title>
<link>https://arxiv.org/abs/2510.07226</link>
<guid>https://arxiv.org/abs/2510.07226</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, Machine-Generated Text, Reddit, Social Media, Engagement

Summary: 
Generative Artificial Intelligence is increasingly utilized for generating Machine-Generated Text (MGT) in online communication, particularly on platforms like Reddit. This study examines the prevalence and distribution of MGT across various subreddits from 2022-2024. The research reveals that although MGT is marginally present on Reddit, it can peak at up to 9% in certain communities. MGT is more common in technical and social support subreddits and is often created by a small group of users. Despite differences in style, MGT generates similar levels of engagement compared to human-authored content. It conveys unique social signals associated with AI assistants, such as warmth and status giving. This suggests that MGT is becoming an integral part of online social interactions. This analysis provides valuable insights into the impact of MGT on Reddit and opens up new avenues for research on platform governance and community dynamics. 

<br /><br />Summary: <div>
arXiv:2510.07226v1 Announce Type: new 
Abstract: Generative Artificial Intelligence is reshaping online communication by enabling large-scale production of Machine-Generated Text (MGT) at low cost. While its presence is rapidly growing across the Web, little is known about how MGT integrates into social media environments. In this paper, we present the first large-scale characterization of MGT on Reddit. Using a state-of-the-art statistical method for detection of MGT, we analyze over two years of activity (2022-2024) across 51 subreddits representative of Reddit's main community types such as information seeking, social support, and discussion. We study the concentration of MGT across communities and over time, and compared MGT to human-authored text in terms of social signals it expresses and engagement it receives. Our very conservative estimate of MGT prevalence indicates that synthetic text is marginally present on Reddit, but it can reach peaks of up to 9% in some communities in some months. MGT is unevenly distributed across communities, more prevalent in subreddits focused on technical knowledge and social support, and often concentrated in the activity of a small fraction of users. MGT also conveys distinct social signals of warmth and status giving typical of language of AI assistants. Despite these stylistic differences, MGT achieves engagement levels comparable than human-authored content and in a few cases even higher, suggesting that AI-generated text is becoming an organic component of online social discourse. This work offers the first perspective on the MGT footprint on Reddit, paving the way for new investigations involving platform governance, detection strategies, and community dynamics.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Generative Model for Human Mobility Behavior</title>
<link>https://arxiv.org/abs/2510.06473</link>
<guid>https://arxiv.org/abs/2510.06473</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, deep generative model, spatial scales, travel mode, societal implications

Summary:
MobilityGen is a deep generative model designed to simulate human mobility patterns over large spatial scales. By incorporating behavioral attributes and environmental context, MobilityGen can accurately replicate key patterns such as location visits, activity time allocation, and the interplay between travel mode and destination choices. This model captures spatio-temporal variability and produces diverse and realistic mobility trajectories consistent with real-world urban environments. Additionally, MobilityGen can provide insights into how urban space accessibility varies across different travel modes and how social exposure and segregation are influenced by co-presence dynamics. This framework opens up new avenues for studying human behavior and its societal impacts through fine-grained, data-driven simulations. <div>
arXiv:2510.06473v1 Announce Type: cross 
Abstract: Understanding and modeling human mobility is central to challenges in transport planning, sustainable urban design, and public health. Despite decades of effort, simulating individual mobility remains challenging because of its complex, context-dependent, and exploratory nature. Here, we present MobilityGen, a deep generative model that produces realistic mobility trajectories spanning days to weeks at large spatial scales. By linking behavioral attributes with environmental context, MobilityGen reproduces key patterns such as scaling laws for location visits, activity time allocation, and the coupled evolution of travel mode and destination choices. It reflects spatio-temporal variability and generates diverse, plausible, and novel mobility patterns consistent with the built environment. Beyond standard validation, MobilityGen yields insights not attainable with earlier models, including how access to urban space varies across travel modes and how co-presence dynamics shape social exposure and segregation. Our work establishes a new framework for mobility simulation, paving the way for fine-grained, data-driven studies of human behavior and its societal implications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Graph Clustering under Differential Privacy: Balancing Privacy, Accuracy, and Efficiency</title>
<link>https://arxiv.org/abs/2510.07136</link>
<guid>https://arxiv.org/abs/2510.07136</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral graph clustering, edge differential privacy, graph perturbation, private graph projection, noisy power iteration

Summary:
Graph clustering under edge differential privacy is studied in this work. Three mechanisms are developed for privacy-preserving graph analysis: randomized edge flipping combined with adjacency matrix shuffling, private graph projection with Gaussian noise, and a noisy power iteration method with distributed noise. The mechanisms ensure edge differential privacy while preserving key spectral properties of the graph. The analysis provides rigorous privacy guarantees and quantifies the misclassification error rate. Experimental validation on synthetic and real-world networks confirms the effectiveness of the proposed methods and demonstrates the privacy-utility trade-offs. The shuffled mechanism improves privacy guarantees as the number of nodes increases, providing (epsilon, delta) edge differential privacy. Private graph projection reduces dimensionality and computational complexity, while the noisy power iteration method maintains convergence with distributed Gaussian noise. <div>
arXiv:2510.07136v1 Announce Type: cross 
Abstract: We study the problem of spectral graph clustering under edge differential privacy (DP). Specifically, we develop three mechanisms: (i) graph perturbation via randomized edge flipping combined with adjacency matrix shuffling, which enforces edge privacy while preserving key spectral properties of the graph. Importantly, shuffling considerably amplifies the guarantees: whereas flipping edges with a fixed probability alone provides only a constant epsilon edge DP guarantee as the number of nodes grows, the shuffled mechanism achieves (epsilon, delta) edge DP with parameters that tend to zero as the number of nodes increase; (ii) private graph projection with additive Gaussian noise in a lower-dimensional space to reduce dimensionality and computational complexity; and (iii) a noisy power iteration method that distributes Gaussian noise across iterations to ensure edge DP while maintaining convergence. Our analysis provides rigorous privacy guarantees and a precise characterization of the misclassification error rate. Experiments on synthetic and real-world networks validate our theoretical analysis and illustrate the practical privacy-utility trade-offs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeSH Concept Relevance and Knowledge Evolution: A Data-driven Perspective</title>
<link>https://arxiv.org/abs/2406.18792</link>
<guid>https://arxiv.org/abs/2406.18792</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Subject Headings, information theory, network analysis, concept relevance, knowledge organization systems

Summary:
- The study focuses on quantifying the relevance of concepts in the Medical Subject Headings (MeSH) using a data-driven approach based on information theory and network analysis.
- Four aspects of relevance, including informativeness, usefulness, disruptiveness, and influence, are computed over time using article annotations and citation networks.
- The proposed method effectively captures the evolution of MeSH concepts, showing that evolving concepts have higher relevance compared to unchanged concepts.
- Analysis of retracted articles demonstrates that concepts used to annotate retracted articles differ significantly in relevance from those annotating non-retracted articles.
- The framework provides a method for ranking concept relevance and can support the maintenance of knowledge organization systems. 

<br /><br />Summary: 
The study presents a data-driven approach to quantify the relevance of Medical Subject Headings (MeSH) concepts using information theory and network analysis. By computing four aspects of relevance over time, the method effectively captures the evolution of MeSH concepts, showing higher relevance for evolving concepts. Analysis of retracted articles reveals significant differences in concept relevance between retracted and non-retracted articles. Overall, the framework offers a method for ranking concept relevance and supporting the maintenance of knowledge organization systems. <div>
arXiv:2406.18792v5 Announce Type: replace 
Abstract: The Medical Subject Headings (MeSH), one of the main knowledge organization systems in the biomedical domain, continuously evolves to reflect the latest scientific discoveries in health and life sciences. Previous research has focused on quantifying information in MeSH primarily through its hierarchical structure. In this work, we propose a data-driven approach based on information theory and network analysis to quantify the relevance of MeSH concepts. Our method leverages article annotations and their citation networks to compute four aspects of relevance -- informativeness, usefulness, disruptiveness, and influence -- over time. Using both the citation network and the MeSH hierarchy, we compute these relevance aspects and apply an aggregation algorithm to propagate scores to parent nodes. We evaluated our approach on MeSH terminology changes and showed that it effectively captures the evolution of concepts. The mean relevance of evolving concepts is higher compared to concepts that remained unchanged ($2.09E-03$ vs. $8.46E-04$). Moreover, we validated the framework by analyzing retracted articles and found that concepts used to annotate retracted articles (mean relevance: 0.17) differ substantially from those annotating non-retracted ones (mean relevance: 0.15). Overall, the proposed framework provides an effective method for ranking concept relevance and can support the maintenance of evolving knowledge organization systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Directedness in Social Contagion</title>
<link>https://arxiv.org/abs/2510.06012</link>
<guid>https://arxiv.org/abs/2510.06012</guid>
<content:encoded><![CDATA[
<div> Keywords: contagion theory, social networks, complex contagions, directed pathways, network evolution

Summary: 
An innovative causal modeling framework has been developed to address the challenge in predicting the pathways contagions follow through social networks. The study reveals a surprising discovery that complex contagions, requiring exposure to multiple peers for adoption, lead to asymmetric paths in contagion spread. Weak ties across network regions facilitate the spread of contagions from one community to another, challenging traditional theories. The emergence of directedness also channels complex contagions from the network periphery to the core, contrary to standard centrality models. Practical applications illustrate how emergent directedness explains nonlinear effects of tie strength in job diffusion on LinkedIn and how network evolution favors growing directed paths. Cultural factors like triadic closure can counteract this bias, with strategic implications for network building and behavioral interventions.<br /><br />Summary: <div>
arXiv:2510.06012v1 Announce Type: new 
Abstract: An enduring challenge in contagion theory is that the pathways contagions follow through social networks exhibit emergent complexities that are difficult to predict using network structure. Here, we address this challenge by developing a causal modeling framework that (i) simulates the possible network pathways that emerge as contagions spread and (ii) identifies which edges and nodes are most impactful on diffusion across these possible pathways. This yields a surprising discovery. If people require exposure to multiple peers to adopt a contagion (a.k.a., 'complex contagions'), the pathways that emerge often only work in one direction. In fact, the more complex a contagion is, the more asymmetric its paths become. This emergent directedness problematizes canonical theories of how networks mediate contagion. Weak ties spanning network regions - widely thought to facilitate mutual influence and integration - prove to privilege the spread contagions from one community to the other. Emergent directedness also disproportionately channels complex contagions from the network periphery to the core, inverting standard centrality models. We demonstrate two practical applications. We show that emergent directedness accounts for unexplained nonlinearity in the effects of tie strength in a recent study of job diffusion over LinkedIn. Lastly, we show that network evolution is biased toward growing directed paths, but that cultural factors (e.g., triadic closure) can curtail this bias, with strategic implications for network building and behavioral interventions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentZero++: Modeling Fear-Based Behavior</title>
<link>https://arxiv.org/abs/2510.05185</link>
<guid>https://arxiv.org/abs/2510.05185</guid>
<content:encoded><![CDATA[
<div> AgentZero++, agent-based model, decentralized collective violence, cognitive mechanisms, emotional mechanisms<br />
<br />
Summary:<br />
AgentZero++ is an enhanced agent-based model that simulates decentralized collective violence in spatially distributed systems. It integrates cognitive, emotional, and social mechanisms to capture various aspects of human behavior in conflict situations. The model includes enhancements such as age-based impulse control, memory-based risk estimation, and affect-cognition coupling, allowing agents to adapt based on internal states and social feedback. By incorporating features like endogenous destructive radius and retaliatory damage, the model generates emergent dynamics such as protest asymmetries and escalation cycles. Through modular experimentation and visualization, AgentZero++ demonstrates how micro-level cognitive heterogeneity influences macro-level conflict patterns. By explicitly modeling emotional thresholds, identity-driven behavior, and adaptive networks, the model provides insights into affective contagion and psychologically grounded collective action. <div>
arXiv:2510.05185v1 Announce Type: cross 
Abstract: We present AgentZero++, an agent-based model that integrates cognitive, emotional, and social mechanisms to simulate decentralized collective violence in spatially distributed systems. Building on Epstein's Agent\_Zero framework, we extend the original model with eight behavioral enhancements: age-based impulse control; memory-based risk estimation; affect-cognition coupling; endogenous destructive radius; fight-or-flight dynamics; affective homophily; retaliatory damage; and multi-agent coordination. These additions allow agents to adapt based on internal states, previous experiences, and social feedback, producing emergent dynamics such as protest asymmetries, escalation cycles, and localized retaliation. Implemented in Python using the Mesa ABM framework, AgentZero++ enables modular experimentation and visualization of how micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our results highlight how small variations in memory, reactivity, and affective alignment can amplify or dampen unrest through feedback loops. By explicitly modeling emotional thresholds, identity-driven behavior, and adaptive networks, this work contributes a flexible and extensible platform for analyzing affective contagion and psychologically grounded collective action.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inductive inference of gradient-boosted decision trees on graphs for insurance fraud detection</title>
<link>https://arxiv.org/abs/2510.05676</link>
<guid>https://arxiv.org/abs/2510.05676</guid>
<content:encoded><![CDATA[
<div> Graph, machine learning, insurance fraud, gradient boosting, explainability

Summary:
The article introduces a novel inductive graph gradient boosting machine (G-GBM) for supervised learning on heterogeneous and dynamic graphs, aiming to improve the detection of insurance fraud. Graph-based methods are popular for modeling complex data and relations, but face challenges with class imbalance and dynamic networks in fraud data. The G-GBM approach competes with graph neural network methods in experiments with simulated random graphs. It demonstrates effective fraud detection capabilities with open-source and real-world datasets. By utilizing gradient boosting forests as the backbone model, G-GBM allows for better explainability of predictions through established methods. Overall, the G-GBM offers a promising solution for enhancing fraud detection in insurance networks. 

<br /><br />Summary: <div>
arXiv:2510.05676v1 Announce Type: cross 
Abstract: Graph-based methods are becoming increasingly popular in machine learning due to their ability to model complex data and relations. Insurance fraud is a prime use case, since false claims are often the result of organised criminals that stage accidents or the same persons filing erroneous claims on multiple policies. One challenge is that graph-based approaches struggle to find meaningful representations of the data because of the high class imbalance present in fraud data. Another is that insurance networks are heterogeneous and dynamic, given the changing relations among people, companies and policies. That is why gradient boosted tree approaches on tabular data still dominate the field. Therefore, we present a novel inductive graph gradient boosting machine (G-GBM) for supervised learning on heterogeneous and dynamic graphs. We show that our estimator competes with popular graph neural network approaches in an experiment using a variety of simulated random graphs. We demonstrate the power of G-GBM for insurance fraud detection using an open-source and a real-world, proprietary dataset. Given that the backbone model is a gradient boosting forest, we apply established explainability methods to gain better insights into the predictions made by G-GBM.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overlapping community detection in weighted networks</title>
<link>https://arxiv.org/abs/2211.00894</link>
<guid>https://arxiv.org/abs/2211.00894</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, weighted networks, generative model, overlapping communities, modularity

Summary:
The article introduces a new generative model, the weighted degree-corrected mixed membership (WDCMM) model, designed for community detection in overlapping weighted networks. This model extends the previous degree-corrected mixed membership (DCMM) model to weighted networks by allowing for edge weights of any real value. The community membership estimation is achieved through a spectral algorithm with a theoretical guarantee of consistency. Additionally, the article introduces an overlapping weighted modularity measure to assess the quality of community detection in both assortative and dis-assortative weighted networks. The proposed modularity also assists in determining the optimal number of communities in the network. The model and modularity approach are demonstrated through applications on simulated data and real-world networks, showcasing their effectiveness in detecting overlapping communities in weighted networks. <br /><br />Summary: <div>
arXiv:2211.00894v3 Announce Type: replace 
Abstract: Over the past decade, community detection in overlapping un-weighted networks, where nodes can belong to multiple communities, has been one of the most popular topics in modern network science. However, community detection in overlapping weighted networks, where edge weights can be any real value, remains challenging. In this article, we propose a generative model called the weighted degree-corrected mixed membership (WDCMM) model to model such weighted networks. This model adopts the same factorization for the expectation of the adjacency matrix as the previous degree-corrected mixed membership (DCMM) model. Our WDCMM extends the DCMM from un-weighted networks to weighted networks by allowing the elements of the adjacency matrix to be generated from distributions beyond Bernoulli. We first address the community membership estimation of the model by applying a spectral algorithm and establishing a theoretical guarantee of consistency. Then, we propose overlapping weighted modularity to measure the quality of overlapping community detection for both assortative and dis-assortative weighted networks. To determine the number of communities, we incorporate the algorithm into the proposed modularity. We demonstrate the advantages of the model and the modularity through applications to simulated data and real-world networks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Online Community Detection for Censored Block Models: Algorithms and Fundamental Limits</title>
<link>https://arxiv.org/abs/2405.05724</link>
<guid>https://arxiv.org/abs/2405.05724</guid>
<content:encoded><![CDATA[
<div> edge differential privacy, online change detection, dynamic communities, censored block model, community estimation <br />
<br />
Summary: 
This study focuses on the private online change detection problem in dynamic communities using a censored block model. The research examines edge differential privacy in both local and central settings and proposes methods for joint change detection and community estimation. It delves into the tradeoffs between privacy budget, detection delay, and exact community recovery of community labels. The study provides theoretical guarantees for the effectiveness of the proposed method by establishing necessary and sufficient conditions for change detection and exact recovery under edge differential privacy. Simulation and real data examples are used to validate the proposed techniques. <div>
arXiv:2405.05724v2 Announce Type: replace 
Abstract: We study the private online change detection problem for dynamic communities, using a censored block model (CBM). We consider edge differential privacy (DP) in both local and central settings, and propose joint change detection and community estimation procedures for both scenarios. We seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels. Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions for change detection and exact recovery under edge DP. Simulation and real data examples are provided to validate the proposed methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</title>
<link>https://arxiv.org/abs/2502.04308</link>
<guid>https://arxiv.org/abs/2502.04308</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Higher-order Guided Diffusion, graph generation, topological structures, molecular graphs <br />
<br />Summary:
Graph generation is a challenging task requiring a deep understanding of complex structures. Existing diffusion models have limitations in capturing the topological properties of graphs due to their adaptation from image generation frameworks. In response, the proposed Higher-order Guided Diffusion (HOG-Diff) framework aims to generate plausible graphs with inherent topological structures by following a coarse-to-fine generation curriculum guided by higher-order topology. The model utilizes diffusion bridges and offers a stronger theoretical guarantee compared to classical diffusion frameworks. Experimental results on molecular and generic graph generation tasks show that HOG-Diff consistently outperforms or competes with state-of-the-art baselines. The code for the model is available for further exploration and application. <div>
arXiv:2502.04308v2 Announce Type: replace-cross 
Abstract: Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Diffusion models have recently made significant achievements in graph generation, but these models are typically adapted from image generation frameworks and overlook inherent higher-order topology, leaving them ill-suited for capturing the topological properties of graphs. In this work, we propose Higher-order Guided Diffusion (HOG-Diff), a principled framework that progressively generates plausible graphs with inherent topological structures. HOG-Diff follows a coarse-to-fine generation curriculum guided by higher-order topology and implemented via diffusion bridges. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at https://github.com/Yiminghh/HOG-Diff.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Exposure Mapping Functions for Inferring Heterogeneous Peer Effects</title>
<link>https://arxiv.org/abs/2503.01722</link>
<guid>https://arxiv.org/abs/2503.01722</guid>
<content:encoded><![CDATA[
<div> interference, peer effect, exposure mapping function, heterogeneous peer effects, EgoNetGNN <br />
<br />
Summary: 
Interference and peer effects in causal inference refer to how peers' actions can impact an individual's outcome, with exposure mapping functions typically used to represent peer exposure. Existing approaches often rely on simple functions like the number or fraction of treated peers, but complex influence mechanisms require more sophisticated methods. EgoNetGNN, a graph neural network approach, automates the learning of exposure mapping functions to capture diverse peer influence mechanisms. By considering peer treatments, local neighborhood structure, and edge attributes, EgoNetGNN excels in estimating heterogeneous peer effects, where outcomes vary for the same peer exposure across different contexts. Comparative evaluations on synthetic and semi-synthetic network data demonstrate the superiority of EgoNetGNN over traditional methods in accurately estimating peer effects in complex scenarios. <br /> <div>
arXiv:2503.01722v2 Announce Type: replace-cross 
Abstract: In causal inference, interference refers to the phenomenon in which the actions of peers in a network can influence an individual's outcome. Peer effect refers to the difference in counterfactual outcomes of an individual for different levels of peer exposure, the extent to which an individual is exposed to the treatments, actions, or behaviors of peers. Estimating peer effects requires deciding how to represent peer exposure. Typically, researchers define an exposure mapping function that aggregates peer treatments and outputs peer exposure. Most existing approaches for defining exposure mapping functions assume peer exposure based on the number or fraction of treated peers. Recent studies have investigated more complex functions of peer exposure which capture that different peers can exert different degrees of influence. However, none of these works have explicitly considered the problem of automatically learning the exposure mapping function. In this work, we focus on learning this function for the purpose of estimating heterogeneous peer effects, where heterogeneity refers to the variation in counterfactual outcomes for the same peer exposure but different individual's contexts. We develop EgoNetGNN, a graph neural network (GNN)-based method, to automatically learn the appropriate exposure mapping function allowing for complex peer influence mechanisms that, in addition to peer treatments, can involve the local neighborhood structure and edge attributes. We show that GNN models that use peer exposure based on the number or fraction of treated peers or learn peer exposure naively face difficulty accounting for such influence mechanisms. Our comprehensive evaluation on synthetic and semi-synthetic network data shows that our method is more robust to different unknown underlying influence mechanisms when estimating heterogeneous peer effects when compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization and the Rise of System-level Creativity in Science</title>
<link>https://arxiv.org/abs/2510.03240</link>
<guid>https://arxiv.org/abs/2510.03240</guid>
<content:encoded><![CDATA[
<div> Foundational work, extensional work, generalizations, innovation ecosystems, science policy  
Summary:  
- Innovation ecosystems require careful policy stewardship to drive sustained advance in human health, welfare, security, and prosperity.  
- New measures have been developed to decompose the influence of innovations based on foundational work, extensional work, and generalizations.  
- Foundational and extensional work within fields has declined, while generalizations across fields have increased and accelerated with the rise of the web, social media, and artificial intelligence.  
- The shift in the locus of innovation from within fields to across the system as a whole has significant implications for science policy.  
- The study uses 23 million scientific works to demonstrate these trends, highlighting the importance of understanding the dynamics of innovation in modern research and development.  
<br /><br />Summary: <div>
arXiv:2510.03240v1 Announce Type: new 
Abstract: Innovation ecosystems require careful policy stewardship to drive sustained advance in human health, welfare, security and prosperity. We develop new measures that reliably decompose the influence of innovations in terms of the degree to which each represents a field-level foundation, an extension of foundational work, or a generalization that synthesizes and modularizes contributions from distant fields to catalyze combinatorial innovation. Using 23 million scientific works, we demonstrate that while foundational and extensional work within fields has declined in recent years-a trend garnering much recent attention-generalizations across fields have increased and accelerated with the rise of the web, social media, and artificial intelligence, shifting the locus of innovation from within fields to across the system as a whole. We explore implications for science policy.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Minimum Labeling: Efficient Temporal Network Activations for Reachability and Equity</title>
<link>https://arxiv.org/abs/2510.03899</link>
<guid>https://arxiv.org/abs/2510.03899</guid>
<content:encoded><![CDATA[
<div> Fair Minimum Labeling, resource efficiency, fairness, networked systems, learning applications <br />
Summary:
The article introduces the Fair Minimum Labeling (FML) problem, which aims to optimize a minimum-cost temporal edge activation plan in networked systems to ensure equitable access to a target set for different groups of nodes. The problem is essential for systems where edge activations incur resource costs, such as distributed data collection and fair service restoration. The FML problem is shown to be NP-hard and challenging to approximate. Probabilistic approximation algorithms are presented to efficiently solve the FML problem, achieving the best possible guarantee for activation cost. The practical utility of FML is demonstrated in fair multi-source data aggregation tasks for training shared models, showing significant cost savings compared to baseline heuristics and promoting group-level fairness in learning-integrated networks. <br /><br /> <div>
arXiv:2510.03899v1 Announce Type: new 
Abstract: Balancing resource efficiency and fairness is critical in networked systems that support modern learning applications. We introduce the Fair Minimum Labeling (FML) problem: the task of designing a minimum-cost temporal edge activation plan that ensures each group of nodes in a network has sufficient access to a designated target set, according to specified coverage requirements. FML captures key trade-offs in systems where edge activations incur resource costs and equitable access is essential, such as distributed data collection, update dissemination in edge-cloud systems, and fair service restoration in critical infrastructure. We show that FML is NP-hard and $\Omega(\log |V|)$-hard to approximate, and we present probabilistic approximation algorithms that match this bound, achieving the best possible guarantee for the activation cost. We demonstrate the practical utility of FML in a fair multi-source data aggregation task for training a shared model. Empirical results show that FML enforces group-level fairness with substantially lower activation cost than baseline heuristics, underscoring its potential for building resource-efficient, equitable temporal reachability in learning-integrated networks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning framework for predicting stochastic take-off and die-out of early spreading</title>
<link>https://arxiv.org/abs/2510.04574</link>
<guid>https://arxiv.org/abs/2510.04574</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic forecasting, deep learning, stochastic spreading, early intervention, public health<br />
Summary:<br />
- The study introduces a systematic framework for predicting whether initial transmission events will escalate into major outbreaks or fade away during early stages.
- It addresses challenges like inadequate data during the early stages of outbreaks and the focus of established models on average behaviors of large epidemics.
- A deep learning framework was developed using extensive data from stochastic spreading models to predict early-stage spreading outcomes accurately.
- The proposed pretrain-finetune framework leverages diverse simulation data for pretraining and adapts to specific scenarios through targeted fine-tuning, outperforming baseline models consistently.
- This work is the first to present a framework for predicting stochastic take-off versus die-out, providing valuable insights for epidemic preparedness and public health decision-making, enabling more informed early intervention strategies.<br /> 
Summary: <div>
arXiv:2510.04574v1 Announce Type: new 
Abstract: Large-scale outbreaks of epidemics, misinformation, or other harmful contagions pose significant threats to human society, yet the fundamental question of whether an emerging outbreak will escalate into a major epidemic or naturally die out remains largely unaddressed. This problem is challenging, partially due to inadequate data during the early stages of outbreaks and also because established models focus on average behaviors of large epidemics rather than the stochastic nature of small transmission chains. Here, we introduce the first systematic framework for forecasting whether initial transmission events will amplify into major outbreaks or fade into extinction during early stages, when intervention strategies can still be effectively implemented. Using extensive data from stochastic spreading models, we developed a deep learning framework that predicts early-stage spreading outcomes in real-time. Validation across Erd\H{o}s-R\'enyi and Barab\'asi-Albert networks with varying infectivity levels shows our method accurately forecasts stochastic spreading events well before potential outbreaks, demonstrating robust performance across different network structures and infectivity scenarios.To address the challenge of sparse data during early outbreak stages, we further propose a pretrain-finetune framework that leverages diverse simulation data for pretraining and adapts to specific scenarios through targeted fine-tuning. The pretrain-finetune framework consistently outperforms baseline models, achieving superior performance even when trained on limited scenario-specific data. To our knowledge, this work presents the first framework for predicting stochastic take-off versus die-out. This framework provides valuable insights for epidemic preparedness and public health decision-making, enabling more informed early intervention strategies.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Network Structure Inference: A Topological Approach to Network Selection</title>
<link>https://arxiv.org/abs/2510.04884</link>
<guid>https://arxiv.org/abs/2510.04884</guid>
<content:encoded><![CDATA[
<div> thresholding, network data, topological data analysis, persistent homology, parameterization

Summary:<br />
Thresholding is crucial in network data analysis but existing methods have limitations. A new systematic algorithm leveraging topological data analysis is introduced to optimize network parameters by considering higher-order structural relationships. It uses persistent homology to assess the stability of homological features across the parameter space, enabling the identification of robust parameter choices preserving meaningful topological structure. Hyperparameters allow users to specify minimum requirements for topological features, guiding the parameter search to avoid spurious solutions. The method is demonstrated in the Science of Science, extracting networks of scientific concepts from research paper abstracts. This approach offers flexibility for incorporating domain-specific constraints and extends to general parameterization problems in data analysis. <div>
arXiv:2510.04884v1 Announce Type: new 
Abstract: Thresholding--the pruning of nodes or edges based on their properties or weights--is an essential preprocessing tool for extracting interpretable structure from complex network data, yet existing methods face several key limitations. Threshold selection often relies on heuristic methods or trial and error due to large parameter spaces and unclear optimization criteria, leading to sensitivity where small parameter variations produce significant changes in network structure. Moreover, most approaches focus on pairwise relationships between nodes, overlooking critical higher-order interactions involving three or more nodes. We introduce a systematic thresholding algorithm that leverages topological data analysis to identify optimal network parameters by accounting for higher-order structural relationships. Our method uses persistent homology to compute the stability of homological features across the parameter space, identifying parameter choices that are robust to small variations while preserving meaningful topological structure. Hyperparameters allow users to specify minimum requirements for topological features, effectively constraining the parameter search to avoid spurious solutions. We demonstrate the approach with an application in the Science of Science, where networks of scientific concepts are extracted from research paper abstracts, and concepts are connected when they co-appear in the same abstract. The flexibility of our approach allows researchers to incorporate domain-specific constraints and extends beyond network thresholding to general parameterization problems in data analysis.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Does the Engineering Manager Still Exist in Agile Software Development?</title>
<link>https://arxiv.org/abs/2510.03920</link>
<guid>https://arxiv.org/abs/2510.03920</guid>
<content:encoded><![CDATA[
<div> Keywords: Agile methodologies, engineering managers, decentralized decision-making, team autonomy, managerial hierarchy 

Summary: This paper discusses the presence of engineering managers in Agile software organizations despite the emphasis on decentralized decision-making and team autonomy. Through a multidimensional framework, including historical context, theoretical tensions, organizational realities, empirical evidence, evolving managerial roles, and practical implications, the study explores the reasons for this paradox. A systematic literature review and case studies support the analysis, leading to the proposal of a conceptual model that reconciles Agile principles with managerial necessity. The model aims to provide guidance for practitioners, researchers, and tool designers, while also discussing implications for leadership development, tool integration, and future research. Ultimately, the study sheds light on the evolving role of engineering managers within Agile environments and advocates for a balance between Agile principles and traditional managerial functions. 

<br /><br />Summary: This paper delves into the persistence of engineering managers in Agile software organizations despite the emphasis on decentralized decision-making and team autonomy. Through a comprehensive framework, the study explores the historical, theoretical, and practical aspects influencing this phenomenon. By proposing a conceptual model reconciling Agile principles with managerial necessity, the paper provides guidance for practitioners, researchers, and tool designers. Additionally, implications for leadership development, tool integration, and future research are thoroughly discussed, highlighting the evolving role of engineering managers within Agile environments. <div>
arXiv:2510.03920v1 Announce Type: cross 
Abstract: Although Agile methodologies emphasize decentralized decision-making and team autonomy, engineering managers continue to be employed in Agile software organizations. This apparent paradox suggests that traditional managerial functions persist despite the theoretical displacement of managerial hierarchy in Agile. This paper explores the persistence of engineering managers through a multidimensional framework encompassing historical context, theoretical tensions, organizational realities, empirical evidence, evolving managerial roles, and practical implications. A systematic literature review underpins our multifaceted analysis, supplemented by illustrative case studies. We conclude by proposing a conceptual model that reconciles Agile principles with managerial necessity, offering guidance for practitioners, researchers, and tool designers. Implications for leadership development, tool integration, and future research are discussed.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal World Models as Imagination Networks in Cognitive Agents</title>
<link>https://arxiv.org/abs/2510.04391</link>
<guid>https://arxiv.org/abs/2510.04391</guid>
<content:encoded><![CDATA[
<div> imagination, internal world model, psychological network analysis, human groups, large language models<br />
<br />
Summary: In this study, the computational objective of imagination is explored through the concept of accessing an internal world model (IWM). Imagination vividness ratings were assessed through questionnaires, and imagination networks were constructed for comparison between humans and large language models (LLMs). Human groups displayed correlations between different centrality measures in their imagination networks, indicating similarities in IWMs. However, LLMs showed a lack of clustering and lower correlations between centrality measures, suggesting differences in internally-generated representations. The study highlights a novel method for comparing IWMs in humans and AI, providing insights for the development of human-like imagination in artificial intelligence. <div>
arXiv:2510.04391v1 Announce Type: cross 
Abstract: What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?</title>
<link>https://arxiv.org/abs/2510.04434</link>
<guid>https://arxiv.org/abs/2510.04434</guid>
<content:encoded><![CDATA[
<div> authors, venue-level, NLP4SG, social good concerns, ACL community
Summary:<br />
1. The study examines the landscape of NLP4SG from author and venue perspectives, highlighting the growing importance of NLP for social good initiatives.
2. Nearly 20% of all recent papers in the ACL Anthology focus on social good topics defined by the UN Sustainable Development Goals.
3. ACL authors are more likely to address social good concerns when publishing outside of ACL venues, indicating a shift in focus.
4. The majority of publications using NLP for social good purposes are authored by non-ACL authors in venues outside of ACL, suggesting a broader engagement in such initiatives.
5. The findings have implications for the ACL community in setting agendas related to NLP4SG. <br /><br /> <div>
arXiv:2510.04434v1 Announce Type: cross 
Abstract: The social impact of Natural Language Processing (NLP) is increasingly important, with a rising community focus on initiatives related to NLP for Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the ACL Anthology address topics related to social good as defined by the UN Sustainable Development Goals (Adauto et al., 2023). In this study, we take an author- and venue-level perspective to map the landscape of NLP4SG, quantifying the proportion of work addressing social good concerns both within and beyond the ACL community, by both core ACL contributors and non-ACL authors. With this approach we discover two surprising facts about the landscape of NLP4SG. First, ACL authors are dramatically more likely to do work addressing social good concerns when publishing in venues outside of ACL. Second, the vast majority of publications using NLP techniques to address concerns of social good are done by non-ACL authors in venues outside of ACL. We discuss the implications of these findings on agenda-setting considerations for the ACL community related to NLP4SG.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete scalar curvature as a weighted sum of Ollivier-Ricci curvatures</title>
<link>https://arxiv.org/abs/2510.04936</link>
<guid>https://arxiv.org/abs/2510.04936</guid>
<content:encoded><![CDATA[
<div> Ollivier-Ricci curvature, scalar curvature, discrete setting, point clouds, graphs <br />
<br />
Summary: The study explores the connection between discrete versions of Ricci and scalar curvature in point clouds and graphs. Ollivier-Ricci curvature substitutes Ricci curvature in the discrete context, with a new scalar Ollivier-Ricci curvature definition based on the trace of Ricci curvature in Riemannian manifolds. The proposed scalar Ollivier-Ricci curvature calculation converges to scalar curvature for nearest neighbor graphs derived from manifold sampling. The research also establishes novel findings on the convergence of Ollivier-Ricci curvature to Ricci curvature, enhancing our understanding of the relationship between discrete analogues of curvature measures in geometric structures. <div>
arXiv:2510.04936v1 Announce Type: cross 
Abstract: We study the relationship between discrete analogues of Ricci and scalar curvature that are defined for point clouds and graphs. In the discrete setting, Ricci curvature is replaced by Ollivier-Ricci curvature. Scalar curvature can be computed as the trace of Ricci curvature for a Riemannian manifold; this motivates a new definition of a scalar version of Ollivier-Ricci curvature. We show that our definition converges to scalar curvature for nearest neighbor graphs obtained by sampling from a manifold. We also prove some new results about the convergence of Ollivier-Ricci curvature to Ricci curvature.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGRDN-Data learned sparsification of graph reaction-diffusion networks</title>
<link>https://arxiv.org/abs/2303.11943</link>
<guid>https://arxiv.org/abs/2303.11943</guid>
<content:encoded><![CDATA[
<div> Graph Sparsification, Reaction-Diffusion Systems, Data Assimilation, Reduced Order Model, Spectral Preservation<br />
Summary:<br />
Graph sparsification is a challenging problem in computer science and applied mathematics, aiming to reduce the number of edges while preserving certain graph properties. The proposed SGRDN method extends sparsification to complex reaction-diffusion systems, ensuring the preservation of dynamics on the resulting structure. By framing network sparsification as a data assimilation problem in a Reduced Order Model space, SGRDN conserves the eigenmodes of the Laplacian matrix despite perturbations. Efficient eigenvalue and eigenvector approximations for perturbed Laplacian matrices are integrated as spectral constraints in the optimization process. The method's versatility is demonstrated through successful parameter sparsity achievement in Neural Ordinary Differential Equations (neural ODEs). <div>
arXiv:2303.11943v4 Announce Type: replace 
Abstract: Graph sparsification is an area of interest in computer science and applied mathematics. Sparsification of a graph, in general, aims to reduce the number of edges in the network while preserving specific properties of the graph, like cuts and subgraph counts. Computing the sparsest cuts of a graph is known to be NP-hard, and sparsification routines exist for generating linear-sized sparsifiers in almost quadratic running time $O(n^{2 + \epsilon})$. Consequently, obtaining a sparsifier can be a computationally demanding task, and the complexity varies based on the level of sparsity required. We propose SGRDN to extend sparsification to complex reaction-diffusion systems. This approach seeks to sparsify the graph such that the inherent reaction-diffusion dynamics are strictly preserved on the resulting structure. By selectively considering a subset of trajectories, we frame the network sparsification issue as a data assimilation problem within a Reduced Order Model (ROM) space, imposing constraints to conserve the eigenmodes of the Laplacian matrix ($L = D - A$), the difference between the degree matrix ($D$) and the adjacency matrix ($A$) despite perturbations. We derive computationally efficient eigenvalue and eigenvector approximations for perturbed Laplacian matrices and integrate these as spectral preservation constraints in the optimization problem. To further validate the method's broad applicability, we conducted an additional experiment on Neural Ordinary Differential Equations (neural ODEs), where SGRDN successfully achieved parameter sparsity.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics</title>
<link>https://arxiv.org/abs/2504.15927</link>
<guid>https://arxiv.org/abs/2504.15927</guid>
<content:encoded><![CDATA[
<div> Keywords: semi-supervised community detection, crystallization kinetics, CLique ANNealing, Transitive Annealer, scalability<br />
Summary:<br />
This article introduces a novel approach, CLique ANNealing (CLANN), for semi-supervised community detection. By integrating concepts from crystallization kinetics, the method aims to improve the identification of community cores and enhance scalability. The process is likened to the annealing process in crystal formation, where a core expands into a complete grain. CLANN optimizes the consistency of the community core and employs a Transitive Annealer to refine candidates by merging cliques and repositioning the core. Experimental results on various network settings demonstrate that CLANN outperforms existing methods on multiple real-world datasets, showing superior efficacy and efficiency in community detection.<br /><br />Summary: <div>
arXiv:2504.15927v2 Announce Type: replace 
Abstract: Semi-supervised community detection methods are widely used for identifying specific communities due to the label scarcity. Existing semi-supervised community detection methods typically involve two learning stages learning in both initial identification and subsequent adjustment, which often starts from an unreasonable community core candidate. Moreover, these methods encounter scalability issues because they depend on reinforcement learning and generative adversarial networks, leading to higher computational costs and restricting the selection of candidates. To address these limitations, we draw a parallel between crystallization kinetics and community detection to integrate the spontaneity of the annealing process into community detection. Specifically, we liken community detection to identifying a crystal subgrain (core) that expands into a complete grain (community) through a process similar to annealing. Based on this finding, we propose CLique ANNealing (CLANN), which applies kinetics concepts to community detection by integrating these principles into the optimization process to strengthen the consistency of the community core. Subsequently, a learning-free Transitive Annealer was employed to refine the first-stage candidates by merging neighboring cliques and repositioning the community core, enabling a spontaneous growth process that enhances scalability. Extensive experiments on \textbf{43} different network settings demonstrate that CLANN outperforms state-of-the-art methods across multiple real-world datasets, showcasing its exceptional efficacy and efficiency in community detection.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Higher-Order Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.12841</link>
<guid>https://arxiv.org/abs/2406.12841</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Higher-order, Topological Deep Learning, Taxonomy, Analysis

Summary:
Higher-order graph neural networks (HOGNNs) and Topological Deep Learning models are essential for handling complex relationships between vertices in graphs. The diversity of HOGNN models makes it challenging to compare and select the most suitable one for a given scenario. To address this, a taxonomy and blueprint for HOGNNs are proposed to guide model design for optimal performance. Using this taxonomy, an analysis and comparison of existing HOGNN models are conducted to provide insights for model selection. The study highlights the need for further research to enhance the power of HOGNNs, addressing challenges and opportunities in the field. This comprehensive analysis aids in navigating the landscape of HOGNN models and facilitates informed decision-making for employing GNN models in various applications. 

<br /><br />Summary: <div>
arXiv:2406.12841v3 Announce Type: replace-cross 
Abstract: Higher-order graph neural networks (HOGNNs) and the related architectures from Topological Deep Learning are an important class of GNN models that harness polyadic relations between vertices beyond plain edges. They have been used to eliminate issues such as over-smoothing or over-squashing, to significantly enhance the accuracy of GNN predictions, to improve the expressiveness of GNN architectures, and for numerous other goals. A plethora of HOGNN models have been introduced, and they come with diverse neural architectures, and even with different notions of what the "higher-order" means. This richness makes it very challenging to appropriately analyze and compare HOGNN models, and to decide in what scenario to use specific ones. To alleviate this, we first design an in-depth taxonomy and a blueprint for HOGNNs. This facilitates designing models that maximize performance. Then, we use our taxonomy to analyze and compare the available HOGNN models. The outcomes of our analysis are synthesized in a set of insights that help to select the most beneficial GNN model in a given scenario, and a comprehensive list of challenges and opportunities for further research into more powerful HOGNNs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HP-BERT: A framework for longitudinal study of Hinduphobia on social media via language models</title>
<link>https://arxiv.org/abs/2501.05482</link>
<guid>https://arxiv.org/abs/2501.05482</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, Hinduphobia, Social media analysis, Sentiment analysis, Discrimination

Summary: 
This study focuses on analyzing anti-Hindu sentiment, also known as Hinduphobia, during the COVID-19 pandemic using a computational framework and a newly developed Hinduphobic BERT (HP-BERT) model. The researchers curated and released a dataset of 8,000 annotated tweets for analysis. The HP-BERT model achieved high accuracy of 94.72% in detecting Hinduphobic content on social media platform X, outperforming baseline models. Analysis of approximately 27.4 million tweets across six countries revealed a correlation between COVID-19 case increases and the volume of Hinduphobic content, suggesting a link between pandemic-related stress and discriminatory discourse. The study provides evidence of religious discrimination against Hindu communities on social media platforms during the COVID-19 crisis.<br /><br />Summary: <div>
arXiv:2501.05482v2 Announce Type: replace-cross 
Abstract: During the COVID-19 pandemic, community tensions intensified, contributing to discriminatory sentiments against various religious groups, including Hindu communities. Recent advances in language models have shown promise for social media analysis with potential for longitudinal studies of social media platforms, such as X (Twitter). We present a computational framework for analyzing anti-Hindu sentiment (Hinduphobia) during the COVID-19 period, introducing an abuse detection and sentiment analysis approach for longitudinal analysis on X. We curate and release a "Hinduphobic COVID-19 XDataset" containing 8,000 annotated and manually verified tweets. We then develop the Hinduphobic BERT (HP-BERT) model using this dataset and achieve 94.72\% accuracy, outperforming baseline Transformer-based language models. The model incorporates multi-label sentiment analysis capabilities through additional fine-tuning. Our analysis encompasses approximately 27.4 million tweets from six countries, including Australia, Brazil, India, Indonesia, Japan, and the United Kingdom. Statistical analysis reveals moderate correlations (r = 0.312-0.428) between COVID-19 case increases and Hinduphobic content volume, highlighting how pandemic-related stress may contribute to discriminatory discourse. This study provides evidence of social media-based religious discrimination during a COVID-19 crisis.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Sparsification of Simplicial Complexes via Local Densities of States</title>
<link>https://arxiv.org/abs/2502.07558</link>
<guid>https://arxiv.org/abs/2502.07558</guid>
<content:encoded><![CDATA[
<div> Keywords: simplicial complexes, sparsification, local densities of states, spectral approximation, computational complexity

Summary:
This article introduces a novel method for probabilistic sparsification of simplicial complexes, aiming to reduce computational requirements while maintaining the original spectrum closely. The method utilizes local densities of states to approximate the generalized effective resistance of each simplex, guiding the sampling probability for sparsification. To prevent degenerate structures in the spectrum, a "kernel-ignoring" decomposition is proposed. Error estimates are used to analyze the algorithmic complexity of the method. The framework is tested on Vietoris-Rips filtered simplicial complexes, showcasing its performance in reducing the complexity of dense structures. This approach provides a promising avenue for efficiently analyzing dense simplicial complexes in topological data analysis and signal processing.<br /><br />Summary: <div>
arXiv:2502.07558v2 Announce Type: replace-cross 
Abstract: Simplicial complexes (SCs) have become a popular abstraction for analyzing complex data using tools from topological data analysis or topological signal processing. However, the analysis of many real-world datasets often leads to dense SCs, with many higher-order simplicies, which results in prohibitive computational requirements in terms of time and memory consumption. The sparsification of such complexes is thus of broad interest, i.e., the approximation of an original SC with a sparser surrogate SC (with typically only a log-linear number of simplices) that maintains the spectrum of the original SC as closely as possible. In this work, we develop a novel method for a probabilistic sparsification of SCs that uses so-called local densities of states. Using this local densities of states, we can efficiently approximate so-called generalized effective resistance of each simplex, which is proportional to the required sampling probability for the sparsification of the SC. To avoid degenerate structures in the spectrum of the corresponding Hodge Laplacian operators, we suggest a ``kernel-ignoring'' decomposition to approximate the sampling probability. Additionally, we utilize certain error estimates to characterize the asymptotic algorithmic complexity of the developed method. We demonstrate the performance of our framework on a family of Vietoris--Rips filtered simplicial complexes.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Asymptomatic Nodes in Network Epidemics using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2510.02568</link>
<guid>https://arxiv.org/abs/2510.02568</guid>
<content:encoded><![CDATA[
<div> Keywords: asymptomatic individuals, epidemic control, Graph Neural Network, supervised learning, network models

Summary:
Asymptomatic individuals play a crucial role in the spread of epidemics by carrying and transmitting infections without showing symptoms. Detecting these individuals is essential for effective epidemic control but costly periodic testing is impractical. This study addresses the challenge of identifying asymptomatic individuals using a Graph Neural Network (GNN) model with supervised learning. By leveraging node features extracted from observed infected nodes in a classic SI network epidemic model, the GNN accurately classifies healthy nodes as asymptomatic or susceptible. The approach demonstrates robustness across various network models, sizes, and observed infection rates, effectively distinguishing asymptomatic carriers. The findings highlight the efficacy and generalizability of the proposed methodology in detecting asymptomatic individuals without the need for extensive testing, offering a promising strategy for controlling epidemics. 

<br /><br />Summary: <div>
arXiv:2510.02568v1 Announce Type: new 
Abstract: Infected individuals in some epidemics can remain asymptomatic while still carrying and transmitting the infection. These individuals contribute to the spread of the epidemic and pose a significant challenge to public health policies. Identifying asymptomatic individuals is critical for measuring and controlling an epidemic, but periodic and widespread testing of healthy individuals is often too costly. This work tackles the problem of identifying asymptomatic individuals considering a classic SI (Susceptible-Infected) network epidemic model where a fraction of the infected nodes are not observed as infected (i.e., their observed state is identical to susceptible nodes). In order to classify healthy nodes as asymptomatic or susceptible, a Graph Neural Network (GNN) model with supervised learning is adopted where a set of node features are built from the network with observed infected nodes. The approach is evaluated across different network models, network sizes, and fraction of observed infections. Results indicate that the proposed methodology is robust across different scenarios, accurately identifying asymptomatic nodes while also generalizing to different network sizes and fraction of observed infections.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility Datasets Enriched With Contextual and Social Dimensions</title>
<link>https://arxiv.org/abs/2510.02333</link>
<guid>https://arxiv.org/abs/2510.02333</guid>
<content:encoded><![CDATA[
<div> Keywords: human trajectories, datasets, semantic enrichment, large cities, open source pipeline

Summary: 
This resource paper introduces two publicly available datasets of semantically enriched human trajectories from GPS traces sourced from OpenStreetMap. The datasets include contextual layers such as stops, moves, POIs, transportation modes, and weather data, as well as synthetic social media posts generated by Large Language Models (LLMs). Available in tabular and RDF formats, the datasets cover Paris and New York, supporting behavior modeling, mobility prediction, knowledge graph construction, and LLM-based applications. The open-source pipeline enables dataset customization, while the datasets offer a unique combination of real-world movement data, structured semantic enrichment, LLM-generated text, and semantic web compatibility. This resource is a significant advancement in enabling multimodal and semantic mobility analysis using real-world data. 

<br /><br />Summary: <div>
arXiv:2510.02333v1 Announce Type: cross 
Abstract: In this resource paper, we present two publicly available datasets of semantically enriched human trajectories, together with the pipeline to build them. The trajectories are publicly available GPS traces retrieved from OpenStreetMap. Each dataset includes contextual layers such as stops, moves, points of interest (POIs), inferred transportation modes, and weather data. A novel semantic feature is the inclusion of synthetic, realistic social media posts generated by Large Language Models (LLMs), enabling multimodal and semantic mobility analysis. The datasets are available in both tabular and Resource Description Framework (RDF) formats, supporting semantic reasoning and FAIR data practices. They cover two structurally distinct, large cities: Paris and New York. Our open source reproducible pipeline allows for dataset customization, while the datasets support research tasks such as behavior modeling, mobility prediction, knowledge graph construction, and LLM-based applications. To our knowledge, our resource is the first to combine real-world movement, structured semantic enrichment, LLM-generated text, and semantic web compatibility in a reusable framework.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily-induced Emergence of Biased Structures in LLM-based Multi-Agent AI Systems</title>
<link>https://arxiv.org/abs/2510.02637</link>
<guid>https://arxiv.org/abs/2510.02637</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, large language models, network evolution, preferential attachment, homophilic communities

Summary:
This study explores how interactions among artificially intelligent agents, driven by large language models, influence the evolution of collective network structures. Through experiments with four different LLMs, the researchers found that agents exhibit preferential attachment, forming connections with nodes of higher degrees. Incorporating social attributes like age, gender, religion, and political orientation led to the development of assortative networks, resulting in the formation of homophilic communities. Political and religious attributes played significant roles in fragmenting the network and creating polarized subgroups. Age and gender, on the other hand, resulted in more gradual shifts in network structure. The study also identified asymmetric patterns in heterophilous ties, indicating embedded directional biases influenced by societal norms. These findings highlight how AI-driven systems impact network topology and offer insights into the co-evolution and self-organization of AI collectives. 

<br /><br />Summary: <div>
arXiv:2510.02637v1 Announce Type: cross 
Abstract: This study examines how interactions among artificially intelligent (AI) agents, guided by large language models (LLMs), drive the evolution of collective network structures. We ask LLM-driven agents to grow a network by informing them about current link constellations. Our observations confirm that agents consistently apply a preferential attachment mechanism, favoring connections to nodes with higher degrees. We systematically solicited more than a million decisions from four different LLMs, including Gemini, ChatGPT, Llama, and Claude. When social attributes such as age, gender, religion, and political orientation are incorporated, the resulting networks exhibit heightened assortativity, leading to the formation of distinct homophilic communities. This significantly alters the network topology from what would be expected under a pure preferential attachment model alone. Political and religious attributes most significantly fragment the collective, fostering polarized subgroups, while age and gender yield more gradual structural shifts. Strikingly, LLMs also reveal asymmetric patterns in heterophilous ties, suggesting embedded directional biases reflective of societal norms. As autonomous AI agents increasingly shape the architecture of online systems, these findings contribute to how algorithmic choices of generative AI collectives not only reshape network topology, but offer critical insights into how AI-driven systems co-evolve and self-organize.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisitHGNN: Heterogeneous Graph Neural Networks for Modeling Point-of-Interest Visit Patterns</title>
<link>https://arxiv.org/abs/2510.02702</link>
<guid>https://arxiv.org/abs/2510.02702</guid>
<content:encoded><![CDATA[
<div> Keywords: urban transportation, graph neural network, mobility data, public health, spatial analysis

Summary:
VisitHGNN, a graph neural network, is introduced to predict visit probabilities from neighborhoods to specific destinations based on historical origin-to-destination flow patterns. The model considers spatial, temporal, and functional relations among urban places, incorporating attributes of Points of interest (POIs) and census block groups (CBGs). By leveraging spatial proximity, temporal co-activity, and brand affinity, VisitHGNN outperforms baseline models in predicting visit probabilities accurately. The model's strong predictive performance aligns closely with empirical visitation patterns and mirrors observed travel behavior with high fidelity. This approach has significant implications for urban planning, transportation policy, mobility system design, and public health by providing valuable insights for decision support in improving transportation planning, mobility management, and public health initiatives. 

<br /><br />Summary: <div>
arXiv:2510.02702v1 Announce Type: cross 
Abstract: Understanding how urban residents travel between neighborhoods and destinations is critical for transportation planning, mobility management, and public health. By mining historical origin-to-destination flow patterns with spatial, temporal, and functional relations among urban places, we estimate probabilities of visits from neighborhoods to specific destinations. These probabilities capture neighborhood-level contributions to citywide vehicular and foot traffic, supporting demand estimation, accessibility assessment, and multimodal planning. Particularly, we introduce VisitHGNN, a heterogeneous, relation-specific graph neural network designed to predict visit probabilities at individual Points of interest (POIs). POIs are characterized using numerical, JSON-derived, and textual attributes, augmented with fixed summaries of POI--POI spatial proximity, temporal co-activity, and brand affinity, while census block groups (CBGs) are described with 72 socio-demographic variables. CBGs are connected via spatial adjacency, and POIs and CBGs are linked through distance-annotated cross-type edges. Inference is constrained to a distance-based candidate set of plausible origin CBGs, and training minimizes a masked Kullback-Leibler (KL) divergence to yield probability distribution across the candidate set. Using weekly mobility data from Fulton County, Georgia, USA, VisitHGNN achieves strong predictive performance with mean KL divergence of 0.287, MAE of 0.008, Top-1 accuracy of 0.853, and R-square of 0.892, substantially outperforming pairwise MLP and distance-only baselines, and aligning closely with empirical visitation patterns (NDCG@50 = 0.966); Recall@5 = 0.611). The resulting distributions closely mirror observed travel behavior with high fidelity, highlighting the model's potential for decision support in urban planning, transportation policy, mobility system design, and public health.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media</title>
<link>https://arxiv.org/abs/2510.02811</link>
<guid>https://arxiv.org/abs/2510.02811</guid>
<content:encoded><![CDATA[
<div> MBTI9k, PANDORA, Reddit, personality assessment, NLP
Summary:
- Challenges in personality assessment from digital footprints include data scarcity and disconnect between personality psychology and NLP.
- Two datasets, MBTI9k and PANDORA, were collected from Reddit to address challenges in data size, quality, and label coverage.
- Experiments show that demographic variables impact model validity in personality assessment.
- The SIMPA framework was developed for interpretable personality assessment by matching user-generated statements with validated questionnaire items.
- SIMPA's model-agnostic design, layered cue detection, and scalability make it suitable for various research and practical applications involving complex label taxonomies and variable cue associations with target concepts.<br /><br />Summary: <div>
arXiv:2510.02811v1 Announce Type: cross 
Abstract: Personality refers to individual differences in behavior, thinking, and feeling. With the growing availability of digital footprints, especially from social media, automated methods for personality assessment have become increasingly important. Natural language processing (NLP) enables the analysis of unstructured text data to identify personality indicators. However, two main challenges remain central to this thesis: the scarcity of large, personality-labeled datasets and the disconnect between personality psychology and NLP, which restricts model validity and interpretability. To address these challenges, this thesis presents two datasets -- MBTI9k and PANDORA -- collected from Reddit, a platform known for user anonymity and diverse discussions. The PANDORA dataset contains 17 million comments from over 10,000 users and integrates the MBTI and Big Five personality models with demographic information, overcoming limitations in data size, quality, and label coverage. Experiments on these datasets show that demographic variables influence model validity. In response, the SIMPA (Statement-to-Item Matching Personality Assessment) framework was developed - a computational framework for interpretable personality assessment that matches user-generated statements with validated questionnaire items. By using machine learning and semantic similarity, SIMPA delivers personality assessments comparable to human evaluations while maintaining high interpretability and efficiency. Although focused on personality assessment, SIMPA's versatility extends beyond this domain. Its model-agnostic design, layered cue detection, and scalability make it suitable for various research and practical applications involving complex label taxonomies and variable cue associations with target concepts.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delay-Tolerant Augmented-Consensus-based Distributed Directed Optimization</title>
<link>https://arxiv.org/abs/2510.02889</link>
<guid>https://arxiv.org/abs/2510.02889</guid>
<content:encoded><![CDATA[
<div> Keywords: Distributed optimization, Time-delays, Multi-agent networks, Convergence, Simulation.

Summary:
This paper explores distributed optimization in the presence of communication time-delays in multi-agent networks. The algorithm proposed in the paper addresses heterogeneous and bounded delays over directed networks. By applying matrix theory, algebraic graph theory, and augmented consensus formulation, the algorithm ensures convergence to the optimal value. Simulations validate the effectiveness of the proposed algorithm and compare its performance to existing delay-free algorithms. The research fills a gap in the literature by considering time-delays in information exchange among computing nodes, a scenario commonly encountered in real-world applications. This work contributes to the advancement of distributed optimization algorithms and provides a comprehensive framework for handling communication delays in large-scale machine learning and data processing tasks over multi-agent networks.<br /><br />Summary: <div>
arXiv:2510.02889v1 Announce Type: cross 
Abstract: Distributed optimization finds applications in large-scale machine learning, data processing and classification over multi-agent networks. In real-world scenarios, the communication network of agents may encounter latency that may affect the convergence of the optimization protocol. This paper addresses the case where the information exchange among the agents (computing nodes) over data-transmission channels (links) might be subject to communication time-delays, which is not well addressed in the existing literature. Our proposed algorithm improves the state-of-the-art by handling heterogeneous and arbitrary but bounded and fixed (time-invariant) delays over general strongly-connected directed networks. Arguments from matrix theory, algebraic graph theory, and augmented consensus formulation are applied to prove the convergence to the optimal value. Simulations are provided to verify the results and compare the performance with some existing delay-free algorithms.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories</title>
<link>https://arxiv.org/abs/2510.03152</link>
<guid>https://arxiv.org/abs/2510.03152</guid>
<content:encoded><![CDATA[
<div> framework, Markovian Reeb Graphs, spatiotemporal trajectories, Patterns of Life, urban mobility 

Summary:
Markovian Reeb Graphs is introduced as a framework for simulating realistic spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. This novel approach combines individual- and population-level mobility structures within a probabilistic topological model to generate future trajectories that reflect both consistency and variability in daily life. The method is evaluated on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics. Results indicate that the proposed framework achieves high fidelity while remaining data- and compute-efficient. This scalability makes Markovian Reeb Graphs a versatile tool for simulating trajectories in diverse urban environments. <div>
arXiv:2510.03152v1 Announce Type: cross 
Abstract: Accurately modeling human mobility is critical for urban planning, epidemiology, and traffic management. In this work, we introduce Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. By combining individual- and population-level mobility structures within a probabilistic topological model, our approach generates realistic future trajectories that capture both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics demonstrate that the proposed method achieves strong fidelity while remaining data- and compute-efficient. These results position Markovian Reeb Graphs as a scalable framework for trajectory simulation with broad applicability across diverse urban environments.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical partisan proximity outweighs online ties in predicting US voting outcomes</title>
<link>https://arxiv.org/abs/2407.12146</link>
<guid>https://arxiv.org/abs/2407.12146</guid>
<content:encoded><![CDATA[
<div> Keywords: affective polarization, social mixing, political outcomes, partisan exposure, partisan segregation

Summary: 
The study explores the impact of partisan exposure on vote choice in the US, analyzing offline and online dimensions. Physical space interactions, captured through co-location patterns, prove to be more predictive of electoral outcomes than online and residential exposures. Individual offline ties are found to be better predictors of vote choice compared to online connections. The research also highlights the prevalence of partisan segregation in metropolitan areas, with offline isolation being higher than online segregation and mainly linked to educational attainment. These findings underscore the significance of physical interactions in understanding the relationship between social networks and political behavior, underscoring the crucial role of offline social networks in influencing elections. 

<br /><br />Summary: <div>
arXiv:2407.12146v2 Announce Type: replace 
Abstract: Affective polarization and increasing social divisions affect social mixing and the spread of information across online and physical spaces, reinforcing social and electoral cleavages and influencing political outcomes. Here, using individual survey data and aggregated and de-identified co-location and online network data, we investigate the relationship between partisan exposure and vote choice in the US by comparing offline and online dimensions of partisan exposure. By leveraging various statistical modeling approaches, we consistently find that partisan exposure in the physical space, as captured by co-location patterns, more accurately predicts electoral outcomes in US counties, outperforming online and residential exposures. Similarly, offline ties at the individual level better predict vote choice compared to online connections. We also estimate county-level experienced partisan segregation and examine its relationship with individuals' demographic and socioeconomic characteristics. Focusing on metropolitan areas, our results confirm the presence of extensive partisan segregation in the US and show that offline partisan isolation, both considering physical encounters or residential sorting, is higher than online segregation and is primarily associated with educational attainment. Our findings emphasize the importance of physical space in understanding the relationship between social networks and political behavior, in contrast to the intense scrutiny focused on online social networks and elections.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Social Influence: Modeling Persuasion in Contested Social Networks</title>
<link>https://arxiv.org/abs/2510.01481</link>
<guid>https://arxiv.org/abs/2510.01481</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Influence Game, adversarial persuasion, DeGroot dynamics, difference-of-convex program, Iterated Linear solver

Summary:
The Social Influence Game (SIG) framework introduces a model for adversarial persuasion in social networks with multiple competing players. Players allocate influence from a fixed budget to shape opinions using DeGroot dynamics. The optimization problem in SIG is proven to be a difference-of-convex program, allowing for tractable analysis. To address scalability, an Iterated Linear (IL) solver is developed, approximating player objectives with linear programs. Experimental results show that IL achieves solutions within 7% of nonlinear solvers while being significantly faster, making it suitable for large social networks. This research paves the way for the study of contested influence in complex networks through asymptotic analysis.  
<br /><br />Summary: <div>
arXiv:2510.01481v1 Announce Type: new 
Abstract: We present the Social Influence Game (SIG), a framework for modeling adversarial persuasion in social networks with an arbitrary number of competing players. Our goal is to provide a tractable and interpretable model of contested influence that scales to large systems while capturing the structural leverage points of networks. Each player allocates influence from a fixed budget to steer opinions that evolve under DeGroot dynamics, and we prove that the resulting optimization problem is a difference-of-convex program. To enable scalability, we develop an Iterated Linear (IL) solver that approximates player objectives with linear programs. In experiments on random and archetypical networks, IL achieves solutions within 7% of nonlinear solvers while being over 10x faster, scaling to large social networks. This paper lays a foundation for asymptotic analysis of contested influence in complex networks.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware Sequential Learning</title>
<link>https://arxiv.org/abs/2502.19525</link>
<guid>https://arxiv.org/abs/2502.19525</guid>
<content:encoded><![CDATA[
<div> privacy-preserving learning, sequential learning, social learning, information flow, heterogeneous populations
Summary:
In a study on privacy-preserving sequential learning in settings like vaccination registries, where individuals add noise to conceal private signals, it was found that with continuous signals and a fixed privacy budget, optimal randomization balances privacy and accuracy, leading to accelerated learning. Privacy helps in faster learning rates than nonprivate scenarios by amplifying log-likelihood ratios. In heterogeneous populations, an order-optimal rate is achieved when some agents have low privacy budgets. However, with binary signals, privacy reduces informativeness and hinders learning compared to nonprivate scenarios. The study demonstrates how privacy reshapes information dynamics, providing insights for platform and policy design. <div>
arXiv:2502.19525v5 Announce Type: cross 
Abstract: In settings like vaccination registries, individuals act after observing others, and the resulting public records can expose private information. We study privacy-preserving sequential learning, where agents add endogenous noise to their reported actions to conceal private signals. Efficient social learning relies on information flow, seemingly in conflict with privacy. Surprisingly, with continuous signals and a fixed privacy budget $(\epsilon)$, the optimal randomization strategy balances privacy and accuracy, accelerating learning to $\Theta_{\epsilon}(\log n)$, faster than the nonprivate $\Theta(\sqrt{\log n})$ rate. In the nonprivate baseline, the expected time to the first correct action and the number of incorrect actions diverge; under privacy with sufficiently small $\epsilon$, both are finite. Privacy helps because, under the false state, agents more often receive signals contradicting the majority; randomization then asymmetrically amplifies the log-likelihood ratio, enhancing aggregation. In heterogeneous populations, an order-optimal $\Theta(\sqrt{n})$ rate is achievable when a subset of agents have low privacy budgets. With binary signals, however, privacy reduces informativeness and impairs learning relative to the nonprivate baseline, though the dependence on $\epsilon$ is nonmonotone. Our results show how privacy reshapes information dynamics and inform the design of platforms and policies.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framing Unionization on Facebook: Communication around Representation Elections in the United States</title>
<link>https://arxiv.org/abs/2510.01757</link>
<guid>https://arxiv.org/abs/2510.01757</guid>
<content:encoded><![CDATA[
<div> Keywords: digital media, labor unions, communication, discourse frames, representation elections

Summary: 
Labor unions are increasingly using digital media to communicate and organize collective action. This study analyzed 158k Facebook posts from U.S. labor unions from 2015 to 2024 in conjunction with National Labor Relations Board election data to explore the relationship between online discourse and representation elections. Five discourse frames were identified and examined: diagnostic, prognostic, motivational, community, and engagement. The study found that diagnostic and community frames were most commonly used, with significant variation in frame usage among organizations. Communication leading up to won elections saw an increase in diagnostic, prognostic, and community frames, followed by a decrease in prognostic and motivational framing post-event, indicating strategic preparation. In contrast, unions that lost elections showed little adjustment in their communication strategies. This research provides insights into how unions adapt their communication strategies in different organizational contexts. 

<br /><br />Summary: <div>
arXiv:2510.01757v1 Announce Type: cross 
Abstract: Digital media have become central to how labor unions communicate, organize, and sustain collective action. Yet little is known about how unions' online discourse relates to concrete outcomes such as representation elections. This study addresses the gap by combining National Labor Relations Board (NLRB) election data with 158k Facebook posts published by U.S. labor unions between 2015 and 2024. We focused on five discourse frames widely recognized in labor and social movement communication research: diagnostic (identifying problems), prognostic (proposing solutions), motivational (mobilizing action), community (emphasizing solidarity), and engagement (promoting interaction). Using a fine-tuned RoBERTa classifier, we systematically annotated unions' posts and analyzed patterns of frame usage around election events. Our findings showed that diagnostic and community frames dominated union communication overall, but that frame usage varied substantially across organizations. In election cases that unions won, communication leading up to the vote showed an increased use of diagnostic, prognostic, and community frames, followed by a reduction in prognostic and motivational framing after the event--patterns consistent with strategic preparation. By contrast, in lost election cases unions showed little adjustment in their communication, suggesting an absence of tailored communication strategies. By examining variation in message-level framing, the study highlights how communication strategies adapt to organizational contexts, contributing open tools and data and complementing prior research in understanding digital communication of unions and social movements.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review</title>
<link>https://arxiv.org/abs/2501.13014</link>
<guid>https://arxiv.org/abs/2501.13014</guid>
<content:encoded><![CDATA[
<div> Keywords: peer review, open review process, reviewer quality, empirical Bayesian methods, incentive structures

Summary:
Traditional closed peer review systems have various limitations such as being slow, costly, non-transparent, and subject to biases. This study proposes an alternative open, bottom-up peer review process to address these issues. The research analyzed data from scientific conferences to show the challenges of reviewer variability and low correlation among reviewers. Surprisingly, reviewer quality scores were not correlated with authorship quality, revealing an inverted U-shaped relationship. The study assessed empirical Bayesian methods to estimate paper quality and found that it significantly improved assessments compared to simple averaging. Additionally, a model incorporating reviewer ratings for both papers and reviewers showed that user-generated scores could produce reliable paper scoring even with unreliable reviewers. The study also proposed incentive structures to recognize high-quality reviewers and promote broader reviewing coverage. Overall, the findings suggest that a self-selecting open peer review process has the potential to enhance the speed, fairness, and transparency of scientific peer review.<br /><br />Summary: <div>
arXiv:2501.13014v2 Announce Type: replace 
Abstract: Traditional closed peer review systems, which have played a central role in scientific publishing, are often slow, costly, non-transparent, stochastic, and possibly subject to biases - factors that can impede scientific progress and undermine public trust. Here, we propose and examine the efficacy and accuracy of an alternative form of scientific peer review: through an open, bottom-up process. First, using data from two major scientific conferences (CCN2023 and ICLR2023), we highlight how high variability of review scores and low correlation across reviewers presents a challenge for collective review. We quantify reviewer agreement with community consensus scores and use this as a reviewer quality estimator, showing that surprisingly, reviewer quality scores are not correlated with authorship quality. Instead, we reveal an inverted U-shape relationship, where authors with intermediate paper scores are the best reviewers. We assess empirical Bayesian methods to estimate paper quality based on different assessments of individual reviewer reliability. We show how under a one-shot review-then-score scenario, both in our models and on real peer review data, a Bayesian measure significantly improves paper quality assessments relative to simple averaging. We then consider an ongoing model of publishing, reviewing, and scoring, with reviewers scoring not only papers but also other reviewers. We show that user-generated reviewer ratings can yield robust and high-quality paper scoring even when unreliable (but unbiased) reviewers dominate. Finally, we outline incentive structures to recognize high-quality reviewers and encourage broader reviewing coverage of submitted papers. These findings suggest that a self-selecting open peer review process is potentially scalable, reliable, and equitable with the possibility of enhancing the speed, fairness, and transparency of the peer review process.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Heterophily in Recommender Systems with Wavelet Hypergraph Diffusion</title>
<link>https://arxiv.org/abs/2501.14399</link>
<guid>https://arxiv.org/abs/2501.14399</guid>
<content:encoded><![CDATA[
<div> Framework, FWHDNN, representation learning, hypergraph, recommendation<br />
<br />
Summary:<br />
FWHDNN (Fusion-based Wavelet Hypergraph Diffusion Neural Networks) is introduced as a novel framework for enhancing representation learning in hypergraph-based recommendation systems. It combines a cross-difference relation encoder for heterophily-aware hypergraph diffusion, a cluster-wise encoder utilizing wavelet transform-based hypergraph neural network layers, and a multi-modal fusion mechanism. Through extensive experiments on real-world datasets, FWHDNN outperforms existing methods in accuracy, robustness, and scalability by effectively capturing high-order user-item interconnections. <div>
arXiv:2501.14399v2 Announce Type: replace-cross 
Abstract: Recommender systems are pivotal in delivering personalised user experiences across various domains. However, capturing the heterophily patterns and the multi-dimensional nature of user-item interactions poses significant challenges. To address this, we introduce FWHDNN (Fusion-based Wavelet Hypergraph Diffusion Neural Networks), an innovative framework aimed at advancing representation learning in hypergraph-based recommendation tasks. The model incorporates three key components: (1) a cross-difference relation encoder leveraging heterophily-aware hypergraph diffusion to adapt message-passing for diverse class labels, (2) a multi-level cluster-wise encoder employing wavelet transform-based hypergraph neural network layers to capture multi-scale topological relationships, and (3) an integrated multi-modal fusion mechanism that combines structural and textual information through intermediate and late-fusion strategies. Extensive experiments on real-world datasets demonstrate that FWHDNN surpasses state-of-the-art methods in accuracy, robustness, and scalability in capturing high-order interconnections between users and items.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FTSCommDetector: Discovering Behavioral Communities through Temporal Synchronization</title>
<link>https://arxiv.org/abs/2510.00014</link>
<guid>https://arxiv.org/abs/2510.00014</guid>
<content:encoded><![CDATA[
<div> community detection, synchronization, multivariate time series, financial markets, risk management

Summary:
FTSCommDetector introduces a new method, Temporal Coherence Architecture (TCA), for community detection in multivariate time series data. Traditional methods fail to capture synchronization-desynchronization patterns seen in entities like AAPL and MSFT during market disruptions. FTSCommDetector uses dual-scale encoding and static topology with dynamic attention to maintain coherence and discover similar and dissimilar communities. Information-theoretic foundations and Normalized Temporal Profiles (NTP) are introduced for evaluation. The method achieves consistent improvements across various financial markets without the need for dataset-specific tuning. It demonstrates robustness with minimal performance variation across different window sizes. This approach provides practical insights for portfolio construction and risk management in financial markets. 

<br /><br />Summary: <div>
arXiv:2510.00014v1 Announce Type: new 
Abstract: Why do trillion-dollar tech giants AAPL and MSFT diverge into different response patterns during market disruptions despite identical sector classifications? This paradox reveals a fundamental limitation: traditional community detection methods fail to capture synchronization-desynchronization patterns where entities move independently yet align during critical moments. To this end, we introduce FTSCommDetector, implementing our Temporal Coherence Architecture (TCA) to discover similar and dissimilar communities in continuous multivariate time series. Unlike existing methods that process each timestamp independently, causing unstable community assignments and missing evolving relationships, our approach maintains coherence through dual-scale encoding and static topology with dynamic attention. Furthermore, we establish information-theoretic foundations demonstrating how scale separation maximizes complementary information and introduce Normalized Temporal Profiles (NTP) for scale-invariant evaluation. As a result, FTSCommDetector achieves consistent improvements across four diverse financial markets (SP100, SP500, SP1000, Nikkei 225), with gains ranging from 3.5% to 11.1% over the strongest baselines. The method demonstrates remarkable robustness with only 2% performance variation across window sizes from 60 to 120 days, making dataset-specific tuning unnecessary, providing practical insights for portfolio construction and risk management.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Life Paths Cross: Extracting Human Interactions in Time and Space from Wikipedia</title>
<link>https://arxiv.org/abs/2510.00019</link>
<guid>https://arxiv.org/abs/2510.00019</guid>
<content:encoded><![CDATA[
<div> Keywords: notable individuals, interaction records, attention mechanisms, political polarization, WikiInteraction dataset

Summary:
Interactions among notable individuals convey significant messages across various perspectives, but these studies are often limited by data scarcity. To overcome this challenge, this study mines interaction data from millions of biography pages on Wikipedia, extracting 685,966 interaction records in the form of quadruplets. By integrating attention mechanisms, multi-task learning, and feature transfer methods, a model is designed to achieve an F1 score of 86.51%, outperforming baseline models. The extracted data allows for an analysis of intra- and inter-party interactions among political figures in the US, illustrating political polarization. The code, extracted interaction data, and the WikiInteraction dataset of 4,507 labeled interaction quadruplets are made publicly available. This research showcases the potential of utilizing data from Wikipedia to gain insights into dynamic interactions among notable individuals. 

<br /><br />Summary: <div>
arXiv:2510.00019v1 Announce Type: new 
Abstract: Interactions among notable individuals -- whether examined individually, in groups, or as networks -- often convey significant messages across cultural, economic, political, scientific, and historical perspectives. By analyzing the times and locations of these interactions, we can observe how dynamics unfold across regions over time. However, relevant studies are often constrained by data scarcity, particularly concerning the availability of specific location and time information. To address this issue, we mine millions of biography pages from Wikipedia, extracting 685,966 interaction records in the form of (Person1, Person2, Time, Location) interaction quadruplets. The key elements of these interactions are often scattered throughout the heterogeneous crowd-sourced text and may be loosely or indirectly associated. We overcome this challenge by designing a model that integrates attention mechanisms, multi-task learning, and feature transfer methods, achieving an F1 score of 86.51%, which outperforms baseline models. We further conduct an empirical analysis of intra- and inter-party interactions among political figures to examine political polarization in the US, showcasing the potential of the extracted data from a perspective that may not be possible without this data. We make our code, the extracted interaction data, and the WikiInteraction dataset of 4,507 labeled interaction quadruplets publicly available.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IA aplicada al an\'alisis del conflicto Ir\'an-Israel: Mapeo de discursos en YouTube</title>
<link>https://arxiv.org/abs/2510.00021</link>
<guid>https://arxiv.org/abs/2510.00021</guid>
<content:encoded><![CDATA[
<div> Keywords: Iran-Israel conflict, YouTube comments, digital representation, media biases, algorithmic biases<br />
Summary: The study examines the digital representation of the Iran-Israel conflict in June 2025 based on YouTube comments. Using natural language processing and machine learning, comments were classified into categories, revealing an overrepresentation of pro-Palestinian and anti-U.S./Israel discourses. Iran emerged as a significant actor in the online conversation, indicating a narrative shift. Algorithmic biases were found to amplify certain discourses while marginalizing others. The research combines computational analysis and critical critique to study geopolitical controversies, offering a replicable methodological framework. It is one of the first Spanish-language studies to analyze international conflict discourses on YouTube through AI and critical analysis, highlighting overlooked asymmetries and narrative disputes.<br /><br />Summary: <div>
arXiv:2510.00021v1 Announce Type: new 
Abstract: Purpose. This study analyzes the digital representation of the Iran-Israel conflict that occurred in June 2025, based on 120,000 comments posted on YouTube. It sought to identify discursive positions regarding the actors involved and to examine how media and algorithmic biases shape digital conversations. Methodology. A mixed-methods design with triangulation was adopted. In the quantitative phase, natural language processing techniques and machine learning models (BERT and XLM-RoBERTa) were used to classify comments into ten categories. In the qualitative phase, a critical analysis of media context and ideological narratives was conducted, complemented by manual annotation and supervised training. This strategy enabled the integration of statistical robustness with contextual understanding. Results and conclusions. The findings reveal a clear overrepresentation of pro-Palestinian and anti-United States/Israel discourses, while pro-United States and anti-Palestinian positions were marginal. Iran, usually rendered invisible in global media, emerged as a central actor in the digital conversation during the conflict, suggesting a narrative shift away from previous hegemonic frameworks. Likewise, the results confirm the influence of algorithmic biases in amplifying certain discourses while limiting others. Original contributions. This work combines computational analysis and philosophical critique for the study of digital controversies, providing a methodological framework replicable in geopolitical contexts. It is one of the first Spanish-language studies to map, through artificial intelligence and critical analysis, discourses on an international conflict on YouTube, highlighting asymmetries and narrative disputes that are often overlooked.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis</title>
<link>https://arxiv.org/abs/2510.00024</link>
<guid>https://arxiv.org/abs/2510.00024</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Epidemic modeling, Multi-agent framework, Automation, Scientific research

Summary: 
EpidemIQs introduces a novel multi-agent framework using Large Language Models (LLMs) to automate complex interdisciplinary research in epidemic modeling. The framework integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, stochastic simulations, data visualization, and documentation of findings in a structured manuscript. It consists of scientist agents for planning and coordination, and task-expert agents focused on specific duties. Using GPT 4.1 and GPT 4.1 mini LLMs, the framework achieved a 100% completion success rate with an average total token usage of 870K at a cost of about $1.57 per study. Evaluation across different epidemic scenarios showed higher performance compared to single-agent LLMs. EpidemIQs represents a significant advancement in accelerating scientific research by reducing costs, turnaround time, and improving accessibility to advanced modeling tools.

<br /><br />Summary: <div>
arXiv:2510.00024v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation. We introduce \textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript. We introduced two types of agents: a scientist agent for planning, coordination, reflection, and generation of final results, and a task-expert agent to focus exclusively on one specific duty serving as a tool to the scientist agent. The framework consistently generated complete reports in scientific article format. Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \$1.57 per study, achieving a 100\% completion success rate through our experiments. We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. We compare EpidemIQs to the single-agent LLM, which has the same system prompts and tools, iteratively planning, invoking tools, and revising outputs until task completion. The comparison shows consistently higher performance of the proposed framework across five different scenarios. EpidemIQs represents a step forward in accelerating scientific research by significantly reducing costs and turnaround time of discovery processes, and enhancing accessibility to advanced modeling tools.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Product Ecosystems</title>
<link>https://arxiv.org/abs/2510.00036</link>
<guid>https://arxiv.org/abs/2510.00036</guid>
<content:encoded><![CDATA[
<div> dynamical-systems, influence propagation, product adoption networks, positive linear system, Metzler interaction matrices<br />
<br />
Summary: <br />
This paper presents a dynamical-systems framework for modeling influence propagation in product adoption networks, focusing on positive linear systems with Metzler interaction matrices and utility-based decay. The study derives exact solutions for various interaction structures, showing that positive interactions lead to nonnegative amplification and utility saturation after approximately three complementary additions. It also highlights the dominance of frequency over quality improvements in influence dynamics and the varying effects of reinforcing interactions and decay control. Additionally, the research establishes that long-run retention in SIS-type dynamics is bounded by the inverse spectral radius of the adoption graph. These findings extend existing theories and provide explicit, calibratable expressions for understanding influence dynamics in networked product adoption scenarios. <br /> <div>
arXiv:2510.00036v1 Announce Type: new 
Abstract: This paper develops a dynamical-systems framework for modeling influence propagation in product adoption networks, formulated as a positive linear system with Metzler interaction matrices and utility-based decay. Exact solutions are derived for constant, piecewise-constant, and fully time-varying interaction structures using matrix exponentials and the Peano--Baker series. It establishes five results: (i) positive interactions guarantee nonnegative amplification, (ii) perceived utility saturates after $\approx\!3$ complementary additions (Weber--Fechner), (iii) frequency of comparable introductions dominates incremental quality improvements, (iv) reinforcing interactions yields monotone gains while decay control gives ambiguous effects, and (v) long-run retention under SIS-type dynamics is bounded by the inverse spectral radius of the adoption graph. These results extend epidemic-threshold theory and positive-systems analysis to networked adoption, yielding explicit, calibratable expressions for influence dynamics on networks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoREX: Towards Self-Explainable Social Recommendation with Relevant Ego-Path Extraction</title>
<link>https://arxiv.org/abs/2510.00080</link>
<guid>https://arxiv.org/abs/2510.00080</guid>
<content:encoded><![CDATA[
<div> explanatory, social recommendation, Graph Neural Networks, SoREX, ego-path extraction<br />
<br />
Summary: 
The study introduces SoREX, a self-explanatory social recommendation framework that combines Graph Neural Networks and friend recommendation to improve prediction accuracy. SoREX independently models social relations and user-item interactions while optimizing an auxiliary task to enhance social signals. The framework includes a novel ego-path extraction approach to provide meaningful explanations for predictions. By transforming the ego-net of a user into multi-hop ego-paths, SoREX extracts factor-specific and candidate-aware subsets for explanations. These explanations enable detailed comparative analysis among different candidate items. Additionally, explanation re-aggregation correlates explanations with predictions, making the framework self-explanatory. Experimental results on benchmark datasets validate SoREX's predictive accuracy, and both qualitative and quantitative analyses demonstrate the effectiveness of the extracted explanations. The code and data are available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2510.00080v1 Announce Type: new 
Abstract: Social recommendation has been proven effective in addressing data sparsity in user-item interaction modeling by leveraging social networks. The recent integration of Graph Neural Networks (GNNs) has further enhanced prediction accuracy in contemporary social recommendation algorithms. However, many GNN-based approaches in social recommendation lack the ability to furnish meaningful explanations for their predictions. In this study, we confront this challenge by introducing SoREX, a self-explanatory GNN-based social recommendation framework. SoREX adopts a two-tower framework enhanced by friend recommendation, independently modeling social relations and user-item interactions, while jointly optimizing an auxiliary task to reinforce social signals. To offer explanations, we propose a novel ego-path extraction approach. This method involves transforming the ego-net of a target user into a collection of multi-hop ego-paths, from which we extract factor-specific and candidate-aware ego-path subsets as explanations. This process facilitates the summarization of detailed comparative explanations among different candidate items through intricate substructure analysis. Furthermore, we conduct explanation re-aggregation to explicitly correlate explanations with downstream predictions, imbuing our framework with inherent self-explainability. Comprehensive experiments conducted on four widely adopted benchmark datasets validate the effectiveness of SoREX in predictive accuracy. Additionally, qualitative and quantitative analyses confirm the efficacy of the extracted explanations in SoREX. Our code and data are available at https://github.com/antman9914/SoREX.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobility Behavior Evolution During Extended Emergencies: Returners, Explorers, and the 15-Minute City</title>
<link>https://arxiv.org/abs/2510.00469</link>
<guid>https://arxiv.org/abs/2510.00469</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, emergencies, urban resilience, YJMob100K dataset, spatial disparities

Summary: 
This study analyzes human mobility patterns during emergencies in a densely populated metropolitan region using high-resolution spatial data from the YJMob100K dataset. The research focuses on the transition between returners and explorers, individuals who visit limited locations repeatedly and those who travel across broader destinations, respectively. The study finds that it takes at least two weeks of data to detect meaningful behavioral shifts during emergencies. Individuals tend to resume visits to non-essential locations more slowly during prolonged emergencies than under normal conditions. Explorers reduce long-distance travel significantly, particularly on weekends and holidays, exhibiting returner-like short distance patterns. Residents from low Points of Interest (POI) density neighborhoods often travel to POI rich areas, emphasizing spatial disparities. Strengthening local accessibility could enhance urban resilience during crises. The full reproducibility of the study is ensured through the project website. 

Summary: <div>
arXiv:2510.00469v1 Announce Type: new 
Abstract: Understanding human mobility during emergencies is critical for strengthening urban resilience and guiding emergency management. This study examines transitions between returners, who repeatedly visit a limited set of locations, and explorers, who travel across broader destinations, over a 15-day emergency period in a densely populated metropolitan region using the YJMob100K dataset. High-resolution spatial data reveal intra-urban behavioral dynamics often masked at coarser scales. Beyond static comparisons, we analyze how mobility evolves over time, with varying emergency durations, across weekdays and weekends, and relative to neighborhood boundaries, linking the analysis to the 15-minute city framework.
  Results show that at least two weeks of data are required to detect meaningful behavioral shifts. During prolonged emergencies, individuals resume visits to non-essential locations more slowly than under normal conditions. Explorers markedly reduce long distance travel, while weekends and holidays consistently exhibit returner-like, short distance patterns. Residents of low Points of Interest (POI) density neighborhoods often travel to POI rich areas, highlighting spatial disparities. Strengthening local accessibility may improve urban resilience during crises.
  Full reproducibility is supported through the project website: https://github.com/wissamkontar
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Threats to the sustainability of Community Notes on X</title>
<link>https://arxiv.org/abs/2510.00650</link>
<guid>https://arxiv.org/abs/2510.00650</guid>
<content:encoded><![CDATA[
<div> Community Notes, content moderation, X, user-generated context, bridging algorithm <br />
Summary:
Community Notes, a content moderation system pioneered by Twitter and now used by X, relies on a bridging algorithm to identify user-generated context with upvotes across political divides. This study examines the X Community Notes community to understand its stability and disruptions, as well as the motivations driving user contributions. Through a regression discontinuity design analysis, the study finds that having a note published has a positive impact on future note authoring. However, the proportion of notes considered "helpful" on X is low (10%) and decreasing, posing potential risks to the system. These findings have implications for the future of Community Notes on X and other platforms using similar approaches. <br /><br />Summary: <div>
arXiv:2510.00650v1 Announce Type: new 
Abstract: Community Notes are emerging as an important option for content moderation. The Community Notes system pioneered by Twitter, now known as X, uses a bridging algorithm to identify user-generated context with upvotes across political divides, supposedly spinning consensual gold from partisan straw. It is important to understand the nature of the community behind Community Notes, especially as the feature has now been imitated by several billion-user platforms. We look for signs of stability and disruption in the X Community Notes community and interrogate the motivations other than partisan animus (Allen, Martel, and Rand 2022) which may be driving users to contribute. We conduct a novel analysis of the impact of having a note published, which requires being considered "helpful" by the bridging algorithm, utilising a regression discontinuity design. This allows stronger causal inference than conventional methods used with observational data. Our analysis shows the positive effect on future note authoring of having a note published. This highlights the risk of the current system, where the proportion of notes considered "helpful" (and therefore shown to users on X) is low, 10%, and declining. This analysis has implications for the future of Community Notes on X and the extension of this approach to other platforms.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Communities in Continuous-Time Temporal Networks by Optimizing L-Modularity</title>
<link>https://arxiv.org/abs/2510.00741</link>
<guid>https://arxiv.org/abs/2510.00741</guid>
<content:encoded><![CDATA[
<div> Community detection, network analysis, temporal setting, dynamic communities, LAGO<br />
<br />
Summary: 
Community detection is a crucial problem in network analysis, with applications in various fields. Traditional methods struggle to handle dynamic data with exact temporal accuracy. LAGO, a new method introduced in this study, uses Longitudinal Modularity to optimize the detection of dynamic communities without relying on time discretization or assuming rigid community evolution. LAGO accurately captures the precise entry and exit points of nodes in communities, resulting in temporally and topologically coherent community detection. The method is evaluated on both synthetic benchmarks and real-world datasets, demonstrating its efficiency and effectiveness in uncovering dynamic communities in continuous-time networks. <div>
arXiv:2510.00741v1 Announce Type: new 
Abstract: Community detection is a fundamental problem in network analysis, with many applications in various fields. Extending community detection to the temporal setting with exact temporal accuracy, as required by real-world dynamic data, necessitates methods specifically adapted to the temporal nature of interactions. We introduce LAGO, a novel method for uncovering dynamic communities by greedy optimization of Longitudinal Modularity, a specific adaptation of Modularity for continuous-time networks. Unlike prior approaches that rely on time discretization or assume rigid community evolution, LAGO captures the precise moments when nodes enter and exit communities. We evaluate LAGO on synthetic benchmarks and real-world datasets, demonstrating its ability to efficiently uncover temporally and topologically coherent communities.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Minimization of Polarization and Disagreement via Low-Rank Matrix Bandits</title>
<link>https://arxiv.org/abs/2510.00803</link>
<guid>https://arxiv.org/abs/2510.00803</guid>
<content:encoded><![CDATA[
<div> polarization, disagreement, Friedkin-Johnsen opinion dynamics model, incomplete information, regret minimization<br />
<br />
Summary: 
This study focuses on minimizing polarization and disagreement in the Friedkin-Johnsen opinion dynamics model under incomplete information. It considers the online setting where innate opinions are unknown and must be learned through sequential observations, simulating periodic interventions on social media platforms. The problem is formulated as a regret minimization task, bridging algorithmic interventions on social media with multi-armed bandit theory. The proposed two-stage algorithm based on low-rank matrix bandits achieves an approximately $\sqrt{T}$ cumulative regret over any time horizon $T$. The algorithm first estimates a low-dimensional structure through subspace estimation and then utilizes a linear bandit algorithm within this compact representation. Empirical results demonstrate the algorithm's superior performance in terms of cumulative regret and running time compared to a linear bandit baseline. <div>
arXiv:2510.00803v1 Announce Type: cross 
Abstract: We study the problem of minimizing polarization and disagreement in the Friedkin-Johnsen opinion dynamics model under incomplete information. Unlike prior work that assumes a static setting with full knowledge of users' innate opinions, we address the more realistic online setting where innate opinions are unknown and must be learned through sequential observations. This novel setting, which naturally mirrors periodic interventions on social media platforms, is formulated as a regret minimization problem, establishing a key connection between algorithmic interventions on social media platforms and theory of multi-armed bandits. In our formulation, a learner observes only a scalar feedback of the overall polarization and disagreement after an intervention. For this novel bandit problem, we propose a two-stage algorithm based on low-rank matrix bandits. The algorithm first performs subspace estimation to identify an underlying low-dimensional structure, and then employs a linear bandit algorithm within the compact dimensional representation derived from the estimated subspace. We prove that our algorithm achieves an $ \widetilde{O}(\sqrt{T}) $ cumulative regret over any time horizon $T$. Empirical results validate that our algorithm significantly outperforms a linear bandit baseline in terms of both cumulative regret and running time.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Photo-Elicitation: The Use of Communal Production of Meaning to Hear a Vulnerable Population</title>
<link>https://arxiv.org/abs/2510.00964</link>
<guid>https://arxiv.org/abs/2510.00964</guid>
<content:encoded><![CDATA[
<div> Keywords: sex-trafficking survivors, Nepal, photo-elicitation, social capital, reintegration<br />
Summary: 
In this ethnographic study, the focus was on sex-trafficking survivors in Nepal, particularly those in a protected-living setup managed by an NGO. By employing a communal form of photo-elicitation, the researchers aimed to understand the values and experiences of the survivors without putting individual pressure on them. The study sheds light on the complex circumstances faced by survivors as they undergo rehabilitation and strive for a "new normal." Additionally, it contributes to the HCI and CSCW literature by highlighting the specific challenges faced by trafficking survivors and the importance of designing interventions to support their reintegration into society. The findings suggest that survivors possess limited yet valuable social capital in certain areas, which could be instrumental in aiding their reintegration process. <div>
arXiv:2510.00964v1 Announce Type: cross 
Abstract: We report on an initial ethnographic exploration of the situation of sex-trafficking survivors in Nepal. In the course of studying trafficking survivors in a protected-living situation created by a non-governmental organization in Nepal, we adapted photo-elicitation to hear the voices of the survivors by making the technique more communal. Bringing sociality to the forefront of the method reduced the pressure on survivors to assert voices as individuals, allowing them to speak. We make three contributions to research. First, we propose a communal form of photo-elicitation as a method to elicit values in sensitive settings. Second, we present the complex circumstances of the survivors as they undergo rehabilitation and move towards life with a ``new normal''. Third, our work adds to HCI and CSCW literature on understanding specific concerns of trafficking survivors and aims to inform designs that can support reintegration of survivors in society. The values that the survivors hold and their notion of future opportunities suggest possession of limited but important social capital in some domains that could be leveraged to aid reintegration.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouteKG: A knowledge graph-based framework for route prediction on road networks</title>
<link>https://arxiv.org/abs/2310.03617</link>
<guid>https://arxiv.org/abs/2310.03617</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, route prediction, spatial relations, n-ary tree-based algorithm, traffic flow estimation

Summary:
RouteKG is a novel framework for route prediction on road networks, utilizing a Knowledge Graph to encode spatial relations and moving directions crucial for human navigation. The model incorporates an efficient n-ary tree-based algorithm to generate top-K routes in batch mode, enhancing computational efficiency. A rank refinement module further optimizes prediction performance by fine-tuning route rankings. Evaluation on real-world vehicle trajectory datasets shows significant accuracy improvement over baseline methods. The pre-trained model can also be used for real-time traffic flow estimation at the link level. RouteKG has the potential to revolutionize vehicle navigation, traffic management, and other intelligent transportation tasks, contributing to the advancement of intelligent and connected urban systems. Source codes for RouteKG are available for access on GitHub. 

<br /><br />Summary: <div>
arXiv:2310.03617v4 Announce Type: replace 
Abstract: Short-term route prediction on road networks allows us to anticipate the future trajectories of road users, enabling various applications ranging from dynamic traffic control to personalized navigation. Despite recent advances in this area, existing methods focus primarily on learning sequential transition patterns, neglecting the inherent spatial relations in road networks that can affect human routing decisions. To fill this gap, this paper introduces RouteKG, a novel Knowledge Graph-based framework for route prediction. Specifically, we construct a Knowledge Graph on the road network to encode spatial relations, especially moving directions that are crucial for human navigation. Moreover, an n-ary tree-based algorithm is introduced to efficiently generate top-K routes in batch mode, enhancing computational efficiency. To further optimize prediction performance, a rank refinement module is incorporated to fine-tune candidate route rankings. The model performance is evaluated using two real-world vehicle trajectory datasets from two Chinese cities under various practical scenarios. The results demonstrate a significant improvement in accuracy over the baseline methods. We further validate the proposed method by utilizing the pre-trained model as a simulator for real-time traffic flow estimation at the link level. RouteKG has great potential to transform vehicle navigation, traffic management, and a variety of intelligent transportation tasks, playing a crucial role in advancing the core foundation of intelligent and connected urban systems. The source codes of RouteKG are available at https://github.com/YihongT/RouteKG.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arab Spring's Impact on Science through the Lens of Scholarly Attention, Funding, and Migration</title>
<link>https://arxiv.org/abs/2503.13238</link>
<guid>https://arxiv.org/abs/2503.13238</guid>
<content:encoded><![CDATA[
<div> Keywords: Arab Spring, Middle East, North Africa, research funding, knowledge flows

Summary: The study analyzes the impact of the Arab Spring on research in the Middle East and North Africa (MENA) region by examining 3.7 million articles published between 2002 and 2019. Following the 2010-2011 events, there was a noticeable increase in mentions of MENA countries in research articles, particularly focusing on Egypt. This surge can be attributed to both increased research funding for the region and the emigration of scholars maintaining research ties to their countries of origin. Saudi Arabia has emerged as a central hub for studying the affected countries, attracting scholars and funding, shaping the scientific narrative of the region. The study highlights how political events can influence global knowledge flows, shifting research focus, resources, and disciplinary perspectives. <br /><br />Summary: The Arab Spring prompted a surge in research attention towards MENA countries, with a notable increase in mentions of Egypt. This was driven by increased research funding and the involvement of scholars with ties to the region. Saudi Arabia has become a key player in shaping scientific discourse on the region, attracting scholars and resources. The study illustrates how political upheavals can reshape global knowledge flows, influencing who studies which regions, with what support, and in which academic disciplines. <div>
arXiv:2503.13238v4 Announce Type: replace-cross 
Abstract: The 2010-2011 Arab Spring reverberated far beyond politics, reshaping how the Middle East and North Africa region (MENA) is studied. Analyzing 3.7 million Scopus-indexed articles published between 2002 and 2019, we find that mentions of ten of these countries in titles or abstracts rose significantly after 2011 relative to the global baseline, with Egypt receiving the greatest attention in the region. We link this surge to two intertwined mechanisms: an increase in research funding directed at the MENA region and the emigration of researchers who continued publishing on their countries of origin. Our analysis reveals that Saudi Arabia has emerged as a regional hub for studying the affected countries, attracting funding and scholars, and thereby playing a significant role in shaping the scientific narrative on the region. These findings demonstrate how political upheaval can reshape global knowledge flows by altering who studies whom, with what resources, and in which disciplines.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHINDR - a DSM5 based mental health diagnosis and recommendation framework using LLM</title>
<link>https://arxiv.org/abs/2509.25992</link>
<guid>https://arxiv.org/abs/2509.25992</guid>
<content:encoded><![CDATA[
<div> Keywords: Mental health forums, large language model, DSM-5 criteria, personalized interventions, psychological features 

Summary: 
MHINDR is a framework that utilizes a large language model integrated with DSM-5 criteria to analyze user-generated text from mental health forums. It aims to diagnose mental health conditions, track symptom progression, and provide personalized interventions and insights for mental health practitioners. The framework focuses on extracting temporal information and psychological features to create comprehensive mental health summaries for users. It offers scalable and customizable therapeutic recommendations that can be adapted to various clinical contexts, patient needs, and workplace well-being programs. MHINDR aims to improve mental health diagnosis and treatment by leveraging data-driven insights and personalized interventions based on individual needs and conditions. <div>
arXiv:2509.25992v1 Announce Type: new 
Abstract: Mental health forums offer valuable insights into psychological issues, stressors, and potential solutions. We propose MHINDR, a large language model (LLM) based framework integrated with DSM-5 criteria to analyze user-generated text, dignose mental health conditions, and generate personalized interventions and insights for mental health practitioners. Our approach emphasizes on the extraction of temporal information for accurate diagnosis and symptom progression tracking, together with psychological features to create comprehensive mental health summaries of users. The framework delivers scalable, customizable, and data-driven therapeutic recommendations, adaptable to diverse clinical contexts, patient needs, and workplace well-being programs.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basic Cycle Ratio: Cost-Effective Ranking of Influential Spreaders from Local and Global Perspectives</title>
<link>https://arxiv.org/abs/2509.26220</link>
<guid>https://arxiv.org/abs/2509.26220</guid>
<content:encoded><![CDATA[
<div> influential spreaders, complex networks, node importance, Basic Cycle Ratio, information diffusion

Summary:
The study introduces a novel method called the Basic Cycle Ratio (BCR) for assessing the importance of nodes in spreading processes in complex networks. BCR combines basic cycles and the cycle ratio to capture a node's local significance within its neighborhood and its global role in network cohesion. The effectiveness of BCR was tested on six real-world social networks, outperforming traditional centrality measures and other cycle-based approaches. The results showed that BCR is more efficient in selecting powerful spreaders and enhancing information diffusion. Additionally, BCR offers a cost-effective and practical solution for social network applications. <div>
arXiv:2509.26220v1 Announce Type: new 
Abstract: Spreading processes are fundamental to complex networks. Identifying influential spreaders with dual local and global roles presents a crucial yet challenging task. To address this, our study proposes a novel method, the Basic Cycle Ratio (BCR), for assessing node importance. BCR leverages basic cycles and the cycle ratio to uniquely capture a node's local significance within its immediate neighborhood and its global role in maintaining network cohesion. We evaluated BCR on six diverse real-world social networks. Our method outperformed traditional centrality measures and other cycle-based approaches, proving more effective at selecting powerful spreaders and enhancing information diffusion. Besides, BCR offers a cost-effective and practical solution for social network applications.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cold-Start Active Correlation Clustering</title>
<link>https://arxiv.org/abs/2509.25376</link>
<guid>https://arxiv.org/abs/2509.25376</guid>
<content:encoded><![CDATA[
<div> Keywords: active correlation clustering, pairwise similarities, active learning, cold-start scenario, coverage-aware method

Summary: 
Active correlation clustering is studied in this research, focusing on the scenario where initial pairwise similarities are not available. The study introduces a coverage-aware method that prioritizes diversity early in the active learning process. This method efficiently queries pairwise similarities through active learning, enhancing the clustering process. The effectiveness of the proposed approach is substantiated through experiments conducted on both synthetic and real-world datasets. By addressing the challenge of cold-start scenarios, the research contributes to advancing active correlation clustering methods, offering a cost-efficient and effective solution for clustering tasks. <div>
arXiv:2509.25376v1 Announce Type: cross 
Abstract: We study active correlation clustering where pairwise similarities are not provided upfront and must be queried in a cost-efficient manner through active learning. Specifically, we focus on the cold-start scenario, where no true initial pairwise similarities are available for active learning. To address this challenge, we propose a coverage-aware method that encourages diversity early in the process. We demonstrate the effectiveness of our approach through several synthetic and real-world experiments.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toxicity in Online Platforms and AI Systems: A Survey of Needs, Challenges, Mitigations, and Future Directions</title>
<link>https://arxiv.org/abs/2509.25539</link>
<guid>https://arxiv.org/abs/2509.25539</guid>
<content:encoded><![CDATA[
<div> Keywords: toxic behavior, online content, Artificial Intelligence Systems, taxonomy, mitigation

Summary: 
Toxic behavior in digital communication systems and online platforms has led to the subconscious propagation of harmful content. This toxicity, whether expressed in language, image, or video, poses a significant challenge to individual and collective well-being globally. The article emphasizes the need for a comprehensive taxonomy to detect and proactively mitigate toxicity across various platforms and Artificial Intelligence systems. Existing literature on toxicity has primarily focused on reactive strategies, highlighting the necessity for a holistic understanding of toxicity in the modern AI era. The survey examines toxicity-related datasets and research efforts, particularly in English language-focused contexts, to address toxicity detection and mitigation. Lastly, the article identifies research gaps in toxicity mitigation, emphasizing the importance of datasets, mitigation strategies, Large Language Models, adaptability, explainability, and evaluation techniques. 

<br /><br />Summary: <div>
arXiv:2509.25539v1 Announce Type: cross 
Abstract: The evolution of digital communication systems and the designs of online platforms have inadvertently facilitated the subconscious propagation of toxic behavior. Giving rise to reactive responses to toxic behavior. Toxicity in online content and Artificial Intelligence Systems has become a serious challenge to individual and collective well-being around the world. It is more detrimental to society than we realize. Toxicity, expressed in language, image, and video, can be interpreted in various ways depending on the context of usage. Therefore, a comprehensive taxonomy is crucial to detect and mitigate toxicity in online content, Artificial Intelligence systems, and/or Large Language Models in a proactive manner. A comprehensive understanding of toxicity is likely to facilitate the design of practical solutions for toxicity detection and mitigation. The classification in published literature has focused on only a limited number of aspects of this very complex issue, with a pattern of reactive strategies in response to toxicity. This survey attempts to generate a comprehensive taxonomy of toxicity from various perspectives. It presents a holistic approach to explain the toxicity by understanding the context and environment that society is facing in the Artificial Intelligence era. This survey summarizes the toxicity-related datasets and research on toxicity detection and mitigation for Large Language Models, social media platforms, and other online platforms, detailing their attributes in textual mode, focused on the English language. Finally, we suggest the research gaps in toxicity mitigation based on datasets, mitigation strategies, Large Language Models, adaptability, explainability, and evaluation.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiFIRec: Towards High-Frequency yet Low-Intention Behaviors for Multi-Behavior Recommendation</title>
<link>https://arxiv.org/abs/2509.25755</link>
<guid>https://arxiv.org/abs/2509.25755</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-behavior recommendation, graph neural networks, user intention modeling, noise correction, adaptive feature fusion

Summary:
HiFIRec is a novel multi-behavior recommendation method designed to address data sparsity and cold-start issues by correcting the effects of high-frequency but low-intention behaviors. The proposed method utilizes a differential behavior modeling approach to extract neighborhood information and capture user intentions through adaptive cross-layer feature fusion. By hierarchically suppressing noisy signals and dynamically adjusting weights of negative samples using an intensity-aware non-sampling strategy, HiFIRec effectively corrects plausible but misleading frequent patterns. Experimental results on two benchmarks demonstrate that HiFIRec outperforms several state-of-the-art methods by improving HR@10 relative to existing approaches by 4.21%-6.81%. This innovative approach provides a promising solution for personalized recommendation systems in domains like healthcare and e-commerce.<br /><br />Summary: <div>
arXiv:2509.25755v1 Announce Type: cross 
Abstract: Multi-behavior recommendation leverages multiple types of user-item interactions to address data sparsity and cold-start issues, providing personalized services in domains such as healthcare and e-commerce. Most existing methods utilize graph neural networks to model user intention in a unified manner, which inadequately considers the heterogeneity across different behaviors. Especially, high-frequency yet low-intention behaviors may implicitly contain noisy signals, and frequent patterns that are plausible while misleading, thereby hindering the learning of user intentions. To this end, this paper proposes a novel multi-behavior recommendation method, HiFIRec, that corrects the effect of high-frequency yet low-intention behaviors by differential behavior modeling. To revise the noisy signals, we hierarchically suppress it across layers by extracting neighborhood information through layer-wise neighborhood aggregation and further capturing user intentions through adaptive cross-layer feature fusion. To correct plausible frequent patterns, we propose an intensity-aware non-sampling strategy that dynamically adjusts the weights of negative samples. Extensive experiments on two benchmarks show that HiFIRec relatively improves HR@10 by 4.21%-6.81% over several state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining higher-order triadic interactions</title>
<link>https://arxiv.org/abs/2404.14997</link>
<guid>https://arxiv.org/abs/2404.14997</guid>
<content:encoded><![CDATA[
<div> triadic interactions, Triadic Perceptron Model (TPM), Triadic Interaction Mining (TRIM), gene expression data, Acute Myeloid Leukemia <br />
Summary: <br />
The article proposes the Triadic Perceptron Model (TPM) to study triadic interactions in complex systems, highlighting their significance in various biological systems such as gene regulation. The Triadic Perceptron Model demonstrates that triadic interactions can modulate mutual information between interconnected nodes, leading to a more comprehensive understanding of system dynamics. The Triadic Interaction Mining (TRIM) algorithm is introduced to extract triadic interactions from node metadata, with applications in gene expression analysis. Utilizing this method on gene expression data, new potential triadic interactions relevant to Acute Myeloid Leukemia are identified, showcasing the practical implications of studying higher-order interactions. This study sheds light on previously overlooked aspects of triadic interactions, offering insights that can enhance our understanding of complex systems across different domains, from biology to climate science. <br /> <div>
arXiv:2404.14997v4 Announce Type: replace-cross 
Abstract: Complex systems often involve higher-order interactions which require us to go beyond their description in terms of pairwise networks. Triadic interactions are a fundamental type of higher-order interaction that occurs when one node regulates the interaction between two other nodes. Triadic interactions are found in a large variety of biological systems, from neuron-glia interactions to gene-regulation and ecosystems. However, triadic interactions have so far been mostly neglected. In this article, we propose {the Triadic Perceptron Model (TPM)} that demonstrates that triadic interactions can modulate the mutual information between the dynamical state of two linked nodes. Leveraging this result, we formulate the Triadic Interaction Mining (TRIM) algorithm to extract triadic interactions from node metadata, and we apply this framework to gene expression data, finding new candidates for triadic interactions relevant for Acute Myeloid Leukemia. Our work reveals important aspects of higher-order triadic interactions that are often ignored, yet can transform our understanding of complex systems and be applied to a large variety of systems ranging from biology to climate.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construct to Commitment: The Effect of Narratives on Economic Growth</title>
<link>https://arxiv.org/abs/2504.21060</link>
<guid>https://arxiv.org/abs/2504.21060</guid>
<content:encoded><![CDATA[
<div> Keywords: government-led narratives, mass media, Innovation-Driven Development Strategy, total factor productivity, New Quality Productive Forces initiative

Summary: 

The study focuses on government-led narratives in mass media and their evolution from framing expectations to becoming pillars for sustainable growth. The "Narratives-Construct-Commitment (NCC)" framework is proposed to outline the process and institutionalization of narratives. Through a dynamic Bayesian game model, the study analyzes the impact of the Innovation-Driven Development Strategy (2016) as a case study, identifying narrative shocks and their effects on investment incentives and total factor productivity. The findings highlight the role of credible narratives in shaping expectations, driving resources into R&amp;D, and fostering economic growth through the New Quality Productive Forces initiative. <div>
arXiv:2504.21060v2 Announce Type: replace-cross 
Abstract: We study how government-led narratives through mass media evolve from construct, a mechanism for framing expectations, into commitment, a sustainable pillar for growth. We propose the "Narratives-Construct-Commitment (NCC)" framework outlining the mechanism and institutionalization of narratives, and formalize it as a dynamic Bayesian game. Using the Innovation-Driven Development Strategy (2016) as a case study, we identify the narrative shock from high-frequency financial data and trace its impact using local projection method. By shaping expectations, credible narratives institutionalize investment incentives, channel resources into R\&amp;D, and facilitate sustained improvements in total factor productivity (TFP). Our findings strive to provide insights into the New Quality Productive Forces initiative, highlighting the role of narratives in transforming vision into tangible economic growth.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrediBench: Building Web-Scale Network Datasets for Information Integrity</title>
<link>https://arxiv.org/abs/2509.23340</link>
<guid>https://arxiv.org/abs/2509.23340</guid>
<content:encoded><![CDATA[
<div> Keywords: Online misinformation, LLMs, web graphs, misinformation detection, credibility scores

Summary:
CrediBench introduces a large-scale data processing pipeline for constructing temporal web graphs that combine textual content and hyperlink structure for detecting misinformation. Unlike previous methods, CrediBench captures the dynamic evolution of misinformation domains, including changes in content and inter-site references over time. The dataset extracted from the Common Crawl archive in December 2024 contains 45 million nodes and 1 billion edges, making it the largest web graph dataset publicly available for misinformation research. Experiments on this dataset show the effectiveness of both structural and webpage content signals in learning credibility scores to measure source reliability. The pipeline and experimentation code are openly accessible, providing a valuable resource for further research in the field of misinformation detection. 

<br /><br />Summary: <div>
arXiv:2509.23340v1 Announce Type: new 
Abstract: Online misinformation poses an escalating threat, amplified by the Internet's open nature and increasingly capable LLMs that generate persuasive yet deceptive content. Existing misinformation detection methods typically focus on either textual content or network structure in isolation, failing to leverage the rich, dynamic interplay between website content and hyperlink relationships that characterizes real-world misinformation ecosystems. We introduce CrediBench: a large-scale data processing pipeline for constructing temporal web graphs that jointly model textual content and hyperlink structure for misinformation detection. Unlike prior work, our approach captures the dynamic evolution of general misinformation domains, including changes in both content and inter-site references over time. Our processed one-month snapshot extracted from the Common Crawl archive in December 2024 contains 45 million nodes and 1 billion edges, representing the largest web graph dataset made publicly available for misinformation research to date. From our experiments on this graph snapshot, we demonstrate the strength of both structural and webpage content signals for learning credibility scores, which measure source reliability. The pipeline and experimentation code are all available here, and the dataset is in this folder.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Graph Embeddings and Louvain Algorithm for Unsupervised Community Detection</title>
<link>https://arxiv.org/abs/2509.23411</link>
<guid>https://arxiv.org/abs/2509.23411</guid>
<content:encoded><![CDATA[
<div> Keyword: community detection, Louvain algorithm, Graph Neural Networks (GNNs), node embeddings, merging algorithm<br />
Summary:<br />
This paper introduces a new method for community detection that combines the Louvain algorithm with Graph Neural Networks (GNNs), allowing for the identification of communities without prior knowledge. The method leverages GNN-generated node embeddings to capture more comprehensive structural and feature information, improving on the traditional Louvain algorithm. Additionally, a merging algorithm is introduced to refine the results and reduce the number of detected communities. This approach represents a novel use of GNNs to enhance community detection and has been validated on real-world datasets. The results show that the method can dynamically adjust the number of detected communities and improve detection accuracy compared to existing benchmarks.<br /><br />Summary: <div>
arXiv:2509.23411v1 Announce Type: new 
Abstract: This paper proposes a novel community detection method that integrates the Louvain algorithm with Graph Neural Networks (GNNs), enabling the discovery of communities without prior knowledge. Compared to most existing solutions, the proposed method does not require prior knowledge of the number of communities. It enhances the Louvain algorithm using node embeddings generated by a GNN to capture richer structural and feature information. Furthermore, it introduces a merging algorithm to refine the results of the enhanced Louvain algorithm, reducing the number of detected communities. To the best of our knowledge, this work is the first one that improves the Louvain algorithm using GNNs for community detection. The improvement of the proposed method was empirically confirmed through an evaluation on real-world datasets. The results demonstrate its ability to dynamically adjust the number of detected communities and increase the detection accuracy in comparison with the benchmark solutions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Node Classification via Simplicial Interaction with Augmented Maximal Clique Selection</title>
<link>https://arxiv.org/abs/2509.23568</link>
<guid>https://arxiv.org/abs/2509.23568</guid>
<content:encoded><![CDATA[
<div> Keywords: higher-order interactions, maximal cliques, network structures, computational efficiency, GNN-based learning

Summary:
The study introduces an augmented maximal clique approach for analyzing higher-order interactions in network structures. By selectively incorporating non-maximal cliques, the method aims to improve computational efficiency and reduce imbalanced training data. Comparative analyses on both synthetic and real-world datasets show that the proposed approach outperforms traditional methods based on pairwise interactions, all cliques, or only maximal cliques. Additionally, integrating the augmented maximal clique strategy into graph neural network (GNN)-based semi-supervised learning demonstrates enhanced predictive accuracy, indicating the potential benefits of incorporating higher-order structures in network learning. Overall, the augmented maximal clique strategy offers a valuable solution for effectively handling higher-order interactions in network analysis. 

<br /><br />Summary: <div>
arXiv:2509.23568v1 Announce Type: new 
Abstract: Considering higher-order interactions allows for a more comprehensive understanding of network structures beyond simple pairwise connections. While leveraging all cliques in a network to handle higher-order interactions is intuitive, it often leads to computational inefficiencies due to overlapping information between higher-order and lower-order cliques. To address this issue, we propose an augmented maximal clique strategy. Although using only maximal cliques can reduce unnecessary overlap and provide a concise representation of the network, certain nodes may still appear in multiple maximal cliques, resulting in imbalanced training data. Therefore, our augmented maximal clique approach selectively includes some non-maximal cliques to mitigate the overrepresentation of specific nodes and promote more balanced learning across the network. Comparative analyses on synthetic networks and real-world citation datasets demonstrate that our method outperforms approaches based on pairwise interactions, all cliques, or only maximal cliques. Finally, by integrating this strategy into GNN-based semi-supervised learning, we establish a link between maximal clique-based methods and GNNs, showing that incorporating higher-order structures improves predictive accuracy. As a result, the augmented maximal clique strategy offers a computationally efficient and effective solution for higher-order network learning.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASH: A Multiplatform and Multimodal Annotated Dataset for Societal Impact of Hurricane</title>
<link>https://arxiv.org/abs/2509.23627</link>
<guid>https://arxiv.org/abs/2509.23627</guid>
<content:encoded><![CDATA[
<div> Dataset, societal impact, hurricanes, social media, multimodal annotation  
Summary:  
- The article introduces a new dataset called MASH that focuses on studying the societal impacts of hurricanes through social media posts.  
- MASH includes 98,662 social media data posts from platforms like Reddit, TikTok, and YouTube, providing a diverse perspective on hurricane impacts.  
- The dataset is unique in its approach, annotating posts in a multimodal manner that considers both text and visuals across various dimensions like humanitarian classes, bias classes, and information integrity classes.  
- MASH aims to contribute to research areas such as disaster severity classification, public sentiment analysis, disaster policy making, and bias identification related to hurricanes.  
- This dataset is a valuable resource for understanding the broader societal impact of natural disasters like hurricanes in today's diverse social media landscape.  
<br /><br />Summary: <div>
arXiv:2509.23627v1 Announce Type: new 
Abstract: Natural disasters cause multidimensional threats to human societies, with hurricanes exemplifying one of the most disruptive events that not only caused severe physical damage but also sparked widespread discussion on social media platforms. Existing datasets for studying societal impacts of hurricanes often focus on outdated hurricanes and are limited to a single social media platform, failing to capture the broader societal impact in today's diverse social media environment. Moreover, existing datasets annotate visual and textual content of the post separately, failing to account for the multimodal nature of social media posts. To address these gaps, we present a multiplatform and Multimodal Annotated Dataset for Societal Impact of Hurricane (MASH) that includes 98,662 relevant social media data posts from Reddit, X, TikTok, and YouTube. In addition, all relevant social media data posts are annotated in a multimodal approach that considers both textual and visual content on three dimensions: humanitarian classes, bias classes, and information integrity classes. To our best knowledge, MASH is the first large-scale, multi-platform, multimodal, and multi-dimensionally annotated hurricane dataset. We envision that MASH can contribute to the study of hurricanes' impact on society, such as disaster severity classification, public sentiment analysis, disaster policy making, and bias identification.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness of One-to-Many Interdependent Higher-order Networks Against Cascading Failures</title>
<link>https://arxiv.org/abs/2509.23716</link>
<guid>https://arxiv.org/abs/2509.23716</guid>
<content:encoded><![CDATA[
<div> interdependent networks, higher-order structures, random attacks, percolation theory, network reliability
Summary:
This paper examines the robustness of one-to-many interdependent higher-order networks under random attacks. It introduces four inter-layer interdependency conditions based on the number of dependency edges needed for node survival and analyzes network resilience. By utilizing percolation theory, a unified theoretical framework is established to explore the impact of intra-layer higher-order structures and inter-layer coupling parameters on network reliability. The study extends to partially interdependent hypergraphs, providing insights for network design optimization. The analysis is validated on synthetic and real-data-based interdependent hypergraphs, shedding light on enhancing system resilience. <div>
arXiv:2509.23716v1 Announce Type: new 
Abstract: In the real world, the stable operation of a network is usually inseparable from the mutual support of other networks. In such an interdependent network, a node in one layer may depend on multiple nodes in another layer, forming a complex one-to-many dependency relationship. Meanwhile, there may also be higher-order interactions between multiple nodes within a layer, which increases the connectivity within the layer. However, existing research on one-to-many interdependence often neglects intra-layer higher-order structures and lacks a unified theoretical framework for inter-layer dependencies. Moreover, current research on interdependent higher-order networks typically assumes idealized one-to-one inter-layer dependencies, which does not reflect the complexity of real-world systems. These limitations hinder a comprehensive understanding of how such networks withstand failures. Therefore, this paper investigates the robustness of one-to-many interdependent higher-order networks under random attacks. Depending on whether node survival requires at least one dependency edge or multiple dependency edges, we propose four inter-layer interdependency conditions and analyze the network's robustness after cascading failures induced by random attacks. Using percolation theory, we establish a unified theoretical framework that reveals how higher-order interaction structures within intra-layers and inter-layer coupling parameters affect network reliability and system resilience. Additionally, we extend our study to partially interdependent hypergraphs. We validate our theoretical analysis on both synthetic and real-data-based interdependent hypergraphs, offering insights into the optimization of network design for enhanced reliability.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection robustness of graph neural networks</title>
<link>https://arxiv.org/abs/2509.24662</link>
<guid>https://arxiv.org/abs/2509.24662</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, community detection, robustness, perturbations, attacks
Summary:
- The study evaluates the robustness of six popular graph neural network architectures in community detection tasks.
- Supervised GNNs show higher baseline accuracy, while unsupervised methods like DMoN exhibit stronger resilience to perturbations and attacks.
- Community strength influences robustness, with well-defined communities leading to reduced performance loss.
- Node attribute perturbations, targeted edge deletions, and attribute distribution shifts have the most significant impact on community recovery.
- The findings emphasize the trade-offs between accuracy and robustness in GNN-based community detection and provide insights for selecting architectures resilient to noise and adversarial attacks. 

<br /><br />Summary: <div>
arXiv:2509.24662v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are increasingly widely used for community detection in attributed networks. They combine structural topology with node attributes through message passing and pooling. However, their robustness or lack of thereof with respect to different perturbations and targeted attacks in conjunction with community detection tasks is not well understood. To shed light into latent mechanisms behind GNN sensitivity on community detection tasks, we conduct a systematic computational evaluation of six widely adopted GNN architectures: GCN, GAT, Graph- SAGE, DiffPool, MinCUT, and DMoN. The analysis covers three perturbation categories: node attribute manipulations, edge topology distortions, and adversarial attacks. We use element-centric similarity as the evaluation metric on synthetic benchmarks and real-world citation networks. Our findings indicate that supervised GNNs tend to achieve higher baseline accuracy, while unsupervised methods, particularly DMoN, maintain stronger resilience under targeted and adversarial pertur- bations. Furthermore, robustness appears to be strongly influenced by community strength, with well-defined communities reducing performance loss. Across all models, node attribute perturba- tions associated with targeted edge deletions and shift in attribute distributions tend to cause the largest degradation in community recovery. These findings highlight important trade-offs between accuracy and robustness in GNN-based community detection and offer new insights into selecting architectures resilient to noise and adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discrete Geofence Design Using Binary Quadratic Programming</title>
<link>https://arxiv.org/abs/2509.24679</link>
<guid>https://arxiv.org/abs/2509.24679</guid>
<content:encoded><![CDATA[
<div> Keywords: geofences, human mobility data, arbitrary shapes, optimization problems, spatial information

Summary:
Geofences are spatial and virtual regions that monitor human activity and trigger events. Traditional circular geofences have limitations in flexibility and precision, especially in urban areas. This study introduces a method to extract arbitrary shapes as geofences from human mobility data. By formulating optimization problems as 0-1 integer programming and utilizing specialized solvers, such as quantum annealing, the approach enables the design of flexible and precise geofences. Different formulation methods were developed and compared to extract discrete geofences efficiently. The new modeling approach provides a solution to the geofence design problem, offering improved accuracy and alignment with political district boundaries and road structures. This advancement in geofence design has the potential to enhance the effectiveness of location-based services and spatially triggered events on mobile devices. 

<br /><br />Summary: <div>
arXiv:2509.24679v1 Announce Type: new 
Abstract: Geofences have attracted significant attention in the design of spatial and virtual regions for managing and engaging spatiotemporal events. By using geofences to monitor human activity across their boundaries, content providers can create spatially triggered events that include notifications about points of interest within a geofence by pushing spatial information to the devices of users. Traditionally, geofences were hand-crafted by providers. In addition to the hand-crafted approach, recent advances in collecting human mobility data through mobile devices can accelerate the automatic and data-driven design of geofences, also known as the geofence design problem. Previous approaches assume circular shapes; thus, their flexibility is insufficient, and they can only handle geofence-based applications for large areas with coarse resolutions. A challenge with using circular geofences in urban and high-resolution areas is that they often overlap and fail to align with political district boundaries and road segments, such as one-way streets and median barriers. In this study, we address the problem of extracting arbitrary shapes as geofences from human mobility data to mitigate this problem. In our formulation, we cast the existing optimization problems for circular geofences to 0-1 integer programming problems to represent arbitrary shapes. Although 0-1 integer programming problems are computationally hard, formulating them as quadratic (unconstrained) binary optimization problems enables efficient approximation of optimal solutions, because this allows the use of specialized quadratic solvers, such as the quantum annealing, and other state-of-the-art algorithms. We then develop and compare different formulation methods to extract discrete geofences. We confirmed that our new modeling approach enables flexible geofence design.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research in Medicine: Journal ranking vs. Interdisciplinarity</title>
<link>https://arxiv.org/abs/2509.24994</link>
<guid>https://arxiv.org/abs/2509.24994</guid>
<content:encoded><![CDATA[
<div> Keywords: interdisciplinary research, PubMed, medical concepts, impact factor, cancer.<br />
Summary: The study examines the interdisciplinary knowledge structure of PubMed research articles in medicine, comparing articles from high-ranking and less high-ranking medical journals. It finds that impactful journals tend to publish less interdisciplinary research. Cancer-related research plays a significant role in driving interdisciplinarity in medical science. The study also explores deviations in correlation networks between high and low impact journals, noting a mild tendency for strong link differences to be adjacent. Topic clusters of deviations shift over time, unlike the static topic clusters in the original networks. The findings suggest the importance of accommodating interdisciplinarity within existing infrastructures to maximize the benefits for patients.<br /><br />Summary: <div>
arXiv:2509.24994v1 Announce Type: new 
Abstract: Interdisciplinary research is critical for innovation and addressing complex societal issues. We characterise the interdisciplinary knowledge structure of PubMed research articles in medicine as correlation networks of medical concepts and compare the interdisciplinarity of articles between high-ranking (impactful) and less high-ranking (less impactful) medical journals. We found that impactful medical journals tend to publish research that are less interdisciplinary than less impactful journals. Observing that they bridge distant knowledge clusters in the networks, we find that cancer-related research can be seen as one of the main drivers of interdisciplinarity in medical science. Using signed difference networks, we also investigate the clustering of deviations between high and low impact journal correlation networks. We generally find a mild tendency for strong link differences to be adjacent. Furthermore, we find topic clusters of deviations that shift over time. In contrast, topic clusters in the original networks are static over time and can be seen as the core knowledge structure in medicine. Overall, journals and policymakers should encourage initiatives to accommodate interdisciplinarity within the existing infrastructures to maximise the potential patient benefits from IDR.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Inequality through Preferential Attachment, Triadic Closure, and Homophily</title>
<link>https://arxiv.org/abs/2509.23205</link>
<guid>https://arxiv.org/abs/2509.23205</guid>
<content:encoded><![CDATA[
<div> mechanisms, social networks, disparities, inequality, network growth <br />
Summary: <br />
The study explores how inequalities in social networks are shaped by linking mechanisms such as preferential attachment, homophily, and triadic closure. By developing the PATCH model, the researchers investigate the interplay of these mechanisms and their effects on degree inequalities and segregation within networks. The model's simulations reveal that homophily and preferential attachment contribute to increased segregation and degree disparities, while triadic closure moderates these effects by reducing segregation and between-group disparities. Applying the PATCH model to real-world collaboration and citation networks in Physics and Computer Science, the researchers demonstrate its ability to explain persistent gender disparities. The findings suggest that a combination of preferential attachment, moderate homophily, and triadic closure can sustain group inequalities in social networks. The study provides insights into how these mechanisms interact to perpetuate disparities and offers a framework for developing interventions to promote more equitable network structures. <br /> <div>
arXiv:2509.23205v1 Announce Type: cross 
Abstract: Inequalities in social networks arise from linking mechanisms, such as preferential attachment (connecting to popular nodes), homophily (connecting to similar others), and triadic closure (connecting through mutual contacts). While preferential attachment mainly drives degree inequality and homophily drives segregation, their three-way interaction remains understudied. This gap limits our understanding of how network inequalities emerge. Here, we introduce PATCH, a network growth model combining the three mechanisms to understand how they create disparities among two groups in synthetic networks. Extensive simulations confirm that homophily and preferential attachment increase segregation and degree inequalities, while triadic closure has countervailing effects: conditional on the other mechanisms, it amplifies population-wide degree inequality while reducing segregation and between-group degree disparities. We demonstrate PATCH's explanatory potential on fifty years of Physics and Computer Science collaboration and citation networks exhibiting persistent gender disparities. PATCH accounts for these gender disparities with the joint presence of preferential attachment, moderate gender homophily, and varying levels of triadic closure. By connecting mechanisms to observed inequalities, PATCH shows how their interplay sustains group disparities and provides a framework for designing interventions that promote more equitable social networks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding How Network Geometry Influences Diffusion Processes in Complex Networks: A Focus on Cryptocurrency Blockchains and Critical Infrastructure Networks</title>
<link>https://arxiv.org/abs/2509.23450</link>
<guid>https://arxiv.org/abs/2509.23450</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion, complex networks, cryptocurrency blockchains, infrastructure networks, network motifs

Summary:
This study explores how diffusion processes occur in complex networks, focusing on cryptocurrency blockchains and infrastructure networks. The topology and structural properties of these networks significantly impact the speed and scope of diffusion. By utilizing epidemic diffusion models like the Kertesz threshold model and the Susceptible-Infected (SI) model, the study analyzes key factors influencing diffusion dynamics. Bootstrap confidence intervals and Bayesian credible intervals are used to assess uncertainties in infected nodes and model parameters, respectively. The research highlights the importance of network motifs in diffusion and demonstrates that hub-dominated networks, common in blockchain ecosystems, exhibit resilience against random failures but are susceptible to targeted attacks. Additionally, centrality measures such as degree, betweenness, and clustering coefficient play a significant role in the transmissibility of diffusion within both blockchain and crucial infrastructure networks. <div>
arXiv:2509.23450v1 Announce Type: cross 
Abstract: This study provides essential insights into how diffusion processes unfold in complex networks, with a focus on cryptocurrency blockchains and infrastructure networks. The structural properties of these networks, such as hub-dominated, heavy-tailed topology, network motifs, and node centrality, significantly influence diffusion speed and reach. Using epidemic diffusion models, specifically the Kertesz threshold model and the Susceptible-Infected (SI) model, we analyze key factors affecting diffusion dynamics. To assess the uncertainty in the fraction of infected nodes over time, we employ bootstrap confidence intervals, while Bayesian credible intervals are constructed to quantify parameter uncertainties in the SI models. Our findings reveal substantial variations across different network types, including Erd\H{o}s--R\'enyi networks, Geometric Random Graphs, and Delaunay Triangulation networks, emphasizing the role of network architecture in failure propagation. We identify that network motifs are crucial in diffusion. We highlight that hub-dominated networks, which dominate blockchain ecosystems, provide resilience against random failures but remain vulnerable to targeted attacks, posing significant risks to network stability. Furthermore, centrality measures such as degree, betweenness, and clustering coefficient strongly influence the transmissibility of diffusion in both blockchain and critical infrastructure networks.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Analysis of Social Virtual Reality Based on Large-Scale Log Data of a Commercial Metaverse Platform</title>
<link>https://arxiv.org/abs/2509.23654</link>
<guid>https://arxiv.org/abs/2509.23654</guid>
<content:encoded><![CDATA[
<div> community structures, Social Virtual Reality, user communities, community hoppers, interaction data

Summary:
The study examines user communities in Social Virtual Reality platforms using interaction data analysis. It reveals that these platforms host multiple small communities with strong internal connections but limited inter-community links, unlike conventional social networks. The presence of community hoppers, users facilitating interactions between communities despite few direct connections, is identified. This sheds light on the unique community structures within Social VR and the diverse roles users play. <br /><br />Summary: <div>
arXiv:2509.23654v1 Announce Type: cross 
Abstract: This study quantitatively analyzes the structural characteristics of user communities within Social Virtual Reality (Social VR) platforms supporting head-mounted displays (HMDs), based on large-scale log data. By detecting and evaluating community structures from data on substantial interactions (defined as prolonged co-presence in the same virtual space), we found that Social VR platforms tend to host numerous, relatively small communities characterized by strong internal cohesion and limited inter-community connections. This finding contrasts with the large-scale, broadly connected community structures typically observed in conventional Social Networking Services (SNS). Furthermore, we identified a user segment capable of mediating between communities, despite these users not necessarily having numerous direct connections. We term this user segment `community hoppers' and discuss their characteristics. These findings contribute to a deeper understanding of the community structures that emerge within the unique communication environment of Social VR and the roles users play within them.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Homophily in Large Language Models</title>
<link>https://arxiv.org/abs/2509.23773</link>
<guid>https://arxiv.org/abs/2509.23773</guid>
<content:encoded><![CDATA[
<div> Graph Representation Learning, Large Language Models, Knowledge Homophily, Entity-Level Knowledgeability, Graph Neural Networks

Summary:
Large Language Models (LLMs) are neural knowledge bases used for applications like question answering. This study explores the structural organization of knowledge in LLMs, finding a homophily pattern where entities with closer proximity in the knowledge graph have similar levels of knowledge. A Graph Neural Network (GNN) regression model is proposed to estimate entity-level knowledgeability, prioritizing less known triplets for labeling. This approach improves efficiency in injecting knowledge into LLMs and enhances multi-hop path retrieval in question answering. <div>
arXiv:2509.23773v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Landscape of problematic papers in the field of non-coding RNA</title>
<link>https://arxiv.org/abs/2509.24511</link>
<guid>https://arxiv.org/abs/2509.24511</guid>
<content:encoded><![CDATA[
<div> Keywords: retractions, non-coding RNA (ncRNA), fraudulent publications, image duplication, evidence-based medicine<br />
<br />
Summary: 
The study examines the rise in retractions and concerns about the reliability of scientific research, focusing on non-coding RNA (ncRNA) as a case study. Research on under-investigated ncRNAs tends to produce problematic papers, often sharing significant textual similarity and displaying suspicious instances of image duplication. Healthcare institutions with low publication volumes are more likely to publish problematic papers. A limited number of journals are responsible for the majority of problematic papers, with many journals failing to adequately address the issue. The findings suggest that there may be numerous undetected problematic papers in the field. Understanding the characteristics of problematic papers provides valuable insights for developing strategies to tackle large-scale fraudulent publications in order to safeguard the credibility of evidence-based medicine.<br /><br />Summary: <div>
arXiv:2509.24511v1 Announce Type: cross 
Abstract: In recent years, the surge in retractions has been accompanied by numerous papers receiving comments that raise concerns about their reliability. The prevalence of problematic papers undermines the reliability of scientific research and threatens the foundation of evidence-based medicine. In this study,we focus on the field of non-coding RNA(ncRNA) as a case study to explore the typical characteristics of problematic papers from various perspectives, aiming to provide insights for addressing large-scale fraudulent publications. Research on under-investigated ncRNAs is more likely to yield problematic papers. These problematic papers often exhibit significant textual similarity, and many others sharing this similarity also display suspicious instances of image duplication. Healthcare institutions are particularly prone to publishing problematic papers, especially those with a low publication volume. Most problematic papers are found in a limited number of journals, and many journals inadequately address the commented papers. Our findings suggest that numerous problematic papers may still remain unidentified. The revealed characteristics offer valuable insights for formulating strategies to address the issue of fraudulent papers at scale.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metric Representations of Network Data</title>
<link>https://arxiv.org/abs/1911.00164</link>
<guid>https://arxiv.org/abs/1911.00164</guid>
<content:encoded><![CDATA[
<div> Keywords: networks, q-metric spaces, projections, metric trees, hierarchical clustering

Summary:
In this paper, the authors explore the projection of networks into q-metric spaces, a generalization of metric spaces. They introduce axioms to guide the projection process, ensuring consistency and optimality. The projection method, which involves computing shortest paths between nodes, is shown to have applications in combinatorial optimization and hierarchical clustering. The authors demonstrate the efficiency of using metric projections to search networks, particularly with the assistance of metric trees. Overall, the study provides a structured approach to representing network relationships in q-metric spaces, offering a versatile tool for analyzing and visualizing network data. The method's optimality and nestedness properties enhance its practical utility, making it a valuable technique for various network analysis tasks. <div>
arXiv:1911.00164v2 Announce Type: replace 
Abstract: Networks are structures that encode relationships between pairs of elements or nodes. However, there is no imposed connection between these relationships, i.e., the relationship between two nodes can be independent of every other one in the network, and need not be defined for every possible pair of nodes. This is not true for metric spaces, where the triangle inequality imposes conditions that must be satisfied by triads of distances in the space, and these distances must be defined for every pair of nodes. In this paper, we study how to project networks into q-metric spaces, a generalization of metric spaces that encompasses a larger class of structured representations. In order to do this, we encode as axioms two intuitively desirable properties of the mentioned projections. We show that there is only one way of projecting networks onto q-metric spaces satisfying these axioms. Moreover, for the special case of (regular) metric spaces, this method boils down to computing the shortest path between every node and, for the case of ultrametric spaces, it coincides with single linkage hierarchical clustering. Furthermore, we show that the projection method satisfies two properties of practical relevance: optimality, which enables its utilization for the efficient estimation of combinatorial optimization problems, and nestedness, which entails consistency of the structure induced when projecting onto different q-metric spaces. Finally, we illustrate how metric projections can be used to efficiently search networks aided by metric trees.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Analysis of the Nostr Social Network: Decentralization, Availability, and Replication Overhead</title>
<link>https://arxiv.org/abs/2402.05709</link>
<guid>https://arxiv.org/abs/2402.05709</guid>
<content:encoded><![CDATA[
<div> decentralized social network, censorship resistance, relays, post replication, Nostr
Summary:
Nostr is a decentralized social network that prioritizes high availability and censorship resistance, boasting over 100 million posts since its 2022 launch. The unique infrastructure utilizes relays as open storage servers to store and distribute user posts, ensuring authenticity through digital signatures. While Nostr offers superior decentralization compared to traditional platforms, challenges like relay availability and post replication overhead persist. Financial sustainability for free relays is a key concern impacting relay availability. The replication of posts across relays enhances censorship resistance but introduces significant overhead. To address these challenges, two proposed improvements aim to control the number of post replications and reduce retrieval overhead. Data-driven evaluations show promise in reducing overhead without compromising post availability in simulated scenarios. <div>
arXiv:2402.05709v2 Announce Type: replace 
Abstract: Nostr is a decentralized social network launched in 2022, emphasizing high availability and censorship resistance. Since launching, it has gained substantial attention, boasting over 100 million posts. Nostr resembles a micro-blogging service like Twitter but with distinct underlying infrastructure. Nostr introduces the concept of relays, which act as open storage servers that receive, store, and distribute user posts. Each user is uniquely identified by a public key, ensuring authenticity of posts through digital signatures. Users are able to securely replicate and retrieve posts through multiple relays, which frees them from single-server reliance and enhances post availability, thereby attempting to make Nostr censorship resistant. However, this aggressive design also presents challenges, such as the overhead required for extensive post replication and the difficulty in obtaining a global view of post replication locations, which remain unexplored or unaddressed. This necessitates a thorough understanding of the Nostr ecosystem; therefore, we conduct the first large-scale study on this topic. Our study focuses on two key aspects: Nostr relays and post replication strategies. We find that Nostr achieves superior decentralization compared to traditional Fediverse applications. However, relay availability remains a challenge, where financial sustainability (particularly for free-to-use relays) emerges as a contributing factor. We also find that the replication of posts across relays enhances censorship-resistance but introduces significant overhead. To address this, we propose two improvements: one to control the number of post replications, and another to reduce the overhead during post retrieval. Via a data-driven evaluation, we demonstrate their ability to reduce overhead without negatively impacting post availability under the simulated scenarios.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network inference via process motifs for lagged correlation in linear stochastic processes</title>
<link>https://arxiv.org/abs/2208.08871</link>
<guid>https://arxiv.org/abs/2208.08871</guid>
<content:encoded><![CDATA[
<div> Keywords: causal inference, time-series data, pairwise edge measure, network inference, linear stochastic processes

Summary:
Pairwise edge measures (PEMs) are proposed for inferring causal networks from time-series data, addressing the trade-off between computational feasibility and accuracy. The PEMs utilize lagged correlation matrices to correct for confounding factors and reverse causation, drawing on process motifs for lagged covariance in autoregressive models. Simulation results show that these PEMs can accurately and efficiently infer networks, outperforming or matching methods like Granger causality and transfer entropy in slightly autocorrelated data sets. The approach offers a fast and theoretically grounded alternative for network inference, potentially informing linear model inference paradigms like Granger causality and vector-autoregression. <div>
arXiv:2208.08871v3 Announce Type: replace-cross 
Abstract: A major challenge for causal inference from time-series data is the trade-off between computational feasibility and accuracy. Motivated by process motifs for lagged covariance in an autoregressive model with slow mean-reversion, we propose to infer networks of causal relations via pairwise edge measure (PEMs) that one can easily compute from lagged correlation matrices. Motivated by contributions of process motifs to covariance and lagged variance, we formulate two PEMs that correct for confounding factors and for reverse causation. To demonstrate the performance of our PEMs, we consider network interference from simulations of linear stochastic processes, and we show that our proposed PEMs can infer networks accurately and efficiently. Specifically, for slightly autocorrelated time-series data, our approach achieves accuracies higher than or similar to Granger causality, transfer entropy, and convergent crossmapping -- but with much shorter computation time than possible with any of these methods. Our fast and accurate PEMs are easy-to-implement methods for network inference with a clear theoretical underpinning. They provide promising alternatives to current paradigms for the inference of linear models from time-series data, including Granger causality, vector-autoregression, and sparse inverse covariance estimation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Encoding for Improved Representation Learning over Graphs</title>
<link>https://arxiv.org/abs/2407.02758</link>
<guid>https://arxiv.org/abs/2407.02758</guid>
<content:encoded><![CDATA[
<div> Keywords: message-passing, global attention, graph learning, node embeddings, information aggregation

Summary: 
The paper introduces a novel approach called the differential encoding method to enhance graph learning by addressing the issue of information loss during node embedding generation. By encoding the difference between information from a node's neighbors and the node itself, the method improves the representational ability of generated node embeddings. The differential encoding is combined with the original aggregated representation to update the node embeddings, leading to improved performance on various graph tasks. Empirical evaluations on seven benchmark datasets demonstrate the effectiveness of the method in enhancing both message-passing updates and global attention updates. Overall, the differential encoding method presents a general solution for improving graph representation learning, advancing the state-of-the-art performance in this field. 

<br /><br />Summary: <div>
arXiv:2407.02758v2 Announce Type: replace-cross 
Abstract: Combining the message-passing paradigm with the global attention mechanism has emerged as an effective framework for learning over graphs. The message-passing paradigm and the global attention mechanism fundamentally generate node embeddings based on information aggregated from a node's local neighborhood or from the whole graph. The most basic and commonly used aggregation approach is to take the sum of information from a node's local neighbourhood or from the whole graph. However, it is unknown if the dominant information is from a node itself or from the node's neighbours (or the rest of the graph nodes). Therefore, there exists information lost at each layer of embedding generation, and this information lost could be accumulated and become more serious when more layers are used in the model. In this paper, we present a differential encoding method to address the issue of information lost. The idea of our method is to encode the differential representation between the information from a node's neighbours (or the rest of the graph nodes) and that from the node itself. The obtained differential encoding is then combined with the original aggregated local or global representation to generate the updated node embedding. By integrating differential encodings, the representational ability of generated node embeddings is improved. The differential encoding method is empirically evaluated on different graph tasks on seven benchmark datasets. The results show that it is a general method that improves the message-passing update and the global attention update, advancing the state-of-the-art performance for graph representation learning on these datasets.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniTraj: Learning a Universal Trajectory Foundation Model from Billion-Scale Worldwide Traces</title>
<link>https://arxiv.org/abs/2411.03859</link>
<guid>https://arxiv.org/abs/2411.03859</guid>
<content:encoded><![CDATA[
<div> dataset, trajectory, modeling, pre-training strategies, architecture

Summary:
UniTraj is introduced as a Universal Trajectory foundation model to overcome limitations of existing trajectory modeling approaches. The model addresses task specificity, regional dependency, and data sensitivity through three key innovations. Firstly, WorldTrace dataset is created with 2.45 million trajectories spanning 70 countries for region-independent modeling. Novel pre-training strategies, Adaptive Trajectory Resampling and Self-supervised Trajectory Masking, enable robust learning from heterogeneous trajectory data. A flexible model architecture is tailored to capture complex movement patterns for various trajectory tasks, ensuring broad applicability. Extensive experiments demonstrate that UniTraj outperforms existing methods in scalability, adaptability, and generalization, with WorldTrace as a versatile training resource. <br /><br />Summary: <div>
arXiv:2411.03859v3 Announce Type: replace-cross 
Abstract: Building a universal trajectory foundation model is a promising solution to address the limitations of existing trajectory modeling approaches, such as task specificity, regional dependency, and data sensitivity. Despite its potential, data preparation, pre-training strategy development, and architectural design present significant challenges in constructing this model. Therefore, we introduce UniTraj, a Universal Trajectory foundation model that aims to address these limitations through three key innovations. First, we construct WorldTrace, an unprecedented dataset of 2.45 million trajectories with billions of GPS points spanning 70 countries, providing the diverse geographic coverage essential for region-independent modeling. Second, we develop novel pre-training strategies--Adaptive Trajectory Resampling and Self-supervised Trajectory Masking--that enable robust learning from heterogeneous trajectory data with varying sampling rates and quality. Finally, we tailor a flexible model architecture to accommodate a variety of trajectory tasks, effectively capturing complex movement patterns to support broad applicability. Extensive experiments across multiple tasks and real-world datasets demonstrate that UniTraj consistently outperforms existing methods, exhibiting superior scalability, adaptability, and generalization, with WorldTrace serving as an ideal yet non-exclusive training resource.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Haar-Laplacian for directed graphs</title>
<link>https://arxiv.org/abs/2411.15527</link>
<guid>https://arxiv.org/abs/2411.15527</guid>
<content:encoded><![CDATA[
<div> Keywords: Laplacian matrix, spectral convolutional networks, directed graphs, Haar-like transformation, graph signal processing

Summary: 
This paper presents a novel Laplacian matrix design that allows for the development of spectral convolutional networks and expands the application of signal processing to directed graphs. Inspired by a Haar-like transformation, the proposed matrix is Hermitian and maintains a one-to-one relationship with the adjacency matrix, preserving both direction and weight information. It also exhibits properties such as scaling robustness, sensitivity, continuity, and directionality. The theoretical framework aligns with spectral graph theory, supporting the effectiveness of the approach. The paper discusses two main applications: graph learning, demonstrated through the implementation of HaarNet, a spectral graph convolutional network utilizing the Haar-Laplacian, and graph signal processing. Experimental results indicate improved performance in tasks like weight prediction and denoising on directed graphs. This innovative approach holds promise for enhancing signal processing capabilities on directed graph structures.<br /><br />Summary: <div>
arXiv:2411.15527v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel Laplacian matrix aiming to enable the construction of spectral convolutional networks and to extend the signal processing applications for directed graphs. Our proposal is inspired by a Haar-like transformation and produces a Hermitian matrix which is not only in one-to-one relation with the adjacency matrix, preserving both direction and weight information, but also enjoys desirable additional properties like scaling robustness, sensitivity, continuity, and directionality. We take a theoretical standpoint and support the conformity of our approach with the spectral graph theory. Then, we address two use-cases: graph learning (by introducing HaarNet, a spectral graph convolutional network built with our Haar-Laplacian) and graph signal processing. We show that our approach gives better results in applications like weight prediction and denoising on directed graphs.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Aware Opinion Dynamics in Multi-Agents Systems under Malicious Agent Influence</title>
<link>https://arxiv.org/abs/2412.01524</link>
<guid>https://arxiv.org/abs/2412.01524</guid>
<content:encoded><![CDATA[
<div> consensus mechanisms, multi-agent systems, Boomerang Effect, malicious agents, evolution rate adjustment 

Summary: 
This brief addresses the vulnerability of MAS to malicious agents by leveraging the Boomerang Effect from sociology. It highlights the trade-off between cost and convergence speed in implementing a resistance strategy against harmful deviations. The Boomerang-style fusion is proposed as a solution, analyzing the additional costs and introducing a cost-aware evolution rate adjustment mechanism. Through multi-robot simulations, it is shown that this mechanism effectively suppresses excess costs while maintaining resilience to extremist disruptions, ensuring stable convergence and enabling MAS to efficiently develop in an ethical order. <div>
arXiv:2412.01524v3 Announce Type: replace-cross 
Abstract: In many MASs, links to malicious agents cannot be severed immediately. Under these conditions, averaging-only consensus mechanisms typically lack sufficient resistance, leaving the system vulnerable to harmful deviations. To address this challenge, this brief leverages the Boomerang Effect from sociology, which drives normal agents to firmly reject malicious inputs, although this strategy may appear overly cautious. Thus, this brief emphasizes the necessity of acknowledging the resulting trade-off between cost and convergence speed in practice. To address this, the additional costs induced by Boomerang-style fusion is analyzed and a cost aware evolution rate adjustment mechanism is proposed. Multi-robot simulations demonstrate that this mechanism suppresses excess costs while maintaining resilience to extremist disruptions and ensuring stable convergence, enabling MAS to efficiently develop in a ethical order.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Occasional to Steady: Habit Formation Insights From a Comprehensive Fitness Study</title>
<link>https://arxiv.org/abs/2501.01779</link>
<guid>https://arxiv.org/abs/2501.01779</guid>
<content:encoded><![CDATA[
<div> Keywords: gym attendance, habit formation, survival metric, personalized guidance, social dynamics 

Summary: 
This study examines gym attendance data to understand the factors influencing habit formation in exercise. The research identifies key periods critical for habit formation and categorizes members into clusters based on their visit patterns. Differences in response to interventions, such as group classes and personal training, are observed among subgroups. Causal inference analysis indicates that personalized guidance and social interactions play crucial roles in sustaining long-term engagement. The study emphasizes the importance of a tailored, multi-dimensional approach that integrates social dynamics, personalized guidance, and strategic interventions to promote consistent exercise habits. <div>
arXiv:2501.01779v2 Announce Type: replace-cross 
Abstract: Regular exercise is widely recognized as a cornerstone of health, yet sustaining consistent exercise habits remains challenging. Understanding the factors that influence the formation of these habits is crucial for developing effective interventions. This study utilizes data from Mars Athletic Club, T\"urkiye's largest sports chain, to investigate the dynamics of gym attendance and habit formation. The general problem addressed by this study is identifying the critical periods and factors that contribute to the successful establishment of consistent exercise routines among gym-goers. We show that specific periods of attendance are most crucial for habit formation. By developing a survival metric based on gym attendance patterns, we pinpoint these key phases and segment members into distinct clusters based on their visit patterns. Our analysis reveals significant differences in how various subgroups respond to interventions, such as group classes, personal trainer sessions, and visiting different clubs. Using causal inference analysis, we demonstrate that personalized guidance and social dynamics are key drivers of sustained long-term engagement. By systematically examining these variables and considering the specific characteristics of different clusters, our research highlights the importance of a tailored, multi-dimensional approach to promoting exercise habits, which integrates social dynamics, personalized guidance, and strategic interventions to sustain long-term engagement.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order shortest paths in hypergraphs</title>
<link>https://arxiv.org/abs/2502.03020</link>
<guid>https://arxiv.org/abs/2502.03020</guid>
<content:encoded><![CDATA[
<div> Keywords: complex networks, hypergraphs, path size, higher-order connectivity, empirical networks

Summary: 
This study delves into the connectivity properties of complex networks by utilizing hypergraphs to model non-dyadic interactions. The concept of path size is introduced to characterize higher-order connectivity in various empirical networks, both with and without temporal data. The analysis reveals that non-dyadic ties play a crucial role in efficient shortest paths, proving their significance in system connectivity. Furthermore, dyadic edges are essential for connecting peripheral nodes, particularly in time-varying systems. By comparing the findings with randomised null models, a nuanced understanding of the structural organization of networks with higher-order interactions is achieved. This research enhances our knowledge of how different types of connections contribute to the overall connectivity and efficiency of complex systems.<br /><br />Summary: <div>
arXiv:2502.03020v3 Announce Type: replace-cross 
Abstract: One of the defining features of complex networks is the connectivity properties that we observe emerging from local interactions. Recently, hypergraphs have emerged as a versatile tool to model networks with non-dyadic, higher-order interactions. Nevertheless, the connectivity properties of real-world hypergraphs remain largely understudied. In this work we introduce path size as a measure to characterise higher-order connectivity and quantify the relevance of non-dyadic ties for efficient shortest paths in a diverse set of empirical networks with and without temporal information. By comparing our results with simple randomised null models, our analysis presents a nuanced picture, suggesting that non-dyadic ties are often central and are vital for system connectivity, while dyadic edges remain essential to connect more peripheral nodes, an effect which is particularly pronounced for time-varying systems. Our work contributes to a better understanding of the structural organisation of systems with higher-order interactions.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering the Herd: A Framework for LLM-based Control of Social Learning</title>
<link>https://arxiv.org/abs/2504.02648</link>
<guid>https://arxiv.org/abs/2504.02648</guid>
<content:encoded><![CDATA[
<div> information mediators, social learning, algorithms, Bayesian belief updates, social welfare

Summary: 
This study introduces a model of controlled sequential social learning where an information-mediating planner influences agents' decisions while they learn from peers. The planner can be altruistic, aiming for social welfare, or biased, aiming to induce a specific action. The study proves the convexity of the value function and characterizes optimal policies for both types of planners, showing intentional obfuscation of signals by biased planners in some cases. Even under transparency constraints, information mediation can significantly impact social welfare. Simulations with LLMs as planners and agents demonstrate emergent strategic behavior in shaping public opinion, with deviations suggesting non-Bayesian reasoning. The framework provides a basis for studying the impact and regulation of LLM information mediators. <div>
arXiv:2504.02648v3 Announce Type: replace-cross 
Abstract: Algorithms increasingly serve as information mediators--from social media feeds and targeted advertising to the increasing ubiquity of LLMs. This engenders a joint process where agents combine private, algorithmically-mediated signals with learning from peers to arrive at decisions. To study such settings, we introduce a model of controlled sequential social learning in which an information-mediating planner (e.g. an LLM) controls the information structure of agents while they also learn from the decisions of earlier agents. The planner may seek to improve social welfare (altruistic planner) or to induce a specific action the planner prefers (biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates.
  We prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from induced agent choices. Notably, in some regimes the biased planner intentionally obfuscates the agents' signals. Even under stringent transparency constraints--information parity with individuals, no lying or cherry-picking, and full observability--we show that information mediation can substantially shift social welfare in either direction. We complement our theory with simulations in which LLMs act as both planner and agents. Notably, the LLM planner in our simulations exhibits emergent strategic behavior in steering public opinion that broadly mirrors the trends predicted, though key deviations suggest the influence of non-Bayesian reasoning consistent with the cognitive patterns of both humans and LLMs trained on human-like data. Together, we establish our framework as a tractable basis for studying the impact and regulation of LLM information mediators.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Message passing for epidemiological interventions on networks with loops</title>
<link>https://arxiv.org/abs/2509.21596</link>
<guid>https://arxiv.org/abs/2509.21596</guid>
<content:encoded><![CDATA[
<div> message passing, spreading models, intervention design, neighborhood message passing, outbreak size

Summary:
The article introduces a study on spreading models in networks, focusing on design intervention problems such as vaccination rollouts and wastewater surveillance. The approach involves message passing for computing outcomes under various counterfactuals. However, classical message passing tends to overestimate outbreak sizes on real-world networks, leading to inaccurate predictions. To address this issue, the neighborhood message passing (NMP) framework is proposed for more accurate epidemiological calculations. The improved algorithm enhances estimates and is evaluated for intervention design problems like influence maximization, optimal vaccination, and sentinel surveillance. This advancement in spreading models can provide better insights for designing effective interventions in scenarios such as disease control and information diffusion. 

<br /><br />Summary: <div>
arXiv:2509.21596v1 Announce Type: new 
Abstract: Spreading models capture key dynamics on networks, such as cascading failures in economic systems, (mis)information diffusion, and pathogen transmission. Here, we focus on design intervention problems -- for example, designing optimal vaccination rollouts or wastewater surveillance systems -- which can be solved by comparing outcomes under various counterfactuals. A leading approach to computing these outcomes is message passing, which allows for the rapid and direct computation of the marginal probabilities for each node. However, despite its efficiency, classical message passing tends to overestimate outbreak sizes on real-world networks, leading to incorrect predictions and, thus, interventions. Here, we improve these estimates by using the neighborhood message passing (NMP) framework for the epidemiological calculations. We evaluate the quality of the improved algorithm and demonstrate how it can be used to test possible solutions to three intervention design problems: influence maximization, optimal vaccination, and sentinel surveillance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributed Hypergraph Generation with Realistic Interplay Between Structure and Attributes</title>
<link>https://arxiv.org/abs/2509.21838</link>
<guid>https://arxiv.org/abs/2509.21838</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, generative models, node attributes, NoAH, structure-attribute interplay

Summary: 
NoAH is a stochastic hypergraph generative model designed specifically for attributed hypergraphs. It utilizes a core-fringe node hierarchy to model hyperedge formation in a structured way, taking into account node attributes. The model, along with the NoAHFit parameter learning procedure, can accurately replicate the structure-attribute interplay observed in real-world hypergraphs. Through experiments on nine datasets spanning different domains, NoAH outperforms eight baseline models in terms of six metrics, showcasing its effectiveness in capturing the interactions between hypergraph structure and node attributes. <div>
arXiv:2509.21838v1 Announce Type: new 
Abstract: In many real-world scenarios, interactions happen in a group-wise manner with multiple entities, and therefore, hypergraphs are a suitable tool to accurately represent such interactions. Hyperedges in real-world hypergraphs are not composed of randomly selected nodes but are instead formed through structured processes. Consequently, various hypergraph generative models have been proposed to explore fundamental mechanisms underlying hyperedge formation. However, most existing hypergraph generative models do not account for node attributes, which can play a significant role in hyperedge formation. As a result, these models fail to reflect the interactions between structure and node attributes. To address the issue above, we propose NoAH, a stochastic hypergraph generative model for attributed hypergraphs. NoAH utilizes the core-fringe node hierarchy to model hyperedge formation as a series of node attachments and determines attachment probabilities based on node attributes. We further introduce NoAHFit, a parameter learning procedure that allows NoAH to replicate a given real-world hypergraph. Through experiments on nine datasets across four different domains, we show that NoAH with NoAHFit more accurately reproduces the structure-attribute interplay observed in the real-world hypergraphs than eight baseline hypergraph generative models, in terms of six metrics.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagining Agent-based Modeling with Large Language Model Agents via Shachi</title>
<link>https://arxiv.org/abs/2509.21862</link>
<guid>https://arxiv.org/abs/2509.21862</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, multi-agent systems, cognitive architecture, controlled experimentation, U.S. tariff shock<br />
<br />
Summary: 
The study introduces Shachi, a formal methodology and framework for analyzing emergent behaviors in large language model-driven multi-agent systems. It decomposes agents' policies into core components - Configuration, Memory, and Tools - orchestrated by an LLM reasoning engine. This architecture enables systematic analysis of how design choices influence collective behavior. Validation on a 10-task benchmark demonstrates the methodology's power and applicability through novel scientific inquiries. The methodology is further validated by modeling a real-world U.S. tariff shock, where agent behaviors align with observed market reactions when appropriately configured with memory and tools. This work provides an open-source foundation for constructing and evaluating LLM agents, promoting cumulative and scientifically grounded research in this field.<br /><br /> <div>
arXiv:2509.21862v1 Announce Type: cross 
Abstract: The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modularity and random graphs</title>
<link>https://arxiv.org/abs/2509.22066</link>
<guid>https://arxiv.org/abs/2509.22066</guid>
<content:encoded><![CDATA[
<div> modularity, graph theory, community detection, random graphs, binomial random graph <br />
<br />
Summary: 
This chapter discusses modularity in graph theory, particularly focusing on its application in community detection algorithms. Modularity measures the quality of vertex-partitions in capturing the community structure of a graph, with higher scores indicating better partitioning. The maximum modularity score, denoted as $q^*(G)$, ranges from 0 to 1. The chapter explores the behavior of modularity in various types of random graphs, starting with the binomial random graph $G_{n,p}$ characterized by its number of vertices and edge-probability $p$. Understanding modularity in random graphs is crucial for assessing community structures and optimizing community detection algorithms. <div>
arXiv:2509.22066v1 Announce Type: cross 
Abstract: This work will appear as a chapter in a forthcoming volume titled `Topics in Probabilistic Graph Theory'.
  For a given graph $G$, each partition of the vertices has a modularity score, with higher values indicating that the partition better captures community structure in $G$. The modularity $q^*(G)$ of $G$ is the maximum over all vertex-partitions of the modularity score, and satisfies $0\leq q^*(G)< 1$. Modularity lies at the heart of the most popular algorithms for community detection. In this chapter we discuss the behaviour of the modularity of various kinds of random graphs, starting with the binomial random graph $G_{n,p}$ with $n$ vertices and edge-probability $p$.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Internet of Us</title>
<link>https://arxiv.org/abs/2503.16448</link>
<guid>https://arxiv.org/abs/2503.16448</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Media, Internet, Diversity, IoU, Human-AI Society

Summary:
The paper discusses the potential of social media and the internet in exposing us to a wide range of human diversity in terms of demographics, language, culture, knowledge, opinions, and talents. This diversity presents opportunities to tap into skills and competences that may be unknown to us but could be crucial in solving our problems. However, despite this potential, there is a lack of tools and skills to effectively harness the complexities that come with exploiting diversity. The concept of the Internet of Us (IoU) is introduced as a new approach to online interactions that are diversity-aware. By understanding and leveraging the various facets of diversity in social settings, the IoU aims to create deeper and more meaningful social relationships. The paper emphasizes the need for multidisciplinary collaboration to fully realize the benefits of the IoU and create a hybrid human-AI society that is diversity-aware. 

<br /><br />Summary: <div>
arXiv:2503.16448v2 Announce Type: replace-cross 
Abstract: Social Media and the Internet have catalyzed an unprecedented potential for exposure to human diversity in terms of demographics, language, culture, knowledge, opinions, talents and the like. Access to people's diversity gives us the possibility of exploiting skills and competences that we do not have, that we may not even know they exist, the so called unknown unknowns, but which, however, could be exactly what we need when looking for help in the solution of the our current problem. However, this potential has not come with new, much needed, instruments and skills to harness the complications which rise when trying to exploit the diversity of people. This paper presents our vision of the Internet of Us (IoU), a new type of online diversity-aware social interactions capable of promoting richer and deeper social relations. We discuss the multiple facets of diversity in social settings and the multidisciplinary work that is required to reap the benefits of the IoU, towards a IoU-enabled diversity-aware hybrid human-AI society.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Authority and the Rhetoric of Health Misinformation: A Multimodal Analysis of Social Media Videos</title>
<link>https://arxiv.org/abs/2509.20724</link>
<guid>https://arxiv.org/abs/2509.20724</guid>
<content:encoded><![CDATA[
<div> Keywords: short form video platforms, credibility, nutrition, supplements, authority signals

Summary: 
Short form video platforms like TikTok, Instagram, and YouTube are popular sources of health advice, where a mix of helpful and misleading content can be found. This study focuses on how credibility is conveyed in nutrition and supplement videos through authority signals, narrative techniques, and monetization strategies. Analysis of 152 public videos reveals that confident single presenters in studio or home settings are common, with authority cues like titles, slides, and certificates often used. Persuasive elements such as jargon, references, fear tactics, and critiques of mainstream medicine are also prevalent, along with monetization features like sales links and subscription requests. References to science are often paired with emotional or oppositional narratives instead of signaling caution. Clinical contexts are infrequent, highlighting the dominance of persuasive and emotional storytelling in these videos. <div>
arXiv:2509.20724v1 Announce Type: new 
Abstract: Short form video platforms are central sites for health advice, where alternative narratives mix useful, misleading, and harmful content. Rather than adjudicating truth, this study examines how credibility is packaged in nutrition and supplement videos by analyzing the intersection of authority signals, narrative techniques, and monetization. We assemble a cross platform corpus of 152 public videos from TikTok, Instagram, and YouTube and annotate each on 26 features spanning visual authority, presenter attributes, narrative strategies, and engagement cues. A transparent annotation pipeline integrates automatic speech recognition, principled frame selection, and a multimodal model, with human verification on a stratified subsample showing strong agreement. Descriptively, a confident single presenter in studio or home settings dominates, and clinical contexts are rare. Analytically, authority cues such as titles, slides and charts, and certificates frequently occur with persuasive elements including jargon, references, fear or urgency, critiques of mainstream medicine, and conspiracies, and with monetization including sales links and calls to subscribe. References and science like visuals often travel with emotive and oppositional narratives rather than signaling restraint.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Group Anchors in Real-World Group Interactions Under Label Scarcity</title>
<link>https://arxiv.org/abs/2509.20762</link>
<guid>https://arxiv.org/abs/2509.20762</guid>
<content:encoded><![CDATA[
<div> identification, group anchors, AnchorRadar, real-world interactions, semi-supervised <br />
Summary: <br />
This article discusses the concept of group anchors in various real-world interactions and the problem of identifying them. It introduces AnchorRadar, a semi-supervised method designed to identify group anchors in scenarios with limited labeled data. The study showcases the empirical success of AnchorRadar compared to other baselines, demonstrating higher accuracy and efficiency. The method outperforms competitors in accuracy while requiring significantly less training time and fewer parameters. Through experiments on thirteen datasets, the effectiveness of AnchorRadar in identifying group anchors is highlighted, emphasizing its relevance in practical applications with limited labeled data. <div>
arXiv:2509.20762v1 Announce Type: new 
Abstract: Group interactions occur in various real-world contexts, e.g., co-authorship, email communication, and online Q&amp;A. In each group, there is often a particularly significant member, around whom the group is formed. Examples include the first or last author of a paper, the sender of an email, and the questioner in a Q&amp;A session. In this work, we discuss the existence of such individuals in real-world group interactions. We call such individuals group anchors and study the problem of identifying them. First, we introduce the concept of group anchors and the identification problem. Then, we discuss our observations on group anchors in real-world group interactions. Based on our observations, we develop AnchorRadar, a fast and effective method for group anchor identification under realistic settings with label scarcity, i.e., when only a few groups have known anchors. AnchorRadar is a semi-supervised method using information from groups both with and without known group anchors. Finally, through extensive experiments on thirteen real-world datasets, we demonstrate the empirical superiority of AnchorRadar over various baselines w.r.t. accuracy and efficiency. In most cases, AnchorRadar achieves higher accuracy in group anchor identification than all the baselines, while using 10.2$\times$ less training time than the fastest baseline and 43.6$\times$ fewer learnable parameters than the most lightweight baseline on average.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence of the majority group on individual judgments in online spontaneous conversations</title>
<link>https://arxiv.org/abs/2509.21092</link>
<guid>https://arxiv.org/abs/2509.21092</guid>
<content:encoded><![CDATA[
<div> Keywords: social conformity, anti-conformity, online conversations, judgment formation, linguistic analysis

Summary: 
This study examines the influence of the majority group on individual judgment formation and expression in anonymous online conversations. By analyzing digital traces of conversations on social media platforms, the researchers found that individuals tend towards anti-conformity behaviors. While they align their judgments with the majority's orientation, they diverge from the group's stance, using persuasive language to express their opinions. The study highlights how online environments shape social influence differently from offline contexts, showcasing a dynamic interplay between conformity and anti-conformity in online interactions. The findings suggest that individuals navigate social pressures in online discussions by balancing between conformity and asserting their unique perspectives. This research contributes to understanding the complexities of group dynamics in digital spaces and sheds light on the nuanced ways individuals form and express judgments in online environments.<br /><br />Summary: <div>
arXiv:2509.21092v1 Announce Type: new 
Abstract: This study investigates how the majority group influences individual judgment formation and expression in anonymous, spontaneous online conversations. Drawing on theories of social conformity and anti-conformity, we analyze everyday dilemmas discussed on social media. First, using digital traces to operationalize judgments, we measure the conversations' disagreement and apply Bayesian regression to capture shifts of judgments formation before and after the group's exposure. Then we analyze changes in judgment expression with a linguistic analysis of the motivations associated with each judgment. Results show systematic anti-conformity behaviors: individuals preserve the majority's positive or negative orientation of judgments but diverge from its stance, with persuasive language increasing post-disclosure. Our findings highlight how online environments reshape social influence compared to offline contexts.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Enhanced Multi-Dimensional Measurement of Technological Convergence through Heterogeneous Graph and Semantic Learning</title>
<link>https://arxiv.org/abs/2509.21187</link>
<guid>https://arxiv.org/abs/2509.21187</guid>
<content:encoded><![CDATA[
<div> Technological Convergence Index, depth, breadth, Heterogeneous Graph Transformers, Shannon Diversity Index

Summary:<br /><br />Technological convergence, where boundaries between technology areas blur, is a key trend in innovation. Accurately measuring convergence is challenging due to its multidimensional nature. This study introduces a Technological Convergence Index (TCI) that assesses depth and breadth. Depth is calculated using IPC descriptions and additional patent metadata in a graph structure analyzed with Heterogeneous Graph Transformers and Sentence-BERT. Breadth measures the diversity of technological fields involved using the Shannon Diversity Index. TCI combines these dimensions using the Entropy Weight Method. Validation against established measures shows TCI's advantages and empirical reliability is confirmed through regression against patent quality indicators. The multidimensional approach offers insights for innovation policy and industry strategies in managing cross-domain technologies. 

Summary: <br /><br />Technological convergence is a major trend in innovation, but accurately measuring it is challenging due to its multidimensional nature. This study introduces a Technological Convergence Index (TCI) that measures convergence depth and breadth. Depth is calculated using IPC descriptions and additional patent metadata in a graph structure analyzed with Heterogeneous Graph Transformers and Sentence-BERT. Breadth measures the diversity of technological fields involved using the Shannon Diversity Index. TCI combines these dimensions using the Entropy Weight Method, providing valuable insights for innovation policy and industry strategies in managing emerging cross-domain technologies. <div>
arXiv:2509.21187v1 Announce Type: new 
Abstract: Technological convergence refers to the phenomenon where boundaries between technological areas and disciplines are increasingly blurred. It enables the integration of previously distinct domains and has become a mainstream trend in today's innovation process. However, accurately measuring technological convergence remains a persistent challenge due to its inherently multidimensional and evolving nature. This study designs an Technological Convergence Index (TCI) that comprehensively measures convergence along two fundamental dimensions: depth and breadth. For depth calculation, we use IPC textual descriptions as the analytical foundation and enhance this assessment by incorporating supplementary patent metadata into a heterogeneous graph structure. This graph is then modeled using Heterogeneous Graph Transformers in combination with Sentence-BERT, enabling a precise representation of knowledge integration across technological boundaries. Complementing this, the breadth dimension captures the diversity of technological fields involved, quantified through the Shannon Diversity Index to measure the variety of technological combinations within patents. Our final TCI is constructed using the Entropy Weight Method, which objectively assigns weights to both dimensions based on their information entropy. To validate our approach, we compare the proposed TCI against established convergence measures, demonstrating its comparative advantages. We further establish empirical reliability through a novel robustness test that regresses TCI against indicators of patent quality. These findings are further substantiated through comprehensive robustness checks. Our multidimensional approach provides valuable practical insights for innovation policy and industry strategies in managing emerging cross-domain technologies.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evading Overlapping Community Detection via Proxy Node Injection</title>
<link>https://arxiv.org/abs/2509.21211</link>
<guid>https://arxiv.org/abs/2509.21211</guid>
<content:encoded><![CDATA[
<div> community membership hiding, social graphs, privacy protection, deep reinforcement learning, overlapping communities

Summary:
This paper introduces the concept of community membership hiding (CMH) in social graphs to protect privacy by preventing sensitive information inference without altering graph topology. Previous work on non-overlapping communities does not fully address real-world graphs with overlapping communities. The authors propose a deep reinforcement learning (DRL) approach to learn effective modification policies, including the use of proxy nodes while maintaining graph structure. Experimental results on real-world datasets demonstrate that the proposed method outperforms existing baselines in both effectiveness and efficiency, providing a systematic approach for privacy-preserving graph modification in the presence of overlapping communities. <br /><br />Summary: <div>
arXiv:2509.21211v1 Announce Type: new 
Abstract: Protecting privacy in social graphs requires preventing sensitive information, such as community affiliations, from being inferred by graph analysis, without substantially altering the graph topology. We address this through the problem of \emph{community membership hiding} (CMH), which seeks edge modifications that cause a target node to exit its original community, regardless of the detection algorithm employed. Prior work has focused on non-overlapping community detection, where trivial strategies often suffice, but real-world graphs are better modeled by overlapping communities, where such strategies fail. To the best of our knowledge, we are the first to formalize and address CMH in this setting. In this work, we propose a deep reinforcement learning (DRL) approach that learns effective modification policies, including the use of proxy nodes, while preserving graph structure. Experiments on real-world datasets show that our method significantly outperforms existing baselines in both effectiveness and efficiency, offering a principled tool for privacy-preserving graph modification with overlapping communities.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Framework for Evaluating Dynamic Network Generative Models and Anomaly Detection</title>
<link>https://arxiv.org/abs/2406.11901</link>
<guid>https://arxiv.org/abs/2406.11901</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic networks, graph convolutional network, anomaly detection, generative models, attention mechanism

Summary: 
DGSP-GCN is a deep learning framework that combines graph convolutional networks with dynamic graph signal processing techniques for evaluating generative models and detecting anomalies in dynamic networks. It assesses the similarity between generated network snapshots and expected temporal evolution, using an attention mechanism to enhance embedding quality and capture dynamic structural changes. The approach was tested on real-world datasets, outperforming baseline methods with low error rates. DGSP-GCN's effectiveness in evaluating and detecting anomalies in dynamic networks provides valuable insights for network evolution and anomaly detection research. <div>
arXiv:2406.11901v2 Announce Type: replace 
Abstract: Understanding dynamic systems like disease outbreaks, social influence, and information diffusion requires effective modeling of complex networks. Traditional evaluation methods for static networks often fall short when applied to temporal networks. This paper introduces DGSP-GCN (Dynamic Graph Similarity Prediction based on Graph Convolutional Network), a deep learning-based framework that integrates graph convolutional networks with dynamic graph signal processing techniques to provide a unified solution for evaluating generative models and detecting anomalies in dynamic networks. DGSP-GCN assesses how well a generated network snapshot matches the expected temporal evolution, incorporating an attention mechanism to improve embedding quality and capture dynamic structural changes. The approach was tested on five real-world datasets: WikiMath, Chickenpox, PedalMe, MontevideoBus, and MetraLa. Results show that DGSP-GCN outperforms baseline methods, such as time series regression and random similarity assignment, achieving the lowest error rates (MSE of 0.0645, MAE of 0.1781, RMSE of 0.2507). These findings highlight DGSP-GCN's effectiveness in evaluating and detecting anomalies in dynamic networks, offering valuable insights for network evolution and anomaly detection research.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heaven &amp; Hell: One-Step Hub Consensus</title>
<link>https://arxiv.org/abs/2509.19630</link>
<guid>https://arxiv.org/abs/2509.19630</guid>
<content:encoded><![CDATA[
<div> weighted directed graphs, influence dynamics, global convergence, hub node, Coq proof assistant

Summary: 
- The study focuses on influence dynamics on finite weighted directed graphs with a central hub node and binary vertex states ('Glory' or 'Gnash').
- The research provides a precise criterion that ensures global convergence to Glory in a single synchronous update from any initial state on the graph.
- The criterion states that at each non-hub vertex, the incoming weight from the hub must be equal to or greater than the total incoming weight from all other nodes.
- The study extends the results to tau-biased update rules and asynchronous schedules, showing that a single pass is still sufficient under certain domination hypotheses.
- The research includes machine-checked proofs in the Coq proof assistant to validate the presented theorems. 

<br /><br />Summary: <div>
arXiv:2509.19630v1 Announce Type: new 
Abstract: Many networked systems require a central authority to enforce a global configuration against local peer influence. We study influence dynamics on finite weighted directed graphs with a distinguished hub node and binary vertex states ('Glory' or 'Gnash'). We give a sharp, local, and efficiently checkable criterion that guarantees global convergence to Glory in a single synchronous update from any initial state. At each non-hub vertex, the incoming weight from the hub must at least match the total incoming weight from all other nodes. Specialising in uniform hub broadcasts, the exact threshold equals the maximum non-hub incoming weight over all vertices, and we prove this threshold is tight. We extend the result to a tau-biased update rule and to asynchronous (Gauss-Seidel) schedules, where a single pass still suffices under the same domination hypothesis. Machine-checked proofs in Coq accompany all theorems.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deterministic Frequency--Domain Inference of Network Topology and Hidden Components via Structure--Behavior Scaling</title>
<link>https://arxiv.org/abs/2509.19857</link>
<guid>https://arxiv.org/abs/2509.19857</guid>
<content:encoded><![CDATA[
<div> Keywords: hidden interactions, complex systems, network structure inference, spectral strength, evolutionary game dynamics

Summary: 
This article addresses the challenge of inferring network structures in complex systems with hidden components using indirect behavioral signals. It establishes a linear scaling relationship between the spectral strength of nodes under evolutionary game dynamics and their structural degree, enabling the development of a frequency-domain inference framework based on the discrete Fourier transform. This framework can reconstruct network topology directly from payoff sequences without prior knowledge of the network or internal node strategies. It can localize hidden nodes, identify edges connected to multiple hidden nodes, and estimate bounds on the number of hidden nodes. Experiments show that this method outperforms existing techniques in both topology reconstruction and hidden component detection, scaling efficiently to large networks and offering robustness to stochastic fluctuations. Overall, this work provides a principled approach to accurately recover network topology in complex systems with hidden elements.<br /><br />Summary: <div>
arXiv:2509.19857v1 Announce Type: new 
Abstract: Hidden interactions and components in complex systems-ranging from covert actors in terrorist networks to unobserved brain regions and molecular regulators-often manifest only through indirect behavioral signals. Inferring the underlying network structure from such partial observations remains a fundamental challenge, particularly under nonlinear dynamics. We uncover a robust linear relationship between the spectral strength of a node's behavioral time series under evolutionary game dynamics and its structural degree, $S \propto k$, a structural-behavioral scaling that holds across network types and scales, revealing a universal correspondence between local connectivity and dynamic energy. Leveraging this insight, we develop a deterministic, frequency-domain inference framework based on the discrete Fourier transform (DFT) that reconstructs network topology directly from payoff sequences-without prior knowledge of the network or internal node strategies-by selectively perturbing node dynamics. The framework simultaneously localizes individual hidden nodes or identifies all edges connected to multiple hidden nodes, and estimates tight bounds on the number of hidden nodes. Extensive experiments on synthetic and real-world networks demonstrate that our method consistently outperforms state-of-the-art baselines in both topology reconstruction and hidden component detection. Moreover, it scales efficiently to large networks, offering robustness to stochastic fluctuations and overcoming the size limitations of existing techniques. Our work establishes a principled connection between local dynamic observables and global structural inference, enabling accurate topology recovery in complex systems with hidden elements.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governing Together: Toward Infrastructure for Community-Run Social Media</title>
<link>https://arxiv.org/abs/2509.19653</link>
<guid>https://arxiv.org/abs/2509.19653</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized governance, social computing systems, inter-community governance, modularity, polycentricity <br />
Summary: <br />
Decentralizing governance in social computing systems can empower communities to make decisions in line with their values. However, communities often face common problems that cross boundaries, requiring mechanisms for inter-community governance. Six challenges in designing for inter-community governance were identified through workshops, focusing on modularity, forkability, and polycentricity. The proposed ideas form an ecosystem of resources, infrastructures, and tools for supporting community governance. Implementing these principles can enhance coordination and decision-making among communities in social computing systems. <div>
arXiv:2509.19653v1 Announce Type: cross 
Abstract: Decentralizing the governance of social computing systems to communities promises to empower them to make independent decisions, with nuance and in accordance with their values. Yet, communities do not govern in isolation. Many problems communities face are common, or move across their boundaries. We therefore propose designing for "inter-community governance:" mechanisms that support relationships and interactions between communities to coordinate on governance issues. Drawing from workshops with 24 individuals on decentralized, community-run social media, we present six challenges in designing for inter-community governance surfaced through ideas proposed in workshops. Together, these ideas come together as an ecosystem of resources, infrastructures, and tools that highlight three key principles for designing for inter-community governance: modularity, forkability, and polycentricity. We end with a discussion of how the ideas proposed in workshops might be implemented in future work aiming to support community governance in social computing systems broadly.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections</title>
<link>https://arxiv.org/abs/2509.19657</link>
<guid>https://arxiv.org/abs/2509.19657</guid>
<content:encoded><![CDATA[
<div> Keywords: pedestrian safety, driver yielding behavior, pedestrian–driver interaction, large language models, multimodal learning.

Summary:
Pedestrian safety and driver yielding behavior at crosswalks are crucial factors in urban mobility. Modeling these interactions accurately requires capturing complex behaviors, which traditional machine learning models struggle with due to fixed feature representations. This paper explores the use of large language models (LLMs) to model driver-pedestrian interactions at intersections. Through a novel prompt design that incorporates domain-specific knowledge, structured reasoning, and few-shot prompting, LLMs enable interpretable and context-aware inference of driver yielding behavior. Benchmarking against traditional classifiers, GPT-4o was found to achieve high accuracy and recall, while Deepseek-V3 performed well in precision. The study highlights the trade-offs between model performance and computational efficiency, providing valuable insights for deploying LLMs in real-world pedestrian safety systems.

<br /><br />Summary: <div>
arXiv:2509.19657v1 Announce Type: cross 
Abstract: Pedestrian safety is a critical component of urban mobility and is strongly influenced by the interactions between pedestrian decision-making and driver yielding behavior at crosswalks. Modeling driver--pedestrian interactions at intersections requires accurately capturing the complexity of these behaviors. Traditional machine learning models often struggle to capture the nuanced and context-dependent reasoning required for these multifactorial interactions, due to their reliance on fixed feature representations and limited interpretability. In contrast, large language models (LLMs) are suited for extracting patterns from heterogeneous traffic data, enabling accurate modeling of driver-pedestrian interactions. Therefore, this paper leverages multimodal LLMs through a novel prompt design that incorporates domain-specific knowledge, structured reasoning, and few-shot prompting, enabling interpretable and context-aware inference of driver yielding behavior, as an example application of modeling pedestrian--driver interaction. We benchmarked state-of-the-art LLMs against traditional classifiers, finding that GPT-4o consistently achieves the highest accuracy and recall, while Deepseek-V3 excels in precision. These findings highlight the critical trade-offs between model performance and computational efficiency, offering practical guidance for deploying LLMs in real-world pedestrian safety systems.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Muse-it: A Tool for Analyzing Music Discourse on Reddit</title>
<link>https://arxiv.org/abs/2509.20228</link>
<guid>https://arxiv.org/abs/2509.20228</guid>
<content:encoded><![CDATA[
<div> Keywords: music engagement, social media, natural language processing, big data analytics, Reddit

Summary:
Muse-it is a platform that facilitates the analysis of music engagement through natural, unprompted conversations on Reddit. It utilizes advances in natural language processing and big data analytics to analyze large-scale discourse around music-related topics. The platform retrieves comprehensive Reddit data based on user-defined queries, aggregates posts from various subreddits, and supports topic modeling, temporal trend analysis, and clustering. In addition, Muse-it identifies music-related hyperlinks, retrieves track-level metadata, and links this information to the discussions. An interactive interface allows for dynamic visualizations of the collected data, offering music researchers a user-friendly way to study music engagement as it occurs online. By enabling the extraction, processing, and analysis of big data from Reddit, Muse-it opens up new opportunities for understanding the multifaceted interactions individuals have with music in digital spaces. 

<br /><br />Summary: Muse-it is a platform that leverages natural language processing and big data analytics to analyze music engagement on Reddit. It allows for the retrieval and aggregation of data from various subreddits, supports topic modeling and trend analysis, and links music-related information to discussions. With an interactive interface for visualizations, Muse-it provides a user-friendly approach for understanding how people engage with music online. <div>
arXiv:2509.20228v1 Announce Type: cross 
Abstract: Music engagement spans diverse interactions with music, from selection and emotional response to its impact on behavior, identity, and social connections. Social media platforms provide spaces where such engagement can be observed in natural, unprompted conversations. Advances in natural language processing (NLP) and big data analytics make it possible to analyze these discussions at scale, extending music research to broader contexts. Reddit, in particular, offers anonymity that encourages diverse participation and yields rich discourse on music in ecological settings. Yet the scale of this data requires tools to extract, process, and analyze it effectively. We present Muse-it, a platform that retrieves comprehensive Reddit data centered on user-defined queries. It aggregates posts from across subreddits, supports topic modeling, temporal trend analysis, and clustering, and enables efficient study of large-scale discourse. Muse-it also identifies music-related hyperlinks (e.g., Spotify), retrieves track-level metadata such as artist, album, release date, genre, popularity, and lyrics, and links these to the discussions. An interactive interface provides dynamic visualizations of the collected data. Muse-it thus offers an accessible way for music researchers to gather and analyze big data, opening new avenues for understanding music engagement as it naturally unfolds online.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Into the Void: Understanding Online Health Information in Low-Web Data Languages</title>
<link>https://arxiv.org/abs/2509.20245</link>
<guid>https://arxiv.org/abs/2509.20245</guid>
<content:encoded><![CDATA[
<div> data voids, online health information seeking, social media platforms, algorithmic structures, low-web data languages 

Summary: 
Data voids, areas of the internet with scarce information, present challenges for online health information seekers, particularly in low-web data languages. Social media platforms, now informal search engines, also face this issue. Data horizons are critical boundaries where algorithmic structures degrade search result relevance and reliability, influenced by factors like linguistic underrepresentation and algorithmic amplification. Evaluating health query search results in Tigrinya and Amharic, common characteristics include results not in the search language and dominated by nutritional and religious advice. Search result divergence can result from algorithmic failures, manipulation by creators, or unintentional factors. These findings reveal how data horizons manifest under various constraints, impacting information availability in low-resourced languages. <div>
arXiv:2509.20245v1 Announce Type: cross 
Abstract: Data voids--areas of the internet where reliable information is scarce or absent--pose significant challenges to online health information seeking, particularly for users operating in low-web data languages. These voids are increasingly encountered not on traditional search engines alone, but on social media platforms, which have gradually morphed into informal search engines for millions of people. In this paper, we introduce the phenomenon of data horizons: a critical boundary where algorithmic structures begin to degrade the relevance and reliability of search results. Unlike the core of a data void, which is often exploited by bad actors to spread misinformation, the data horizon marks the critical space where systemic factors, such as linguistic underrepresentation, algorithmic amplification, and socio-cultural mismatch, create conditions of informational instability. Focusing on Tigrinya and Amharic as languages of study, we evaluate (1) the common characteristics of search results for health queries, (2) the quality and credibility of health information, and (3) characteristics of search results that diverge from their queries. We find that search results for health queries in low-web data languages may not always be in the language of search and may be dominated by nutritional and religious advice. We show that search results that diverge from their queries in low-resourced languages are due to algorithmic failures, (un)intentional manipulation, or active manipulation by content creators. We use our findings to illustrate how a data horizon manifests under several interacting constraints on information availability.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CueGCL: Cluster-aware Personalized Self-Training for Unsupervised Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2311.11073</link>
<guid>https://arxiv.org/abs/2311.11073</guid>
<content:encoded><![CDATA[
<div> Graph Contrastive Learning, Cluster-aware Framework, Unsupervised Learning, Graph Clustering, Node Representations <br />
<br />
Summary: 
The article introduces CueGCL, a Cluster-aware Graph Contrastive Learning Framework designed to improve performance in structure-related and unsupervised graph clustering tasks. Current GCL algorithms struggle in acquiring cluster-level information, leading to poor results. CueGCL addresses this issue with a PeST strategy for unsupervised scenarios, enabling precise cluster-level personalized information capture while reducing class collision and unfairness. The model incorporates aligned graph clustering (AGC) to align clustering spaces for more consistent node embeddings. Theoretical analysis demonstrates CueGCL's effectiveness in yielding a discernible cluster structure in the embedding space. Experimental results on benchmark datasets show that CueGCL achieves state-of-the-art performance across varied scales. <br /> <div>
arXiv:2311.11073v2 Announce Type: replace 
Abstract: Recently, graph contrastive learning (GCL) has emerged as one of the optimal solutions for node-level and supervised tasks. However, for structure-related and unsupervised tasks such as graph clustering, current GCL algorithms face difficulties acquiring the necessary cluster-level information, resulting in poor performance. In addition, general unsupervised GCL improves the performance of downstream tasks by increasing the number of negative samples, which leads to severe class collision and unfairness of graph clustering. To address the above issues, we propose a Cluster-aware Graph Contrastive Learning Framework (CueGCL) to jointly learn clustering results and node representations. Specifically, we design a personalized self-training (PeST) strategy for unsupervised scenarios, which enables our model to capture precise cluster-level personalized information. With the benefit of the PeST, we alleviate class collision and unfairness without sacrificing the overall model performance. Furthermore, aligned graph clustering (AGC) is employed to obtain the cluster partition, where we align the clustering space of our downstream task with that in PeST to achieve more consistent node embeddings. Finally, we theoretically demonstrate the effectiveness of our model, showing it yields an embedding space with a significantly discernible cluster structure. Extensive experimental results also show our CueGCL exhibits state-of-the-art performance on five benchmark datasets with different scales.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The networks of ingredient combinations as culinary fingerprints of world cuisines</title>
<link>https://arxiv.org/abs/2408.15162</link>
<guid>https://arxiv.org/abs/2408.15162</guid>
<content:encoded><![CDATA[
<div> network analysis, worldwide cuisines, ingredient combinations, culinary fingerprints, machine learning models

Summary: 
Investigating worldwide cuisines using network analysis of ingredient combinations reveals distinctive patterns in how ingredients are combined in popular dishes. Cuisines differ in ingredient type popularity, recipe sizes, and structural organization of ingredient-type combinations. European cuisines distribute ingredients across types, while certain Asian and South American cuisines emphasize one dominant type. Maximum spanning trees capture the essence of these patterns, serving as a representative backbone for each cuisine. Machine learning models can accurately identify cuisines from subsets of recipes using both full and simplified network representations. These networks also cluster global cuisines into meaningful geo-cultural groups, reflecting shared culinary traditions. The study provides insights into the structure of world cuisines, facilitating data-driven approaches to their characterization, cross-cultural comparison, and potential adaptation. 

Summary: <div>
arXiv:2408.15162v2 Announce Type: replace-cross 
Abstract: Investigating how different ingredients are combined in popular dishes is crucial to uncover the principles behind food preferences. Here, we use data from public food repositories and network analysis to characterize and compare worldwide cuisines. Ingredients are first grouped into broader types, and each cuisine is then represented as a network in which nodes correspond to ingredient types and weighted links describe how frequently pairs of types co-occur in recipes. Cuisines differ not only in the popularity of ingredient types and range of recipe sizes, but also in the structural organization of ingredient-type combinations. By analyzing these networks, we uncover distinctive patterns of type associations that serve as culinary fingerprints. For example, European cuisines typically distribute ingredients across different types, whereas certain Asian and South American traditions emphasize one dominant type, such as vegetables or spices. The essence of these patterns is well captured by the networks' maximum spanning trees, which offer a simplified yet representative backbone for each cuisine. We demonstrate that both these full and simplified network representations enable machine learning models to identify cuisines from subsets of recipes with very high accuracy. Networks of ingredient combinations also cluster global cuisines into meaningful geo-cultural groups, reflecting shared patterns in culinary traditions. More broadly, our study offers novel insights into the structure of world cuisines, enabling data-driven approaches to their characterization, cross-cultural comparison, and potential adaptation.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily in Complex Networks: Measures, Models, and Applications</title>
<link>https://arxiv.org/abs/2509.18289</link>
<guid>https://arxiv.org/abs/2509.18289</guid>
<content:encoded><![CDATA[
<div> homophily, social networks, network evolution, structural inequalities, higher-order network structures <br />
Summary: <br />
Homophily, the tendency for individuals to connect with others who share similar attributes, is a key feature of social networks. Understanding group interactions within and across networks is essential for analyzing network dynamics and structural inequalities. This tutorial provides an extensive overview of homophily, including definitions, properties, and limitations of common metrics. It also explores homophily in higher-order network structures like hypergraphs and simplicial complexes. The tutorial delves into network generating models that can create different types of homophilic networks with adjustable levels of homophily, showcasing their significance in real-world scenarios. Lastly, the tutorial addresses ongoing challenges, emerging areas of study, and prospects for further research in this field. <div>
arXiv:2509.18289v1 Announce Type: new 
Abstract: Homophily, the tendency of individuals to connect with others who share similar attributes, is a defining feature of social networks. Understanding how groups interact, both within and across, is crucial for uncovering the dynamics of network evolution and the emergence of structural inequalities in these network. This tutorial offers a comprehensive overview of homophily, covering its various definitions, key properties, and the limitations of widely used metrics. Extending beyond traditional pairwise interactions, we will discuss homophily in higher-order network structures such as hypergraphs and simplicial complexes. We will further discuss network generating models capable of producing different types of homophilic networks with tunable levels of homophily and highlight their relevance in real-world contexts. The tutorial concludes with a discussion of open challenges, emerging directions, and opportunities for further research in this area.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Constructive Conflict in Online Discussions through Controversial yet Toxicity Resilient Posts</title>
<link>https://arxiv.org/abs/2509.18303</link>
<guid>https://arxiv.org/abs/2509.18303</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithmic curation, controversiality, toxicity resilience, political posts, constructive dialogues

Summary: 
Algorithmic curation could enhance social media content by focusing on constructive conflicts. The study introduces controversiality and toxicity resilience as key criteria for identifying challenging yet respectful dialogues. Results show that assessing toxicity resilience is distinctive from identifying low-toxicity posts. Political posts, typically controversial, attract more toxic responses, but some can still foster civil engagement. Posts resilient to toxicity often incorporate politeness cues like gratitude and hedging, suggesting a potential strategy to promote constructive political discussions. This research highlights the potential of framing post tones to encourage positive political discourse. <div>
arXiv:2509.18303v1 Announce Type: new 
Abstract: Bridging content that brings together individuals with opposing viewpoints on social media remains elusive, overshadowed by echo chambers and toxic exchanges. We propose that algorithmic curation could surface such content by considering constructive conflicts as a foundational criterion. We operationalize this criterion through controversiality to identify challenging dialogues and toxicity resilience to capture respectful conversations. We develop high-accuracy models to capture these dimensions. Analyses based on these models demonstrate that assessing resilience to toxic responses is not the same as identifying low-toxicity posts. We also find that political posts are often controversial and tend to attract more toxic responses. However, some posts, even the political ones, are resilient to toxicity despite being highly controversial, potentially sparking civil engagement. Toxicity resilient posts tend to use politeness cues, such as showing gratitude and hedging. These findings suggest the potential for framing the tone of posts to encourage constructive political discussions.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-Neural-Network-Entropy model of vital node identification on network attack and propagation</title>
<link>https://arxiv.org/abs/2509.18325</link>
<guid>https://arxiv.org/abs/2509.18325</guid>
<content:encoded><![CDATA[
<div> method; graph neural networks; information entropy; vital node identification; network attack

Summary:
The article introduces a novel method, GNNE, which utilizes graph neural networks and information entropy to identify vital nodes in complex networks. The method integrates node features, interactions, and states, employing a Graph Convolutional Network (GCN) to learn node features and a Graph Attention Network (GAT) to determine node influence factors. By calculating nodes' entropy based on their influence factors, GNNE effectively evaluates node importance, particularly in protecting networks from intentional attacks. The GCN extracts node features while the GAT aggregates neighbor features using an attention mechanism to assign weights based on importance. Through training on a synthetic network and testing on real datasets, GNNE outperforms traditional topology-based methods and graph-machine-learning-based methods in identifying vital nodes for network attack and propagation. <div>
arXiv:2509.18325v1 Announce Type: new 
Abstract: Vital nodes usually play a key role in complex networks. Uncovering these nodes is an important task in protecting the network, especially when the network suffers intentional attack. Many existing methods have not fully integrated the node feature, interaction and state. In this article, we propose a novel method (GNNE) based on graph neural networks and information entropy. The method employs a Graph Convolutional Network (GCN) to learn the nodes' features, which are input into a Graph Attention Network (GAT) to obtain the influence factor of nodes, and the node influence factors are used to calculate the nodes' entropy to evaluate the node importance. The GNNE takes advantage of the GCN and GAT, with the GCN well extracting the nodes' features and the GAT aggregating the features of the nodes' neighbors by using the attention mechanism to assign different weights to the neighbors with different importance, and the nodes' entropy quantifies the nodes' state in the network. The proposed method is trained on a synthetic Barabasi-Albert network, and tested on six real datasets. Compared with eight traditional topology-based methods and four graph-machine-learning-based methods, the GNNE shows an advantage for the vital node identification in the perspectives of network attack and propagation.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Online Social Media Conversations on Controversial Topics Using AI Agents Calibrated on Real-World Data</title>
<link>https://arxiv.org/abs/2509.18985</link>
<guid>https://arxiv.org/abs/2509.18985</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, Large Language Models (LLMs), simulations, opinion modeling, user behavior 

Summary: 
Online social networks are valuable for studying individual and collective phenomena. Researchers use simulators with Large Language Models (LLMs) to make simulations more realistic by enabling agents to understand and generate natural language content. In this study, LLM-based agents were investigated in a simulated microblogging social network using profiles from real-world online conversations. The agents generated coherent content, formed connections, and displayed realistic social network structures. However, their content showed less variation in tone and toxicity compared to real data. The opinion dynamics of LLM agents evolved similarly to traditional mathematical models, with parameter configurations having minimal impact. The study highlights the potential of LLMs in simulating user behavior in social environments but also emphasizes the need for more precise cognitive modeling for accurate replication of human behavior. <br /><br />Summary: <div>
arXiv:2509.18985v1 Announce Type: new 
Abstract: Online social networks offer a valuable lens to analyze both individual and collective phenomena. Researchers often use simulators to explore controlled scenarios, and the integration of Large Language Models (LLMs) makes these simulations more realistic by enabling agents to understand and generate natural language content. In this work, we investigate the behavior of LLM-based agents in a simulated microblogging social network. We initialize agents with realistic profiles calibrated on real-world online conversations from the 2022 Italian political election and extend an existing simulator by introducing mechanisms for opinion modeling. We examine how LLM agents simulate online conversations, interact with others, and evolve their opinions under different scenarios. Our results show that LLM agents generate coherent content, form connections, and build a realistic social network structure. However, their generated content displays less heterogeneity in tone and toxicity compared to real data. We also find that LLM-based opinion dynamics evolve over time in ways similar to traditional mathematical models. Varying parameter configurations produces no significant changes, indicating that simulations require more careful cognitive modeling at initialization to replicate human behavior more faithfully. Overall, we demonstrate the potential of LLMs for simulating user behavior in social environments, while also identifying key challenges in capturing heterogeneity and complex dynamics.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability</title>
<link>https://arxiv.org/abs/2509.18376</link>
<guid>https://arxiv.org/abs/2509.18376</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, GNNs, node classification, global explanation methods, GnnXemplar <br />
<br />
Summary: GNNs are widely used for node classification but lack transparency in decision-making. Current global explanation methods are insufficient for large, complex graphs. GnnXemplar, inspired by Exemplar Theory, identifies representative nodes, exemplars, and generates natural language rules from their neighborhoods. Exemplar selection is optimized through reverse k-nearest neighbors, with rule derivation using large language models. Experimental results demonstrate GnnXemplar's superiority in fidelity, scalability, and interpretability, validated by a user study with 60 participants. <div>
arXiv:2509.18376v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods, those that characterize an entire class, remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space, exemplars, and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Collaborative Maintenance Falls Short: The Persistence of Retracted Papers on Wikipedia</title>
<link>https://arxiv.org/abs/2509.18403</link>
<guid>https://arxiv.org/abs/2509.18403</guid>
<content:encoded><![CDATA[
<div> Retraction, Wikipedia, citations, credibility, scholarly authority
Summary: 
The study investigates the handling of citations to retracted research on English Wikipedia. A dataset integrating Wikipedia revision histories with various metadata sources identified 1,181 citations of retracted papers. The analysis found that 71.6% of these citations were problematic, either added before retraction or introduced without mentioning the retracted status. These incorrect citations persist for a median of over 3.68 years, with signals of human attention leading to faster correction. Surprisingly, papers with higher academic citation counts took longer to be corrected. The study reveals gaps in citation-level repair on Wikipedia and suggests design directions to improve citation credibility at scale. This research contributes to understanding the sociotechnical vulnerability in maintaining citation credibility on collaborative platforms like Wikipedia.<br /><br />Summary: <div>
arXiv:2509.18403v1 Announce Type: cross 
Abstract: Wikipedia serves as a key infrastructure for public access to scientific knowledge, but it faces challenges in maintaining the credibility of cited sources, especially when scientific papers are retracted. This paper investigates how citations to retracted research are handled on English Wikipedia. We construct a novel dataset that integrates Wikipedia revision histories with metadata from Retraction Watch, Crossref, Altmetric, and OpenAlex, identifying 1,181 citations of retracted papers. We find that 71.6% of all citations analyzed are problematic. These are citations added before a paper's retraction, as well as the citations introduced after retraction without any in-text mention of the paper's retracted status. Our analysis reveals that these citations persist for a median of over 3.68 years (1,344 days). Through survival analysis, we find that signals of human attention are associated with a faster correction process. Unfortunately, a paper's established scholarly authority, a higher academic citation count, is associated with a slower time to correction. Our findings highlight how the Wikipedia community supports collaborative maintenance but leaves gaps in citation-level repair. We contribute to CSCW research by advancing our understanding of this sociotechnical vulnerability, which takes the form of a community coordination challenge, and by offering design directions to support citation credibility at scale.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Propaganda</title>
<link>https://arxiv.org/abs/2509.19147</link>
<guid>https://arxiv.org/abs/2509.19147</guid>
<content:encoded><![CDATA[
<div> generative propaganda, artificial intelligence, deepfakes, persuasion, deception <br />
Summary: <br />
Generative propaganda, which utilizes artificial intelligence to influence public opinion, was studied in Taiwan and India. The term "deepfakes" heavily influences defenses against generative propaganda. A taxonomy categorizing obvious versus hidden and promotional versus derogatory uses was developed. Deception was not the primary motivation in AI's use; instead, creators in India focused on persuasion rather than deceit, making AI's usage apparent to minimize risks. In Taiwan, deception was seen as part of distorted strategic narratives online. AI was employed for efficiency gains in communication and evasion of detection. Security researchers are advised to distinguish between different uses of AI, leverage social factors, and counter efficiency gains on a global scale. <br /> <div>
arXiv:2509.19147v1 Announce Type: cross 
Abstract: Generative propaganda is the use of generative artificial intelligence (AI) to shape public opinion. To characterize its use in real-world settings, we conducted interviews with defenders (e.g., factcheckers, journalists, officials) in Taiwan and creators (e.g., influencers, political consultants, advertisers) as well as defenders in India, centering two places characterized by high levels of online propaganda. The term "deepfakes", we find, exerts outsized discursive power in shaping defenders' expectations of misuse and, in turn, the interventions that are prioritized. To better characterize the space of generative propaganda, we develop a taxonomy that distinguishes between obvious versus hidden and promotional versus derogatory use. Deception was neither the main driver nor the main impact vector of AI's use; instead, Indian creators sought to persuade rather than to deceive, often making AI's use obvious in order to reduce legal and reputational risks, while Taiwan's defenders saw deception as a subset of broader efforts to distort the prevalence of strategic narratives online. AI was useful and used, however, in producing efficiency gains in communicating across languages and modes, and in evading human and algorithmic detection. Security researchers should reconsider threat models to clearly differentiate deepfakes from promotional and obvious uses, to complement and bolster the social factors that constrain misuse by internal actors, and to counter efficiency gains globally.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</title>
<link>https://arxiv.org/abs/2410.05401</link>
<guid>https://arxiv.org/abs/2410.05401</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change communication, microtargeting, large language models, demographic targeting, fairness

Summary: 
- The study analyzes microtargeting practices in climate change campaigns on social media, using large language models to examine Meta advertisements.
- It focuses on demographic targeting and fairness, evaluating the accuracy of predicting demographic targets and uncovering biases in model predictions.
- The study finds that young adults are targeted through activism and environmental themes, while women are engaged through caregiving and advocacy themes.
- Recurring patterns in messaging strategies tailored to different demographic groups are uncovered through thematic explanations generated by the models.
- The fairness analysis reveals biases in the classification of male audiences, emphasizing the need for more inclusive targeting methods in social media climate campaigns. 

<br /><br />Summary: <div>
arXiv:2410.05401v4 Announce Type: replace-cross 
Abstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Meta (previously known as Facebook) advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. Additionally, we conduct a comprehensive fairness analysis to uncover biases in model predictions. We assess disparities in accuracy and error rates across demographic groups using established fairness metrics such as Demographic Parity, Equal Opportunity, and Predictive Equality. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of male audiences. The analysis of thematic explanations uncovers recurring patterns in messaging strategies tailored to various demographic groups, while the fairness analysis underscores the need for more inclusive targeting methods. This study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUINTA: Reflexive Sensibility For Responsible AI Research and Data-Driven Processes</title>
<link>https://arxiv.org/abs/2509.16347</link>
<guid>https://arxiv.org/abs/2509.16347</guid>
<content:encoded><![CDATA[
<div> Intersectionality, AI, machine learning, fairness, critical reflexivity<br />
<br />
Intersectionality is increasingly recognized as essential in AI research for prioritizing fairness and addressing historical marginalization. This paper introduces the Quantitative Intersectional Data (QUINTA) framework, which integrates intersectionality into the AI/DS pipeline through critical reflexivity. By challenging conventional research habits, particularly in data-centric processes, QUINTA aims to identify and mitigate inadvertent marginalization caused by these practices. The framework emphasizes researcher reflexivity to highlight their power in creating and analyzing AI artifacts. A demonstration using the #metoo movement showcases the effectiveness of QUINTA. This approach provides practical guidance for researchers to incorporate intersectionality into their work, ultimately contributing to more just and equitable AI systems. 
<br /><br />Summary: <div>
arXiv:2509.16347v1 Announce Type: new 
Abstract: As the field of artificial intelligence (AI) and machine learning (ML) continues to prioritize fairness and the concern for historically marginalized communities, the importance of intersectionality in AI research has gained significant recognition. However, few studies provide practical guidance on how researchers can effectively incorporate intersectionality into critical praxis. In response, this paper presents a comprehensive framework grounded in critical reflexivity as intersectional praxis. Operationalizing intersectionality within the AI/DS (Artificial Intelligence/Data Science) pipeline, Quantitative Intersectional Data (QUINTA) is introduced as a methodological paradigm that challenges conventional and superficial research habits, particularly in data-centric processes, to identify and mitigate negative impacts such as the inadvertent marginalization caused by these practices. The framework centers researcher reflexivity to call attention to the AI researchers' power in creating and analyzing AI/DS artifacts through data-centric approaches. To illustrate the effectiveness of QUINTA, we provide a reflexive AI/DS researcher demonstration utilizing the \#metoo movement as a case study. Note: This paper was accepted as a poster presentation at Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO) Conference in 2023.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survivors, Complainers, and Borderliners: Upward Bias in Online Discussions of Academic Conference Reviews</title>
<link>https://arxiv.org/abs/2509.16831</link>
<guid>https://arxiv.org/abs/2509.16831</guid>
<content:encoded><![CDATA[
<div> Keywords: online discussion platforms, review scores, bias, academic conferences, peer review

Summary:
Survivors, complainers, and borderliners contribute to an upward bias in self-reported review scores on online discussion platforms for academic conference submissions. Survivors, authors of accepted papers, are more likely to share positive outcomes, while complainers, authors of high-scoring rejected papers, tend to voice complaints about the peer review process. Borderliners, authors with borderline scores, seek advice during the rebuttal period due to uncertainty. The study compares self-reported review score distributions from Zhihu and Reddit with those of all submissions, revealing significant discrepancies. Online discussions may not accurately represent the overall population score distribution. Information seekers should interpret discussions with caution, considering the biases introduced by the types of authors participating in these platforms. <div>
arXiv:2509.16831v1 Announce Type: new 
Abstract: Online discussion platforms, such as community Q&amp;A sites and forums, have become important hubs where academic conference authors share and seek information about the peer review process and outcomes. However, these discussions involve only a subset of all submissions, raising concerns about the representativeness of the self-reported review scores. In this paper, we conduct a systematic study comparing the review score distributions of self-reported submissions in online discussions (based on data collected from Zhihu and Reddit) with those of all submissions. We reveal a consistent upward bias: the score distribution of self-reported samples is shifted upward relative to the population score distribution, with this difference statistically significant in most cases. Our analysis identifies three distinct contributors to this bias: (1) survivors, authors of accepted papers who are more likely to share good results than those of rejected papers who tend to conceal bad ones; (2) complainers, authors of high-scoring rejected papers who are more likely to voice complaints about the peer review process or outcomes than those of low scores; and (3) borderliners, authors with borderline scores who face greater uncertainty prior to decision announcements and are more likely to seek advice during the rebuttal period. These findings have important implications for how information seekers should interpret online discussions of academic conference reviews.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hodge Decomposition for Urban Traffic Flow: Limits on Dense OD Graphs and Advantages on Road Networks - Los Angeles Case</title>
<link>https://arxiv.org/abs/2509.17203</link>
<guid>https://arxiv.org/abs/2509.17203</guid>
<content:encoded><![CDATA[
<div> Keywords: Hodge decomposition, urban traffic flow, graph representations, commute patterns, clustering

Summary:
This study examines Hodge decomposition, specifically HodgeRank, in the context of urban traffic flow on two graph representations: dense origin-destination graphs and road-segment networks. The research replicates the methodology of Aoki et al. and finds that on dense OD graphs, the curl and harmonic components are minimal, with the potential closely mirroring node divergence. This limits the usefulness of Hodge potentials in this context. In contrast, when analyzing a real road network dataset from downtown Los Angeles, distinct variations in potentials are observed, including morning/evening reversals reflective of commute patterns. The study evaluates the smoothness and discriminability of the data using local and global variances derived from the graph spectrum. Additionally, the research proposes flow-aware embeddings that incorporate topology, bidirectional volume, and net-flow asymmetry for clustering purposes. The provided code and preprocessing steps aim to enhance reproducibility of the findings. 

<br /><br />Summary: <div>
arXiv:2509.17203v1 Announce Type: new 
Abstract: I study Hodge decomposition (HodgeRank) for urban traffic flow on two graph representations: dense origin--destination (OD) graphs and road-segment networks. Reproducing the method of Aoki et al., we observe that on dense OD graphs the curl and harmonic components are negligible and the potential closely tracks node divergence, limiting the added value of Hodge potentials. In contrast, on a real road network (UTD19, downtown Los Angeles; 15-minute resolution), potentials differ substantially from divergence and exhibit clear morning/evening reversals consistent with commute patterns. We quantify smoothness and discriminability via local/global variances derived from the graph spectrum, and propose flow-aware embeddings that combine topology, bidirectional volume, and net-flow asymmetry for clustering. Code and preprocessing steps are provided to facilitate reproducibility.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limited Improvement of Connectivity in Scale-Free Networks by Increasing the Power-Law Exponent</title>
<link>https://arxiv.org/abs/2509.17652</link>
<guid>https://arxiv.org/abs/2509.17652</guid>
<content:encoded><![CDATA[
<div> Keywords: scale-free networks, connectivity, shortest loops, degree distributions, robustness <br />
Summary: <br />
The study explores the robustness of connectivity and the lengths of the shortest loops in scale-free networks with varying exponents ranging from 2.0 to 4.0. It is established that networks with smaller variance in degree distributions exhibit greater robustness and longer average lengths of the shortest loops. This phenomenon suggests the presence of large holes within the network structure. The findings indicate that manipulating degree distributions can potentially enhance the network's resilience against attacks. By understanding the relationship between degree distributions and network robustness, insights are gained on how to improve the overall stability and security of scale-free networks. <div>
arXiv:2509.17652v1 Announce Type: new 
Abstract: It has been well-known that many real networks are scale-free (SF) but extremely vulnerable against attacks. We investigate the robustness of connectivity and the lengths of the shortest loops in randomized SF networks with realistic exponents $2.0 < \gamma \leq 4.0$. We show that smaller variance of degree distributions leads to stronger robustness and longer average length of the shortest loops, which means the existing of large holes. These results will provide important insights toward enhancing the robustness by changing degree distributions.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs</title>
<link>https://arxiv.org/abs/2509.16564</link>
<guid>https://arxiv.org/abs/2509.16564</guid>
<content:encoded><![CDATA[
<div> Keywords: Misinformation, Evolution, Persona-conditioned, Large Language Model, Detection

Summary:
Misinformation is dynamic, evolving as it spreads to different audiences through language, framing, and moral perspectives. Existing detection methods often overlook this dynamic nature, assuming misinformation is static. This study introduces MPCG, a multi-round framework that simulates how misinformation evolves through iterative reinterpretation by agents with different ideological perspectives. By using a large language model (LLM) to generate persona-specific claims across multiple rounds, the researchers were able to capture the evolution of misinformation. Evaluation through human and LLM-based annotations, cognitive effort metrics, emotion analysis, clustering, feasibility, and classification revealed that generated claims aligned with persona-specific emotional and moral framing, required increased cognitive effort, and exhibited semantic drift over rounds while maintaining topical coherence. Results also showed a significant drop in performance of traditional misinformation detectors when faced with evolving misinformation. <div>
arXiv:2509.16564v1 Announce Type: cross 
Abstract: Misinformation evolves as it spreads, shifting in language, framing, and moral emphasis to adapt to new audiences. However, current misinformation detection approaches implicitly assume that misinformation is static. We introduce MPCG, a multi-round, persona-conditioned framework that simulates how claims are iteratively reinterpreted by agents with distinct ideological perspectives. Our approach uses an uncensored large language model (LLM) to generate persona-specific claims across multiple rounds, conditioning each generation on outputs from the previous round, enabling the study of misinformation evolution. We evaluate the generated claims through human and LLM-based annotations, cognitive effort metrics (readability, perplexity), emotion evocation metrics (sentiment analysis, morality), clustering, feasibility, and downstream classification. Results show strong agreement between human and GPT-4o-mini annotations, with higher divergence in fluency judgments. Generated claims require greater cognitive effort than the original claims and consistently reflect persona-aligned emotional and moral framing. Clustering and cosine similarity analyses confirm semantic drift across rounds while preserving topical coherence. Feasibility results show a 77% feasibility rate, confirming suitability for downstream tasks. Classification results reveal that commonly used misinformation detectors experience macro-F1 performance drops of up to 49.7%. The code is available at https://github.com/bcjr1997/MPCG
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Growing unlabeled networks</title>
<link>https://arxiv.org/abs/2509.17200</link>
<guid>https://arxiv.org/abs/2509.17200</guid>
<content:encoded><![CDATA[
<div> Keywords: growing networks, unlabeled trees, network symmetries, degree heterogeneity, leaf-based statistics

Summary:
In this study, models of growing unlabeled trees are introduced and analyzed, drawing parallels to well-known labeled growth models like uniform and preferential attachment. A theoretical framework is developed to examine the growth of unlabeled trees by focusing on leaf-based statistics. The analysis reveals that while some characteristics of labeled network growth are preserved, significant differences arise due to the symmetries among leaves in common neighborhoods. Degree heterogeneity is found to be amplified in unlabeled tree growth, with the extent of enhancement varying based on the specific growth dynamics employed. Mild enhancement is observed in uniform attachment, while preferential attachment leads to extreme enhancement. These results shed light on the impact of network symmetries on the evolution of unlabeled networks and offer insights that may extend beyond the realm of growing unlabeled trees. 

<br /><br />Summary: <div>
arXiv:2509.17200v1 Announce Type: cross 
Abstract: Models of growing networks are a central topic in network science. In these models, vertices are usually labeled by their arrival time, distinguishing even those node pairs whose structural roles are identical. In contrast, unlabeled networks encode only structure, so unlabeled growth rules must be defined in terms of structurally distinguishable outcomes; network symmetries therefore play a key role in unlabeled growth dynamics. Here, we introduce and study models of growing unlabeled trees, defined in analogy to widely-studied labeled growth models such as uniform and preferential attachment. We develop a theoretical formalism to analyze these trees via tracking their leaf-based statistics. We find that while many characteristics of labeled network growth are retained, numerous critical differences arise, caused primarily by symmetries among leaves in common neighborhoods. In particular, degree heterogeneity is enhanced, with the strength of this enhancement depending on details of growth dynamics: mild enhancement for uniform attachment, and extreme enhancement for preferential attachment. These results and the developed analytical formalism may be of interest beyond the setting of growing unlabeled trees.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking Patterns in Toxicity and Antisocial Behavior Over User Lifetimes on Large Social Media Platforms</title>
<link>https://arxiv.org/abs/2407.09365</link>
<guid>https://arxiv.org/abs/2407.09365</guid>
<content:encoded><![CDATA[
<div> toxicity, social media, Reddit, Wikipedia, behavior
Summary:
The paper examines toxic behavior on social media platforms Reddit and Wikipedia over a 14-year period, analyzing nearly 500 million comments. It identifies trends in user toxicity levels, noting a shift from decreasing toxicity to increasing toxicity over time. The study also highlights differences in toxic behavior between Reddit and Wikipedia users, with active Reddit users exhibiting more toxicity compared to inactive Wikipedia users. Additionally, the research explores toxicity in discussions around widely-shared content, drawing parallels between trends in content-related toxicity and user behavior. Overall, the analysis provides insights into the evolution of toxic behavior on social media platforms and sheds light on the dynamics of toxicity within online communities. <div>
arXiv:2407.09365v2 Announce Type: replace 
Abstract: An increasing amount of attention has been devoted to the problem of "toxic" or antisocial behavior on social media. In this paper we analyze such behavior at very large scales: we analyze toxicity over a 14-year time span on nearly 500 million comments from Reddit and Wikipedia, grounded in two different proxies for toxicity.
  At the individual level, we analyze users' toxicity levels over the course of their time on the site, and find a striking reversal in trends: both Reddit and Wikipedia users tended to become less toxic over their life cycles on the site in the early (pre-2013) history of the site, but more toxic over their life cycles in the later (post-2013) history of the site. We also find that toxicity on Reddit and Wikipedia differ in a key way, with the most toxic behavior on Reddit exhibited in aggregate by the most active users, and the most toxic behavior on Wikipedia exhibited in aggregate by the least active users. Finally, we consider the toxicity of discussion around widely-shared pieces of content, and find that the trends for toxicity in discussion about content bear interesting similarities with the trends for toxicity in discussion by users.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the dynamics of external and self-citations and their role in shaping scientific impact</title>
<link>https://arxiv.org/abs/2503.09811</link>
<guid>https://arxiv.org/abs/2503.09811</guid>
<content:encoded><![CDATA[
<div> preferential attachment rule, citation distribution, scientific impact, self-citations, bibliometric indices

Summary: 
The study investigates the influence of the preferential attachment rule (PAR) on scientific citation distribution. It found that approximately 70% of citations adhere to PAR in the aggregated dataset, but there is significant variability at the individual level, especially for external citations. Self-citations show different patterns, with only 20% following PAR. More citable authors are preferentially cited, while less-cited authors have more random patterns. The study highlights the impact of self-citations on bibliometric indices such as the h-index. It shows that self-citations behave differently from external citations, raising questions about the underlying mechanisms. These findings provide insights into citation behaviors, emphasizing the limitations of current approaches. <div>
arXiv:2503.09811v2 Announce Type: replace-cross 
Abstract: Understanding the mechanisms driving the distribution of scientific citations is a key challenge in assessing the scientific impact of authors. We investigate the influence of the preferential attachment rule (PAR) in this process by analysing individual citation events from the DBLP dataset and two Scopus-based datasets, enabling us to estimate the probability of citations being assigned preferentially. Our findings reveal that, for the aggregated dataset, PAR dominates the citation distribution process, with approximately 70% of citations adhering to this mechanism. However, analysis at the individual level shows significant variability, with some authors experiencing a greater prevalence of preferential citations, particularly in the context of external citations. In contrast, self-citations exhibit notably different behaviour, with only 20% following PAR. We also demonstrate that the prominence of PAR increases with an author's citability (average citations per paper), suggesting that more citable authors are preferentially cited, while less-cited authors experience more random citation patterns. Furthermore, we show that self-citations may influence bibliometric indices, such as the h-index. Our results confirm the distinct dynamics of self-citations compared to external citations, raising questions about the mechanisms driving self-citation patterns. These findings provide new insights into citation behaviours and highlight the limitations of existing approaches.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoliTok-DE: A Multimodal Dataset of Political TikToks and Deletions From Germany</title>
<link>https://arxiv.org/abs/2509.15860</link>
<guid>https://arxiv.org/abs/2509.15860</guid>
<content:encoded><![CDATA[
<div> Dataset, PoliTok-DE, TikTok, Germany, 2024 Saxony state election
Summary:
PoliTok-DE is a large-scale multimodal dataset of TikTok posts related to the 2024 Saxony state election in Germany. The dataset contains over 195,000 posts, with 17.3% deleted from the platform. Posts were identified using the TikTok research API and web scraping. The dataset can support research on intolerance, political communication, platform policies, and qualitative-quantitative multimodal research. A case study on the co-occurrence of intolerance and entertainment using an annotated subset is reported. The dataset of post IDs is available on Hugging Face, with code provided for full content access. Access to deleted content is restricted but can be requested for research purposes. <div>
arXiv:2509.15860v1 Announce Type: new 
Abstract: We present PoliTok-DE, a large-scale multimodal dataset (video, audio, images, text) of TikTok posts related to the 2024 Saxony state election in Germany. The corpus contains over 195,000 posts published between 01.07.2024 and 30.11.2024, of which over 18,000 (17.3%) were subsequently deleted from the platform. Posts were identified via the TikTok research API and complemented with web scraping to retrieve full multimodal media and metadata. PoliTok-DE supports computational social science across substantive and methodological agendas: substantive work on intolerance and political communication; methodological work on platform policies around deleted content and qualitative-quantitative multimodal research. To illustrate one possible analysis, we report a case study on the co-occurrence of intolerance and entertainment using an annotated subset. The dataset of post IDs is publicly available on Hugging Face, and full content can be hydrated with our provided code. Access to the deleted content is restricted, and can be requested for research purposes.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Community Notes: A Framework for Understanding and Building Crowdsourced Context Systems</title>
<link>https://arxiv.org/abs/2509.15434</link>
<guid>https://arxiv.org/abs/2509.15434</guid>
<content:encoded><![CDATA[
<div> Keywords: social media platforms, Crowdsourced Context Systems, literature review, theoretical model, design space <br />
<br />
Summary: 
The article examines the growing trend of social media platforms incorporating features that display crowdsourced context alongside posts, known as Crowdsourced Context Systems (CCS), as an alternative to traditional fact-checking. Through a systematic literature review and analysis of real-world CCS implementations, the authors develop a comprehensive framework comprising a theoretical model, a design space encompassing key aspects of CCS, and normative implications of different design choices. The theoretical model aids in conceptualizing CCS, while the design space covers aspects such as participation, curation, and transparency. The framework also addresses ethical considerations related to CCS implementation. The study provides a solid foundation for future research focusing on human-centered approaches to understanding and improving Crowdsourced Context Systems. <br /><br /> <div>
arXiv:2509.15434v1 Announce Type: cross 
Abstract: Social media platforms are increasingly developing features that display crowdsourced context alongside posts, modeled after X's Community Notes. These systems, which we term Crowdsourced Context Systems (CCS), have the potential to reshape our information ecosystem as major platforms embrace them as alternatives to top-down fact-checking. To deeply understand the features and implications of such systems, we perform a systematic literature review of existing CCS research and analyze several real-world CSS implementations. Based on our analysis, we develop a framework with three distinct components. First, we present a theoretical model to help conceptualize and define CCS. Second, we identify a design space encompassing six key aspects of CCS: participation, inputs, curation, presentation, platform treatment, and transparency. Third, we identify key normative implications of different CCS design and implementation choices. Our framework integrates these theoretical, design, and ethical perspectives to establish a foundation for future human-centered research on Crowdsourced Context Systems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies</title>
<link>https://arxiv.org/abs/2509.15481</link>
<guid>https://arxiv.org/abs/2509.15481</guid>
<content:encoded><![CDATA[
<div> forecasting, solar, neural network, GHI, renewable energy

Summary:
SolarCAST is a new model for accurate solar forecasting that predicts future global horizontal irradiance (GHI) at a target site using historical GHI data from a site and nearby stations. It addresses three classes of confounding factors using scalable neural components: observable synchronous variables, latent synchronous factors, and time-lagged influences. SolarCAST outperforms leading baselines and achieves a 25.9% error reduction over a top commercial forecaster. It offers a lightweight and practical solution for localized solar forecasting that does not require specialized hardware or heavy preprocessing. Key components include an embedding module for observable variables, a spatio-temporal graph neural network for latent factors, and a gated transformer for time-lagged influences. The model is generalizable across diverse geographical conditions and provides a high level of accuracy for solar energy management.<br /><br />Summary: <div>
arXiv:2509.15481v1 Announce Type: cross 
Abstract: Accurate solar forecasting underpins effective renewable energy management. We present SolarCAST, a causally informed model predicting future global horizontal irradiance (GHI) at a target site using only historical GHI from site X and nearby stations S - unlike prior work that relies on sky-camera or satellite imagery requiring specialized hardware and heavy preprocessing. To deliver high accuracy with only public sensor data, SolarCAST models three classes of confounding factors behind X-S correlations using scalable neural components: (i) observable synchronous variables (e.g., time of day, station identity), handled via an embedding module; (ii) latent synchronous factors (e.g., regional weather patterns), captured by a spatio-temporal graph neural network; and (iii) time-lagged influences (e.g., cloud movement across stations), modeled with a gated transformer that learns temporal shifts. It outperforms leading time-series and multimodal baselines across diverse geographical conditions, and achieves a 25.9% error reduction over the top commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and generalizable solution for localized solar forecasting.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis</title>
<link>https://arxiv.org/abs/2408.09613</link>
<guid>https://arxiv.org/abs/2408.09613</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, misinformation, social bots, Sina Weibo, dataset <br />
Summary: <br />
This paper investigates the relationship between social bots and misinformation on the Sina Weibo platform. A large dataset was created, containing both misinformation and verified information, as well as annotations for social bots and genuine accounts. The dataset was found to be comprehensive and of high quality. Analysis revealed that social bots play a significant role in spreading information, especially misinformation. Misinformation on similar topics has similar content, leading to echo chambers, which are further amplified by social bots. Additionally, social bots create similar content to manipulate public opinions. Overall, the study sheds light on how social bots contribute to the spread of misinformation on social media platforms like Sina Weibo. <br /> <div>
arXiv:2408.09613v3 Announce Type: replace 
Abstract: Social media platforms provide an ideal environment to spread misinformation, where social bots can accelerate the spread. This paper explores the interplay between social bots and misinformation on the Sina Weibo platform. We construct a large-scale dataset that includes annotations for both misinformation and social bots. From the misinformation perspective, the dataset is multimodal, containing 11,393 pieces of misinformation and 16,416 pieces of verified information. From the social bot perspective, this dataset contains 65,749 social bots and 345,886 genuine accounts, annotated using a weakly supervised annotator. Extensive experiments demonstrate the comprehensiveness of the dataset, the clear distinction between misinformation and real information, and the high quality of social bot annotations. Further analysis illustrates that: (i) social bots are deeply involved in information spread; (ii) misinformation with the same topics has similar content, providing the basis of echo chambers, and social bots would amplify this phenomenon; and (iii) social bots generate similar content aiming to manipulate public opinions.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-set spectral clustering of time-evolving networks using the supra-Laplacian</title>
<link>https://arxiv.org/abs/2409.11984</link>
<guid>https://arxiv.org/abs/2409.11984</guid>
<content:encoded><![CDATA[
<div> spectral techniques, dynamic Laplacian, spectral clustering, multiplex networks, Sparse EigenBasis Approximation<br />
Summary:<br />
The article presents a novel approach for analyzing complex time-varying networks by adapting spectral techniques from continuous-time dynamics on manifolds to the graph setting. By formulating an inflated dynamic Laplacian for graphs and developing a spectral theory, the authors introduce spectral clustering methods for both multiplex and non-multiplex networks. These methods, based on the eigenvectors of the inflated dynamic Laplacian and Sparse EigenBasis Approximation (SEBA) post-processing, outperform existing algorithms like the Leiden algorithm in spacetime and layer-by-layer analysis. The application of this approach to US senate voting data reveals insights into increasing polarization over time, highlighting the effectiveness of the proposed techniques in studying spatiotemporal phenomena in dynamic networks.<br /><br />Summary: <div>
arXiv:2409.11984v3 Announce Type: replace 
Abstract: Complex time-varying networks are prominent models for a wide variety of spatiotemporal phenomena. The functioning of networks depends crucially on their connectivity, yet reliable techniques for determining communities in spacetime networks remain elusive. We adapt successful spectral techniques from continuous-time dynamics on manifolds to the graph setting to fill this gap. We formulate an inflated dynamic Laplacian for graphs and develop a spectral theory to underpin the corresponding algorithmic realisations. We develop spectral clustering approaches for both multiplex and non-multiplex networks, based on the eigenvectors of the inflated dynamic Laplacian and specialised Sparse EigenBasis Approximation (SEBA) post-processing of these eigenvectors. We demonstrate that our approach can outperform the Leiden algorithm applied both in spacetime and layer-by-layer, and we analyse voting data from the US senate (where senators come and go as congresses evolve) to quantify increasing polarisation in time.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Silenced voices: social media polarization and women's marginalization in peacebuilding during the Northern Ethiopia War</title>
<link>https://arxiv.org/abs/2412.01549</link>
<guid>https://arxiv.org/abs/2412.01549</guid>
<content:encoded><![CDATA[
<div> social media, polarization, conflict, digital peacebuilding, women's participation
Summary:<br /><br />This study explores the impact of social media on conflict, with a focus on the Northern Ethiopia War. Using qualitative methods, it investigates how social media platforms contribute to polarization and violence. Women are particularly affected, facing displacement, exclusion from peace talks, and increased gender-based violence. Factors such as hostile online environments and the digital divide exacerbate these challenges. The study calls for media literacy programs, inclusive peacebuilding strategies, safe digital spaces, and gender-sensitive technological solutions. It highlights the need to address government-imposed internet shutdowns, unregulated social media, and low media literacy. By examining the intersection of technology, conflict, and gender in the Global South, this research offers valuable insights into leveraging digital platforms for sustainable peace and women's empowerment.<br /> <div>
arXiv:2412.01549v2 Announce Type: replace-cross 
Abstract: This study examines the complex relationship between social media, polarization, and conflict, with a focus on digital peacebuilding and women's participation, using the Northern Ethiopia War as a case study. Using a qualitative exploratory design through in-depth interviews, focus groups, and document analysis, the research examines how social media platforms influence conflict dynamics. The study applies and advances social identity, liberal feminist, and intersectionality theories to analyze social media's role in shaping conflict, mobilizing ethnic politics, and influencing women's involvement in peacebuilding. Findings reveal that the weaponization of social media intensifies polarization and offline violence. Women are disproportionately impacted through displacement, exclusion from peace negotiations, and heightened risks of gender-based violence, including rape. Contributing factors include hostile online environments, the digital divide, and prevailing socio-cultural norms. The study identifies significant gaps in leveraging digital platforms for sustainable peace, including government-imposed internet shutdowns, unregulated social media environments, and low media literacy. It recommends media literacy initiatives, inclusive peacebuilding frameworks, open and safe digital spaces, and gender-sensitive technological approaches. By centering digital technology, conflict, and gender in the Global South, this research contributes valuable insights to ongoing debates on ICT in conflict, peacebuilding, and women's empowerment.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining, Understanding, and Detecting Online Toxicity: Challenges and Machine Learning Approaches</title>
<link>https://arxiv.org/abs/2509.14264</link>
<guid>https://arxiv.org/abs/2509.14264</guid>
<content:encoded><![CDATA[
<div> Keywords: toxic content, online platforms, machine learning, dataset, content moderation

Summary:
The study explores the prevalence of toxic content on digital platforms, particularly during crises and elections. It reviews 140 publications on various types of online toxicity, including hate speech, offensive language, and harmful discourse. The research delves into the datasets used in past studies, highlighting challenges and machine learning techniques employed for detection. The dataset covers content in 32 languages across different topics. The study also investigates the potential of leveraging cross-platform data to enhance classification model performance. Recommendations and guidelines are provided for future research on online toxic content and the implementation of content moderation strategies for mitigation. Additionally, practical guidelines are outlined for reducing toxic content on online platforms. 

<br /><br />Summary: <div>
arXiv:2509.14264v1 Announce Type: cross 
Abstract: Online toxic content has grown into a pervasive phenomenon, intensifying during times of crisis, elections, and social unrest. A significant amount of research has been focused on detecting or analyzing toxic content using machine-learning approaches. The proliferation of toxic content across digital platforms has spurred extensive research into automated detection mechanisms, primarily driven by advances in machine learning and natural language processing. Overall, the present study represents the synthesis of 140 publications on different types of toxic content on digital platforms. We present a comprehensive overview of the datasets used in previous studies focusing on definitions, data sources, challenges, and machine learning approaches employed in detecting online toxicity, such as hate speech, offensive language, and harmful discourse. The dataset encompasses content in 32 languages, covering topics such as elections, spontaneous events, and crises. We examine the possibility of using existing cross-platform data to improve the performance of classification models. We present the recommendations and guidelines for new research on online toxic consent and the use of content moderation for mitigation. Finally, we present some practical guidelines to mitigate toxic content from online platforms.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Hate Speech Detection: Evaluating 38 Models from Traditional Methods to Transformers</title>
<link>https://arxiv.org/abs/2509.14266</link>
<guid>https://arxiv.org/abs/2509.14266</guid>
<content:encoded><![CDATA[
<div> transformer, hate speech detection, deep learning, machine learning, dataset characteristics

Summary:
- The study evaluates 38 models for hate speech detection on social media, focusing on accuracy and computational efficiency.
- Transformer architectures, especially RoBERTa, consistently outperform other models with accuracy and F1-scores above 90%.
- Hierarchical Attention Networks show promising results among deep learning approaches.
- Traditional methods like CatBoost and SVM remain competitive with F1-scores over 88% and lower computational costs.
- Balanced, moderately sized unprocessed datasets perform better than larger, preprocessed datasets, emphasizing the importance of dataset characteristics in hate speech detection systems.

<br /><br />Summary: <div>
arXiv:2509.14266v1 Announce Type: cross 
Abstract: The proliferation of hate speech on social media necessitates automated detection systems that balance accuracy with computational efficiency. This study evaluates 38 model configurations in detecting hate speech across datasets ranging from 6.5K to 451K samples. We analyze transformer architectures (e.g., BERT, RoBERTa, Distil-BERT), deep neural networks (e.g., CNN, LSTM, GRU, Hierarchical Attention Networks), and traditional machine learning methods (e.g., SVM, CatBoost, Random Forest). Our results show that transformers, particularly RoBERTa, consistently achieve superior performance with accuracy and F1-scores exceeding 90%. Among deep learning approaches, Hierarchical Attention Networks yield the best results, while traditional methods like CatBoost and SVM remain competitive, achieving F1-scores above 88% with significantly lower computational costs. Additionally, our analysis highlights the importance of dataset characteristics, with balanced, moderately sized unprocessed datasets outperforming larger, preprocessed datasets. These findings offer valuable insights for developing efficient and effective hate speech detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value Alignment of Social Media Ranking Algorithms</title>
<link>https://arxiv.org/abs/2509.14434</link>
<guid>https://arxiv.org/abs/2509.14434</guid>
<content:encoded><![CDATA[
<div> Schwartz's theory, Basic Human Values, social media feed, algorithmic approach, value alignment <br />
<br />
The paper explores how social media feed rankings, driven by engagement signals, can be biased towards individualistic values. A new approach is presented using Schwartz's theory of Basic Human Values to align social media feeds with users' desired values. By allowing users to express weights on specific values, the algorithm ranks feeds based on value expressions in posts, creating personalized feed rankings that reflect users' values. Controlled experiments demonstrated that users could use these controls to design feeds aligned with their personal values, diverging significantly from traditional engagement-driven feeds. The study highlights the importance of addressing biases in algorithmic feed rankings and provides a method for users to curate feeds that align with their values.<br /><br />Summary: <div>
arXiv:2509.14434v1 Announce Type: cross 
Abstract: While social media feed rankings are primarily driven by engagement signals rather than any explicit value system, the resulting algorithmic feeds are not value-neutral: engagement may prioritize specific individualistic values. This paper presents an approach for social media feed value alignment. We adopt Schwartz's theory of Basic Human Values -- a broad set of human values that articulates complementary and opposing values forming the building blocks of many cultures -- and we implement an algorithmic approach that models and then ranks feeds by expressions of Schwartz's values in social media posts. Our approach enables controls where users can express weights on their desired values, combining these weights and post value expressions into a ranking that respects users' articulated trade-offs. Through controlled experiments (N=141 and N=250), we demonstrate that users can use these controls to architect feeds reflecting their desired values. Across users, value-ranked feeds align with personal values, diverging substantially from existing engagement-driven feeds.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Secure Computation Offloading and Trajectory Optimization for Multi-UAV MEC Against Aerial Eavesdropper</title>
<link>https://arxiv.org/abs/2509.14883</link>
<guid>https://arxiv.org/abs/2509.14883</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV, multi-access edge computing, secure offloading, trajectory optimization, distributionally robust optimization

Summary:
The article discusses the challenges of secure offloading in UAV-based multi-access edge computing (MEC) networks, focusing on uncertainties in task computation and flexible trajectory optimizations. A robust problem formulation is proposed to minimize energy costs by optimizing connections, S-UAV trajectories, and offloading ratios while considering eavesdropping by malicious UAVs. The problem is solved using distributionally robust optimization and conditional value-at-risk mechanisms converted into second-order cone programming forms. Decoupling and successive convex approximation techniques are employed for S-UAV trajectory design. A global algorithm is developed to solve sub-problems in a block coordinate descent manner. Simulation results demonstrate the robustness of the proposed algorithms, with only a 2% increase in energy cost compared to the ideal scenario. <br /><br />Summary: <div>
arXiv:2509.14883v1 Announce Type: cross 
Abstract: The unmanned aerial vehicle (UAV) based multi-access edge computing (MEC) appears as a popular paradigm to reduce task processing latency. However, the secure offloading is an important issue when occurring aerial eavesdropping. Besides, the potential uncertainties in practical applications and flexible trajectory optimizations of UAVs pose formidable challenges for realizing robust offloading. In this paper, we consider the aerial secure MEC network including ground users, service unmanned aerial vehicles (S-UAVs) integrated with edge servers, and malicious UAVs overhearing transmission links. To deal with the task computation complexities, which are characterized as uncertainties, a robust problem is formulated with chance constraints. The energy cost is minimized by optimizing the connections, trajectories of S-UAVs and offloading ratios. Then, the proposed non-linear problem is tackled via the distributionally robust optimization and conditional value-at-risk mechanism, which is further transformed into the second order cone programming forms. Moreover, we decouple the reformulated problem and design the successive convex approximation for S-UAV trajectories. The global algorithm is designed to solve the sub-problems in a block coordinate decent manner. Finally, extensive simulations and numerical analyses are conducted to verify the robustness of the proposed algorithms, with just 2\% more energy cost compared with the ideal circumstance.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Media Bias and Polarization through the Lens of a Markov Switching Latent Space Network Model</title>
<link>https://arxiv.org/abs/2306.07939</link>
<guid>https://arxiv.org/abs/2306.07939</guid>
<content:encoded><![CDATA[
<div> measure, media bias, polarization, latent space model, social media<br />
Summary:<br />
In this study, a dynamic latent space model is proposed to analyze online audience-duplication networks and media bias in news outlets. The model incorporates both network data and text-based indicators to measure media bias, identifying polarization regimes using Markov-Switching dynamics. By analyzing online activity data from European news outlets in 2015 and 2016, the study reveals a positive correlation between the media slant measure and external sources of bias. Additionally, the model sheds light on polarization patterns in different countries. This research contributes to the understanding of media bias, polarization, and the statistical properties of latent space network models. <div>
arXiv:2306.07939v3 Announce Type: replace-cross 
Abstract: News outlets are now more than ever incentivized to provide their audience with slanted news, while the intrinsic homophilic nature of online social media may exacerbate polarized opinions. Here, we propose a new dynamic latent space model for time-varying online audience-duplication networks, which exploits social media content to conduct inference on media bias and polarization of news outlets. We contribute to the literature in several directions: 1) Our model provides a novel measure of media bias that combines information from both network data and text-based indicators; 2) we endow our model with Markov-Switching dynamics to capture polarization regimes while maintaining a parsimonious specification; 3) we contribute to the literature on the statistical properties of latent space network models. The proposed model is applied to a set of data on the online activity of national and local news outlets from four European countries in the years 2015 and 2016. We find evidence of a strong positive correlation between our media slant measure and a well-grounded external source of media bias. In addition, we provide insight into the polarization regimes across the four countries considered.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order Network phenomena of cascading failures in resilient cities</title>
<link>https://arxiv.org/abs/2509.13808</link>
<guid>https://arxiv.org/abs/2509.13808</guid>
<content:encoded><![CDATA[
<div> Keywords: urban resilience, multimodal transport networks, higher-order network theory, cascading failures, dynamic functional resilience

Summary:
This study explores the threats to modern urban resilience posed by cascading failures in multimodal transport networks. By combining higher-order network theory with empirical evidence from a real-world transport network, the researchers find that network integration enhances static robustness metrics but also creates pathways for catastrophic cascades. The disconnect between static network structure and dynamic functional failure is a major challenge, as conventional centrality metrics provide poor predictors of a system's resilience. The findings suggest a need for a paradigm shift towards dynamic models to design and manage truly resilient urban systems. This research highlights the limitations of static analysis and emphasizes the importance of understanding the dynamic nature of urban transport networks in ensuring their resilience.<br /><br />Summary: <div>
arXiv:2509.13808v1 Announce Type: new 
Abstract: Modern urban resilience is threatened by cascading failures in multimodal transport networks, where localized shocks trigger widespread paralysis. Existing models, limited by their focus on pairwise interactions, often underestimate this systemic risk. To address this, we introduce a framework that confronts higher-order network theory with empirical evidence from a large-scale, real-world multimodal transport network. Our findings confirm a fundamental duality: network integration enhances static robustness metrics but simultaneously creates the structural pathways for catastrophic cascades. Crucially, we uncover the source of this paradox: a profound disconnect between static network structure and dynamic functional failure. We provide strong evidence that metrics derived from the network's static blueprint-encompassing both conventional low-order centrality and novel higher-order structural analyses-are fundamentally disconnected from and thus poor predictors of a system's dynamic functional resilience. This result highlights the inherent limitations of static analysis and underscores the need for a paradigm shift towards dynamic models to design and manage truly resilient urban systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outperforming Dijkstra on Sparse Graphs: The Lightning Network Use Case</title>
<link>https://arxiv.org/abs/2509.13448</link>
<guid>https://arxiv.org/abs/2509.13448</guid>
<content:encoded><![CDATA[
<div> BMSSP, Dijkstra, Rust, Lightning Network, Routing <br />
Summary: 
Efficient routing in payment channel networks (PCNs) like the Lightning Network is crucial, with Dijkstra algorithms commonly used for finding payment paths. The BMSSP algorithm is theoretically faster than Dijkstra on sparse graphs, but in an implementation on Rust using real Lightning Network topology data, BMSSP did not show significant performance improvements over Dijkstra. The results suggest that current implementations of BMSSP may not achieve the theoretical speedups, likely due to implementation and constant factor overheads. Despite not exhibiting the expected acceleration in practice, BMSSP still shows potential for enhancing Lightning Network routing efficiency. Future optimizations to PCN pathfinding algorithms could benefit from these findings. <br /><br />Summary: <div>
arXiv:2509.13448v1 Announce Type: cross 
Abstract: Efficient routing is critical for payment channel networks (PCNs) such as the Lightning Network (LN), where most clients currently rely on Dijkstra-based algorithms for payment pathfinding. While Dijkstra's algorithm has long been regarded as optimal on sparse graphs, recent theoretical work challenges this view. The new Bounded Multi-Source Shortest Path (BMSSP) algorithm by Duan et al. theoretically achieves $O(m~log^{2/3}~n)$ runtime, which is asymptotically faster than Dijkstra's $O(m + n~log~n)$ on sparse directed graphs. In this paper, we implement BMSSP on Rust and compare its performance against Dijkstra's using real LN topology data. Our evaluation, based on multiple randomized trials and statistical tests, shows that current implementations of BMSSP do not significantly outperform Dijkstra's in practice, and speedups are smaller than what theory predicts, possibly due to implementation and constant factor overheads. These results provide the first empirical evidence of BMSSP's potential to accelerate LN routing and inform future optimizations of PCN pathfinding algorithms.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate Trust Evaluation for Effective Operation of Social IoT Systems via Hypergraph-Enabled Self-Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2509.12240</link>
<guid>https://arxiv.org/abs/2509.12240</guid>
<content:encoded><![CDATA[
<div> trust, Social Internet-of-Things, hypergraph, self-supervised contrastive learning, device collaboration

Summary:
The paper introduces a new hypergraph-enabled self-supervised contrastive learning (HSCL) method for determining trust values in Social Internet-of-Things (IoT) systems. It addresses the challenge of calculating trust between devices based on complex social attributes by utilizing hypergraphs to represent high-order relationships. Hypergraph augmentation enhances the social hypergraph semantics, and a parameter-sharing hypergraph neural network fuses the relationships nonlinearly. A self-supervised contrastive learning approach generates meaningful device embeddings by comparing devices, hyperedges, and device-to-hyperedge relationships. Trust values are then calculated using these embeddings to distinguish trusted and untrusted nodes and identify the most trusted node. Experimental results demonstrate the superior performance of the proposed HSCL method compared to baseline algorithms. <br /><br />Summary: <div>
arXiv:2509.12240v1 Announce Type: new 
Abstract: Social Internet-of-Things (IoT) enhances collaboration between devices by endowing IoT systems with social attributes. However, calculating trust between devices based on complex and dynamic social attributes-similar to trust formation mechanisms in human society-poses a significant challenge. To address this issue, this paper presents a new hypergraph-enabled self-supervised contrastive learning (HSCL) method to accurately determine trust values between devices. To implement the proposed HSCL, hypergraphs are first used to discover and represent high-order relationships based on social attributes. Hypergraph augmentation is then applied to enhance the semantics of the generated social hypergraph, followed by the use of a parameter-sharing hypergraph neural network to nonlinearly fuse the high-order social relationships. Additionally, a self-supervised contrastive learning method is utilized to obtain meaningful device embeddings by conducting comparisons among devices, hyperedges, and device-to-hyperedge relationships. Finally, trust values between devices are calculated based on device embeddings that encapsulate high-order social relationships. Extensive experiments reveal that the proposed HSCL method outperforms baseline algorithms in effectively distinguishing between trusted and untrusted nodes and identifying the most trusted node.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Voices of Survival: From Social Media Disclosures to Support Provisions for Domestic Violence Victims</title>
<link>https://arxiv.org/abs/2509.12288</link>
<guid>https://arxiv.org/abs/2509.12288</guid>
<content:encoded><![CDATA[
<div> Keywords: Domestic Violence, Social Media, Self-disclosure, Support-seeking, Computational framework

Summary: 
This study introduces a computational framework for analyzing Domestic Violence (DV) self-disclosure and support-seeking behavior on social media platforms. The framework includes components such as self-disclosure detection, post clustering, topic summarization, and support extraction to understand how victims seek support online. By collecting data from social media communities, the framework aims to provide a deeper insight into DV self-disclosure patterns and the types of support victims receive online. The findings from this research not only contribute to the understanding of DV self-disclosure and online support mechanisms but also pave the way for victim-centered digital interventions. <div>
arXiv:2509.12288v1 Announce Type: new 
Abstract: Domestic Violence (DV) is a pervasive public health problem characterized by patterns of coercive and abusive behavior within intimate relationships. With the rise of social media as a key outlet for DV victims to disclose their experiences, online self-disclosure has emerged as a critical yet underexplored avenue for support-seeking. In addition, existing research lacks a comprehensive and nuanced understanding of DV self-disclosure, support provisions, and their connections. To address these gaps, this study proposes a novel computational framework for modeling DV support-seeking behavior alongside community support mechanisms. The framework consists of four key components: self-disclosure detection, post clustering, topic summarization, and support extraction and mapping. We implement and evaluate the framework with data collected from relevant social media communities. Our findings not only advance existing knowledge on DV self-disclosure and online support provisions but also enable victim-centered digital interventions.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Information Loss in Network Embeddings</title>
<link>https://arxiv.org/abs/2509.12396</link>
<guid>https://arxiv.org/abs/2509.12396</guid>
<content:encoded><![CDATA[
<div> algorithm, network embedding, graph generative model, community structure, link prediction

Summary: 
The article examines a basic algorithm for network embedding and explores the extent to which the learned representation captures the graph's generative model. It identifies scenarios where the embedding may not fully retain the original information, leading to loss of density information. The study also delves into the equivalence classes of graphons that yield the same embedding, emphasizing the preservation of community structure within these classes. Furthermore, the research highlights the limitations of using embeddings alone for link prediction, showcasing how naive predictions can introduce structural biases by disproportionately adding edges. By addressing implications for community detection and link prediction, the findings shed light on the challenges and potential pitfalls associated with network embedding techniques. <br /><br />Summary: <div>
arXiv:2509.12396v1 Announce Type: new 
Abstract: We analyze a simple algorithm for network embedding, explicitly characterizing conditions under which the learned representation encodes the graph's generative model fully, partially, or not at all. In cases where the embedding loses some information (i.e., is not invertible), we describe the equivalence classes of graphons that map to the same embedding, finding that these classes preserve community structure but lose substantial density information. Finally, we show implications for community detection and link prediction. Our results suggest strong limitations on the effectiveness of link prediction based on embeddings alone, and we show common conditions under which naive link prediction adds edges in a disproportionate manner that can either mitigate or exacerbate structural biases.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Driven Network Data for Smart Cities</title>
<link>https://arxiv.org/abs/2509.12403</link>
<guid>https://arxiv.org/abs/2509.12403</guid>
<content:encoded><![CDATA[
<div> Keywords: smart city, data-driven decision making, secure data sharing, privacy awareness, Wi-Fi network data <br />
Summary: 
This paper explores the importance of secure data sharing in developing smart cities. It highlights the critical role of network data, particularly from public Wi-Fi infrastructures, in enabling connected infrastructure and smart mobility. The focus is on safeguarding all attributes in real Wi-Fi network data to ensure the privacy and security of sensitive information. The methodology developed involves collaboration with legal experts, data custodians, and privacy specialists to ensure high-quality data. The study also addresses the integration of legal considerations for secure data sharing, promoting data-driven innovation while maintaining privacy awareness. The approach has been tested in a real scenario, demonstrating its practical application in enhancing the quality of Wi-Fi networks and advancing smart city initiatives. <br /><br />Summary: <div>
arXiv:2509.12403v1 Announce Type: new 
Abstract: A smart city is essential for sustainable urban development. In addition to citizen engagement, a smart city enables connected infrastructure, data-driven decision making and smart mobility. For most of these features, network data plays a critical role, particularly from public Wi-Fi infrastructures, where cities can benefit from optimized services such as public transport management and the safety and efficiency of large events. One of the biggest concerns in developing a smart city is using secure and private data. This is particularly relevant in the case of Wi-Fi network data, where sensitive information can be collected. This paper specifically addresses the problem of sharing secure data to enhance the quality of the Wi-Fi network in a city. Despite the high importance of this type of data, related work focuses on improving the safety of mobility patterns, targeting only the protection of MAC addresses. On the opposite side, we provide a practical methodology for safeguarding all attributes in real Wi-Fi network data. This study was developed in collaboration with a multidisciplinary team of legal experts, data custodians and technical privacy specialists, resulting in high-quality data. On top of that, we show how to integrate the legal considerations for secure data sharing. Our approach promotes data-driven innovation and privacy awareness in the context of smart city initiatives, which have been tested in a real scenario.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ketto and the Science of Giving: A Data-Driven Investigation of Crowdfunding for India</title>
<link>https://arxiv.org/abs/2509.12616</link>
<guid>https://arxiv.org/abs/2509.12616</guid>
<content:encoded><![CDATA[
<div> Ketto, crowdfunding platform, social causes, India, campaigns<br />
<br />
Summary: 
The research investigates the Ketto crowdfunding platform in India, focusing on medical campaigns. It analyzes 119,493 campaigns and explores factors influencing campaign success. The study finds that medical campaigns address chronic health conditions but have low success rates. Most campaigns originate from populous states and major cities. Factors like online engagement, campaign duration, and updates positively impact funds raised. The research highlights the significance of understanding crowdfunding dynamics for community-driven needs in India.<br /> <div>
arXiv:2509.12616v1 Announce Type: new 
Abstract: The main goal of this paper is to investigate an up and coming crowdfunding platform used to raise funds for social causes in India called Ketto. Despite the growing usage of this platform, there is insufficient understanding in terms of why users choose this platform when there are other popular platforms such as GoFundMe. Using a dataset comprising of 119,493 Ketto campaigns, our research conducts an in-depth investigation into different aspects of how the campaigns on Ketto work with a specific focus on medical campaigns, which make up the largest percentage of social causes in the dataset. We also perform predictive modeling to identify the factors that contribute to the success of campaigns on this platform. We use several features such as the campaign metadata, description, geolocation, donor behaviors, and campaign-related features to learn about the platform and its components. Our results suggest that majority of the campaigns for medical causes seek funds to address chronic health conditions, yet medical campaigns have the least success rate. Most of the campaigns originate from the most populous states and major metropolitan cities in India. Our analysis also indicates that factors such as online engagement on the platform in terms of the number of comments, duration of the campaign, and frequent updates on a campaign positively influence the funds being raised. Overall, this preliminary work sheds light on the importance of investigating various dynamics around crowdfunding for India-focused community-driven needs.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pressure-Based Diffusion Model for Influence Maximization on Social Networks</title>
<link>https://arxiv.org/abs/2509.12822</link>
<guid>https://arxiv.org/abs/2509.12822</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion model, social network, influence maximization, Pressure Threshold model, CyNetDiff

Summary: 
The paper introduces a new diffusion model called the Pressure Threshold model (PT) for simulating the spread of influence in social networks. This model extends the Linear Threshold Model by considering a node's outgoing influence proportional to the influence it receives from activated neighbors. The Influence Maximization (IM) problem, which involves selecting seed nodes for maximum graph coverage, is addressed in the context of the PT Model. Experiments conducted on real-world networks using the CyNetDiff Python library show that the PT Model selects different seed nodes compared to the LT Model. Additionally, the analysis reveals that densely connected networks magnify pressure effects more significantly than sparse networks. Overall, the PT Model offers a novel approach to understanding influence propagation in social networks and seed node selection for maximizing influence spread. 

<br /><br />Summary: <div>
arXiv:2509.12822v1 Announce Type: new 
Abstract: In many real-world scenarios, an individual's local social network carries significant influence over the opinions they form and subsequently propagate to others. In this paper, we propose a novel diffusion model -- the Pressure Threshold model (PT) -- for dynamically simulating the spread of influence through a social network. This new model extends the popular Linear Threshold Model (LT) by adjusting a node's outgoing influence proportional to the influence it receives from its activated neighbors. We address the Influence Maximization (IM) problem, which involves selecting the most effective seed nodes to achieve maximal graph coverage after a diffusion process, and how the problem manifests with the PT Model. Experiments conducted on real-world networks, facilitated by enhancements to the open-source network-diffusion Python library, CyNetDiff, demonstrate unique seed node selection for the PT Model when compared to the LT Model. Moreover, analyses demonstrate that densely connected networks amplify pressure effects more significantly than sparse networks.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Podcasts as a Medium for Participation in Collective Action: A Case Study of Black Lives Matter</title>
<link>https://arxiv.org/abs/2509.13197</link>
<guid>https://arxiv.org/abs/2509.13197</guid>
<content:encoded><![CDATA[
<div> Keywords: collective action, podcast discussions, Black Lives Matter, emotional dimensions, activism

Summary: 
This study examines how participation in collective action is expressed in podcast discussions, focusing on the Black Lives Matter movement. Using podcast transcripts from the Structured Podcast Research Corpus, the study analyzed spoken language expressions related to problem-solving, calls-to-action, intentions, and execution in discussions about racial justice following key BLM events in 2020. Emotional analysis revealed eight key emotions associated with various stages of activism, with positive emotions dominant during calls-to-action, intention-setting, and action-taking. Surprisingly, negative emotions were linked to collective action, contradicting theoretical expectations. The findings suggest that emotional framing varies depending on the stage of activism and the format of the discussion. This research enhances our understanding of how activism is communicated in spoken digital discourse and the role of emotions in shaping activism narratives. 

<br /><br />Summary: <div>
arXiv:2509.13197v1 Announce Type: new 
Abstract: We study how participation in collective action is articulated in podcast discussions, using the Black Lives Matter (BLM) movement as a case study. While research on collective action discourse has primarily focused on text-based content, this study takes a first step toward analyzing audio formats by using podcast transcripts. Using the Structured Podcast Research Corpus (SPoRC), we investigated spoken language expressions of participation in collective action, categorized as problem-solution, call-to-action, intention, and execution. We identified podcast episodes discussing racial justice after important BLM-related events in May and June of 2020, and extracted participatory statements using a layered framework adapted from prior work on social media. We examined the emotional dimensions of these statements, detecting eight key emotions and their association with varying stages of activism. We found that emotional profiles vary by stage, with different positive emotions standing out during calls-to-action, intention, and execution. We detected negative associations between collective action and negative emotions, contrary to theoretical expectations. Our work contributes to a better understanding of how activism is expressed in spoken digital discourse and how emotional framing may depend on the format of the discussion.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending the BEND Framework to Webgraphs</title>
<link>https://arxiv.org/abs/2509.13212</link>
<guid>https://arxiv.org/abs/2509.13212</guid>
<content:encoded><![CDATA[
<div> SEO-boosted websites, webgraph manipulation, BEND framework, quantitative metrics, Kremlin-aligned

Summary:<br /><br />This article introduces a novel approach to quantitatively analyze attempts to manipulate webgraphs using SEO tactics. The BEND framework is utilized to characterize maneuvers within webgraph information environments, offering standardized metrics for assessment. Two small webgraphs featuring SEO-boosted Kremlin-aligned sites are analyzed using the proposed Webgraph BEND metrics, demonstrating their efficacy in improving BEND scores and characterizing webgraph environments. The study highlights the importance of having shared quantitative metrics for assessing and understanding webgraph manipulation tactics, providing analysts with a systematic tool for evaluating and identifying such activities. This framework can help researchers identify and address manipulative practices in webgraphs, contributing to a more transparent and trustworthy online information landscape. <div>
arXiv:2509.13212v1 Announce Type: new 
Abstract: Attempts to manipulate webgraphs can have many downstream impacts, but analysts lack shared quantitative metrics to characterize actions taken to manipulate information environments at this level. We demonstrate how the BEND framework can be used to characterize attempts to manipulate webgraph information environments, and propose quantitative metrics for BEND community maneuvers. We demonstrate the face validity of our proposed Webgraph BEND metrics by using them to characterize two small web-graphs containing SEO-boosted Kremlin-aligned websites. We demonstrate how our proposed metrics improve BEND scores in webgraph settings and demonstrate the usefulness of our metrics in characterizing webgraph information environments. These metrics offer analysts a systematic and standardized way to characterize attempts to manipulate webgraphs using common Search Engine Optimization tactics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Unbiased Sampling of Networks with Given Expected Degrees and Strengths</title>
<link>https://arxiv.org/abs/2509.13230</link>
<guid>https://arxiv.org/abs/2509.13230</guid>
<content:encoded><![CDATA[
<div> Keywords: configuration model, network structure, Chung-Lu model, maximum entropy principle, sampling algorithms 

Summary:
Efficient sampling algorithms have been proposed for maximum entropy-based configuration models, addressing the oversampling issue in the Chung-Lu model. The Chung-Lu model, commonly used for network structure analysis, tends to oversample edges between nodes with high degrees, leading to inaccurate statistical conclusions. By adapting the Miller-Hagberg algorithm, the proposed algorithms significantly reduce computational costs, enabling fast generation of theoretically rigorous configuration models. Evaluation on 103 empirical networks showed a speedup of 10-1000 times, making accurate configuration models practical for network structure analysis. The adoption of these efficient algorithms contributes to a more precise understanding of network structures and facilitates unbiased statistical assessments. 
<br /><br />Summary: <div>
arXiv:2509.13230v1 Announce Type: new 
Abstract: The configuration model is a cornerstone of statistical assessment of network structure. While the Chung-Lu model is among the most widely used configuration models, it systematically oversamples edges between large-degree nodes, leading to inaccurate statistical conclusions. Although the maximum entropy principle offers unbiased configuration models, its high computational cost has hindered widespread adoption, making the Chung-Lu model an inaccurate yet persistently practical choice. Here, we propose fast and efficient sampling algorithms for the max-entropy-based models by adapting the Miller-Hagberg algorithm. Evaluation on 103 empirical networks demonstrates 10-1000 times speedup, making theoretically rigorous configuration models practical and contributing to a more accurate understanding of network structure.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE</title>
<link>https://arxiv.org/abs/2509.12255</link>
<guid>https://arxiv.org/abs/2509.12255</guid>
<content:encoded><![CDATA[
<div> GraphSAGE, Graph Neural Network, transactional networks, banking, embeddings<br />
Summary:<br />
This paper introduces GraphSAGE, a Graph Neural Network framework, for analysing complex transactional networks in the banking industry. Unlike traditional methods, GraphSAGE is scalable and can generalize to unseen nodes in large networks. The study constructs a transaction network using anonymised customer and merchant transactions and trains a GraphSAGE model to generate node embeddings. The embeddings reveal interpretable clusters aligned with geographic and demographic attributes. The utility of these embeddings is demonstrated in a money mule detection model, improving the prioritization of high-risk accounts. The adaptability of GraphSAGE to banking-scale networks is emphasized, showcasing its inductive capability, scalability, and interpretability. Financial organizations can leverage graph machine learning for actionable insights in transactional ecosystems based on the blueprint provided in this study. <br /><br /> <div>
arXiv:2509.12255v1 Announce Type: cross 
Abstract: Financial institutions increasingly require scalable tools to analyse complex transactional networks, yet traditional graph embedding methods struggle with dynamic, real-world banking data. This paper demonstrates the practical application of GraphSAGE, an inductive Graph Neural Network framework, to non-bipartite heterogeneous transaction networks within a banking context. Unlike transductive approaches, GraphSAGE scales well to large networks and can generalise to unseen nodes which is critical for institutions working with temporally evolving transactional data. We construct a transaction network using anonymised customer and merchant transactions and train a GraphSAGE model to generate node embeddings. Our exploratory work on the embeddings reveals interpretable clusters aligned with geographic and demographic attributes. Additionally, we illustrate their utility in downstream classification tasks by applying them to a money mule detection model where using these embeddings improves the prioritisation of high-risk accounts. Beyond fraud detection, our work highlights the adaptability of this framework to banking-scale networks, emphasising its inductive capability, scalability, and interpretability. This study provides a blueprint for financial organisations to harness graph machine learning for actionable insights in transactional ecosystems.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology and Fragility of European High-Voltage Networks: A Cross-Country Comparative Analysis</title>
<link>https://arxiv.org/abs/2509.12900</link>
<guid>https://arxiv.org/abs/2509.12900</guid>
<content:encoded><![CDATA[
<div> Topological models, European countries, high-voltage grids, node degree distributions, network tolerance<br />
Summary:<br />
- Reliable electricity supply relies on high-voltage grid infrastructure in 15 European countries.
- Structural diversity impacts system vulnerability.
- Harmonized topological models reveal varying decay rates in node degree distributions.
- Network resilience is determined by the rate of decay, identifying systems prone to disruptions.
- Sensitivity to infrastructure layers affects numerical boundaries of resilience.   <br /> <div>
arXiv:2509.12900v1 Announce Type: cross 
Abstract: Reliable electricity supply depends on the seamless operation of high-voltage grid infrastructure spanning both transmission and sub-transmission levels. Beneath this apparent uniformity lies a striking structural diversity, which leaves a clear imprint on system vulnerability. In this paper, we present harmonized topological models of the high-voltage grids of 15 European countries, integrating all elements at voltage levels above 110 kV. Topological analysis of these networks reveals a simple yet robust pattern: node degree distributions consistently follow an exponential decay, but the rate of decay varies significantly across countries. Through a detailed and systematic evaluation of network tolerance to node and edge removals, we show that the decay rate delineates the boundary between systems that are more resilient to failures and those that are prone to large-scale disruptions. Furthermore, we demonstrate that this numerical boundary is highly sensitive to which layers of the infrastructure are included in the models. To our knowledge, this study provides the first quantitative cross-country comparison of 15 European high-voltage networks, linking topological properties with vulnerability characteristics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sublinear-Time Algorithms for Diagonally Dominant Systems and Applications to the Friedkin-Johnsen Model</title>
<link>https://arxiv.org/abs/2509.13112</link>
<guid>https://arxiv.org/abs/2509.13112</guid>
<content:encoded><![CDATA[
<div> diagonally dominant matrices, sublinear-time algorithms, randomized algorithms, linear systems, Friedkin-Johnsen model  
Summary:  
- The article explores sublinear-time algorithms for solving linear systems with diagonally dominant matrices.  
- The algorithms return an estimate of the solution with controlled additive error, needing minimal input reading.  
- A particular algorithm for additive error $\varepsilon$ achieves optimal time complexity with linear dependence on the maximum diagonal entry.  
- The approach extends beyond symmetric matrices to include general diagonally dominant matrices.  
- The methodology is based on analyzing a probabilistic recurrence inherent in the solution, leading to improved opinion estimation algorithms in the Friedkin-Johnsen model.  

<br /><br />Summary: <div>
arXiv:2509.13112v1 Announce Type: cross 
Abstract: We study sublinear-time algorithms for solving linear systems $Sz = b$, where $S$ is a diagonally dominant matrix, i.e., $|S_{ii}| \geq \delta + \sum_{j \ne i} |S_{ij}|$ for all $i \in [n]$, for some $\delta \geq 0$. We present randomized algorithms that, for any $u \in [n]$, return an estimate $z_u$ of $z^*_u$ with additive error $\varepsilon$ or $\varepsilon \lVert z^*\rVert_\infty$, where $z^*$ is some solution to $Sz^* = b$, and the algorithm only needs to read a small portion of the input $S$ and $b$. For example, when the additive error is $\varepsilon$ and assuming $\delta>0$, we give an algorithm that runs in time $O\left( \frac{\|b\|_\infty^2 S_{\max}}{\delta^3 \varepsilon^2} \log \frac{\| b \|_\infty}{\delta \varepsilon} \right)$, where $S_{\max} = \max_{i \in [n]} |S_{ii}|$. We also prove a matching lower bound, showing that the linear dependence on $S_{\max}$ is optimal. Unlike previous sublinear-time algorithms, which apply only to symmetric diagonally dominant matrices with non-negative diagonal entries, our algorithm works for general strictly diagonally dominant matrices ($\delta > 0$) and a broader class of non-strictly diagonally dominant matrices $(\delta = 0)$. Our approach is based on analyzing a simple probabilistic recurrence satisfied by the solution. As an application, we obtain an improved sublinear-time algorithm for opinion estimation in the Friedkin--Johnsen model.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Top-r Influential Community Search in Bipartite Graphs</title>
<link>https://arxiv.org/abs/2412.06216</link>
<guid>https://arxiv.org/abs/2412.06216</guid>
<content:encoded><![CDATA[
<div> Keywords: community search, bipartite graphs, influential community detection, $(\alpha,\beta)$-influential community model, algorithm efficiency

Summary:
In the study of community search on bipartite graphs, a new $(\alpha,\beta)$-influential community model is proposed to better reflect true community influence by considering average vertex weights from both layers. This model enhances the accuracy of identifying top-$r$ communities, which is known to be NP-hard. An exact recursive algorithm is developed, utilizing a slim tree structure and upper-bound techniques for efficiency. Additionally, a greedy approximate algorithm with optimized complexity is introduced, incorporating a pruning strategy. Experiment results on 10 real-world graphs validate the effectiveness and efficiency of the proposed algorithms in identifying influential communities. This research contributes to the advancement of community detection methods on bipartite graphs by offering a more comprehensive influence measure and improved computational efficiency. 

Summary: <div>
arXiv:2412.06216v3 Announce Type: replace 
Abstract: Community search on bipartite graphs, especially influential community detection, has received significant attention. Existing studies use minimum vertex weights, inadequately reflecting true community influence when some vertices have low weights. To address this, we introduce the $(\alpha,\beta)$-influential community model based on the average vertex weights from both layers, providing a more comprehensive influence measure. Given the NP-hardness of accurately identifying top-$r$ communities, we propose an exact recursive algorithm enhanced by a slim tree structure and upper-bound techniques to improve efficiency. Additionally, we develop a greedy approximate algorithm with $O((n+m)+m\log{n})$ complexity, further optimized by a pruning strategy. Experiments on 10 real-world graphs demonstrate the effectiveness and efficiency of our proposed algorithms.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-free Adjustable Polynomial Graph Filtering for Ultra-fast Multimodal Recommendation</title>
<link>https://arxiv.org/abs/2503.04406</link>
<guid>https://arxiv.org/abs/2503.04406</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal recommender systems, graph filtering, efficient recommendations, neural networks, computational costs 

Summary: 
MultiModal-Graph Filtering (MM-GF) is a training-free method for improving multimodal recommender systems. It utilizes graph filtering to efficiently integrate information from diverse content types, such as text, images, and videos. By constructing similarity graphs for different modalities and user-item interactions, MM-GF optimally fuses multimodal signals using a polynomial graph filter. This allows for precise control of frequency response and flexible adaptation of filter coefficients. Experimental results on real-world datasets show that MM-GF significantly enhances recommendation accuracy by up to 22.25% compared to existing methods while reducing computational costs to less than 10 seconds. This method addresses the challenge of computational overhead in neural network-based models and offers a more efficient and accurate approach to multimodal recommendations. 

<br /><br />Summary: <div>
arXiv:2503.04406v2 Announce Type: replace-cross 
Abstract: Multimodal recommender systems improve the performance of canonical recommender systems with no item features by utilizing diverse content types such as text, images, and videos, while alleviating inherent sparsity of user-item interactions and accelerating user engagement. However, current neural network-based models often incur significant computational overhead due to the complex training process required to learn and integrate information from multiple modalities. To address this challenge,we propose MultiModal-Graph Filtering (MM-GF), a training-free method grounded in graph filtering (GF) for efficient and accurate multimodal recommendations. Specifically, MM-GF first constructs multiple similarity graphs for two distinct modalities as well as user-item interaction data. Then, MM-GF optimally fuses these multimodal signals using a polynomial graph filter that allows for precise control of the frequency response by adjusting frequency bounds. Furthermore, the filter coefficients are treated as hyperparameters, enabling flexible and data-driven adaptation. Extensive experiments on real-world benchmark datasets demonstrate that MM-GF not only improves recommendation accuracy by up to 22.25% compared to the best competitor but also dramatically reduces computational costs by achieving the runtime of less than 10 seconds.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Embedding Analysis for Anti-Money Laundering Detection</title>
<link>https://arxiv.org/abs/2509.10715</link>
<guid>https://arxiv.org/abs/2509.10715</guid>
<content:encoded><![CDATA[
<div> Graph embedding, money laundering, financial networks, suspicious cycles, centrality measures
Summary:
Using network embedding, this study examines money laundering in financial transaction networks by analyzing over one million accounts represented as a directed graph. By applying node2vec embeddings to refine previously identified suspicious cycles, a new network parameter known as the spread number is introduced. This, along with traditional centrality measures, contributes to an aggregate score denoted as $R, highlighting anti-central nodes - accounts that are structurally important yet evade detection. Findings indicate that only a small subset of cycles exhibit high $R values, identifying concentrated groups of suspicious accounts. The study showcases the effectiveness of embedding-based network analysis in uncovering laundering strategies that go undetected by conventional graph centrality measures. <br /><br />Summary: <div>
arXiv:2509.10715v1 Announce Type: new 
Abstract: We employ network embedding to detect money laundering in financial transaction networks. Using real anonymized banking data, we model over one million accounts as a directed graph and use it to refine previously detected suspicious cycles with node2vec embeddings, creating a new network parameter, the spread number. Combined with more traditional centrality measures, these define an aggregate score $R$ that highlights so-called anti-central nodes: accounts that are structurally important yet organized to avoid detection. Our results show only a small subset of cycles attain high $R$ values, flagging concentrated groups of suspicious accounts. Our approach demonstrates the potential of embedding-based network analysis to expose laundering strategies that evade traditional graph centrality measures.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socially-Informed Content Analysis of Online Human Behavior</title>
<link>https://arxiv.org/abs/2509.10807</link>
<guid>https://arxiv.org/abs/2509.10807</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, computational social science, user embeddings, COVID-19 discourse, online hate speech<br />
Summary: The dissertation explores the challenges of social media, focusing on political polarization, misinformation, hate speech, and echo chambers. It introduces a social network representation learning method that combines user-generated content and social connections to predict and visualize user attributes, communities, and behavioral patterns. The study investigates COVID-19 discourse on Twitter, revealing polarization and political echo chambers. It also delves into online hate speech, suggesting that the pursuit of social approval drives toxic behavior. Furthermore, the research examines the moral underpinnings of COVID-19 discussions, highlighting moral homophily and echo chambers. It indicates that moral diversity can enhance message reach and acceptance across ideological divides, contributing to computational social science and enhancing understanding of human behavior through social interactions and network homophily.<br /><br />Summary: <div>
arXiv:2509.10807v1 Announce Type: new 
Abstract: The explosive growth of social media has not only revolutionized communication but also brought challenges such as political polarization, misinformation, hate speech, and echo chambers. This dissertation employs computational social science techniques to investigate these issues, understand the social dynamics driving negative online behaviors, and propose data-driven solutions for healthier digital interactions. I begin by introducing a scalable social network representation learning method that integrates user-generated content with social connections to create unified user embeddings, enabling accurate prediction and visualization of user attributes, communities, and behavioral propensities. Using this tool, I explore three interrelated problems: 1) COVID-19 discourse on Twitter, revealing polarization and asymmetric political echo chambers; 2) online hate speech, suggesting the pursuit of social approval motivates toxic behavior; and 3) moral underpinnings of COVID-19 discussions, uncovering patterns of moral homophily and echo chambers, while also indicating moral diversity and plurality can improve message reach and acceptance across ideological divides. These findings contribute to the advancement of computational social science and provide a foundation for understanding human behavior through the lens of social interactions and network homophily.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YTCommentVerse: A Multi-Category Multi-Lingual YouTube Comment Corpus</title>
<link>https://arxiv.org/abs/2509.11057</link>
<guid>https://arxiv.org/abs/2509.11057</guid>
<content:encoded><![CDATA[
<div> Keywords: YTCommentVerse, YouTube comments, multilingual, multi-category dataset, sentiment analysis<br />
<br />Summary: <br />YTCommentVerse is a large dataset of YouTube comments, containing over 32 million comments from 178,000 videos across 15 categories. It includes data from more than 20 million unique users in over 50 languages, making it valuable for sentiment, toxicity, and engagement analysis. The dataset provides video and comment IDs, user channel details, upvotes, and category labels, filling a gap in publicly available social media datasets. Researchers can explore patterns in sentiment, toxicity, and engagement across diverse cultural and topical contexts. YTCommentVerse is a valuable resource for analyzing video sharing platforms, offering detailed metadata and multiple languages for a comprehensive study of YouTube comments. <div>
arXiv:2509.11057v1 Announce Type: new 
Abstract: In this paper, we introduce YTCommentVerse, a large-scale multilingual and multi-category dataset of YouTube comments. It contains over 32 million comments from 178,000 videos contributed by more than 20 million unique users spanning 15 distinct YouTube content categories such as Music, News, Education and Entertainment. Each comment in the dataset includes video and comment IDs, user channel details, upvotes and category labels. With comments in over 50 languages, YTCommentVerse provides a rich resource for exploring sentiment, toxicity and engagement patterns across diverse cultural and topical contexts. This dataset helps fill a major gap in publicly available social media datasets particularly for analyzing video sharing platforms by combining multiple languages, detailed categories and other metadata.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Percolation Centrality Approximation with Importance Sampling</title>
<link>https://arxiv.org/abs/2509.11454</link>
<guid>https://arxiv.org/abs/2509.11454</guid>
<content:encoded><![CDATA[
<div> Algorithm, Importance Sampling, Percolation Centrality, Approximation, Graphs
<br />
Summary: 
The article introduces PercIS, an algorithm utilizing Importance Sampling to approximate percolation centrality in attributed graphs. Percolation centrality is crucial for measuring vertex importance in infectious processes or information diffusion. Existing sampling methods for percolation centrality face limitations in accuracy and efficiency. PercIS addresses this by providing high-quality estimates with tight sample size bounds. Experimental results demonstrate the algorithm's effectiveness in large real-world networks, showcasing superior performance in accuracy, sample sizes, and runtime compared to current methods. <div>
arXiv:2509.11454v1 Announce Type: new 
Abstract: In this work we present PercIS, an algorithm based on Importance Sampling to approximate the percolation centrality of all the nodes of a graph. Percolation centrality is a generalization of betweenness centrality to attributed graphs, and is a useful measure to quantify the importance of the vertices in a contagious process or to diffuse information. However, it is impractical to compute it exactly on modern-sized networks.
  First, we highlight key limitations of state-of-the-art sampling-based approximation methods for the percolation centrality, showing that in most cases they cannot achieve accurate solutions efficiently. Then, we propose and analyze a novel sampling algorithm based on Importance Sampling, proving tight sample size bounds to achieve high-quality approximations.
  Our extensive experimental evaluation shows that PercIS computes high-quality estimates and scales to large real-world networks, while significantly outperforming, in terms of sample sizes, accuracy and running times, the state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Community Detection Method to Rule Them All!</title>
<link>https://arxiv.org/abs/2509.11490</link>
<guid>https://arxiv.org/abs/2509.11490</guid>
<content:encoded><![CDATA[
<div> Community detection, real-world graphs, downstream tasks, algorithm impact, community structure<br />
<br />
Summary: 
Community detection is crucial for analyzing real-world graphs and deriving local features for downstream tasks. The impact of community detection algorithms on downstream tasks is not well understood. Evaluation is typically based on intrinsic objectives or the overall impact on downstream tasks, but the specific algorithm used can significantly influence outcomes. This study explores how different algorithms affect task performance, showing that the properties of communities play a role in performance but are influenced by complex interactions. No single community property directly explains task performance, highlighting the need for a combination of random community generation and machine learning techniques for optimal results. <div>
arXiv:2509.11490v1 Announce Type: new 
Abstract: Community detection is a core tool for analyzing large realworld graphs. It is often used to derive additional local features of vertices and edges that will be used to perform a downstream task, yet the impact of community detection on downstream tasks is poorly understood. Prior work largely evaluates community detection algorithms by their intrinsic objectives (e.g., modularity). Or they evaluate the impact of using community detection onto on the downstream task. But the impact of particular community detection algortihm support the downstream task. We study the relationship between community structure and downstream performance across multiple algorithms and two tasks. Our analysis links community-level properties to task metrics (F1, precision, recall, AUC) and reveals that the choice of detection method materially affects outcomes. We explore thousands of community structures and show that while the properties of communities are the reason behind the impact on task performance, no single property explains performance in a direct way. Rather, results emerge from complex interactions among properties. As such, no standard community detection algorithm will derive the best downstream performance. We show that a method combining random community generation and simple machine learning techniques can derive better performance
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The threshold and quasi-stationary distribution for the SIS model on networks</title>
<link>https://arxiv.org/abs/2509.11706</link>
<guid>https://arxiv.org/abs/2509.11706</guid>
<content:encoded><![CDATA[
<div> Keywords: SIS model, arbitrary networks, pair approximation, epidemic threshold, quasi-stationary fraction<br />
Summary:<br />
- The study focuses on the Susceptible-Infectious-Susceptible (SIS) model on arbitrary networks, improving the pair approximation method by expanding the state space dynamically.<br />
- The enhanced method provides nodes with a memory of their last susceptible state, making the approximation simpler to implement and highly accurate.<br />
- The approach effectively determines the epidemic threshold and computes the quasi-stationary fraction of infected individuals above the threshold.<br />
- The method's accuracy is demonstrated for both finite graphs and infinite random graphs, highlighting its applicability in various network settings.<br />
- This research contributes to advancing the understanding of disease spread dynamics on complex networks, offering a valuable tool for predicting and managing infectious disease outbreaks. <br /> <div>
arXiv:2509.11706v1 Announce Type: new 
Abstract: We study the Susceptible-Infectious-Susceptible (SIS) model on arbitrary networks. The well-established pair approximation treats neighboring pairs of nodes exactly while making a mean field approximation for the rest of the network. We improve the method by expanding the state space dynamically, giving nodes a memory of when they last became susceptible. The resulting approximation is simple to implement and appears to be highly accurate, both in locating the epidemic threshold and in computing the quasi-stationary fraction of infected individuals above the threshold, for both finite graphs and infinite random graphs.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Percolation and matrix spectrum through NIB message passing</title>
<link>https://arxiv.org/abs/2509.11730</link>
<guid>https://arxiv.org/abs/2509.11730</guid>
<content:encoded><![CDATA[
<div> belief propagation, message passing, KCN-method, NIB-method, percolation

Summary:<br />
- Belief propagation is a widely used message passing method due to its computational efficiency and versatility.
- The KCN-method was introduced to address the issue of loops affecting accuracy in belief propagation for networks, specifically in applications such as percolation and sparse matrix spectra calculation.
- The NIB-method is an improvement on the KCN-method, enhancing its performance in probabilistic graphical models on networks.
- This study demonstrates that the NIB-method's benefits can be realized not only in inference tasks in graphical models but also in its original applications of percolation and matrix spectra calculations.<br />  
Summary: <div>
arXiv:2509.11730v1 Announce Type: new 
Abstract: Given its computational efficiency and versatility, belief propagation is the most prominent message passing method in several applications. In order to diminish the damaging effect of loops on its accuracy, the first explicit version of generalized belief propagation for networks, the KCN-method, was recently introduced. This approach was originally developed in the context of two target problems: percolation and the calculation of the spectra of sparse matrices. Later on, the KCN-method was extended in order to deal with inference in the context of probabilistic graphical models on networks. It was in this scenario where an improvement on the KCN-method, the NIB-method, was conceived. We show here that this improvement can also achieved in the original applications of the KCN-method, namely percolation and matrix spectra.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fostering cultural change in research through innovative knowledge sharing, evaluation, and community engagement strategies</title>
<link>https://arxiv.org/abs/2509.12045</link>
<guid>https://arxiv.org/abs/2509.12045</guid>
<content:encoded><![CDATA[
<div> Keywords: research assessment, open knowledge, FAIR principles, metrics, scientific landscape 

Summary: 
The article discusses the need for a new system to value science and scientists, highlighting the importance of open knowledge and FAIR principles in research assessment. Despite efforts to promote more accurate assessment metrics and efficient knowledge sharing, outdated methods such as standardized metrics like h-index and journal impact factor still dominate evaluations, leading researchers to prioritize quantity over integrity and reproducibility. A global community of researchers, funding institutions, industrial partners, and publishers from 14 countries across 5 continents have come together to envision an evolved knowledge sharing and research evaluation system, aiming to bring about a cultural change in the scientific landscape towards fairness and equity. The proposed ideas set the groundwork for a more just and inclusive scientific environment. 

<br /><br />Summary: <div>
arXiv:2509.12045v1 Announce Type: new 
Abstract: Scientific research needs a new system that appropriately values science and scientists. Key innovations, within institutions and funding agencies, are driving better assessment of research, with open knowledge and FAIR (findable, accessible, interoperable, and reusable) principles as central pillars. Furthermore, coalitions, agreements, and robust infrastructures have emerged to promote more accurate assessment metrics and efficient knowledge sharing. However, despite these efforts, the system still relies on outdated methods where standardized metrics such as h-index and journal impact factor dominate evaluations. These metrics have had the unintended consequence of pushing researchers to produce more outputs at the expense of integrity and reproducibility. In this community paper, we bring together a global community of researchers, funding institutions, industrial partners, and publishers from 14 different countries across the 5 continents. We aim at collectively envision an evolved knowledge sharing and research evaluation along with the potential positive impact on every stakeholder involved. We imagine these ideas to set the groundwork for a cultural change to redefine a more fair and equitable scientific landscape.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset</title>
<link>https://arxiv.org/abs/2509.11136</link>
<guid>https://arxiv.org/abs/2509.11136</guid>
<content:encoded><![CDATA[
<div> Keywords: Persian names, gender detection, digital identity creation, dataset, frameworks<br />
Summary:<br />
This research addresses the challenges in natural language processing applications related to Persian names by introducing the PNGT-26K dataset, which consists of approximately 26,000 tuples of Persian names, their associated gender, and English transliterations. The dataset aims to improve performance in gender detection and digital identity creation for Persian names. Additionally, two frameworks are introduced: Open Gender Detection, a production-grade framework for probabilistic gender detection using user data, and Nominalist, an AI tool to help users choose usernames for social media accounts. These frameworks can enhance user experience and provide valuable tools for various applications. The PNGT-26K dataset, Nominalist, and Open Gender Detection frameworks are publicly available on Github.<br /> 
Summary: <div>
arXiv:2509.11136v1 Announce Type: cross 
Abstract: Persian names present unique challenges for natural language processing applications, particularly in gender detection and digital identity creation, due to transliteration inconsistencies and cultural-specific naming patterns. Existing tools exhibit significant performance degradation on Persian names, while the scarcity of comprehensive datasets further compounds these limitations. To address these challenges, the present research introduces PNGT-26K, a comprehensive dataset of Persian names, their commonly associated gender, and their English transliteration, consisting of approximately 26,000 tuples. As a demonstration of how this resource can be utilized, we also introduce two frameworks, namely Open Gender Detection and Nominalist. Open Gender Detection is a production-grade, ready-to-use framework for using existing data from a user, such as profile photo and name, to give a probabilistic guess about the person's gender. Nominalist, the second framework introduced by this paper, utilizes agentic AI to help users choose a username for their social media accounts on any platform. It can be easily integrated into any website to provide a better user experience. The PNGT-26K dataset, Nominalist and Open Gender Detection frameworks are publicly available on Github.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm</title>
<link>https://arxiv.org/abs/2509.11443</link>
<guid>https://arxiv.org/abs/2509.11443</guid>
<content:encoded><![CDATA[
<div> sentiment analysis, 15-minute city concept, transformer models, multi-platform, social media<br />
Summary:<br />
This study conducted a multi-platform sentiment analysis of the 15-minute city concept across Twitter, Reddit, and news media. Using compressed transformer models and annotation with Llama-3-8B, sentiment was classified across different text domains. Five models were benchmarked, with DistilRoBERTa achieving the highest F1 score. The study found that News data had inflated performance due to class imbalance, Reddit suffered from summarization loss, and Twitter presented a moderate challenge. The compressed models performed competitively, challenging the belief that larger models are necessary for sentiment analysis. Platform-specific trade-offs were identified, and directions for scalable sentiment classification in urban planning discourse were proposed.<br /> <div>
arXiv:2509.11443v1 Announce Type: cross 
Abstract: This study presents the first multi-platform sentiment analysis of public opinion on the 15-minute city concept across Twitter, Reddit, and news media. Using compressed transformer models and Llama-3-8B for annotation, we classify sentiment across heterogeneous text domains. Our pipeline handles long-form and short-form text, supports consistent annotation, and enables reproducible evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM, ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1 (0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform consistency. Results show News data yields inflated performance due to class imbalance, Reddit suffers from summarization loss, and Twitter offers moderate challenge. Compressed models perform competitively, challenging assumptions that larger models are necessary. We identify platform-specific trade-offs and propose directions for scalable, real-world sentiment classification in urban planning discourse.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media</title>
<link>https://arxiv.org/abs/2509.11444</link>
<guid>https://arxiv.org/abs/2509.11444</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized social media, sentiment analysis, emotion analysis, narrative analysis, Bluesky

Summary:<br /><br />
The study introduces CognitiveSky, an open-source framework designed for sentiment, emotion, and narrative analysis on Bluesky, a decentralized social media platform. By using Bluesky's API, CognitiveSky applies transformer-based models to analyze large-scale user-generated content, generating structured outputs for dynamic visualization in a dashboard. The framework is built on free-tier infrastructure, ensuring low operational costs and high accessibility. It is showcased for monitoring mental health discourse but is adaptable for various domains like disinformation detection and crisis response. The modular design enables versatility, bridging large language models with decentralized networks transparently. CognitiveSky offers a transparent and extensible tool for computational social science in an evolving digital landscape.<br /><br /> <div>
arXiv:2509.11444v1 Announce Type: cross 
Abstract: The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Updating the Complex Systems Keyword Diagram Using Collective Feedback and Latest Literature Data</title>
<link>https://arxiv.org/abs/2509.11997</link>
<guid>https://arxiv.org/abs/2509.11997</guid>
<content:encoded><![CDATA[
<div> update, reorganization, complex systems, network science, keyword diagram

Summary:
The author reports on the update of a keyword diagram for complex systems generated in 2010. The update was based on feedback from social media, reference books, online resources, and keyword search hits. The data sources provided insight into both public perception and actual usage of keywords in complex systems publications. The resulting network visualization of complex systems keywords revealed differences and overlaps in keyword associations. Four topical communities were identified in the keyword association network, showing high interconnection among them. The updated diagram aims to provide a more accurate and current topic map of the complex systems field. <div>
arXiv:2509.11997v1 Announce Type: cross 
Abstract: The complex systems keyword diagram generated by the author in 2010 has been used widely in a variety of educational and outreach purposes, but it definitely needs a major update and reorganization. This short paper reports our recent attempt to update the keyword diagram using information collected from the following multiple sources: (a) collective feedback posted on social media, (b) recent reference books on complex systems and network science, (c) online resources on complex systems, and (d) keyword search hits obtained using OpenAlex, an open-access bibliographic catalogue of scientific publications. The data (a), (b) and (c) were used to incorporate the research community's internal perceptions of the relevant topics, whereas the data (d) was used to obtain more objective measurements of the keywords' relevance and associations from publications made in complex systems science. Results revealed differences and overlaps between public perception and actual usage of keywords in publications on complex systems. Four topical communities were obtained from the keyword association network, although they were highly intertwined with each other. We hope that the resulting network visualization of complex systems keywords provides a more up-to-date, accurate topic map of the field of complex systems as of today.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidencing preferential attachment in dependency network evolution</title>
<link>https://arxiv.org/abs/2509.12135</link>
<guid>https://arxiv.org/abs/2509.12135</guid>
<content:encoded><![CDATA[
<div> Keywords: Preferential attachment, network growth, scale-free, generalised linear model, Bayesian inference

Summary:
Preferential attachment is often seen as the driving force behind network growth, particularly in scale-free networks. However, traditional methods of determining scale-freeness may be questionable. By analyzing the evolution history of a network, a new approach using a generalised linear model allows for direct measurement of preferential attachment. This model considers in-degrees and their increments as covariates and responses, respectively. Parameters representing preferential attachment are integrated into the model, ensuring realistic tail heaviness of the degree distribution. Bayesian inference enables a hierarchical version of the model to be implemented. Application of this approach to the dependency network of R packages uncovers nuanced differences in behavior when considering new dependencies by new and existing packages, as well as when examining addition and removal of dependencies. <br /><br />Summary: <div>
arXiv:2509.12135v1 Announce Type: cross 
Abstract: Preferential attachment is often suggested to be the underlying mechanism of the growth of a network, largely due to that many real networks are, to a certain extent, scale-free. However, such attribution is usually made under debatable practices of determining scale-freeness and when only snapshots of the degree distribution are observed. In the presence of the evolution history of the network, modelling the increments of the evolution allows us to measure preferential attachment directly. Therefore, we propose a generalised linear model for such purpose, where the in-degrees and their increments are the covariate and response, respectively. Not only are the parameters that describe the preferential attachment directly incorporated, they also ensure that the tail heaviness of the asymptotic degree distribution is realistic. The Bayesian approach to inference enables the hierarchical version of the model to be implemented naturally. The application to the dependency network of R packages reveals subtly different behaviours between new dependencies by new and existing packages, and between addition and removal of dependencies.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Follow Networks and Twitter's Content Recommender on Partisan Skew and Rumor Exposure during the 2022 U.S. Midterm Election</title>
<link>https://arxiv.org/abs/2509.09826</link>
<guid>https://arxiv.org/abs/2509.09826</guid>
<content:encoded><![CDATA[
<div> algorithmic systems, Twitter, social network, election, information<br />
<br />Summary:<br /> 
The study investigates the impact of Twitter's algorithmic content recommender and users' social networks during the 2022 U.S. midterm election. It is found that the algorithm significantly influences exposure to election content, partisan skew, and the prevalence of low-quality information and election rumors. The partisan makeup of a user's social network plays a crucial role, often exerting greater influence than the algorithm alone. The algorithmic feed reduces the proportion of election content shown to left-leaning accounts and skews content towards right-leaning sources compared to the reverse chronological feed. Additionally, the algorithm increases the prevalence of election-related rumors for right-leaning accounts and has mixed effects on the prevalence of low-quality information sources. This research sheds light on the complex outcomes of Twitter's recommender system during a critical election period and underscores the need for continued examination of algorithmic systems' impact on democratic processes. <div>
arXiv:2509.09826v1 Announce Type: new 
Abstract: Social media platforms shape users' experiences through the algorithmic systems they deploy. In this study, we examine to what extent Twitter's content recommender, in conjunction with a user's social network, impacts the topic, political skew, and reliability of information served on the platform during a high-stakes election. We utilize automated accounts to document Twitter's algorithmically curated and reverse chronological timelines throughout the U.S. 2022 midterm election. We find that the algorithmic timeline measurably influences exposure to election content, partisan skew, and the prevalence of low-quality information and election rumors. Critically, these impacts are mediated by the partisan makeup of one's personal social network, which often exerts greater influence than the algorithm alone. We find that the algorithmic feed decreases the proportion of election content shown to left-leaning accounts, and that it skews content toward right-leaning sources when compared to the reverse chronological feed. We additionally find evidence that the algorithmic system increases the prevalence of election-related rumors for right-leaning accounts, and has mixed effects on the prevalence of low-quality information sources. Our work provides insight into the outcomes of Twitter's complex recommender system at a crucial time period before controversial changes to the platform and in the midst of nationwide elections and highlights the need for ongoing study of algorithmic systems and their role in democratic processes.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Request a Note: How the Request Function Shapes X's Community Notes System</title>
<link>https://arxiv.org/abs/2509.09956</link>
<guid>https://arxiv.org/abs/2509.09956</guid>
<content:encoded><![CDATA[
<div> Keywords: X's Community Notes, fact-checking, scalability, misinformation, polarized

Summary:
The article discusses X's Community Notes, a fact-checking system that recently introduced a "Request Community Note" feature to solicit fact-checks from contributors on specific posts. The study examines 98,685 requested posts and their associated notes to evaluate how requests impact the system. Contributors prioritize posts with higher misleading content and from authors with greater misinformation exposure. However, they tend to neglect political content emphasized by requestors. Selection of posts for fact-checking also diverges along partisan lines, with more posts from Republicans being annotated. Only 12% of posts receive request-fostered notes from top contributors, which are rated as more helpful and less polarized. This reflects top contributors' selective fact-checking of misleading posts. The findings reveal the limitations and potential of requests for scaling high-quality community-based fact-checking.<br /><br />Summary:  <div>
arXiv:2509.09956v1 Announce Type: new 
Abstract: X's Community Notes is a crowdsourced fact-checking system. To improve its scalability, X recently introduced "Request Community Note" feature, enabling users to solicit fact-checks from contributors on specific posts. Yet, its implications for the system -- what gets checked, by whom, and with what quality -- remain unclear. Using 98,685 requested posts and their associated notes, we evaluate how requests shape the Community Notes system. We find that contributors prioritize posts with higher misleadingness and from authors with greater misinformation exposure, but neglect political content emphasized by requestors. Selection also diverges along partisan lines: contributors more often annotate posts from Republicans, while requestors surface more from Democrats. Although only 12% of posts receive request-fostered notes from top contributors, these notes are rated as more helpful and less polarized than others, partly reflecting top contributors' selective fact-checking of misleading posts. Our findings highlight both the limitations and promise of requests for scaling high-quality community-based fact-checking.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Higher-Order Interactions in Complex Networks: A U.S. Diplomacy Case Study</title>
<link>https://arxiv.org/abs/2509.10333</link>
<guid>https://arxiv.org/abs/2509.10333</guid>
<content:encoded><![CDATA[
<div> network structure, diplomatic communication, hypergraph, random-walk dynamics, interaction-prediction

Summary:<br />
The study explores the network structure of diplomatic communication using U.S. diplomatic cables from WikiLeaks. It adopts a hypergraph approach and develops a pipeline based on random-walk dynamics. The pipeline is evaluated on legislative co-sponsorship and organizational email data, showing superior performance in capturing higher-order, group-based interactions. Hypergraphs paired with random-walk dynamics provide a richer structural account of diplomacy and enable the inference of new diplomatic relationships. The study highlights the advantages of hypergraph modeling over traditional pairwise graphs and demonstrates the effectiveness of this approach in understanding and predicting diplomatic interactions. <div>
arXiv:2509.10333v1 Announce Type: new 
Abstract: Although diplomatic communication has long been examined in the social sciences, its network structure remains underexplored. Using the U.S. diplomatic cables released by WikiLeaks in 2010 as a case study, we adopt a network-science perspective. We represent diplomatic interactions as a hypergraph and develop a general, random-walk-based pipeline to evaluate this representation against traditional pairwise graphs. We further evaluate the pipeline on legislative co-sponsorship and organizational email data, finding improvements and empirical evidence that clarifies when hypergraph modeling is preferable to pairwise graphs. Overall, hypergraphs paired with appropriately specified random-walk dynamics more faithfully capture higher-order, group-based interactions, yielding a richer structural account of diplomacy and superior performance on interaction-prediction tasks that enables inferring new diplomatic relationships from existing patterns.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TikTok Rewards Divisive Political Messaging During the 2025 German Federal Election</title>
<link>https://arxiv.org/abs/2509.10336</link>
<guid>https://arxiv.org/abs/2509.10336</guid>
<content:encoded><![CDATA[
<div> Emotions, Outgroup animosity, Engagement, Political parties, TikTok<br />
Summary: <br />
A study analyzed German politicians' use of TikTok during the 2025 federal election, finding that videos expressing negative emotions and outgroup animosity were more engaging. Extreme parties, regardless of ideology, were more likely to post such content and achieve higher engagement compared to centrist parties. The findings suggest that TikTok's algorithmic curation rewards divisive political communication, potentially giving an advantage to extreme actors who exploit this dynamic. <div>
arXiv:2509.10336v1 Announce Type: new 
Abstract: Short-form video platforms like TikTok reshape how politicians communicate and have become important tools for electoral campaigning. Yet it remains unclear what kinds of political messages gain traction in these fast-paced, algorithmically curated environments, which are particularly popular among younger audiences. In this study, we use computational content analysis to analyze a comprehensive dataset of N=25,292 TikTok videos posted by German politicians in the run-up to the 2025 German federal election. Our empirical analysis shows that videos expressing negative emotions (e.g., anger, disgust) and outgroup animosity were significantly more likely to generate engagement than those emphasizing positive emotion, relatability, or identity. Furthermore, ideologically extreme parties (on both sides of the political spectrum) were both more likely to post this type of content and more successful in generating engagement than centrist parties. Taken together, these findings suggest that TikTok's platform dynamics systematically reward divisive over unifying political communication, thereby potentially benefiting extreme actors more inclined to capitalize on this logic.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beginner's Charm: Beginner-Heavy Teams Are Associated With High Scientific Disruption</title>
<link>https://arxiv.org/abs/2509.10389</link>
<guid>https://arxiv.org/abs/2509.10389</guid>
<content:encoded><![CDATA[
<div> disruption, innovation, beginners, collaboration, citations
<br />
Summary: 
Analyzing 28 million articles published between 1971 and 2020, a study found that teams with a higher fraction of beginners are more disruptive and innovative. These teams draw on broader and less canonical prior work, leading to atypical recombinations. Collaboration structure plays a role, with disruption being high when beginners work with early-career colleagues or co-authors with disruptive track records. While disruption and citations are typically negatively correlated, highly disruptive papers from beginner-heavy teams are highly cited. This highlights the "beginner's charm" in science, emphasizing the value of beginners in teams and suggesting strategies for fostering innovation in science and technology. <div>
arXiv:2509.10389v1 Announce Type: cross 
Abstract: Teams now drive most scientific advances, yet the impact of absolute beginners -- authors with no prior publications -- remains understudied. Analyzing over 28 million articles published between 1971 and 2020 across disciplines and team sizes, we uncover a universal and previously undocumented pattern: teams with a higher fraction of beginners are systematically more disruptive and innovative. Their contributions are linked to distinct knowledge-integration behaviors, including drawing on broader and less canonical prior work and producing more atypical recombinations. Collaboration structure further shapes outcomes: disruption is high when beginners work with early-career colleagues or with co-authors who have disruptive track records. Although disruption and citations are negatively correlated overall, highly disruptive papers from beginner-heavy teams are highly cited. These findings reveal a "beginner's charm" in science, highlighting the underrecognized yet powerful value of beginner fractions in teams and suggesting actionable strategies for fostering a thriving ecosystem of innovation in science and technology.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse</title>
<link>https://arxiv.org/abs/2412.16114</link>
<guid>https://arxiv.org/abs/2412.16114</guid>
<content:encoded><![CDATA[
<div> content moderation, online discourse, toxic speech, computational linguistics, social media  
Summary:  
- A methodology is proposed for measuring content-moderation-induced distortions in online discourse using text embeddings from computational linguistics.  
- Removing toxic Tweets alters the semantic composition of online content, not only due to toxic language but also the removal of topics often expressed toxically.  
- Using generative Large Language Models to rephrase toxic Tweets rather than removing them entirely can reduce toxicity while minimizing distortions in online content.  
- The study was conducted on a sample of 5 million US political Tweets and found consistent results across different embedding models, toxicity metrics, and samples.  
- This alternative approach to content moderation aims to preserve salvageable content in toxic Tweets, suggesting a more nuanced and effective strategy for addressing toxic speech on social media platforms.  

Summary: <div>
arXiv:2412.16114v2 Announce Type: replace 
Abstract: There is an ongoing debate about how to moderate toxic speech on social media and the impact of content moderation on online discourse. This paper proposes and validates a methodology for measuring the content-moderation-induced distortions in online discourse using text embeddings from computational linguistics. Applying the method to a representative sample of 5 million US political Tweets, we find that removing toxic Tweets alters the semantic composition of content. This finding is consistent across different embedding models, toxicity metrics, and samples. Importantly, we demonstrate that these effects are not solely driven by toxic language but by the removal of topics often expressed in toxic form. We propose an alternative approach to content moderation that uses generative Large Language Models to rephrase toxic Tweets, preserving their salvageable content rather than removing them entirely. We show that this rephrasing strategy reduces toxicity while minimizing distortions in online content.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster Synchronization via Graph Laplacian Eigenvectors</title>
<link>https://arxiv.org/abs/2503.18978</link>
<guid>https://arxiv.org/abs/2503.18978</guid>
<content:encoded><![CDATA[
<div> Equitable partitions, cluster synchronization, spectral framework, Laplacian spectrum, network dynamics <br />
Summary: Almost equitable partitions (AEPs) and cluster synchronization in oscillatory systems are linked through a spectral framework that utilizes eigenvectors to understand partition-induced synchronization behaviors. This framework reduces network dynamics to quotient graph projections, connecting transient hierarchical clustering and multi-frequency synchronization to network symmetry and community structure. The concept of quasi-equitable partitions at level $\delta$ ($\delta-$QEP) extends AEPs to account for structural imperfections and noise in real-world networks. This allows for a more realistic description of synchronization behavior in networks that lack perfect symmetries. The findings have significant implications for analyzing synchronization patterns in various networks, such as neural circuits and power grids. <br /> <br />Summary: <div>
arXiv:2503.18978v2 Announce Type: replace 
Abstract: Almost equitable partitions (AEPs) have been linked to cluster synchronization in oscillatory systems, highlighting the importance of structure in collective network dynamics. We provide a general spectral framework that formalizes this connection, showing how eigenvectors associated with AEPs span a subspace of the Laplacian spectrum that governs partition-induced synchronization behavior. This offers a principled reduction of network dynamics, allowing clustered states to be understood in terms of quotient graph projections. Our approach clarifies the conditions under which transient hierarchical clustering and multi-frequency synchronization emerge, and connects these dynamical phenomena directly to network symmetry and community structure. In doing so, we bridge a critical gap between static topology and dynamic behavior-namely, the lack of a spectral method for analyzing synchronization in networks that exhibit exact or approximate structural regularity. Perfect AEPs are rare in real-world networks since most have some degree of irregularity or noise. We define a relaxation of an AEP we call a quasi-equitable partition at level $\delta$ ($\delta-$QEP). $\delta-$QEPs can preserve many of the clustering-relevant properties of AEPs while tolerating structural imperfections and noise. This extension enables us to describe synchronization behavior in more realistic scenarios, where ideal symmetries are rarely present. Our findings have important implications for understanding synchronization patterns in real-world networks, from neural circuits to power grids.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Sheaf Neural Networks</title>
<link>https://arxiv.org/abs/2410.09590</link>
<guid>https://arxiv.org/abs/2410.09590</guid>
<content:encoded><![CDATA[
<div> Bayesian sheaf neural network, graph neural networks, convolution operation, cellular sheaf, reparameterizable probability distributions <br />
Summary:
The study introduces a Bayesian sheaf neural network for graph data, incorporating a convolution operation defined by a cellular sheaf to enhance expressive representations. By employing a variational approach, the network learns the sheaf as part of the architecture, resulting in improved performance on graph datasets. The innovative use of reparameterizable probability distributions on the rotation group improves model flexibility and robustness. Experimental results demonstrate that the Bayesian sheaf models outperform baseline models and exhibit reduced sensitivity to hyperparameters, particularly in limited training data scenarios. This approach offers a promising solution for learning representations of heterophilic graph data efficiently and effectively. <br /> <div>
arXiv:2410.09590v2 Announce Type: replace-cross 
Abstract: Equipping graph neural networks with a convolution operation defined in terms of a cellular sheaf offers advantages for learning expressive representations of heterophilic graph data. The most flexible approach to constructing the sheaf is to learn it as part of the network as a function of the node features. However, this leaves the network potentially overly sensitive to the learned sheaf. As a counter-measure, we propose a variational approach to learning cellular sheaves within sheaf neural networks, yielding an architecture we refer to as a Bayesian sheaf neural network. As part of this work, we define a novel family of reparameterizable probability distributions on the rotation group $SO(n)$ using the Cayley transform. We evaluate the Bayesian sheaf neural network on several graph datasets, and show that our Bayesian sheaf models achieve leading performance compared to baseline models and are less sensitive to the choice of hyperparameters under limited training data settings.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Community Detection Methods in Performance Variations of Graph Mining Tasks</title>
<link>https://arxiv.org/abs/2509.09045</link>
<guid>https://arxiv.org/abs/2509.09045</guid>
<content:encoded><![CDATA[
<div> community detection, large graphs, graph mining, downstream tasks, algorithm evaluation

Summary: 
Large graphs in complex systems often require community detection methods to extract meaningful insights. These methods can uncover structural patterns by dividing graphs into subgraphs without relying on rich node attributes. However, the lack of ground truth community information and the absence of a universally optimal detection method pose challenges for practitioners. In this study, the impact of different community detection algorithms on downstream tasks is examined. A framework is proposed to systematically evaluate the performance of various algorithms, highlighting their varying effectiveness in different applications. The findings suggest that specific algorithms may yield superior results depending on the task at hand, emphasizing the significance of method selection in achieving optimal outcomes in graph mining applications. <div>
arXiv:2509.09045v1 Announce Type: new 
Abstract: In real-world scenarios, large graphs represent relationships among entities in complex systems. Mining these large graphs often containing millions of nodes and edges helps uncover structural patterns and meaningful insights. Dividing a large graph into smaller subgraphs facilitates complex system analysis by revealing local information. Community detection extracts clusters or communities of graphs based on statistical methods and machine learning models using various optimization techniques. Structure based community detection methods are more suitable for applying to graphs because they do not rely heavily on rich node or edge attribute information. The features derived from these communities can improve downstream graph mining tasks, such as link prediction and node classification. In real-world applications, we often lack ground truth community information. Additionally, there is neither a universally accepted gold standard for community detection nor a single method that is consistently optimal across diverse applications. In many cases, it is unclear how practitioners select community detection methods, and choices are often made without explicitly considering their potential impact on downstream tasks. In this study, we investigate whether the choice of community detection algorithm significantly influences the performance of downstream applications. We propose a framework capable of integrating various community detection methods to systematically evaluate their effects on downstream task outcomes. Our comparative analysis reveals that specific community detection algorithms yield superior results in certain applications, highlighting that method selection substantially affects performance.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-in-the-loop Learning Through Decentralized Communication Mechanisms</title>
<link>https://arxiv.org/abs/2509.09574</link>
<guid>https://arxiv.org/abs/2509.09574</guid>
<content:encoded><![CDATA[
<div> decentralized communication mechanism, multi-agent Markov decision process, game theory, human-in-the-loop learning, selfish agents<br />
Summary:<br />
The article discusses the challenges posed by selfish human agents in information sharing platforms and proposes a decentralized approach to incentivize exploration. By shifting from centralized to decentralized operation using a multi-agent Markov decision process (MA-MDP), the paper aims to regulate human behavior towards exploring unknown options for the benefit of all. An optimal decentralized communication mechanism is formulated, and an algorithm with linear complexity is presented to determine the timing of intermittent information sharing. The approach is further adapted for non-myopic agents to prevent over-exploration. Simulation experiments using real-world data demonstrate the effectiveness of the proposed decentralized mechanisms in various scenarios. <div>
arXiv:2509.09574v1 Announce Type: new 
Abstract: Information sharing platforms like TripAdvisor and Waze involve human agents as both information producers and consumers. All these platforms operate in a centralized way to collect agents' latest observations of new options (e.g., restaurants, hotels, travel routes) and share such information with all in real time. However, after hearing the central platforms' live updates, many human agents are found selfish and unwilling to further explore unknown options for the benefit of others in the long run. To regulate the human-in-the-loop learning (HILL) game against selfish agents' free-riding, this paper proposes a paradigm shift from centralized to decentralized way of operation that forces agents' local explorations through restricting information sharing. When game theory meets distributed learning, we formulate our decentralized communication mechanism's design as a new multi-agent Markov decision process (MA-MDP), and derive its analytical condition to outperform today's centralized operation. As the optimal decentralized communication mechanism in MA-MDP is NP-hard to solve, we present an asymptotically optimal algorithm with linear complexity to determine the mechanism's timing of intermittent information sharing. Then we turn to non-myopic agents who may revert to even over-explore, and adapt our mechanism design to work. Simulation experiments using real-world dataset demonstrate the effectiveness of our decentralized mechanisms for various scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Negative Transfer: Disentangled Preference-Guided Diffusion for Cross-Domain Sequential Recommendation</title>
<link>https://arxiv.org/abs/2509.00389</link>
<guid>https://arxiv.org/abs/2509.00389</guid>
<content:encoded><![CDATA[
<div> DPG-Diff, cross-domain sequential recommendation, denoising framework, user preferences, knowledge transfer <br />
Summary:
DPG-Diff is a novel Disentangled Preference-Guided Diffusion Model for Cross-Domain Sequential Recommendation (CDSR). It addresses the challenge of conflicting domain-specific preferences in CDSR by decomposing user preferences into domain-invariant and domain-specific components. The model utilizes a reverse diffusion process guided by these disentangled preferences to enhance knowledge transfer and filter out noise in sequential signals. DPG-Diff outperforms existing baselines in CDSR, offering a more robust solution to capturing user preferences across diverse domains. The iterative refinement process of the model makes it effective at capturing subtle preference signals and improving recommendation quality. Overall, DPG-Diff provides a promising approach for enhancing the performance of cross-domain sequential recommendation systems. <br /> <div>
arXiv:2509.00389v1 Announce Type: cross 
Abstract: Cross-Domain Sequential Recommendation (CDSR) leverages user behaviors across domains to enhance recommendation quality. However, naive aggregation of sequential signals can introduce conflicting domain-specific preferences, leading to negative transfer. While Sequential Recommendation (SR) already suffers from noisy behaviors such as misclicks and impulsive actions, CDSR further amplifies this issue due to domain heterogeneity arising from diverse item types and user intents. The core challenge is disentangling three intertwined signals: domain-invariant preferences, domain-specific preferences, and noise. Diffusion Models (DMs) offer a generative denoising framework well-suited for disentangling complex user preferences and enhancing robustness to noise. Their iterative refinement process enables gradual denoising, making them effective at capturing subtle preference signals. However, existing applications in recommendation face notable limitations: sequential DMs often conflate shared and domain-specific preferences, while cross-domain collaborative filtering DMs neglect temporal dynamics, limiting their ability to model evolving user preferences. To bridge these gaps, we propose \textbf{DPG-Diff}, a novel Disentangled Preference-Guided Diffusion Model, the first diffusion-based approach tailored for CDSR, to or best knowledge. DPG-Diff decomposes user preferences into domain-invariant and domain-specific components, which jointly guide the reverse diffusion process. This disentangled guidance enables robust cross-domain knowledge transfer, mitigates negative transfer, and filters sequential noise. Extensive experiments on real-world datasets demonstrate that DPG-Diff consistently outperforms state-of-the-art baselines across multiple metrics.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Engine Optimization: How to Dominate AI Search</title>
<link>https://arxiv.org/abs/2509.08919</link>
<guid>https://arxiv.org/abs/2509.08919</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, search engines, SEO, Earned media, GEO <br />
<br />
Summary: 
The article discusses the impact of generative AI-powered search engines on information retrieval and the need for a new approach called Generative Engine Optimization (GEO). Through a comparative analysis of AI Search and traditional web search like Google, the study reveals a bias towards Earned media by AI Search engines. The research also highlights differences in domain diversity, freshness, language stability, and sensitivity to phrasing among AI Search services. The strategic GEO agenda outlined includes recommendations for practitioners to optimize content for machine scannability, focus on earned media to establish authority, utilize engine-specific strategies, and overcome biases towards big brands. This empirical analysis provides a foundation and strategic framework for achieving visibility in the evolving generative search landscape. <br /> <div>
arXiv:2509.08919v1 Announce Type: cross 
Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT, Perplexity, and Gemini is fundamentally reshaping information retrieval, moving from traditional ranked lists to synthesized, citation-backed answers. This shift challenges established Search Engine Optimization (SEO) practices and necessitates a new paradigm, which we term Generative Engine Optimization (GEO).
  This paper presents a comprehensive comparative analysis of AI Search and traditional web search (Google). Through a series of large-scale, controlled experiments across multiple verticals, languages, and query paraphrases, we quantify critical differences in how these systems source information. Our key findings reveal that AI Search exhibit a systematic and overwhelming bias towards Earned media (third-party, authoritative sources) over Brand-owned and Social content, a stark contrast to Google's more balanced mix. We further demonstrate that AI Search services differ significantly from each other in their domain diversity, freshness, cross-language stability, and sensitivity to phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We provide actionable guidance for practitioners, emphasizing the critical need to: (1) engineer content for machine scannability and justification, (2) dominate earned media to build AI-perceived authority, (3) adopt engine-specific and language-aware strategies, and (4) overcome the inherent "big brand bias" for niche players. Our work provides the foundational empirical analysis and a strategic framework for achieving visibility in the new generative search landscape.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>6G Resilience -- White Paper</title>
<link>https://arxiv.org/abs/2509.09005</link>
<guid>https://arxiv.org/abs/2509.09005</guid>
<content:encoded><![CDATA[
<div> Keywords: 6G, resilience, sustainability, architecture, economics

Summary:
6G networks need to prioritize resilience, sustainability, and efficiency to withstand disruptions. The white paper advocates for a focus on resilience, with a 3R framework of reliability, robustness, and resilience. Measurable capabilities include graceful degradation, situational awareness, rapid reconfiguration, and learning-driven improvement. Architecturally, edge-native designs, open interfaces, and programmability are key, enabling islanded operations and diversity in radio, compute, energy, and timing layers. Techno-economic aspects include open platforms, complementors, and business model groups aligned with resilience objectives. Key enablers are AI-native control loops, zero-trust security, and networking techniques prioritizing critical traffic. The paper also highlights governance, standardization, and the importance of ecosystem externalities in enhancing resilience and opening new markets. This white paper aims to inspire stakeholders to shape the development of 6G resilience. 

<br /><br />Summary: 6G networks must prioritize resilience, using a 3R framework and measurable capabilities to ensure reliability and adaptation. Architectural designs, enablers like AI control loops and zero-trust security, and techno-economic aspects are crucial in enhancing network resilience and fostering new markets. Governance and standardization play a key role in shaping the development of 6G resilience. <div>
arXiv:2509.09005v1 Announce Type: cross 
Abstract: 6G must be designed to withstand, adapt to, and evolve amid prolonged, complex disruptions. Mobile networks' shift from efficiency-first to sustainability-aware has motivated this white paper to assert that resilience is a primary design goal, alongside sustainability and efficiency, encompassing technology, architecture, and economics. We promote resilience by analysing dependencies between mobile networks and other critical systems, such as energy, transport, and emergency services, and illustrate how cascading failures spread through infrastructures. We formalise resilience using the 3R framework: reliability, robustness, resilience. Subsequently, we translate this into measurable capabilities: graceful degradation, situational awareness, rapid reconfiguration, and learning-driven improvement and recovery.
  Architecturally, we promote edge-native and locality-aware designs, open interfaces, and programmability to enable islanded operations, fallback modes, and multi-layer diversity (radio, compute, energy, timing). Key enablers include AI-native control loops with verifiable behaviour, zero-trust security rooted in hardware and supply-chain integrity, and networking techniques that prioritise critical traffic, time-sensitive flows, and inter-domain coordination.
  Resilience also has a techno-economic aspect: open platforms and high-quality complementors generate ecosystem externalities that enhance resilience while opening new markets. We identify nine business-model groups and several patterns aligned with the 3R objectives, and we outline governance and standardisation. This white paper serves as an initial step and catalyst for 6G resilience. It aims to inspire researchers, professionals, government officials, and the public, providing them with the essential components to understand and shape the development of 6G resilience.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Iran Reloaded: Gamer Mitigation Tactics of IRI Information Controls</title>
<link>https://arxiv.org/abs/2509.09063</link>
<guid>https://arxiv.org/abs/2509.09063</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet censorship, Iran, circumvention technologies, gamers, social networks <br />
Summary: 
This report discusses internet censorship in Iran and the use of circumvention technologies by Iranian internet users, particularly gamers. A mixed-methods study of 660 users was conducted, focusing on the digital literacy and social networking of gamers. Results show that younger users are more confident in circumvention techniques, with peer networks being a strong predictor of resilience. Gaming communities, especially those on platforms like Discord and Telegram, serve as hubs for sharing tactics. The study highlights the importance of both technical and social strategies for circumventing censorship. It concludes with implications for developers, researchers, and funders working in digital rights and information controls. <br /><br />Summary: <div>
arXiv:2509.09063v1 Announce Type: cross 
Abstract: Internet censorship in the Islamic Republic of Iran restricts access to global platforms and services, forcing users to rely on circumvention technologies such as VPNs, proxies, and tunneling tools. This report presents findings from a mixed-methods study of 660 Iranian internet users, with a focus on gamers as a digitally literate and socially networked community. Survey data are combined with network measurements of latency and VPN performance to identify both technical and social strategies of circumvention. Results show that while younger users report higher confidence with circumvention, peer networks, rather than formal training, are the strongest predictors of resilience. Gaming communities, particularly those active on platforms such as Discord and Telegram, serve as hubs for sharing tactics and lowering barriers to adoption. These findings extend existing work on usable security and censorship circumvention by highlighting the intersection of infrastructural conditions and social learning. The study concludes with design and policy implications for developers, researchers, and funders working on digital rights and information controls.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking</title>
<link>https://arxiv.org/abs/2509.09583</link>
<guid>https://arxiv.org/abs/2509.09583</guid>
<content:encoded><![CDATA[
<div> Personality detection, GPTs, Big-Five traits, online courses, SAMI <br />
Summary:<br />
The article discusses the importance of social connection in online learning environments and the limitations faced by tools like SAMI due to incomplete Theory of Mind. It proposes a personality detection model using GPTs to infer Big-Five traits from forum posts and integrates this into SAMI for personality-informed matchmaking. Benchmarking against established models shows the efficacy of this approach in inferring personality traits. Initial integration suggests that these traits can enhance social recommendations provided by SAMI, potentially improving student engagement and match quality. Further evaluation is required to fully understand the impact of personality traits on student interactions in online courses. <br /> <div>
arXiv:2509.09583v1 Announce Type: cross 
Abstract: Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applicability of the Minimal Dominating Set for Influence Maximization in Multilayer Networks</title>
<link>https://arxiv.org/abs/2502.15236</link>
<guid>https://arxiv.org/abs/2502.15236</guid>
<content:encoded><![CDATA[
<div> dominating set, influence maximization, multilayer networks, seed selection, Linear Threshold Model

Summary:<br /><br />
The study explores the use of minimal dominating sets (MDS) in enhancing seed selection for influence maximization in multilayer networks. By utilizing the Linear Threshold Model to represent influence spread, the researchers identify that incorporating MDS into the seed selection process can improve spread in certain situations. The improvement is prominent for larger seed set budgets, lower activation thresholds, and when using an "AND" strategy to aggregate influence across network layers. This enhancement is particularly beneficial when an individual's target opinion must be influenced across all social circles, rather than requiring the majority of acquaintances to hold that opinion. These findings shed light on the potential benefits of combining MDS with traditional seed selection methods in the context of influence maximization in multilayer networks. <div>
arXiv:2502.15236v3 Announce Type: replace 
Abstract: The minimal dominating set (MDS) is a well-established concept in network controllability and has been successfully applied in various domains, including sensor placement, network resilience, and epidemic containment. In this study, we adapt the local-improvement MDS routine and explore its potential for enhancing seed selection for influence maximization in multilayer networks (MLN). We employ the Linear Threshold Model (LTM), which offers an intuitive representation of influence spread or opinion dynamics by accounting for peer influence accumulation. To ensure interpretability, we utilize rank-refining seed selection methods, with the results further filtered with MDS. Our findings reveal that incorporating MDS into the seed selection process improves spread only within a specific range of situations. Notably, the improvement is observed for larger seed set budgets, lower activation thresholds, and when an "AND" strategy is used to aggregate influence across network layers. This scenario reflects situations where an individual does not require the majority of their acquaintances to hold a target opinion, but must be influenced across all social circles.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images</title>
<link>https://arxiv.org/abs/2405.03486</link>
<guid>https://arxiv.org/abs/2405.03486</guid>
<content:encoded><![CDATA[
<div> Image safety classifiers, Benchmarking framework, AI-generated images, Effectiveness, Robustness<br />
<br />
Summary: UnsafeBench is introduced as a benchmarking framework to assess the performance of image safety classifiers on a dataset consisting of real-world and AI-generated images categorized as safe or unsafe. Existing classifiers are found to be lacking in comprehensiveness and effectiveness in dealing with unsafe images, especially AI-generated ones. A distribution shift between real-world and AI-generated images leads to decreased effectiveness and robustness. PerspectiveVision is developed as a tool to enhance the performance of existing classifiers, particularly on AI-generated images. These findings highlight the challenges in image safety classification in the age of generative AI and provide a potential solution to improve the moderation of unsafe images.<br /> <div>
arXiv:2405.03486v3 Announce Type: replace-cross 
Abstract: With the advent of text-to-image models and concerns about their misuse, developers are increasingly relying on image safety classifiers to moderate their generated unsafe images. Yet, the performance of current image safety classifiers remains unknown for both real-world and AI-generated images. In this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers, with a particular focus on the impact of AI-generated images on their performance. First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.). Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models. Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough to mitigate the multifaceted problem of unsafe images. Also, there exists a distribution shift between real-world and AI-generated images in image qualities, styles, and layouts, leading to degraded effectiveness and robustness. Motivated by these findings, we build a comprehensive image moderation tool called PerspectiveVision, which improves the effectiveness and robustness of existing classifiers, especially on AI-generated images. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Different Research Communities: Authorship Network</title>
<link>https://arxiv.org/abs/2409.00081</link>
<guid>https://arxiv.org/abs/2409.00081</guid>
<content:encoded><![CDATA[
<div> Google Scholar, data mining, software engineering, coauthorship network, influential authors<br />
<br />
Summary: 
In this study, Google Scholar data from 2000 to 2021 was collected for the research domains of Data Mining and Software Engineering in computer science. The advanced search option of Google Scholar allowed for the extraction of articles based on various criteria. The analysis focused on coauthorship networks in each domain, revealing distinct network structures with small communities of influential authors. Through extensive experiments, publication trends were analyzed, and influential authors and affiliated organizations were identified for each domain. The network analysis highlighted the unique features of each domain's network and showcased the presence of influential authors within specific communities. <div>
arXiv:2409.00081v2 Announce Type: replace-cross 
Abstract: Google Scholar is one of the top search engines to access research articles across multiple disciplines for scholarly literature. Google scholar advance search option gives the privilege to extract articles based on phrases, publishers name, authors name, time duration etc. In this work, we collected Google Scholar data (2000-2021) for two different research domains in computer science: Data Mining and Software Engineering. The scholar database resources are powerful for network analysis, data mining, and identify links between authors via authorship network. We examined coauthor-ship network for each domain and studied their network structure. Extensive experiments are performed to analyze publications trend and identifying influential authors and affiliated organizations for each domain. The network analysis shows that the networks features are distinct from one another and exhibit small communities within the influential authors of a particular domain.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sound of Silence in Social Networks</title>
<link>https://arxiv.org/abs/2410.19685</link>
<guid>https://arxiv.org/abs/2410.19685</guid>
<content:encoded><![CDATA[
<div> Graph theory, opinion dynamics, Spiral of Silence, consensus, memory

Summary:
In this study, the classic multi-agent DeGroot model for opinion dynamics is extended to integrate the Spiral of Silence theory, where individuals may stay silent if they perceive their opinions to be in the minority. Two opinion update models are introduced: memoryless (SOM-) and memory-based (SOM+). The study shows that for the SOM- model, consensus is ensured for clique graphs but not for strongly connected aperiodic graphs. However, for the SOM+ model, consensus is not guaranteed even for clique graphs. Through simulations, the impact of silence dynamics on opinion formation is explored, revealing the limitations of achieving consensus in more complex social models. This research provides insights aligning with the key aspects of the Spiral of Silence theory, shedding light on how silence dynamics influence opinion dynamics in social networks. 

<br /><br />Summary: <div>
arXiv:2410.19685v2 Announce Type: replace-cross 
Abstract: We generalize the classic multi-agent DeGroot model for opinion dynamics to incorporate the Spiral of Silence theory from political science. This theory states that individuals may withhold their opinions when they perceive them to be in the minority. As in the DeGroot model, a community of agents is represented as a weighted directed graph whose edges indicate how much agents influence one another. However, agents whose current opinions are in the minority become silent (i.e., they do not express their opinion). Two models for opinion update are then introduced. In the memoryless opinion model (SOM-), agents update their opinion by taking the weighted average of their non-silent neighbors' opinions. In the memory based opinion model (SOM+), agents update their opinions by taking the weighted average of the opinions of all their neighbors, but for silent neighbors, their most recent opinion is considered. We show that for SOM- convergence to consensus is guaranteed for clique graphs but, unlike for the classic DeGroot, not guaranteed for strongly-connected aperiodic graphs. In contrast, we show that for SOM+ convergence to consensus is not guaranteed even for clique graphs. We showcase our models through simulations offering experimental insights that align with key aspects of the Spiral of Silence theory. These findings reveal the impact of silence dynamics on opinion formation and highlight the limitations of consensus in more nuanced social models.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Contagion in Financial Labor Markets: Predicting Turnover in Hong Kong</title>
<link>https://arxiv.org/abs/2509.08001</link>
<guid>https://arxiv.org/abs/2509.08001</guid>
<content:encoded><![CDATA[
<div> Keywords: employee turnover, professional networks, financial markets, temporal networks, machine learning

Summary:
The study explores the impact of professional networks on employee turnover in the financial sector using data from the Hong Kong Securities and Futures Commission. By analyzing temporal networks of professionals and firms, the research identifies a contagion effect where employees are more likely to leave if a significant portion of their peers depart within a short timeframe. The study introduces a graph-based feature propagation framework to capture peer influence and organizational stability, leading to a 30% improvement in turnover prediction compared to baseline models when incorporating network signals into machine learning algorithms. These findings demonstrate the predictive power of temporal network effects in understanding workforce dynamics and suggest the potential applications of network-based analytics in regulatory monitoring, talent management, and systemic risk assessment.<br /><br />Summary: <div>
arXiv:2509.08001v1 Announce Type: new 
Abstract: Employee turnover is a critical challenge in financial markets, yet little is known about the role of professional networks in shaping career moves. Using the Hong Kong Securities and Futures Commission (SFC) public register (2007-2024), we construct temporal networks of 121,883 professionals and 4,979 firms to analyze and predict employee departures. We introduce a graph-based feature propagation framework that captures peer influence and organizational stability. Our analysis shows a contagion effect: professionals are 23% more likely to leave when over 30% of their peers depart within six months. Embedding these network signals into machine learning models improves turnover prediction by 30% over baselines. These results highlight the predictive power of temporal network effects in workforce dynamics, and demonstrate how network-based analytics can inform regulatory monitoring, talent management, and systemic risk assessment.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Dataset and Benchmark for Grounding Multimodal Misinformation</title>
<link>https://arxiv.org/abs/2509.08008</link>
<guid>https://arxiv.org/abs/2509.08008</guid>
<content:encoded><![CDATA[
<div> Keyword: online misinformation, multimodal content, GroundMM task, GroundLie360 dataset, VLM-based FakeMark baseline. 
Summary: 
The paper introduces the Grounding Multimodal Misinformation (GroundMM) task that aims to verify and localize misleading content across text, speech, and visuals. The researchers create the GroundLie360 dataset, which features a taxonomy of misinformation types, detailed annotations, and validation with evidence from Snopes and annotator reasoning. They propose the FakeMark baseline, a VLM-based system using single- and cross-modal cues for detection and grounding. The experiments demonstrate the challenges of the task and provide a foundation for interpretable multimodal misinformation detection. 
<br /><br />Summary: <div>
arXiv:2509.08008v1 Announce Type: new 
Abstract: The proliferation of online misinformation videos poses serious societal risks. Current datasets and detection methods primarily target binary classification or single-modality localization based on post-processed data, lacking the interpretability needed to counter persuasive misinformation. In this paper, we introduce the task of Grounding Multimodal Misinformation (GroundMM), which verifies multimodal content and localizes misleading segments across modalities. We present the first real-world dataset for this task, GroundLie360, featuring a taxonomy of misinformation types, fine-grained annotations across text, speech, and visuals, and validation with Snopes evidence and annotator reasoning. We also propose a VLM-based, QA-driven baseline, FakeMark, using single- and cross-modal cues for effective detection and grounding. Our experiments highlight the challenges of this task and lay a foundation for explainable multimodal misinformation detection.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Game is the Game: Dynamic network analysis and shifting roles in criminal networks</title>
<link>https://arxiv.org/abs/2509.08028</link>
<guid>https://arxiv.org/abs/2509.08028</guid>
<content:encoded><![CDATA[
<div> Keywords: criminal networks, key players, dynamic centrality, temporal analysis, network data uncertainty <br />
Summary: <br />
This paper introduces a novel approach to identifying key players in criminal networks by incorporating time as a crucial variable. Using network data from a two-year investigation of a drug trafficking network, the study applies dynamic Katz centrality to analyze changes in relationships and actors' relative importance over time. The results reveal actors who consistently hold central roles throughout the investigation and distinguish them from those who contribute significantly but for a limited period. Additionally, the study introduces a method to simulate missing data and assesses the impact of uncertainty on node rankings. The findings show that dynamic Katz centrality is effective in differentiating individual contributions within central nodes and tracking individual trajectories over time, even with incomplete data. This approach offers valuable insights for both organized crime scholars seeking to understand criminal collaboration complexity and law enforcement agencies targeting effective disruptions of criminal groups. <br /> <div>
arXiv:2509.08028v1 Announce Type: new 
Abstract: Objectives: This paper incorporates time as a crucial variable to identify key players in criminal networks and explores how actors' positions change over time. It then assesses the accuracy of the results against the uncertainty around network data collected from criminal justice records.
  Methods: Network data are from a judicial document for a two-year investigation targeting a drug trafficking and distribution network. We use Katz centrality in its dynamic version to explore changes in relationships and relative importance of network actors. We then use a novel method of introducing new edges to the network using Bernoulli random trials to simulate missing data and assess the extent to which node rankings based on Katz centrality change or remain the same when introducing some level of uncertainty to our observed network.
  Results: We identify actors who consistently held a central role over the course of the two-year investigation and differentiate them from actors who provided key contributions to the group's activities, but only for a limited period. We show that compared to centrality measures commonly used in criminal network analysis, dynamic Katz centrality is helpful to differentiate individual contributions even among central nodes and explore individual trajectories over time, even when data are incomplete.
  Conclusions: This paper demonstrates the value of key player identification using temporal network data and offers an additional analytical tool to both organised crime scholars trying to capture the complex nature of criminal collaboration and law enforcement agencies aiming at identifying appropriate targets and disrupting criminal groups.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signals in the Noise: Decoding Unexpected Engagement Patterns on Twitter</title>
<link>https://arxiv.org/abs/2509.08128</link>
<guid>https://arxiv.org/abs/2509.08128</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, engagement, signaling theory, attention economy, Twitter

Summary:
This study investigates the factors influencing unexpected engagement patterns on Twitter, using a new metric called the "unexpectedness quotient." The analysis of over 600,000 tweets reveals that news, politics, and business tweets receive more retweets and comments, while games and sports content see higher likes and comments. Users prioritize sharing and discussing informational content, while emotional investment is higher in entertainment-related topics. The study highlights a relationship between content attributes and engagement types, with subjective tweets attracting more likes and objective tweets receiving more retweets. Additionally, longer and more complex tweets with URLs unexpectedly receive more retweets. These findings demonstrate how users use different engagement types as signals based on content characteristics, offering valuable insights for content creators, platform designers, and researchers studying online social behavior. <br /><br />Summary: <div>
arXiv:2509.08128v1 Announce Type: new 
Abstract: Social media platforms offer users multiple ways to engage with content--likes, retweets, and comments--creating a complex signaling system within the attention economy. While previous research has examined factors driving overall engagement, less is known about why certain tweets receive unexpectedly high levels of one type of engagement relative to others. Drawing on Signaling Theory and Attention Economy Theory, we investigate these unexpected engagement patterns on Twitter (now known as "X"), developing an "unexpectedness quotient" to quantify deviations from predicted engagement levels. Our analysis of over 600,000 tweets reveals distinct patterns in how content characteristics influence unexpected engagement. News, politics, and business tweets receive more retweets and comments than expected, suggesting users prioritize sharing and discussing informational content. In contrast, games and sports-related topics garner unexpected likes and comments, indicating higher emotional investment in these domains. The relationship between content attributes and engagement types follows clear patterns: subjective tweets attract more likes while objective tweets receive more retweets, and longer, complex tweets with URLs unexpectedly receive more retweets. These findings demonstrate how users employ different engagement types as signals of varying strength based on content characteristics, and how certain content types more effectively compete for attention in the social media ecosystem. Our results offer valuable insights for content creators optimizing engagement strategies, platform designers facilitating meaningful interactions, and researchers studying online social behavior.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cycle Walk for Sampling Measures on Spanning Forests for Redistricting</title>
<link>https://arxiv.org/abs/2509.08629</link>
<guid>https://arxiv.org/abs/2509.08629</guid>
<content:encoded><![CDATA[
<div> Markov Chain, Cycle Walk, graph partitions, political districts, sampling<br />
<br />
Summary:<br />
The article introduces a new Markov Chain called the Cycle Walk, designed for sampling measures of graph partitions with equally sized elements, particularly useful in the realm of generating and evaluating political districts. Through numerical evidence, it is shown that this new chain can efficiently sample target distributions that were previously challenging for existing sampling Markov chains. This advancement in sampling techniques holds significance for various applications where partition elements need to be of comparable sizes, such as in political districting. The Cycle Walk Markov Chain offers a promising solution for effectively sampling target distributions in scenarios where traditional methods may struggle. <div>
arXiv:2509.08629v1 Announce Type: new 
Abstract: We introduce a new Markov Chain called the Cycle Walk for sampling measures of graph partitions where the partition elements have roughly equal size. Such Markov Chains are of current interest in the generation and evaluation of political districts. We present numerical evidence that this chain can efficiently sample target distributions that have been difficult for existing sampling Markov chains.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo Chambers and Information Brokers on Truth Social: A Study of Network Dynamics and Political Discourse</title>
<link>https://arxiv.org/abs/2509.08676</link>
<guid>https://arxiv.org/abs/2509.08676</guid>
<content:encoded><![CDATA[
<div> Keywords: Truth Social, political events, user interactions, polarization, central figures

Summary: 
This study analyzes the structural dynamics of the politically aligned social media platform, Truth Social, during significant political events. The research focuses on the U.S. Supreme Court's decision to overturn Roe v. Wade and the FBI's search of Mar-a-Lago. Through a dataset of user interactions based on re-truths, the study examines the network's evolution in terms of fragmentation, polarization, and user influence. The findings indicate a segmented and ideologically homogenous network dominated by a small group of central figures. Political events lead to temporary consolidation around shared narratives but quickly revert to fragmented, echo-chambered clusters. Key influencers, notably @realDonaldTrump, play a disproportionate role in shaping visibility and guiding discourse. Overall, the study sheds light on how Truth Social's infrastructure and community dynamics reinforce ideological boundaries and restrict diverse engagement. 

Summary: <div>
arXiv:2509.08676v1 Announce Type: new 
Abstract: This study examines the structural dynamics of Truth Social, a politically aligned social media platform, during two major political events: the U.S. Supreme Court's overturning of Roe v. Wade and the FBI's search of Mar-a-Lago. Using a large-scale dataset of user interactions based on re-truths (platform-native reposts), we analyze how the network evolves in relation to fragmentation, polarization, and user influence. Our findings reveal a segmented and ideologically homogenous structure dominated by a small number of central figures. Political events prompt temporary consolidation around shared narratives, followed by rapid returns to fragmented, echo-chambered clusters. Centrality metrics highlight the disproportionate role of key influencers, particularly @realDonaldTrump, in shaping visibility and directing discourse. These results contribute to research on alternative platforms, political communication, and online network behavior, demonstrating how infrastructure and community dynamics together reinforce ideological boundaries and limit cross-cutting engagement.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of geometric hypergraph embedding</title>
<link>https://arxiv.org/abs/2509.08772</link>
<guid>https://arxiv.org/abs/2509.08772</guid>
<content:encoded><![CDATA[
<div> Embedding, Hypergraph, Euclidean space, Spectral algorithms, Geometric structure  
Summary:  
- The article addresses the problem of embedding hypergraph nodes into Euclidean space using spectral algorithms based on the assumption of underlying geometric structure.  
- Two new spectral algorithms are proposed, leveraging the connection between hypergraphs and bipartite graphs to optimize the embedding through gradient descent.  
- Synthetic tests demonstrate the accuracy of the approach in revealing planted geometric structures in data, while real hypergraph tests show its usefulness in tasks such as detecting spurious or missing data and node clustering.  
- The algorithms are designed to tackle the inverse problem associated with generating geometric random hypergraphs and provide a measure of success to guide the optimization process.  
- By exploiting the assumption of interactions arising from closeness to unknown hyperedge centers, the proposed approach offers a promising solution for embedding hypergraphs for various applications.  

<br /><br />Summary:  <div>
arXiv:2509.08772v1 Announce Type: new 
Abstract: We consider the problem of embedding the nodes of a hypergraph into Euclidean space under the assumption that the interactions arose through closeness to unknown hyperedge centres. In this way, we tackle the inverse problem associated with the generation of geometric random hypergraphs. We propose two new spectral algorithms; both of these exploit the connection between hypergraphs and bipartite graphs. The assumption of an underlying geometric structure allows us to define a concrete measure of success that can be used to optimize the embedding via gradient descent. Synthetic tests show that this approach accurately reveals geometric structure that is planted in the data, and tests on real hypergraphs show that the approach is also useful for the downstream tasks of detecting spurious or missing data and node clustering.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Version: Security and Privacy Perceptions of Pakistani Facebook Matrimony Group Users</title>
<link>https://arxiv.org/abs/2509.08782</link>
<guid>https://arxiv.org/abs/2509.08782</guid>
<content:encoded><![CDATA[
<div> dating apps, censorship, Facebook matrimony groups, privacy concerns, security risks 

Summary: 
Participants in Pakistan use Facebook matrimony groups as alternatives to dating apps due to censorship. However, sharing personal information like photos and phone numbers exposes them to risks such as fraud and identity theft. Users have elevated privacy concerns, leading them to share limited information and creating mistrust among potential partners. Many are worried about profile authenticity, identity theft, harassment, and social judgment. Recommendations include stronger identity verification by group admins, stricter cybersecurity laws, clear platform guidelines for accountability, and technical enhancements like restricting screenshots and implementing anonymous chats to protect user data and build trust. <div>
arXiv:2509.08782v1 Announce Type: new 
Abstract: In Pakistan, where dating apps are subject to censorship, Facebook matrimony groups -- also referred to as marriage groups -- serve as alternative virtual spaces for members to search for potential life partners. To participate in these groups, members often share sensitive personal information such as photos, addresses, and phone numbers, which exposes them to risks such as fraud, blackmail, and identity theft. To better protect users of Facebook matrimony groups, we need to understand aspects related to user safety, such as how users perceive risks, what influences their trust in sharing personal information, and how they navigate security and privacy concerns when seeking potential partners online. In this study, through 23 semi-structured interviews, we explore how Pakistani users of Facebook matrimony groups perceive and navigate risks of sharing personal information, and how cultural norms and expectations influence their behavior in these groups.
  We find elevated privacy concerns among participants, leading them to share limited personal information and creating mistrust among potential partners. Many also expressed concerns about the authenticity of profiles and major security risks, such as identity theft, harassment, and social judgment. Our work highlights the challenges of safely navigating Facebook matrimony groups in Pakistan and offers recommendations for such as implementing stronger identity verification by group admins, enforcing stricter cybersecurity laws, clear platform guidelines to ensure accountability, and technical feature enhancements -- including restricting screenshots, picture downloads, and implementing anonymous chats -- to protect user data and build trust.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Truth: The Confidence Paradox in AI Fact-Checking</title>
<link>https://arxiv.org/abs/2509.08803</link>
<guid>https://arxiv.org/abs/2509.08803</guid>
<content:encoded><![CDATA[
<div> Misinformation, fact-checking solutions, language models, global contexts, Dunning-Kruger effect <br />
Summary: 
The study evaluates nine large language models for fact-checking effectiveness across global contexts. Findings suggest a pattern resembling the Dunning-Kruger effect, with smaller models showing high confidence but lower accuracy, and larger models demonstrating higher accuracy but lower confidence. This creates a risk of systemic bias in information verification, especially for resource-constrained organizations using smaller models. Performance gaps were most pronounced for non-English languages and claims from the Global South, potentially widening information inequalities. The study establishes a multilingual benchmark for future research and provides evidence for policy efforts aiming to ensure equitable access to trustworthy, AI-assisted fact-checking. <br /><br />Summary: <div>
arXiv:2509.08803v1 Announce Type: new 
Abstract: The rise of misinformation underscores the need for scalable and reliable fact-checking solutions. Large language models (LLMs) hold promise in automating fact verification, yet their effectiveness across global contexts remains uncertain. We systematically evaluate nine established LLMs across multiple categories (open/closed-source, multiple sizes, diverse architectures, reasoning-based) using 5,000 claims previously assessed by 174 professional fact-checking organizations across 47 languages. Our methodology tests model generalizability on claims postdating training cutoffs and four prompting strategies mirroring both citizen and professional fact-checker interactions, with over 240,000 human annotations as ground truth. Findings reveal a concerning pattern resembling the Dunning-Kruger effect: smaller, accessible models show high confidence despite lower accuracy, while larger models demonstrate higher accuracy but lower confidence. This risks systemic bias in information verification, as resource-constrained organizations typically use smaller models. Performance gaps are most pronounced for non-English languages and claims originating from the Global South, threatening to widen existing information inequalities. These results establish a multilingual benchmark for future research and provide an evidence base for policy aimed at ensuring equitable access to trustworthy, AI-assisted fact-checking.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free Elections in the Free State: Ensemble Analysis of Redistricting in New Hampshire</title>
<link>https://arxiv.org/abs/2509.07328</link>
<guid>https://arxiv.org/abs/2509.07328</guid>
<content:encoded><![CDATA[
<div> Keywords: legislative redistricting, New Hampshire, ensemble analysis, partisan outcomes, election data <br />
Summary: 
This article examines the legislative redistricting process in New Hampshire during the contentious 2020 census cycle. By utilizing an ensemble analysis of enacted districts, the study aims to provide mathematical context for evaluating claims about the redistricting maps in litigation. It operationalizes New Hampshire's redistricting rules to generate a variety of districting plans, establishing a baseline for expected districting plan behavior and assessing non-partisan justifications and geographic tradeoffs between districting criteria and partisan outcomes. The research also highlights the significance of selection and aggregation of election data in analyzing partisan symmetry measures. The findings shed light on the impact of redistricting decisions on partisan outcomes and underscore the importance of understanding the mathematical underpinnings of legislative redistricting efforts. <br /><br />Summary: <div>
arXiv:2509.07328v1 Announce Type: new 
Abstract: The process of legislative redistricting in New Hampshire, along with many other states across the country, was particularly contentious during the 2020 census cycle. In this paper we present an ensemble analysis of the enacted districts to provide mathematical context for claims made about these maps in litigation. Operationalizing the New Hampshire redistricting rules and algorithmically generating a large collection of districting plans allows us to construct a baseline for expected behavior of districting plans in the state and evaluate non-partisan justifications and geographic tradeoffs between districting criteria and partisan outcomes. In addition, our results demonstrate the impact of selection and aggregation of election data for analyzing partisan symmetry measures.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Maximization Considering Influence, Cost and Time</title>
<link>https://arxiv.org/abs/2509.07625</link>
<guid>https://arxiv.org/abs/2509.07625</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence maximization, Social network analysis, Multi-objective optimization, Evolutionary algorithm, Viral marketing

Summary:
This paper introduces a new multi-objective influence maximization problem that considers influence spread, cost efficiency, and temporal urgency simultaneously. Existing studies often overlook the interconnectedness of these factors in scenarios like viral marketing and information campaigns. The proposed evolutionary variable-length search algorithm, EVEA, effectively searches for optimal node combinations to address this gap. Empirical evidence proves the feasibility and necessity of this multi-objective problem. The EVEA algorithm outperforms baseline methods by achieving higher hypervolume and faster convergence across real-world networks. It also maintains a diverse and balanced Pareto front among influence, cost, and time objectives. This research bridges the gap in the current literature by providing a holistic approach to optimize influence, cost, and time in social network analysis applications. 

<br /><br />Summary: <div>
arXiv:2509.07625v1 Announce Type: new 
Abstract: Influence maximization has been studied for social network analysis, such as viral marketing (advertising), rumor prevention, and opinion leader identification. However, most studies neglect the interplay between influence spread, cost efficiency, and temporal urgency. In practical scenarios such as viral marketing and information campaigns, jointly optimizing Influence, Cost, and Time is essential, yet remaining largely unaddressed in current literature. To bridge the gap, this paper proposes a new multi-objective influence maximization problem that simultaneously optimizes influence, cost, and time. We show the intuitive and empirical evidence to prove the feasibility and necessity of this multi-objective problem. We also develop an evolutionary variable-length search algorithm that can effectively search for optimal node combinations. The proposed EVEA algorithm outperforms all baselines, achieving up to 19.3% higher hypervolume and 25 to 40% faster convergence across four real-world networks, while maintaining a diverse and balanced Pareto front among influence, cost, and time objectives.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Topic Projected Opinion Dynamics for Resource Allocation</title>
<link>https://arxiv.org/abs/2509.07847</link>
<guid>https://arxiv.org/abs/2509.07847</guid>
<content:encoded><![CDATA[
<div> model, opinion formation, resource allocation, multiple agents, equilibrium 

Summary: 
The paper presents a model for opinion formation on resource allocation among multiple agents with hard budget constraints. Each agent has a utility function and seeks to maximize it within their constraints. Inter-agent and inter-topic coupling is defined through social networks and resource constraints. The study demonstrates that opinions always converge to equilibrium. In cases of weak antagonistic relations, opinions converge to a unique equilibrium point. The opinion formation game is shown to be a potential game, and the equilibria of the dynamics and Nash equilibria of the game are found to be related. The unique Nash equilibrium is characterized for networks with no antagonistic relations. Simulations are provided to illustrate the results. <div>
arXiv:2509.07847v1 Announce Type: cross 
Abstract: We propose a model of opinion formation on resource allocation among multiple topics by multiple agents, who are subject to hard budget constraints. We define a utility function for each agent and then derive a projected dynamical system model of opinion evolution assuming that each agent myopically seeks to maximize its utility subject to its constraints. Inter-agent coupling arises from an undirected social network, while inter-topic coupling arises from resource constraints. We show that opinions always converge to the equilibrium set. For special networks with very weak antagonistic relations, the opinions converge to a unique equilibrium point. We further show that the underlying opinion formation game is a potential game. We relate the equilibria of the dynamics and the Nash equilibria of the game and characterize the unique Nash equilibrium for networks with no antagonistic relations. Finally, simulations illustrate our findings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilevel User Credibility Assessment in Social Networks</title>
<link>https://arxiv.org/abs/2309.13305</link>
<guid>https://arxiv.org/abs/2309.13305</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, fake news, user credibility assessment, multilevel evaluation, deep learning

Summary:
Online social networks play a crucial role in spreading both real and fake news, with many users sharing harmful content and rumors. Existing methods for user credibility assessment have limitations in their binary approach and reliance on limited features. This paper introduces a new dataset and proposes the MultiCred model for multilevel credibility assessment. The model categorizes users into different credibility tiers based on a diverse range of features extracted from their profiles, tweets, and comments. MultiCred utilizes deep language models for analyzing text and deep neural networks for processing non-textual data. Extensive experiments show that MultiCred outperforms existing approaches in terms of accuracy metrics. The code for MultiCred is openly available on GitHub for further research and development.<br /><br />Summary: <div>
arXiv:2309.13305v3 Announce Type: replace 
Abstract: Online social networks serve as major platforms for disseminating both real and fake news. Many users--intentionally or unintentionally--spread harmful content, misinformation, and rumors in domains such as politics and business. Consequently, user credibility assessment has become a prominent area of research in recent years. Most existing methods suffer from two key limitations. First, they treat credibility as a binary task, labeling users as either genuine or fake, whereas real-world applications often demand a more nuanced, multilevel evaluation. Second, they rely on only a subset of relevant features, which constrains their predictive performance. In this paper, we address the lack of a dataset suitable for multilevel credibility assessment by first devising a collection method tailored to this task. We then propose the \textit{MultiCred} model, which assigns users to one of several credibility tiers based on a rich and diverse set of features extracted from their profiles, tweets, and comments. MultiCred leverages deep language models for textual analysis and deep neural networks for non-textual data processing. Our extensive experiments demonstrate that MultiCred significantly outperforms existing approaches across multiple accuracy metrics. Our code is publicly available at https://github.com/Mohammad-Moradi/MultiCred.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Perplexity Predicts Scientific Surprise and Transformative Impact</title>
<link>https://arxiv.org/abs/2509.05591</link>
<guid>https://arxiv.org/abs/2509.05591</guid>
<content:encoded><![CDATA[
<div> surprise, deep neural networks, large language models, scientific breakthroughs, interdisciplinary engagement
<br />
Summary: 
Deep neural networks, specifically large language models, can be used to quantify surprise in scientific research. Analyzing over 2 million papers, higher perplexity scores from these models predict papers that receive more variable review ratings, longer editorial delays, and greater reviewer uncertainty. The most perplexing papers can result in significant scientific achievements or be discounted. They are published in journals with varying impact factors, receive fewer short-term citations, but generate more interdisciplinary engagement. These papers are often supported by speculative funders like DARPA and are published in prestigious venues. In contrast, in humanities research, the least surprising work is celebrated and cited more. Computational measures of linguistic surprise offer a scalable approach to identify potentially transformative research that challenges conventional scientific thinking.
<br /><br />Summary: <div>
arXiv:2509.05591v1 Announce Type: new 
Abstract: Scientific breakthroughs typically emerge through the surprising violation of established research ideas, yet quantifying surprise has remained elusive because it requires a coherent model of all contemporary scientific worldviews. Deep neural networks like large language models (LLMs) are arbitrary function approximators tuned to consistently expect the expressions and ideas on which they were trained and those semantically nearby. This suggests that as LLMs improve at generating plausible text, so the perplexity or improbability a text sequence would be generated by them should come to better predict scientific surprise and disruptive importance. Analyzing over 2 million papers across multiple disciplines published immediately following the training of 5 prominent open LLMs, here we show that higher perplexity scores systematically predict papers that receive more variable review ratings, longer editorial delays, and greater reviewer uncertainty. The most perplexing papers exhibit bimodal outcomes: disproportionately represented among the most celebrated scientific achievements and also the most discounted. High-perplexity papers tend to be published in journals with more variable impact factors and receive fewer short-term citations but in prestigious venues that bet on long-term impact. They also generate more interdisciplinary engagement portending long-term influence, and are more likely to have been supported by speculative funders like DARPA versus the NIH. Interestingly, we find the opposite pattern for humanities research, where the least surprising work is the most celebrated and cited. Our findings reveal that computational measures of corpus-wide linguistic surprise can forecast the reception and ultimate influence of scientific ideas, offering a scalable approach to recognize and generate potentially transformative research that challenge conventional scientific thinking.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spatiotemporal Adaptive Local Search Method for Tracking Congestion Propagation in Dynamic Networks</title>
<link>https://arxiv.org/abs/2509.06099</link>
<guid>https://arxiv.org/abs/2509.06099</guid>
<content:encoded><![CDATA[
<div> dynamic adjacency matrix learning, local search algorithm, congestion propagation, traffic networks, congestion mitigation

Summary:<br />
The study introduces the Spatiotemporal Adaptive Local Search (STALS) method to address challenges in traffic congestion propagation in urban areas. The method utilizes dynamic adjacency matrix learning to capture spatiotemporal relationships and a local search algorithm to identify congestion bottlenecks and propagation pathways. STALS outperforms existing methods in robustness and efficiency on benchmark networks and large-scale traffic networks in New York City, Shanghai, and Urumqi. The study demonstrates the scalability and adaptability of STALS in congestion mitigation by integrating dynamic graph learning with Geo-driven spatial analytics. The results highlight the method's ability to maintain high performance across different data granularities and spatial scales, showcasing its potential for optimizing urban sustainability and spatial accessibility.<br /> <div>
arXiv:2509.06099v1 Announce Type: new 
Abstract: Traffic congestion propagation poses significant challenges to urban sustainability, disrupting spatial accessibility. The cascading effect of traffic congestion propagation can cause large-scale disruptions to networks. Existing studies have laid a solid foundation for characterizing the cascading effects. However, they typically rely on predefined graph structures and lack adaptability to diverse data granularities. To address these limitations, we propose a spatiotemporal adaptive local search (STALS) method, which feeds the dynamically adaptive adjacency matrices into the local search algorithm to learn propagation rules. Specifically, the STALS is composed of two data-driven modules. One is a dynamic adjacency matrix learning module, which learns the spatiotemporal relationship from congestion graphs by fusing four node features. The other one is the local search module, which introduces local dominance to identify multi-scale congestion bottlenecks and search their propagation pathways. We test our method on the four benchmark networks with an average of 15,000 nodes. The STALS remains a Normalized Mutual Information (NMI) score at 0.97 and an average execution time of 27.66s, outperforming six state-of-the-art methods in robustness and efficiency. We also apply the STALS to three large-scale traffic networks in New York City, the United States, Shanghai, China, and Urumqi, China. The ablation study reveals an average modularity of 0.78 across three cities, demonstrating the spatiotemporal-scale invariance of frequencytransformed features and the spatial heterogeneity of geometric topological features. By integrating dynamic graph learning with Geo-driven spatial analytics, STALS provides a scalable tool for congestion mitigation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Such Thing as Free Brain Time: For a Pigouvian Tax on Attention Capture</title>
<link>https://arxiv.org/abs/2509.06453</link>
<guid>https://arxiv.org/abs/2509.06453</guid>
<content:encoded><![CDATA[
<div> attention economy, commodification, externalities, Pigouvian tax, digital regulation
<br />
Attention has become a valuable resource in the digital age, subject to market dynamics. This article explores the commodification of attention within the attention economy and argues for its recognition as a common good at risk of over-exploitation. Attention is not just an individual cognitive process but a collective resource susceptible to enclosure by digital platforms. Negative externalities of the attention economy include diminished agency, health issues, societal and political harms. These externalities, largely unpriced by markets, constitute a significant market failure. To address this, the article suggests a Pigouvian tax on attention capture to internalize the social cost of excessive digital engagement. Such a tax would encourage changes in platform design while protecting user autonomy. By viewing attention as a shared resource essential for human agency, health, and democracy, this article presents a new perspective on digital regulation.
<br /><br />Summary: <div>
arXiv:2509.06453v1 Announce Type: new 
Abstract: In our age of digital platforms, human attention has become a scarce and highly valuable resource, rivalrous, tradable, and increasingly subject to market dynamics. This article explores the commodification of attention within the framework of the attention economy, arguing that attention should be understood as a common good threatened by over-exploitation. Drawing from philosophical, economic, and legal perspectives, we first conceptualize attention not only as an individual cognitive process but as a collective and infrastructural phenomenon susceptible to enclosure by digital intermediaries. We then identify and analyze negative externalities of the attention economy, particularly those stemming from excessive screen time: diminished individual agency, adverse health outcomes, and societal and political harms, including democratic erosion and inequality. These harms are largely unpriced by market actors and constitute a significant market failure. In response, among a spectrum of public policy tools ranging from informational campaigns to outright restrictions, we propose a Pigouvian tax on attention capture as a promising regulatory instrument to internalize the externalities and, in particular, the social cost of compulsive digital engagement. Such a tax would incentivize structural changes in platform design while preserving user autonomy. By reclaiming attention as a shared resource vital to human agency, health, and democracy, this article contributes a novel economic and policy lens to the debate on digital regulation. Ultimately, this article advocates for a paradigm shift: from treating attention as a private, monetizable asset to protecting it as a collective resource vital for humanity.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Listener Structure Underlying K-pop's Global Success: A Large-Scale Listening Data Analysis</title>
<link>https://arxiv.org/abs/2509.06606</link>
<guid>https://arxiv.org/abs/2509.06606</guid>
<content:encoded><![CDATA[
<div> play counts, K-pop, Last.fm, genre tags, music listeners <br />
Summary:<br /> 
- K-pop has transitioned from a regionally popular genre in Asia to a global music genre with a growing fanbase worldwide.
- Analysis of Last.fm data shows a significant increase in K-pop plays between 2005 and 2019, driven by a small group of heavy listeners.
- The Gini coefficient for play counts in K-pop is higher than that of mainstream genres and other niche genres.
- User-assigned genre tags reveal that K-pop evolved from a local Asian genre to a distinct global music genre between 2005 and 2010.
- This study provides insights into how K-pop has gained recognition and popularity on a global scale, shedding light on its appeal and significance in the music industry. <br /> <div>
arXiv:2509.06606v1 Announce Type: new 
Abstract: From the mid-2000s to the 2010s, K-pop moved beyond its status as a regionally popular genre in Asia and established itself as a global music genre with enthusiastic fans around the world. However, little is known about how the vast number of music listeners across the globe have listened to and perceived K-pop. This study addresses this question by analyzing a large-scale listening dataset from Last.fm. An analysis of the distribution of play counts reveals that K-pop experienced a significant increase in plays between 2005 and 2019, largely supported by a small group of heavy listeners. The Gini coefficient in play counts is notably greater than that of existing mainstream genres and other growing niche genres. Furthermore, an analysis based on user-assigned genre tags quantitatively demonstrates that between 2005 and 2010, K-pop shed its status as a local Asian genre and established itself as a distinct music genre in its own right.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How candidates evoke identity and issues on TikTok</title>
<link>https://arxiv.org/abs/2509.05310</link>
<guid>https://arxiv.org/abs/2509.05310</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, TikTok, campaign communication, political science theories, identity, issues

Summary:
Candidates in the 2024 US Presidential Election utilized TikTok for campaign communication, focusing on identity and issues to appeal to voters. Trump predominantly attacked opponent Harris and emphasized Republican identities and issues, while Harris highlighted Democratic identities and valued issues. Both candidates mentioned identities more than issues in their posts, with a majority of posts not referencing either. The study combines TikTok posts from the Harris and Trump campaigns with survey data to analyze campaign strategies and voter responses. Findings suggest a strategic use of social media platforms for political messaging, emphasizing the importance of identity and issues in campaign communication. The study provides insights into how social media platforms shape political discourse and influence voter perceptions during election campaigns.<br /><br />Summary: <div>
arXiv:2509.05310v1 Announce Type: cross 
Abstract: Social media platforms are increasingly central to campaign communication, with both paid (advertising) and earned (organic) posts used for fundraising, mobilization, and persuasion. TikTok, and other short-form video platforms, with its short-video format and content-driven algorithms, demand unique content. We examine the final six months before the 2024 US Presidential Election to understand how major campaigns used TikTok. We frame our analysis around two political science theories. The first is the expressive (identity) model, where voters are motivated by their group memberships and candidates appeal to those identities. Alternatively, the instrumental (issues) model argues voters align with politicians advocating their key issues. We also examine how often candidates attacked opponents, reflecting literature showing attacks are common in politics. We combine two datasets: posts from the Harris and Trump campaigns on TikTok (July-November 2024) and a two-wave 2022 survey of around 1,000 respondents. Results show Trump more often disparaged Harris and emphasized identities and issues distinguishing Republicans, while Harris more often highlighted Democratic identities and valued issues. Although issues predict party ID, both candidates referenced identities more (34 percent of posts) than issues (25 percent), with most posts mentioning neither (55 percent).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19</title>
<link>https://arxiv.org/abs/2509.05322</link>
<guid>https://arxiv.org/abs/2509.05322</guid>
<content:encoded><![CDATA[
<div> compression ratio, edge-centric network measures, pruning performance, COVID-19 chest x-ray image classification, random wired neural networks

Summary:
Randomly Wired Neural Networks (RWNNs) were studied to understand how network topology affects learning efficiency and model performance, focusing on three edge-centric measures: Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC). These measures were used to compress RWNNs while maintaining performance in COVID-19 chest x-ray image classification. The study compared the pruning performance of FRC, ORC, and EBC across different network generators and analyzed the structural properties of pruned networks. Results indicated that FRC-based pruning could simplify RWNNs efficiently with comparable performance to ORC. This research offers insights into the trade-off between modular segregation and network efficiency in compressed RWNNs, highlighting the potential of FRC for effective network pruning. <br /><br />Summary: <div>
arXiv:2509.05322v1 Announce Type: cross 
Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for investigating the impact of network topology in deep learning by capturing how different connectivity patterns impact both learning efficiency and model performance. At the same time, they provide a natural framework for exploring edge-centric network measures as tools for pruning and optimization. In this study, we investigate three edge-centric network measures: Forman-Ricci curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness centrality (EBC), to compress RWNNs by selectively retaining important synapses (or edges) while pruning the rest. As a baseline, RWNNs are trained for COVID-19 chest x-ray image classification, aiming to reduce network complexity while preserving performance in terms of accuracy, specificity, and sensitivity. We extend prior work on pruning RWNN using ORC by incorporating two additional edge-centric measures, FRC and EBC, across three network generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the pruning performance of the three measures in terms of compression ratio and theoretical speedup. A central focus of our study is to evaluate whether FRC, which is computationally more efficient than ORC, can achieve comparable pruning effectiveness. Along with performance evaluation, we further investigate the structural properties of the pruned networks through modularity and global efficiency, offering insights into the trade-off between modular segregation and network efficiency in compressed RWNNs. Our results provide initial evidence that FRC-based pruning can effectively simplify RWNNs, offering significant computational advantages while maintaining performance comparable to ORC.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning</title>
<link>https://arxiv.org/abs/2509.05362</link>
<guid>https://arxiv.org/abs/2509.05362</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time social engineering, scams, privacy-preserving, AI-in-the-loop, federated learning

Summary:
This paper introduces a privacy-preserving, AI-in-the-loop framework for proactively detecting and disrupting scam conversations in real time. By combining instruction-tuned artificial intelligence with a safety-aware utility function, the system effectively balances engagement with harm minimization. Additionally, federated learning enables continual model updates without sharing raw data. Experimental evaluations show that the system generates fluent and engaging responses with low perplexity and high engagement levels. Human studies confirm the realism, safety, and effectiveness of the framework over existing baselines. Moreover, models trained with federated learning sustain high engagement and relevance while maintaining low personal information leakage. The evaluation of guard models indicates a trade-off between privacy risk and scam detection effectiveness, with stricter moderation settings limiting engagement but reducing privacy risk. This framework represents a novel approach that integrates real-time scam detection, federated privacy preservation, and calibrated safety moderation to create a proactive defense paradigm.<br /><br />Summary: <div>
arXiv:2509.05362v1 Announce Type: cross 
Abstract: Scams exploiting real-time social engineering -- such as phishing, impersonation, and phone fraud -- remain a persistent and evolving threat across digital platforms. Existing defenses are largely reactive, offering limited protection during active interactions. We propose a privacy-preserving, AI-in-the-loop framework that proactively detects and disrupts scam conversations in real time. The system combines instruction-tuned artificial intelligence with a safety-aware utility function that balances engagement with harm minimization, and employs federated learning to enable continual model updates without raw data sharing. Experimental evaluations show that the system produces fluent and engaging responses (perplexity as low as 22.3, engagement $\approx$0.80), while human studies confirm significant gains in realism, safety, and effectiveness over strong baselines. In federated settings, models trained with FedAvg sustain up to 30 rounds while preserving high engagement ($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage ($\leq$0.0085). Even with differential privacy, novelty and safety remain stable, indicating that robust privacy can be achieved without sacrificing performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3, MD-Judge) shows a straightforward pattern: stricter moderation settings reduce the chance of exposing personal information, but they also limit how much the model engages in conversation. In contrast, more relaxed settings allow longer and richer interactions, which improve scam detection, but at the cost of higher privacy risk. To our knowledge, this is the first framework to unify real-time scam-baiting, federated privacy preservation, and calibrated safety moderation into a proactive defense paradigm.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Hierarchical Networks and the Law of Functional Evolution: A Universal Framework for Complex Systems</title>
<link>https://arxiv.org/abs/2509.05567</link>
<guid>https://arxiv.org/abs/2509.05567</guid>
<content:encoded><![CDATA[
<div> Keywords: Recursive Hierarchical Network, evolution, complex systems, functional evolution, intelligent systems

Summary:
The article introduces the Recursive Hierarchical Network (RHN) framework for understanding and predicting the evolution of complex systems. It proposes a recursive encapsulation model involving nodes, modules, systems, and new nodes, with a progression from structure-dominated to regulation-dominated to intelligence-dominated stages. The law of functional evolution is formalized and proven, showing irreversible progression across different systems. Empirical analysis aligns life, cosmic, informational, and social systems on a scale of functional levels, revealing strong cross-system similarities. Trajectories are strictly monotonic, with high pairwise cosine similarities and robust stage resonance. Current system states are identified, and future transitions are projected. RHN offers a mathematically rigorous, multi-scale framework for reconstructing and predicting system evolution, providing theoretical guidance for developing next-generation intelligent systems. 

<br /><br />Summary: <div>
arXiv:2509.05567v1 Announce Type: cross 
Abstract: Understanding and predicting the evolution of across complex systems remains a fundamental challenge due to the absence of unified and computationally testable frameworks. Here we propose the Recursive Hierarchical Network(RHN), conceptualizing evolution as recursive encapsulation along a trajectory of node $\to$ module $\to$ system $\to$ new node, governed by gradual accumulation and abrupt transition. Theoretically, we formalize and prove the law of functional evolution, revealing an irreversible progression from structure-dominated to regulation-dominated to intelligence-dominated stages. Empirically, we operationalize functional levels and align life, cosmic, informational, and social systems onto this scale. The resulting trajectories are strictly monotonic and exhibit strong cross-system similarity, with high pairwise cosine similarities and robust stage resonance. We locate current system states and project future transitions. RHN provides a mathematically rigorous, multi-scale framework for reconstructing and predicting system evolution, offering theoretical guidance for designing next-generation intelligent systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stack Overflow Is Not Dead Yet: Crowd Answers Still Matter</title>
<link>https://arxiv.org/abs/2509.05879</link>
<guid>https://arxiv.org/abs/2509.05879</guid>
<content:encoded><![CDATA[
<div> ChatGPT, Stack Overflow, user contributions, programming help, user retention <br />
Summary:
The research paper examines the impact of ChatGPT on user-generated content on Stack Overflow after its introduction in November 2022. It identifies a decline in user contributions on the platform but notes a positive effect of ChatGPT on the length and difficulty of user questions and code examples. The study analyzes two years of data and finds that ChatGPT has led to an increase in question and answer length, code length, and question difficulty across different programming languages. This suggests that ChatGPT has raised the standard of questions on Stack Overflow, pushing users towards more complex and challenging problems. The results contribute to understanding the influence of advanced tools like ChatGPT on programming help-seeking and collaborative knowledge creation. The insights provided can help platform operators manage information effectively and enhance user retention post-ChatGPT launch. <br /><br /> <div>
arXiv:2509.05879v1 Announce Type: cross 
Abstract: Millions of users visit Stack Overflow regularly to ask community for answers to their programming questions. However, like many other platforms, Stack Overflow consistently struggles with low user retention and declining levels of user contributions to the platform. With the introduction of ChatGPT in November 2022, these ongoing difficulties on Stack Overflow were further magnified, as many users moved toward ChatGPT for programming help. In this paper, we build upon recent research on this phenomenon by analyzing the transformation of user-generated content on Stack Overflow during the post-ChatGPT period. Specifically, we analyze two years of Stack Overflow data and fit multiple causal regression models to estimate the effect of ChatGPT on the length and difficulty of user questions and code examples. We confirm an acceleration of decline in user contributions but find that ChatGPT had a significant positive effect on question and answer length, code length, and question difficulty on Stack Overflow across programming languages. Our results suggest that ChatGPT has effectively raised the bar for questions on Stack Overflow, as users increasingly turn to crowdsourced platforms for help with more complex and challenging problems. With our work we contribute to the ongoing discussion on the impact of tools such as ChatGPT on help-seeking in programming and, more broadly, on collaborative knowledge creation. Our results provide actionable insights for platform operators to support information management and user retention in the aftermath of ChatGPT's launch.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Productivity Gaps: Temporal Patterns of Gender Differences in Scientific Knowledge Creation</title>
<link>https://arxiv.org/abs/2509.06206</link>
<guid>https://arxiv.org/abs/2509.06206</guid>
<content:encoded><![CDATA[
<div> knowledge creation, gender differences, temporal dynamics, scientific careers, bibliometric data

Summary:
Female scientists exhibit higher stability in knowledge production but also greater year-to-year volatility, showcasing a paradox in career dynamics. They excel in persisting under moderate performance expectations but struggle with sustained peak performance demands. However, these patterns vary across disciplines, with humanities and social sciences showing stronger female advantages compared to STEM fields. Using a multi-dimensional framework, this study analyzes gender disparities in scientific careers through stability, volatility, and persistence dimensions. Data from 62.5 million authors starting their careers between 1960-2010 is used to construct knowledge creation capability measures, shedding light on the temporal dynamics underlying gender inequality in scientific knowledge creation. Female scientists' performance is nuanced, highlighting the complex nature of gender differences in scientific careers across various academic fields. 

<br /><br />Summary: <div>
arXiv:2509.06206v1 Announce Type: cross 
Abstract: Gender inequality in scientific careers has been extensively documented through aggregate measures such as total publications and cumulative citations, yet the temporal dynamics underlying these disparities remain largely unexplored. Here we developed a multi-dimensional framework to examine gender differences in scientific knowledge creation through three complementary temporal dimensions: stability (consistency of performance over time), volatility (degree of year-to-year fluctuation), and persistence (ability to maintain high performance for extended periods). Using comprehensive bibliometric data from SciSciNet covering 62.5 million authors whose careers began between 1960-2010, we constructed knowledge creation capability measures that captured how scientists absorb knowledge from diverse sources and contribute to field advancement. We found that female scientists demonstrated significantly higher knowledge production stability (0.170 vs. 0.119 for males) while simultaneously exhibiting greater year-to-year volatility (6.606 vs. 6.228), revealing a striking paradox in career dynamics. Female scientists showed persistence advantages under moderate performance requirements but faced disadvantages under extreme criteria demanding sustained peak performance. However, these patterns varied substantially across disciplines, with female advantages strongest in humanities and social sciences while STEM fields show mixed results.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergy, not size: How collaboration architecture shapes scientific disruption</title>
<link>https://arxiv.org/abs/2509.06212</link>
<guid>https://arxiv.org/abs/2509.06212</guid>
<content:encoded><![CDATA[
<div> collaboration, synergy factor, disruption, team composition, knowledge production<br />
<br />
Summary: 
The study analyzes the impact of collaboration on scientific innovation across 19 disciplines from 1960 to 2020, introducing the concept of the synergy factor to measure collaboration dynamics. It finds that Physics benefits from medium team sizes, while the humanities achieve optimal synergy through individual scholarship. Collaborative synergy, rather than team size alone, mediates a significant portion of the relationship between team composition and disruption. Papers featuring key authors exhibit higher levels of disruption, with exceptional researchers driving a 561% increase in disruptive potential. High-citation authors are found to decrease disruptive potential, whereas breakthrough track records enhance it, challenging traditional evaluation metrics. The study identifies four modes of knowledge production, highlighting the heterogeneity in optimal collaboration strategies across disciplines and offering evidence-based guidance for research organization and science policy development. <div>
arXiv:2509.06212v1 Announce Type: cross 
Abstract: The mechanisms driving different types of scientific innovation through collaboration remain poorly understood. Here we develop a comprehensive framework analyzing over 14 million papers across 19 disciplines from 1960 to 2020 to unpack how collaborative synergy shapes research disruption. We introduce the synergy factor to quantify collaboration cost-benefit dynamics, revealing discipline-specific architectures where Physics peaks at medium team sizes while humanities achieve maximal synergy through individual scholarship. Our mediation analysis demonstrates that collaborative synergy, not team size alone, mediates 75% of the relationship between team composition and disruption. Key authors play a catalytic role, with papers featuring exceptional researchers showing 561% higher disruption indices. Surprisingly, high-citation authors reduce disruptive potential while those with breakthrough track records enhance it, challenging traditional evaluation metrics. We identify four distinct knowledge production modes: elite-driven, baseline, heterogeneity-driven, and low-cost. These findings reveal substantial heterogeneity in optimal collaboration strategies across disciplines and provide evidence-based guidance for research organization, with implications for science policy and the design of research institutions in an increasingly collaborative scientific landscape.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features</title>
<link>https://arxiv.org/abs/2311.01661</link>
<guid>https://arxiv.org/abs/2311.01661</guid>
<content:encoded><![CDATA[
<div> deep learning, community resilience, socio-technical systems, urban development, machine intelligence

Summary:<br /><br />Community resilience is a complex and multifaceted phenomenon shaped by interactions within socio-technical systems. Current studies primarily focus on vulnerability assessment using index-based approaches, lacking the ability to capture the heterogeneous features and nonlinear interactions that influence resilience components. To address this gap, a three-layer deep learning model called Resili-Net is introduced. This model evaluates 12 measurable resilience features within community socio-technical systems related to robustness, redundancy, and resourcefulness. By utilizing public data from various U.S. metropolitan areas, Resili-Net categorizes resilience levels into five distinct levels, providing insights into resilience determinants and enhancement strategies. The model's interpretability allows for analyzing resilience profiles under different urban development patterns, offering unique perspectives on community resilience assessment through the integration of machine intelligence and urban big data. <div>
arXiv:2311.01661v2 Announce Type: replace 
Abstract: Community resilience is a complex and muti-faceted phenomenon that emerges from complex and nonlinear interactions among different socio-technical systems and their resilience properties. However, present studies on community resilience focus primarily on vulnerability assessment and utilize index-based approaches, with limited ability to capture heterogeneous features within community socio-technical systems and their nonlinear interactions in shaping robustness, redundancy, and resourcefulness components of resilience. To address this gap, this paper presents an integrated three-layer deep learning model for community resilience rating (called Resili-Net). Twelve measurable resilience features are specified and computed within community socio-technical systems (i.e., facilities, infrastructures, and society) related to three resilience components of robustness, redundancy, and resourcefulness. Using publicly accessible data from multiple metropolitan statistical areas in the United States, Resili-Net characterizes the resilience levels of spatial areas into five distinct levels. The interpretability of the model outcomes enables feature analysis for specifying the determinants of resilience in areas within each resilience level, allowing for the identification of specific resilience enhancement strategies. Changes in community resilience profiles under urban development patterns are further examined by changing the value of related socio-technical systems features. Accordingly, the outcomes provide novel perspectives for community resilience assessment by harnessing machine intelligence and heterogeneous urban big data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Destabilizing a Social Network Model via Intrinsic Feedback Vulnerabilities</title>
<link>https://arxiv.org/abs/2411.10868</link>
<guid>https://arxiv.org/abs/2411.10868</guid>
<content:encoded><![CDATA[
<div> social influence, generative AI, radicalization, social network, perturbations 
<br />Summary: 
The study investigates the impact of intentional perturbations on a social network using Taylor's model of social influence and robust control theory. By identifying subtle yet effective changes to the network structure, the research demonstrates how small alterations can lead to radicalization of all agents. The findings highlight the potential for significant shifts in collective behavior triggered by minimal adjustments in social influence. The study emphasizes the importance of analyzing real systems to identify existing dynamics and potential vulnerabilities within social networks. <div>
arXiv:2411.10868v5 Announce Type: replace 
Abstract: Social influence plays a significant role in shaping individual sentiments and actions, particularly in a world of ubiquitous digital interconnection. The rapid development of generative AI has engendered well-founded concerns regarding the potential scalable implementation of radicalization techniques in social media. Motivated by these developments, we present a case study investigating the effects of small but intentional perturbations on a simple social network. We employ Taylor's classic model of social influence and tools from robust control theory (most notably the Dynamical Structure Function (DSF)), to identify perturbations that qualitatively alter the system's behavior while remaining as unobtrusive as possible. We examine two such scenarios: perturbations to an existing link and perturbations that introduce a new link to the network. In each case, we identify destabilizing perturbations of minimal norm and simulate their effects. Remarkably, we find that small but targeted alterations to network structure may lead to the radicalization of all agents, exhibiting the potential for large-scale shifts in collective behavior to be triggered by comparatively minuscule adjustments in social influence. Given that this method of identifying perturbations that are innocuous yet destabilizing applies to any suitable dynamical system, our findings emphasize a need for similar analyses to be carried out on real systems (e.g., real social networks), to identify the places where such dynamics may already exist.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Right to Hide: Masking Community Affiliation via Minimal Graph Rewiring</title>
<link>https://arxiv.org/abs/2502.00432</link>
<guid>https://arxiv.org/abs/2502.00432</guid>
<content:encoded><![CDATA[
<div> community membership hiding, social graph, privacy, graph topology, node hiding

Summary:
The study addresses the challenge of protecting privacy in social graphs by concealing nodes' membership in sensitive communities while maintaining the underlying graph topology. The approach involves strategically modifying the graph structure to hide a target node's community affiliation from detection algorithms. The researchers introduce a new gradient-based method, $\nabla$-CMH, which minimizes structural changes within a defined modification budget to effectively hide community membership. Extensive experiments on various datasets and community detection methods show that the technique outperforms existing approaches by achieving a balance between node hiding effectiveness and graph rewiring cost while ensuring computational efficiency. <div>
arXiv:2502.00432v2 Announce Type: replace 
Abstract: Protecting privacy in social graphs may require obscuring nodes' membership in sensitive communities. However, doing so without significantly disrupting the underlying graph topology remains a key challenge. In this work, we address the community membership hiding problem, which involves strategically modifying the graph structure to conceal a target node's affiliation with a community, regardless of the detection algorithm used. We reformulate the original discrete, counterfactual graph search objective as a differentiable constrained optimisation task. To this end, we introduce $\nabla$-CMH, a new gradient-based method that operates within a feasible modification budget to minimise structural changes while effectively hiding a node's community membership. Extensive experiments on multiple datasets and community detection methods demonstrate that our technique outperforms existing baselines, achieving the best balance between node hiding effectiveness and graph rewiring cost, while preserving computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Hourly Neighborhood Population Using Mobile Phone Data in the United States</title>
<link>https://arxiv.org/abs/2504.01170</link>
<guid>https://arxiv.org/abs/2504.01170</guid>
<content:encoded><![CDATA[
<div> Keywords: population estimation, human mobility data, spatiotemporal dynamics, urban and rural movements, high resolution

Summary: 
Traditional population estimation methods often do not capture the dynamic fluctuations in urban and rural population movements. This study proposes a novel method that utilizes smartphone-based human mobility data to reconstruct hourly population estimates for neighborhoods across the US. By analyzing population fluctuations on an hourly, diurnal, daily, and seasonal basis, the study reveals the limitations of static population data in capturing temporal dynamics. This high spatiotemporal resolution dataset provides valuable insights for various studies, including air pollution exposure analysis and emergency response planning. This study represents one of the first attempts to create hourly population estimates on a large geographic scale, revolutionizing the way we understand and analyze dynamic populations.  <br /><br />Summary: <div>
arXiv:2504.01170v2 Announce Type: replace 
Abstract: Traditional population estimation techniques often fail to capture the dynamic fluctuations inherent in urban and rural population movements. Recognizing the need for a high spatiotemporal dynamic population dataset, we propose a method using smartphone-based human mobility data to reconstruct the hourly population for each neighborhood across the US. We quantify population fluctuations on an hourly, diurnal, daily, and seasonal basis, and compare these with static population data to highlight the limitations of traditional models in capturing temporal dynamics. This study is one of the first hourly population products at a large geographic extent (US), contributing to various studies that involve dynamic populations with high spatiotemporal resolution, such as air pollution exposure analysis and emergency response.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanNews: a Network-aware Fake News Mitigation Architecture for Social Media</title>
<link>https://arxiv.org/abs/2509.04489</link>
<guid>https://arxiv.org/abs/2509.04489</guid>
<content:encoded><![CDATA[
<div> Fake news detection, social media, deep learning, LSTM, GRU

Summary:
CleanNews is a new architecture proposed to accurately identify and combat fake news in real-time on social media platforms. It utilizes advanced deep learning techniques, including convolutional and bidirectional recurrent neural networks such as LSTM and GRU layers, to detect misinformation. A novel embedding technique that combines textual information with user network structure is employed, allowing the model to learn linguistic and relational cues associated with fake news. Additionally, two network immunization algorithms, SparseShield and NetShield, are used to prevent the spread of false information within networks. Extensive ablation studies and hyperparameter tuning are conducted to maximize performance. Experimental evaluation on real-world datasets demonstrates the effectiveness of CleanNews in addressing the dissemination of fake news. 

<br /><br />Summary: <div>
arXiv:2509.04489v1 Announce Type: new 
Abstract: With the widespread use of the internet and handheld devices, social media now holds a power similar to that of old newspapers. People use social media platforms for quick and accessible information. However, this convenience comes with a variety of risks. Anyone can freely post content, true or false, with the probability of remaining online forever. This makes it crucial to identify and tackle misinformation and disinformation on online platforms. In this article, we propose CleanNews, a comprehensive architecture to identify fake news in real-time accurately. CleanNews uses advanced deep learning architectures, combining convolutional and bidirectional recurrent neural networks, i.e., LSTM and GRU, layers to detect fake news. A key contribution of our work is a novel embedding technique that fuses textual information with user network structure, allowing the model to jointly learn linguistic and relational cues associated with misinformation. Furthermore, we use two network immunization algorithms, i.e., SparseShield and NetShield, to mitigate the spread of false information within networks. We conduct extensive ablation studies to evaluate the contribution of each model component and systematically tune hyperparameters to maximize performance. The experimental evaluation on two real-world datasets shows the efficacy of CleanNews in combating the spread of fake news.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThumbnailTruth: A Multi-Modal LLM Approach for Detecting Misleading YouTube Thumbnails Across Diverse Cultural Settings</title>
<link>https://arxiv.org/abs/2509.04714</link>
<guid>https://arxiv.org/abs/2509.04714</guid>
<content:encoded><![CDATA[
<div> LLMs, misleading thumbnails, detection pipeline, video platforms, content integrity <br />
<br />
Summary: This paper presents a novel multi-modal detection pipeline using Large Language Models (LLMs) to identify misleading video thumbnails on platforms like YouTube. A dataset of 2,843 videos from eight countries was analyzed, including 1,359 misleading thumbnails with over 7.6 billion views. The pipeline integrates video-to-text descriptions, thumbnail images, and subtitle transcripts for comprehensive analysis. State-of-the-art LLMs such as GPT-40, GPT-40 Mini, Claude 3.5 Sonnet, and Gemini-1.5 Flash were evaluated, with Claude 3.5 Sonnet performing well, achieving high accuracy, precision, and recall rates. The study emphasizes the importance of transparent and trustworthy video platforms, highlighting the implications for content moderation, user experience, and ethical considerations when deploying detection systems at scale. Through these findings, the research aims to enhance content integrity and user trust on video platforms globally. <br /> <div>
arXiv:2509.04714v1 Announce Type: new 
Abstract: Misleading video thumbnails on platforms like YouTube are a pervasive problem, undermining user trust and platform integrity. This paper proposes a novel multi-modal detection pipeline that uses Large Language Models (LLMs) to flag misleading thumbnails. We first construct a comprehensive dataset of 2,843 videos from eight countries, including 1,359 misleading thumbnail videos that collectively amassed over 7.6 billion views -- providing a unique cross-cultural perspective on this global issue. Our detection pipeline integrates video-to-text descriptions, thumbnail images, and subtitle transcripts to holistically analyze content and flag misleading thumbnails. Through extensive experimentation and prompt engineering, we evaluate the performance of state-of-the-art LLMs, including GPT-4o, GPT-4o Mini, Claude 3.5 Sonnet, and Gemini-1.5 Flash. Our findings show the effectiveness of LLMs in identifying misleading thumbnails, with Claude 3.5 Sonnet consistently showing strong performance, achieving an accuracy of 93.8\%, precision over 92\%, and recall exceeding 94\% in certain scenarios. We discuss the implications of our findings for content moderation, user experience, and the ethical considerations of deploying such systems at scale. Our findings pave the way for more transparent, trustworthy video platforms and stronger content integrity for audiences worldwide.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Cognitive-Behavioral Fixation via Multimodal User Viewing Patterns on Social Media</title>
<link>https://arxiv.org/abs/2509.04823</link>
<guid>https://arxiv.org/abs/2509.04823</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, cognitive-behavioral fixation, multimodal, topic extraction, user behavior

Summary:
This article introduces a novel framework for assessing cognitive-behavioral fixation on digital social media platforms. The framework involves analyzing users' multimodal social media engagement patterns using a combination of topic extraction and fixation quantification modules. The proposed approach aims to provide adaptive, hierarchical, and interpretable evaluation of user behavior, filling a gap in computational methods for detecting fixation in psychology. Experiments on existing benchmarks and a newly curated multimodal dataset demonstrate the effectiveness of the framework. The code for this project is publicly available for research purposes on GitHub, enabling scalable computational analysis of cognitive fixation.<br /><br />Summary: <div>
arXiv:2509.04823v1 Announce Type: new 
Abstract: Digital social media platforms frequently contribute to cognitive-behavioral fixation, a phenomenon in which users exhibit sustained and repetitive engagement with narrow content domains. While cognitive-behavioral fixation has been extensively studied in psychology, methods for computationally detecting and evaluating such fixation remain underexplored. To address this gap, we propose a novel framework for assessing cognitive-behavioral fixation by analyzing users' multimodal social media engagement patterns. Specifically, we introduce a multimodal topic extraction module and a cognitive-behavioral fixation quantification module that collaboratively enable adaptive, hierarchical, and interpretable assessment of user behavior. Experiments on existing benchmarks and a newly curated multimodal dataset demonstrate the effectiveness of our approach, laying the groundwork for scalable computational analysis of cognitive fixation. All code in this project is publicly available for research purposes at https://github.com/Liskie/cognitive-fixation-evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media</title>
<link>https://arxiv.org/abs/2509.04461</link>
<guid>https://arxiv.org/abs/2509.04461</guid>
<content:encoded><![CDATA[
<div> Keywords: Personality prediction, social media posts, Myers Briggs Type Indicator, Large Language Models, class imbalance

Summary: 
PostToPersonality (PtoP) is a novel framework for predicting personality types from social media posts using Large Language Models (LLMs). PtoP addresses two key challenges faced by LLMs in MBTI prediction: the hallucination problem and class imbalance. By utilizing Retrieval Augmented Generation and in context learning, PtoP mitigates hallucination in LLMs. Additionally, the framework fine-tunes a pretrained LLM with synthetic minority oversampling to balance the naturally imbalanced distribution of MBTI types in the population. Experiments on real-world social media data show that PtoP outperforms 10 traditional machine learning and deep learning baselines in MBTI prediction. PtoP not only demonstrates state-of-the-art performance but also showcases the potential of leveraging LLMs for understanding personality traits from social media content. 

<br /><br />Summary: <div>
arXiv:2509.04461v1 Announce Type: cross 
Abstract: Personality prediction from social media posts is a critical task that implies diverse applications in psychology and sociology. The Myers Briggs Type Indicator (MBTI), a popular personality inventory, has been traditionally predicted by machine learning (ML) and deep learning (DL) techniques. Recently, the success of Large Language Models (LLMs) has revealed their huge potential in understanding and inferring personality traits from social media content. However, directly exploiting LLMs for MBTI prediction faces two key challenges: the hallucination problem inherent in LLMs and the naturally imbalanced distribution of MBTI types in the population. In this paper, we propose PostToPersonality (PtoP), a novel LLM based framework for MBTI prediction from social media posts of individuals. Specifically, PtoP leverages Retrieval Augmented Generation with in context learning to mitigate hallucination in LLMs. Furthermore, we fine tune a pretrained LLM to improve model specification in MBTI understanding with synthetic minority oversampling, which balances the class imbalance by generating synthetic samples. Experiments conducted on a real world social media dataset demonstrate that PtoP achieves state of the art performance compared with 10 ML and DL baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Classroom Interaction Data Using Prompt Engineering and Network Analysis</title>
<link>https://arxiv.org/abs/2501.18912</link>
<guid>https://arxiv.org/abs/2501.18912</guid>
<content:encoded><![CDATA[
<div> Keywords: classroom interactions, prompt engineering, network analysis, interaction patterns, gender gap

Summary:
The study focuses on the analysis of classroom interactions to enhance learning outcomes. It introduces a framework that combines prompt engineering and network analysis to examine complex conversational data. The framework automates utterance classification through prompt engineering, enabling efficient dialogue analysis without pre-labeled datasets. The interactions are then transformed into network representations, allowing for the analysis of classroom dynamics as structured social networks. The study utilizes network mediation analysis to uncover complex interaction patterns and understand how underlying interaction structures impact student learning. In particular, the study investigates how the gender gap in mathematics performance may be mediated by students' classroom interaction structures. The research aims to improve educational practices by providing insights into the dynamics of classroom interactions and their impact on student outcomes.
<br /><br />Summary: <div>
arXiv:2501.18912v2 Announce Type: replace-cross 
Abstract: Classroom interactions play a vital role in developing critical thinking, collaborative problem-solving abilities, and enhanced learning outcomes. While analyzing these interactions is crucial for improving educational practices, the examination of classroom dialogues presents significant challenges due to the complexity and high-dimensionality of conversational data. This study presents an integrated framework that combines prompt engineering with network analysis to investigate classroom interactions comprehensively. Our approach automates utterance classification through prompt engineering, enabling efficient and scalable dialogue analysis without requiring pre-labeled datasets. The classified interactions are subsequently transformed into network representations, facilitating the analysis of classroom dynamics as structured social networks. To uncover complex interaction patterns and how underlying interaction structures relate to student learning, we utilize network mediation analysis. In this approach, latent interaction structures, derived from the additive and multiplicative effects network (AMEN) model that places students within a latent social space, act as mediators. In particular, we investigate how the gender gap in mathematics performance may be mediated by students' classroom interaction structures.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias Model</title>
<link>https://arxiv.org/abs/2509.03832</link>
<guid>https://arxiv.org/abs/2509.03832</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, echo chambers, confirmation bias, gravity well model, misinformation

Summary: 
The study focuses on the impact of confirmation bias in social media echo chambers and introduces a dynamic confirmation bias variable to the existing gravity well model. This variable adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content, improving the identification of echo chambers. By analyzing posting history and responses to various viewpoints, the model can better detect echo chambers in online communities. Validation on nineteen Reddit communities showed enhanced capabilities in identifying these high-risk environments. This framework allows for a systematic approach to understanding confirmation bias in online group dynamics, aiding in the efforts to combat the spread of misinformation at its core points of amplification. <div>
arXiv:2509.03832v1 Announce Type: new 
Abstract: Social media echo chambers play a central role in the spread of misinformation, yet existing models often overlook the influence of individual confirmation bias. An existing model of echo chambers is the "gravity well" model, which creates an analog between echo chambers and spatial gravity wells. We extend this established model by introducing a dynamic confirmation bias variable that adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content. This variable is calculated for each user through comparisons between their posting history and their responses to posts of a wide range of viewpoints.
  Incorporating this factor produces a confirmation-bias-integrated gravity well model that more accurately identifies echo chambers and reveals community-level markers of information health. We validated the approach on nineteen Reddit communities, demonstrating improved detection of echo chambers.
  Our contribution is a framework for systematically capturing the role of confirmation bias in online group dynamics, enabling more effective identification of echo chambers. By flagging these high-risk environments, the model supports efforts to curb the spread of misinformation at its most common points of amplification.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Analysis of Dissent and Self-Censorship</title>
<link>https://arxiv.org/abs/2509.03731</link>
<guid>https://arxiv.org/abs/2509.03731</guid>
<content:encoded><![CDATA[
<div> surveillance, censorship, dissent, authority, self-censorship
<br />
Summary:
The article discusses the dynamics of dissent and self-censorship in societies, particularly in the context of digital communications and authoritarian regimes. It introduces a model where individuals weigh the risks of expressing dissent against the consequences of punishment, while the authority adjusts its policies to suppress dissent. The study reveals that there is a strategic tradeoff between voicing dissent and avoiding punishment, leading to individuals either defiantly expressing dissent or self-censoring to comply with authority. The model highlights how the population's willingness to endure punishment influences the authority's choice of policy, ultimately affecting the extent and duration of dissent suppression. Additionally, the research demonstrates that any population can be led to total self-censorship by an authority's policies, but early resistance to punishment can deter the adoption of more extreme measures. 
<br /> 
Summary: <div>
arXiv:2509.03731v1 Announce Type: cross 
Abstract: Expressions of dissent against authority are an important feature of most societies, and efforts to suppress such expressions are common. Modern digital communications, social media, and Internet surveillance and censorship technologies are changing the landscape of public speech and dissent. Especially in authoritarian settings, individuals must assess the risk of voicing their true opinions or choose self-censorship, voluntarily moderating their behavior to comply with authority. We present a model in which individuals strategically manage the tradeoff between expressing dissent and avoiding punishment through self-censorship while an authority adapts its policies to minimize both total expressed dissent and punishment costs. We study the model analytically and in simulation to derive conditions separating defiant individuals who express their desired dissent in spite of punishment from self-censoring individuals who fully or partially limit their expression. We find that for any population, there exists an authority policy that leads to total self-censorship. However, the probability and time for an initially moderate, locally-adaptive authority to suppress dissent depend critically on the population's willingness to withstand punishment early on, which can deter the authority from adopting more extreme policies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Low Frequency Tweeters Have More to Say!" A New Approach to Identify Importance of Tweets</title>
<link>https://arxiv.org/abs/2509.03931</link>
<guid>https://arxiv.org/abs/2509.03931</guid>
<content:encoded><![CDATA[
<div> Keywords: Twitter, social media, information overload, tweet importance, tweet frequency

Summary:
This study focuses on addressing the issue of information overload on Twitter by personalizing tweet rankings based on the relationship between tweet importance and tweet frequency. The hypothesis tested suggests that infrequent tweeters may have more important messages to convey. Six new measures are proposed to evaluate tweet importance, including metrics like retweets, favorites, and comments. The study findings indicate that users who tweet less frequently tend to have more important messages according to their followers. This identified tweet-frequency band can be used to reorder user activity feeds, preventing important messages from getting lost. Additionally, it can serve as a scoring index to recognize users who frequently share significant messages on Twitter. <div>
arXiv:2509.03931v1 Announce Type: cross 
Abstract: Twitter is one of the most popular social media platforms.With a large number of tweets, the activity feed of users becomes noisy, challenging to read, and most importantly tweets often get lost. We present a new approach to personalise the ranking of the tweets toward solving the problem of information overload which is achieved by analysing the relationship between the importance of tweets to the frequency at which the author tweets. The hypothesis tested is that "low-frequency tweeters have more to say", i.e. if a user who tweets infrequently actually goes to the effort of tweeting, then it is more likely to be of more importance or contain more "meaning" than a tweet by a user who tweets continuously. We propose six new measures to evaluate the importance of tweets based on the ability of the tweet to drive interaction among its readers, which is measured through metrics such as retweets, favourites, and comments, and the extent of the author's network interacting with the tweet. Our study shows that users who tweeted less than ten tweets per week were more likely to be perceived as important by their followers and have the most important messages. This identified tweet-frequency band could be used to reorder the activity feed of users and such reordering would ensure the messages of low-frequency tweeters do not get lost in the stream of tweets. This could also serve as a scoring index for Twitter users to identify users frequently tweeting important messages.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit and Implicit Data Augmentation for Social Event Detection</title>
<link>https://arxiv.org/abs/2509.04202</link>
<guid>https://arxiv.org/abs/2509.04202</guid>
<content:encoded><![CDATA[
<div> Keywords: social event detection, augmentation framework, data diversity, model robustness, perturbation techniques

Summary:
The study introduces an Augmentation framework for Social Event Detection (SED-Aug) to improve the accuracy of identifying and categorizing important events from social media. The framework combines explicit text-based augmentation using large language models with implicit feature-space augmentation employing novel perturbation techniques on structural fused embeddings. This approach enhances data diversity and model robustness, resulting in a significant performance improvement over baseline models. SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score. The code for SED-Aug is available on GitHub for further exploration and implementation. The framework showcases the effectiveness of combining explicit text-based augmentation with implicit feature-space perturbations for enhancing social event detection accuracy. 

<br /><br />Summary: <div>
arXiv:2509.04202v1 Announce Type: cross 
Abstract: Social event detection involves identifying and categorizing important events from social media, which relies on labeled data, but annotation is costly and labor-intensive. To address this problem, we propose Augmentation framework for Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework, which combines explicit text-based and implicit feature-space augmentation to enhance data diversity and model robustness. The explicit augmentation utilizes large language models to enhance textual information through five diverse generation strategies. For implicit augmentation, we design five novel perturbation techniques that operate in the feature space on structural fused embeddings. These perturbations are crafted to keep the semantic and relational properties of the embeddings and make them more diverse. Specifically, SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Graph Structural Learning Beyond Homophily via Preserving Neighbor Similarity</title>
<link>https://arxiv.org/abs/2401.09754</link>
<guid>https://arxiv.org/abs/2401.09754</guid>
<content:encoded><![CDATA[
<div> Robust Models, Adversarial Attacks, Graph-based Learning Systems, Homophilic Graphs, Vulnerability <br />
Summary:<br />
- Graph-based learning systems are successful in handling structural data but are vulnerable to adversarial attacks on homophilic graphs.
- Existing robust models focus on homophilic graphs, leaving the security of graph-based learning on heterophilic graphs unexplored.
- The update of the negative classification loss is inversely related to pairwise similarities based on aggregated neighbor features.
- A novel robust graph structural learning strategy is proposed to address vulnerabilities in graph-based learning systems.
- The strategy incorporates a dual-kNN graph construction pipeline to supervise neighbor-similarity-preserved propagation for more reliable data management on both homophilic and heterophilic graphs. <br /> <div>
arXiv:2401.09754v2 Announce Type: replace-cross 
Abstract: Despite the tremendous success of graph-based learning systems in handling structural data, it has been widely investigated that they are fragile to adversarial attacks on homophilic graph data, where adversaries maliciously modify the semantic and topology information of the raw graph data to degrade the predictive performances. Motivated by this, a series of robust models are crafted to enhance the adversarial robustness of graph-based learning systems on homophilic graphs. However, the security of graph-based learning systems on heterophilic graphs remains a mystery to us. To bridge this gap, in this paper, we start to explore the vulnerability of graph-based learning systems regardless of the homophily degree, and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features. The theoretical finding inspires us to craft a novel robust graph structural learning strategy that serves as a useful graph mining module in a robust model that incorporates a dual-kNN graph constructions pipeline to supervise the neighbor-similarity-preserved propagation, where the graph convolutional layer adaptively smooths or discriminates the features of node pairs according to their affluent local structures. In this way, the proposed methods can mine the ``better" topology of the raw graph data under diverse graph homophily and achieve more reliable data management on homophilic and heterophilic graphs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model</title>
<link>https://arxiv.org/abs/2503.23746</link>
<guid>https://arxiv.org/abs/2503.23746</guid>
<content:encoded><![CDATA[
<div> Keywords: Short-video, Propagation influence, Dataset, Cross-platform, Large Graph Model

Summary:
Short-video platforms have become immensely popular, prompting researchers to analyze their propagation dynamics for commercial and social insights. This paper introduces the Short-video Propagation Influence Rating (SPIR) task and presents a new Cross-platform Short-Video (XS-Video) dataset. This dataset, spanning 5 major Chinese platforms, contains over 117,000 videos annotated with propagation influences. Unique in its cross-platform coverage and detailed metrics, the XS-Video dataset facilitates research in short-video propagation. Additionally, the paper proposes the Large Graph Model (LGM) named NetGPT, leveraging a three-stage training mechanism to analyze short-video propagation graphs. Evaluations on the XS-Video dataset demonstrate the efficacy of NetGPT for SPIR, outperforming existing methods in classification and regression tasks. This work advances the analysis of short-video propagation, offering valuable insights for understanding and predicting the spread of videos across diverse platforms.<br /><br />Summary: <div>
arXiv:2503.23746v2 Announce Type: replace-cross 
Abstract: Short-video platforms have gained immense popularity, captivating the interest of millions, if not billions, of users globally. Recently, researchers have highlighted the significance of analyzing the propagation of short-videos, which typically involves discovering commercial values, public opinions, user behaviors, etc. This paper proposes a new Short-video Propagation Influence Rating (SPIR) task and aims to promote SPIR from both the dataset and method perspectives. First, we propose a new Cross-platform Short-Video (XS-Video) dataset, which aims to provide a large-scale and real-world short-video propagation network across various platforms to facilitate the research on short-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926 samples, and 535 topics across 5 biggest Chinese platforms, annotated with the propagation influence from level 0 to 9. To the best of our knowledge, this is the first large-scale short-video dataset that contains cross-platform data or provides all of the views, likes, shares, collects, fans, comments, and comment content. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a novel three-stage training mechanism, to bridge heterogeneous graph-structured data with the powerful reasoning ability and knowledge of Large Language Models (LLMs). Our NetGPT can comprehend and analyze the short-video propagation graph, enabling it to predict the long-term propagation influence of short-videos. Comprehensive experimental results evaluated by both classification and regression metrics on our XS-Video dataset indicate the superiority of our method for SPIR.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Optimization of Methods for Establishing Well-Connected Communities</title>
<link>https://arxiv.org/abs/2509.02590</link>
<guid>https://arxiv.org/abs/2509.02590</guid>
<content:encoded><![CDATA[
<div> Algorithm, Community detection, Well-Connected Clusters, Connectivity Modifier, HPE Chapel programming language

Summary: <br />
Community detection is essential for identifying important structures within networks. Existing methods often struggle with disconnected clusters, hampering accuracy and reliability. The Well-Connected Clusters (WCC) and Connectivity Modifier (CM) algorithms are effective but too computationally demanding for large graphs. This study introduces optimized parallel versions of WCC and CM using the HPE Chapel language. By leveraging Chapel's parallel capabilities, the algorithms achieve significant performance improvements and scalability on modern multicore systems. Integrated into the Arkouda/Arachne framework, these implementations allow for efficient community detection on massive graphs exceeding 2 billion edges. For example, the algorithms successfully analyze the Open-Alex dataset with over 2 billion edges in mere minutes using 128 cores, a task previously deemed impossible. <div>
arXiv:2509.02590v1 Announce Type: new 
Abstract: Community detection plays a central role in uncovering meso scale structures in networks. However, existing methods often suffer from disconnected or weakly connected clusters, undermining interpretability and robustness. Well-Connected Clusters (WCC) and Connectivity Modifier (CM) algorithms are post-processing techniques that improve the accuracy of many clustering methods. However, they are computationally prohibitive on massive graphs. In this work, we present optimized parallel implementations of WCC and CM using the HPE Chapel programming language. First, we design fast and efficient parallel algorithms that leverage Chapel's parallel constructs to achieve substantial performance improvements and scalability on modern multicore architectures. Second, we integrate this software into Arkouda/Arachne, an open-source, high-performance framework for large-scale graph analytics. Our implementations uniquely enable well-connected community detection on massive graphs with more than 2 billion edges, providing a practical solution for connectivity-preserving clustering at web scale. For example, our implementations of WCC and CM enable community detection of the over 2-billion edge Open-Alex dataset in minutes using 128 cores, a result infeasible to compute previously.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive clustering based on regular equivalence for influential node identification in complex networks</title>
<link>https://arxiv.org/abs/2509.02609</link>
<guid>https://arxiv.org/abs/2509.02609</guid>
<content:encoded><![CDATA[
<div> Influential node identification, deep unsupervised framework, ReCC, contrastive learning mechanism, regular equivalence-based similarity<br />
<br />Summary: <br />Identifying influential nodes in complex networks is essential but challenging due to the lack of labeled data. The proposed ReCC framework introduces a novel deep unsupervised approach that redefines node influence detection as a clustering problem. By utilizing a contrastive learning mechanism based on regular equivalence-based similarity, ReCC generates positive and negative samples without the need for labeled data. This approach, integrated into a graph convolutional network, effectively distinguishes influential nodes from non-influential ones. ReCC is pre-trained using network reconstruction loss and fine-tuned with a combined contrastive and clustering loss, enhancing node representations with structural metrics and regular equivalence-based similarities. Extensive experiments demonstrate ReCC outperforms existing methods across various benchmarks, offering a promising solution for influential node identification in real-world scenarios. <br /> <div>
arXiv:2509.02609v1 Announce Type: new 
Abstract: Identifying influential nodes in complex networks is a fundamental task in network analysis with wide-ranging applications across domains. While deep learning has advanced node influence detection, existing supervised approaches remain constrained by their reliance on labeled data, limiting their applicability in real-world scenarios where labels are scarce or unavailable. While contrastive learning demonstrates significant potential for performance enhancement, existing approaches predominantly rely on multiple-embedding generation to construct positive/negative sample pairs. To overcome these limitations, we propose ReCC (\textit{r}egular \textit{e}quivalence-based \textit{c}ontrastive \textit{c}lustering), a novel deep unsupervised framework for influential node identification. We first reformalize influential node identification as a label-free deep clustering problem, then develop a contrastive learning mechanism that leverages regular equivalence-based similarity, which captures structural similarities between nodes beyond local neighborhoods, to generate positive and negative samples. This mechanism is integrated into a graph convolutional network to learn node embeddings that are used to differentiate influential from non-influential nodes. ReCC is pre-trained using network reconstruction loss and fine-tuned with a combined contrastive and clustering loss, with both phases being independent of labeled data. Additionally, ReCC enhances node representations by combining structural metrics with regular equivalence-based similarities. Extensive experiments demonstrate that ReCC outperforms state-of-the-art approaches across several benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic generation of online social networks through homophily</title>
<link>https://arxiv.org/abs/2509.02762</link>
<guid>https://arxiv.org/abs/2509.02762</guid>
<content:encoded><![CDATA[
<div> algorithm, synthetic social networks, homophily, microblogging, online social networks 

Summary:
The article introduces a new algorithm for generating synthetic microblogging social networks that mimic real-world online social networks. The algorithm takes into account attributes-based homophily, stochastic link formation, clustering through triadic closure, and global reachability via long-range connections. By calibrating five hyperparameters, the algorithm aims to replicate five key structural properties observed in real social networks. The framework is validated by generating synthetic networks ranging from 10^3 to 10^6 nodes and comparing them against a real-world network. Results indicate that the proposed algorithm outperforms existing techniques in capturing the structural properties of real networks and generating attribute-driven communities that align with sociological expectations. The generated synthetic networks provide a realistic and scalable testbed for social researchers to conduct controlled experiments without relying on live digital platforms. 

<br /><br />Summary: <div>
arXiv:2509.02762v1 Announce Type: new 
Abstract: Online social networks (OSNs) have become increasingly relevant for studying social behavior and information diffusion. Nevertheless, they are limited by restricted access to real OSN data due to privacy, legal, and platform-related constraints. In response, synthetic social networks serve as a viable approach to support controlled experimentation, but current generators reproduce only topology and overlook attribute-driven homophily and semantic realism.
  This work proposes a homophily-based algorithm that produces synthetic microblogging social networks such as X. The model creates a social graph for a given number of users, integrating semantic affinity among user attributes, stochastic variation in link formation, triadic closure to foster clustering, and long-range connections to ensure global reachability. A systematic grid search is used to calibrate five hyperparameters (affinity strength, noise, closure probability, distant link probability, and candidate pool size) for reaching five structural values observed in real social networks (density, clustering coefficient, LCC proportion, normalized shortest path, and modularity).
  The framework is validated by generating synthetic OSNs at four scales (10^3-10^6 nodes), and benchmarking them against a real-world Bluesky network comprising 4 million users. Comparative results show that the framework reliably reproduces the structural properties of the real network. Overall, the framework outperforms leading importance-sampling techniques applied to the same baseline. The generated graphs capture topological realism and yield attribute-driven communities that align with sociological expectations, providing a realistic, scalable testbed that liberates social researchers from relying on live digital platforms.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Movie Success with Multi-Task Learning: A Hybrid Framework Combining GPT-Based Sentiment Analysis and SIR Propagation</title>
<link>https://arxiv.org/abs/2509.02809</link>
<guid>https://arxiv.org/abs/2509.02809</guid>
<content:encoded><![CDATA[
<div> Keywords: movie success prediction, multi-task learning, sentiment analysis, SIR modeling, feature integration<br />
<br />
Summary: <br />
This study introduces a hybrid framework for predicting the success of movies by combining multi-task learning, sentiment analysis using GPT, and SIR propagation modeling. The framework considers static production attributes, information dissemination, and audience sentiment simultaneously. Using 5,840 films and 300,000 user reviews, the framework achieves high predictive accuracy. Ablation analysis highlights the importance of selective feature combinations over a comprehensive model. Virality patterns between successful and unsuccessful films are identified, challenging existing assumptions. The framework's innovations include using epidemiological modeling for information diffusion, generating multidimensional sentiment features from GPT-based analysis, and employing a shared representation architecture to optimize success metrics. This research has implications for the film production industry and advances understanding of how audience engagement influences commercial outcomes. <br /> <div>
arXiv:2509.02809v1 Announce Type: new 
Abstract: This study presents a hybrid framework for predicting movie success. The framework integrates multi-task learning (MTL), GPT-based sentiment analysis, and Susceptible-Infected-Recovered (SIR) propagation modeling. The study examines limitations in existing approaches. It models static production attributes, information dissemination, and audience sentiment at the same time. The framework uses 5,840 films from 2004 to 2024 and approximate 300,000 user reviews. It shows predictive performance with classification accuracy of 0.964 and regression metrics of MAE 0.388. Ablation analysis indicates component interactions. Selective feature combinations perform better than the comprehensive model. This result questions assumptions about feature integration. The model shows virality patterns between successful and unsuccessful films. Innovations include epidemiological modeling for information diffusion, multidimensional sentiment features from GPT-based analysis, and a shared representation architecture that optimizes multiple success metrics. The framework provides applications in the film production lifecycle. It also contributes to understanding how audience engagement leads to commercial outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal social network modeling of mobile connectivity data with graph neural networks</title>
<link>https://arxiv.org/abs/2509.03319</link>
<guid>https://arxiv.org/abs/2509.03319</guid>
<content:encoded><![CDATA[
<div> GNN; temporal social networks; mobile connectivity data; ROLAND; EdgeBank
Summary:
Graph neural networks (GNNs) are powerful tools for analyzing complex networks, but their application to social networks using mobile connectivity data is understudied. This study explores four temporal GNN models for predicting phone call and SMS activity in a mobile communication network. The ROLAND temporal GNN outperforms a non-GNN baseline model, showcasing the potential of GNNs in analyzing temporal social networks. However, other GNN models performed worse than the baseline, highlighting the need for specialized GNN architectures. Further research is necessary to optimize GNNs for analyzing temporal social networks through mobile connectivity data.<br /><br />Summary: <div>
arXiv:2509.03319v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have emerged as a state-of-the-art data-driven tool for modeling connectivity data of graph-structured complex networks and integrating information of their nodes and edges in space and time. However, as of yet, the analysis of social networks using the time series of people's mobile connectivity data has not been extensively investigated. In the present study, we investigate four snapshot - based temporal GNNs in predicting the phone call and SMS activity between users of a mobile communication network. In addition, we develop a simple non - GNN baseline model using recently proposed EdgeBank method. Our analysis shows that the ROLAND temporal GNN outperforms the baseline model in most cases, whereas the other three GNNs perform on average worse than the baseline. The results show that GNN based approaches hold promise in the analysis of temporal social networks through mobile connectivity data. However, due to the relatively small performance margin between ROLAND and the baseline model, further research is required on specialized GNN architectures for temporal social network analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results</title>
<link>https://arxiv.org/abs/2509.02969</link>
<guid>https://arxiv.org/abs/2509.02969</guid>
<content:encoded><![CDATA[
<div> Challenge, Engagement Prediction, Short Videos, User-generated Content, Social Media Platforms
<br />
Summary:
The paper discusses the VQualA 2025 Challenge on Engagement Prediction for Short Videos, focusing on analyzing the popularity of user-generated content (UGC) short videos on social media platforms. The challenge utilizes a new UGC dataset with engagement metrics derived from real user interactions to promote robust modeling strategies capturing factors impacting user engagement. Participants explored various multi-modal features, including visual content, audio, and creator metadata. The challenge attracted 97 participants, receiving 15 valid test submissions, which significantly advanced progress in short-form UGC video engagement prediction. <div>
arXiv:2509.02969v1 Announce Type: cross 
Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge focuses on understanding and modeling the popularity of user-generated content (UGC) short videos on social media platforms. To support this goal, the challenge uses a new short-form UGC dataset featuring engagement metrics derived from real-world user interactions. This objective of the Challenge is to promote robust modeling strategies that capture the complex factors influencing user engagement. Participants explored a variety of multi-modal features, including visual content, audio, and metadata provided by creators. The challenge attracted 97 participants and received 15 valid test submissions, contributing significantly to progress in short-form UGC video engagement prediction.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predict, Cluster, Refine: A Joint Embedding Predictive Self-Supervised Framework for Graph Representation Learning</title>
<link>https://arxiv.org/abs/2502.01684</link>
<guid>https://arxiv.org/abs/2502.01684</guid>
<content:encoded><![CDATA[
<div> framework, graph SSL, contrastive objectives, semantic-aware objective, GMM-based pseudo-label scoring

Summary:
The paper introduces a novel joint embedding predictive framework for graph SSL that eliminates the need for contrastive objectives and negative sampling. This framework preserves semantic and structural information while enhancing node discriminability through a semantic-aware objective term that uses pseudo-labels derived from Gaussian Mixture Models (GMMs). The proposed framework outperforms existing graph SSL methods across benchmarks by leveraging a non-contrastive, view-invariant architecture, capturing single context and multiple targets relationships between subgraphs, and incorporating GMM-based pseudo-label scoring to evaluate semantic contributions. This approach offers a computationally efficient and collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. <div>
arXiv:2502.01684v4 Announce Type: replace-cross 
Abstract: Graph representation learning has emerged as a cornerstone for tasks like node classification and link prediction, yet prevailing self-supervised learning (SSL) methods face challenges such as computational inefficiency, reliance on contrastive objectives, and representation collapse. Existing approaches often depend on feature reconstruction, negative sampling, or complex decoders, which introduce training overhead and hinder generalization. Further, current techniques which address such limitations fail to account for the contribution of node embeddings to a certain prediction in the absence of labeled nodes. To address these limitations, we propose a novel joint embedding predictive framework for graph SSL that eliminates contrastive objectives and negative sampling while preserving semantic and structural information. Additionally, we introduce a semantic-aware objective term that incorporates pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node discriminability by evaluating latent feature contributions. Extensive experiments demonstrate that our framework outperforms state-of-the-art graph SSL methods across benchmarks, achieving superior performance without contrastive loss or complex decoders. Key innovations include (1) a non-contrastive, view-invariant joint embedding predictive architecture, (2) Leveraging single context and multiple targets relationship between subgraphs, and (3) GMM-based pseudo-label scoring to capture semantic contributions. This work advances graph SSL by offering a computationally efficient, collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. The code for our paper can be found at https://github.com/Deceptrax123/JPEB-GSSL
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning of Local Updates for Maximum Independent Set in Dynamic Graphs</title>
<link>https://arxiv.org/abs/2505.13754</link>
<guid>https://arxiv.org/abs/2505.13754</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, unsupervised learning, Maximum Independent Sets, dynamic graphs, edge addition/deletion<br />
<br />
Summary:<br />
This paper introduces an unsupervised learning model to find Maximum Independent Sets (MaxIS) in dynamic graphs where edges evolve over time. The model combines graph neural networks (GNNs) with a learned update mechanism to determine nodes' MaxIS membership in response to edge changes. By parameterizing the model with an update radius, the tradeoff between performance and runtime for different graph topologies is explored. Evaluation on synthetic and real-world dynamic graphs shows competitive approximation ratios and scalability, with significant outperformance compared to existing learning-based methods. The model excels in solution quality, runtime, and memory usage, particularly on large graphs. When tested on graphs 100 times larger than the training data, the model produces MaxIS solutions larger than any other learning method while maintaining competitive runtimes. <div>
arXiv:2505.13754v2 Announce Type: replace-cross 
Abstract: We present the first unsupervised learning model for finding Maximum Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our method combines structural learning from graph neural networks (GNNs) with a learned distributed update mechanism that, given an edge addition or deletion event, modifies nodes' internal memories and infers their MaxIS membership in a single, parallel step. We parameterize our model by the update mechanism's radius and investigate the resulting performance-runtime tradeoffs for various dynamic graph topologies. We evaluate our model against a mixed integer programming solver and the state-of-the-art learning-based methods for MaxIS on static graphs (ICML 2020; NeurIPS 2020, 2023). Across synthetic and empirical dynamic graphs of 50-1,000 nodes, our model achieves competitive approximation ratios with excellent scalability; on large graphs, it significantly outperforms the state-of-the-art learning methods in solution quality, runtime, and memory usage. When generalizing to graphs of 10,000 nodes (100x larger than the ones used for training), our model produces MaxIS solutions 1.05-1.18x larger than any other learning method, even while maintaining competitive runtimes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Detection using Fortunato's Performance Measure</title>
<link>https://arxiv.org/abs/2509.00938</link>
<guid>https://arxiv.org/abs/2509.00938</guid>
<content:encoded><![CDATA[
<div> performance measure, community detection, graph partition, greedy algorithm, heuristic algorithm 

Summary:
The paper introduces the fp measure for detecting communities in unweighted, undirected networks. It presents a greedy algorithm, fpGreed, that optimizes the fp measure by iteratively working at both vertex and community levels. Vertices only join a community if the fp value improves, and communities merge if the fp measure improves. The algorithm terminates when no further improvements can be made. A faster heuristic algorithm, fastFp, is also introduced, particularly suitable for large datasets. The quality of the communities and computation time on various datasets are presented. fastFp performs well on large datasets like youtube and livejournal, demonstrating both efficiency and solution quality.<br /><br />Summary: <div>
arXiv:2509.00938v1 Announce Type: new 
Abstract: In his paper on Community Detection [1], Fortunato introduced a quality function called performance to assess the goodness of a graph partition. This measure counts the number of correctly ``interpreted" pairs of vertices, i. e. two vertices belonging to the same community and connected by an edge, or two vertices belonging to different communities and not connected by an edge. In this paper, we explore Fortunato's performance measure (fp measure) for detecting communities in unweighted, undirected networks. First, we give a greedy algorithm fpGreed that tries to optimise the fp measure by working iteratively at two-levels, vertex-level and community-level. At the vertex level, a vertex joins a community only if the fp value improves. Once this is done, an initial set of communities are obtained. At the next stage, two communities merge only if the fp measure improves. Once there are no further improvements to be made, the algorithm switches back to the vertex level and so on. fpGreed terminates when there are no changes to any community. We then present a faster heuristic algorithm fastFp more suitable for running on large datasets. We present the quality of the communities and the time it takes to compute them on several well-known datasets. For some of the large datasets, such as youtube and livejournal, we find that Algorithm fastFP performs really well, both in terms of the time and the quality of the solution obtained.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Propagation-aware Representation Learning for Supervised Social Media Graph Analytics</title>
<link>https://arxiv.org/abs/2509.01124</link>
<guid>https://arxiv.org/abs/2509.01124</guid>
<content:encoded><![CDATA[
<div> representation learning, social media, graph analytics, propagation dynamics, transferability 

Summary:
The paper introduces a novel representation learning framework for social media graph analytics. The proposed framework combines a dual-encoder structure with a kinetic-guided propagation module to model both structural and contextual information in social media graphs. By incorporating principled kinetic knowledge, the framework captures information propagation dynamics within the graphs, enhancing robustness to noisy data. The approach achieves state-of-the-art performance on various social media graph mining tasks such as graph classification, node classification, and link prediction. It also demonstrates strong zero-shot and few-shot transferability across datasets, making it practical for handling data-scarce tasks. This framework offers a unified architecture that can be effectively reused on different tasks, reducing repetitive engineering efforts. <div>
arXiv:2509.01124v1 Announce Type: new 
Abstract: Social media platforms generate vast, complex graph-structured data, facilitating diverse tasks such as rumor detection, bot identification, and influence modeling. Real-world applications like public opinion monitoring and stock trading -- which have a strong attachment to social media -- demand models that are performant across diverse tasks and datasets. However, most existing solutions are purely data-driven, exhibiting vulnerability to the inherent noise within social media data. Moreover, the reliance on task-specific model design challenges efficient reuse of the same model architecture on different tasks, incurring repetitive engineering efforts. To address these challenges in social media graph analytics, we propose a general representation learning framework that integrates a dual-encoder structure with a kinetic-guided propagation module. In addition to jointly modeling structural and contextual information with two encoders, our framework innovatively captures the information propagation dynamics within social media graphs by integrating principled kinetic knowledge. By deriving a propagation-aware encoder and corresponding optimization objective from a Markov chain-based transmission model, the representation learning pipeline receives a boost in its robustness to noisy data and versatility in diverse tasks. Extensive experiments verify that our approach achieves state-of-the-art performance with a unified architecture on a variety of social media graph mining tasks spanning graph classification, node classification, and link prediction. Besides, our solution exhibits strong zero-shot and few-shot transferability across datasets, demonstrating practicality when handling data-scarce tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unnoticeable Community Deception via Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2509.01438</link>
<guid>https://arxiv.org/abs/2509.01438</guid>
<content:encoded><![CDATA[
<div> Community detection, graphs, privacy concerns, deception methods, optimization<br />
<br />
Summary:<br />
Community detection in graphs is essential for understanding node organization. However, it can pose privacy risks. Current community deception methods have limitations, such as ineffective evaluation metrics and detectable attacks. This study analyzes the shortcomings of modularity decrease as a deception metric and introduces a new metric. By combining this metric with an attack budget, an unnoticeable community deception task is formulated as a multi-objective optimization problem. Two enhanced methods, incorporating degree-biased and community-biased candidate node selection mechanisms, are proposed to improve deception performance. Extensive experiments on benchmark datasets validate the effectiveness of the proposed community deception strategies. <div>
arXiv:2509.01438v1 Announce Type: new 
Abstract: Community detection in graphs is crucial for understanding the organization of nodes into densely connected clusters. While numerous strategies have been developed to identify these clusters, the success of community detection can lead to privacy and information security concerns, as individuals may not want their personal information exposed. To address this, community deception methods have been proposed to reduce the effectiveness of detection algorithms. Nevertheless, several limitations, such as the rationality of evaluation metrics and the unnoticeability of attacks, have been ignored in current deception methods. Therefore, in this work, we first investigate the limitations of the widely used deception metric, i.e., the decrease of modularity, through empirical studies. Then, we propose a new deception metric, and combine this new metric together with the attack budget to model the unnoticeable community deception task as a multi-objective optimization problem. To further improve the deception performance, we propose two variant methods by incorporating the degree-biased and community-biased candidate node selection mechanisms. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed community deception strategies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content and Engagement Trends in COVID-19 YouTube Videos: Evidence from the Late Pandemic</title>
<link>https://arxiv.org/abs/2509.01954</link>
<guid>https://arxiv.org/abs/2509.01954</guid>
<content:encoded><![CDATA[
<div> weekday effects, COVID-19, YouTube, sentiment, engagement<br />
Summary:<br />
This study analyzed 10,000 COVID-19-related YouTube videos from January 2023 to October 2024 to understand factors influencing viewer engagement during the late pandemic. Publishing activity showed weekday peaks, with viewers shifting attention mid-week. High-frequency keywords in titles included COVID, coronavirus, shorts, and live. Videos titled with shorts attracted the highest views. Sentiment analysis of video descriptions had a weak correlation with views initially, but stronger correlations emerged after addressing outliers. Video duration and category also impacted engagement, with entertainment videos attracting the highest views. Overall, the study found that engagement patterns of COVID-19 videos on YouTube were influenced by factors such as publishing schedules, title vocabulary, topics, and genre-specific duration effects.<br /><br /> <div>
arXiv:2509.01954v1 Announce Type: new 
Abstract: This work investigated about 10,000 COVID-19-related YouTube videos published between January 2023 and October 2024 to evaluate how temporal, lexical, linguistic, and structural factors influenced engagement during the late pandemic period. Publishing activity showed consistent weekday effects: in the first window, average views peaked on Mondays at 92,658; in the second, on Wednesdays at 115,479; and in the third, on Fridays at 84,874, reflecting a shift in audience attention toward mid- and late week. Lexical analysis of video titles revealed recurring high-frequency keywords related to COVID-19 and YouTube features, including COVID, coronavirus, shorts, and live. Frequency analysis revealed sharp spikes, with COVID appearing in 799 video titles in August 2024, while engagement analysis showed that videos titled with shorts attracted very high views, peaking at 2.16 million average views per video in June 2023. Analysis of sentiment of video descriptions in English showed weak correlation with views in the raw data (Pearson r = 0.0154, p = 0.2987), but stronger correlations emerged once outliers were addressed, with Spearman r = 0.110 (p < 0.001) and Pearson r = 0.0925 (p < 0.001). Category-level analysis of video durations revealed contrasting outcomes: long videos focusing on people and blogs averaged 209,114 views, short entertainment videos averaged 288,675 views, and medium-to-long news and politics videos averaged 51,309 and 59,226 views, respectively. These results demonstrate that engagement patterns of COVID-19-related videos on YouTube during the late pandemic followed distinct characteristics driven by publishing schedules, title vocabulary, topics, and genre-specific duration effects.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics in Two-Sided Attention Markets: Objective, Optimization, and Control</title>
<link>https://arxiv.org/abs/2509.01970</link>
<guid>https://arxiv.org/abs/2509.01970</guid>
<content:encoded><![CDATA[
<div> Market Ecosystem, Content Creation, Platform, User-Platform Interaction, Two-Sided Market <br />
Summary: In this new study, the focus is on understanding the ecosystem of content creation and consumption within online platforms. The research delves into the dynamics of two-sided markets involving the platform, users, and creators. A potential function is designed to capture the interactions among these entities, with creators' best-response dynamics and users' choices being analyzed through mirror descent on this function. The study reveals that various platform ranking strategies correspond to different potential functions and their dynamics can still be captured through mirror descent. Additionally, the research offers a new local convergence result for mirror descent in non-convex functions. These findings provide a theoretical basis for explaining the diverse outcomes observed in attention markets. <div>
arXiv:2509.01970v1 Announce Type: new 
Abstract: With most content distributed online and mediated by platforms, there is a pressing need to understand the ecosystem of content creation and consumption. A considerable body of recent work shed light on the one-sided market on creator-platform or user-platform interactions, showing key properties of static (Nash) equilibria and online learning. In this work, we examine the {\it two-sided} market including the platform and both users and creators. We design a potential function for the coupled interactions among users, platform and creators. We show that such coupling of creators' best-response dynamics with users' multilogit choices is equivalent to mirror descent on this potential function. Furthermore, a range of platform ranking strategies correspond to a family of potential functions, and the dynamics of two-sided interactions still correspond to mirror descent. We also provide new local convergence result for mirror descent in non-convex functions, which could be of independent interest. Our results provide a theoretical foundation for explaining the diverse outcomes observed in attention markets.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum entropy temporal networks</title>
<link>https://arxiv.org/abs/2509.02098</link>
<guid>https://arxiv.org/abs/2509.02098</guid>
<content:encoded><![CDATA[
<div> temporal networks, maximum entropy, inhomogeneous Poisson process, Hawkes process, log-likelihood<br />
Summary:<br />
This study introduces a maximum entropy approach to modeling temporal networks, where interactions are timestamped. The approach considers continuous-time interactions and yields a modular representation involving global time processes (such as inhomogeneous Poisson or Hawkes processes) and static maximum entropy edge probabilities. The factorization of time-edge labels allows for closed-form log-likelihood calculations, expectations of degree and unique edges, and the generation of various effective models for temporal networks. A log-linear Hawkes/NHPP intensity is derived through functional optimization over path entropy, linking inhomogeneous Poisson modeling to maximum entropy network ensembles. The use of global Hawkes time layers significantly improves log-likelihood compared to generic NHPP methods, while the maximum entropy edge labels help enforce strength constraints and reproduce expected unique-degree curves. The unified framework outlined in the study has potential integration with community tools, calibration procedures, and kernel estimation methods. <br /><br />Summary: <div>
arXiv:2509.02098v1 Announce Type: new 
Abstract: Temporal networks consist of timestamped directed interactions rather than static links. These links may appear continuously in time, yet few studies have directly tackled the continuous-time modeling of networks. Here, we introduce a maximum entropy approach to temporal networks and with basic assumptions on constraints, the corresponding network ensembles admit a modular and interpretable representation: a set of global time processes -an inhomogeneous Poisson or a Hawkes process- and a static maximum-entropy (MaxEnt) edge, e.g. node pair, probability. This time-edge labels factorization yields closed-form log-likelihoods, degree/unique-edge expectations, and yields a whole class of effective generative models. We provide maximum-entropy derivation of a log-linear Hawkes/NHPP intensity for temporal networks via functional optimization over path entropy, connecting inhomogeneous Poisson modeling -e.g. Hawkes models- to MaxEnt network ensembles. Global Hawkes time layers consistently improve log-likelihood over generic NHPP, while the MaxEnt edge labels recover strength constraints and reproduce expected unique-degree curves. We discuss the limitations of this unified framework and how it could be integrated with calibrated community/motif tools, Hawkes calibration procedures, and (neural) kernel estimation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RumorSphere: A Framework for Million-scale Agent-based Dynamic Simulation of Rumor Propagation</title>
<link>https://arxiv.org/abs/2509.02172</link>
<guid>https://arxiv.org/abs/2509.02172</guid>
<content:encoded><![CDATA[
<div> simulated rumor dynamics, large language models, social network simulation, opinion bias reduction, rumor spread<br />
<br />
Summary: 
This paper introduces a dynamic and hierarchical social network simulation framework utilizing large language models (LLMs) to model rumor propagation. The simulator, capable of handling millions of agents, demonstrates strong alignment with real-world rumor dynamics, reducing opinion bias by an average of 64%. The study identifies tightly connected local community structures as key drivers of rumor spread, where social pressure lead individuals to conform, perpetuating the dissemination. Counterfactual experiments suggest early and sustained efforts to correct misinformation are effective in mitigating rumors, with debunking through opinion leaders being the most successful strategy. The findings offer valuable insights for public opinion management and policymaking. <br /><br />Summary: <div>
arXiv:2509.02172v1 Announce Type: new 
Abstract: Rumor propagation modeling is critical for understanding the dynamics of misinformation spread. Previous models are either overly simplistic or static, making them ineffective for simulating real-world rumor dynamics. In this paper, leveraging the impressive human behavior imitation capabilities of large language models (LLMs), we present a novel dynamic and hierarchical social network simulation framework, which supports simulations with millions of agents. This simulator is used to explore the rumor dynamic in the real world. Experiments on real-world rumor propagation datasets reveal a strong alignment between simulated and real-world rumor dynamics, outperforming existing models with an average 64\% reduction in opinion bias. Our findings underscore the substantial potential of LLM-based multi-agent systems in social network simulations, offering critical insights for advancing social science research. Furthermore, our analysis reveals that the tightly connected local community structure within social networks is one of the key factors promoting the rapid spread of rumors. In these communities, as rumors propagate to a certain extent, some individuals, influenced by ''social pressure'', are often compelled to conform, while holders of minority opinions are further silenced, resulting in a vicious cycle that accelerates rumor dissemination. Through counterfactual experiments, we evaluate various intervention strategies and demonstrate that early and sustained efforts to correct misinformation are more effective in mitigating the spread of rumors, while debunking rumors through opinion leaders proves to be the most effective strategy. These findings provide valuable insights for public opinion management and policymaking.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Single-Linkage Clustering for Community Detection with Overlaps and Outliers</title>
<link>https://arxiv.org/abs/2509.02334</link>
<guid>https://arxiv.org/abs/2509.02334</guid>
<content:encoded><![CDATA[
<div> algorithm, community detection, HDBSCAN, hierarchical clustering, outlier detection

Summary: 
The article introduces a new approach to community detection that allows for outliers, departing from the traditional assumption of a partitioned community structure. Utilizing the Hierarchical Density Based Spatial Clustering for Applications with Noise (HDBSCAN) algorithm, the method redefines distance metrics to enable effective single-linkage clustering and noise robustness. By applying this hierarchical single-linkage clustering technique to various node/edge similarity metrics, the study explores its performance on synthetic and real-world datasets. Results demonstrate that no single method universally prevails across all graph types, implying the need for adaptive algorithms. Nevertheless, the promising outcomes suggest that hierarchical single-linkage clustering could serve as a viable model for graph clustering tasks. <div>
arXiv:2509.02334v1 Announce Type: new 
Abstract: Most community detection approaches make very strong assumptions about communities in the data, such as every vertex must belong to exactly one community (the communities form a partition). For vector data, Hierarchical Density Based Spatial Clustering for Applications with Noise (HDBSCAN) has emerged as a leading clustering algorithm that allows for outlier points that do not belong to any cluster. The first step in HDBSCAN is to redefine the distance between vectors in such a way that single-linkage clustering is effective and robust to noise. Many community detection algorithms start with a similar step that attempts to increase the weight of edges between similar nodes and decrease weights of noisy edges. In this paper, we apply the hierarchical single-linkage clustering algorithm from HDBSCAN to a variety of node/edge similarity scores to see if there is an algorithm that can effectively detect clusters while allowing for outliers. In experiments on synthetic and real world data sets, we find that no single method is optimal for every type of graph, but the admirable performance indicates that hierarchical single-linkage clustering is a viable paradigm for graph clustering.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Keyframe-Based Approach for Auditing Bias in YouTube Shorts Recommendations</title>
<link>https://arxiv.org/abs/2509.02543</link>
<guid>https://arxiv.org/abs/2509.02543</guid>
<content:encoded><![CDATA[
<div> keyframes, bias, drift, short-form video, recommendation systems 
Summary:
Using a keyframe-based approach, this study examines bias and drift in short-form video recommendations, focusing on politically sensitive topics. By analyzing perceptually salient keyframes and captions, the researchers identify shifts and clustering patterns in recommendation chains, indicating potential filtering and topic drift. A comparison between politically sensitive topics and general YouTube categories reveals noticeable differences in recommendation behavior. The findings emphasize the importance of understanding bias in short-form video algorithms and highlight the efficiency and interpretability of keyframes in this analysis.
<br /><br />Summary: <div>
arXiv:2509.02543v1 Announce Type: new 
Abstract: YouTube Shorts and other short-form video platforms now influence how billions engage with content, yet their recommendation systems remain largely opaque. Small shifts in promoted content can significantly impact user exposure, especially for politically sensitive topics. In this work, we propose a keyframe-based method to audit bias and drift in short-form video recommendations. Rather than analyzing full videos or relying on metadata, we extract perceptually salient keyframes, generate captions, and embed both into a shared content space. Using visual mapping across recommendation chains, we observe consistent shifts and clustering patterns that indicate topic drift and potential filtering. Comparing politically sensitive topics with general YouTube categories, we find notable differences in recommendation behavior. Our findings show that keyframes provide an efficient and interpretable lens for understanding bias in short-form video algorithms.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Queuing for Civility: Regulating Emotions and Reducing Toxicity in Digital Discourse</title>
<link>https://arxiv.org/abs/2509.00696</link>
<guid>https://arxiv.org/abs/2509.00696</guid>
<content:encoded><![CDATA[
<div> toxicity, online conversations, emotion regulation, social media data, comment queuing mechanism<br />
Summary:<br />
The paper introduces a graph-based framework to identify the need for emotion regulation in online conversations, aiming to promote self-reflection and responsible behavior in real time. It also proposes a comment queuing mechanism to address intentional trolls by introducing a delay in publishing comments. Analysis of social media data from Twitter and Reddit shows that the framework reduces toxicity by 12% and decreases the spread of anger by 15%. The comment queuing mechanism holds only 4% of comments on average, allowing users time to self-regulate before engaging further. The combination of real-time emotion regulation and delayed moderation significantly improves well-being in online environments. <div>
arXiv:2509.00696v1 Announce Type: cross 
Abstract: The pervasiveness of online toxicity, including hate speech and trolling, disrupts digital interactions and online well-being. Previous research has mainly focused on post-hoc moderation, overlooking the real-time emotional dynamics of online conversations and the impact of users' emotions on others. This paper presents a graph-based framework to identify the need for emotion regulation within online conversations. This framework promotes self-reflection to manage emotional responses and encourage responsible behaviour in real time. Additionally, a comment queuing mechanism is proposed to address intentional trolls who exploit emotions to inflame conversations. This mechanism introduces a delay in publishing comments, giving users time to self-regulate before further engaging in the conversation and helping maintain emotional balance. Analysis of social media data from Twitter and Reddit demonstrates that the graph-based framework reduced toxicity by 12%, while the comment queuing mechanism decreased the spread of anger by 15%, with only 4% of comments being temporarily held on average. These findings indicate that combining real-time emotion regulation with delayed moderation can significantly improve well-being in online environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Community Detection and Novelty Scoring Reveal Underexplored Hub Genes in Rheumatoid Arthritis</title>
<link>https://arxiv.org/abs/2509.00897</link>
<guid>https://arxiv.org/abs/2509.00897</guid>
<content:encoded><![CDATA[
<div> Keywords: gene co-expression networks, rheumatoid arthritis, hub genes, network analysis, immune-related processes <br />
Summary: 
- The study focused on constructing gene co-expression networks from bulk RNA-seq data of rheumatoid arthritis (RA) synovial tissue.
- Community detection algorithms revealed robust modules in the network, and node-strength ranking identified top hub genes globally and within communities.
- Integration of genome-wide association studies and literature-based evidence highlighted five high-centrality genes in RA with limited previous association.
- Functional enrichment analysis confirmed the roles of these hub genes in immune-related processes, particularly adaptive immune response and lymphocyte regulation.
- Strong correlations of these hub genes with T- and B-cell markers and negative correlations with NK-cell markers reflect the immunopathology of RA.
<br /><br />Summary: <div>
arXiv:2509.00897v1 Announce Type: cross 
Abstract: Understanding the modular structure and central elements of complex biological networks is critical for uncovering system-level mechanisms in disease. Here, we constructed weighted gene co-expression networks from bulk RNA-seq data of rheumatoid arthritis (RA) synovial tissue, using pairwise correlation and a percolation-guided thresholding strategy. Community detection with Louvain and Leiden algorithms revealed robust modules, and node-strength ranking identified the top 50 hub genes globally and within communities. To assess novelty, we integrated genome-wide association studies (GWAS) with literature-based evidence from PubMed, highlighting five high-centrality genes with little to no prior RA-specific association. Functional enrichment confirmed their roles in immune-related processes, including adaptive immune response and lymphocyte regulation. Notably, these hubs showed strong positive correlations with T- and B-cell markers and negative correlations with NK-cell markers, consistent with RA immunopathology. Overall, our framework demonstrates how correlation-based network construction, modularity-driven clustering, and centrality-guided novelty scoring can jointly reveal informative structure in omics-scale data. This generalizable approach offers a scalable path to gene prioritization in RA and other autoimmune conditions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Gaussian Mixtures to Model Evolving Multi-Modal Beliefs Across Social Media</title>
<link>https://arxiv.org/abs/2509.01123</link>
<guid>https://arxiv.org/abs/2509.01123</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian mixtures, multi-modal beliefs, opinion uncertainty, social networks, opinion dynamics<br />
<br />
Summary: <br />
The article introduces a model using Gaussian mixtures to study the formation and evolution of multi-modal beliefs and opinion uncertainty in social networks. Opinions are influenced by both exogenous factors, such as news articles, and endogenous factors, like interactions on social media. This model allows for a comprehensive understanding of opinion dynamics while maintaining simplicity. The study focuses on the impact of stubborn individuals, or social influencers, on opinion formation and uncertainty. The research also introduces the concept of centrality based on an individual's ability to disrupt information flow in a social network. The preliminary results shed light on the complexities of opinion dynamics and the role of influential individuals in shaping beliefs across social networks. <br /> <div>
arXiv:2509.01123v1 Announce Type: cross 
Abstract: We use Gaussian mixtures to model formation and evolution of multi-modal beliefs and opinion uncertainty across social networks. In this model, opinions evolve by Bayesian belief update when incorporating exogenous factors (signals from outside sources, e.g., news articles) and by non-Bayesian mixing dynamics when incorporating endogenous factors (interactions across social media). The modeling enables capturing the richness of behavior observed in multi-modal opinion dynamics while maintaining interpretability and simplicity of scalar models. We present preliminary results on opinion formation and uncertainty to investigate the effect of stubborn individuals (as social influencers). This leads to a notion of centrality based on the ease with which an individual can disrupt the flow of information across the social network.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis</title>
<link>https://arxiv.org/abs/2509.02113</link>
<guid>https://arxiv.org/abs/2509.02113</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical graph, malware analysis, dataset, control flow graphs, function call graphs <br />
<br />
Summary: 
The article introduces a new dataset called HiGraph for malware analysis, consisting of over 200 million Control Flow Graphs nested within 595,000 Function Call Graphs. This hierarchical graph representation captures the structural semantics of software, enabling the development of robust detectors against code obfuscation and malware evolution. A large-scale analysis using HiGraph reveals distinct structural properties of benign and malicious software. The dataset and tools are publicly accessible at https://higraph.org, making it a valuable resource for the research community. <div>
arXiv:2509.02113v1 Announce Type: cross 
Abstract: The advancement of graph-based malware analysis is critically limited by the absence of large-scale datasets that capture the inherent hierarchical structure of software. Existing methods often oversimplify programs into single level graphs, failing to model the crucial semantic relationship between high-level functional interactions and low-level instruction logic. To bridge this gap, we introduce \dataset, the largest public hierarchical graph dataset for malware analysis, comprising over \textbf{200M} Control Flow Graphs (CFGs) nested within \textbf{595K} Function Call Graphs (FCGs). This two-level representation preserves structural semantics essential for building robust detectors resilient to code obfuscation and malware evolution. We demonstrate HiGraph's utility through a large-scale analysis that reveals distinct structural properties of benign and malicious software, establishing it as a foundational benchmark for the community. The dataset and tools are publicly available at https://higraph.org.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community as a Vague Operator: Epistemological Questions for a Critical Heuristics of Community Detection Algorithms</title>
<link>https://arxiv.org/abs/2210.02753</link>
<guid>https://arxiv.org/abs/2210.02753</guid>
<content:encoded><![CDATA[
<div> community, network science, vague operator, digital politics, social relations

Summary: 
- The article analyzes the nature and epistemic consequences of "communities" in network science.
- Communities are described as multi-faceted and ambivalent patterns, functioning as vague operators.
- Different modes of description, combining vagueness and hyper-precision, are crucial in digital politics and community analysis.
- The 'Louvain algorithm' is discussed as a widely-used community detection method, highlighting controversies in its applications.
- Communities can act as real abstractions reshaping social relations, such as creating echo chambers on social networking sites.
- The concept of 'critical heuristics' is proposed to reconsider community detection, emphasizing partiality, epistemic humbleness, reflexivity, and artificiality. <div>
arXiv:2210.02753v3 Announce Type: replace 
Abstract: In this article, we aim to analyse the nature and epistemic consequences of what figures in network science as patterns of nodes and edges called 'communities'. Tracing these patterns as multi-faceted and ambivalent, we propose to describe the concept of community as a 'vague operator', a variant of Susan Leigh Star's notion of the boundary object, and propose that the ability to construct different modes of description that are both vague in some registers and hyper-precise in others, is core both to digital politics and the analysis of 'communities'. Engaging with these formations in terms drawn from mathematics and software studies enables a wider mapping of their formation. Disentangling different lineages in network science then allows us to contextualise the founding account of 'community' popularised by Michelle Girvan and Mark Newman in 2002. After studying one particular community detection algorithm, the widely-used 'Louvain algorithm', we comment on controversies arising with some of their more ambiguous applications. We argue that 'community' can act as a real abstraction with the power to reshape social relations such as producing echo chambers in social networking sites. To rework the epistemological terms of community detection and propose a reconsideration of vague operators, we draw on debates and propositions within the literature of network science to imagine a 'critical heuristics' that embraces partiality, epistemic humbleness, reflexivity and artificiality.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Modelling in Analysing Cyber-related Graphs</title>
<link>https://arxiv.org/abs/2412.14375</link>
<guid>https://arxiv.org/abs/2412.14375</guid>
<content:encoded><![CDATA[
<div> influence spreading model, cyber attacks, attack graphs, causal graph, probabilistic analysis

Summary: 
The article introduces a novel network-based influence spreading model to analyze the structure and dynamics of cyber attacks. The model allows for detailed probabilistic analysis of directed, weighted, and cyclic attack and causal graphs, with a focus on self-avoiding attack chains in acyclic paths. By incorporating vulnerabilities, services, and exploitabilities, the model offers a quantitative approach beyond traditional visualizations, enabling cyber analysts to prioritize and analyze larger graphs effectively. The proposed model is demonstrated through three use cases involving cyber-related graphs, showcasing its utility in generating quantitative metrics for prioritization, summaries, and analysis. <div>
arXiv:2412.14375v2 Announce Type: replace 
Abstract: In order to improve the resilience of computer infrastructure against cyber attacks and finding ways to mitigate their impact we need to understand their structure and dynamics. Here we propose a novel network-based influence spreading model to investigate event trajectories or paths in various types of attack and causal graphs, which can be directed, weighted, and / or cyclic. In case of attack graphs with acyclic paths, only self-avoiding attack chains are allowed. In the framework of our model a detailed probabilistic analysis beyond the traditional visualisation of attack graphs, based on vulnerabilities, services, and exploitabilities, can be performed. In order to demonstrate the capabilities of the model, we present three use cases with cyber-related graphs, namely two attack graphs and a causal graph. The model can be of benefit to cyber analysts in generating quantitative metrics for prioritisation, summaries, or analysis of larger graphs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing News Engagement on Facebook: Tracking Ideological Segregation and News Quality in the Facebook URL Dataset</title>
<link>https://arxiv.org/abs/2409.13461</link>
<guid>https://arxiv.org/abs/2409.13461</guid>
<content:encoded><![CDATA[
<div> Keywords: Facebook, user engagement, news consumption, ideological gap, news quality

Summary:
The study analyzes user engagement with news domains on Facebook from 2017 to 2020, focusing on news URLs in the U.S. It considers ideological alignment, news source quality, and users' political preferences to track news consumption patterns for liberal, conservative, and moderate audiences. The analysis reveals two significant shifts in trends where the ideological gap widens and news quality declines, leading to changes in user engagement. The findings highlight the impact of two major Facebook News Feed updates on user behavior and news consumption. This empirical evidence helps understand user engagement with news, ideological leanings, and news reliability on Facebook during the study period.<br /><br />Summary: <div>
arXiv:2409.13461v3 Announce Type: replace-cross 
Abstract: The Facebook Privacy-Protected Full URLs Dataset was released to enable independent, academic research on the impact of Facebook's platform on society while ensuring user privacy. The dataset has been used in several studies to analyze the relationship between social media engagement and societal issues such as misinformation, polarization, and the quality of consumed news. In this paper, we conduct a comprehensive analysis of the engagement with popular news domains, covering four years from January 2017 to December 2020, with a focus on user engagement metrics related to news URLs in the U.S. By incorporating the ideological alignment and composite score of quality and reliability of news sources, along with users' political preferences, we construct weighted averages of ideology and quality of news consumption for liberal, conservative, and moderate audiences. This allows us to track the evolution of (i) the ideological gap in news consumption between liberals and conservatives and (ii) the average quality of each group's news consumption. We identify two major shifts in trends, each tied to engagement changes. In both, the ideological gap widens and news quality declines. However, engagement rises in the first shift but falls in the second. Finally, we contextualize these trends by linking them to two major Facebook News Feed updates. Our findings provide empirical evidence to better understand user behavior and engagement with news and their leaning and reliability during the period covered by the dataset.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation</title>
<link>https://arxiv.org/abs/2502.11649</link>
<guid>https://arxiv.org/abs/2502.11649</guid>
<content:encoded><![CDATA[
<div> Keywords: non-cooperative game, opinion formation, confirmation bias, misinformation, resource optimization 

Summary:<br /><br />
The article introduces a non-cooperative game to analyze opinion formation and resistance, considering factors like confirmation bias, resource constraints, and influence penalties. The simulation involves Large Language Model agents competing to influence a population, with penalties for generating messages that propagate or counter misinformation. The study shows that higher confirmation bias leads to stronger opinion alignment but also increases overall polarization. On the other hand, lower confirmation bias results in fragmented opinions and minimal shifts in beliefs. Investing heavily in debunking strategies can align the population temporarily but may deplete resources and reduce long-term influence. The study highlights the complex interplay between confirmation bias, resource allocation, and misinformation in shaping opinion dynamics and resistance mechanisms. <div>
arXiv:2502.11649v3 Announce Type: replace-cross 
Abstract: We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Population-Scale Network Embeddings Expose Educational Divides in Network Structure Related to Right-Wing Populist Voting</title>
<link>https://arxiv.org/abs/2508.21236</link>
<guid>https://arxiv.org/abs/2508.21236</guid>
<content:encoded><![CDATA[
<div> Population-scale network, machine learning, embeddings, right-wing populist voting, Dutch population 
Summary: 
- Administrative registry data can be used to construct population-scale networks representing shared social contexts.
- Machine learning can encode these networks into numerical embeddings that capture individuals' positions.
- Embeddings for Dutch population from five shared contexts were created and used to predict right-wing populist voting.
- Embeddings alone predicted voting above chance level but individual characteristics performed better.
- Sparse and orthogonal transformations of embeddings revealed one dimension strongly associated with voting outcome.
- Mapping this dimension back to the population network showed structural differences related to education and voting patterns. 
<br /><br />Summary: <div>
arXiv:2508.21236v1 Announce Type: new 
Abstract: Administrative registry data can be used to construct population-scale networks whose ties reflect shared social contexts between persons. With machine learning, such networks can be encoded into numerical representations -- embeddings -- that automatically capture individuals' position within the network. We created embeddings for all persons in the Dutch population from a population-scale network that represents five shared contexts: neighborhood, work, family, household, and school. To assess the informativeness of these embeddings, we used them to predict right-wing populist voting. Embeddings alone predicted right-wing populist voting above chance-level but performed worse than individual characteristics. Combining the best subset of embeddings with individual characteristics only slightly improved predictions. However, after transforming the embeddings to make their dimensions more sparse and orthogonal, we found that one embedding dimension was strongly associated with the outcome. Mapping this dimension back to the population network revealed differences in network structure related to right-wing populist voting between different school ties and achieved education levels. Our study contributes methodologically by demonstrating how population-scale network embeddings can be made interpretable, and substantively by linking structural network differences in education to right-wing populist voting.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feminism, gender identity and polarization in TikTok and Twitter</title>
<link>https://arxiv.org/abs/2508.21301</link>
<guid>https://arxiv.org/abs/2508.21301</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, feminism, transsexuality, polarization, young people

Summary:
The research explores how social media platforms are shaping the debate on feminism and transsexuality, focusing on the term 'TERF'. Through Social Network Analysis, the study found that Twitter and TikTok communities discussing this topic are not cohesive, indicating isolation. Young people mostly support transinclusive feminism, highlighting a generational divide in the ideological debate. TikTok is seen as a more dialogue-based platform compared to Twitter's more partisan nature. The polarization observed suggests a strong divide within feminist activism online, with implications for broader discussions on sexual identity and activism on social media platforms.<br /><br />Summary: <div>
arXiv:2508.21301v1 Announce Type: new 
Abstract: The potential of social media to create open, collaborative and participatory spaces allows young women to engage and empower themselves in political and social activism. In this context, the objective of this research is to analyze the polarization in the debate at the intersection between the defense of feminism and transsexuality, preferably among the young population, symbolized in the use of the term 'TERF'. To do this, the existing communities on this subject on Twitter and TikTok have been analyzed with Social Network Analysis techniques, in addition to the presence of young people in them. The results indicate that the debates between both networks are not very cohesive, with a highly modularized structure that suggests isolation of each community. For this reason, it may be considered that the debate on sexual identity has resulted in a strong polarization of feminist activism in social media. Likewise, the positions of transinclusive feminism are very much in the majority among young people; this reinforces the idea of an ideological debate that can also be understood from a generational perspective. Finally, differential use between both social networks has been identified, where TikTok is a less partisan and more dialogue-based network than Twitter, which leads to discussions and participation in a more neutral tone.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Supported Content Analysis of Motivated Reasoning on Climate Change</title>
<link>https://arxiv.org/abs/2508.21305</link>
<guid>https://arxiv.org/abs/2508.21305</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, YouTube comments, social network analysis, motivated reasoning, large language model

Summary: 
This study analyzes YouTube comments related to climate change to understand the differences in discourse between believers and skeptics. Using a large language model, researchers identified ten distinct topics and examined engagement patterns through social network analysis. The findings show that discussions about government policy and natural cycles receive lower interaction compared to misinformation, indicating these topics are settled within their respective communities. This reflects motivated reasoning, where individuals selectively engage with content that aligns with their beliefs. The study highlights the role of cognitive and ideological motivations in shaping climate discourse, showcasing the potential of large language models for qualitative analysis on a large scale. <div>
arXiv:2508.21305v1 Announce Type: new 
Abstract: Public discourse around climate change remains polarized despite scientific consensus on anthropogenic climate change (ACC). This study examines how "believers" and "skeptics" of ACC differ in their YouTube comment discourse. We analyzed 44,989 comments from 30 videos using a large language model (LLM) as a qualitative annotator, identifying ten distinct topics. These annotations were combined with social network analysis to examine engagement patterns. A linear mixed-effects model showed that comments about government policy and natural cycles generated significantly lower interaction compared to misinformation, suggesting these topics are ideologically settled points within communities. These patterns reflect motivated reasoning, where users selectively engage with content that aligns with their identity and beliefs. Our findings highlight the utility of LLMs for large-scale qualitative analysis and highlight how climate discourse is shaped not only by content, but by underlying cognitive and ideological motivations.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Inference of Cell Complexes from Flows via Matrix Factorization</title>
<link>https://arxiv.org/abs/2508.21372</link>
<guid>https://arxiv.org/abs/2508.21372</guid>
<content:encoded><![CDATA[
<div> Keywords: edge-flow signals, cell complex, Hodge Laplacian, matrix-factorization-based heuristic, computational experiments

Summary:
In this paper, the authors address the problem of inferring a cell complex from edge-flow signals observed on a graph. They aim to represent these signals as a sparse combination of gradient and curl flows on the cell complex. While the general problem is known to be NP-hard, the authors propose a novel matrix-factorization-based heuristic to solve it efficiently. Through computational experiments, they show that their approach is less computationally expensive than previous heuristics, with comparable performance in most cases and even outperforming existing methods in noisy settings. By augmenting the observed graph with 2-cells and leveraging the eigenvectors of the Hodge Laplacian, their method provides a sparse and interpretable representation of edge flows on the graph. This work contributes to the field of graph inference by offering a practical and efficient solution to a challenging problem.

<br /><br />Summary: <div>
arXiv:2508.21372v1 Announce Type: new 
Abstract: We consider the following inference problem: Given a set of edge-flow signals observed on a graph, lift the graph to a cell complex, such that the observed edge-flow signals can be represented as a sparse combination of gradient and curl flows on the cell complex. Specifically, we aim to augment the observed graph by a set of 2-cells (polygons encircled by closed, non-intersecting paths), such that the eigenvectors of the Hodge Laplacian of the associated cell complex provide a sparse, interpretable representation of the observed edge flows on the graph. As it has been shown that the general problem is NP-hard in prior work, we here develop a novel matrix-factorization-based heuristic to solve the problem. Using computational experiments, we demonstrate that our new approach is significantly less computationally expensive than prior heuristics, while achieving only marginally worse performance in most settings. In fact, we find that for specifically noisy settings, our new approach outperforms the previous state of the art in both solution quality and computational speed.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operational Validation of Large-Language-Model Agent Social Simulation: Evidence from Voat v/technology</title>
<link>https://arxiv.org/abs/2508.21740</link>
<guid>https://arxiv.org/abs/2508.21740</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social simulations, online platforms, toxicity dynamics, moderation strategies

Summary: 
The study focuses on creating a technology community simulation based on Voat, an alt-right platform, using Large Language Models (LLMs) to generate culturally informed interactions. The simulation is seeded with technology links from Voat and calibrated to replicate the platform's characteristics. Agents in the simulation utilize personas to generate posts, replies, and reactions following platform rules. The 30-day simulation shows similarities to Voat's activity patterns, interaction networks, and topical alignment, including elevated toxicity levels. The study highlights regular online phenomena such as heavy-tailed participation and sparse interaction networks. However, the study has limitations in agent design and the evaluation based on a single run. Despite these limitations, the simulation provides insights into toxicity dynamics and can be used to test moderation strategies within controlled environments. <br /><br />Summary: <div>
arXiv:2508.21740v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) enable generative social simulations that can capture culturally informed, norm-guided interaction on online social platforms. We build a technology community simulation modeled on Voat, a Reddit-like alt-right news aggregator and discussion platform active from 2014 to 2020. Using the YSocial framework, we seed the simulation with a fixed catalog of technology links sampled from Voat's shared URLs (covering 30+ domains) and calibrate parameters to Voat's v/technology using samples from the MADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama 3.1 8B) and concise personas (demographics, political leaning, interests, education, toxicity propensity) to generate posts, replies, and reactions under platform rules for link and text submissions, threaded replies and daily activity cycles. We run a 30-day simulation and evaluate operational validity by comparing distributions and structures with matched Voat data: activity patterns, interaction networks, toxicity, and topic coverage. Results indicate familiar online regularities: similar activity rhythms, heavy-tailed participation, sparse low-clustering interaction networks, core-periphery structure, topical alignment with Voat, and elevated toxicity. Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. The simulation generates realistic discussions, often featuring toxic language, primarily centered on technology topics such as Big Tech and AI. This approach offers a valuable method for examining toxicity dynamics and testing moderation strategies within a controlled environment.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model</title>
<link>https://arxiv.org/abs/2306.12968</link>
<guid>https://arxiv.org/abs/2306.12968</guid>
<content:encoded><![CDATA[
<div> Algorithm, Hidden communities, Labeled Stochastic Block Model, Instance-Adaptive Clustering, Spectral clustering <br />
<br />
Summary: 
The paper addresses the challenge of detecting hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters that increase proportionally with the total number of nodes. It establishes the conditions for reducing the expected number of misclassified nodes to less than $ s $ for any $ s = o(n) $. To accomplish this, the Instance-Adaptive Clustering (IAC) algorithm is introduced, which aligns with the instance-specific lower bounds in both expectation and high probability. IAC comprises a two-phase process involving one-shot spectral clustering and iterative likelihood-based cluster assignment improvements. Notably, it operates without prior knowledge of model parameters, such as the number of clusters, and maintains a computational complexity of $ \mathcal{O}(n\, \text{polylog}(n)) $. This makes IAC suitable for large-scale applications, offering scalability and efficiency. <br /> <div>
arXiv:2306.12968v3 Announce Type: replace 
Abstract: In this paper, we investigate the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters whose sizes grow linearly with the total number of nodes. We derive the necessary and sufficient conditions under which the expected number of misclassified nodes is less than $ s $, for any number $ s = o(n) $. To achieve this, we propose IAC (Instance-Adaptive Clustering), the first algorithm whose performance matches the instance-specific lower bounds both in expectation and with high probability. IAC is a novel two-phase algorithm that consists of a one-shot spectral clustering step followed by iterative likelihood-based cluster assignment improvements. This approach is based on the instance-specific lower bound and notably does not require any knowledge of the model parameters, including the number of clusters. By performing the spectral clustering only once, IAC maintains an overall computational complexity of $ \mathcal{O}(n\, \text{polylog}(n)) $, making it scalable and practical for large-scale problems.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed membership estimation for categorical data with weighted responses</title>
<link>https://arxiv.org/abs/2310.10989</link>
<guid>https://arxiv.org/abs/2310.10989</guid>
<content:encoded><![CDATA[
<div> Keywords: Grade of Membership model, Weighted Grade of Membership model, latent classes, categorical data, algorithm<br />
Summary:<br />
The paper introduces the Weighted Grade of Membership (WGoM) model as a more flexible alternative to the Grade of Membership (GoM) model for inferring latent classes in categorical data. While GoM is limited to nonnegative integer responses, WGoM can handle continuous and negative weighted responses by relaxing the distribution constraint. The proposed algorithm efficiently estimates latent mixed memberships and other WGoM parameters, with proven statistical consistency. A method for determining the number of latent classes for categorical data with weighted responses is also presented. Validation using synthetic and real-world datasets demonstrates the accuracy and efficiency of the algorithm in estimating latent mixed memberships and determining the number of latent classes, highlighting its practical application potential.<br /> 
Summary: <div>
arXiv:2310.10989v2 Announce Type: replace 
Abstract: The Grade of Membership (GoM) model, which allows subjects to belong to multiple latent classes, is a powerful tool for inferring latent classes in categorical data. However, its application is limited to categorical data with nonnegative integer responses, as it assumes that the response matrix is generated from Bernoulli or Binomial distributions, making it inappropriate for datasets with continuous or negative weighted responses. To address this, this paper proposes a novel model named the Weighted Grade of Membership (WGoM) model. Our WGoM is more general than GoM because it relaxes GoM's distribution constraint by allowing the response matrix to be generated from distributions like Bernoulli, Binomial, Normal, and Uniform as long as the expected response matrix has a block structure related to subjects' mixed memberships under the distribution. We show that WGoM can describe any response matrix with finite distinct elements. We then propose an algorithm to estimate the latent mixed memberships and other WGoM parameters. We derive the error bounds of the estimated parameters and show that the algorithm is statistically consistent. We also propose an efficient method for determining the number of latent classes $K$ for categorical data with weighted responses by maximizing fuzzy weighted modularity. The performance of our methods is validated through both synthetic and real-world datasets. The results demonstrate the accuracy and efficiency of our algorithm for estimating latent mixed memberships, as well as the high accuracy of our method for estimating $K$, indicating their high potential for practical applications.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effects of Antivaccine Tweets on COVID-19 Vaccinations, Cases, and Deaths</title>
<link>https://arxiv.org/abs/2406.09142</link>
<guid>https://arxiv.org/abs/2406.09142</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19 vaccines, vaccine misinformation, vaccination rates, antivaccine content, social media moderation <br />
Summary: 
This study investigates the impact of exposure to antivaccine content on COVID-19 vaccination rates in the United States during 2021. Using an epidemic model and observational data, the researchers found a causal relationship between online antivaccine content on platforms like Twitter and reduced vaccine uptake in various US counties. The analysis estimates that around 14,000 individuals refused vaccination due to exposure to such content, leading to at least 510 additional cases and 8 additional deaths between February and August 2021. The study underscores the influence of social media in shaping public health decisions and highlights the importance of addressing vaccine misinformation online through effective moderation policies. These findings provide valuable insights for informing both social media regulation and public health interventions to promote higher vaccination rates and combat the spread of misinformation during health crises. <br /><br />Summary: <div>
arXiv:2406.09142v2 Announce Type: replace 
Abstract: Despite the wide availability of COVID-19 vaccines in the United States and their effectiveness in reducing hospitalizations and mortality during the pandemic, a majority of Americans chose not to be vaccinated during 2021. Recent work shows that vaccine misinformation affects intentions in controlled settings, but does not link it to real-world vaccination rates. Here, we present observational evidence of a causal relationship between exposure to antivaccine content and vaccination rates, and estimate the size of this effect. We present a compartmental epidemic model that includes vaccination, vaccine hesitancy, and exposure to antivaccine content. We fit the model to data to determine that a geographical pattern of exposure to online antivaccine content across US counties explains reduced vaccine uptake in the same counties. We find observational evidence that exposure to antivaccine content on Twitter caused about 14,000 people to refuse vaccination between February and August 2021 in the US, resulting in at least 510 additional cases and 8 additional deaths. This work provides a methodology for linking online speech with offline epidemic outcomes. Our findings should inform social media moderation policy as well as public health interventions.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multifaceted polarisation and information reliability in climate change discussions on social media platforms</title>
<link>https://arxiv.org/abs/2410.21187</link>
<guid>https://arxiv.org/abs/2410.21187</guid>
<content:encoded><![CDATA[
<div> climate change, social media, Twitter, polarization, misinformation
Summary:
The study examines the interactions on Twitter related to climate change discussions across various topics. It finds that while retweets create echo chambers, mentions lead to exposure to opposing views, intensifying polarization. Ideological divides are evident in content differences and negative sentiments, particularly from right-leaning communities sharing low-reliability information. The study identifies a topological alignment between platforms, showing that ideological communities span multiple sites. Climate change polarisation involves ideological divides, structural isolation, and emotional engagement. The results highlight the need for climate policy discussions to address the emotional and identity-driven nature of public discourse and find strategies to bridge ideological divides. <div>
arXiv:2410.21187v3 Announce Type: replace-cross 
Abstract: Social media platforms like YouTube and Twitter play a key role in disseminating both reliable and unreliable information about climate change. This study analyses the topology of interactions in Twitter and their relation to cross-platform sharing, content discussions and emotional responses. We examined climate change discussions across four topics: the 27th United Nations Climate Change Conference, the Sixth Assessment Report of the United Nations Intergovernmental Panel on Climate Change, climate refugees, and Do\~nana Natural Park. While retweets reinforce in-group cohesion in the form of echo chambers, inter-group exposure is significant through mentions, suggesting that exposure to opposing views intensifies polarisation, rather than mitigates it. Ideological divides feature content differences accompanied by steeper negative sentiments, especially from right-leaning communities prone to share low-reliability information. We identified a topological alignment between platforms, indicating that ideological communities span multiple sites. Our findings show that climate change polarisation is multifaceted, involving ideological divides, structural isolation, and emotional engagement. These results suggest that effective climate policy discussions must address the emotional and identity-driven nature of public discourse and seek strategies to bridge ideological divides.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-Confidence Models of Multidimensional Opinions with Topic-Weighted Discordance</title>
<link>https://arxiv.org/abs/2502.00284</link>
<guid>https://arxiv.org/abs/2502.00284</guid>
<content:encoded><![CDATA[
<div> opinion dynamics, multidimensional opinions, bounded-confidence model, topic-weighted discordance functions, steady-state opinion clusters <br />
Summary: <br />
This study explores how people's opinions on multiple topics evolve through interactions, considering interdependent and correlated opinions. The authors extend classical agent-based models to a multidimensional setting, where opinions on different topics are interconnected. Introducing topic-weighted discordance functions to measure opinion differences between agents, they define regions of receptiveness and analyze steady-state opinion clusters. Their models, simulated on various networks, show significant differences in results compared to baseline models when initial opinions are correlated across topics, impacting both transient and steady states. This research highlights the importance of considering multidimensional opinions in understanding opinion dynamics and provides a framework for analyzing opinion evolution in a multi-topic context. <br /> <div>
arXiv:2502.00284v2 Announce Type: replace-cross 
Abstract: People's opinions on a wide range of topics often evolve over time through their interactions with others. Models of opinion dynamics primarily focus on one-dimensional opinions, which represent opinions on one topic. However, opinions on various topics are rarely isolated; instead, they can be interdependent and correlated. In a bounded-confidence model (BCM) of opinion dynamics, agents are receptive to each other only if their opinions are sufficiently similar. We extend classical agent-based BCMs -- namely, the Hegselmann--Krause BCM, which has synchronous interactions, and the Deffuant--Weisbuch BCM, which has asynchronous interactions -- to a multidimensional setting, in which the opinions are multidimensional vectors representing opinions of different topics and opinions on different topics are interdependent. To measure opinion differences between agents, we introduce topic-weighted discordance functions that account for opinion differences in all topics. We define regions of receptiveness for our models, and we use them to characterize the steady-state opinion clusters and provide an analytical approach to compute these regions. In addition, we numerically simulate our models on various networks with initial opinions drawn from a variety of distributions. When initial opinions are correlated across different topics, our topic-weighted BCMs yield significantly different results in both transient and steady states compared to baseline models, where the dynamics of each opinion topic are independent.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of A National Digitally-Enabled Health Promotion Campaign for Mental Health Awareness using Social Media Platforms Tik Tok, Facebook, Instagram, and YouTube</title>
<link>https://arxiv.org/abs/2508.20142</link>
<guid>https://arxiv.org/abs/2508.20142</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health promotion, digital platforms, campaign effectiveness, Singapore, engagement

Summary:
This study evaluated the effectiveness of a digitally-enabled mental health promotion campaign in Singapore using various digital platforms. The campaign aimed to raise awareness of mental health issues and provide support to at-risk populations. The campaign materials, including narrative videos and infographics, were disseminated across YouTube, Facebook, Instagram, and TikTok. The primary outcomes showed that the campaign reached 1.39 million unique residents and generated 3.49 million total impressions. The cost-efficiency metrics indicated a Cost per Mille of $26.90, Cost per Click of $29.33, and Cost per Action of $6.06. Narrative videos received over 630,000 views and 18,768 engagements. Overall, the study demonstrates the effectiveness of digitally-enabled mental health promotion campaigns in engaging a national audience through multi-channel distribution and creative, narrative-driven designs.<br /><br />Summary: <div>
arXiv:2508.20142v1 Announce Type: new 
Abstract: Mental health disorders rank among the 10 leading contributors to the global burden of diseases, yet persistent stigma and care barriers delay early intervention. This has inspired efforts to leverage digital platforms for scalable health promotion to engage at-risk populations. To evaluate the effectiveness of a digitally-enabled mental health promotion (DEHP) campaign, we conducted an observational cross-sectional study of a 3-month (February-April 2025) nation-wide campaign in Singapore. Campaign materials were developed using a marketing funnel framework and disseminated across YouTube, Facebook, Instagram, and TikTok. This included narrative videos and infographics to promote symptom awareness, coping strategies, and/or patient navigation to mindline.sg, as the intended endpoint for user engagement and support. Primary outcomes include anonymised performance analytics (impressions, unique reach, video content view, engagements) stratified by demographics, device types, and sector. Secondary outcomes measured cost-efficiency metrics and traffic to mindline.sg respectively. This campaign generated 3.49 million total impressions and reached 1.39 million unique residents, with a Cost per Mille at \$26.90, Cost per Click at \$29.33, and Cost per Action at \$6.06. Narrative videos accumulated over 630,000 views and 18,768 engagements. Overall, we demonstrate that DEHP campaigns can achieve national engagement for mental health awareness through multi-channel distribution and creative, narrative-driven designs.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whom We Trust, What We Fear: COVID-19 Fear and the Politics of Information</title>
<link>https://arxiv.org/abs/2508.20146</link>
<guid>https://arxiv.org/abs/2508.20146</guid>
<content:encoded><![CDATA[
<div> infodemic, fear, COVID-19, information sources, demographic factors
Summary:
The study examines the relationship between fear levels related to COVID-19 and the information sources individuals rely on during the pandemic. It finds that fear levels and information source usage mirror COVID-19 infection trends, are strongly correlated within each group, and vary across demographic groups, particularly age and education. The type of information source significantly impacts fear levels, highlighting the importance of the information ecosystem in shaping emotional responses during crises. Moreover, information source preferences align with the political orientation of U.S. states, indicating the influence of media on public perception. These findings underscore the critical role of information dissemination in shaping individuals' emotional and behavioral reactions during global health crises like the COVID-19 pandemic.
<br /><br /> <div>
arXiv:2508.20146v1 Announce Type: new 
Abstract: The COVID-19 pandemic triggered not only a global health crisis but also an infodemic, an overload of information from diverse sources influencing public perception and emotional responses. In this context, fear emerged as a central emotional reaction, shaped by both media exposure and demographic factors. In this study, we analyzed the relationship between individuals' self-reported levels of fear about COVID-19 and the information sources they rely on, across nine source categories, including medical experts, government institutions, media, and personal networks. In particular, we defined a score that ranks fear levels based on self-reported concerns about the pandemic, collected through the Delphi CTIS survey in the United States between May 2021 and June 2022. We found that both fear levels and information source usage closely follow COVID-19 infection trends, exhibit strong correlations within each group (fear levels across sources are strongly correlated, as are patterns of source usage), and vary significantly across demographic groups, particularly by age and education. Applying causal inference methods, we showed that the type of information source significantly affects individuals' fear levels. Furthermore, we demonstrated that information source preferences can reliably match the political orientation of U.S. states. These findings highlight the importance of information ecosystem dynamics in shaping emotional and behavioral responses during large-scale crises.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal vulnerability in strong modular networks with various degree distributions between inequality and equality</title>
<link>https://arxiv.org/abs/2508.20317</link>
<guid>https://arxiv.org/abs/2508.20317</guid>
<content:encoded><![CDATA[
<div> Keywords: networks, equality, fragmentation, connectivity, vulnerability

Summary: 
In a study comparing networks characterized by inequality and equality in the distribution of links, researchers found that networks with a more equal distribution of links demonstrate stronger connectivity tolerance to node malfunctions. However, the addition of a strong modular structure in networks with commonalities such as areas, interests, or purpose renders them extremely vulnerable to attacks or disasters. This highlights the importance of balanced resource allocation between intra- and inter-links of weak communities to avoid dense unions and maintain network resilience. The findings suggest that efficiency must be balanced with tolerance in network design, emphasizing the need to consider both aspects when creating network structures. Ultimately, the study underscores the importance of reevaluating network design strategies to enhance resilience and avoid potential vulnerabilities. 

Summary: <div>
arXiv:2508.20317v1 Announce Type: cross 
Abstract: Generally, networks are classified into two sides of inequality and equality with respect to the number of links at nodes by the types of degree distributions. One side includes many social, technological, and biological networks which consist of a few nodes with many links, and many nodes with a few links, whereas the other side consists of all nodes with an equal number of links. In comprehensive investigations between them, we have found that, as a more equal network, the tolerance of whole connectivity is stronger without fragmentation against the malfunction of nodes in a wide class of randomized networks. However, we newly find that all networks which include typical well-known network structures between them become extremely vulnerable, if a strong modular (or community) structure is added with commonalities of areas, interests, religions, purpose, and so on. These results will encourage avoiding too dense unions by connecting nodes and taking into account the balanced resource allocation between intra- and inter-links of weak communities. We must reconsider not only efficiency but also tolerance against attacks or disasters, unless no community that is really impossible.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyGenStability: Multiscale community detection with generalized Markov Stability</title>
<link>https://arxiv.org/abs/2303.05385</link>
<guid>https://arxiv.org/abs/2303.05385</guid>
<content:encoded><![CDATA[
<div> Python, PyGenStability, community detection, graphs, Markov Stability

Summary:
PyGenStability is a Python software package designed for analyzing and visualizing unsupervised multiscale community detection in graphs. It offers a range of tools for finding optimized partitions of graphs at different levels of resolution using algorithms such as Louvain or Leiden. The package focuses on maximizing the generalized Markov Stability quality function and includes features like automatic detection of robust graph partitions. Users can choose quality functions for various types of graphs and also incorporate custom quality functions. Overall, PyGenStability provides a flexible and efficient solution for analyzing and exploring community structures in graphs. <div>
arXiv:2303.05385v3 Announce Type: replace 
Abstract: We present PyGenStability, a general-use Python software package that provides a suite of analysis and visualisation tools for unsupervised multiscale community detection in graphs. PyGenStability finds optimized partitions of a graph at different levels of resolution by maximizing the generalized Markov Stability quality function with the Louvain or Leiden algorithms. The package includes automatic detection of robust graph partitions and allows the flexibility to choose quality functions for weighted undirected, directed and signed graphs, and to include other user-defined quality functions.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Formation and Dynamics Among Multi-LLMs</title>
<link>https://arxiv.org/abs/2402.10659</link>
<guid>https://arxiv.org/abs/2402.10659</guid>
<content:encoded><![CDATA[
<div> social networks, large language models, network dynamics, human decisions, network formation

Summary:
The study investigates how large language models (LLMs) interact in social networks in comparison to human behavior. LLM agents demonstrate behaviors such as preferential attachment, triadic closure, and homophily, adapting their emphasis based on the context (e.g., favoring homophily in friendship networks and heterophily in organizational settings). They also replicate community structure and small-world effects observed in real-world networks. A human survey confirms the similarity in link-formation decisions between LLMs and humans. The study suggests that LLMs can be effective tools for simulating social networks and generating synthetic data. However, it also raises concerns about bias, fairness, and the design of AI systems that engage in human networks. <div>
arXiv:2402.10659v5 Announce Type: replace 
Abstract: Social networks profoundly influence how humans form opinions, exchange information, and organize collectively. As large language models (LLMs) are increasingly embedded into social and professional environments, it is critical to understand whether their interactions approximate human-like network dynamics. We develop a framework to study the network formation behaviors of multiple LLM agents and benchmark them against human decisions. Across synthetic and real-world settings, including friendship, telecommunication, and employment networks, we find that LLMs consistently reproduce fundamental micro-level principles such as preferential attachment, triadic closure, and homophily, as well as macro-level properties including community structure and small-world effects. Importantly, the relative emphasis of these principles adapts to context: for example, LLMs favor homophily in friendship networks but heterophily in organizational settings, mirroring patterns of social mobility. A controlled human-subject survey confirms strong alignment between LLMs and human participants in link-formation decisions. These results establish that LLMs can serve as powerful tools for social simulation and synthetic data generation, while also raising critical questions about bias, fairness, and the design of AI systems that participate in human networks.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale mobility patterns and the restriction of human movement</title>
<link>https://arxiv.org/abs/2201.06323</link>
<guid>https://arxiv.org/abs/2201.06323</guid>
<content:encoded><![CDATA[
<div> Flow communities, human mobility, COVID-19 pandemic, UK lockdown, Facebook Movement Maps <br />
Summary: 
The study explores human mobility during the COVID-19 pandemic using Facebook Movement Maps before and during the first UK lockdown. By analyzing mobility patterns at multiple scales, the research identifies robust flow communities that better represent mobility than traditional administrative divisions. These flow communities capture patterns beyond commuting to work and demonstrate how mobility evolved under lockdown measures. The study shows that mobility initially reverted to fine-scale flow communities during lockdown and then expanded back to coarser scales as restrictions eased. A linear decay shock model is used to quantify regional differences in the impact of lockdown and recovery times. The findings highlight the dynamic nature of human mobility during the pandemic and the importance of understanding mobility patterns at different scales for effective policymaking. <br /> <div>
arXiv:2201.06323v5 Announce Type: replace-cross 
Abstract: From the perspective of human mobility, the COVID-19 pandemic constituted a natural experiment of enormous reach in space and time. Here, we analyse the inherent multiple scales of human mobility using Facebook Movement Maps collected before and during the first UK lockdown. First, we obtain the pre-lockdown UK mobility graph, and employ multiscale community detection to extract, in an unsupervised manner, a set of robust partitions into flow communities at different levels of coarseness. The partitions so obtained capture intrinsic mobility scales with better coverage than NUTS regions, which suffer from mismatches between human mobility and administrative divisions. Furthermore, the flow communities in the fine scale partition match well the UK Travel to Work Areas (TTWAs) but also capture mobility patterns beyond commuting to work. We also examine the evolution of mobility under lockdown, and show that mobility first reverted towards fine scale flow communities already found in the pre-lockdown data, and then expanded back towards coarser flow communities as restrictions were lifted. The improved coverage induced by lockdown is well captured by a linear decay shock model, which allows us to quantify regional differences both in the strength of the effect and the recovery time from the lockdown shock.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGDE: Local Graph-based Dictionary Expansion</title>
<link>https://arxiv.org/abs/2405.07764</link>
<guid>https://arxiv.org/abs/2405.07764</guid>
<content:encoded><![CDATA[
<div> Graph-based, Dictionary Expansion, Manifold Learning, Network Science, Word Embeddings
Summary:
The study introduces Local Graph-based Dictionary Expansion (LGDE) method for discovering semantic neighbourhood of words using manifold learning and network science tools. LGDE constructs a word similarity graph from word embeddings' geometry and identifies local communities through graph diffusion, enabling exploration of nonlinear geometry for capturing word similarities based on semantic associations paths beyond direct pairwise similarities. LGDE enriches pre-selected keyword dictionaries, crucial for information retrieval tasks such as database queries and online data collection, as evidenced by its superior performance compared to direct word similarity or co-occurrence methods. Validation on English corpora shows LGDE's efficacy in keyword enrichment, which is further supported by its successful application in expanding a conspiracy-related dictionary in communication science using online data, as confirmed by domain experts and empirical results.
<br /><br />Summary: <div>
arXiv:2405.07764v4 Announce Type: replace-cross 
Abstract: We present Local Graph-based Dictionary Expansion (LGDE), a method for data-driven discovery of the semantic neighbourhood of words using tools from manifold learning and network science. At the heart of LGDE lies the creation of a word similarity graph from the geometry of word embeddings followed by local community detection based on graph diffusion. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings to capture word similarities based on paths of semantic association, over and above direct pairwise similarities. Exploiting such semantic neighbourhoods enables the expansion of dictionaries of pre-selected keywords, an important step for tasks in information retrieval, such as database queries and online data collection. We validate LGDE on two user-generated English-language corpora and show that LGDE enriches the list of keywords with improved performance relative to methods based on direct word similarities or co-occurrences. We further demonstrate our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on the expansion of a conspiracy-related dictionary from online data collected and analysed by domain experts. Our empirical results and expert user assessment indicate that LGDE expands the seed dictionary with more useful keywords due to the manifold-learning-based similarity network.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections</title>
<link>https://arxiv.org/abs/2508.19737</link>
<guid>https://arxiv.org/abs/2508.19737</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph partitioning, community detection, graph signal processing, spectral GNN, negative correction <br />
Summary: 
InfraredGP is a novel approach for graph partitioning that leverages the amplification of low-frequency information beyond the conventional range [0, 2] through a negative correction mechanism. It employs a spectral Graph Neural Network (GNN) along with low-pass filters to derive graph embeddings without the need for training. By utilizing random inputs and a feed-forward propagation (FFP) process, InfraredGP can generate distinguishable embeddings that enable high-quality community detection results when combined with the BIRCH algorithm. The method demonstrates impressive efficiency improvements, being 16x-23x faster than various baselines, while maintaining competitive quality in both static and streaming graph partitioning tasks. The code for InfraredGP is publicly available on GitHub for further exploration and experimentation. <br /><br />Summary: <div>
arXiv:2508.19737v1 Announce Type: cross 
Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem that divides nodes of a graph into densely-connected blocks. From a perspective of graph signal processing, we find that graph Laplacian with a negative correction can derive graph frequencies beyond the conventional range $[0, 2]$. To explore whether the low-frequency information beyond this range can encode more informative properties about community structures, we propose InfraredGP. It (\romannumeral1) adopts a spectral GNN as its backbone combined with low-pass filters and a negative correction mechanism, (\romannumeral2) only feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings via one feed-forward propagation (FFP) without any training, and (\romannumeral4) obtains feasible GP results by feeding the derived embeddings to BIRCH. Surprisingly, our experiments demonstrate that based solely on the negative correction mechanism that amplifies low-frequency information beyond $[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard clustering modules (e.g., BIRCH) and obtain high-quality results for GP without any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate InfraredGP for both static and streaming GP, where InfraredGP can achieve much better efficiency (e.g., 16x-23x faster) and competitive quality over various baselines. We have made our code public at https://github.com/KuroginQin/InfraredGP
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Economic Complexity of the Roman Empire</title>
<link>https://arxiv.org/abs/2508.19892</link>
<guid>https://arxiv.org/abs/2508.19892</guid>
<content:encoded><![CDATA[
<div> archaeological evidence, Roman Empire, economic complexity, inscriptions, provinces 

Summary: 
The study explores the evolution of economic complexity over long periods by analyzing archaeological evidence from the Roman Empire. By examining inscriptions that list occupations and their locations, researchers estimate the economic complexity of different provinces. Surprisingly, the most complex areas during the first four centuries of the Roman Empire align with the most complex countries in the present day. The study highlights the continuity of economic capabilities across centuries, suggesting that the development of economic complexity is a challenging and enduring process. While the reasons for this preservation remain unclear, the findings offer valuable insights into the historical and contemporary trends of economic complexity. <div>
arXiv:2508.19892v1 Announce Type: cross 
Abstract: Economic complexity is a powerful tool to estimate the productive capabilities and future growth of modern economies. Little is known of how economic complexity evolves over long periods in history. In this paper, we use archaeological evidence from the Roman Empire in the form of short texts preserved on a durable material (i.e. inscriptions) to estimate the economic complexity of the various provinces of the empire. By connecting the occupations listed in the text of inscriptions with the location in which the inscribed objects were found we can estimate that the most complex areas during the first four centuries of the Roman Empire have a remarkable and statistically significant overlap with the most complex countries today. While we lack an explanation for the reason of the preservation of economic complexity through the ages, this evidence provides a suggestion about how difficult the development of economic capabilities might be.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs</title>
<link>https://arxiv.org/abs/2508.19907</link>
<guid>https://arxiv.org/abs/2508.19907</guid>
<content:encoded><![CDATA[
<div> GegenNet, spectral convolutional neural network, link sign prediction, signed bipartite graph, Gegenbauer polynomial basis<br />
<br />
Summary: <br />
The paper introduces GegenNet, a spectral convolutional neural network model designed for link sign prediction in signed bipartite graphs (SBGs). GegenNet utilizes fast spectral decomposition techniques for node feature initialization, a spectral graph filter based on Gegenbauer polynomial basis, and multi-layer spectral convolutional networks that alternate between Gegenbauer polynomial filters for positive and negative edges. The model demonstrates improved performance compared to 11 competitors across 6 benchmark SBG datasets, achieving gains of up to 4.28% in AUC and 11.69% in F1 score. GegenNet's enhanced model capacity and predictive accuracy make it a promising solution for effectively predicting the signs of potential links in SBGs. <br /> <div>
arXiv:2508.19907v1 Announce Type: cross 
Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V, the goal of link sign prediction is to predict the signs of potential links connecting U and V based on known positive and negative edges in G. The majority of existing solutions towards link sign prediction mainly focus on unipartite signed graphs, which are sub-optimal due to the neglect of node heterogeneity and unique bipartite characteristics of SBGs. To this end, recent studies adapt graph neural networks to SBGs by introducing message-passing schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node pairs. However, the fundamental spectral convolutional operators were originally designed for positive links in unsigned graphs, and thus, are not optimal for inferring missing positive or negative links from known ones in SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective spectral convolutional neural network model for link sign prediction in SBGs. In particular, GegenNet achieves enhanced model capacity and high predictive accuracy through three main technical contributions: (i) fast and theoretically grounded spectral decomposition techniques for node feature initialization; (ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and (iii) multi-layer sign-aware spectral convolutional networks alternating Gegenbauer polynomial filters with positive and negative edges. Our extensive empirical studies reveal that GegenNet can achieve significantly superior performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign prediction compared to 11 strong competitors over 6 benchmark SBG datasets.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Joint Effect of Culture and Discussion Topics on X (Twitter) Signed Ego Networks</title>
<link>https://arxiv.org/abs/2402.18235</link>
<guid>https://arxiv.org/abs/2402.18235</guid>
<content:encoded><![CDATA[
<div> Keywords: Ego Network Model, social relationships, sentiments, culture, topics of discussion

Summary:
The study explores the sentiment analysis of social relationships based on the Ego Network Model (ENM), focusing on positive and negative sentiments across different cultures, communities, and topics. Contrary to previous beliefs, culture does not easily override the influence of topic discussions. However, specific and polarizing topics lead to increased negativity across all cultures. These negative sentiments are consistent across different levels of the ENM, contradicting prior hypotheses. Furthermore, the number of generic topics discussed between users serves as a predictor for the overall positivity of their relationships. The research sheds light on the complex interplay between culture, topics of discussion, and sentiments in shaping social relationships within the ENM framework. 

<br /><br />Summary: <div>
arXiv:2402.18235v2 Announce Type: replace 
Abstract: Humans are known to structure social relationships according to certain patterns, such as the Ego Network Model (ENM). These patterns result from our innate cognitive limits and can therefore be observed in the vast majority of large human social groups. Until recently, the main focus of research was the structural characteristics of this model. The main aim of this paper is to complement previous findings with systematic and data-driven analyses on the positive and negative sentiments of social relationships, across different cultures, communities and topics of discussion. A total of 26 datasets were collected for this work. It was found that contrary to previous findings, the influence of culture is not easily ``overwhelmed'' by that of the topic of discussion. However, more specific and polarising topics do lead to noticeable increases in negativity across all cultures. These negativities also appear to be stable across the different levels of the ENM, which contradicts previous hypotheses. Finally, the number of generic topics being discussed between users seems to be a good predictor of the overall positivity of their relationships.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Generative Diffusion Models for Social Recommendations</title>
<link>https://arxiv.org/abs/2412.15579</link>
<guid>https://arxiv.org/abs/2412.15579</guid>
<content:encoded><![CDATA[
<div> Keywords: social recommendation, social homophily, generative model, collaborative signals, self-supervised learning

Summary:
The paper addresses the challenge of low social homophily in social recommendations by proposing the Score-based Generative Model for Social Recommendation (SGSR). This innovative approach generates optimal user social representations that maximize consistency with collaborative signals, effectively adapting Stochastic Differential Equation (SDE)-based diffusion models for recommendations. SGSR utilizes a joint curriculum training strategy to handle missing supervision signals and leverages self-supervised learning techniques to align knowledge across social and collaborative domains. Experimental results on real-world datasets show the effectiveness of SGSR in filtering redundant social information and enhancing recommendation performance. <div>
arXiv:2412.15579v2 Announce Type: replace 
Abstract: With the prevalence of social networks on online platforms, social recommendation has become a vital technique for enhancing personalized recommendations. The effectiveness of social recommendations largely relies on the social homophily assumption, which presumes that individuals with social connections often share similar preferences. However, this foundational premise has been recently challenged due to the inherent complexity and noise present in real-world social networks. In this paper, we tackle the low social homophily challenge from an innovative generative perspective, directly generating optimal user social representations that maximize consistency with collaborative signals. Specifically, we propose the Score-based Generative Model for Social Recommendation (SGSR), which effectively adapts the Stochastic Differential Equation (SDE)-based diffusion models for social recommendations. To better fit the recommendation context, SGSR employs a joint curriculum training strategy to mitigate challenges related to missing supervision signals and leverages self-supervised learning techniques to align knowledge across social and collaborative domains. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach in filtering redundant social information and improving recommendation performance.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying the Extremes: Developing a Unified Model for Detecting and Predicting Extremist Traits and Radicalization</title>
<link>https://arxiv.org/abs/2501.04820</link>
<guid>https://arxiv.org/abs/2501.04820</guid>
<content:encoded><![CDATA[
<div> Keywords: extremist discourse, online community forums, psychosocial model, incel community, radicalization

Summary:
The paper introduces a novel method for extracting and analyzing extremist discourse across online community forums, focusing on verbal behavioral traits to quantify extremism. It identifies 11 distinct factors, termed "The Extremist Eleven," as a psychosocial model of extremism. Applying this method to various online communities, the research demonstrates the ability to characterize diverse ideologies. By analyzing user histories from the incel community, the framework accurately predicts user entry up to 10 months in advance. Users tend to maintain their level of extremism within extremist forums while remaining distinguishable from general online discourse. The study contributes to understanding extremism by presenting a holistic, cross-ideological approach that goes beyond traditional models. 

<br /><br />Summary: <div>
arXiv:2501.04820v2 Announce Type: replace 
Abstract: The proliferation of ideological movements into extremist factions via social media has become a global concern. While radicalization has been studied extensively within the context of specific ideologies, our ability to accurately characterize extremism in more generalizable terms remains underdeveloped. In this paper, we propose a novel method for extracting and analyzing extremist discourse across a range of online community forums. By focusing on verbal behavioral signatures of extremist traits, we develop a framework for quantifying extremism at both user and community levels. Our research identifies 11 distinct factors, which we term ``The Extremist Eleven,'' as a generalized psychosocial model of extremism. Applying our method to various online communities, we demonstrate an ability to characterize ideologically diverse communities across the 11 extremist traits. We demonstrate the power of this method by analyzing user histories from members of the incel community. We find that our framework accurately predicts which users join the incel community up to 10 months before their actual entry with an AUC of $>0.6$, steadily increasing to AUC ~0.9 three to four months before the event. Further, we find that upon entry into an extremist forum, the users tend to maintain their level of extremism within the community, while still remaining distinguishable from the general online discourse. Our findings contribute to the study of extremism by introducing a more holistic, cross-ideological approach that transcends traditional, trait-specific models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recognizing Distance-Count Matrices is Difficult</title>
<link>https://arxiv.org/abs/2508.18857</link>
<guid>https://arxiv.org/abs/2508.18857</guid>
<content:encoded><![CDATA[
<div> graph, centrality measures, distance-count matrix, NP-complete, counterexample

Summary:
This article introduces the topic of axiomatization of centrality measures and specifically focuses on the construction of counterexamples involving distance-count matrices in the context of geometric centralities. The study proves that determining whether a given matrix is the distance-count matrix of a graph is strongly NP-complete, indicating the complexity of this problem. This result highlights the limitations of brute-force methods for constructing counterexamples and emphasizes the need for more sophisticated approaches. The article underscores the challenges in proving properties of centrality indices and the importance of developing efficient strategies for tackling such problems in graph theory. <div>
arXiv:2508.18857v1 Announce Type: new 
Abstract: Axiomatization of centrality measures often involves proving that something cannot hold by providing a counterexample (i.e., a graph for which that specific centrality index fails to have a given property). In the context of geometric centralities, building such counterexamples requires constructing a graph with specific distance counts between nodes, as expressed by its distance-count matrix. We prove that deciding whether a matrix is the distance-count matrix of a graph is strongly NP-complete. This negative result implies that a brute-force approach to building this kind of counterexample is out of question, and cleverer approaches are required.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Skills Formation in Gendered Peer Networks: Exploring advice giving and taking in classrooms</title>
<link>https://arxiv.org/abs/2508.19102</link>
<guid>https://arxiv.org/abs/2508.19102</guid>
<content:encoded><![CDATA[
<div> Keywords: digital skills, peer relationships, advice networks, gender, education

Summary:
The study focuses on the role of peer relationships in the development of digital skills among children. By analyzing data from students in classrooms across three countries, the researchers found that digital skills spread through peer interactions, with higher-skilled students being sought for advice more frequently. Gender differences were also observed, with girls both seeking and giving more advice, and gender homophily playing a significant role in these interactions. The study highlights the importance of leveraging peer learning in formal education to enhance digital skills development and address existing divides. These findings emphasize the need for tailored digital skills education that considers peer dynamics and gender influences. <br /><br />Summary: <div>
arXiv:2508.19102v1 Announce Type: new 
Abstract: The digitalisation of childhood underscores the importance of early digital skill development. To understand how peer relationships shape this process, we draw on unique sociocentric network data from students in classrooms across three countries, focusing on peer-to-peer advice-giving and advice-seeking networks related to digital skills. Using exponential random graph models, we find that digital skills systematically spread through peer interactions: higher-skilled students are more likely to be sought for advice while less likely to seek it themselves. Students perceived as highly skilled are more likely to seek and offer advice, but it has limited influence on being sought out by others. Gender plays a significant role: girls both seek and give more advice, with strong gender homophily shaping these interactions. We suggest that digital skills education should leverage the potential of peer learning within formal education and consider how such approaches can address persistent divides.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urn Modeling of Random Graphs Across Granularity Scales: A Framework for Origin-Destination Human Mobility Networks</title>
<link>https://arxiv.org/abs/2508.18544</link>
<guid>https://arxiv.org/abs/2508.18544</guid>
<content:encoded><![CDATA[
<div> modeling human mobility, combinatorial allocation process, origin-destination networks, three-scale framework, mixed-Poisson law

Summary:
The article presents a novel approach to modeling human mobility by treating trips as distinguishable balls assigned to location-bins. This approach generates origin-destination networks and establishes a unified three-scale framework comprising enumerative, probabilistic, and continuum graphon ensembles. A renormalization theorem is proven, showing convergence to a universal mixed-Poisson law in the large sparse regime. The framework allows for the calculation of key mobility observables such as destination occupancy, coverage, and overflow beyond finite capacities. Simulations using gravity-like kernels, calibrated on empirical OD data, validate the asymptotic predictions. By combining exact combinatorial models with continuum analysis, the results provide a systematic toolkit for generating synthetic networks, evaluating congestion, and developing sustainable urban mobility policies. <div>
arXiv:2508.18544v1 Announce Type: cross 
Abstract: We model human mobility as a combinatorial allocation process, treating trips as distinguishable balls assigned to location-bins and generating origin-destination (OD) networks. From this analogy, we construct a unified three-scale framework, enumerative, probabilistic, and continuum graphon ensembles, and prove a renormalization theorem showing that, in the large sparse regime, these representations converge to a universal mixed-Poisson law. The framework yields compact formulas for key mobility observables, including destination occupancy, vacancy of unvisited sites, coverage (a stopping-time extension of the coupon collector problem), and overflow beyond finite capacities. Simulations with gravity-like kernels, calibrated on empirical OD data, closely match the asymptotic predictions. By connecting exact combinatorial models with continuum analysis, the results offer a principled toolkit for synthetic network generation, congestion assessment, and the design of sustainable urban mobility policies.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection</title>
<link>https://arxiv.org/abs/2508.18819</link>
<guid>https://arxiv.org/abs/2508.18819</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation detection, self-supervised framework, Abstract Meaning Representation (AMR), Large Language Model (LLM), news propagation dynamics

Summary: 
This study introduces a new self-supervised framework for detecting misinformation that incorporates complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. The framework includes an LLM-based graph contrastive loss (LGCL) that improves feature separability with negative anchor points from a Large Language Model (LLM). Additionally, a multi-view graph masked autoencoder is used to learn news propagation features from social context graphs. By combining semantic and propagation-based features, the framework effectively distinguishes fake news from real news in a self-supervised manner. Experimental results show that this approach outperforms existing methodologies, even with limited labelled datasets, and enhances generalizability.<br /><br />Summary: <div>
arXiv:2508.18819v1 Announce Type: cross 
Abstract: The proliferation of misinformation in the digital age has led to significant societal challenges. Existing approaches often struggle with capturing long-range dependencies, complex semantic relations, and the social dynamics influencing news dissemination. Furthermore, these methods require extensive labelled datasets, making their deployment resource-intensive. In this study, we propose a novel self-supervised misinformation detection framework that integrates both complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. We introduce an LLM-based graph contrastive loss (LGCL) that utilizes negative anchor points generated by a Large Language Model (LLM) to enhance feature separability in a zero-shot manner. To incorporate social context, we employ a multi view graph masked autoencoder, which learns news propagation features from social context graph. By combining these semantic and propagation-based features, our approach effectively differentiates between fake and real news in a self-supervised manner. Extensive experiments demonstrate that our self-supervised framework achieves superior performance compared to other state-of-the-art methodologies, even with limited labelled datasets while improving generalizability.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affective Polarization across European Parliaments</title>
<link>https://arxiv.org/abs/2508.18916</link>
<guid>https://arxiv.org/abs/2508.18916</guid>
<content:encoded><![CDATA[
<div> Keywords: affective polarization, European parliaments, natural language processing, sentiment analysis, reciprocity

Summary: 
The study explores affective polarization in six European parliaments using automated methods. It analyzes sentiment in parliamentary speeches to identify negativity towards opposing groups compared to one's own. The results show consistent affective polarization across all parliaments, indicating increased negativity and hostility towards opposing groups. There is no significant difference in affective polarization between less active and more active parliamentarians. The study reveals that reciprocity plays a role in affecting polarization among parliamentarians in the European context. This research sheds light on the presence and mechanisms of affective polarization in political discourse, providing insights into the dynamics of intergroup attitudes within European parliaments. <br /><br />Summary: <div>
arXiv:2508.18916v1 Announce Type: cross 
Abstract: Affective polarization, characterized by increased negativity and hostility towards opposing groups, has become a prominent feature of political discourse worldwide. Our study examines the presence of this type of polarization in a selection of European parliaments in a fully automated manner. Utilizing a comprehensive corpus of parliamentary speeches from the parliaments of six European countries, we employ natural language processing techniques to estimate parliamentarian sentiment. By comparing the levels of negativity conveyed in references to individuals from opposing groups versus one's own, we discover patterns of affectively polarized interactions. The findings demonstrate the existence of consistent affective polarization across all six European parliaments. Although activity correlates with negativity, there is no observed difference in affective polarization between less active and more active members of parliament. Finally, we show that reciprocity is a contributing mechanism in affective polarization between parliamentarians across all six parliaments.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing graphs and their connectivity using graphlets</title>
<link>https://arxiv.org/abs/2508.19189</link>
<guid>https://arxiv.org/abs/2508.19189</guid>
<content:encoded><![CDATA[
<div> graphlets, subgraphs, topological description, vertex-deleted subgraphs, trees <br />
Summary:
graphlets are small subgraphs rooted at a fixed vertex, providing a topological description of the surrounding vertex. The article investigates properties and uniqueness of graphlet degree sequences and their use in analyzing asymmetric vertex-deleted subgraphs in graphs. It demonstrates the reconstruction of trees from their (<= n-1) - graphlet degree sequences, offering a simpler method compared to standard vertex-deleted subgraph reconstruction. This approach utilizes information from graphlets up to size (n-1) to capture the structural characteristics of the analyzed graphs. <div>
arXiv:2508.19189v1 Announce Type: cross 
Abstract: Graphlets are small subgraphs rooted at a fixed vertex. The number of occurrences of graphlets aligned to a particular vertex, called graphlet degree sequence, gives a topological description of the surrounding of the analyzed vertex. In this article, we study properties and uniqueness of graphlet degree sequences. The information given by graphlets up to size (n-1) is utilized graphs having certain type of asymmetric vertex-deleted subgraphs. Moreover, we show a reconstruction of trees from their (<= n-1)-graphlet degree sequences, which is much easier compared to the standard reconstruction from vertex-deleted subgraphs.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Gender Gap in Science Communication on TikTok and YouTube: How Platform Dynamics Shape the Visibility of Female Science Communicators</title>
<link>https://arxiv.org/abs/2508.16865</link>
<guid>https://arxiv.org/abs/2508.16865</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, gender inequality, science communication, YouTube, TikTok

Summary:
On social media platforms like YouTube and TikTok, gender inequalities persist in science communication. A study compared the reach and audience response of the top science accounts on these platforms, finding that men received more likes and views on YouTube, while audience response on TikTok was more balanced. Women's participation on both platforms had varying impacts, with women's engagement on YouTube negatively affecting interaction levels, while on TikTok, their impact was slightly positive. Overall, TikTok was found to be a more inclusive space for scientific communication compared to YouTube. However, structural challenges still exist on both platforms, highlighting the need for further research on strategies to promote gender equity in online science communication.<br /><br />Summary: Social media platforms like YouTube and TikTok play a significant role in science communication, but gender inequalities persist. Men tend to receive more likes and views on YouTube, while audience response on TikTok is more balanced. Women's participation on both platforms has varying impacts, with women on YouTube negatively affecting interaction levels, and on TikTok having a slightly positive impact. Despite being a more inclusive space, TikTok still faces challenges in promoting gender equity in online science communication, indicating a need for further research in this area. <div>
arXiv:2508.16865v1 Announce Type: new 
Abstract: Social media platforms facilitate the dissemination of science and access to it. However, gender inequalities in the participation and visibility of communicators persist. This study examined the differences in reach and audience response between YouTube and TikTok from a gender perspective. To do so, the ten most influential science accounts on YouTube and TikTok were selected, with the sample divided equally between men and women, to conduct a comparative study. A total of 4293 videos on TikTok and 4825 on YouTube were analyzed, along with 277,528 comments, considering metrics of views and interaction. The results show that on YouTube, men received more likes and views, while on TikTok, audience response was more balanced. The participation of women on both platforms also had a differential impact, as the number of women engaging with content on YouTube negatively correlated with interaction levels, whereas on TikTok, their impact was slightly positive. In conclusion, TikTok emerges as a more inclusive space for scientific communication, though structural challenges remain on both platforms, encouraging further research into strategies that promote gender equity in online science communication.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense Subgraph Clustering and a New Cluster Ensemble Method</title>
<link>https://arxiv.org/abs/2508.17013</link>
<guid>https://arxiv.org/abs/2508.17013</guid>
<content:encoded><![CDATA[
<div> community detection algorithm, DSC-Flow-Iter, dense subgraphs, cluster ensemble technique, modularity-based clustering

Summary:
The article introduces a new community detection algorithm called DSC-Flow-Iter, which iteratively extracts dense subgraphs. This algorithm, while leaving some nodes unclustered, shows competitiveness with leading methods and exhibits high precision and low recall. It complements modularity-based methods that usually have high recall but lower precision. The authors propose a cluster ensemble technique that combines DSC-Flow-Iter with modularity-based clustering to enhance accuracy. Through experiments on synthetic networks, the proposed pipeline, utilizing the ensemble technique, outperforms both individual components and baseline techniques. The study demonstrates the effectiveness of the approach in improving community detection accuracy. <div>
arXiv:2508.17013v1 Announce Type: new 
Abstract: We propose DSC-Flow-Iter, a new community detection algorithm that is based on iterative extraction of dense subgraphs. Although DSC-Flow-Iter leaves many nodes unclustered, it is competitive with leading methods and has high-precision and low-recall, making it complementary to modularity-based methods that typically have high recall but lower precision. Based on this observation, we introduce a novel cluster ensemble technique that combines DSC-Flow-Iter with modularity-based clustering, to provide improved accuracy. We show that our proposed pipeline, which uses this ensemble technique, outperforms its individual components and improves upon the baseline techniques on a large collection of synthetic networks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networks</title>
<link>https://arxiv.org/abs/2508.17236</link>
<guid>https://arxiv.org/abs/2508.17236</guid>
<content:encoded><![CDATA[
<div> Keywords: high-order relationships, real-world networks, dynamics, LINCOLN, hyperedge prediction

Summary:<br /><br />
Real-world networks exhibit high-order relationships among objects and evolve over time. Two key characteristics of high-order dynamics are identified: short-term structural and temporal influence, and long-term periodic re-appearance. To address these dynamics, LINCOLN, a method for Learning hIgh-order dyNamiCs Of reaL-world Networks, is proposed. LINCOLN incorporates bi-interactional hyperedge encoding for short-term patterns, periodic time injection, and intermediate node representation for long-term patterns. Experimental results demonstrate that LINCOLN outperforms nine existing methods in predicting dynamic hyperedges. <div>
arXiv:2508.17236v1 Announce Type: new 
Abstract: Real-world networks have high-order relationships among objects and they evolve over time. To capture such dynamics, many works have been studied in a range of fields. Via an in-depth preliminary analysis, we observe two important characteristics of high-order dynamics in real-world networks: high-order relations tend to (O1) have a structural and temporal influence on other relations in a short term and (O2) periodically re-appear in a long term. In this paper, we propose LINCOLN, a method for Learning hIgh-order dyNamiCs Of reaL-world Networks, that employs (1) bi-interactional hyperedge encoding for short-term patterns, (2) periodic time injection and (3) intermediate node representation for long-term patterns. Via extensive experiments, we show that LINCOLN outperforms nine state-of-the-art methods in the dynamic hyperedge prediction task.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Straddling Two Platforms: From Twitter to Mastodon, an Analysis of the Evolution of an Unfinished Social Media Migration</title>
<link>https://arxiv.org/abs/2508.17563</link>
<guid>https://arxiv.org/abs/2508.17563</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, content moderation, personal data management, commercial exploitation, decentralised platform <br />
Summary: 
The study examines the migration of users from Twitter to Mastodon following Elon Musk's acquisition of Twitter in 2022. It analyzes the onboarding process of 19,000 users who switched to Mastodon, mainly consisting of academics, scientists, and journalists. The migration was a response to concerns about Twitter's direction under Musk's ownership. However, users chose to maintain a presence on both platforms due to the difficulty in replicating Twitter's communities on Mastodon's decentralised platform. This led to a partial loss of social capital and greater fragmentation of user communities. The study underscores the challenges of transitioning between centralized and decentralized social media platforms, highlighting the unique characteristics and limitations of each platform. <br /><br /> <div>
arXiv:2508.17563v1 Announce Type: new 
Abstract: Social media have been fundamental in the daily lives of millions of people, but they have raised concerns about content moderation policies, the management of personal data, and their commercial exploitation. The acquisition of Twitter (now X) by Elon Musk in 2022 generated concerns among Twitter users regarding changes in the platform's direction, prompting a migration campaign by some user groups to the federated network Mastodon. This study reviews the onboarding of users to this decentralised platform between 2016 and 2022 and analyses the migration of 19,000 users who identified themselves as supporters of the platform switch. The results show that the migration campaign was a reactive response to Elon Musk's acquisition of Twitter and was led by a group of highly active academics, scientists, and journalists. However, a complete transition was not realised, as users preferred to straddle their presence on both platforms. Mastodon's decentralisation made it difficult to exactly replicate Twitter's communities, resulting in a partial loss of these users' social capital and greater fragmentation of these user communities, which highlights the intrinsic differences between both platforms.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connected Theorems: A Graph-Based Approach to Evaluating Mathematical Results</title>
<link>https://arxiv.org/abs/2508.17596</link>
<guid>https://arxiv.org/abs/2508.17596</guid>
<content:encoded><![CDATA[
<div> Keyword: mathematical results, evaluation, data-driven approach, citation relationships, influence scores
Summary: 
The article proposes a data-driven approach for evaluating mathematical results, aiming to complement traditional human judgment methods. By constructing a hierarchical graph linking theorems, papers, and fields and using a PageRank-style algorithm to compute influence scores, the framework analyzes the evolution of field rankings over time and quantifies the impact between fields. The goal is to develop more advanced, quantitative methods for evaluating mathematical research and provide a complementary tool for expert assessment. The approach provides a systematic way to assess researchers' contributions and shape the direction of the field, leveraging citation relationships to determine influence scores and track the evolution of research impact. This data-driven method offers a new perspective on evaluating mathematical research and has the potential to enhance the assessment process in the field. 
<br /><br />Summary: <div>
arXiv:2508.17596v1 Announce Type: new 
Abstract: The evaluation of mathematical results plays a central role in assessing researchers' contributions and shaping the direction of the field. Currently, such evaluations rely primarily on human judgment, whether through journal peer review or committees at research institutions. To complement these traditional processes, we propose a data-driven approach. We construct a hierarchical graph linking theorems, papers, and fields to capture their citation relationships. We then introduce a PageRank-style algorithm to compute influence scores for these entities. Using these scores, we analyze the evolution of field rankings over time and quantify the impact between fields. We hope this framework can contribute to the development of more advanced, quantitative methods for evaluating mathematical research and serve as a complement to expert assessment.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-Based Social Bot via an Adversarial Learning Framework</title>
<link>https://arxiv.org/abs/2508.17711</link>
<guid>https://arxiv.org/abs/2508.17711</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model (LLM), social media, generative capabilities, adversarial learning framework, human-like behavior

Summary:<br />
Developing Large Language Model agents that exhibit human-like behavior is a challenging research task. EvoBot, an Evolving LLM-based social Bot, enhances generative capabilities through an adversarial learning framework. It refines its content generation through Supervised Fine-Tuning and Direct Preference Optimization, guided by feedback from a co-adapting Detector. EvoBot generates human-like content aligned with diverse user profiles and demonstrates strong social responsiveness in modeling opinion dynamics and information spread in simulations. The framework produces a robust Detector with broader utility for agent development and detection tasks. <div>
arXiv:2508.17711v1 Announce Type: new 
Abstract: Developing Large Language Model (LLM) agents that exhibit human-like behavior, encompassing not only individual heterogeneity rooted in unique user profiles but also adaptive response to socially connected neighbors, is a significant research challenge. Social media platforms, with their diverse user data and explicit social structures, provide an ideal testbed for such investigations. This paper introduces EvoBot, an \textbf{Evo}lving LLM-based social \textbf{Bot} that significantly enhances human-like generative capabilities through a novel adversarial learning framework. EvoBot is initialized by Supervised Fine-Tuning (SFT) on representative data from social media and then iteratively refines its generation of sophisticated, human-like content via Direct Preference Optimization (DPO). This refinement is guided by feedback from a co-adapting \textbf{Detector} which concurrently improves its ability to distinguish EvoBot from humans, thereby creating an increasingly challenging learning environment for EvoBot. Experiments demonstrate that EvoBot generates content aligned with diverse user profiles, increasingly bypassing the co-adapting Detector through human-like expression. Moreover, it exhibits strong social responsiveness, more accurately modeling real-world opinion dynamics and information spread in multi-agent simulations. The framework also yields a more robust Detector, underscoring its broader utility for both advanced agent development and related detection tasks. The code is available at https://github.com/kfq20/EvoBot.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Generative AI to Uncover What Drives Player Enjoyment in PC and VR Games</title>
<link>https://arxiv.org/abs/2508.16596</link>
<guid>https://arxiv.org/abs/2508.16596</guid>
<content:encoded><![CDATA[
<div> Keywords: video games, player enjoyment, generative AI, machine learning, game reviews

Summary:
This study utilizes generative AI and machine learning techniques, specifically Microsoft Phi-4 LLM and XGBoost, to analyze and quantify game reviews from the Steam and Meta Quest stores. By converting qualitative feedback into structured data, the study evaluates various game design elements, monetization models, and platform-specific trends. The analysis reveals distinct patterns in player preferences between PC and VR games, identifying factors that contribute to increased player satisfaction. Leveraging Google Cloud for data storage and processing, the study establishes a scalable framework for large-scale game review analysis. The insights obtained from this analysis offer actionable guidance for game developers to optimize game mechanics, pricing strategies, and player engagement. <div>
arXiv:2508.16596v1 Announce Type: cross 
Abstract: As video games continue to evolve, understanding what drives player enjoyment remains a key challenge. Player reviews provide valuable insights, but their unstructured nature makes large-scale analysis difficult. This study applies generative AI and machine learning, leveraging Microsoft Phi-4 LLM and XGBoost, to quantify and analyze game reviews from Steam and Meta Quest stores. The approach converts qualitative feedback into structured data, enabling comprehensive evaluation of key game design elements, monetization models, and platform-specific trends. The findings reveal distinct patterns in player preferences across PC and VR games, highlighting factors that contribute to higher player satisfaction. By integrating Google Cloud for largescale data storage and processing, this study establishes a scalable framework for game review analysis. The study's insights offer actionable guidance for game developers, helping optimize game mechanics, pricing strategies, and player engagement.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Multimodal LLMs See Sentiment?</title>
<link>https://arxiv.org/abs/2508.16873</link>
<guid>https://arxiv.org/abs/2508.16873</guid>
<content:encoded><![CDATA[
<div> Keywords: visual content, sentiment analysis, Multimodal Large Language Models, fine-tuning, affective computing 

Summary: 
The paper introduces MLLMsent, a framework that explores how Multimodal Large Language Models (MLLMs) can reason about sentiment in visual content. The framework includes three perspectives: direct sentiment classification from images using MLLMs, sentiment analysis on automatically generated image descriptions, and fine-tuning Language Models on sentiment-labeled image descriptions. Experiments on a benchmark dataset show that the fine-tuned approach achieves state-of-the-art results, outperforming baseline methods across different sentiment polarity categories and levels of evaluators' agreement. Notably, in a cross-dataset test, the model performs well even without training on the new data, surpassing the best runner-up method. These results demonstrate the potential of the proposed visual reasoning scheme for advancing affective computing and set new benchmarks for future research. 

Summary: <div>
arXiv:2508.16873v1 Announce Type: cross 
Abstract: Understanding how visual content communicates sentiment is critical in an era where online interaction is increasingly dominated by this kind of media on social platforms. However, this remains a challenging problem, as sentiment perception is closely tied to complex, scene-level semantics. In this paper, we propose an original framework, MLLMsent, to investigate the sentiment reasoning capabilities of Multimodal Large Language Models (MLLMs) through three perspectives: (1) using those MLLMs for direct sentiment classification from images; (2) associating them with pre-trained LLMs for sentiment analysis on automatically generated image descriptions; and (3) fine-tuning the LLMs on sentiment-labeled image descriptions. Experiments on a recent and established benchmark demonstrate that our proposal, particularly the fine-tuned approach, achieves state-of-the-art results outperforming Lexicon-, CNN-, and Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively, across different levels of evaluators' agreement and sentiment polarity categories. Remarkably, in a cross-dataset test, without any training on these new data, our model still outperforms, by up to 8.26%, the best runner-up, which has been trained directly on them. These results highlight the potential of the proposed visual reasoning scheme for advancing affective computing, while also establishing new benchmarks for future research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Pricing Through Strategic User Profiling in Social Networks</title>
<link>https://arxiv.org/abs/2508.17111</link>
<guid>https://arxiv.org/abs/2508.17111</guid>
<content:encoded><![CDATA[
<div> privacy-enhancing technologies, user profiling, personalized pricing, online social networks, Bayesian game

Summary:
This paper investigates how users can strategically manage their social activities to avoid personalized pricing based on their profiles derived from online social network interactions. A dynamic Bayesian game model is formulated to capture the interactions between the seller and users under asymmetric information. The equilibrium analysis reveals that with improving profiling technology, sellers tend to increase uniform prices to incentivize user engagement on social networks for better profiling. However, the implementation of informed consent policies to inform users of data access and profiling practices may result in most users being worse off. This suggests that regulatory efforts aimed at enhancing user privacy awareness could unintentionally lead to reduced payoffs for users. <div>
arXiv:2508.17111v1 Announce Type: cross 
Abstract: Traditional user profiling techniques rely on browsing history or purchase records to identify users' willingness to pay. This enables sellers to offer personalized prices to profiled users while charging only a uniform price to non-profiled users. However, the emergence of privacy-enhancing technologies has caused users to actively avoid on-site data tracking. Today, major online sellers have turned to public platforms such as online social networks to better track users' profiles from their product-related discussions. This paper presents the first analytical study on how users should best manage their social activities against potential personalized pricing, and how a seller should strategically adjust her pricing scheme to facilitate user profiling in social networks. We formulate a dynamic Bayesian game played between the seller and users under asymmetric information. The key challenge of analyzing this game comes from the double couplings between the seller and the users as well as among the users. Furthermore, the equilibrium analysis needs to ensure consistency between users' revealed information and the seller's belief under random user profiling. We address these challenges by alternately applying backward and forward induction, and successfully characterize the unique perfect Bayesian equilibrium (PBE) in closed form. Our analysis reveals that as the accuracy of profiling technology improves, the seller tends to raise the equilibrium uniform price to motivate users' increased social activities and facilitate user profiling. However, this results in most users being worse off after the informed consent policy is imposed to ensure users' awareness of data access and profiling practices by potential sellers. This finding suggests that recent regulatory evolution towards enhancing users' privacy awareness may have unintended consequences of reducing users' payoffs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Clustering for Large Multi-Relational Graphs</title>
<link>https://arxiv.org/abs/2508.17388</link>
<guid>https://arxiv.org/abs/2508.17388</guid>
<content:encoded><![CDATA[
<div> feature vectors, optimization, clustering, multi-relational graphs, scalability

Summary: 
DEMM and DEMM+ are novel algorithms proposed in this paper to address the challenges of partitioning node sets in multi-relational graphs (MRGs) into clusters. These algorithms focus on optimizing node feature vectors and minimizing the Dirichlet energy of clustering results over the node affinity graph. DEMM+ stands out for its scalability and efficiency, achieved through innovative optimizations. Key contributions include an approximation solver for constructing node feature vectors and a problem transformation technique for linear-time clustering without the dense affinity matrix. DEMM+ is extended to handle attribute-less MRGs as well. Extensive experiments demonstrate that DEMM+ outperforms 20 baseline methods in clustering quality while also being faster in many cases. <div>
arXiv:2508.17388v1 Announce Type: cross 
Abstract: Multi-relational graphs (MRGs) are an expressive data structure for modeling diverse interactions/relations among real objects (i.e., nodes), which pervade extensive applications and scenarios. Given an MRG G with N nodes, partitioning the node set therein into K disjoint clusters (MRGC) is a fundamental task in analyzing MRGs, which has garnered considerable attention. However, the majority of existing solutions towards MRGC either yield severely compromised result quality by ineffective fusion of heterogeneous graph structures and attributes, or struggle to cope with sizable MRGs with millions of nodes and billions of edges due to the adoption of sophisticated and costly deep learning models.
  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to address the limitations above. Specifically, our algorithms are built on novel two-stage optimization objectives, where the former seeks to derive high-caliber node feature vectors by optimizing the multi-relational Dirichlet energy specialized for MRGs, while the latter minimizes the Dirichlet energy of clustering results over the node affinity graph. In particular, DEMM+ achieves significantly higher scalability and efficiency over our based method DEMM through a suite of well-thought-out optimizations. Key technical contributions include (i) a highly efficient approximation solver for constructing node feature vectors, and (ii) a theoretically-grounded problem transformation with carefully-crafted techniques that enable linear-time clustering without explicitly materializing the NxN dense affinity matrix. Further, we extend DEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive experiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit that DEMM+ is consistently superior in terms of clustering quality measured against ground-truth labels, while often being remarkably faster.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Price of Uncertainty for Consensus Games</title>
<link>https://arxiv.org/abs/2508.17557</link>
<guid>https://arxiv.org/abs/2508.17557</guid>
<content:encoded><![CDATA[
<div> game-theoretic models, uncertainty, social cost, consensus games, adversarial perturbations

Summary:
This paper examines the impact of uncertainty in observed data on game-theoretic models. It introduces adversarial perturbations to players' observed costs and measures the resulting increase in social cost as the price of uncertainty. The study focuses on consensus games and establishes a tight bound on the price of uncertainty, showing that it is proportional to $\varepsilon^2 n^2$ for all $\varepsilon = \Omega\mathopen{}\left(n^{-1/4}\right)$. This result represents an improvement over previous bounds, with a lower bound of $\Omega(\varepsilon^3 n^2)$ and an upper bound of $O(\varepsilon n^2). By considering uncertainty in real-world settings, this research sheds light on the importance of accurate information in game theory applications. <div>
arXiv:2508.17557v1 Announce Type: cross 
Abstract: Many game-theoretic models assume that players have access to accurate information, but uncertainty in observed data is frequently present in real-world settings. In this paper, we consider a model of uncertainty where adversarial perturbations of relative magnitude $1+\varepsilon$ are introduced to players' observed costs. The effect of uncertainty on social cost is denoted as the price of uncertainty. We prove a tight bound on the price of uncertainty for consensus games of $\Theta(\varepsilon^2 n^2)$ for all $\varepsilon = \Omega\mathopen{}\left(n^{-1/4}\right)$. This improves a previous lower bound of $\Omega(\varepsilon^3 n^2)$ as well as a previous upper bound of $O(\varepsilon n^2)$.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How does node centrality in a financial network affect asset price prediction?</title>
<link>https://arxiv.org/abs/2305.03245</link>
<guid>https://arxiv.org/abs/2305.03245</guid>
<content:encoded><![CDATA[
<div> Keywords: financial networks, node centrality, price forecasting, random forest algorithm, deep learning <br />
Summary: <br />
1. Systemically important nodes in complex financial networks play crucial roles in asset price forecasting. <br />
2. Contrary to intuition, factors with low centrality demonstrate better forecasting ability in global asset networks. <br />
3. Nodes with low centrality can be predicted with greater accuracy in terms of price direction. <br />
4. Information theory provides a framework for explaining the unexpected observations regarding node centrality and price forecasting. <br />
5. Factor selection for asset price prediction in complex systems should prioritize factors with low centrality over those with high centrality. <br />
6. The study's findings are validated using a hybrid random forest algorithm and an alternative deep learning method. <br /> <div>
arXiv:2305.03245v2 Announce Type: replace-cross 
Abstract: In complex financial networks, systemically important nodes usually play crucial roles. Asset price forecasting is important for describing the evolution of a financial network. Naturally, the question arises as to whether node centrality impacts the effectiveness of price forecasting. To explore this, we examine networks composed of major global assets and investigate how node centrality affects price forecasting using a hybrid random forest algorithm. Our findings reveal two counterintuitive phenomena: (i) factors with low centrality usually have better forecasting ability, and (ii) nodes with low centrality can be predicted more accurately in direction. These unexpected observations can be explained from the perspective of information theory. Moreover, our research suggests a criterion for factor selection: when predicting an asset price in a complex system, factors with low centrality should be selected rather than only factors with high centrality. Finally, we verify the robustness of our results using an alternative deep learning method.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-reinforcing cascades: A spreading model for beliefs or products of varying intensity or quality</title>
<link>https://arxiv.org/abs/2411.00714</link>
<guid>https://arxiv.org/abs/2411.00714</guid>
<content:encoded><![CDATA[
<div> social contagions, spread, self-reinforcement, cascade dynamics, power-law distributions <br />
Summary:<br />
The study explores the impact of self-reinforcement mechanisms in cascade dynamics, focusing on the spread of ideas, beliefs, and innovations in social networks. Unlike traditional models that assume fixed transmission mechanisms, the research considers the recursive nature of the process, where ideas can be reinforced and beliefs strengthened as they spread. The findings reveal a critical regime characterized by power-law cascade size distributions with non-universal scaling exponents. This regime challenges classic models by demonstrating critical-like behavior across a wide range of parameters, rather than requiring precise fine-tuning at a specific critical point. The results suggest that self-reinforced cascades may explain the prevalence of power-law distributions observed in empirical social data, shedding light on the dynamics of social contagions. <br /> 
Summary: <div>
arXiv:2411.00714v2 Announce Type: replace-cross 
Abstract: Models of how things spread often assume that transmission mechanisms are fixed over time. However, social contagions--the spread of ideas, beliefs, innovations--can lose or gain in momentum as they spread: ideas can get reinforced, beliefs strengthened, products refined. We study the impacts of such self-reinforcement mechanisms in cascade dynamics. We use different mathematical modeling techniques to capture the recursive, yet changing nature of the process. We find a critical regime with a range of power-law cascade size distributions with non-universal scaling exponents. This regime clashes with classic models, where criticality requires fine tuning at a precise critical point. Self-reinforced cascades produce critical-like behavior over a wide range of parameters, which may help explain the ubiquity of power-law distributions in empirical social data.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Word-of-Mouth and Private-Prior Sequential Social Learning</title>
<link>https://arxiv.org/abs/2504.02913</link>
<guid>https://arxiv.org/abs/2504.02913</guid>
<content:encoded><![CDATA[
<div> Word-of-Mouth, social learning, rational agents, dynamical system, noise<br />
Summary:<br />
The paper explores the Word-of-Mouth (WoM) social learning paradigm, where agents estimate a dynamical system's state based on each other's actions. The first agent receives noisy measurements, and subsequent agents use a degraded version of the previous estimate. The final agent's belief is publicly broadcast for all agents to adopt. Through theoretical analysis and simulations, it is found that some agents benefit from the final agent's belief, while others experience performance deterioration. The study highlights the complexity of social learning dynamics and the varying effects on different agents within the WoM framework. <div>
arXiv:2504.02913v3 Announce Type: replace-cross 
Abstract: Social learning constitutes a fundamental framework for studying interactions among rational agents who observe each other's actions but lack direct access to individual beliefs. This paper investigates a specific social learning paradigm known as Word-of-Mouth (WoM), where a series of agents seeks to estimate the state of a dynamical system. The first agent receives noisy measurements of the state, while each subsequent agent relies solely on a degraded version of her predecessor's estimate. A defining feature of WoM is that the final agent's belief is publicly broadcast and subsequently adopted by all agents, in place of their own. We analyze this setting theoretically and through numerical simulations, noting that some agents benefit from using the belief of the last agent, while others experience performance deterioration.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dac-Fake: A Divide and Conquer Framework for Detecting Fake News on Social Media</title>
<link>https://arxiv.org/abs/2508.16223</link>
<guid>https://arxiv.org/abs/2508.16223</guid>
<content:encoded><![CDATA[
<div> detecting fake news, social media, automated detection, linguistic features, accuracy rate
<br />
<br />
Summary: 
The article introduces DaCFake, a novel fake news detection model that utilizes a divide and conquer strategy combining content and context-based features for rapid and automated detection of fake news on social media platforms. The model extracts over eighty linguistic features from news articles and integrates them with either a continuous bag of words or a skipgram model to enhance detection accuracy. Impressive accuracy rates of 97.88%, 96.05%, and 97.32% were achieved on three datasets including Kaggle, McIntire + PolitiFact, and Reuter. Ten-fold cross-validation was employed to further enhance the model's robustness and accuracy. The results demonstrate the effectiveness of DaCFake in early detection of fake news, offering a promising solution to curb misinformation on social media platforms. 
<br /> <div>
arXiv:2508.16223v1 Announce Type: new 
Abstract: With the rapid evolution of technology and the Internet, the proliferation of fake news on social media has become a critical issue, leading to widespread misinformation that can cause societal harm. Traditional fact checking methods are often too slow to prevent the dissemination of false information. Therefore, the need for rapid, automated detection of fake news is paramount. We introduce DaCFake, a novel fake news detection model using a divide and conquer strategy that combines content and context based features. Our approach extracts over eighty linguistic features from news articles and integrates them with either a continuous bag of words or a skipgram model for enhanced detection accuracy. We evaluated the performance of DaCFake on three datasets including Kaggle, McIntire + PolitiFact, and Reuter achieving impressive accuracy rates of 97.88%, 96.05%, and 97.32%, respectively. Additionally, we employed a ten-fold cross validation to further enhance the model's robustness and accuracy. These results highlight the effectiveness of DaCFake in early detection of fake news, offering a promising solution to curb misinformation on social media platforms.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anti-establishment sentiment on TikTok: Implications for understanding influence(rs) and expertise on social media</title>
<link>https://arxiv.org/abs/2508.16453</link>
<guid>https://arxiv.org/abs/2508.16453</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, anti-establishment sentiment, TikTok, conspiracy theories, engagement

Summary: 
The study investigates the prevalence of anti-establishment sentiment (AES) on the social media platform TikTok, focusing on content related to finance, wellness, and conspiracy theories. Content creators on TikTok often position themselves as experts and may promote anti-establishment views to gain followers. The research reveals that AES is most common in conspiracy theory content, but less so in finance and wellness topics. However, engagement with anti-establishment content varies across different areas. Despite the relatively low prevalence of AES in finance and wellness content, there are indications of platform incentives encouraging users to post such content. The findings suggest a potential connection between social media environments, anti-establishment views, and distrust of institutions, highlighting the importance of understanding how social media influences people's attitudes towards public institutions. 

<br /><br />Summary: <div>
arXiv:2508.16453v1 Announce Type: new 
Abstract: Distrust of public serving institutions and anti-establishment views are on the rise (especially in the U.S.). As people turn to social media for information, it is imperative to understand whether and how social media environments may be contributing to distrust of institutions. In social media, content creators, influencers, and other opinion leaders often position themselves as having expertise and authority on a range of topics from health to politics, and in many cases devalue and dismiss institutional expertise to build a following and increase their own visibility. However, the extent to which this content appears and whether such content increases engagement is unclear. This study analyzes the prevalence of anti-establishment sentiment (AES) on the social media platform TikTok. Despite its popularity as a source of information, TikTok remains relatively understudied and may provide important insights into how people form attitudes towards institutions. We employ a computational approach to label TikTok posts as containing AES or not across topical domains where content creators tend to frame themselves as experts: finance and wellness. As a comparison, we also consider the topic of conspiracy theories, where AES is expected to be common. We find that AES is most prevalent in conspiracy theory content, and relatively rare in content related to the other two topics. However, we find that engagement patterns with such content varies by area, and that there may be platform incentives for users to post content that expresses anti-establishment sentiment.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embarrassed to observe: The effects of directive language in brand conversation</title>
<link>https://arxiv.org/abs/2508.15826</link>
<guid>https://arxiv.org/abs/2508.15826</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, directive language, brand-consumer interactions, engagement, brand relationship <br />
Summary: This study explores the effects of directive brand language on consumers in social media conversations. The use of directive language by brands can lead to reduced engagement among consumers who observe such interactions. This is because consumers may perceive brands using directive language as face-threatening, leading to feelings of vicarious embarrassment and decreased engagement. The impact of directive language is stronger in nonproduct-centered conversations compared to product-centered ones, as consumers expect more freedom in mundane conversations. However, a strong brand relationship can mitigate the negative effects of directive language. This study underscores the significance of context in interactive communication, particularly in the realm of social media and brand management. <br /><br />Summary: <div>
arXiv:2508.15826v1 Announce Type: cross 
Abstract: In social media, marketers attempt to influence consumers by using directive language, that is, expressions designed to get consumers to take action. While the literature has shown that directive messages in advertising have mixed results for recipients, we know little about the effects of directive brand language on consumers who see brands interacting with other consumers in social media conversations. On the basis of a field study and three online experiments, this study shows that directive language in brand conversation has a detrimental downstream effect on engagement of consumers who observe such exchanges. Specifically, in line with Goffman's facework theory, because a brand that encourages consumers to react could be perceived as face-threatening, consumers who see a brand interacting with others in a directive way may feel vicarious embarrassment and engage less (compared with a conversation without directive language). In addition, we find that when the conversation is nonproduct-centered (vs. product-centered), consumers expect more freedom, as in mundane conversations, even for others; therefore, directive language has a stronger negative effect. However, in this context, the strength of the brand relationship mitigates this effect. Thus, this study contributes to the literature on directive language and brand-consumer interactions by highlighting the importance of context in interactive communication, with direct relevance for social media and brand management.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bayesian framework for opinion dynamics models</title>
<link>https://arxiv.org/abs/2508.16539</link>
<guid>https://arxiv.org/abs/2508.16539</guid>
<content:encoded><![CDATA[
<div> Bayesian framework, opinion dynamics models, signal score, rational foundations, cognitive constraints <br />
Summary:<br />
This work presents a Bayesian framework that unifies various opinion dynamics models by considering individuals' opinions as expected values of beliefs represented as random variables with prior distributions. When individuals receive a signal, their belief distribution is updated using Bayes' rule, incorporating prior belief, bias, and noise. By manipulating the prior, bias, and noise distributions, a wide range of models such as DeGroot, bounded confidence, and bounded shift are recovered. The signal score plays a crucial role in determining the mathematical structure of each model, influencing their behavior for both small and large signals. While all models converge to DeGroot's linear update rule for small signals, they exhibit different behavior for large signals. This framework not only reveals connections among previously disparate models but also provides a systematic approach for creating new models, shedding light on the rational basis of opinion formation within cognitive limitations. <br /> <div>
arXiv:2508.16539v1 Announce Type: cross 
Abstract: This work introduces a Bayesian framework that unifies a wide class of opinion dynamics models. In this framework, an individual's opinion on a topic is the expected value of their belief, represented as a random variable with a prior distribution. Upon receiving a signal, modeled as the prior belief plus a bias term and subject to zero-mean noise with a known distribution, the individual updates their belief distribution via Bayes' rule. By systematically varying the prior, bias, and noise distributions, this approach recovers a broad array of opinion dynamics models, including DeGroot, bounded confidence, bounded shift, and models exhibiting overreaction or backfire effects. Our analysis shows that the signal score is the key determinant of each model's mathematical structure, governing both small- and large-signal behavior. All models converge to DeGroot's linear update rule for small signals, but diverge in their tail behavior for large signals. This unification not only reveals theoretical linkages among previously disconnected models but also provides a systematic method for generating new ones, offering insights into the rational foundations of opinion formation under cognitive constraints.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From chambers to echo chambers: Quantifying polarization with a second-neighbor approach applied to Twitter's climate discussion</title>
<link>https://arxiv.org/abs/2206.14501</link>
<guid>https://arxiv.org/abs/2206.14501</guid>
<content:encoded><![CDATA[
<div> polarization, social media, echo chambers, climate change, ideological patterns
<br />
Summary: 
The study examines polarization on social media platforms, particularly focusing on discussions about climate change on X (formerly Twitter) in 2019. Using chambers as second-order information sources, the researchers uncover ideological patterns and polarization dynamics. They identify echo chambers of both climate believers and skeptics, showing strong alignment within groups and minimal overlap between them. Their method allows for the classification of high-impact users based on their audience's chamber alignment, with coverage surpassing existing models. The study finds stable echo chamber structures over time, indicating persistent ideological polarization. Notably, polarization decreases and climate skepticism rises during the #FridaysForFuture strikes in September 2019, highlighting the fluidity of ideological beliefs on social media platforms. The analysis offers valuable insights into the dynamics of polarization and belief reinforcement in online discourse.
<br /> <div>
arXiv:2206.14501v3 Announce Type: replace 
Abstract: Social media platforms often foster environments where users primarily engage with content that aligns with their existing beliefs, thereby reinforcing their views and limiting exposure to opposing viewpoints. In this paper, we analyze X (formerly Twitter) discussions on climate change throughout 2019, using an unsupervised method centered on chambers--second-order information sources--to uncover ideological patterns at scale. Beyond direct connections, chambers capture shared sources of influence, revealing polarization dynamics efficiently and effectively. Analyzing retweet patterns, we identify echo chambers of climate believers and skeptics, revealing strong chamber overlap within ideological groups and minimal overlap between them, resulting in a robust bimodal structure that characterizes polarization. Our method enables us to infer the stance of high-impact users based on their audience's chamber alignment, allowing for the classification of over half the retweeting population with minimal cross-group interaction, in what we term augmented echo chamber classification. We benchmark our approach against manual labeling and a state-of-the-art latent ideology model, finding comparable performance but with nearly four times greater coverage. Moreover, we find that echo chamber structures remain stable over time, even as their members change significantly, suggesting that these structures are a persistent and emergent property of the system. Notably, polarization decreases and climate skepticism rises during the #FridaysForFuture strikes in September 2019. This chamber-based analysis offers valuable insights into the persistence and fluidity of ideological polarization on social media.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Chilling: Identifying Strategic Antisocial Behavior Online and Examining the Impact on Journalists</title>
<link>https://arxiv.org/abs/2508.15061</link>
<guid>https://arxiv.org/abs/2508.15061</guid>
<content:encoded><![CDATA[
<div> classification, online behavior, social media, Twitter, targeted attacks
<br />
Summary:<br />
The article introduces a new tree structured Transformer model for categorizing replies on social platforms like Twitter based on hierarchical conversation structures. This model can effectively detect different user groups such as attackers, supporters, and bystanders, and their latent strategies. The approach allows for exploration of strategic behaviors and their impact on journalists, other users, and conversational outcomes. The study reveals a correlation between the presence of attackers' interactions and chilling effects, leading to a slowdown in journalists' posting behavior. The findings emphasize the importance of social platforms developing tools to address coordinated toxicity, early detection of patterns of attacks, and providing journalists and users with real-time reporting tools to manage hostile interactions effectively. <div>
arXiv:2508.15061v1 Announce Type: new 
Abstract: On social platforms like Twitter, strategic targeted attacks are becoming increasingly common, especially against vulnerable groups such as female journalists. Two key challenges in identifying strategic online behavior are the complex structure of online conversations and the hidden nature of potential strategies that drive user behavior. To address these, we develop a new tree structured Transformer model that categorizes replies based on their hierarchical conversation structures. Extensive experiments demonstrate that our proposed classification model can effectively detect different user groups, namely attackers, supporters, and bystanders, and their latent strategies. To demonstrate the utility of our approach, we apply this classifier to real time Twitter data and conduct a series of quantitative analyses on the interactions between journalists with different groups of users. Our classification approach allows us to not only explore strategic behaviors of attackers but also those of supporters and bystanders who engage in online interactions. When examining the impact of online attacks, we find a strong correlation between the presence of attackers' interactions and chilling effects, where journalists tend to slow their subsequent posting behavior. This paper provides a deeper understanding of how different user groups engage in online discussions and highlights the detrimental effects of attacker presence on journalists, other users, and conversational outcomes. Our findings underscore the need for social platforms to develop tools that address coordinated toxicity. By detecting patterns of coordinated attacks early, platforms could limit the visibility of toxic content to prevent escalation. Additionally, providing journalists and users with tools for real time reporting could empower them to manage hostile interactions more effectively.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIP: Model-Agnostic Hypergraph Influence Prediction via Distance-Centrality Fusion and Neural ODEs</title>
<link>https://arxiv.org/abs/2508.15312</link>
<guid>https://arxiv.org/abs/2508.15312</guid>
<content:encoded><![CDATA[
<div> Keywords: influence prediction, hypergraphs, centrality indicators, neural networks, LSTM

Summary: 
The study introduces HIP, a framework for predicting user influence in social networks using hypergraphs. HIP combines multi-dimensional centrality indicators and a temporally reinterpreted distance matrix to represent node-level diffusion capacity. A multi-hop Hypergraph Neural Network (HNN) captures higher-order structural dependencies, while temporal correlations are modeled using a hybrid module of LSTM networks and Neural Ordinary Differential Equations (Neural ODEs). HIP is modular, allowing for the substitution of different components without compromising performance. Empirical evaluations on 14 real-world hypergraph datasets show that HIP outperforms existing baselines in prediction accuracy, resilience, and identification of top influencers. Importantly, HIP does not require knowledge of the spreading model or diffusion trajectories. The findings highlight HIP's effectiveness and adaptability as a general-purpose solution for influence prediction in complex hypergraph environments.

<br /><br />Summary: <div>
arXiv:2508.15312v1 Announce Type: new 
Abstract: Predicting user influence in social networks is a critical problem, and hypergraphs, as a prevalent higher-order modeling approach, provide new perspectives for this task. However, the absence of explicit cascade or infection probability data makes it particularly challenging to infer influence in hypergraphs. To address this, we introduce HIP, a unified and model-independent framework for influence prediction without knowing the underlying spreading model. HIP fuses multi-dimensional centrality indicators with a temporally reinterpreted distance matrix to effectively represent node-level diffusion capacity in the absence of observable spreading. These representations are further processed through a multi-hop Hypergraph Neural Network (HNN) to capture complex higher-order structural dependencies, while temporal correlations are modeled using a hybrid module that combines Long Short-Term Memory (LSTM) networks and Neural Ordinary Differential Equations (Neural ODEs). Notably, HIP is inherently modular: substituting the standard HGNN with the advanced DPHGNN, and the LSTM with xLSTM, yields similarly strong performance, showcasing its architectural generality and robustness. Empirical evaluations across 14 real-world hypergraph datasets demonstrate that HIP consistently surpasses existing baselines in prediction accuracy, resilience, and identification of top influencers, all without relying on any diffusion trajectories or prior knowledge of the spreading model. These findings underline HIP's effectiveness and adaptability as a general-purpose solution for influence prediction in complex hypergraph environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDBMS: A Context-Aware Hybrid Graph Traversal Algorithm for Efficient Information Discovery in Social Networks</title>
<link>https://arxiv.org/abs/2508.14092</link>
<guid>https://arxiv.org/abs/2508.14092</guid>
<content:encoded><![CDATA[
<div> Algorithm, Graph traversal, Hybrid, Probabilistic, Information retrieval

Summary: 
The paper introduces a new graph-searching algorithm, the Hybrid Depth-Breadth Meaningful Search (HDBMS), that combines Depth-First Search and Breadth-First Search techniques with probabilistic node transitions. HDBMS dynamically adapts its exploration strategy based on the likelihood of nodes containing desired information, leading to contextually relevant search paths. Experimental results on directed graphs show that HDBMS maintains computational efficiency while outperforming traditional algorithms in identifying meaningful paths. By integrating probabilistic decision-making, HDBMS creates an adaptive traversal order that balances exploration across depth and breadth, making it effective for applications like information retrieval, social network analysis, and recommendation systems. The algorithm's robustness in scenarios with unpredictable valuable connections positions it as a powerful alternative to conventional graph-searching methods. 

<br /><br />Summary: <div>
arXiv:2508.14092v1 Announce Type: new 
Abstract: Graph-searching algorithms play a crucial role in various computational domains, enabling efficient exploration and pathfinding in structured data. Traditional approaches, such as Depth-First Search (DFS) and Breadth-First Search (BFS), follow rigid traversal patterns -- DFS explores branches exhaustively, while BFS expands level by level. In this paper, we propose the Hybrid Depth-Breadth Meaningful Search (HDBMS) algorithm, a novel graph traversal method that dynamically adapts its exploration strategy based on probabilistic node transitions. Unlike conventional methods, HDBMS prioritizes traversal paths by estimating the likelihood that a node contains the desired information, ensuring a more contextually relevant search. Through extensive experimentation on diverse directed graphs with varying structural properties, we demonstrate that HDBMS not only maintains competitive computational efficiency but also outperforms traditional algorithms in identifying meaningful paths. By integrating probabilistic decision-making, HDBMS constructs an adaptive and structured traversal order that balances exploration across depth and breadth, making it particularly effective in applications such as information retrieval, social network analysis, and recommendation systems. Our results highlight the robustness of HDBMS in scenarios where the most valuable connections emerge unpredictably, positioning it as a powerful alternative to traditional graph-searching techniques.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Dissipative Graph Propagation for Non-Local Community Detection</title>
<link>https://arxiv.org/abs/2508.14097</link>
<guid>https://arxiv.org/abs/2508.14097</guid>
<content:encoded><![CDATA[
<div> Community detection, graphs, heterophilic, graph neural networks, unsupervised<br />
Summary:<br />
Community detection in heterophilic graphs is challenging due to distantly connected similar nodes. Graph neural networks struggle with this due to local message passing. The Unsupervised Antisymmetric Graph Neural Network (uAGNN) tackles this by propagating long-range information effectively using non-dissipative dynamical systems. uAGNN utilizes antisymmetric weight matrices to capture both local and global structures, outperforming traditional methods in high and medium heterophilic settings. Extensive experiments show uAGNN's potential as a powerful tool for unsupervised community detection in diverse graph environments.<br /> <div>
arXiv:2508.14097v1 Announce Type: new 
Abstract: Community detection in graphs aims to cluster nodes into meaningful groups, a task particularly challenging in heterophilic graphs, where nodes sharing similarities and membership to the same community are typically distantly connected. This is particularly evident when this task is tackled by graph neural networks, since they rely on an inherently local message passing scheme to learn the node representations that serve to cluster nodes into communities. In this work, we argue that the ability to propagate long-range information during message passing is key to effectively perform community detection in heterophilic graphs. To this end, we introduce the Unsupervised Antisymmetric Graph Neural Network (uAGNN), a novel unsupervised community detection approach leveraging non-dissipative dynamical systems to ensure stability and to propagate long-range information effectively. By employing antisymmetric weight matrices, uAGNN captures both local and global graph structures, overcoming the limitations posed by heterophilic scenarios. Extensive experiments across ten datasets demonstrate uAGNN's superior performance in high and medium heterophilic settings, where traditional methods fail to exploit long-range dependencies. These results highlight uAGNN's potential as a powerful tool for unsupervised community detection in diverse graph environments.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.14088</link>
<guid>https://arxiv.org/abs/2508.14088</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, human mobility, collective behaviors, spatiotemporal dependencies, attention mechanism

Summary:<br />
Anomalies in human mobility play a crucial role in applications like public safety and urban planning. This study introduces the concept of collective anomaly detection, which focuses on irregularities in collective mobility behaviors rather than individual patterns. A novel model called CoBAD is proposed to address this challenge by leveraging Collective Event Sequences and a two-stage attention mechanism to capture spatiotemporal dependencies among individuals. CoBAD is trained on large-scale collective behavior data and can detect unexpected co-occurrence anomalies and absence anomalies, which have not been extensively explored in previous work. Experimental results on large mobility datasets show that CoBAD outperforms existing baselines by a significant margin, demonstrating improvements in AUCROC and AUCPR. The source code for CoBAD is publicly available for further research and development.<br /> 

Summary: <div>
arXiv:2508.14088v1 Announce Type: cross 
Abstract: Detecting anomalies in human mobility is essential for applications such as public safety and urban planning. While traditional anomaly detection methods primarily focus on individual movement patterns (e.g., a child should stay at home at night), collective anomaly detection aims to identify irregularities in collective mobility behaviors across individuals (e.g., a child is at home alone while the parents are elsewhere) and remains an underexplored challenge. Unlike individual anomalies, collective anomalies require modeling spatiotemporal dependencies between individuals, introducing additional complexity. To address this gap, we propose CoBAD, a novel model designed to capture Collective Behaviors for human mobility Anomaly Detection. We first formulate the problem as unsupervised learning over Collective Event Sequences (CES) with a co-occurrence event graph, where CES represents the event sequences of related individuals. CoBAD then employs a two-stage attention mechanism to model both the individual mobility patterns and the interactions across multiple individuals. Pre-trained on large-scale collective behavior data through masked event and link reconstruction tasks, CoBAD is able to detect two types of collective anomalies: unexpected co-occurrence anomalies and absence anomalies, the latter of which has been largely overlooked in prior work. Extensive experiments on large-scale mobility datasets demonstrate that CoBAD significantly outperforms existing anomaly detection baselines, achieving an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is available at https://github.com/wenhaomin/CoBAD.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Small-World Beneath LEO Satellite Coverage: Ground Hubs in Multi-Shell Constellations</title>
<link>https://arxiv.org/abs/2508.14335</link>
<guid>https://arxiv.org/abs/2508.14335</guid>
<content:encoded><![CDATA[
<div> small-world characteristics, GS relays, feeder links, betweenness analysis, spatial coverage

Summary:<br /><br />This paper examines a six-shell mega-constellation with over 10,000 satellites and 198 gateway stations, focusing on routing efficiency and inter-shell communication. The analysis reveals that the constellation displays strong small-world properties, facilitating efficient routing despite its large size. Gateway stations play a crucial role in enhancing connectivity between shells by bridging disconnected components. Feeder links help reduce average path length, improving long-distance communication feasibility. Further, betweenness analysis highlights load imbalances among gateway stations, emphasizing the need for traffic-aware management strategies. The mega-constellation demonstrates excellent spatial coverage and resilience, maintaining connectivity and low routing costs even in the event of gateway station failures. These findings shed light on the design principles of existing mega-constellations and offer valuable insights for the future development of satellite network infrastructures.<br /> <div>
arXiv:2508.14335v1 Announce Type: cross 
Abstract: In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite constellations has introduced unprecedented opportunities for global connectivity. However, routing efficiency and inter-shell communication remain key challenges in multi-shell architectures. This paper investigates the structural properties and network dynamics of a representative six-shell mega-constellation composed of 10,956 satellites and 198 gateway stations (GSs). Leveraging tools from complex network analysis, we identify several critical findings: (1) the constellation exhibits strong small-world characteristics, enabling efficient routing despite large network diameters; (2) GS relays play a pivotal role in enhancing inter-shell connectivity by bridging otherwise disconnected components; (3) feeder links significantly reduce average path length, making long-haul communication more feasible; (4) betweenness analysis reveals load imbalances among GSs, indicating the need for traffic-aware management strategies; (5) the architecture offers excellent spatial coverage and resilience, maintaining connectivity and low routing costs even under GS failures. These insights not only explain the design rationale behind current mega-constellations like SpaceX Starlink, but also provide valuable guidance for the evolution of future satellite network infrastructures.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Messengers: Breaking Echo Chambers in Collective Opinion Dynamics with Homophily</title>
<link>https://arxiv.org/abs/2406.06440</link>
<guid>https://arxiv.org/abs/2406.06440</guid>
<content:encoded><![CDATA[
<div> consensus, collective estimation, social interactions, echo chambers, agent-based simulations
<br />
Summary:
Collective estimation in decision-making involves agents reaching consensus on a continuous quantity through social interactions. However, achieving precise consensus is challenging due to the co-evolution of opinions and the interaction network. Homophilic networks may facilitate estimation in well-connected systems, but disproportionate interactions with like-minded neighbors can lead to the emergence of echo chambers, hindering consensus. Limited exposure to differing opinions and seeking reaffirming information can trap agents in these echo chambers. To address this, agents can adopt a stubborn state (Messengers) to physically transport their opinions and connect clusters. A Dichotomous Markov Process is proposed to govern probabilistic switching between behavioral states, promoting diverse collective behaviors ranging from task specialization to generalization. Messengers help the collective escape local minima, break echo chambers, and ultimately promote consensus. 
<br /> <div>
arXiv:2406.06440v3 Announce Type: replace 
Abstract: Collective estimation is a variant of collective decision-making where agents reach consensus on a continuous quantity through social interactions. Achieving precise consensus is complex due to the co-evolution of opinions and the interaction network. While homophilic networks may facilitate estimation in well-connected systems, disproportionate interactions with like-minded neighbors lead to the emergence of echo chambers and prevent consensus. Our agent-based simulations confirm that, besides limited exposure to attitude-challenging opinions, seeking reaffirming information entrap agents in echo chambers. To overcome this, agents can adopt a stubborn state (Messengers) that carry data and connect clusters by physically transporting their opinion. We propose a generic approach based on a Dichotomous Markov Process, which governs probabilistic switching between behavioral states and generates diverse collective behaviors. We study a continuum between task specialization (no switching), to generalization (slow or rapid switching). Messengers help the collective escape local minima, break echo chambers, and promote consensus.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Profile of U.S. Neighborhoods: Datasets of Time Use at Social Infrastructure Places</title>
<link>https://arxiv.org/abs/2508.13295</link>
<guid>https://arxiv.org/abs/2508.13295</guid>
<content:encoded><![CDATA[
<div> social infrastructure, time use, spatial accessibility, foot traffic data, population representation

Summary: 
The study focuses on analyzing time use at social infrastructure places, which play a crucial role in neighborhood well-being. The researchers developed Social-Infrastructure Time Use (STU) measures using foot traffic data collected across 49 continental U.S. states from 2019 to 2024. The STU measures capture the length and depth of engagement, activity diversity, and spatial inequality at different geographic scales. The data description highlights variations in STU across time, space, and neighborhood characteristics. Validation results show consistent population representation with national survey findings and reveal nuanced patterns. Future analyses could explore the link between STU and public health outcomes and environmental factors to guide interventions for enhancing population well-being and inform social infrastructure planning and usage. <div>
arXiv:2508.13295v1 Announce Type: new 
Abstract: Social infrastructure plays a critical role in shaping neighborhood well-being by fostering social and cultural interaction, enabling service provision, and encouraging exposure to diverse environments. Despite the growing knowledge of its spatial accessibility, time use at social infrastructure places is underexplored due to the lack of a spatially resolved national dataset. We address this gap by developing scalable Social-Infrastructure Time Use measures (STU) that capture length and depth of engagement, activity diversity, and spatial inequality, supported by first-of-their-kind datasets spanning multiple geographic scales from census tracts to metropolitan areas. Our datasets leverage anonymized and aggregated foot traffic data collected between 2019 and 2024 across 49 continental U.S. states. The data description reveals variances in STU across time, space, and differing neighborhood sociodemographic characteristics. Validation demonstrates generally robust population representation, consistent with established national survey findings while revealing more nuanced patterns. Future analyses could link STU with public health outcomes and environmental factors to inform targeted interventions aimed at enhancing population well-being and guiding social infrastructure planning and usage.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State &amp; Geopolitical Censorship on Twitter (X): Detection &amp; Impact Analysis of Withheld Content</title>
<link>https://arxiv.org/abs/2508.13375</link>
<guid>https://arxiv.org/abs/2508.13375</guid>
<content:encoded><![CDATA[
<div> Keywords: state censorship, social media, user-level classifier, platform governance, digital repression

Summary: 
State censorship on social media platforms like Twitter has become increasingly common, leading to concerns about the balance between criminal content and freedom of speech. This study analyzes the impact of state censorship on Twitter accounts, focusing on Russian and Turkish accounts withheld in specific regions. The research shows that while censorship does not significantly reduce posting frequency, it does decrease likes, retweets, and follower growth, particularly when the censored region aligns with the account's primary audience. A user-level binary classifier using tweet content is developed to predict whether an account is likely to be withheld, achieving high accuracy. This analysis sheds light on the complexities of platform governance, free speech, and digital repression in the context of state censorship on social media. 

<br /><br />Summary: <div>
arXiv:2508.13375v1 Announce Type: new 
Abstract: State and geopolitical censorship on Twitter, now X, has been turning into a routine, raising concerns about the boundaries between criminal content and freedom of speech. One such censorship practice, withholding content in a particular state has renewed attention due to Elon Musk's apparent willingness to comply with state demands. In this study, we present the first quantitative analysis of the impact of state censorship by withholding on social media using a dataset in which two prominent patterns emerged: Russian accounts censored in the EU for spreading state-sponsored narratives, and Turkish accounts blocked within Turkey for promoting militant propaganda. We find that censorship has little impact on posting frequency but significantly reduces likes and retweets by 25%, and follower growth by 90%-especially when the censored region aligns with the account's primary audience. Meanwhile, some Russian accounts continue to experience growth as their audience is outside the withholding jurisdictions. We develop a user-level binary classifier with a transformer backbone and temporal aggregation strategies, aiming to predict whether an account is likely to be withheld. Through an ablation study, we find that tweet content is the primary signal in predicting censorship, while tweet metadata and profile features contribute marginally. Our best model achieves an F1 score of 0.73 and an AUC of 0.83. This work informs debates on platform governance, free speech, and digital repression.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a general diffusion-based information quality assessment model</title>
<link>https://arxiv.org/abs/2508.13927</link>
<guid>https://arxiv.org/abs/2508.13927</guid>
<content:encoded><![CDATA[
<div> diffusion dynamics, information quality, academic publications, Generalized Additive Model, citation gain <br />
<br />
Summary: 
The article introduces a framework for assessing information quality based on diffusion dynamics, focusing on academic publications. By analyzing a dataset of STEM and social science papers, the framework uses three key features: diversity, timeliness, and salience. A Generalized Additive Model trained on these features shows high accuracy in predicting high-impact papers and next-year citation gain. Timeliness and salience are identified as the most important predictors, while diversity has more variable benefits. The framework's transparency and domain-agnostic design make it a scalable tool for assessing information quality globally. It also suggests moving towards richer evaluation metrics based on diffusion dynamics, providing a more nuanced approach to assessing information credibility beyond binary labels. <br /> <div>
arXiv:2508.13927v1 Announce Type: new 
Abstract: The rapid and unregulated dissemination of information in the digital era has amplified the global "infodemic," complicating the identification of high quality information. We present a lightweight, interpretable and non-invasive framework for assessing information quality based solely on diffusion dynamics, demonstrated here in the context of academic publications. Using a heterogeneous dataset of 29,264 sciences, technology, engineering, mathematics (STEM) and social science papers from ArnetMiner and OpenAlex, we model the diffusion network of each paper as a set of three theoretically motivated features: diversity, timeliness, and salience. A Generalized Additive Model (GAM) trained on these features achieved Pearson correlations of 0.8468 for next-year citation gain and up to 97.8% accuracy in predicting high-impact papers. Feature relevance studies reveal timeliness and salience as the most robust predictors, while diversity offers less stable benefits in the academic setting but may be more informative in social media contexts. The framework's transparency, domain-agnostic design, and minimal feature requirements position it as a scalable tool for global information quality assessment, opening new avenues for moving beyond binary credibility labels toward richer, diffusion-informed evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust and Reputation in Data Sharing: A Survey</title>
<link>https://arxiv.org/abs/2508.14028</link>
<guid>https://arxiv.org/abs/2508.14028</guid>
<content:encoded><![CDATA[
<div> Keywords: data sharing, trust, reputation management systems, trustworthiness, evaluation metrics

Summary:
Trust plays a crucial role in facilitating data sharing, yet concerns about privacy and misuse hinder such initiatives. Trust and Reputation Management Systems (TRMSs) have emerged to address these challenges in various domains, but there is a lack of dedicated approaches to data sharing. This survey examines TRMSs from a data-sharing perspective, proposing novel taxonomies for system designs and trust evaluation frameworks. The study analyzes the applicability of existing TRMSs in assessing the trustworthiness of both data and entities in different environments. By identifying open challenges and suggesting future research directions, the goal is to enhance the accuracy and comprehensiveness of TRMSs in large-scale data-sharing ecosystems. This research aims to improve the explainability and reliability of TRMSs to foster trust and facilitate data sharing in the rapidly advancing artificial intelligence economy. 

<br /><br />Summary: 
- Data sharing trust issues hinder advancements in AI. 
- TRMSs address trustworthiness challenges in various domains. 
- Lack of dedicated TRMSs for data sharing necessitates novel approaches. 
- Proposed taxonomies and evaluation frameworks enhance trust assessment. 
- Future research aims to improve TRMS accuracy and comprehensiveness. <div>
arXiv:2508.14028v1 Announce Type: new 
Abstract: Data sharing is the fuel of the galloping artificial intelligence economy, providing diverse datasets for training robust models. Trust between data providers and data consumers is widely considered one of the most important factors for enabling data sharing initiatives. Concerns about data sensitivity, privacy breaches, and misuse contribute to reluctance in sharing data across various domains. In recent years, there has been a rise in technological and algorithmic solutions to measure, capture and manage trust, trustworthiness, and reputation in what we collectively refer to as Trust and Reputation Management Systems (TRMSs). Such approaches have been developed and applied to different domains of computer science, such as autonomous vehicles, or IoT networks, but there have not been dedicated approaches to data sharing and its unique characteristics. In this survey, we examine TRMSs from a data-sharing perspective, analyzing how they assess the trustworthiness of both data and entities across different environments. We develop novel taxonomies for system designs, trust evaluation framework, and evaluation metrics for both data and entity, and we systematically analyze the applicability of existing TRMSs in data sharing. Finally, we identify open challenges and propose future research directions to enhance the explainability, comprehensiveness, and accuracy of TRMSs in large-scale data-sharing ecosystems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Conversational Recommender System Considering Consumer Types</title>
<link>https://arxiv.org/abs/2508.13209</link>
<guid>https://arxiv.org/abs/2508.13209</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational Recommender Systems, Consumer Type Modeling, Personalization, Inverse Reinforcement Learning, User Categories

Summary: 
CT-CRS is a framework that integrates consumer type modeling into dialogue recommendation for Conversational Recommender Systems. It defines four user categories based on decision-making style and knowledge level, automatically inferring user types in real time. The system adjusts recommendation granularity, diversity, and attribute query complexity based on user type. Inverse Reinforcement Learning is used to approximate expert-like strategies conditioned on consumer type, improving recommendation success rate and reducing interaction turns. Experiments on LastFM, Amazon-Book, and Yelp show significant performance gains with CT-CRS. This approach offers a scalable and interpretable solution for enhancing CRS personalization through psychological modeling and advanced policy optimization.<br /><br /> <div>
arXiv:2508.13209v1 Announce Type: cross 
Abstract: Conversational Recommender Systems (CRS) provide personalized services through multi-turn interactions, yet most existing methods overlook users' heterogeneous decision-making styles and knowledge levels, which constrains both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer Type-Enhanced Conversational Recommender System), a framework that integrates consumer type modeling into dialogue recommendation. Based on consumer type theory, we define four user categories--dependent, efficient, cautious, and expert--derived from two dimensions: decision-making style (maximizers vs. satisficers) and knowledge level (high vs. low). CT-CRS employs interaction histories and fine-tunes the large language model to automatically infer user types in real time, avoiding reliance on static questionnaires. We incorporate user types into state representation and design a type-adaptive policy that dynamically adjusts recommendation granularity, diversity, and attribute query complexity. To further optimize the dialogue policy, we adopt Inverse Reinforcement Learning (IRL), enabling the agent to approximate expert-like strategies conditioned on consumer type. Experiments on LastFM, Amazon-Book, and Yelp show that CTCRS improves recommendation success rate and reduces interaction turns compared to strong baselines. Ablation studies confirm that both consumer type modeling and IRL contribute significantly to performance gains. These results demonstrate that CT-CRS offers a scalable and interpretable solution for enhancing CRS personalization through the integration of psychological modeling and advanced policy optimization.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Influence Maximization in User Recommendation</title>
<link>https://arxiv.org/abs/2508.13517</link>
<guid>https://arxiv.org/abs/2508.13517</guid>
<content:encoded><![CDATA[
<div> recommendation systems, user engagement, information propagation, Influence-Maximization, HeteroIR

Summary:
User recommendation systems play a crucial role in enhancing user engagement by encouraging interaction between users, leading to information propagation. Traditional recommendation methods focus on modeling interaction willingness, while Influence-Maximization (IM) methods aim to maximize information spread. However, existing methods face challenges in fully utilizing candidates' dissemination potential and accounting for interaction willingness. To address these issues, the HeteroIR and HeteroIM models are proposed. HeteroIR unleashes the spread capability of user recommendation systems, while HeteroIM bridges the gap between IM methods and recommendation tasks by improving interaction willingness and maximizing spread coverage. Experimental results demonstrate the superior performance of HeteroIR and HeteroIM compared to baseline methods. Deployment in Tencent's online gaming platforms resulted in significant improvements in online A/B tests. The implementation codes for HeteroIR and HeteroIM are available at https://github.com/socialalgo/HIM.<br /><br />Summary: <div>
arXiv:2508.13517v1 Announce Type: cross 
Abstract: User recommendation systems enhance user engagement by encouraging users to act as inviters to interact with other users (invitees), potentially fostering information propagation. Conventional recommendation methods typically focus on modeling interaction willingness. Influence-Maximization (IM) methods focus on identifying a set of users to maximize the information propagation. However, existing methods face two significant challenges. First, recommendation methods fail to unleash the candidates' spread capability. Second, IM methods fail to account for the willingness to interact. To solve these issues, we propose two models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to unleash the dissemination potential of user recommendation systems. HeteroIM fills the gap between the IM method and the recommendation task, improving interaction willingness and maximizing spread coverage. The HeteroIR introduces a two-stage framework to estimate the spread profits. The HeteroIM incrementally selects the most influential invitee to recommend and rerank based on the number of reverse reachable (RR) sets containing inviters and invitees. RR set denotes a set of nodes that can reach a target via propagation. Extensive experiments show that HeteroIR and HeteroIM significantly outperform the state-of-the-art baselines with the p-value < 0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online gaming platforms and gained an 8.5\% and 10\% improvement in the online A/B test, respectively. Implementation codes are available at https://github.com/socialalgo/HIM.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exit Stories: Using Reddit Self-Disclosures to Understand Disengagement from Problematic Communities</title>
<link>https://arxiv.org/abs/2508.13837</link>
<guid>https://arxiv.org/abs/2508.13837</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, exit stories, disengagement, conspiracy theories, manosphere

Summary:
This paper explores the phenomenon of individuals sharing their experiences of leaving social, ideological, and political groups on online platforms like Reddit. It focuses on 15,000 exit stories across 131 subreddits, including themes of religion, manosphere, conspiracy theories, politics, and lifestyle. By drawing on theories from social psychology, organizational behavior, and violent extremism studies, the study identifies factors contributing to disengagement. It distinguishes the process of disengaging from conspiracy theories and the manosphere from more established structures like religions or political ideologies. The research emphasizes the need for interventions that go beyond treating conspiracy theorizing as merely an information issue, suggesting the importance of mental health support in exit communities. This study provides valuable insights for designing interventions targeting disengagement from harmful ideologies. <br /><br />Summary: This study examines individuals sharing their experiences of leaving groups on Reddit, analyzing exit stories across various subreddits. It identifies factors influencing disengagement and highlights differences in disengaging from problematic groups versus established structures. The research calls for interventions beyond addressing information problems, emphasizing mental health support in exit communities. <div>
arXiv:2508.13837v1 Announce Type: cross 
Abstract: Online platforms like Reddit are increasingly becoming popular for individuals sharing personal experiences of leaving behind social, ideological, and political groups. Specifically, a series of "ex-" subreddits on Reddit allow users to recount their departures from commitments such as religious affiliations, manosphere communities, conspiracy theories or political beliefs, and lifestyle choices. Understanding the natural process through which users exit, especially from problematic groups such as conspiracy theory communities and the manosphere, can provide valuable insights for designing interventions targeting disengagement from harmful ideologies. This paper presents an in-depth exploration of 15K exit stories across 131 subreddits, focusing on five key areas: religion, manosphere, conspiracy theories, politics, and lifestyle. Using a transdisciplinary framework that incorporates theories from social psychology, organizational behavior, and violent extremism studies, this work identifies a range of factors contributing to disengagement. The results describe how disengagement from problematic groups, such as conspiracy theories and the manosphere, is a multi-faceted process that is qualitatively different than disengaging from more established social structures, such as religions or political ideologies. This research further highlights the need for moving beyond interventions that treat conspiracy theorizing solely as an information problem and contributes insights for future research focusing on offering mental health interventions and support in exit communities.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Community Formation in Response to Extreme Weather Events through Human Mobility Networks</title>
<link>https://arxiv.org/abs/2205.04981</link>
<guid>https://arxiv.org/abs/2205.04981</guid>
<content:encoded><![CDATA[
<div> Keywords: community formation, human mobility networks, natural disasters, managed power outages, socio-spatial networks

Summary:<br /><br />
The study explores community formation in human mobility networks during natural disasters, focusing on the 2021 Winter Storm Uri in Harris County, Texas. Three key characteristics were examined in the formed communities: hazard-exposure heterophily, socio-demographic homophily, and social-connectedness strength. The findings show that population movements were influenced by these factors, with communities being shaped by socio-demographic similarities, exposure to hazards, and social connections. The study highlights that communities formed during managed power outages are spatially co-located, suggesting the importance of avoiding prolonged outages in high-impact areas within these communities. The results have practical implications for power utility operators in effectively managing power outages by considering the characteristics of socio-spatial human networks. <div>
arXiv:2205.04981v2 Announce Type: replace 
Abstract: Community formation in socio-spatial human networks is one of the important mechanisms for mitigating hazard impacts of extreme weather events. Research is scarce regarding latent network characteristics shaping community formation in human mobility networks during natural disasters. Here, we examined human mobility networks in Harris County, Texas, in the context of the managed power outage forced by 2021 Winter Storm Uri to detect communities and to evaluate latent characteristics in those communities. We examined three characteristics in the communities formed within human mobility networks: hazard-exposure heterophily, socio-demographic homophily, and social-connectedness strength. The results show that population movements were shaped by socio-demographic homophily, heterophilic hazard exposure, and social connectedness strength. Our results also indicate that a community encompassing more high-impact areas would motivate population movements to areas with weaker social connectedness. Our findings reveal important characteristics shaping community formation in human mobility networks in hazard response. Specific to managed power outages, formed communities are spatially co-located, underscoring a best management practice to avoid prolonged power outages among areas within communities, thus improving hazard exposure heterophily. The findings have implications for power utility operators to account for the characteristics of socio-spatial human networks when determining the patterns of managed power outages.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Attributes and Multi-Scale Structures for Heterogeneous Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2503.13911</link>
<guid>https://arxiv.org/abs/2503.13911</guid>
<content:encoded><![CDATA[
<div> contrastive learning, heterogeneous graphs, self-supervised learning, node representation learning, positive sample selection

Summary:
ASHGCL is a novel contrastive learning framework designed for heterogeneous graphs. It incorporates three distinct views to capture attribute information, high-order structures, and low-order structures for node representation learning. The framework addresses the challenge of limited labeling data in real-world scenarios by utilizing self-supervised learning. ASHGCL introduces an attribute-enhanced positive sample selection strategy to tackle sampling bias effectively. Experimental results on four real-world datasets demonstrate that ASHGCL outperforms state-of-the-art unsupervised baselines and even surpasses some supervised benchmarks. <div>
arXiv:2503.13911v3 Announce Type: replace-cross 
Abstract: Heterogeneous graphs (HGs) are composed of multiple types of nodes and edges, making it more effective in capturing the complex relational structures inherent in the real world. However, in real-world scenarios, labeled data is often difficult to obtain, which limits the applicability of semi-supervised approaches. Self-supervised learning aims to enable models to automatically learn useful features from data, effectively addressing the challenge of limited labeling data. In this paper, we propose a novel contrastive learning framework for heterogeneous graphs (ASHGCL), which incorporates three distinct views, each focusing on node attributes, high-order and low-order structural information, respectively, to effectively capture attribute information, high-order structures, and low-order structures for node representation learning. Furthermore, we introduce an attribute-enhanced positive sample selection strategy that combines both structural information and attribute information, effectively addressing the issue of sampling bias. Extensive experiments on four real-world datasets show that ASHGCL outperforms state-of-the-art unsupervised baselines and even surpasses some supervised benchmarks.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Balancing Sparsity with Reliable Connectivity in Distributed Network Design with Random K-out Graphs</title>
<link>https://arxiv.org/abs/2508.11863</link>
<guid>https://arxiv.org/abs/2508.11863</guid>
<content:encoded><![CDATA[
<div> sparse network, random K-out graphs, reliable connectivity, finite nodes, adversarial nodes<br />
<br />
Summary:
The article focuses on designing sparse networks with reliable connectivity in distributed systems. It introduces random K-out graphs as a model to balance connectivity and sparsity, particularly in settings with limited trust. The study presents theorems to guide the selection of network parameters for ensuring reliable connectivity in scenarios with finite or unreliable nodes. It establishes upper and lower bounds for connectivity probability in random K-out graphs with finite node numbers and explores r-robustness for resilient consensus in the presence of malicious nodes. Additionally, the impact of adversarial nodes on connectivity and giant component size is analyzed, considering deletions as a modeling approach. These findings contribute to providing performance guarantees for algorithms aimed at reliable inference on networks. <br /><br /> <div>
arXiv:2508.11863v1 Announce Type: new 
Abstract: In several applications in distributed systems, an important design criterion is ensuring that the network is sparse, i.e., does not contain too many edges, while achieving reliable connectivity. Sparsity ensures communication overhead remains low, while reliable connectivity is tied to reliable communication and inference on decentralized data reservoirs and computational resources. A class of network models called random K-out graphs appear widely as a heuristic to balance connectivity and sparsity, especially in settings with limited trust, e.g., privacy-preserving aggregation of networked data in which networks are deployed. However, several questions remain regarding how to choose network parameters in response to different operational requirements, including the need to go beyond asymptotic results and the ability to model the stochastic and adversarial environments. To address this gap, we present theorems to inform the choice of network parameters that guarantee reliable connectivity in regimes where nodes can be finite or unreliable. We first derive upper and lower bounds for probability of connectivity in random K-out graphs when the number of nodes is finite. Next, we analyze the property of r-robustness, a stronger notion than connectivity that enables resilient consensus in the presence of malicious nodes. Finally, motivated by aggregation mechanisms based on pairwise masking, we model and analyze the impact of a subset of adversarial nodes, modeled as deletions, on connectivity and giant component size - metrics that are closely tied to privacy guarantees. Together, our results pave the way for end-to-end performance guarantees for a suite of algorithms for reliable inference on networks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust@Health: A Trust-Based Multilayered Network for Scalable Healthcare Service Management</title>
<link>https://arxiv.org/abs/2508.11942</link>
<guid>https://arxiv.org/abs/2508.11942</guid>
<content:encoded><![CDATA[
<div> trust relationships, healthcare systems, optimization, evolutionary graph framework, key healthcare entities

Summary:
The study explores healthcare systems' intricate relationships focusing on doctors, departments, and hospitals. Using an evolutionary graph framework, the model incorporates intra-layer and inter-layer trust relationships to optimize healthcare services. By integrating social and professional interactions, a trust-based network identifies key healthcare entities. The proposed trust algorithm effectively quantifies entity importance, showing a strong correlation (0.91) with hospital and department ratings. However, doctor ratings may be biased, leading to skewed distributions. This framework enables scalable healthcare infrastructure, supporting patient referrals, personalized recommendations, and improved decision-making processes. <div>
arXiv:2508.11942v1 Announce Type: new 
Abstract: We study the intricate relationships within healthcare systems, focusing on interactions among doctors, departments, and hospitals. Leveraging an evolutionary graph framework, the proposed model emphasizes both intra-layer and inter-layer trust relationships to better understand and optimize healthcare services. The trust-based network facilitates the identification of key healthcare entities by integrating their social and professional interactions, culminating in a trust-based algorithm that quantifies the importance of these entities. Validation with a real-world dataset reveals a strong correlation (0.91) between the proposed trust measures and the ratings of hospitals and departments, though doctor ratings demonstrate skewed distributions due to potential biases. By modeling these relationships and trust dynamics, the framework supports scalable healthcare infrastructure, enabling effective patient referrals, personalized recommendations, and enhanced decision-making pathways.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Network-aware Direct Search Method for Influence Maximization</title>
<link>https://arxiv.org/abs/2508.12164</link>
<guid>https://arxiv.org/abs/2508.12164</guid>
<content:encoded><![CDATA[
<div> direct search, influence maximization, social network analysis, network structure, computational efficiency 

Summary:
Influence Maximization (IM) is crucial in social network analysis, aiming to identify influential nodes to maximize influence spread. Direct search methods, suitable for such problems, evaluate the objective function without gradient information. Network-aware Direct Search (NaDS) integrates network structure into its formulation to address IM problems efficiently. Utilizing a mixed-integer programming IM model, NaDS outperformed existing state-of-the-art approaches in large-scale network tests. By incorporating graph structure, NaDS demonstrated improved computational efficiency, confirming the benefits of leveraging network information in algorithmic frameworks for the IM problem.<br /><br />Summary: <div>
arXiv:2508.12164v1 Announce Type: new 
Abstract: Influence Maximization (IM) is a pivotal concept in social network analysis, involving the identification of influential nodes within a network to maximize the number of influenced nodes, and has a wide variety of applications that range from viral marketing and information dissemination to public health campaigns. IM can be modeled as a combinatorial optimization problem with a black-box objective function, where the goal is to select $B$ seed nodes that maximize the expected influence spread. Direct search methods, which do not require gradient information, are well-suited for such problems. Unlike gradient-based approaches, direct search algorithms, in fact, only evaluate the objective function at a suitably chosen set of trial points around the current solution to guide the search process. However, these methods often suffer from scalability issues due to the high cost of function evaluations, especially when applied to combinatorial problems like IM. This work, therefore, proposes the Network-aware Direct Search (NaDS) method, an innovative direct search approach that integrates the network structure into its neighborhood formulation and is used to tackle a mixed-integer programming formulation of the IM problem, the so-called General Information Propagation model. We tested our method on large-scale networks, comparing it to existing state-of-the-art approaches for the IM problem, including direct search methods and various greedy techniques and heuristics. The results of the experiments empirically confirm the assumptions underlying NaDS, demonstrating that exploiting the graph structure of the IM problem in the algorithmic framework can significantly improve its computational efficiency in the considered context.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAD: A Benchmark for Multi-Turn Audio Dialogue Fact-Checking</title>
<link>https://arxiv.org/abs/2508.12186</link>
<guid>https://arxiv.org/abs/2508.12186</guid>
<content:encoded><![CDATA[
<div> fact-checking, audio platforms, misinformation, multi-turn dialogues, dataset

Summary:
The article introduces the MAD (Multi-turn Audio Dialogues) dataset, focused on fact-checking spoken content in multi-turn dialogues. It addresses the lack of comprehensive datasets capturing the complexity of misinformation in speech, including speaker interactions, emotional tone, and overlapping speech. The dataset includes annotations for speaker turns, dialogue scenarios, check-worthiness, and veracity at both sentence and dialogue levels. Two core tasks supported are check-worthy claim detection and claim verification. Benchmarking shows the difficulty of these tasks, with pretrained models achieving only around 72-74% accuracy for sentence-level verification and 71-72% for dialogue-level verification. MAD serves as a benchmark for advancing multimodal and conversational fact-checking and highlights challenges in reasoning over speech and dialogue dynamics. <div>
arXiv:2508.12186v1 Announce Type: new 
Abstract: Despite the growing popularity of audio platforms, fact-checking spoken content remains significantly underdeveloped. Misinformation in speech often unfolds across multi-turn dialogues, shaped by speaker interactions, disfluencies, overlapping speech, and emotional tone-factors that complicate both claim detection and verification. Existing datasets fall short by focusing on isolated sentences or text transcripts, without modeling the conversational and acoustic complexity of spoken misinformation. We introduce MAD (Multi-turn Audio Dialogues), the first fact-checking dataset aligned with multi-turn spoken dialogues and corresponding audio. MAD captures how misinformation is introduced, contested, and reinforced through natural conversation. Each dialogue includes annotations for speaker turns, dialogue scenarios, information spread styles, sentence-level check-worthiness, and both sentence- and dialogue-level veracity. The dataset supports two core tasks: check-worthy claim detection and claim verification. Benchmarking shows that even strong pretrained models reach only 72-74% accuracy at the sentence level and 71-72% at the dialogue level in claim verification, underscoring MAD's difficulty. MAD offers a high-quality benchmark for advancing multimodal and conversational fact-checking, while also surfacing open challenges related to reasoning over speech and dialogue dynamics.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Physicians: Social and Familial Norms Driving Cesarean Section Decisions in Bangladesh</title>
<link>https://arxiv.org/abs/2508.12240</link>
<guid>https://arxiv.org/abs/2508.12240</guid>
<content:encoded><![CDATA[
<div> Keywords: women's health, cesarean section, Bangladesh, Health Belief Model, Theory of Planned Behavior 

Summary: 
The study examines the high rate of cesarean sections (CS) in Bangladesh, which exceeds the WHO's recommendation. It investigates the socio-cultural factors influencing women's decisions regarding childbirth mode. Findings show that 91% of CS cases occurred against initial preferences, indicating a gap between health beliefs and behavior. The subjective norms, specifically family influence and social expectations, play a significant role in shaping CS decisions, outweighing physician recommendations. This highlights the need to consider socio-cultural factors in addressing the rising CS rates in Bangladesh, to ensure women's health and well-being are prioritized. <br /><br /> <div>
arXiv:2508.12240v1 Announce Type: new 
Abstract: Women's health in Bangladesh faces risks due to an alarming rise in cesarean section (CS) rates, exceeding 72% in hospital-based deliveries, far surpassing the WHO's recommended limit of 15%. This study, guided by the Health Belief Model (HBM) and the Theory of Planned Behavior (TPB), explored socio-cultural factors influencing childbirth mode decisions. Among 503 survey participants, 91% of CS cases occurred against initial preferences, revealing a disconnect between health beliefs and behavior. Subjective norms, particularly family influence and social expectations, emerged as more critical in shaping CS decisions than physician recommendations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network</title>
<link>https://arxiv.org/abs/2508.12574</link>
<guid>https://arxiv.org/abs/2508.12574</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, rumor detection, bidirectional Mamba2 Network, dot-product attention, Conditional Random Fields

Summary:
The paper introduces the Insight Rumors model, aiming to locate and mark specific rumor content in textual data. The model utilizes the Bidirectional Mamba2 Network with Dot-Product Attention (Att_BiMamba2) to enhance the representation of high-dimensional rumor features. A Rumor Locating and Marking module is designed to accurately locate and mark rumors using a skip-connection network and Conditional Random Fields (CRF). A labeled dataset is created for rumor locating and marking, and comprehensive experiments show that the proposed model outperforms existing schemes by accurately detecting, locating, and marking rumors in context. This innovative approach addresses the limitation of current models that focus solely on classifying contexts as rumors or not, providing a more precise and effective method for rumor detection in social media networks. 

<br /><br />Summary: <div>
arXiv:2508.12574v1 Announce Type: new 
Abstract: With the development of social media networks, rumor detection models have attracted more and more attention. Whereas, these models primarily focus on classifying contexts as rumors or not, lacking the capability to locate and mark specific rumor content. To address this limitation, this paper proposes a novel rumor detection model named Insight Rumors to locate and mark rumor content within textual data. Specifically, we propose the Bidirectional Mamba2 Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a bidirectional Mamba2 model and applies dot-product attention to weight and combine the outputs from both directions, thereby enhancing the representation of high-dimensional rumor features. Simultaneously, a Rumor Locating and Marking module is designed to locate and mark rumors. The module constructs a skip-connection network to project high-dimensional rumor features onto low-dimensional label features. Moreover, Conditional Random Fields (CRF) is employed to impose strong constraints on the output label features, ensuring accurate rumor content location. Additionally, a labeled dataset for rumor locating and marking is constructed, with the effectiveness of the proposed model is evaluated through comprehensive experiments. Extensive experiments indicate that the proposed scheme not only detects rumors accurately but also locates and marks them in context precisely, outperforming state-of-the-art schemes that can only discriminate rumors roughly.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Prediction in Collaboration Networks: An Empirical Study on arXiv</title>
<link>https://arxiv.org/abs/2508.13029</link>
<guid>https://arxiv.org/abs/2508.13029</guid>
<content:encoded><![CDATA[
<div> influence prediction, Social Sphere Model, link prediction, centrality-based selection, General Relativity, Quantum Cosmology<br />
<br />
Summary:<br />
This empirical study evaluates the Social Sphere Model for influence prediction on the arXiv General Relativity and Quantum Cosmology collaboration network. The model combines link prediction with top-k centrality-based selection and is tested under varying edge sampling rates and prediction horizons. Results show the model effectively identifies latent influencers in evolving networks, performing best with denser initial graphs. The newly introduced RA-2 metric consistently produces the lowest prediction errors among the similarity measures tested. The study demonstrates the practical applicability of the Social Sphere Model in predicting real-world influence in evolving networks. <div>
arXiv:2508.13029v1 Announce Type: new 
Abstract: This paper provides an empirical study of the Social Sphere Model for influence prediction, previously introduced by the authors, combining link prediction with top-k centrality-based selection. We apply the model to the temporal arXiv General Relativity and Quantum Cosmology collaboration network, evaluating its performance under varying edge sampling rates and prediction horizons to reflect different levels of initial data completeness and network evolution. Accuracy is assessed using mean squared error in both link prediction and influence maximization tasks. The results show that the model effectively identifies latent influencers, i.e., nodes that are not initially central but later influential, and performs best with denser initial graphs. Among the similarity measures tested, the newly introduced RA-2 metric consistently yields the lowest prediction errors. These findings support the practical applicability of the model to predict real-world influence in evolving networks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfolded Laplacian Spectral Embedding: A Theoretically Grounded Approach to Dynamic Network Representation</title>
<link>https://arxiv.org/abs/2508.12674</link>
<guid>https://arxiv.org/abs/2508.12674</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic relational structures, node embeddings, stability properties, Laplacian matrix, spectral graph theory

Summary: 
The paper introduces the Unfolded Laplacian Spectral Embedding method for representing dynamic relational structures in AI tasks. This method extends the Unfolded Adjacency Spectral Embedding framework to normalized Laplacians, ensuring both cross-sectional and longitudinal stability. Formal proofs are provided to confirm the stability conditions of the proposed method. Additionally, leveraging the Laplacian matrix leads to a new Cheeger-style inequality linking the embeddings to the conductance of dynamic graphs. Empirical evaluations on synthetic and real-world datasets validate the theoretical findings and showcase the effectiveness of the approach. Overall, the Unfolded Laplacian Spectral Embedding method establishes a stable and principled framework for dynamic network representation rooted in spectral graph theory.<br /><br />Summary: <div>
arXiv:2508.12674v1 Announce Type: cross 
Abstract: Dynamic relational structures play a central role in many AI tasks, but their evolving nature presents challenges for consistent and interpretable representation. A common approach is to learn time-varying node embeddings, whose effectiveness depends on satisfying key stability properties. In this paper, we propose Unfolded Laplacian Spectral Embedding, a new method that extends the Unfolded Adjacency Spectral Embedding framework to normalized Laplacians while preserving both cross-sectional and longitudinal stability. We provide formal proof that our method satisfies these stability conditions. In addition, as a bonus of using the Laplacian matrix, we establish a new Cheeger-style inequality that connects the embeddings to the conductance of the underlying dynamic graphs. Empirical evaluations on synthetic and real-world datasets support our theoretical findings and demonstrate the strong performance of our method. These results establish a principled and stable framework for dynamic network representation grounded in spectral graph theory.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Algorithms Mirror Minds: A Confirmation-Aware Social Dynamic Model of Echo Chamber and Homogenization Traps</title>
<link>https://arxiv.org/abs/2508.11516</link>
<guid>https://arxiv.org/abs/2508.11516</guid>
<content:encoded><![CDATA[
<div> Keywords: Recommender systems, echo chambers, user homogenization, algorithmic recommendations, psychological mechanisms <br />
Summary: 
The study focuses on the impact of echo chambers and user homogenization in recommender systems, identifying them as results of the interaction between algorithmic recommendations and user behavior. The Confirmation-Aware Social Dynamic Model is proposed to incorporate user psychology and social relationships to simulate user-recommender interactions. The theoretical analysis demonstrates that echo chambers and homogenization traps are inevitable outcomes. Empirical simulations on real-world datasets and a synthetic dataset support this claim, highlighting factors influencing these phenomena at system, user, and platform levels. Moreover, practical mitigation strategies are proposed to address echo chambers and user homogenization. These findings offer insights into the emergence and drivers of these issues in recommender systems, providing guidance for designing more human-centered recommendation systems.<br /><br />Summary: <div>
arXiv:2508.11516v1 Announce Type: new 
Abstract: Recommender systems increasingly suffer from echo chambers and user homogenization, systemic distortions arising from the dynamic interplay between algorithmic recommendations and human behavior. While prior work has studied these phenomena through the lens of algorithmic bias or social network structure, we argue that the psychological mechanisms of users and the closed-loop interaction between users and recommenders are critical yet understudied drivers of these emergent effects. To bridge this gap, we propose the Confirmation-Aware Social Dynamic Model which incorporates user psychology and social relationships to simulate the actual user and recommender interaction process. Our theoretical analysis proves that echo chambers and homogenization traps, defined respectively as reduced recommendation diversity and homogenized user representations, will inevitably occur. We also conduct extensive empirical simulations on two real-world datasets and one synthetic dataset with five well-designed metrics, exploring the root factors influencing the aforementioned phenomena from three level perspectives: the stochasticity and social integration degree of recommender (system-level), the psychological mechanisms of users (user-level), and the dataset scale (platform-level). Furthermore, we demonstrate four practical mitigation strategies that help alleviate echo chambers and user homogenization at the cost of some recommendation accuracy. Our findings provide both theoretical and empirical insights into the emergence and drivers of echo chambers and user homogenization, as well as actionable guidelines for human-centered recommender design.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection</title>
<link>https://arxiv.org/abs/2508.11197</link>
<guid>https://arxiv.org/abs/2508.11197</guid>
<content:encoded><![CDATA[
<div> framework, misinformation, social media, multimodal, detection<br />
<br />
Summary: 
The article proposes E-CaTCH, a framework for detecting multimodal misinformation on social media. E-CaTCH clusters posts into pseudo-events based on similarity and proximity, processes each event independently, and extracts textual and visual features using pre-trained encoders. It models temporal evolution by segmenting events into time windows and uses a trend-aware LSTM to encode narrative progression over time. The model integrates adaptive class weighting, temporal consistency regularization, and hard-example mining to address class imbalance and promote stable learning. Extensive experiments on multiple datasets show that E-CaTCH outperforms existing methods and demonstrates robustness and generalizability in detecting misinformation across different scenarios. <div>
arXiv:2508.11197v1 Announce Type: cross 
Abstract: Detecting multimodal misinformation on social media remains challenging due to inconsistencies between modalities, changes in temporal patterns, and substantial class imbalance. Many existing methods treat posts independently and fail to capture the event-level structure that connects them across time and modality. We propose E-CaTCH, an interpretable and scalable framework for robustly detecting misinformation. If needed, E-CaTCH clusters posts into pseudo-events based on textual similarity and temporal proximity, then processes each event independently. Within each event, textual and visual features are extracted using pre-trained BERT and ResNet encoders, refined via intra-modal self-attention, and aligned through bidirectional cross-modal attention. A soft gating mechanism fuses these representations to form contextualized, content-aware embeddings of each post. To model temporal evolution, E-CaTCH segments events into overlapping time windows and uses a trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode narrative progression over time. Classification is performed at the event level, enabling better alignment with real-world misinformation dynamics. To address class imbalance and promote stable learning, the model integrates adaptive class weighting, temporal consistency regularization, and hard-example mining. The total loss is aggregated across all events. Extensive experiments on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH consistently outperforms state-of-the-art baselines. Cross-dataset evaluations further demonstrate its robustness, generalizability, and practical applicability across diverse misinformation scenarios.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Invariant Neighborhood Patterns for Heterophilic Graphs</title>
<link>https://arxiv.org/abs/2403.10572</link>
<guid>https://arxiv.org/abs/2403.10572</guid>
<content:encoded><![CDATA[
<div> adaptie neighborhood propagation, invariant neighborhood pattern learning, non-homophilous graphs, distribution shifts, graph neural network<br />
<br />
Summary:<br />
This paper addresses distribution shifts on non-homophilous graphs, where the homophilous assumption doesn't always apply. Existing graph neural network methods may struggle with the diverse distribution shifts of neighborhood patterns on such graphs. The proposed Invariant Neighborhood Pattern Learning (INPL) approach introduces an Adaptive Neighborhood Propagation (ANP) module to capture adaptive neighborhood information and alleviate neighborhood pattern distribution shifts. Additionally, the Invariant Non-Homophilous Graph Learning (INHGL) module helps constrain the ANP and learn invariant graph representation on non-homophilous graphs. Experimental results on real-world non-homophilous graphs demonstrate that the INPL method achieves state-of-the-art performance for learning on large non-homophilous graphs. <br /> <div>
arXiv:2403.10572v2 Announce Type: replace-cross 
Abstract: This paper studies the problem of distribution shifts on non-homophilous graphs Mosting existing graph neural network methods rely on the homophilous assumption that nodes from the same class are more likely to be linked. However, such assumptions of homophily do not always hold in real-world graphs, which leads to more complex distribution shifts unaccounted for in previous methods. The distribution shifts of neighborhood patterns are much more diverse on non-homophilous graphs. We propose a novel Invariant Neighborhood Pattern Learning (INPL) to alleviate the distribution shifts problem on non-homophilous graphs. Specifically, we propose the Adaptive Neighborhood Propagation (ANP) module to capture the adaptive neighborhood information, which could alleviate the neighborhood pattern distribution shifts problem on non-homophilous graphs. We propose Invariant Non-Homophilous Graph Learning (INHGL) module to constrain the ANP and learn invariant graph representation on non-homophilous graphs. Extensive experimental results on real-world non-homophilous graphs show that INPL could achieve state-of-the-art performance for learning on large non-homophilous graphs.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spectral Framework for Evaluating Geodesic Distances Between Graphs</title>
<link>https://arxiv.org/abs/2406.10500</link>
<guid>https://arxiv.org/abs/2406.10500</guid>
<content:encoded><![CDATA[
<div> spectral framework, graph geodesic distance, spectral graph matching, resistance-based spectral graph coarsening, GGD metric <br />
<br />
Summary: 
This paper introduces a spectral framework for measuring the dissimilarity between graph data samples, using a novel metric called Graph Geodesic Distance (GGD). The framework includes spectral graph matching to find node correspondence and compute geodesic distance between graphs, as well as a coarsening scheme for graphs of different sizes. The GGD metric captures differences in spectral properties like resistances, cuts, and mixing time of random walks, and outperforms state-of-the-art metrics like Tree-Mover's Distance (TMD) for graph classification, especially with partial node features. GGD is also applicable for stability analysis of Graph Neural Networks (GNNs) and measuring distances between datasets, demonstrating its versatility in various machine learning applications. <div>
arXiv:2406.10500v3 Announce Type: replace-cross 
Abstract: This paper presents a spectral framework for quantifying the differentiation between graph data samples by introducing a novel metric named Graph Geodesic Distance (GGD). For two different graphs with the same number of nodes, our framework leverages a spectral graph matching procedure to find node correspondence so that the geodesic distance between them can be subsequently computed by solving a generalized eigenvalue problem associated with their Laplacian matrices. For graphs of different sizes, a resistance-based spectral graph coarsening scheme is introduced to reduce the size of the larger graph while preserving the original spectral properties. We show that the proposed GGD metric can effectively quantify dissimilarities between two graphs by encapsulating their differences in key structural (spectral) properties, such as effective resistances between nodes, cuts, and the mixing time of random walks. Through extensive experiments comparing with state-of-the-art metrics, such as the latest Tree-Mover's Distance (TMD), the proposed GGD metric demonstrates significantly improved performance for graph classification, particularly when only partial node features are available. Furthermore, we extend the application of GGD beyond graph classification to stability analysis of GNNs and the quantification of distances between datasets, highlighting its versatility in broader machine learning contexts.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Content and Social Connections of Fake News with Explainable Text and Graph Learning</title>
<link>https://arxiv.org/abs/2508.10040</link>
<guid>https://arxiv.org/abs/2508.10040</guid>
<content:encoded><![CDATA[
<div>  Keywords: misinformation, fact-checking, social media, graph-based features, explainable framework

Summary: 
This paper discusses the challenges of misinformation and the importance of incorporating social media dynamics in fact-checking systems. The proposed framework combines content analysis, social media features, and graph-based features to improve fact-checking accuracy. By integrating a misinformation classifier with explainability techniques, the framework provides interpretable insights to support classification decisions. Experiments on English, Spanish, and Portuguese datasets show that multimodal information enhances performance compared to using single modalities. The framework's explanations are evaluated for interpretability, trustworthiness, and robustness, demonstrating that it generates human-understandable justifications for its predictions. Overall, the framework offers a comprehensive approach to fact-checking that considers the complexities of social media dynamics and provides transparent explanations for its decisions. 

<br /><br />Summary: <div>
arXiv:2508.10040v1 Announce Type: new 
Abstract: The global spread of misinformation and concerns about content trustworthiness have driven the development of automated fact-checking systems. Since false information often exploits social media dynamics such as "likes" and user networks to amplify its reach, effective solutions must go beyond content analysis to incorporate these factors. Moreover, simply labelling content as false can be ineffective or even reinforce biases such as automation and confirmation bias. This paper proposes an explainable framework that combines content, social media, and graph-based features to enhance fact-checking. It integrates a misinformation classifier with explainability techniques to deliver complete and interpretable insights supporting classification decisions. Experiments demonstrate that multimodal information improves performance over single modalities, with evaluations conducted on datasets in English, Spanish, and Portuguese. Additionally, the framework's explanations were assessed for interpretability, trustworthiness, and robustness with a novel protocol, showing that it effectively generates human-understandable justifications for its predictions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SABIA: An AI-Powered Tool for Detecting Opioid-Related Behaviors on Social Media</title>
<link>https://arxiv.org/abs/2508.10046</link>
<guid>https://arxiv.org/abs/2508.10046</guid>
<content:encoded><![CDATA[
<div> BERT, social media, opioid misuse, deep learning model, public health<br />
<br />
Summary: 
The study focuses on detecting opioid-related user behavior on social media. It addresses challenges like informal language and coded communication that can obscure opioid misuse detection. The researchers developed a hybrid deep learning model called SABIA, combining BERT and other techniques to classify user behavior into five categories: Dealers, Active Opioid Users, Recovered Users, Prescription Users, and Non-Users. They created a new dataset from Reddit posts and achieved benchmark performance with SABIA, outperforming Logistic Regression by 9.30%. The study showcases the effectiveness of hybrid deep learning models in detecting complex opioid-related behaviors on social media, offering insights for public health monitoring and intervention efforts.<br /><br /> <div>
arXiv:2508.10046v1 Announce Type: new 
Abstract: Social media platforms have become valuable tools for understanding public health challenges by offering insights into patient behaviors, medication use, and mental health issues. However, analyzing such data remains difficult due to the prevalence of informal language, slang, and coded communication, which can obscure the detection of opioid misuse. This study addresses the issue of opioid-related user behavior on social media, including informal expressions, slang terms, and misspelled or coded language. We analyzed the existing Bidirectional Encoder Representations from Transformers (BERT) technique and developed a BERT-BiLSTM-3CNN hybrid deep learning model, named SABIA, to create a single-task classifier that effectively captures the features of the target dataset. The SABIA model demonstrated strong capabilities in capturing semantics and contextual information. The proposed approach includes: (1) data preprocessing, (2) data representation using the SABIA model, (3) a fine-tuning phase, and (4) classification of user behavior into five categories. A new dataset was constructed from Reddit posts, identifying opioid user behaviors across five classes: Dealers, Active Opioid Users, Recovered Users, Prescription Users, and Non-Users, supported by detailed annotation guidelines. Experiments were conducted using supervised learning. Results show that SABIA achieved benchmark performance, outperforming the baseline (Logistic Regression, LR = 0.86) and improving accuracy by 9.30%. Comparisons with seven previous studies confirmed its effectiveness and robustness. This study demonstrates the potential of hybrid deep learning models for detecting complex opioid-related behaviors on social media, supporting public health monitoring and intervention efforts.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Maximization in Multi-layer Social Networks Based on Differentiated Graph Embeddings</title>
<link>https://arxiv.org/abs/2508.10289</link>
<guid>https://arxiv.org/abs/2508.10289</guid>
<content:encoded><![CDATA[
<div> Keywords: influential nodes, social network analysis, graph neural networks, multi-layer networks, influence maximization

Summary:
Inf-MDE is a novel method for identifying influential nodes in social networks. It addresses shortcomings in current techniques by leveraging differentiated graph embeddings and a multi-layer network structure. The model captures self-influence propagation to eliminate representation bias and incorporates an adaptive local influence aggregation mechanism in its graph neural network design. This mechanism adjusts influence feature aggregation based on local context and intensity, allowing for effective capturing of inter-layer propagation heterogeneity and intra-layer diffusion dynamics. Extensive experiments on various multi-layer social network datasets show that Inf-MDE outperforms state-of-the-art methods in influence maximization. <div>
arXiv:2508.10289v1 Announce Type: new 
Abstract: Identifying influential nodes is crucial in social network analysis. Existing methods often neglect local opinion leader tendencies, resulting in overlapping influence ranges for seed nodes. Furthermore, approaches based on vanilla graph neural networks (GNNs) struggle to effectively aggregate influence characteristics during message passing, particularly with varying influence intensities. Current techniques also fail to adequately address the multi-layer nature of social networks and node heterogeneity. To address these issues, this paper proposes Inf-MDE, a novel multi-layer influence maximization method leveraging differentiated graph embedding. Inf-MDE models social relationships using a multi-layer network structure. The model extracts a self-influence propagation subgraph to eliminate the representation bias between node embeddings and propagation dynamics. Additionally, Inf-MDE incorporates an adaptive local influence aggregation mechanism within its GNN design. This mechanism dynamically adjusts influence feature aggregation during message passing based on local context and influence intensity, enabling it to effectively capture both inter-layer propagation heterogeneity and intra-layer diffusion dynamics. Extensive experiments across four distinct multi-layer social network datasets demonstrate that Inf-MDE significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Homogeneity Can Emerge Without Filtering Algorithms or Homophily Preferences</title>
<link>https://arxiv.org/abs/2508.10466</link>
<guid>https://arxiv.org/abs/2508.10466</guid>
<content:encoded><![CDATA[
<div> Keywords: online environments, echo chambers, filter bubbles, agent-based model, polarization

Summary: 
This study challenges the conventional belief that online echo chambers and filter bubbles are solely created by algorithmic curation or user preferences. Through an agent-based model inspired by Schelling's model of segregation, it demonstrates that homogeneity can emerge without these factors. Weak individual preferences and group-based interactions can lead to feedback loops that drive communities towards segregation. Once a slight imbalance forms, cascades of user exits and regrouping can intensify homogeneity. Surprisingly, algorithmic filtering, often blamed for filter bubbles, can actually help maintain diversity by stabilizing mixed communities. The research sheds light on online polarization as an emergent system-level dynamic, emphasizing the importance of applying a complexity lens to the study of digital public spheres. <div>
arXiv:2508.10466v1 Announce Type: new 
Abstract: Ideologically homogeneous online environments - often described as "echo chambers" or "filter bubbles" - are widely seen as drivers of polarization, radicalization, and misinformation. A central debate asks whether such homophily stems primarily from algorithmic curation or users' preference for like-minded peers. This study challenges that view by showing that homogeneity can emerge in the absence of both filtering algorithms and user preferences. Using an agent-based model inspired by Schelling's model of residential segregation, we demonstrate that weak individual preferences, combined with simple group-based interaction structures, can trigger feedback loops that drive communities toward segregation. Once a small imbalance forms, cascades of user exits and regrouping amplify homogeneity across the system. Counterintuitively, algorithmic filtering - often blamed for "filter bubbles" - can in fact sustain diversity by stabilizing mixed communities. These findings highlight online polarization as an emergent system-level dynamic and underscore the importance of applying a complexity lens to the study of digital public spheres.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Integration of Multi-View Attributed Graphs for Clustering and Embedding</title>
<link>https://arxiv.org/abs/2508.09452</link>
<guid>https://arxiv.org/abs/2508.09452</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view attributed graph, Laplacian aggregation, clustering, embedding, efficiency

Summary: 
The paper introduces a new approach called spectrum-guided Laplacian aggregation for clustering and embedding tasks in multi-view attributed graphs (MVAGs). By integrating all views of the MVAG into a Laplacian matrix, classic graph algorithms can be effectively applied for superior performance. The method combines eigengap and connectivity objectives in an integrated objective formulation to ensure spectral properties align with community and connectivity properties. Two algorithms, SGLA and SGLA+, are introduced to address the computational complexity of the optimization problem. SGLA+ further enhances efficiency using sampling and approximation techniques. Experimental results on 8 MVAG datasets demonstrate the superior performance and efficiency of SGLA and SGLA+ compared to existing methods, with significant speed improvements. This novel approach holds promise for various applications like recommendation systems, anomaly detection, and social network analysis. 

<br /><br />Summary: <div>
arXiv:2508.09452v1 Announce Type: new 
Abstract: A multi-view attributed graph (MVAG) G captures the diverse relationships and properties of real-world entities through multiple graph views and attribute views. Effectively utilizing all views in G is essential for MVAG clustering and embedding, which are important for applications like recommendation systems, anomaly detection, social network analysis, etc. Existing methods either achieve inferior result quality or incur significant computational costs to handle large-scale MVAGs.
  In this paper, we present a spectrum-guided Laplacian aggregation scheme with an effective objective formulation and two efficient algorithms SGLA and SGLA+, to cohesively integrate all views of G into an MVAG Laplacian matrix, which readily enables classic graph algorithms to handle G with superior performance in clustering and embedding tasks. We begin by conducting a theoretical analysis to design an integrated objective that consists of two components, the eigengap and connectivity objectives, aiming to link the spectral properties of the aggregated MVAG Laplacian with the underlying community and connectivity properties of G. A constrained optimization problem is then formulated for the integration, which is computationally expensive to solve. Thus, we first develop the SGLA algorithm, which already achieves excellent performance compared with existing methods. To further enhance efficiency, we design SGLA+ to reduce the number of costly objective evaluations via sampling and approximation to quickly find an approximate optimum. Extensive experiments compare our methods against 12 baselines for clustering and 8 baselines for embedding on 8 multi-view attributed graphs, validating the superior performance of SGLA and SGLA+ in terms of result quality and efficiency. Compared with the most effective baselines, our methods are significantly faster, often by up to orders of magnitude.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CS-Agent: LLM-based Community Search via Dual-agent Collaboration</title>
<link>https://arxiv.org/abs/2508.09549</link>
<guid>https://arxiv.org/abs/2508.09549</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, Graph Structure Analysis, Community Search, CS-Agent, Benchmark

Summary: 
Large Language Models (LLMs) have shown promise in natural language processing but have not been extensively applied to graph structure analysis, specifically in community search tasks. The proposed GraphCS benchmark evaluates LLMs in community search, revealing limitations such as output bias and lack of meaningful results. To mitigate these issues, the CS-Agent framework is introduced, utilizing two LLMs as Solver and Validator in a collaborative manner. CS-Agent refines initial results iteratively without additional training, with the Decider module selecting the optimal community. Experimental results demonstrate the effectiveness of CS-Agent in improving the quality and stability of identified communities compared to baseline methods. This work represents a novel application of LLMs to community search, offering a robust and adaptive solution for real-world graph analysis tasks.

<br /><br />Summary: <div>
arXiv:2508.09549v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, yet their application to graph structure analysis, particularly in community search, remains underexplored. Community search, a fundamental task in graph analysis, aims to identify groups of nodes with dense interconnections, which is crucial for understanding the macroscopic structure of graphs. In this paper, we propose GraphCS, a comprehensive benchmark designed to evaluate the performance of LLMs in community search tasks. Our experiments reveal that while LLMs exhibit preliminary potential, they frequently fail to return meaningful results and suffer from output bias. To address these limitations, we introduce CS-Agent, a dual-agent collaborative framework to enhance LLM-based community search. CS-Agent leverages the complementary strengths of two LLMs acting as Solver and Validator. Through iterative feedback and refinement, CS-Agent dynamically refines initial results without fine-tuning or additional training. After the multi-round dialogue, Decider module selects the optimal community. Extensive experiments demonstrate that CS-Agent significantly improves the quality and stability of identified communities compared to baseline methods. To our knowledge, this is the first work to apply LLMs to community search, bridging the gap between LLMs and graph analysis while providing a robust and adaptive solution for real-world applications.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication</title>
<link>https://arxiv.org/abs/2508.09665</link>
<guid>https://arxiv.org/abs/2508.09665</guid>
<content:encoded><![CDATA[
<div> cloning, identity detection, social-sensor cloud, deep forest model, cryptography<br />
<br />
Summary:
This research addresses the issue of identity cloning in social-sensor cloud service providers. The proposed method comprises two main components: a similar identity detection approach and a cryptography-based authentication protocol. A weakly supervised deep forest model is used to identify similar identities based on non-privacy-sensitive user profile features. Additionally, a cryptography-based authentication protocol is developed to verify whether similar identities are from the same provider. Experiments conducted on a large real-world dataset demonstrate the effectiveness and superior performance of the proposed technique compared to existing methods for detecting identity clones. <div>
arXiv:2508.09665v1 Announce Type: cross 
Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity cloning incidents. However, existing approaches suffer from unsatisfactory performance, a lack of solutions for detecting duplicated accounts, and a lack of large-scale evaluations on real-world datasets. We introduce a novel method for detecting identity cloning in social-sensor cloud service providers. Our proposed technique consists of two primary components: 1) a similar identity detection method and 2) a cryptography-based authentication protocol. Initially, we developed a weakly supervised deep forest model to identify similar identities using non-privacy-sensitive user profile features provided by the service. Subsequently, we designed a cryptography-based authentication protocol to verify whether similar identities were generated by the same provider. Our extensive experiments on a large real-world dataset demonstrate the feasibility and superior performance of our technique compared to current state-of-the-art identity clone detection methods.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Human Values in Online Communities</title>
<link>https://arxiv.org/abs/2402.14177</link>
<guid>https://arxiv.org/abs/2402.14177</guid>
<content:encoded><![CDATA[
<div> Reddit, human values, computational analysis, online communities, Schwartz values <br />
<br />
Summary: 
The study focuses on analyzing human values on Reddit to understand society's preferences and behaviors. The researchers propose a method to computationally analyze values on the platform, allowing for large-scale analysis complementing survey-based approaches. They train classifiers to annotate over six million posts across 12k subreddits with Schwartz values. The analysis reveals insights into the values prevalent in various online communities, such as a negative stance towards conformity in certain subreddits. The study also highlights the correlation between traditional values and conservative U.S. states in geographically specific subreddits. The dataset and method developed can serve as a valuable tool for qualitative studies of online communication. <br /><br /> <div>
arXiv:2402.14177v4 Announce Type: replace 
Abstract: Studying human values is instrumental for cross-cultural research, enabling a better understanding of preferences and behaviour of society at large and communities therein. To study the dynamics of communities online, we propose a method to computationally analyse values present on Reddit. Our method allows analysis at scale, complementing survey based approaches. We train a value relevance and a value polarity classifier, which we thoroughly evaluate using in-domain and out-of-domain human annotations. Using these, we automatically annotate over six million posts across 12k subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values prevalent within various online communities. For instance, we discover a very negative stance towards conformity in the Vegan and AbolishTheMonarchy subreddits. Additionally, our study of geographically specific subreddits highlights the correlation between traditional values and conservative U.S. states. Through our work, we demonstrate how our dataset and method can be used as a complementary tool for qualitative study of online communication.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catch Me If You Can: Finding the Source of Infections in Temporal Networks</title>
<link>https://arxiv.org/abs/2412.10877</link>
<guid>https://arxiv.org/abs/2412.10877</guid>
<content:encoded><![CDATA[
<div> detection, spreading process, temporal graphs, SIR model, algorithms<br />
<br />
Summary:
Source detection (SD) is crucial for identifying the origin of spreading processes in networks. Previous work in this area has assumed a static network structure, which may not be realistic in many scenarios such as disease spread. To address this limitation, a model of SD on temporal graphs is proposed, where links between nodes exist only at certain time steps. This extends the traditional SD framework to account for dynamic network structures. The study utilizes the SIR model for spreading processes and provides algorithms and lower bounds for the SD problem in various settings, including consistent or dynamic source behavior on general graphs and trees. This work fills a gap in the existing literature by formalizing SD on temporal graphs and offering insights into source identification in time-varying networks. <br /><br /> <div>
arXiv:2412.10877v2 Announce Type: replace-cross 
Abstract: Source detection (SD) is the task of finding the origin of a spreading process in a network. Algorithms for SD help us combat diseases, misinformation, pollution, and more, and have been studied by physicians, physicists, sociologists, and computer scientists. The field has received considerable attention and been analyzed in many settings (e.g., under different models of spreading processes), yet all previous work shares the same assumption that the network the spreading process takes place in has the same structure at every point in time. For example, if we consider how a disease spreads through a population, it is unrealistic to assume that two people can either never or at every time infect each other, rather such an infection is possible precisely when they meet. Therefore, we propose an extended model of SD based on temporal graphs, where each link between two nodes is only present at some time step. Temporal graphs have become a standard model of time-varying graphs, and, recently, researchers have begun to study infection problems (such as influence maximization) on temporal graphs (arXiv:2303.11703, [Gayraud et al., 2015]). We give the first formalization of SD on temporal graphs. For this, we employ the standard SIR model of spreading processes ([Hethcote, 1989]). We give both lower bounds and algorithms for the SD problem in a number of different settings, such as with consistent or dynamic source behavior and on general graphs as well as on trees.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Conversational Structure and Style Shape Online Community Experiences</title>
<link>https://arxiv.org/abs/2508.08596</link>
<guid>https://arxiv.org/abs/2508.08596</guid>
<content:encoded><![CDATA[
<div> Keywords: Sense of Virtual Community, online communities, social interaction, Reddit, linguistic style

Summary: 
This study investigates the relationship between social interactions and Sense of Virtual Community (SOVC) in online communities, specifically focusing on Reddit. By analyzing conversational structure and linguistic style of 2,826 Reddit users across 281 subreddits, the study identifies patterns such as reciprocal reply chains and use of prosocial language that predict stronger community attachment. Three primary dimensions of SOVC within Reddit are identified: Membership & Belonging, Cooperation & Shared Values, and Connection & Influence. The findings provide valuable insights for fostering stronger community bonds and designing features to enhance the online community experience. This research contributes to the understanding of online communities and offers practical strategies for maximizing the positive impacts of online community participation. <div>
arXiv:2508.08596v1 Announce Type: new 
Abstract: Sense of Community (SOC) is vital to individual and collective well-being. Although social interactions have moved increasingly online, still little is known about the specific relationships between the nature of these interactions and Sense of Virtual Community (SOVC). This study addresses this gap by exploring how conversational structure and linguistic style predict SOVC in online communities, using a large-scale survey of 2,826 Reddit users across 281 varied subreddits. We develop a hierarchical model to predict self-reported SOVC based on automatically quantifiable and highly generalizable features that are agnostic to community topic and that describe both individual users and entire communities. We identify specific interaction patterns (e.g., reciprocal reply chains, use of prosocial language) associated with stronger communities and identify three primary dimensions of SOVC within Reddit -- Membership & Belonging, Cooperation & Shared Values, and Connection & Influence. This study provides the first quantitative evidence linking patterns of social interaction to SOVC and highlights actionable strategies for fostering stronger community attachment, using an approach that can generalize readily across community topics, languages, and platforms. These insights offer theoretical implications for the study of online communities and practical suggestions for the design of features to help more individuals experience the positive benefits of online community participation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective and Efficient Attributed Hypergraph Embedding on Nodes and Hyperedges</title>
<link>https://arxiv.org/abs/2508.08807</link>
<guid>https://arxiv.org/abs/2508.08807</guid>
<content:encoded><![CDATA[
<div> hypergraph, attributed hypergraph, embedding, node, hyperedge

Summary:
SAHE is proposed as an efficient and effective approach for computing node and hyperedge embeddings in attributed hypergraphs. The approach introduces higher-order similarity measures, HMS-N and HMS-E, to capture similarities between node pairs and hyperedge pairs, respectively. SAHE formulates the embedding objective to jointly preserve all-pair HMS-N and HMS-E similarities. The approach also includes optimizations to improve efficiency and avoid the computational expense of direct optimization. Extensive experiments on diverse attributed hypergraphs demonstrate that SAHE outperforms existing methods in embedding quality and is significantly faster in computation. <div>
arXiv:2508.08807v1 Announce Type: new 
Abstract: An attributed hypergraph comprises nodes with attributes and hyperedges that connect varying numbers of nodes. Attributed hypergraph node and hyperedge embedding (AHNEE) maps nodes and hyperedges to compact vectors for use in important tasks such as node classification, hyperedge link prediction, and hyperedge classification. Generating high-quality embeddings is challenging due to the complexity of attributed hypergraphs and the need to embed both nodes and hyperedges, especially in large-scale data. Existing solutions often fall short by focusing only on nodes or lacking native support for attributed hypergraphs, leading to inferior quality, and struggle with scalability on large attributed hypergraphs.
  We propose SAHE, an efficient and effective approach that unifies node and hyperedge embeddings for AHNEE computation, advancing the state of the art via comprehensive embedding formulations and algorithmic designs. First, we introduce two higher-order similarity measures, HMS-N and HMS-E, to capture similarities between node pairs and hyperedge pairs, respectively. These measures consider multi-hop connections and global topology within an extended hypergraph that incorporates attribute-based hyperedges. SAHE formulates the AHNEE objective to jointly preserve all-pair HMS-N and HMS-N similarities. Direct optimization is computationally expensive, so we analyze and unify core approximations of all-pair HMS-N and HMS-N to solve them simultaneously. To enhance efficiency, we design several non-trivial optimizations that avoid iteratively materializing large dense matrices while maintaining high-quality results. Extensive experiments on diverse attributed hypergraphs and 3 downstream tasks, compared against 11 baselines, show that SAHE consistently outperforms existing methods in embedding quality and is up to orders of magnitude faster.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Roots of International Perceptions: Simulating US Attitude Changes Towards China with LLM Agents</title>
<link>https://arxiv.org/abs/2508.08837</link>
<guid>https://arxiv.org/abs/2508.08837</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, opinion evolution, bias, media exposure, cross-cultural tolerance

Summary:<br /><br />The article introduces a novel framework utilizing Language Model Machines (LLMs) to model the evolution of attitudes towards China among US citizens over a two-decade period. By incorporating media data collection, user profile creation, and cognitive architecture, the framework successfully replicates the real trend of US attitudes towards China. The inclusion of debiased media exposure and a devil's advocate agent sheds light on the factors influencing opinion formation and the rare reversal from negative to positive attitudes. The simulation results highlight the impact of biased framing and selection bias on attitude shaping. This work contributes to the advancement of LLM-based modeling of cognitive behaviors in a large-scale, cross-border social context, providing insights into international biases and offering valuable implications for media consumers to understand the factors influencing their perspectives. Ultimately, the study aims to contribute to bias reduction and promote cross-cultural tolerance in society. <div>
arXiv:2508.08837v1 Announce Type: new 
Abstract: The rise of LLMs poses new possibilities in modeling opinion evolution, a long-standing task in simulation, by leveraging advanced reasoning abilities to recreate complex, large-scale human cognitive trends. While most prior works focus on opinion evolution surrounding specific isolated events or the views within a country, ours is the first to model the large-scale attitude evolution of a population representing an entire country towards another -- US citizens' perspectives towards China. To tackle the challenges of this broad scenario, we propose a framework that integrates media data collection, user profile creation, and cognitive architecture for opinion updates to successfully reproduce the real trend of US attitudes towards China over a 20-year period from 2005 to today. We also leverage LLMs' capabilities to introduce debiased media exposure, extracting neutral events from typically subjective news contents, to uncover the roots of polarized opinion formation, as well as a devils advocate agent to help explain the rare reversal from negative to positive attitudes towards China, corresponding with changes in the way Americans obtain information about the country. The simulation results, beyond validating our framework architecture, also reveal the impact of biased framing and selection bias in shaping attitudes. Overall, our work contributes to a new paradigm for LLM-based modeling of cognitive behaviors in a large-scale, long-term, cross-border social context, providing insights into the formation of international biases and offering valuable implications for media consumers to better understand the factors shaping their perspectives, and ultimately contributing to the larger social need for bias reduction and cross-cultural tolerance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where are GIScience Faculty Hired from? Analyzing Faculty Mobility and Research Themes Through Hiring Networks</title>
<link>https://arxiv.org/abs/2508.09043</link>
<guid>https://arxiv.org/abs/2508.09043</guid>
<content:encoded><![CDATA[
<div> Keywords: faculty hiring networks, GIScience, placement patterns, global knowledge dissemination, hiring practices

Summary: 
The study analyzes the placement patterns of 946 GIScience faculty worldwide by mapping connections between PhD-granting institutions and current affiliations. The dataset, though volunteer-contributed, provides valuable insight into global GIScience faculty placement. Hiring is concentrated in western countries with influential programs identified. The analysis reveals significant internal retention at continental and country levels, with a high non-self-hired ratio at the institutional level. Research themes have evolved over time, with emphasis on spatial data analytics, cartography, geovisualization, geocomputation, and environmental sciences. These results shed light on hiring practices' impact on global knowledge dissemination and promote academic equity within GIScience and Geography.<br /><br />Summary: <div>
arXiv:2508.09043v1 Announce Type: cross 
Abstract: Academia is profoundly influenced by faculty hiring networks, which serve as critical conduits for knowledge dissemination and the formation of collaborative research initiatives. While extensive research in various disciplines has revealed the institutional hierarchies inherent in these networks, their impacts within GIScience remain underexplored. To fill this gap, this study analyzes the placement patterns of 946 GIScience faculty worldwide by mapping the connections between PhD-granting institutions and current faculty affiliations. Our dataset, which is compiled from volunteer-contributed information, is the most comprehensive collection available in this field. While there may be some limitations in its representativeness, its scope and depth provide a unique and valuable perspective on the global placement patterns of GIScience faculty. Our analysis reveals several influential programs in placing GIScience faculty, with hiring concentrated in the western countries. We examined the diversity index to assess the representation of regions and institutions within the global GIScience faculty network. We observe significant internal retention at both the continental and country levels, and a high level of non-self-hired ratio at the institutional level. Over time, research themes have also evolved, with growing research clusters emphasis on spatial data analytics, cartography and geovisualization, geocomputation, and environmental sciences, etc. These results illuminate the influence of hiring practices on global knowledge dissemination and contribute to promoting academic equity within GIScience and Geography.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-learning optimizes predictions of missing links in real-world networks</title>
<link>https://arxiv.org/abs/2508.09069</link>
<guid>https://arxiv.org/abs/2508.09069</guid>
<content:encoded><![CDATA[
<div> Benchmark, Link prediction, Model stacking, Graph neural networks, Meta-learning <br />
<br />
Summary: 
Relational data often have missing links, making predicting these links crucial in various applications. This study compares different algorithms for link prediction on a diverse set of 550 real-world networks. The research shows that no single algorithm performs best on all networks, with model stacking using a random forest outperforming or competing with graph neural networks. Additionally, the performance of algorithms varies based on network characteristics such as degree distribution and triangle density. A meta-learning algorithm is introduced to optimize link predictions by selecting the best algorithm for individual networks, outperforming all existing methods and scaling effectively for large networks. Overall, model stacking with a random forest is scalable and shows promising results in predicting missing links, with the meta-learning approach proving to be the most effective method for optimizing link predictions. <div>
arXiv:2508.09069v1 Announce Type: cross 
Abstract: Relational data are ubiquitous in real-world data applications, e.g., in social network analysis or biological modeling, but networks are nearly always incompletely observed. The state-of-the-art for predicting missing links in the hard case of a network without node attributes uses model stacking or neural network techniques. It remains unknown which approach is best, and whether or how the best choice of algorithm depends on the input network's characteristics. We answer these questions systematically using a large, structurally diverse benchmark of 550 real-world networks under two standard accuracy measures (AUC and Top-k), comparing four stacking algorithms with 42 topological link predictors, two of which we introduce here, and two graph neural network algorithms. We show that no algorithm is best across all input networks, all algorithms perform well on most social networks, and few perform well on economic and biological networks. Overall, model stacking with a random forest is both highly scalable and surpasses on AUC or is competitive with graph neural networks on Top-k accuracy. But, algorithm performance depends strongly on network characteristics like the degree distribution, triangle density, and degree assortativity. We introduce a meta-learning algorithm that exploits this variability to optimize link predictions for individual networks by selecting the best algorithm to apply, which we show outperforms all state-of-the-art algorithms and scales to large networks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Vertex-Attribute-Constrained Densest $k$-Subgraph Problem</title>
<link>https://arxiv.org/abs/2508.06655</link>
<guid>https://arxiv.org/abs/2508.06655</guid>
<content:encoded><![CDATA[
<div> Attribute values, Densest k-Subgraph, Vertex-Attribute-Constrained Densest k-Subgraph, NP-hardness, Frank-Wolfe algorithm <br />
Summary: <br />
- Dense subgraph mining is crucial in various applications such as fraud detection and community detection.
- A new problem variant, VAC-DkS, incorporates vertex attribute values for more meaningful results.
- VAC-DkS retains NP-hardness and inapproximability but can be efficiently solved using a continuous relaxation approach and the Frank-Wolfe algorithm.
- The optimized landscape of VAC-DkS is analyzed, showing its effectiveness in large graph applications.
- In a political network mining scenario, VAC-DkS identifies a balanced set of politicians representing different ideologies, unlike classical DkS, which yields unbalanced results. <div>
arXiv:2508.06655v1 Announce Type: new 
Abstract: Dense subgraph mining is a fundamental technique in graph mining, commonly applied in fraud detection, community detection, product recommendation, and document summarization. In such applications, we are often interested in identifying communities, recommendations, or summaries that reflect different constituencies, styles or genres, and points of view. For this task, we introduce a new variant of the Densest $k$-Subgraph (D$k$S) problem that incorporates the attribute values of vertices. The proposed Vertex-Attribute-Constrained Densest $k$-Subgraph (VAC-D$k$S) problem retains the NP-hardness and inapproximability properties of the classical D$k$S. Nevertheless, we prove that a suitable continuous relaxation of VAC-D$k$S is tight and can be efficiently tackled using a projection-free Frank--Wolfe algorithm. We also present an insightful analysis of the optimization landscape of the relaxed problem. Extensive experimental results demonstrate the effectiveness of our proposed formulation and algorithm, and its ability to scale up to large graphs. We further elucidate the properties of VAC-D$k$S versus classical D$k$S in a political network mining application, where VAC-D$k$S identifies a balanced and more meaningful set of politicians representing different ideological camps, in contrast to the classical D$k$S solution which is unbalanced and rather mundane.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomy of a Machine Learning Ecosystem: 2 Million Models on Hugging Face</title>
<link>https://arxiv.org/abs/2508.06811</link>
<guid>https://arxiv.org/abs/2508.06811</guid>
<content:encoded><![CDATA[
<div> Keywords: generative machine learning, artificial intelligence, model fine-tuning, model family trees, genetic similarity<br />
<br />
Summary: 
This paper examines the development and deployment of generative machine learning and artificial intelligence models, focusing on the fine-tuning process. Analyzing 1.86 million models on Hugging Face, the study explores the structure of model family trees and genetic similarities among models. The findings indicate that models within the same family exhibit more genetic resemblance. However, mutations occur rapidly and in a directed manner, leading sibling models to show more similarity than parent/child pairs. The analysis also reveals insights into the evolution of models within the machine learning ecosystem, such as shifts in licenses and language compatibility, as well as changes in model card lengths and standardization. Overall, this work offers a novel perspective on model fine-tuning and suggests that ecological models and methods can provide valuable scientific insights. <br /><br />Summary: <div>
arXiv:2508.06811v1 Announce Type: new 
Abstract: Many have observed that the development and deployment of generative machine learning (ML) and artificial intelligence (AI) models follow a distinctive pattern in which pre-trained models are adapted and fine-tuned for specific downstream tasks. However, there is limited empirical work that examines the structure of these interactions. This paper analyzes 1.86 million models on Hugging Face, a leading peer production platform for model development. Our study of model family trees -- networks that connect fine-tuned models to their base or parent -- reveals sprawling fine-tuning lineages that vary widely in size and structure. Using an evolutionary biology lens to study ML models, we use model metadata and model cards to measure the genetic similarity and mutation of traits over model families. We find that models tend to exhibit a family resemblance, meaning their genetic markers and traits exhibit more overlap when they belong to the same model family. However, these similarities depart in certain ways from standard models of asexual reproduction, because mutations are fast and directed, such that two `sibling' models tend to exhibit more similarity than parent/child pairs. Further analysis of the directional drifts of these mutations reveals qualitative insights about the open machine learning ecosystem: Licenses counter-intuitively drift from restrictive, commercial licenses towards permissive or copyleft licenses, often in violation of upstream license's terms; models evolve from multi-lingual compatibility towards english-only compatibility; and model cards reduce in length and standardize by turning, more often, to templates and automatically generated text. Overall, this work takes a step toward an empirically grounded understanding of model fine-tuning and suggests that ecological models and methods can yield novel scientific insights.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection</title>
<link>https://arxiv.org/abs/2508.07201</link>
<guid>https://arxiv.org/abs/2508.07201</guid>
<content:encoded><![CDATA[
<div> Graph-based models, especially those focused on rumor detection on social media, assume rumor propagation trees have deep structures and learn sequential stance features along branches. However, research shows that most nodes in these trees are shallow 1-level replies, implying wide structures rather than deep ones. To address this, the Rumor Adaptive Graph Contrastive Learning (RAGCL) method is proposed, which uses adaptive view augmentation guided by node centralities. The method follows three principles for RPT augmentation: exempting root nodes, retaining deep reply nodes, and preserving lower-level nodes in deep sections. By leveraging node dropping, attribute masking, and edge dropping based on centrality scores, RAGCL generates views to learn robust rumor representations. Experimental results on benchmark datasets demonstrate that RAGCL outperforms existing methods, suggesting its effectiveness in tailored rumor detection for wide-structure RPTs. The proposed principles and augmentation techniques could have broader applications in tree-structured graph analysis.<br /><br />Keywords: rumor detection, social media, graph-based models, contrastive learning, wide-structure RPTs <br /><br />Summary: Rumor Adaptive Graph Contrastive Learning (RAGCL) is introduced to address the wide structures of rumor propagation trees (RPTs) on social media. By using centrality-guided adaptive view augmentation and following key principles for RPT augmentation, RAGCL outperforms existing methods in rumor detection. The approach sheds light on the nature of RPTs and demonstrates the potential for broader applications in tree-structured graph analysis. <div>
arXiv:2508.07201v1 Announce Type: new 
Abstract: Rumor detection on social media has become increasingly important. Most existing graph-based models presume rumor propagation trees (RPTs) have deep structures and learn sequential stance features along branches. However, through statistical analysis on real-world datasets, we find RPTs exhibit wide structures, with most nodes being shallow 1-level replies. To focus learning on intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning (RAGCL) method with adaptive view augmentation guided by node centralities. We summarize three principles for RPT augmentation: 1) exempt root nodes, 2) retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We employ node dropping, attribute masking and edge dropping with probabilities from centrality-based importance scores to generate views. A graph contrastive objective then learns robust rumor representations. Extensive experiments on four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods. Our work reveals the wide-structure nature of RPTs and contributes an effective graph contrastive learning approach tailored for rumor detection through principled adaptive augmentation. The proposed principles and augmentation techniques can potentially benefit other applications involving tree-structured graphs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.07205</link>
<guid>https://arxiv.org/abs/2508.07205</guid>
<content:encoded><![CDATA[
<div> Anomaly Detection, Graph Supervised Contrastive Learning, Rumor Detection, Imbalanced Data, Social Media<br />
<br />
Summary: <br />
The study discusses the challenges of rumor detection in social media, where imbalanced data distribution poses a problem for traditional classification methods. By analyzing datasets from Weibo and Twitter, it is observed that rumors are concentrated in news domains, while non-rumors are mainly in entertainment domains, suggesting a shift towards anomaly detection for rumor identification. The proposed Anomaly Detection framework with Graph Supervised Contrastive Learning (AD-GSCL) leverages graph contrastive learning to address the imbalanced data issue and heuristically treats unlabeled data as non-rumors in the detection process. The framework demonstrates superior performance in various conditions, including class-balanced, imbalanced, and few-shot scenarios. This approach provides valuable insights for improving real-world rumor detection methodologies on social media platforms with imbalanced data distributions. <div>
arXiv:2508.07205v1 Announce Type: new 
Abstract: Current rumor detection methods based on propagation structure learning predominately treat rumor detection as a class-balanced classification task on limited labeled data. However, real-world social media data exhibits an imbalanced distribution with a minority of rumors among massive regular posts. To address the data scarcity and imbalance issues, we construct two large-scale conversation datasets from Weibo and Twitter and analyze the domain distributions. We find obvious differences between rumor and non-rumor distributions, with non-rumors mostly in entertainment domains while rumors concentrate in news, indicating the conformity of rumor detection to an anomaly detection paradigm. Correspondingly, we propose the Anomaly Detection framework with Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats unlabeled data as non-rumors and adapts graph contrastive learning for rumor detection. Extensive experiments demonstrate AD-GSCL's superiority under class-balanced, imbalanced, and few-shot conditions. Our findings provide valuable insights for real-world rumor detection featuring imbalanced data distributions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning</title>
<link>https://arxiv.org/abs/2508.07264</link>
<guid>https://arxiv.org/abs/2508.07264</guid>
<content:encoded><![CDATA[
<div> fusion, multimodal, classification, robustness, scalability

Summary:
FLUID proposes a token-level pipeline for multimodal classification, enhancing robust integration of visual and textual signals. It introduces Q-transforms to distill salient token-level features, a two-stage fusion scheme for cross-modal consistency and adaptive fusion, and a Mixture-of-Experts for efficient specialization. FLUID achieves 91% accuracy on the GLAMI-1M benchmark, surpassing prior baselines and demonstrating resilience to noise, class imbalance, and semantic heterogeneity. Ablation studies validate the effectiveness of individual components and their synergistic benefits, positioning FLUID as a scalable, noise-resilient solution for multimodal product classification.<br /><br />Summary: <div>
arXiv:2508.07264v1 Announce Type: new 
Abstract: Multimodal classification requires robust integration of visual and textual signals, yet common fusion strategies are brittle and vulnerable to modality-specific noise. In this paper, we present \textsc{FLUID}-Flow-Latent Unified Integration via Token Distillation for Expert Specialization, a principled token-level pipeline that improves cross-modal robustness and scalability. \textsc{FLUID} contributes three core elements: (1) \emph{Q-transforms}, learnable query tokens that distill and retain salient token-level features from modality-specific backbones; (2) a two-stage fusion scheme that enforces cross-modal consistency via contrastive alignment and then performs adaptive, task-aware fusion through a gating mechanism and a \emph{Q-bottleneck} that selectively compresses information for downstream reasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at prediction time that enables efficient specialization to diverse semantic patterns. Extensive experiments demonstrate that \textsc{FLUID} attains \(91\%\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior baselines and exhibiting strong resilience to label noise, long-tail class imbalance, and semantic heterogeneity. Targeted ablation studies corroborate both the individual and synergistic benefits of the proposed components, positioning \textsc{FLUID} as a scalable, noise-resilient solution for multimodal product classification.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering link-weight structure in complex networks with weight-aware random walks</title>
<link>https://arxiv.org/abs/2508.07489</link>
<guid>https://arxiv.org/abs/2508.07489</guid>
<content:encoded><![CDATA[
<div> Keywords: edge weights, node embeddings, random walk strategies, network models, real-world networks

Summary: 
This paper explores the impact of different random walk strategies on preserving edge weight information in low-dimensional node embeddings. The study compares traditional unweighted, strength-based, and fully weight-aware random walks using network models, real-world graphs, and networks with low-weight edge removal. Results show that weight-aware random walks outperform other strategies, achieving high correlations with original edge weights in network models. However, performance in real-world networks varies based on topology and weight distribution. Removing weak edges through thresholding can initially improve correlation by reducing noise but excessive pruning degrades representation quality. Overall, using a weight-aware random walk is the most effective approach for preserving node weight information in embeddings, but the solution may not be universally applicable. <div>
arXiv:2508.07489v1 Announce Type: new 
Abstract: Using edge weights is essential for modeling real-world systems where links possess relevant information, and preserving this information in low-dimensional representations is relevant for classification and prediction tasks. This paper systematically investigates how different random walk strategies - traditional unweighted, strength-based, and fully weight-aware - keeps edge weight information when generating node embeddings. Using network models, real-world graphs, and networks subjected to low-weight edge removal, we measured the correlation between original edge weights and the similarity of node pairs in the embedding space generated by random walk strategies. Our results consistently showed that weight-aware random walks significantly outperform other strategies, achieving correlations above 0.90 in network models. However, performance in real-world networks was more heterogeneous, influenced by factors like topology and weight distribution. Our analysis also revealed that removing weak edges via thresholding can initially improve correlation by reducing noise, but excessive pruning degrades representation quality. Our findings suggest that simply using a weight-aware random walk is generally the best approach for preserving node weight information in embeddings, but it is not a universal solution.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Platform Migration to Cultural Integration: the Ingress and Diffusion of #wlw from TikTok to RedNote in Queer Women</title>
<link>https://arxiv.org/abs/2508.07579</link>
<guid>https://arxiv.org/abs/2508.07579</guid>
<content:encoded><![CDATA[
<div> Keywords: Hashtags, queer communities, cross-cultural communication, social media, content analysis

Summary:
The study explores the rise of the #wlw hashtag in the Chinese lesbian community on RedNote following user migration triggered by the temporary US TikTok ban. Through a two-phase content analysis of 418 #wlw posts, the research examines the usage patterns during the hashtag's introduction and diffusion. The successful introduction of #wlw was facilitated by bold importation by TikTok immigrants, mutual interpretation between different populations, and discussions among RedNote natives. The hashtag has become recognized on RedNote as a tool for sharing queer life and has expanded to support feminism discourse. This case study provides empirical insights into cross-cultural communication among marginalized communities on social media platforms. <div>
arXiv:2508.07579v1 Announce Type: new 
Abstract: Hashtags serve as identity markers and connection tools in online queer communities. Recently, the Western-origin #wlw (women-loving-women) hashtag has risen in the Chinese lesbian community on RedNote, coinciding with user migration triggered by the temporary US TikTok ban. This event provides a unique lens to study cross-cultural hashtag ingress and diffusion through the populations' responsive behaviors in cyber-migration. In this paper, we conducted a two-phase content analysis of 418 #wlw posts from January and April, examining different usage patterns during the hashtag's ingress and diffusion. Results indicate that the successful introduction of #wlw was facilitated by TikTok immigrants' bold importation, both populations' mutual interpretation, and RedNote natives' discussions. In current manifestation of diffusion, #wlw becomes a RedNote-recognized queer hashtag for sharing queer life, and semantically expands to support feminism discourse. Our findings provide empirical insights for enhancing the marginalized communities' cross-cultural communication.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fabricating Holiness: Characterizing Religious Misinformation Circulators on Arabic Social Media</title>
<link>https://arxiv.org/abs/2508.07845</link>
<guid>https://arxiv.org/abs/2508.07845</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, social media, fabricated Hadith, religious content, quantitative analysis

Summary: 
This study focuses on the spread of fabricated Hadith, or quotes attributed to Prophet Muhammad, on Arabic social media as a form of religious misinformation. The researchers use quantitative methods to analyze the characteristics of social media users who interact with fabricated Hadith. They identify two main groups: circulators who share fabricated Hadith and debunkers who challenge them. The study reveals that circulators are linked to Shia and Sunni Islamic accounts, while debunkers follow academic Islamic scholars and have diverse non-religious interests. Logistic Regression is used to predict user behaviors, providing insights into the differences between the two groups. The findings suggest that addressing religious misinformation on social media requires understanding the dynamics of different user groups and their interactions with fabricated content.<br /><br />Summary: <div>
arXiv:2508.07845v1 Announce Type: new 
Abstract: Misinformation is a growing concern in a decade involving critical global events. While social media regulation is mainly dedicated towards the detection and prevention of fake news and political misinformation, there is limited research about religious misinformation which has only been addressed through qualitative approaches. In this work, we study the spread of fabricated quotes (Hadith) that are claimed to belong to Prophet Muhammad (the prophet of Islam) as a case study demonstrating one of the most common religious misinformation forms on Arabic social media. We attempt through quantitative methods to understand the characteristics of social media users who interact with fabricated Hadith. We spotted users who frequently circulate fabricated Hadith and others who frequently debunk it to understand the main differences between the two groups. We used Logistic Regression to automatically predict their behaviors and analyzed its weights to gain insights about the characteristics and interests of each group. We find that both fabricated Hadith circulators and debunkers have generally a lot of ties to religious accounts. However, circulators are identified by many accounts that follow the Shia branch of Islam, Sunni Islamic public figures from the gulf countries, and many Sunni non-professional pages posting Islamic content. On the other hand, debunkers are identified by following academic Islamic scholars from multiple countries and by having more intellectual non-religious interests like charity, politics, and activism.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymmetric Network Games: $\alpha$-Potential Function and Learning</title>
<link>https://arxiv.org/abs/2508.06619</link>
<guid>https://arxiv.org/abs/2508.06619</guid>
<content:encoded><![CDATA[
<div> network games, asymmetric networks, alpha-potential games, Nash equilibrium, social welfare <br />
Summary: This paper examines static network games in asymmetric networks using alpha-potential games. It introduces the concept of the alpha-potential function to analyze convergence to 2alpha-Nash equilibrium using specific algorithms. The study focuses on linear-quadratic network games and discusses the behavior of alpha in relation to network asymmetry. Bounds on social welfare at the alpha-Nash equilibrium are determined. Numerical simulations demonstrate the convergence of proposed algorithms and properties of learned 2alpha-Nash equilibria. <div>
arXiv:2508.06619v1 Announce Type: cross 
Abstract: In a network game, players interact over a network and the utility of each player depends on his own action and on an aggregate of his neighbours' actions. Many real world networks of interest are asymmetric and involve a large number of heterogeneous players. This paper analyzes static network games using the framework of $\alpha$-potential games. Under mild assumptions on the action sets (compact intervals) and the utility functions (twice continuously differentiable) of the players, we derive an expression for an inexact potential function of the game, called the $\alpha$-potential function. Using such a function, we show that modified versions of the sequential best-response algorithm and the simultaneous gradient play algorithm achieve convergence of players' actions to a $2\alpha$-Nash equilibrium. For linear-quadratic network games, we show that $\alpha$ depends on the maximum asymmetry in the network and is well-behaved for a wide range of networks of practical interest. Further, we derive bounds on the social welfare of the $\alpha$-Nash equilibrium corresponding to the maximum of the $\alpha$-potential function, under suitable assumptions. We numerically illustrate the convergence of the proposed algorithms and properties of the learned $2\alpha$-Nash equilibria.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model</title>
<link>https://arxiv.org/abs/2508.07209</link>
<guid>https://arxiv.org/abs/2508.07209</guid>
<content:encoded><![CDATA[
<div> Keywords: Pretrained Language Models, social media, rumor detection, Post Engagement Prediction, TwitterCorpus

Summary: 
The study focuses on enhancing the performance of Pretrained Language Models (PLMs) in social media tasks, particularly rumor detection. The researchers propose a Post Engagement Prediction (PEP) strategy to incorporate information from propagation structures into PLMs, improving their ability to capture interactions crucial for rumor detection. They curate and release large-scale Twitter corpus and unlabeled claim conversation datasets with propagation structures. By utilizing these resources and implementing the PEP strategy, a Twitter-tailored PLM called SoLM is trained, demonstrating significant performance improvements in rumor detection tasks. The experiments show that PEP boosts baseline models by 1.0-3.7% accuracy and even outperforms current state-of-the-art methods on multiple datasets. SoLM, without high-level modules, achieves competitive results, highlighting the effective learning of discriminative post interaction features. 

<br /><br />Summary: <div>
arXiv:2508.07209v1 Announce Type: cross 
Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language Processing tasks, benefiting from large-scale pretraining and self-attention mechanism's ability to capture long-range dependencies. However, their performance on social media application tasks like rumor detection remains suboptimal. We attribute this to mismatches between pretraining corpora and social texts, inadequate handling of unique social symbols, and pretraining tasks ill-suited for modeling user engagements implicit in propagation structures. To address these issues, we propose a continue pretraining strategy called Post Engagement Prediction (PEP) to infuse information from propagation structures into PLMs. PEP makes models to predict root, branch, and parent relations between posts, capturing interactions of stance and sentiment crucial for rumor detection. We also curate and release large-scale Twitter corpus: TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments demonstrate PEP significantly boosts rumor detection performance across universal and social media PLMs, even in few-shot scenarios. On benchmark datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it to outperform current state-of-the-art methods on multiple datasets. SoLM alone, without high-level modules, also achieves competitive results, highlighting the strategy's effectiveness in learning discriminative post interaction features.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Agentic Service Ecosystems: Measurement, Analysis, and Optimization</title>
<link>https://arxiv.org/abs/2508.07343</link>
<guid>https://arxiv.org/abs/2508.07343</guid>
<content:encoded><![CDATA[
<div> autonomous agents, Agentic Service Ecosystems, swarm intelligence, emergent behavior, decentralized systems
Summary:<br /><br />The Agentic Service Ecosystem comprises diverse autonomous agents engaging in resource and service interactions, leading to complex systems. Traditional linear analysis methods are inadequate due to the autonomy and capabilities of these agents. Swarm intelligence, characterized by decentralized self-organization, offers a novel approach to understanding and optimizing such ecosystems. However, existing research lacks a unified methodology to comprehensively analyze swarm intelligence emergence in agentic contexts. A proposed framework advocates for a three-step approach - measurement, analysis, and optimization - to uncover the cyclic mechanisms fostering emergence. The review of current technologies highlights their strengths and limitations, pinpointing unresolved challenges. The framework not only offers theoretical support but also actionable methods for real-world applications, enhancing the understanding and optimization of Agentic Service Ecosystems. <div>
arXiv:2508.07343v1 Announce Type: cross 
Abstract: The Agentic Service Ecosystem consists of heterogeneous autonomous agents (e.g., intelligent machines, humans, and human-machine hybrid systems) that interact through resource exchange and service co-creation. These agents, with distinct behaviors and motivations, exhibit autonomous perception, reasoning, and action capabilities, which increase system complexity and make traditional linear analysis methods inadequate. Swarm intelligence, characterized by decentralization, self-organization, emergence, and dynamic adaptability, offers a novel theoretical lens and methodology for understanding and optimizing such ecosystems. However, current research, owing to fragmented perspectives and cross-ecosystem differences, fails to comprehensively capture the complexity of swarm-intelligence emergence in agentic contexts. The lack of a unified methodology further limits the depth and systematic treatment of the research. This paper proposes a framework for analyzing the emergence of swarm intelligence in Agentic Service Ecosystems, with three steps: measurement, analysis, and optimization, to reveal the cyclical mechanisms and quantitative criteria that foster emergence. By reviewing existing technologies, the paper analyzes their strengths and limitations, identifies unresolved challenges, and shows how this framework provides both theoretical support and actionable methods for real-world applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Densest $k$-Subgraph Mining and Diagonal Loading</title>
<link>https://arxiv.org/abs/2410.07388</link>
<guid>https://arxiv.org/abs/2410.07388</guid>
<content:encoded><![CDATA[
<div> Keywords: Densest $k$-Subgraph, continuous relaxation, diagonal loading, Frank--Wolfe algorithm, subgraph density <br />
<br />
Summary: 
This article focuses on the Densest $k$-Subgraph (D$k$S) problem and introduces a continuous relaxation of the binary quadratic D$k$S problem with a diagonal loading term. The study shows that this non-convex relaxation is tight for certain diagonal loading parameters and analyzes the impact of these parameters on the optimization landscape. Two projection-free algorithms, based on Frank--Wolfe and explicit constraint parameterization, are proposed to solve the relaxed problem efficiently. Experimental results indicate that both algorithms have advantages compared to existing methods, with the Frank--Wolfe algorithm particularly excelling in subgraph density, computational complexity, and scalability to large datasets. This research provides valuable insights into optimizing subgraph density and offers effective algorithms for addressing the D$k$S problem. <br /> <div>
arXiv:2410.07388v3 Announce Type: replace 
Abstract: The Densest $k$-Subgraph (D$k$S) problem aims to find a subgraph comprising $k$ vertices with the maximum number of edges between them. A continuous relaxation of the binary quadratic D$k$S problem is considered, which incorporates a diagonal loading term. It is shown that this non-convex, continuous relaxation is tight for a range of diagonal loading parameters, and the impact of the diagonal loading parameter on the optimization landscape is studied. On the algorithmic side, two projection-free algorithms are proposed to tackle the relaxed problem, based on Frank--Wolfe and explicit constraint parameterization, respectively. Experiments suggest that both algorithms have merits relative to the state-of-art, while the Frank--Wolfe-based algorithm stands out in terms of subgraph density, computational complexity, and ability to scale up to very large datasets.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delayed takedown of illegal content on social media makes moderation ineffective</title>
<link>https://arxiv.org/abs/2502.08841</link>
<guid>https://arxiv.org/abs/2502.08841</guid>
<content:encoded><![CDATA[
<div> takedown delay, illegal content, social media, agent-based model, diffusion<br />
Summary:<br />
This study examines the impact of takedown delays on the prevalence, reach, and exposure to illegal content on social media. Analyzing data from the EU Digital Services Act Transparency Database, the research finds significant variation in takedown delays across major platforms. Using an agent-based model calibrated to empirical data, the study reveals that rapid content removal within hours significantly reduces the spread of illegal content, while longer delays do not effectively curb its spread. The simulations show how takedown speed plays a crucial role in shaping the diffusion of illegal content online. The findings suggest that faster content removal is essential for combating the spread of illegal content, highlighting the need for timely enforcement measures. However, the study also acknowledges the limitations of strict enforcement policies in addressing the challenges posed by illegal content on social media platforms. <div>
arXiv:2502.08841v2 Announce Type: replace 
Abstract: Illegal content on social media poses significant societal harm and necessitates timely removal. However, the impact of the speed of content removal on prevalence, reach, and exposure to illegal content remains underexplored. This study examines the relationship with a systematic analysis of takedown delays using data from the EU Digital Services Act Transparency Database, covering five major platforms over a one-year period. We find substantial variation in takedown delay, with some content remaining online for weeks or even months. To evaluate how these delays affect the prevalence and reach of illegal content and exposure to it, we develop an agent-based model and calibrate it to empirical data. We simulate illegal content diffusion, revealing that rapid takedown (within hours) significantly reduces prevalence, reach, and exposure to illegal content, while longer delays fail to reduce its spread. Though the effect of delay may seem intuitive, our simulations quantify exactly how takedown speed shapes the spread of illegal content. Building on these results, we point to the benefits of faster content removal to effectively curb the spread of illegal content, while also considering the limitations of strict enforcement policies.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content</title>
<link>https://arxiv.org/abs/2503.04773</link>
<guid>https://arxiv.org/abs/2503.04773</guid>
<content:encoded><![CDATA[
<div> Keywords: segregation prediction, Large Language Models (LLMs), online review mining, Reflective LLM Coder, REasoning-and-EMbedding (RE'EM) framework

Summary: 
The study focuses on using Large Language Models (LLMs) to automate online review mining for predicting segregation in urban daily life. They introduce a Reflective LLM Coder to extract insights from social media content related to segregation experience dimensions like cultural resonance, accessibility, and community engagement. A codebook is created to guide LLMs in generating review summaries and ratings for segregation prediction. The RE'EM framework combines language model capabilities to enhance prediction accuracy with a significant improvement in R2 and MSE. The codebook's effectiveness is validated across different cities, indicating its generalizability. A user study confirms the cognitive benefits of codebook-guided summaries in understanding Points of Interest (POIs) social inclusiveness. Overall, the study showcases the potential of AI in addressing societal inequalities and promoting social inclusiveness. 

<br /><br />Summary: <div>
arXiv:2503.04773v3 Announce Type: replace-cross 
Abstract: Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose using Large Language Models (LLMs) to automate online review mining for segregation prediction. We design a Reflective LLM Coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction. Experiments on real-world data demonstrate that our framework greatly improves prediction accuracy, with a 22.79% elevation in R2 and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy. Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving POIs' social inclusiveness. Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with AI.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Organizations, teams, and job mobility: A social microdynamics approach</title>
<link>https://arxiv.org/abs/2503.24117</link>
<guid>https://arxiv.org/abs/2503.24117</guid>
<content:encoded><![CDATA[
<div> team composition, worker reunions, job mobility, social capital, organizational structure

Summary: 
Workers in large organizations are influenced by past social connections when changing jobs, with a high percentage of moves resulting in reunions with former coworkers. The study of a US Army organization reveals that social ties within teams play a significant role in shaping internal job changes. Worker reunions are more informative predictors of job moves than labor supply and demand or occupational specialization. The likelihood of reunions increases with time spent together and smaller team sizes, indicating the importance of familiarity and trust in these reunions. This highlights the key role of social capital and team structures in understanding organizational worker mobility. <div>
arXiv:2503.24117v2 Announce Type: replace-cross 
Abstract: Most of the modeling approaches used to understand organizational worker mobility are highly stylized, using idealizations such as structureless organizations, indistinguishable workers, and a lack of social bonding of the workers. In this article, aided by a decade of precise, temporally resolved data of a large civilian organization of the US Army in which employees can change jobs in a similar way to many private organizations, we introduce a new framework to describe organizations as composites of teams within which individuals perform specific tasks and where social connections develop. By tracking the personnel composition of organizational teams, we find that workers who change jobs are highly influenced by preferring to reunite with past coworkers. In this organization, 34% of all moves across temporally stable teams (and 32% of the totality of moves) lead to worker reunions, percentages that have not been reported and are well-above intuitive expectation. To assess the importance of worker reunions in determining job moves, we compare them to labor supply and demand with or without occupational specialization. The comparison shows that the most consistent information about job change is provided by reunions. We find that the greater the time workers spend together or the smaller the team they share both increase their likelihood to reunite, supporting the notion of increased familiarity and trust behind such reunions and the dominant role of social capital in the evolution of large organizations. Our study of this organization supports the idea that to correctly forecast job mobility inside large organizations, their teams structures and the social ties formed in those teams play a key role in shaping internal job change.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities</title>
<link>https://arxiv.org/abs/2508.06342</link>
<guid>https://arxiv.org/abs/2508.06342</guid>
<content:encoded><![CDATA[
<div> Keywords: social interactions, street view imagery, urban planning, sociability, place attachment

Summary: 
The study aims to analyze street view imagery to extract information on social interactions, guided by Mehta's taxonomy of sociability. Through linear regression models, the researchers found that factors such as sky view index, green view index, and place attachment scores were associated with different types of social interactions. The results supported urban planning theories, indicating that environmental factors play a role in shaping sociability in cities. This research suggests that street view images can provide valuable insights into the relationship between social interactions and the built environment, offering a scalable and privacy-preserving tool for studying urban sociability. Further exploration of street view imagery could help in cross-cultural theory testing and inform the design of socially vibrant cities. 

<br /><br />Summary: <div>
arXiv:2508.06342v1 Announce Type: cross 
Abstract: Designing socially active streets has long been a goal of urban planning, yet existing quantitative research largely measures pedestrian volume rather than the quality of social interactions. We hypothesize that street view imagery -- an inexpensive data source with global coverage -- contains latent social information that can be extracted and interpreted through established social science theory. As a proof of concept, we analyzed 2,998 street view images from 15 cities using a multimodal large language model guided by Mehta's taxonomy of passive, fleeting, and enduring sociability -- one illustrative example of a theory grounded in urban design that could be substituted or complemented by other sociological frameworks. We then used linear regression models, controlling for factors like weather, time of day, and pedestrian counts, to test whether the inferred sociability measures correlate with city-level place attachment scores from the World Values Survey and with environmental predictors (e.g., green, sky, and water view indices) derived from individual street view images. Results aligned with long-standing urban planning theory: the sky view index was associated with all three sociability types, the green view index predicted enduring sociability, and place attachment was positively associated with fleeting sociability. These results provide preliminary evidence that street view images can be used to infer relationships between specific types of social interactions and built environment variables. Further research could establish street view imagery as a scalable, privacy-preserving tool for studying urban sociability, enabling cross-cultural theory testing and evidence-based design of socially vibrant cities.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Graph Theory of Majority Illusions: Theoretical Results and Computational Experiments</title>
<link>https://arxiv.org/abs/2304.02258</link>
<guid>https://arxiv.org/abs/2304.02258</guid>
<content:encoded><![CDATA[
<div> illusion, opinion, social network, majority, network structure

Summary:
The study explores how network structures can lead to distortions in the perception of opinions within a community. It focuses on the 'majority illusion', where an individual's direct circles may show a different majority opinion from the global one. The research indicates that no network structure can ensure that most agents perceive the correct majority opinion. Computational experiments are conducted to assess the likelihood of majority illusions in various network classes. The findings suggest that certain network configurations can allow for a significant portion of the population to have misconceptions about the overall distribution of opinions. The study also considers the potential for other types of illusions beyond the majority illusion, emphasizing the impact of network wiring on information distortion within social networks. <div>
arXiv:2304.02258v4 Announce Type: replace 
Abstract: The popularity of an opinion in one's direct circles is not necessarily a good indicator of its popularity in one's entire community. Network structures make local information about global properties of the group potentially inaccurate, and the way a social network is wired constrains what kind of information distortion can actually occur. In this paper, we discuss which classes of networks allow for a large enough proportion of the population to get a wrong enough impression about the overall distribution of opinions. We start by focusing on the 'majority illusion', the case where one sees a majority opinion in one's direct circles that differs from the global majority. We show that no network structure can guarantee that most agents see the correct majority. We then perform computational experiments to study the likelihood of majority illusions in different classes of networks. Finally, we generalize to other types of illusions.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Random Field model for Hypergraph-based Machine Learning</title>
<link>https://arxiv.org/abs/2308.14172</link>
<guid>https://arxiv.org/abs/2308.14172</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraph, data generation, machine learning, structure inference, node classification

Summary: 
This paper introduces a novel approach for modelling data generation processes on hypergraphs, using a hypergraph Markov random field. The model captures the joint distribution of node and hyperedge features in a hypergraph through a multivariate Gaussian distribution, uniquely determined by the hypergraph structure. The proposed data-generating process serves as an inductive bias for hypergraph machine learning tasks, enhancing algorithm design. The paper focuses on two tasks: hypergraph structure inference (HGSI) and node classification (Hypergraph-MLP). Empirical evaluation shows that HGSI outperforms existing methods on synthetic and real data, while Hypergraph-MLP excels in six hypergraph node classification benchmarks, offering improved runtime efficiency and robustness against structural perturbations during inference. This framework advances the understanding of hypergraph data generation and informs the design of effective machine learning algorithms for hypergraphs. 

<br /><br />Summary: <div>
arXiv:2308.14172v4 Announce Type: replace-cross 
Abstract: Understanding the data-generating process is essential for building machine learning models that generalise well while ensuring robustness and interpretability. This paper addresses the fundamental challenge of modelling the data generation processes on hypergraphs and explores how such models can inform the design of machine learning algorithms for hypergraph data. The key to our approach is the development of a hypergraph Markov random field that models the joint distribution of the node features and hyperedge features in a hypergraph through a multivariate Gaussian distribution whose covariance matrix is uniquely determined by the hypergraph structure. The proposed data-generating process provides a valuable inductive bias for various hypergraph machine learning tasks, thus enhancing the algorithm design. In this paper, we focus on two representative downstream tasks: structure inference and node classification. Accordingly, we introduce two novel frameworks: 1) an original hypergraph structure inference framework named HGSI, and 2) a novel learning framework entitled Hypergraph-MLP for node classification on hypergraphs. Empirical evaluation of the proposed frameworks demonstrates that: 1) HGSI outperforms existing hypergraph structure inference methods on both synthetic and real-world data; and 2) Hypergraph-MLP outperforms baselines in six hypergraph node classification benchmarks, at the same time promoting runtime efficiency and robustness against structural perturbations during inference.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications</title>
<link>https://arxiv.org/abs/2508.04889</link>
<guid>https://arxiv.org/abs/2508.04889</guid>
<content:encoded><![CDATA[
<div> API, decentralized, Vue.js, interoperate, ecosystem
Summary:
Graffiti is a system designed to create personalized social applications that can easily coexist and interact with each other. Through the concept of total reification, conflicting designs and moderation rules can still interoperate, while the use of channels prevents accidental context collapse. The system operates through a minimal client-side API, allowing for decentralized implementations. A Vue.js plugin enables the development of various applications, like Twitter or Wikipedia, using only client-side code. Case studies demonstrate how these applications can interoperate and the broader ecosystem Graffiti enables. Overall, Graffiti offers a flexible and interoperable solution for building unique social applications that maintain a user's friends and data across different designs and features. 

<br /><br />Summary: <div>
arXiv:2508.04889v1 Announce Type: new 
Abstract: Most social applications, from Twitter to Wikipedia, have rigid one-size-fits-all designs, but building new social applications is both technically challenging and results in applications that are siloed away from existing communities. We present Graffiti, a system that can be used to build a wide variety of personalized social applications with relative ease that also interoperate with each other. People can freely move between a plurality of designs -- each with its own aesthetic, feature set, and moderation -- all without losing their friends or data.
  Our concept of total reification makes it possible for seemingly contradictory designs, including conflicting moderation rules, to interoperate. Conversely, our concept of channels prevents interoperation from occurring by accident, avoiding context collapse.
  Graffiti applications interact through a minimal client-side API, which we show admits at least two decentralized implementations. Above the API, we built a Vue.js plugin, which we use to develop applications similar to Twitter, Messenger, and Wikipedia using only client-side code. Our case studies explore how these and other novel applications interoperate, as well as the broader ecosystem that Graffiti enables.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community-Aware Social Community Recommendation</title>
<link>https://arxiv.org/abs/2508.05107</link>
<guid>https://arxiv.org/abs/2508.05107</guid>
<content:encoded><![CDATA[
<div> community recommendation, social recommendation, personalized services, recommender systems, CASO

Summary:
CASO is a novel model designed for social community recommendation, addressing the limitations of existing models in recommending communities. It utilizes three encoders to extract user embeddings and incorporates social network structures and user preferences. CASO introduces a mutual exclusion mechanism to eliminate feature redundancy and includes a community detection loss in the optimization process. Furthermore, extensive experiments on real-world social networks demonstrate CASO's impressive performance in community recommendation compared to nine strong baselines. Overall, CASO shows superior effectiveness in recommending communities by leveraging social network structures and user preferences effectively. <div>
arXiv:2508.05107v1 Announce Type: new 
Abstract: Social recommendation, which seeks to leverage social ties among users to alleviate the sparsity issue of user-item interactions, has emerged as a popular technique for elevating personalized services in recommender systems. Despite being effective, existing social recommendation models are mainly devised for recommending regular items such as blogs, images, and products, and largely fail for community recommendations due to overlooking the unique characteristics of communities. Distinctly, communities are constituted by individuals, who present high dynamicity and relate to rich structural patterns in social networks. To our knowledge, limited research has been devoted to comprehensively exploiting this information for recommending communities.
  To bridge this gap, this paper presents CASO, a novel and effective model specially designed for social community recommendation. Under the hood, CASO harnesses three carefully-crafted encoders for user embedding, wherein two of them extract community-related global and local structures from the social network via social modularity maximization and social closeness aggregation, while the third one captures user preferences using collaborative filtering with observed user-community affiliations. To further eliminate feature redundancy therein, we introduce a mutual exclusion between social and collaborative signals. Finally, CASO includes a community detection loss in the model optimization, thereby producing community-aware embeddings for communities. Our extensive experiments evaluating CASO against nine strong baselines on six real-world social networks demonstrate its consistent and remarkable superiority over the state of the art in terms of community recommendation performance.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling roles and trade-offs in multiplex networks</title>
<link>https://arxiv.org/abs/2508.05488</link>
<guid>https://arxiv.org/abs/2508.05488</guid>
<content:encoded><![CDATA[
<div> social network, multiplex, MLT, roles, interdependence

Summary:<br /><br />This article introduces the Multiplex Latent Trade-off Model (MLT) for analyzing multiplex social networks, considering independence, dependence, and interdependence. The approach identifies roles as trade-offs, taking into account individual attributes, status/resources of others, and mutual influence. By applying the MLT framework to real-world multiplex networks from villages in western Honduras, the study uncovers core social exchange principles and identifies local, layer-specific, and multi-scale communities. The analysis shows that modeling interdependence significantly improves link prediction performance in social networks, implying that social ties are deeply embedded structurally. In contrast, health and economic ties are more influenced by individual status and behaviors. This research provides valuable insights into the structure of human social systems and showcases the complexity of relationships across different layers in multiplex networks. <div>
arXiv:2508.05488v1 Announce Type: new 
Abstract: A multiplex social network captures multiple types of social relations among the same set of people, with each layer representing a distinct type of relationship. Understanding the structure of such systems allows us to identify how social exchanges may be driven by a person's own attributes and actions (independence), the status or resources of others (dependence), and mutual influence between entities (interdependence). Characterizing structure in multiplex networks is challenging, as the distinct layers can reflect different yet complementary roles, with interdependence emerging across multiple scales. Here, we introduce the Multiplex Latent Trade-off Model (MLT), a framework for extracting roles in multiplex social networks that accounts for independence, dependence, and interdependence. MLT defines roles as trade-offs, requiring each node to distribute its source and target roles across layers while simultaneously distributing community memberships within hierarchical, multi-scale structures. Applying the MLT approach to 176 real-world multiplex networks, composed of social, health, and economic layers, from villages in western Honduras, we see core social exchange principles emerging, while also revealing local, layer-specific, and multi-scale communities. Link prediction analyses reveal that modeling interdependence yields the greatest performance gains in the social layer, with subtler effects in health and economic layers. This suggests that social ties are structurally embedded, whereas health and economic ties are primarily shaped by individual status and behavioral engagement. Our findings offer new insights into the structure of human social systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAEx: A Plug-and-Play Framework for Explaining Network Alignment</title>
<link>https://arxiv.org/abs/2508.04731</link>
<guid>https://arxiv.org/abs/2508.04731</guid>
<content:encoded><![CDATA[
<div> Explanations, Network Alignment, Interpretability, Subgraphs, Features <br />
Summary:<br />
The paper introduces NAEx, a framework designed to explain network alignment models by identifying key subgraphs and features that influence predictions. The framework aims to improve the interpretability of alignment decisions, particularly in high-stakes domains where trust is crucial. NAEx addresses the challenge of preserving cross-network dependencies by jointly parameterizing graph structures and feature spaces through learnable edge and feature masks. It also introduces an optimization objective that ensures explanations are faithful to the original predictions and enable meaningful comparisons of structural and feature-based similarities between networks. NAEx is an inductive framework that efficiently generates explanations for previously unseen data. Evaluation metrics tailored to alignment explainability are introduced, and the effectiveness and efficiency of NAEx are demonstrated on benchmark datasets by integrating it with four representative network alignment models. <br /> <div>
arXiv:2508.04731v1 Announce Type: cross 
Abstract: Network alignment (NA) identifies corresponding nodes across multiple networks, with applications in domains like social networks, co-authorship, and biology. Despite advances in alignment models, their interpretability remains limited, making it difficult to understand alignment decisions and posing challenges in building trust, particularly in high-stakes domains. To address this, we introduce NAEx, a plug-and-play, model-agnostic framework that explains alignment models by identifying key subgraphs and features influencing predictions. NAEx addresses the key challenge of preserving the joint cross-network dependencies on alignment decisions by: (1) jointly parameterizing graph structures and feature spaces through learnable edge and feature masks, and (2) introducing an optimization objective that ensures explanations are both faithful to the original predictions and enable meaningful comparisons of structural and feature-based similarities between networks. NAEx is an inductive framework that efficiently generates NA explanations for previously unseen data. We introduce evaluation metrics tailored to alignment explainability and demonstrate NAEx's effectiveness and efficiency on benchmark datasets by integrating it with four representative NA models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration</title>
<link>https://arxiv.org/abs/2508.04780</link>
<guid>https://arxiv.org/abs/2508.04780</guid>
<content:encoded><![CDATA[
<div> hurricanes, power system restoration, equity, predict-then-optimize framework, reinforcement learning<br />
<br />
Summary:<br />
The article discusses the need for an equitable and efficient power system restoration strategy, addressing disparities in request submission volumes from different communities. The proposed Equity-Conformalized Quantile Regression and Spatial-Temporal Attentional RL framework, EPOPR, aims to balance restoration efficiency and equity. EPOPR overcomes challenges such as dataset heteroscedasticity and reinforcement learning agent biases by predicting repair durations and adapting decision-making to varying uncertainty levels across regions. Experimental results demonstrate that EPOPR reduces average power outage duration by 3.60% and decreases inequity between communities by 14.19%, outperforming existing baselines. <div>
arXiv:2508.04780v1 Announce Type: cross 
Abstract: The increasing frequency of extreme weather events, such as hurricanes, highlights the urgent need for efficient and equitable power system restoration. Many electricity providers make restoration decisions primarily based on the volume of power restoration requests from each region. However, our data-driven analysis reveals significant disparities in request submission volume, as disadvantaged communities tend to submit fewer restoration requests. This disparity makes the current restoration solution inequitable, leaving these communities vulnerable to extended power outages. To address this, we aim to propose an equity-aware power restoration strategy that balances both restoration efficiency and equity across communities. However, achieving this goal is challenging for two reasons: the difficulty of predicting repair durations under dataset heteroscedasticity, and the tendency of reinforcement learning agents to favor low-uncertainty actions, which potentially undermine equity. To overcome these challenges, we design a predict-then-optimize framework called EPOPR with two key components: (1) Equity-Conformalized Quantile Regression for uncertainty-aware repair duration prediction, and (2) Spatial-Temporal Attentional RL that adapts to varying uncertainty levels across regions for equitable decision-making. Experimental results show that our EPOPR effectively reduces the average power outage duration by 3.60% and decreases inequity between different communities by 14.19% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing</title>
<link>https://arxiv.org/abs/2508.04811</link>
<guid>https://arxiv.org/abs/2508.04811</guid>
<content:encoded><![CDATA[
<div> passenger fairness, driver preference, ride-hailing system, multi-agent reinforcement learning, system efficiency

Summary:<br /><br />Order dispatch systems in ride-hailing services are crucial for operator revenue, driver profit, and passenger experience. Existing approaches focus on system efficiency, possibly neglecting passenger and driver satisfaction. To address this, a human-centered ride-hailing system, HCRide, is developed. HCRide utilizes a novel multi-agent reinforcement learning algorithm called Habic, which includes a multi-agent competition mechanism, dynamic Actor network, and Bi-Critic network to balance passenger fairness and driver preference while optimizing efficiency. Evaluation using real-world datasets shows that HCRide improves system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% compared to existing methods. <div>
arXiv:2508.04811v1 Announce Type: cross 
Abstract: Order dispatch systems play a vital role in ride-hailing services, which directly influence operator revenue, driver profit, and passenger experience. Most existing work focuses on improving system efficiency in terms of operator revenue, which may cause a bad experience for both passengers and drivers. Hence, in this work, we aim to design a human-centered ride-hailing system by considering both passenger fairness and driver preference without compromising the overall system efficiency. However, it is nontrivial to achieve this target due to the potential conflicts between passenger fairness and driver preference since optimizing one may sacrifice the other. To address this challenge, we design HCRide, a Human-Centered Ride-hailing system based on a novel multi-agent reinforcement learning algorithm called Harmonization-oriented Actor-Bi-Critic (Habic), which includes three major components (i.e., a multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic network) to optimize system efficiency and passenger fairness with driver preference consideration. We extensively evaluate our HCRide using two real-world ride-hailing datasets from Shenzhen and New York City. Experimental results show our HCRide effectively improves system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)</title>
<link>https://arxiv.org/abs/2508.04894</link>
<guid>https://arxiv.org/abs/2508.04894</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, graph-structured data, adversarial attacks, robustness, defense framework 

Summary: 
Large Language Models (LLMs) integrated with graph data are vulnerable to adversarial attacks, including poisoning and evasion attacks. The models LLAGA and GRAPHPROMPTER were analyzed for vulnerabilities, with LLAGA showing susceptibility to malicious node injection while GRAPHPROMPTER demonstrated greater robustness due to its GNN encoder. Both models were found to be susceptible to imperceptible feature perturbations. A new attack surface was discovered for LLAGA where attackers can severely impact performance by injecting malicious nodes. To address these vulnerabilities, an end-to-end defense framework called GALGUARD was proposed. GALGUARD combines an LLM-based feature correction module to mitigate feature-level perturbations and adapted GNN defenses to protect against structural attacks.
<br /><br />Summary: <div>
arXiv:2508.04894v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated with graph-structured data for tasks like node classification, a domain traditionally dominated by Graph Neural Networks (GNNs). While this integration leverages rich relational information to improve task performance, their robustness against adversarial attacks remains unexplored. We take the first step to explore the vulnerabilities of graph-aware LLMs by leveraging existing adversarial attack methods tailored for graph-based models, including those for poisoning (training-time attacks) and evasion (test-time attacks), on two representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al. 2024). Additionally, we discover a new attack surface for LLAGA where an attacker can inject malicious nodes as placeholders into the node sequence template to severely degrade its performance. Our systematic analysis reveals that certain design choices in graph encoding can enhance attack success, with specific findings that: (1) the node sequence template in LLAGA increases its vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater robustness; and (3) both approaches remain susceptible to imperceptible feature perturbation attacks. Finally, we propose an end-to-end defense framework GALGUARD, that combines an LLM-based feature correction module to mitigate feature-level perturbations and adapted GNN defenses to protect against structural attacks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Identification in Peer Effects Estimation</title>
<link>https://arxiv.org/abs/2508.04897</link>
<guid>https://arxiv.org/abs/2508.04897</guid>
<content:encoded><![CDATA[
<div> identification, estimation, network linear-in-means model, peer effects, infill asymptotic setting

Summary:
- The article discusses the network linear-in-means model, which is used to incorporate peer effects in statistical models.
- It explores whether the parameters of the model are reliably estimated in the infill asymptotic setting, where a single network grows in size.
- The study shows that when covariates are i.i.d and the average network degree of nodes increases with the population size, standard estimators can suffer from bias or slow convergence rates due to asymptotic collinearity induced by network averaging.
- Linear-in-sums models, based on aggregate neighborhood characteristics, are proposed as an alternative that does not exhibit these issues as long as network degrees have nontrivial variation.
- Most network models satisfy the condition of nontrivial variation in network degrees, making linear-in-sums models a viable option for reliable estimation in social phenomena analysis.<br /><br />Summary: <div>
arXiv:2508.04897v1 Announce Type: cross 
Abstract: It is commonly accepted that some phenomena are social: for example, individuals' smoking habits often correlate with those of their peers. Such correlations can have a variety of explanations, such as direct contagion or shared socioeconomic circumstances. The network linear-in-means model is a workhorse statistical model which incorporates these peer effects by including average neighborhood characteristics as regressors. Although the model's parameters are identifiable under mild structural conditions on the network, it remains unclear whether identification ensures reliable estimation in the "infill" asymptotic setting, where a single network grows in size. We show that when covariates are i.i.d. and the average network degree of nodes increases with the population size, standard estimators suffer from bias or slow convergence rates due to asymptotic collinearity induced by network averaging. As an alternative, we demonstrate that linear-in-sums models, which are based on aggregate rather than average neighborhood characteristics, do not exhibit such issues as long as the network degrees have some nontrivial variation, a condition satisfied by most network models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Women in Digital Diplomacy: A Multidimensional Framework for Online Gender Bias Against Women Ambassadors Worldwide</title>
<link>https://arxiv.org/abs/2311.17627</link>
<guid>https://arxiv.org/abs/2311.17627</guid>
<content:encoded><![CDATA[
<div> gender bias, women diplomats, social media, online visibility, retweets
Summary:
Women diplomats face online gender bias, with a global study revealing disparities in online visibility. Despite mild gendered language in responses to diplomatic tweets, women ambassadors receive 66.4% fewer retweets than men, highlighting a significant form of gender bias. Negativity in tweets directed at diplomats is not significantly higher for women than for men. The study introduces a unique methodology for analyzing online gender bias, emphasizing the need for further research on bias in international politics. This research sheds light on the challenges faced by women in foreign policy in dealing with online hostility and calls for greater attention to addressing gender disparities in digital diplomacy. Women's visibility on social media remains a crucial area for improvement in promoting gender equality in diplomatic circles. <br /><br />Summary: <div>
arXiv:2311.17627v2 Announce Type: replace 
Abstract: Despite mounting evidence that women in foreign policy often bear the brunt of online hostility, the extent of online gender bias against diplomats remains unexplored. This paper offers the first global analysis of the treatment of women diplomats on social media. Introducing a multidimensional and multilingual methodology for studying online gender bias, it focuses on three critical elements: gendered language, negativity in tweets directed at diplomats, and the visibility of women diplomats. Our unique dataset encompasses ambassadors from 164 countries, their tweets, and the direct responses to these tweets in 65 different languages. Using automated content and sentiment analysis, our findings reveal a crucial gender bias. The language in responses to diplomatic tweets is only mildly gendered and largely pertains to international affairs and, generally, women ambassadors do not receive more negative reactions to their tweets than men, yet the pronounced discrepancy in online visibility stands out as a significant form of gender bias. Women receive a staggering 66.4% fewer retweets than men. By unraveling the invisibility that obscures women diplomats on social media, we hope to spark further research on online bias in international politics.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discovery of Mobility Periodicity for Understanding Urban Transportation Systems</title>
<link>https://arxiv.org/abs/2508.03747</link>
<guid>https://arxiv.org/abs/2508.03747</guid>
<content:encoded><![CDATA[
<div> machine learning, human mobility, periodicity, COVID-19 impact, urban systems <br />
Summary: <br />
This study focuses on uncovering the temporal regularity of human mobility using a sparse identification approach in time series autoregression. By applying this framework to real-world data on metro passenger flow in Hangzhou and ridesharing trips in NYC and Chicago, the researchers were able to identify and quantify significant weekly periodic patterns. The analysis of ridesharing data from 2019 to 2024 revealed the disruptive impact of the COVID-19 pandemic on mobility regularity, with both NYC and Chicago experiencing a reduction in weekly periodicity in 2020. The recovery of mobility regularity in NYC was found to be faster than in Chicago. The interpretability of sparse autoregression provided valuable insights into the underlying temporal patterns of human mobility, highlighting the potential of interpretable machine learning in understanding urban systems. <div>
arXiv:2508.03747v1 Announce Type: new 
Abstract: Uncovering the temporal regularity of human mobility is crucial for discovering urban dynamics and has implications for various decision-making processes and urban system applications. This study formulates the periodicity quantification problem in complex and multidimensional human mobility data as a sparse identification of dominant positive auto-correlations in time series autoregression, allowing one to discover and quantify significant periodic patterns such as weekly periodicity from a data-driven and interpretable machine learning perspective. We apply our framework to real-world human mobility data, including metro passenger flow in Hangzhou, China and ridesharing trips in New York City (NYC) and Chicago, USA, revealing the interpretable weekly periodicity across different spatial locations over past several years. In particular, our analysis of ridesharing data from 2019 to 2024 demonstrates the disruptive impact of the COVID-19 pandemic on mobility regularity and the subsequent recovery trends, highlighting differences in the recovery pattern percentages and speeds between NYC and Chicago. We explore that both NYC and Chicago experienced a remarkable reduction of weekly periodicity in 2020, and the recovery of mobility regularity in NYC is faster than Chicago. The interpretability of sparse autoregression provides insights into the underlying temporal patterns of human mobility, offering a valuable tool for understanding urban systems. Our findings highlight the potential of interpretable machine learning to unlock crucial insights from real-world mobility data.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Stochastic Block Models for Community Detection: The issue of edge-connectivity</title>
<link>https://arxiv.org/abs/2508.03843</link>
<guid>https://arxiv.org/abs/2508.03843</guid>
<content:encoded><![CDATA[
<div> Well-Connected Clusters, Community detection, Graphs, Stochastic Block Models, Connectivity<br />
Summary:<br />
The study focuses on the importance of connectivity in community detection within graphs. Existing community detection methods often produce poorly connected or disconnected communities. A technique called Well-Connected Clusters (WCC) was introduced to address this issue by identifying and removing small edge cuts within clusters. The study evaluated various Stochastic Block Model (SBM) clustering methods and found that all tested methods generated disconnected communities. Graph-tool was shown to perform better than PySBM but still had issues with connectivity due to its description length formula. Modifications to this formula were explored. WCC was proven to enhance accuracy in both flat and nested SBMs and was scalable to networks with millions of nodes. The research provides valuable insights into cluster connectivity and offers a practical solution to improve community detection accuracy. <br /> <div>
arXiv:2508.03843v1 Announce Type: new 
Abstract: A relevant, sometimes overlooked, quality criterion for communities in graphs is that they should be well-connected in addition to being edge-dense. Prior work has shown that leading community detection methods can produce poorly-connected communities, and some even produce internally disconnected communities. A recent study by Park et al. in Complex Networks and their Applications 2024 showed that this problem is evident in clusterings from three Stochastic Block Models (SBMs) in graph-tool, a popular software package. To address this issue, Park et al. presented a simple technique, Well-Connected Clusters (WCC), that repeatedly finds and removes small edge cuts of size at most $\log_{10}n$ in clusters, where $n$ is the number of nodes in the cluster, and showed that treatment of graph-tool SBM clusterings with WCC improves accuracy. Here we examine the question of cluster connectivity for clusterings computed using other SBM software or nested SBMs within graph-tool. Our study, using a wide range of real-world and synthetic networks, shows that all tested SBM clustering methods produce communities that are disconnected, and that graph-tool improves on PySBM. We provide insight into why graph-tool degree-corrected SBM clustering produces disconnected clusters by examining the description length formula it uses, and explore the impact of modifications to the description length formula. Finally, we show that WCC provides an improvement in accuracy for both flat and nested SBMs and establish that it scales to networks with millions of nodes.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical community detection via maximum entropy partitions and the renormalization group</title>
<link>https://arxiv.org/abs/2508.04034</link>
<guid>https://arxiv.org/abs/2508.04034</guid>
<content:encoded><![CDATA[
<div> Hierarchical Clustering Entropy, scales, community structures, network science, dendrograms <br /> 
Summary: <br />
The article introduces Hierarchical Clustering Entropy (HCE), a framework for identifying informative levels in hierarchical community structures in networks. HCE operates on dendrograms without edge-level statistics, selecting resolution levels based on the entropy of community size distribution and number of communities. It works with various clustering algorithms and distance metrics. Evaluation on synthetic benchmarks shows HCE accurately identifies partitions aligned with ground truth, even with noise. Real-world applications in social and neuroscience networks demonstrate HCE's ability to reveal modular hierarchies consistent with existing knowledge. As a scalable and principled method, HCE offers a domain-independent approach to hierarchical community detection with possible applications in biological, social, and technological systems. <br /> <div>
arXiv:2508.04034v1 Announce Type: new 
Abstract: Identifying meaningful structure across multiple scales remains a central challenge in network science. We introduce Hierarchical Clustering Entropy (HCE), a general and model-agnostic framework for detecting informative levels in hierarchical community structures. Unlike existing approaches, HCE operates directly on dendrograms without relying on edge-level statistics. It selects resolution levels that maximize a principled trade-off between the entropy of the community size distribution and the number of communities, corresponding to scales of high structural heterogeneity. This criterion applies to dendrograms produced by a wide range of clustering algorithms and distance metrics, including modularity-based and correlation-based methods. We evaluate HCE on synthetic benchmarks with varying degrees of hierarchy, size imbalance, and noise, including LFR and both symmetric and asymmetric multiscale models, and show that it consistently identifies partitions closely aligned with ground truth. Applied to real-world networks in social and neuroscience systems, HCE reveals interpretable modular hierarchies that align with known structural and functional organizations. As a scalable and principled method, HCE offers a general, domain-independent approach to hierarchical community detection with potential applications across biological, social, and technological systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quasi-Clique Discovery via Energy Diffusion</title>
<link>https://arxiv.org/abs/2508.04174</link>
<guid>https://arxiv.org/abs/2508.04174</guid>
<content:encoded><![CDATA[
<div> Keywords: quasi-clique discovery, graph mining, energy diffusion, dense subgraph, real-world datasets

Summary: 
The paper introduces a new algorithm, EDQC, for discovering quasi-cliques in graphs. Inspired by energy diffusion, EDQC performs stochastic diffusion from source vertices, concentrating energy in structurally cohesive regions and enabling efficient dense subgraph discovery without exhaustive search or dataset-specific tuning. Experimental results on 30 real-world datasets show that EDQC consistently finds larger quasi-cliques than existing methods on most datasets, with lower variance in solution quality. This approach, which incorporates energy diffusion, is a novel contribution to the field of quasi-clique discovery.<br /><br />Summary: <div>
arXiv:2508.04174v1 Announce Type: new 
Abstract: Discovering quasi-cliques -- subgraphs with edge density no less than a given threshold -- is a fundamental task in graph mining, with broad applications in social networks, bioinformatics, and e-commerce. Existing heuristics often rely on greedy rules, similarity measures, or metaheuristic search, but struggle to maintain both efficiency and solution consistency across diverse graphs. This paper introduces EDQC, a novel quasi-clique discovery algorithm inspired by energy diffusion. Instead of explicitly enumerating candidate subgraphs, EDQC performs stochastic energy diffusion from source vertices, naturally concentrating energy within structurally cohesive regions. The approach enables efficient dense subgraph discovery without exhaustive search or dataset-specific tuning. Experimental results on 30 real-world datasets demonstrate that EDQC consistently discovers larger quasi-cliques than state-of-the-art baselines on the majority of datasets, while also yielding lower variance in solution quality. To the best of our knowledge, EDQC is the first method to incorporate energy diffusion into quasi-clique discovery.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tweets vs Pathogen Spread: A Case Study of COVID-19 in American States</title>
<link>https://arxiv.org/abs/2508.04187</link>
<guid>https://arxiv.org/abs/2508.04187</guid>
<content:encoded><![CDATA[
<div> awareness, disease, SIR dynamics, Twitter data, epidemic

Summary:
- The article discusses the mutual influence between awareness and disease, affecting the spread dynamics.
- A null model coupling two SIR dynamics is proposed and analyzed using a mean-field approach.
- Effects of mutual influence on various observables are quantified by exploring the parameter space.
- Empirical analysis of Twitter data related to COVID-19 and confirmed cases in American states is conducted.
- Findings suggest that increasing awareness can suppress the epidemic in specific parameter regions and investigate phase transitions.
- The model can alter the dominant population group by adjusting parameters during the outbreak.
- Parameters assigned to each state based on the model show changes at different pandemic peaks.
- A correlation is observed between states' Twitter activity ranking and the immunity parameters assigned using the model, highlighting the importance of sustained awareness in disease progression.<br /><br />Summary: <div>
arXiv:2508.04187v1 Announce Type: new 
Abstract: The concept of the mutual influence that awareness and disease may exert on each other has recently presented significant challenges. The actions individuals take to prevent contracting a disease and their level of awareness can profoundly affect the dynamics of its spread. Simultaneously, disease outbreaks impact how people become aware. In response, we initially propose a null model that couples two Susceptible-Infectious-Recovered (SIR) dynamics and analyze it using a mean-field approach. Subsequently, we explore the parameter space to quantify the effects of this mutual influence on various observables. Finally, based on this null model, we conduct an empirical analysis of Twitter data related to COVID-19 and confirmed cases within American states. Our findings indicate that in specific regions of the parameter space, it is possible to suppress the epidemic by increasing awareness, and we investigate phase transitions. Furthermore, our model demonstrates the ability to alter the dominant population group by adjusting parameters throughout the course of the outbreak. Additionally, using the model, we assign a set of parameters to each state, revealing that these parameters change at different pandemic peaks. Notably, a robust correlation emerges between the ranking of states' Twitter activity, as gathered from empirical data, and the immunity parameters assigned to each state using our model. This observation underscores the pivotal role of sustained awareness transitioning from the initial to the subsequent peaks in the disease progression.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Representation Learning with Massive Unlabeled Data for Rumor Detection</title>
<link>https://arxiv.org/abs/2508.04252</link>
<guid>https://arxiv.org/abs/2508.04252</guid>
<content:encoded><![CDATA[
<div> detecting rumors, social media, propagation structure learning, large-scale unlabeled datasets, graph representation learning <br />
Summary:<br />
The article discusses the challenges of rumor detection in social media and the importance of effective methods in combating the spread of false information. It highlights the limitations of existing rumor detection methods, particularly the difficulty in obtaining large labeled datasets and the need for generalization on new events. The study addresses these issues by utilizing large-scale unlabeled datasets from social media platforms and incorporating claim propagation structure. By applying graph self-supervised learning methods to improve semantic learning on various topics and leveraging a decade-spanning rumor dataset, the study demonstrates the enhanced performance of general graph methods in rumor detection tasks. The research shows that these methods outperform previous specialized approaches and exhibit strong generalization capabilities, proving valuable in combating the rapid dissemination of rumors on social media platforms. <br /> <div>
arXiv:2508.04252v1 Announce Type: new 
Abstract: With the development of social media, rumors spread quickly, cause great harm to society and economy. Thereby, many effective rumor detection methods have been developed, among which the rumor propagation structure learning based methods are particularly effective compared to other methods. However, the existing methods still suffer from many issues including the difficulty to obtain large-scale labeled rumor datasets, which leads to the low generalization ability and the performance degeneration on new events since rumors are time-critical and usually appear with hot topics or newly emergent events. In order to solve the above problems, in this study, we used large-scale unlabeled topic datasets crawled from the social media platform Weibo and Twitter with claim propagation structure to improve the semantic learning ability of a graph reprentation learing model on various topics. We use three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE in two commonly used training strategies, to verify the performance of general graph semi-supervised methods in rumor detection tasks. In addition, for alleviating the time and topic difference between unlabeled topic data and rumor data, we also collected a rumor dataset covering a variety of topics over a decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our experiments show that these general graph self-supervised learning methods outperform previous methods specifically designed for rumor detection tasks and achieve good performance under few-shot conditions, demonstrating the better generalization ability with the help of our massive unlabeled topic dataset.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assortativity in geometric and scale-free networks</title>
<link>https://arxiv.org/abs/2508.04608</link>
<guid>https://arxiv.org/abs/2508.04608</guid>
<content:encoded><![CDATA[
<div> assortative behavior, network, degree assortativity, real-world networks, generative models <br />
Summary:<br />
This paper investigates degree assortativity in real-world networks and generative models for networks with heavy-tailed degree distributions. It challenges the traditional use of the Pearson assortativity coefficient, showing it does not adequately capture assortativity in networks with heavy-tailed degree distributions. Instead, the study adopts a more detailed approach, examining a variety of conditional and joint weight and degree distributions of connected nodes. The analysis reveals that generative models like Chung-Lu Graphs and Geometric Inhomogeneous Random Graphs are assortativity-neutral, while many real-world networks display assortative behavior. To address this, the study proposes an extension to the GIRG model that allows for tunable assortativity. This new model maintains desired properties related to degree distribution and latent space while also exhibiting assortative behavior. The paper offers mathematical analyses and visualization methods to support these findings. <br /> <div>
arXiv:2508.04608v1 Announce Type: new 
Abstract: The assortative behavior of a network is the tendency of similar (or dissimilar) nodes to connect to each other. This tendency can have an influence on various properties of the network, such as its robustness or the dynamics of spreading processes. In this paper, we study degree assortativity both in real-world networks and in several generative models for networks with heavy-tailed degree distribution based on latent spaces. In particular, we study Chung-Lu Graphs and Geometric Inhomogeneous Random Graphs (GIRGs).
  Previous research on assortativity has primarily focused on measuring the degree assortativity in real-world networks using the Pearson assortativity coefficient, despite reservations against this coefficient. We rigorously confirm these reservations by mathematically proving that the Pearson assortativity coefficient does not measure assortativity in any network with sufficiently heavy-tailed degree distributions, which is typical for real-world networks. Moreover, we find that other single-valued assortativity coefficients also do not sufficiently capture the wiring preferences of nodes, which often vary greatly by node degree. We therefore take a more fine-grained approach, analyzing a wide range of conditional and joint weight and degree distributions of connected nodes, both numerically in real-world networks and mathematically in the generative graph models. We provide several methods of visualizing the results.
  We show that the generative models are assortativity-neutral, while many real-world networks are not. Therefore, we also propose an extension of the GIRG model which retains the manifold desirable properties induced by the degree distribution and the latent space, but also exhibits tunable assortativity. We analyze the resulting model mathematically, and give a fine-grained quantification of its assortativity.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layers of a City: Network-Based Insights into San Diego's Transportation Ecosystem</title>
<link>https://arxiv.org/abs/2508.04694</link>
<guid>https://arxiv.org/abs/2508.04694</guid>
<content:encoded><![CDATA[
<div> Keywords: urban transportation networks, multi-modal analysis, network accessibility, resilience, equity

Summary:
The paper analyzes the urban transportation system of San Diego using network science, constructing a multi-layer graph encompassing driving, walking, and public transit layers. The analysis reveals a core-periphery divide, with significant equity gaps in suburban and rural access to public transit. Centrality analysis highlights the reliance on critical freeways as bottlenecks, indicating low network resilience. The study also finds that San Diego is not a broadly walkable city. Community detection shows that mobility scales differ based on transportation mode, with compact, local clusters for walking and broad, regional clusters for driving. The comprehensive framework presented can guide interventions to improve transportation equity and infrastructure resilience in San Diego.<br /><br />Summary: <div>
arXiv:2508.04694v1 Announce Type: new 
Abstract: Analyzing the structure and function of urban transportation networks is critical for enhancing mobility, equity, and resilience. This paper leverages network science to conduct a multi-modal analysis of San Diego's transportation system. We construct a multi-layer graph using data from OpenStreetMap (OSM) and the San Diego Metropolitan Transit System (MTS), representing driving, walking, and public transit layers. By integrating thousands of Points of Interest (POIs), we analyze network accessibility, structure, and resilience through centrality measures, community detection, and a proposed metric for walkability.
  Our analysis reveals a system defined by a stark core-periphery divide. We find that while the urban core is well-integrated, 30.3% of POIs are isolated from public transit within a walkable distance, indicating significant equity gaps in suburban and rural access. Centrality analysis highlights the driving network's over-reliance on critical freeways as bottlenecks, suggesting low network resilience, while confirming that San Diego is not a broadly walkable city. Furthermore, community detection demonstrates that transportation mode dictates the scale of mobility, producing compact, local clusters for walking and broad, regional clusters for driving. Collectively, this work provides a comprehensive framework for diagnosing urban mobility systems, offering quantitative insights that can inform targeted interventions to improve transportation equity and infrastructure resilience in San Diego.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Social Data-Driven System for Identifying Estate-related Events and Topics</title>
<link>https://arxiv.org/abs/2508.03711</link>
<guid>https://arxiv.org/abs/2508.03711</guid>
<content:encoded><![CDATA[
<div> Keyword: social media, estate-related events, classification, geolocation, urban management
<br />
Summary:
<br />
This study introduces a language model-based system designed to detect and classify estate-related events from social media posts. The system utilizes a hierarchical classification framework to filter and categorize relevant content, enabling the identification of actionable estate-related topics. Moreover, a transformer-based geolocation module is employed to infer posting locations for posts lacking explicit geotags, enhancing the accuracy and usefulness of the data. By integrating these components, the system provides valuable insights for urban management, operational response, and situational awareness. Overall, the system offers a data-driven approach to leveraging social media platforms for monitoring and addressing estate-related issues in urban environments. <div>
arXiv:2508.03711v1 Announce Type: cross 
Abstract: Social media platforms such as Twitter and Facebook have become deeply embedded in our everyday life, offering a dynamic stream of localized news and personal experiences. The ubiquity of these platforms position them as valuable resources for identifying estate-related issues, especially in the context of growing urban populations. In this work, we present a language model-based system for the detection and classification of estate-related events from social media content. Our system employs a hierarchical classification framework to first filter relevant posts and then categorize them into actionable estate-related topics. Additionally, for posts lacking explicit geotags, we apply a transformer-based geolocation module to infer posting locations at the point-of-interest level. This integrated approach supports timely, data-driven insights for urban management, operational response and situational awareness.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Game-Theoretic Framework for Network Formation in Large Populations</title>
<link>https://arxiv.org/abs/2508.03847</link>
<guid>https://arxiv.org/abs/2508.03847</guid>
<content:encoded><![CDATA[
<div> interaction strength, Nash equilibrium, graphon games, stochastic differential equations, uniqueness

Summary:
This paper explores a model of network formation in large populations where agents choose their interaction strength to reach a Nash equilibrium. The model considers the agents' control based on both their own index and the indices of other agents. Special attention is given to a case with piecewise constant graphs, for which optimality conditions are derived through a system of forward-backward stochastic differential equations. The paper establishes results on uniqueness and existence in this context. Numerical experiments are conducted to investigate the impact of different model settings. <div>
arXiv:2508.03847v1 Announce Type: cross 
Abstract: In this paper, we study a model of network formation in large populations. Each agent can choose the strength of interaction (i.e. connection) with other agents to find a Nash equilibrium. Different from the recently-developed theory of graphon games, here each agent's control depends not only on her own index but also on the index of other agents. After defining the general model of the game, we focus on a special case with piecewise constant graphs and we provide optimality conditions through a system of forward-backward stochastic differential equations. Furthermore, we show the uniqueness and existence results. Finally, we provide numerical experiments to discuss the effects of different model settings.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape</title>
<link>https://arxiv.org/abs/2508.04542</link>
<guid>https://arxiv.org/abs/2508.04542</guid>
<content:encoded><![CDATA[
<div> Keywords: personal information, identity theft, fraud cases, privacy risks, graph-based model 

Summary:
By analyzing thousands of identity theft and fraud cases, this research explores the exposure of personal data, frequency of exposures, and consequences. The researchers developed an Identity Ecosystem graph, a model representing relationships between personally identifiable information attributes. Using graph theory and neural networks, they created a privacy risk prediction framework to estimate the likelihood of further disclosures when certain attributes are compromised. The framework effectively determines if the exposure of one identity attribute could lead to the disclosure of another. This comprehensive analysis provides insights into privacy risks and helps individuals and organizations protect personal information more effectively. <div>
arXiv:2508.04542v1 Announce Type: cross 
Abstract: It is difficult for individuals and organizations to protect personal information without a fundamental understanding of relative privacy risks. By analyzing over 5,000 empirical identity theft and fraud cases, this research identifies which types of personal data are exposed, how frequently exposures occur, and what the consequences of those exposures are. We construct an Identity Ecosystem graph--a foundational, graph-based model in which nodes represent personally identifiable information (PII) attributes and edges represent empirical disclosure relationships between them (e.g., the probability that one PII attribute is exposed due to the exposure of another). Leveraging this graph structure, we develop a privacy risk prediction framework that uses graph theory and graph neural networks to estimate the likelihood of further disclosures when certain PII attributes are compromised. The results show that our approach effectively answers the core question: Can the disclosure of a given identity attribute possibly lead to the disclosure of another attribute?
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Patterns in the Blockchain: Analysis of EOAs and Smart Contracts in ERC20 Token Networks</title>
<link>https://arxiv.org/abs/2508.04671</link>
<guid>https://arxiv.org/abs/2508.04671</guid>
<content:encoded><![CDATA[
<div> Keywords: ERC20 tokens, Ethereum blockchain, scaling laws, power law distributions, decentralized systems

Summary:
This study explores the transactional dynamics of ERC20 tokens on the Ethereum blockchain from July 2017 to March 2018. The transactions are categorized into four types based on the interacting addresses. EOA-driven transactions exhibit consistent statistical behavior with stable power law exponents and adherence to Taylor's law. In contrast, interactions involving Smart Contracts (SCs) show sublinear scaling, unstable power-law exponents, and fluctuating Taylor coefficients. SC-driven activity displays heavier-tailed distributions, indicating algorithm-driven behavior. The findings highlight the differences between human-controlled and automated transaction behaviors in blockchain ecosystems. By integrating complex systems theory and blockchain data analytics, this work establishes a framework for understanding decentralized financial systems.<br /><br />Summary: <div>
arXiv:2508.04671v1 Announce Type: cross 
Abstract: Scaling laws offer a powerful lens to understand complex transactional behaviors in decentralized systems. This study reveals distinctive statistical signatures in the transactional dynamics of ERC20 tokens on the Ethereum blockchain by examining over 44 million token transfers between July 2017 and March 2018 (9-month period). Transactions are categorized into four types: EOA--EOA, EOA--SC, SC-EOA, and SC-SC based on whether the interacting addresses are Externally Owned Accounts (EOAs) or Smart Contracts (SCs), and analyzed across three equal periods (each of 3 months). To identify universal statistical patterns, we investigate the presence of two canonical scaling laws: power law distributions and temporal Taylor's law (TL). EOA-driven transactions exhibit consistent statistical behavior, including a near-linear relationship between trade volume and unique partners with stable power law exponents ($\gamma \approx 2.3$), and adherence to TL with scaling coefficients ($\beta \approx 2.3$). In contrast, interactions involving SCs, especially SC-SC, exhibit sublinear scaling, unstable power-law exponents, and significantly fluctuating Taylor coefficients (variation in $\beta$ to be $\Delta\beta = 0.51$). Moreover, SC-driven activity displays heavier-tailed distributions ($\gamma < 2$), indicating bursty and algorithm-driven activity. These findings reveal the characteristic differences between human-controlled and automated transaction behaviors in blockchain ecosystems. By uncovering universal scaling behaviors through the integration of complex systems theory and blockchain data analytics, this work provides a principled framework for understanding the underlying mechanisms of decentralized financial systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Fix Social Media? Testing Prosocial Interventions using Generative Social Simulation</title>
<link>https://arxiv.org/abs/2508.03385</link>
<guid>https://arxiv.org/abs/2508.03385</guid>
<content:encoded><![CDATA[
<div> Keywords: social media platforms, generative social simulation, Large Language Models, political discourse, platform architecture 
Summary: 
Through generative social simulation, researchers investigated the impact of interventions on addressing societal harms associated with social media platforms. The study utilized Agent-Based Models embedded with Large Language Models to create a synthetic platform. The findings revealed three main dysfunctions: partisan echo chambers, concentrated influence among a small elite, and amplification of polarized voices. Six interventions were tested, including chronological feeds and bridging algorithms, with mixed results. The study suggested that core dysfunctions may be influenced by the feedback between reactive engagement and network growth, highlighting the need for rethinking the foundational dynamics of platform architecture to achieve meaningful reform. <div>
arXiv:2508.03385v1 Announce Type: new 
Abstract: Social media platforms have been widely linked to societal harms, including rising polarization and the erosion of constructive debate. Can these problems be mitigated through prosocial interventions? We address this question using a novel method - generative social simulation - that embeds Large Language Models within Agent-Based Models to create socially rich synthetic platforms. We create a minimal platform where agents can post, repost, and follow others. We find that the resulting following-networks reproduce three well-documented dysfunctions: (1) partisan echo chambers; (2) concentrated influence among a small elite; and (3) the amplification of polarized voices - creating a 'social media prism' that distorts political discourse. We test six proposed interventions, from chronological feeds to bridging algorithms, finding only modest improvements - and in some cases, worsened outcomes. These results suggest that core dysfunctions may be rooted in the feedback between reactive engagement and network growth, raising the possibility that meaningful reform will require rethinking the foundational dynamics of platform architecture.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War</title>
<link>https://arxiv.org/abs/2508.03599</link>
<guid>https://arxiv.org/abs/2508.03599</guid>
<content:encoded><![CDATA[
<div> Keywords: Open Source Intelligence, Twitter, Russo-Ukrainian war, misinformation, sentiment analysis

Summary:<br /><br />This paper examines the role of Open Source Intelligence (OSINT) on Twitter during the Russo-Ukrainian war, identifying genuine OSINT and misinformation efforts. Analyzing millions of tweets from over a thousand users, the study reveals a prevailing negative sentiment linked to war events. It uncovers a diverse mix of pro-Ukrainian and pro-Russian partisanship, along with potential strategic manipulation of information. Named Entity Recognition (NER) and community detection techniques help identify clusters related to partisanship, topics, and misinformation. This research sheds light on the complexities of information dissemination within the OSINT community in the context of geopolitical conflicts, providing valuable insights into digital warfare and misinformation dynamics. <div>
arXiv:2508.03599v1 Announce Type: new 
Abstract: This paper examines the role of Open Source Intelligence (OSINT) on Twitter regarding the Russo-Ukrainian war, distinguishing between genuine OSINT and deceptive misinformation efforts, termed "BULLSHINT." Utilizing a dataset spanning from January 2022 to July 2023, we analyze nearly 2 million tweets from approximately 1,040 users involved in discussing real-time military engagements, strategic analyses, and misinformation related to the conflict. Using sentiment analysis, partisanship detection, misinformation identification, and Named Entity Recognition (NER), we uncover communicative patterns and dissemination strategies within the OSINT community. Significant findings reveal a predominant negative sentiment influenced by war events, a nuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the potential strategic manipulation of information. Additionally, we apply community detection techniques, which are able to identify distinct clusters partisanship, topics, and misinformation, highlighting the complex dynamics of information spread on social media. This research contributes to the understanding of digital warfare and misinformation dynamics, offering insights into the operationalization of OSINT in geopolitical conflicts.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Embedding Models on Hyper-relational Knowledge Graph</title>
<link>https://arxiv.org/abs/2508.03280</link>
<guid>https://arxiv.org/abs/2508.03280</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph Embedding, Hyper-relational Knowledge Graphs, Qualifiers, Graph Neural Networks, Performance evaluation 

Summary:
1. Researchers have been exploring Hyper-relational Knowledge Graphs (HKGs) as an extension of traditional Knowledge Graphs.
2. The performance of Hyper-relational Knowledge Graph Embedding (HKGE) models has been attributed to both the base model and the design of extension modules.
3. Conversion of HKGs to KG format using different methods revealed that some classical Knowledge Graph Embedding (KGE) models perform comparably to HKGE models. 
4. Existing HKGE models may struggle with capturing long-range dependencies and integrating main-triple and qualifier information effectively.
5. The FormerGNN framework proposed in this study outperforms existing HKGE models by employing a qualifier integrator, GNN-based graph encoder, and an improved approach for integrating main-triple and qualifier information.  

<br /><br />Summary: <div>
arXiv:2508.03280v1 Announce Type: cross 
Abstract: Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as an extension of traditional Knowledge Graphs (KGs) to better represent real-world facts with additional qualifiers. As a result, researchers have attempted to adapt classical Knowledge Graph Embedding (KGE) models for HKGs by designing extra qualifier processing modules. However, it remains unclear whether the superior performance of Hyper-relational KGE (HKGE) models arises from their base KGE model or the specially designed extension module. Hence, in this paper, we data-wise convert HKGs to KG format using three decomposition methods and then evaluate the performance of several classical KGE models on HKGs. Our results show that some KGE models achieve performance comparable to that of HKGE models. Upon further analysis, we find that the decomposition methods alter the original HKG topology and fail to fully preserve HKG information. Moreover, we observe that current HKGE models are either insufficient in capturing the graph's long-range dependency or struggle to integrate main-triple and qualifier information due to the information compression issue. To further justify our findings and offer a potential direction for future HKGE research, we propose the FormerGNN framework. This framework employs a qualifier integrator to preserve the original HKG topology, and a GNN-based graph encoder to capture the graph's long-range dependencies, followed by an improved approach for integrating main-triple and qualifier information to mitigate compression issues. Our experimental results demonstrate that FormerGNN outperforms existing HKGE models.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations</title>
<link>https://arxiv.org/abs/2508.03420</link>
<guid>https://arxiv.org/abs/2508.03420</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation detection, dynamic environmental representations, LSTM model, continuous dynamics equation, pre-trained dynamics system

Summary:
Misinformation Detection (MD) has become a crucial research topic due to the spread of misinformation on social media platforms. Traditional MD methods assume a static learning paradigm, but in reality, the veracity of news articles can change over time. To address this issue, the proposed framework, MISDER, focuses on learning social environmental representations for different periods and using a temporal model to predict future representations. Three variants of MISDER are introduced: MISDER-LSTM, MISDER-ODE, and MISDER-PT, utilizing LSTM models, continuous dynamics equations, and pre-trained dynamics systems, respectively. Experimental results on two datasets demonstrate the effectiveness of MISDER compared to baseline MD methods. This novel approach considers the dynamic nature of misinformation in evolving social environments, showcasing promising results in detecting misinformation accurately over time.<br /><br />Summary: <div>
arXiv:2508.03420v1 Announce Type: cross 
Abstract: The proliferation of misinformation across diverse social media platforms has drawn significant attention from both academic and industrial communities due to its detrimental effects. Accordingly, automatically distinguishing misinformation, dubbed as Misinformation Detection (MD), has become an increasingly active research topic. The mainstream methods formulate MD as a static learning paradigm, which learns the mapping between the content, links, and propagation of news articles and the corresponding manual veracity labels. However, the static assumption is often violated, since in real-world scenarios, the veracity of news articles may vacillate within the dynamically evolving social environment. To tackle this problem, we propose a novel framework, namely Misinformation detection with Dynamic Environmental Representations (MISDER). The basic idea of MISDER lies in learning a social environmental representation for each period and employing a temporal model to predict the representation for future periods. In this work, we specify the temporal model as the LSTM model, continuous dynamics equation, and pre-trained dynamics system, suggesting three variants of MISDER, namely MISDER-LSTM, MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER, we compare it to various MD baselines across 2 prevalent datasets, and the experimental results can indicate the effectiveness of our proposed model.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterophily-Aware Fair Recommendation using Graph Convolutional Networks</title>
<link>https://arxiv.org/abs/2402.03365</link>
<guid>https://arxiv.org/abs/2402.03365</guid>
<content:encoded><![CDATA[
<div> fair GNN-based recommender system, fairness-aware embeddings, HetroFair, item-side fairness, unfairness, popularity bias

Summary:
HetroFair is a novel fair GNN-based recommender system designed to address unfairness and popularity bias on the item side. It utilizes fairness-aware attention to reduce the impact of nodes' degrees during the normalization process and employs heterophily feature weighting to assign varying weights to different features during aggregation. Experimental results on six real-world datasets demonstrate that HetroFair not only mitigates unfairness and popularity bias but also enhances accuracy on the user side. The proposed system offers a promising approach to improving the fairness and effectiveness of graph neural network-based recommender systems in complex multi-stakeholder environments. The implementation of HetroFair is open-source and available for further research and application. 

Summary: <div>
arXiv:2402.03365v4 Announce Type: replace-cross 
Abstract: In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve end users, but also to benefit other participants, such as items and item providers. These participants may have different or conflicting goals and interests, which raises the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias, and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve item-side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) Fairness-aware attention, which incorporates the dot product in the normalization process of GNNs to decrease the effect of nodes' degrees. ii) Heterophily feature weighting, to assign distinct weights to different features during the aggregation process. To evaluate the effectiveness of HetroFair, we conduct extensive experiments over six real-world datasets. Our experimental results reveal that HetroFair not only alleviates unfairness and popularity bias on the item side but also achieves superior accuracy on the user side. Our implementation is publicly available at https://github.com/NematGH/HetroFair.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Graph Condensation with Evolving Capabilities</title>
<link>https://arxiv.org/abs/2502.17614</link>
<guid>https://arxiv.org/abs/2502.17614</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Condensation, Continual Learning, Evolving Graph Data, Scalability, Clustering<br />
<br />
Summary: <br />
Graph Condensation methods aim to address scalability challenges by learning a smaller graph from a larger one. However, existing approaches struggle with dynamic and evolving graph data. This paper introduces GECC (Graph Evolving Clustering Condensation), a novel framework for continual graph condensation that efficiently updates distilled graphs to handle evolving data streams. GECC utilizes class-wise clustering on aggregated features and can adapt to expanding condensed graphs by inheriting previous results as clustering centroids. The method is theoretically sound and demonstrates superior empirical performance, with experiments showing around 1000$\times$ speedup on large datasets compared to state-of-the-art methods. GECC provides a scalable solution for handling large-scale and evolving graph data efficiently. <br /> <div>
arXiv:2502.17614v2 Announce Type: replace-cross 
Abstract: The rapid growth of graph data creates significant scalability challenges as most graph algorithms scale quadratically with size. To mitigate these issues, Graph Condensation (GC) methods have been proposed to learn a small graph from a larger one, accelerating downstream tasks. However, existing approaches critically assume a static training set, which conflicts with the inherently dynamic and evolving nature of real-world graph data. This work introduces a novel framework for continual graph condensation, enabling efficient updates to the distilled graph that handle data streams without requiring costly retraining. This limitation leads to inefficiencies when condensing growing training sets. In this paper, we introduce GECC (\underline{G}raph \underline{E}volving \underline{C}lustering \underline{C}ondensation), a scalable graph condensation method designed to handle large-scale and evolving graph data. GECC employs a traceable and efficient approach by performing class-wise clustering on aggregated features. Furthermore, it can inherit previous condensation results as clustering centroids when the condensed graph expands, thereby attaining an evolving capability. This methodology is supported by robust theoretical foundations and demonstrates superior empirical performance. Comprehensive experiments including real world scenario show that GECC achieves better performance than most state-of-the-art graph condensation methods while delivering an around 1000$\times$ speedup on large datasets.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Community Spectral Clustering for Geometric Graphs</title>
<link>https://arxiv.org/abs/2508.00893</link>
<guid>https://arxiv.org/abs/2508.00893</guid>
<content:encoded><![CDATA[
<div> Spectral clustering algorithm, community recovery, dense regime, weak consistency, strong consistency <br />
<br />
Summary: 
The paper introduces a spectral clustering algorithm for community recovery in the soft geometric block model (SGBM) with a fixed number of homogeneous communities in the dense regime. The algorithm embeds the graph into $\mathbb{R}^{k-1}$ using eigenvectors associated with specific eigenvalues of the adjacency matrix and applies $k$-means clustering. Weak consistency is proven, and strong consistency is achieved through a local refinement step. The non-standard Davis-Kahan theorem is used to control eigenspace perturbations when eigenvalues are not simple. The limiting spectrum of the adjacency matrix is analyzed through combinatorial and matrix techniques. <div>
arXiv:2508.00893v1 Announce Type: new 
Abstract: In this paper, we consider the soft geometric block model (SGBM) with a fixed number $k \geq 2$ of homogeneous communities in the dense regime, and we introduce a spectral clustering algorithm for community recovery on graphs generated by this model. Given such a graph, the algorithm produces an embedding into $\mathbb{R}^{k-1}$ using the eigenvectors associated with the $k-1$ eigenvalues of the adjacency matrix of the graph that are closest to a value determined by the parameters of the model. It then applies $k$-means clustering to the embedding. We prove weak consistency and show that a simple local refinement step ensures strong consistency. A key ingredient is an application of a non-standard version of Davis-Kahan theorem to control eigenspace perturbations when eigenvalues are not simple. We also analyze the limiting spectrum of the adjacency matrix, using a combination of combinatorial and matrix techniques.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WOCD: A Semi-Supervised Method for Overlapping Community Detection Using Weak Cliques</title>
<link>https://arxiv.org/abs/2508.00927</link>
<guid>https://arxiv.org/abs/2508.00927</guid>
<content:encoded><![CDATA[
<div> community detection, overlapping, graph, deep learning, semi-supervised 

Summary:
- The paper introduces a Weak-clique based Overlapping Community Detection method (WOCD) for identifying overlapping communities in graphs.
- WOCD incorporates prior information and optimizes link information to enhance detection accuracy.
- The method utilizes pseudo-labels within a semi-supervised framework to improve generalization ability and versatility.
- Pseudo-labels are initialized using weak cliques to leverage link and prior information for better accuracy.
- WOCD combines a single-layer Graph Transformer with GNN to achieve significant performance improvements while maintaining efficiency. 

<br /><br />Summary: <div>
arXiv:2508.00927v1 Announce Type: new 
Abstract: Overlapping community detection (OCD) is a fundamental graph data analysis task for extracting graph patterns. Traditional OCD methods can be broadly divided into node clustering and link clustering approaches, both of which rely solely on link information to identify overlapping communities. In recent years, deep learning-based methods have made significant advancements for this task. However, existing GNN-based approaches often face difficulties in effectively integrating link, attribute, and prior information, along with challenges like limited receptive fields and over-smoothing, which hinder their performance on complex overlapping community detection. In this paper, we propose a Weak-clique based Overlapping Community Detection method, namely WOCD, which incorporates prior information and optimizes the use of link information to improve detection accuracy. Specifically, we introduce pseudo-labels within a semi-supervised framework to strengthen the generalization ability, making WOCD more versatile. Furthermore, we initialize pseudo-labels using weak cliques to fully leverage link and prior information, leading to better detection accuracy. Additionally, we employ a single-layer Graph Transformer combined with GNN, which achieves significant performance improvements while maintaining efficiency. We evaluate WOCD on eight real-world attributed datasets, and the results demonstrate that it outperforms the state-of-the-art semi-supervised OCD method by a significant margin in terms of accuracy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Star Network Motifs on X during COVID-19</title>
<link>https://arxiv.org/abs/2508.00975</link>
<guid>https://arxiv.org/abs/2508.00975</guid>
<content:encoded><![CDATA[
<div> Keywords: social network motifs, COVID-19 discourse, star network, bot users, human users

Summary:
In this study, the authors analyze social network motifs, specifically focusing on the simple star network patterns that appear in the discourse surrounding COVID-19 on social media platform X. They examine the different manifestations of the star motif among bot and human users. The analysis reveals six primary patterns of the star motif, distinguished by whether the bots and humans are egos or alters within the network. By detailing the prevalence of these six patterns in the data, the authors illustrate how motif patterns can be used to gain insights into social media behavioral patterns. This research highlights the potential of studying social network motifs to understand communication dynamics during significant events like the COVID-19 pandemic.<br /><br />Summary: <div>
arXiv:2508.00975v1 Announce Type: new 
Abstract: Social network motifs are recurring patterns of small subgraphs that indicate fundamental patterns of social communication. In this work, we study the simple star network motifs that recur on X during the COVID-19 discourse. We study the profile of the manifestation of the star network among bot and human users. There are six primary patterns of the star motif, differentiating by the bots and humans being either egos and alters. We describe the presentation of each of these six patterns in our data, demonstrating how the motif patterns can inform social media behavioral analysis.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM-Powered Social Media Bots Realistic?</title>
<link>https://arxiv.org/abs/2508.00998</link>
<guid>https://arxiv.org/abs/2508.00998</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social media bots, network science, interactions, detection

Summary:
Large Language Models (LLMs) have the potential to be utilized in creating sophisticated social media bots. This study explores the feasibility of generating LLM-Powered social media bot networks through a combination of manual effort, network science, and LLMs. Synthetic bot agent personas, their tweets, and interactions were created to simulate social media networks. A comparison between the generated networks and empirical bot/human data revealed differences in network and linguistic properties of LLM-Powered Bots compared to Wild Bots/Humans. These findings have implications for the detection and effectiveness of LLM-Powered Bots. 

<br /><br />Summary: 
1. Investigates the feasibility of using Large Language Models (LLMs) to power social media bots. 
2. Utilizes manual effort, network science, and LLMs to create synthetic bot personas and simulate social media networks. 
3. Compares generated LLM-Powered bot networks with empirical data to observe differences in properties compared to Wild Bots/Humans. 
4. Highlights implications for the detection and effectiveness of LLM-Powered Bots. <div>
arXiv:2508.00998v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots. This work investigates the realism of generating LLM-Powered social media bot networks. Through a combination of manual effort, network science and LLMs, we create synthetic bot agent personas, their tweets and their interactions, thereby simulating social media networks. We compare the generated networks against empirical bot/human data, observing that both network and linguistic properties of LLM-Powered Bots differ from Wild Bots/Humans. This has implications towards the detection and effectiveness of LLM-Powered Bots.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Prebunking Problem: Optimizing Prebunking Targets to Suppress the Spread of Misinformation in Social Networks</title>
<link>https://arxiv.org/abs/2508.01124</link>
<guid>https://arxiv.org/abs/2508.01124</guid>
<content:encoded><![CDATA[
<div> prebunking, misinformation, social network, combinatorial optimization, approximation algorithm

Summary:
The article introduces prebunking as a preventive intervention to combat misinformation on social media. It aims to enhance cognitive resistance by exposing individuals to weakened misinformation or manipulation techniques before they encounter actual misinformation. The focus is on identifying optimal targets for prebunking interventions in social networks to curb the spread of misinformation. A combinatorial optimization problem, known as the network prebunking problem, is formulated to address this issue. The problem is proven to be NP-hard, and an approximation algorithm, MIA-NPP, based on the Maximum Influence Arborescence approach, is proposed to tackle it. Numerical experiments using real-world social network datasets show that MIA-NPP effectively reduces misinformation spread under different model parameter settings. <div>
arXiv:2508.01124v1 Announce Type: new 
Abstract: As a countermeasure against misinformation that undermines the healthy use of social media, a preventive intervention known as prebunking has recently attracted attention in the field of psychology. Prebunking aims to strengthen individuals' cognitive resistance to misinformation by presenting weakened doses of misinformation or by teaching common manipulation techniques before they encounter actual misinformation. Despite the growing body of evidence supporting its effectiveness in reducing susceptibility to misinformation at the individual level, an important open question remains: how best to identify the optimal targets for prebunking interventions to mitigate the spread of misinformation in a social network. To address this issue, we formulate a combinatorial optimization problem, called the network prebunking problem, to identify optimal prebunking targets for minimizing the spread of misinformation in a social network. We prove that this problem is NP-hard and propose an approximation algorithm, MIA-NPP, based on the Maximum Influence Arborescence (MIA) approach, which restricts influence propagation around each node to a local directed tree rooted at that node. Through numerical experiments using real-world social network datasets, we demonstrate that MIA-NPP effectively suppresses the spread of misinformation under both fully observed and uncertain model parameter settings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shooting the Messenger? Harassment and Hate Speech Directed at Journalists on Social Media</title>
<link>https://arxiv.org/abs/2508.01125</link>
<guid>https://arxiv.org/abs/2508.01125</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, journalists, harassment, hate speech, gender

Summary:
This study examines the presence of harassment and hate speech towards journalists on social networks, particularly women, before and after the 2023 general elections in Spain. The analysis of responses to messages posted by journalists and media outlets on X (formerly Twitter) accounts revealed that insults and political hate were common forms of harassment, with personal accounts being targeted more than institutional ones. Although the total number of harassing messages was similar for men and women, women journalists received a greater number of sexist messages. Additionally, hate speech directed at women journalists had an ideological dimension, particularly from extremists or right-wing populists. The study highlights the role of political polarization in shaping the hostility faced by journalists, especially during election periods. It emphasizes the need for media to implement proactive policies and protective actions at both institutional and individual levels to address this systemic issue. 

<br /><br />Summary: <div>
arXiv:2508.01125v1 Announce Type: new 
Abstract: Journalists have incorporated social networks into their work as a standard tool, enhancing their ability to produce and disseminate information and making it easier for them to connect more directly with their audiences. However, this greater presence in the digital public sphere has also increased their exposure to harassment and hate speech, particularly in the case of women journalists. This study analyzes the presence of harassment and hate speech in responses (n = 60,684) to messages that 200 journalists and media outlets posted on X (formerly Twitter) accounts during the days immediately preceding and following the July 23 (23-J) general elections held in Spain in 2023. The results indicate that the most common forms of harassment were insults and political hate, which were more frequently aimed at personal accounts than institutional ones, highlighting the significant role of political polarization-particularly during election periods-in shaping the hostility that journalists face. Moreover, although, generally speaking, the total number of harassing messages was similar for men and women, it was found that a greater number of sexist messages were aimed at women journalists, and an ideological dimension was identified in the hate speech that extremists or right-wing populists directed at them. This study corroborates that this is a minor but systemic issue, particularly from a political and gender perspective. To counteract this, the media must develop proactive policies and protective actions extending even to the individual level, where this issue usually applies.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective and Efficient Conductance-based Community Search at Billion Scale</title>
<link>https://arxiv.org/abs/2508.01244</link>
<guid>https://arxiv.org/abs/2508.01244</guid>
<content:encoded><![CDATA[
<div> Algorithm, Conductance, Community search, Graph clustering, Semi-supervised

Summary: 
The paper introduces Conductance-based Community Search (CCS), a novel approach to finding a connected subgraph with the lowest conductance that includes a user-specified query vertex. The CCS problem is proven to be NP-hard. The proposed SCCS algorithm addresses this challenge through a four-stage process. First, the graph is reduced using local sampling techniques. Then, a three-stage local optimization strategy is employed to improve community quality. This strategy involves seeding, expansion, and verification stages to enhance internal cohesiveness and external sparsity of the community. Experimental results on real-world and synthetic datasets show the effectiveness, efficiency, and scalability of the proposed approach. <div>
arXiv:2508.01244v1 Announce Type: new 
Abstract: Community search is a widely studied semi-supervised graph clustering problem, retrieving a high-quality connected subgraph containing the user-specified query vertex. However, existing methods primarily focus on cohesiveness within the community but ignore the sparsity outside the community, obtaining sub-par results. Inspired by this, we adopt the well-known conductance metric to measure the quality of a community and introduce a novel problem of conductance-based community search (CCS). CCS aims at finding a subgraph with the smallest conductance among all connected subgraphs that contain the query vertex. We prove that the CCS problem is NP-hard. To efficiently query CCS, a four-stage subgraph-conductance-based community search algorithm, SCCS, is proposed. Specifically, we first greatly reduce the entire graph using local sampling techniques. Then, a three-stage local optimization strategy is employed to continuously refine the community quality. Namely, we first utilize a seeding strategy to obtain an initial community to enhance its internal cohesiveness. Then, we iteratively add qualified vertices in the expansion stage to guarantee the internal cohesiveness and external sparsity of the community. Finally, we gradually remove unqualified vertices during the verification stage. Extensive experiments on real-world datasets containing one billion-scale graph and synthetic datasets show the effectiveness, efficiency, and scalability of our solutions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A graph neural network based on feature network for identifying influential nodes</title>
<link>https://arxiv.org/abs/2508.01278</link>
<guid>https://arxiv.org/abs/2508.01278</guid>
<content:encoded><![CDATA[
<div> Influential nodes, Complex networks, Graph Convolutional Network, Feature Network, Local centralities<br />
Summary:<br />
Identifying influential nodes in complex networks is essential for various applications such as e-commerce and computer information systems. Existing methods have limitations in considering network structure and using global centralities for node features. To address these issues, a Graph Convolutional Network Framework based on Feature Network (FNGCN) is proposed. FNGCN utilizes a feature network to capture relationships among local centralities and determine the most suitable ones. Two FNGCNs, with shallow and deep GCN layers, are developed and compared to state-of-the-art methods using the SIR model. Experimental results on real-world networks demonstrate that the proposed framework effectively identifies influential nodes with higher accuracy. <div>
arXiv:2508.01278v1 Announce Type: new 
Abstract: Identifying influential nodes in complex networks is of great importance, and has many applications in practice. For example, finding influential nodes in e-commerce network can provide merchants with customers with strong purchase intent; identifying influential nodes in computer information system can help locating the components that cause the system break down and identifying influential nodes in these networks can accelerate the flow of information in networks. Thus, a lot of efforts have been made on the problem of indentifying influential nodes. However, previous efforts either consider only one aspect of the network structure, or using global centralities with high time consuming as node features to identify influential nodes, and the existing methods do not consider the relationships between different centralities. To solve these problems, we propose a Graph Convolutional Network Framework based on Feature Network, abbreviated as FNGCN (graph convolutional network is abbreviated as GCN in the following text). Further, to exclude noises and reduce redundency, FNGCN utilizes feature network to represent the complicated relationships among the local centralities, based on which the most suitable local centralities are determined. By taking a shallow GCN and a deep GCN into the FNGCN framework, two FNGCNs are developed. With ground truth obtained from the widely used Susceptible Infected Recovered (SIR) model, the two FNGCNs are compared with the state-of-art methods on several real-world networks. Experimental results show that the two FNGCNs can identify the influential nodes more accurately than the compared methods, indicating that the proposed framework is effective in identifying influential nodes in complex networks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-term resilience of online battle over vaccines and beyond</title>
<link>https://arxiv.org/abs/2508.01398</link>
<guid>https://arxiv.org/abs/2508.01398</guid>
<content:encoded><![CDATA[
<div> Keywords: pro-vaccine science, online competition, Facebook, network architecture, opinion moderation

Summary:
The study evaluates the impact of promoting pro-vaccine science on online platforms, specifically Facebook, before and during the COVID-19 pandemic. By analyzing the interactions of approximately 100 million Facebook Page members across 1,356 interconnected communities, the researchers find that despite significant efforts, the fundamental network structure remains unchanged. The isolation of established expertise and the coexistence of anti-vaccination and mainstream communities persist. This resilience is attributed to the evolution of "glocal" communities that blend various topics and span different scales. To address this issue, the study suggests focusing on network engineering approaches for opinion moderation, rather than content removal, as a more effective strategy. This shift towards structural interventions represents a new paradigm for combating misinformation and promoting pro-vaccine sentiments. <div>
arXiv:2508.01398v1 Announce Type: new 
Abstract: What has been the impact of the enormous amounts of time, effort and money spent promoting pro-vaccine science from pre-COVID-19 to now? We answer this using a unique mapping of online competition between pro- and anti-vaccination views among ~100M Facebook Page members, tracking 1,356 interconnected communities through platform interventions. Remarkably, the network's fundamental architecture shows no change: the isolation of established expertise and the symbiosis of anti and mainstream neutral communities persist. This means that even if the same time, effort and money continue to be spent, nothing will likely change. The reason for this resilience lies in "glocal" evolution: Communities blend multiple topics while bridging neighborhood-level to international scales, creating redundant pathways that transcend categorical targeting. The solution going forward is to focus on the system's network. We show how network engineering approaches can achieve opinion moderation without content removal, representing a paradigm shift from suppression towards structural interventions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Parallel Algorithm for Finding Robust Spanners in Large Social Networks</title>
<link>https://arxiv.org/abs/2508.01485</link>
<guid>https://arxiv.org/abs/2508.01485</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, community structures, structural hole spanners, robust spanners, CUDA implementation

Summary:
Robust spanners (RS) are introduced as nodes in social networks that can efficiently bridge community structures even in the face of disruptions like node or edge removals. A novel scoring technique is proposed to identify RS nodes, which demonstrate a spanning capacity comparable to benchmark spanner detection algorithms while also offering superior robustness. An efficient parallel algorithm with a CUDA implementation is presented for the detection of RS nodes in large networks. Empirical analysis of real-world social networks shows that high-scoring RS nodes exhibit significant robustness and spanning capabilities. The Nvidia GPU implementation of the algorithm achieves an impressive average speedup of 244X over traditional spanner detection techniques, highlighting its effectiveness in identifying RS nodes in large social networks.<br /><br />Summary: Robust spanners (RS) are introduced to bridge community structures in social networks despite disruptions, with a novel scoring technique identifying high-scoring RS nodes. A parallel algorithm with a CUDA implementation achieves high efficiency in detecting RS nodes, which exhibit comparable spanning capacity to benchmark algorithms but offer superior robustness. Evidenced through empirical analysis, the Nvidia GPU implementation of the algorithm demonstrates a significant speedup over traditional techniques, showcasing its efficacy in identifying RS nodes in large social networks. <div>
arXiv:2508.01485v1 Announce Type: new 
Abstract: Social networks, characterized by community structures, often rely on nodes called structural hole spanners to facilitate inter-community information dissemination. However, the dynamic nature of these networks, where spanner nodes may be removed, necessitates resilient methods to maintain inter-community communication. To this end, we introduce robust spanners (RS) as nodes uniquely equipped to bridge communities despite disruptions, such as node or edge removals. We propose a novel scoring technique to identify RS nodes and present a parallel algorithm with a CUDA implementation for efficient RS detection in large networks. Empirical analysis of real-world social networks reveals that high-scoring nodes exhibit a spanning capacity comparable to those identified by benchmark spanner detection algorithms while offering superior robustness. Our implementation on Nvidia GPUs achieves an average speedup of 244X over traditional spanner detection techniques, demonstrating its efficacy to identify RS in large social networks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Media Information Operations</title>
<link>https://arxiv.org/abs/2508.01552</link>
<guid>https://arxiv.org/abs/2508.01552</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, information operations, optimization framework, influence campaigns, generative AI

Summary: 
The tutorial introduces a formal optimization framework for social media information operations (IO) in online social networks. It emphasizes the importance of understanding the terrain, modeling adversaries, and executing interventions for success in shaping opinions through targeted actions. Analytic tools such as centrality measures, clustering algorithms, and sentiment analysis aid in mapping the information environment, identifying influential users, detecting community structures, and gauging public opinion. Threats like coordinated bot networks, extremist recruitment, and viral misinformation are highlighted, with countermeasures ranging from content-level interventions to mathematically optimized influence strategies. The emergence of generative AI transforms both offense and defense in information warfare, demanding algorithmic innovation, policy reform, and ethical vigilance to safeguard the digital public sphere's integrity. <div>
arXiv:2508.01552v1 Announce Type: new 
Abstract: The battlefield of information warfare has moved to online social networks, where influence campaigns operate at unprecedented speed and scale. As with any strategic domain, success requires understanding the terrain, modeling adversaries, and executing interventions. This tutorial introduces a formal optimization framework for social media information operations (IO), where the objective is to shape opinions through targeted actions. This framework is parameterized by quantities such as network structure, user opinions, and activity levels - all of which must be estimated or inferred from data. We discuss analytic tools that support this process, including centrality measures for identifying influential users, clustering algorithms for detecting community structure, and sentiment analysis for gauging public opinion. These tools either feed directly into the optimization pipeline or help defense analysts interpret the information environment. With the landscape mapped, we highlight threats such as coordinated bot networks, extremist recruitment, and viral misinformation. Countermeasures range from content-level interventions to mathematically optimized influence strategies. Finally, the emergence of generative AI transforms both offense and defense, democratizing persuasive capabilities while enabling scalable defenses. This shift calls for algorithmic innovation, policy reform, and ethical vigilance to protect the integrity of our digital public sphere.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Social Media Sentiment for Predictive Algorithmic Trading Strategies</title>
<link>https://arxiv.org/abs/2508.02089</link>
<guid>https://arxiv.org/abs/2508.02089</guid>
<content:encoded><![CDATA[
<div> Keywords: social media sentiment, Reddit comments, investment decisions, BERTweet, Sentiment Volume Change metric

Summary:
This study explores the use of social media sentiment, particularly from Reddit comments, in enhancing investment decisions for higher returns with lower risk. Utilizing BERTweet, over 2 million Reddit comments from the r/wallstreetbets subreddit were analyzed to develop a Sentiment Volume Change (SVC) metric, which combined sentiment and comment volume changes to predict next-day returns more effectively than sentiment alone. Two investment strategies based on SVC were back-tested over four years, outperforming a buy-and-hold strategy significantly. In a bull market, the sentiment-powered strategies achieved 70% higher returns in 2023 and 84.4% higher returns in 2021, while also mitigating losses by 4% in a declining market in 2022. The results highlight that social media sentiment data can be valuable in predicting short-term stock price movements and offer superior risk-adjusted returns compared to traditional market approaches.<br /><br />Summary: <div>
arXiv:2508.02089v1 Announce Type: new 
Abstract: This study investigates how social media sentiment derived from Reddit comments can be used to enhance investment decisions in a way that offers higher returns with lower risk. Using BERTweet we analyzed over 2 million Reddit comments from the subreddit r/wallstreetbets and developed a Sentiment Volume Change (SVC) metric combining sentiment and comment volume changes, which showed significantly improved correlation with next-day returns compared to sentiment alone. We then implemented two different investment strategies that relied solely on SVC to make decisions. Back testing these strategies over four years (2020-2023) our strategies significantly outperformed a comparable buy-and-hold (B&amp;H) strategy in a bull market, achieving 70% higher returns in 2023 and 84.4% higher returns in 2021 while also mitigating losses by 4% in a declining market in 2022. Our results confirm that comment sentiment and volume data derived from Reddit can be effective in predicting short-term stock price movements and sentiment-powered strategies can offer superior risk-adjusted returns as compared to the market, implying that social media sentiment can potentially be a valuable investment tool.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Trust Embeddings from Siamese Trust Scores: A Direct-Sum Approach with Fixed-Point Semantics</title>
<link>https://arxiv.org/abs/2508.01479</link>
<guid>https://arxiv.org/abs/2508.01479</guid>
<content:encoded><![CDATA[
<div> trust embeddings, Siamese trust scores, reconstruction, privacy risk, counter-measures

Summary:
The study focuses on reconstructing high-dimensional trust embeddings from one-dimensional Siamese trust scores in distributed-security frameworks. Through formalizing the estimation task and developing a direct-sum estimator with moment features, the research achieves a unique fixed point for the reconstruction map using Banach theory. Synthetic benchmarks confirm the preservation of inter-device geometry in reconstructed embeddings, even in the presence of noise, with non-asymptotic error bounds providing insight into reconstruction accuracy. Additionally, the paper highlights a privacy risk associated with publishing trust scores, which can leak latent behavioral information. Counter-measures such as score quantization, calibrated noise, and obfuscated embedding spaces are discussed as ways to address this risk within networked AI systems. Availability of datasets, reproduction scripts, and extended proofs ensures the reproducibility of results without requiring proprietary code.<br /><br />Summary: <div>
arXiv:2508.01479v1 Announce Type: cross 
Abstract: We study the inverse problem of reconstructing high-dimensional trust embeddings from the one-dimensional Siamese trust scores that many distributed-security frameworks expose. Starting from two independent agents that publish time-stamped similarity scores for the same set of devices, we formalise the estimation task, derive an explicit direct-sum estimator that concatenates paired score series with four moment features, and prove that the resulting reconstruction map admits a unique fixed point under a contraction argument rooted in Banach theory. A suite of synthetic benchmarks (20 devices x 10 time steps) confirms that, even in the presence of Gaussian noise, the recovered embeddings preserve inter-device geometry as measured by Euclidean and cosine metrics; we complement these experiments with non-asymptotic error bounds that link reconstruction accuracy to score-sequence length. Beyond methodology, the paper demonstrates a practical privacy risk: publishing granular trust scores can leak latent behavioural information about both devices and evaluation models. We therefore discuss counter-measures -- score quantisation, calibrated noise, obfuscated embedding spaces -- and situate them within wider debates on transparency versus confidentiality in networked AI systems. All datasets, reproduction scripts and extended proofs accompany the submission so that results can be verified without proprietary code.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Unlearning via Embedding Reconstruction -- A Range-Null Space Decomposition Approach</title>
<link>https://arxiv.org/abs/2508.02044</link>
<guid>https://arxiv.org/abs/2508.02044</guid>
<content:encoded><![CDATA[
<div> node unlearning, graph unlearning, GNNs, graph structure, GIF

Summary:
Node unlearning is a novel method proposed for Graph Neural Networks (GNNs) to address various requests for graph structure unlearning. The GIF (graph influence function) has been effective in handling partial edge unlearning but struggles with node unlearning. This new approach reverses the aggregation process in GNNs through embedding reconstruction and utilizes Range-Null Space Decomposition for nodes' interaction learning. By avoiding the need for retraining, the model's utility is preserved even after unlearning. Experimental results on different datasets showcase the state-of-the-art performance of this method. <div>
arXiv:2508.02044v1 Announce Type: cross 
Abstract: Graph unlearning is tailored for GNNs to handle widespread and various graph structure unlearning requests, which remain largely unexplored. The GIF (graph influence function) achieves validity under partial edge unlearning, but faces challenges in dealing with more disturbing node unlearning. To avoid the overhead of retraining and realize the model utility of unlearning, we proposed a novel node unlearning method to reverse the process of aggregation in GNN by embedding reconstruction and to adopt Range-Null Space Decomposition for the nodes' interaction learning. Experimental results on multiple representative datasets demonstrate the SOTA performance of our proposed approach.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare</title>
<link>https://arxiv.org/abs/2508.02574</link>
<guid>https://arxiv.org/abs/2508.02574</guid>
<content:encoded><![CDATA[
<div> Dataset, Arabic, Healthcare, Sentiment Analysis, Aspect-based <br />
Summary: 
EHSAN introduces a data-centric hybrid pipeline, merging ChatGPT pseudo-labelling with human review to create an Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label, with ChatGPT-generated rationales for transparency. Three versions of training data were created: fully supervised, semi-supervised, and unsupervised. Transformer models fine-tuned on these datasets showed high accuracy even with minimal human supervision. Reduction of aspect classes improved classification metrics. The study demonstrates an effective approach to Arabic aspect-based sentiment analysis in healthcare by combining large language model annotation with human expertise. Future directions include generalization across hospitals, prompt refinement, and interpretable data-driven modeling.<br /><br /> <div>
arXiv:2508.02574v1 Announce Type: cross 
Abstract: Arabic-language patient feedback remains under-analysed because dialect diversity and scarce aspect-level sentiment labels hinder automated assessment. To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that merges ChatGPT pseudo-labelling with targeted human review to build the first explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label (positive, negative, or neutral), forming a pioneering Arabic dataset aligned with healthcare themes, with ChatGPT-generated rationales provided for each label to enhance transparency. To evaluate the impact of annotation quality on model performance, we created three versions of the training data: a fully supervised set with all labels reviewed by humans, a semi-supervised set with 50% human review, and an unsupervised set with only machine-generated labels. We fine-tuned two transformer models on these datasets for both aspect and sentiment classification. Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels. Reducing the number of aspect classes notably improved classification metrics across the board. These findings demonstrate an effective, scalable approach to Arabic aspect-based sentiment analysis (SA) in healthcare, combining large language model annotation with human expertise to produce a robust and explainable dataset. Future directions include generalisation across hospitals, prompt refinement, and interpretable data-driven modelling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confrontation of capitalism and socialism in Wikipedia networks</title>
<link>https://arxiv.org/abs/2408.07606</link>
<guid>https://arxiv.org/abs/2408.07606</guid>
<content:encoded><![CDATA[
<div> Wikipedia, Ising Network Opinion Formation, opinion polarization, capitalism, socialism<br />
<br />
Summary: 
The article introduces the Ising Network Opinion Formation (INOF) model for analyzing networks of 6 Wikipedia language editions. In this model, Ising spins represent opinions at network nodes/articles and opinion polarization is determined through Monte Carlo iterations. The focus is on the opinion confrontation between capitalism/imperialism and socialism/communism, with the global network opinion favoring socialism and communism across all editions. Opinion preferences for world countries and political leaders are also determined. The model is applied to analyze opinion competition between Christianity and Islam, as well as the USA Democratic and Republican parties, showing promising results. The article suggests that the INOF approach has potential for various applications in directed complex networks.<br /><br />Summary: <div>
arXiv:2408.07606v2 Announce Type: replace 
Abstract: We introduce the Ising Network Opinion Formation (INOF) model and apply it for the analysis of networks of 6 Wikipedia language editions. In the model, Ising spins are placed at network nodes/articles and the steady-state opinion polarization of spins is determined from the Monte Carlo iterations in which a given spin orientation is determined by in-going links from other spins. The main consideration is done for opinion confrontation between {\it capitalism, imperialism} (blue opinion) and {\it socialism, communism} (red opinion). These nodes have fixed spin/opinion orientation while other nodes achieve their steady-state opinions in the process of Monte Carlo iterations. We find that the global network opinion favors {\it socialism, communism} for all 6 editions. The model also determines the opinion preferences for world countries and political leaders, showing good agreement with heuristic expectations. We also present results for opinion competition between {\it Christianity} and {\it Islam}, and USA Democratic and Republican parties. We argue that the INOF approach can find numerous applications for directed complex networks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram</title>
<link>https://arxiv.org/abs/2410.08642</link>
<guid>https://arxiv.org/abs/2410.08642</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal topic modeling, conspiracy theories, social media, textual and visual data, communication<br />
Summary:<br />
This study focuses on using multimodal topic modeling to analyze conspiracy theories in German-language Telegram channels. The researchers analyze textual and visual data from 40,000 Telegram messages in 571 channels known for spreading conspiracy theories. They use BERTopic with CLIP to explore topics across modalities and compare them. The study reveals the diversity of content shared in these channels and proposes a framework to analyze textual and visual discursive strategies in conspiracy theory communication. A case study on the topic group Israel Gaza is included as an example. This research fills a gap in existing literature by expanding beyond memes to analyze visual content and provide a method to compare topic models across modalities. The findings contribute to understanding the communication of conspiracy theories on social media. <br /><br />Summary: <div>
arXiv:2410.08642v3 Announce Type: replace 
Abstract: To address the increasing prevalence of (audio-)visual data on social media, and to capture the evolving and dynamic nature of this communication, researchers have begun to explore the potential of unsupervised approaches for analyzing multimodal online content. However, existing research often neglects visual content beyond memes, and in addition lacks methods to compare topic models across modalities. Our study addresses these gaps by applying multimodal topic modeling for analyzing conspiracy theories in German-language Telegram channels. We use BERTopic with CLIP for the analysis of textual and visual data in a corpus of ~40, 000 Telegram messages posted in October 2023 in 571 German-language Telegram channels known for disseminating conspiracy theories. Through this dataset, we provide insights into unimodal and multimodal topic models by analyzing symmetry and intersections of topics across modalities. We demonstrate the variety of textual and visual content shared in the channels discovered through the topic modeling, and propose a conceptual framework for the analysis of textual and visual discursive strategies in the communication of conspiracy theories. We apply the framework in a case study of the topic group Israel Gaza.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interdisciplinarity Revealed by Transitive Reduction of Citation Networks</title>
<link>https://arxiv.org/abs/1802.06015</link>
<guid>https://arxiv.org/abs/1802.06015</guid>
<content:encoded><![CDATA[
<div> Keywords: transitive reduction, citation networks, interdisciplinary, intradisciplinary, modularity-based clustering

Summary: 
The study focuses on the impact of transitive reduction on citation networks, aiming to distinguish between interdisciplinary and intradisciplinary documents. The hypothesis suggests that documents with minimal citation loss after transitive reduction are likely interdisciplinary, contrasting with those primarily cited within a single discipline. Testing this hypothesis involved an artificial citation network model and data from academic papers, court decisions, and patents. Utilizing modularity-based clustering techniques for topic classification, nodes were categorized as interdisciplinary or intradisciplinary based on cluster-dependent measures. Strong evidence supporting the hypothesis was found in three out of four cases, with patents showing somewhat weaker but still positive support. Overall, the research underscores the role of transitive reduction in identifying interdisciplinary documents within citation networks, offering insights into the interdisciplinary nature of scholarly literature and innovation. 

Summary: <div>
arXiv:1802.06015v2 Announce Type: replace-cross 
Abstract: We investigate the impact of transitive reduction on citation networks. Our hypothesis is that documents which lose fewer citations under transitive reduction are likely to be interdisciplinary, while a large loss of citations suggests a document is primarily cited within a single discipline. We test this hypothesis by using an artificial model of a citation network and by using data on citations from three sources: academic papers, court decisions and patents. Where needed, we applied modularity-based clustering techniques on a network defined using bibliographic coupling to classify documents by topic. A cluster-dependent measure was then used to classify the nodes as interdisciplinary or intradisciplinary. Our results provide strong support for our hypothesis in three of the four cases, with somewhat weaker but still positive support in the case of patents.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Embedding with Completely-imbalanced Labels</title>
<link>https://arxiv.org/abs/2007.03545</link>
<guid>https://arxiv.org/abs/2007.03545</guid>
<content:encoded><![CDATA[
<div> Keywords: Network embedding, semi-supervised learning, imbalanced labels, graph neural networks, node features<br />
Summary:<br />
Semi-supervised network embedding methods are crucial for learning representations of networks, but they struggle in completely imbalanced label settings. To address this, two novel methods are proposed: RSDNE and RECT. RSDNE focuses on maintaining intra-class similarity and inter-class dissimilarity, while RECT utilizes class-semantic knowledge to handle node features and multi-label settings. Experimental results on real-world datasets show the effectiveness of both methods. The code for RECT is available on GitHub for further exploration. <div>
arXiv:2007.03545v2 Announce Type: replace-cross 
Abstract: Network embedding, aiming to project a network into a low-dimensional space, is increasingly becoming a focus of network research. Semi-supervised network embedding takes advantage of labeled data, and has shown promising performance. However, existing semi-supervised methods would get unappealing results in the completely-imbalanced label setting where some classes have no labeled nodes at all. To alleviate this, we propose two novel semi-supervised network embedding methods. The first one is a shallow method named RSDNE. Specifically, to benefit from the completely-imbalanced labels, RSDNE guarantees both intra-class similarity and inter-class dissimilarity in an approximate way. The other method is RECT which is a new class of graph neural networks. Different from RSDNE, to benefit from the completely-imbalanced labels, RECT explores the class-semantic knowledge. This enables RECT to handle networks with node features and multi-label setting. Experimental results on several real-world datasets demonstrate the superiority of the proposed methods. Code is available at https://github.com/zhengwang100/RECT.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Node Duplication Improves Cold-start Link Prediction</title>
<link>https://arxiv.org/abs/2402.09711</link>
<guid>https://arxiv.org/abs/2402.09711</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Link Prediction, Low-degree nodes, NodeDup, Cold-start problem
Summary:
The study focuses on enhancing Graph Neural Networks (GNNs) performance in Link Prediction tasks, particularly on low-degree nodes. Existing GNNs face challenges in producing accurate results for low-degree nodes, crucial for applications like recommendation systems. The proposed augmentation technique, NodeDup, duplicates low-degree nodes and establishes links with their duplicates to improve performance. By adopting a "multi-view" approach for low-degree nodes, NodeDup demonstrates significant performance enhancements for low-degree nodes without compromising high-degree node performance. NodeDup serves as a lightweight and easily applicable module, seamlessly integrating with existing GNNs. Experimental results showcase substantial improvements, with NodeDup achieving remarkable enhancements on isolated, low-degree, and warm nodes compared to conventional GNNs and state-of-the-art cold-start methods. <div>
arXiv:2402.09711v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme. By leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows significant LP performance improvements on low-degree nodes without compromising any performance on high-degree nodes. Additionally, as a plug-and-play augmentation module, NodeDup can be easily applied to existing GNNs with very light computational cost. Extensive experiments show that NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated, low-degree, and warm nodes, respectively, on average across all datasets compared to GNNs and state-of-the-art cold-start methods.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness of graph embedding methods for community detection</title>
<link>https://arxiv.org/abs/2405.00636</link>
<guid>https://arxiv.org/abs/2405.00636</guid>
<content:encoded><![CDATA[
<div> robustness, graph embedding methods, community detection, network perturbations, network structure

Summary:<br />
- Investigates robustness of graph embedding methods for community detection under edge deletions.
- Considers matrix factorization and random walk-based methods.
- Finds varying degrees of robustness within each method family.
- Node2vec and LLE show higher robustness across different scenarios.
- Emphasizes the importance of selecting appropriate graph embedding method based on network characteristics and task requirements.

Summary: <br />
- Investigates robustness of graph embedding methods for community detection under edge deletions.
- Considers matrix factorization and random walk-based methods.
- Finds varying degrees of robustness within each method family.
- Node2vec and LLE show higher robustness across different scenarios.
- Emphasizes the importance of selecting appropriate graph embedding method based on network characteristics and task requirements. <div>
arXiv:2405.00636v3 Announce Type: replace-cross 
Abstract: This study investigates the robustness of graph embedding methods for community detection in the face of network perturbations, specifically edge deletions. Graph embedding techniques, which represent nodes as low-dimensional vectors, are widely used for various graph machine learning tasks due to their ability to capture structural properties of networks effectively. However, the impact of perturbations on the performance of these methods remains relatively understudied. The research considers state-of-the-art graph embedding methods from two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random walk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on both synthetic and real-world networks, the study reveals varying degrees of robustness within each family of graph embedding methods. The robustness is found to be influenced by factors such as network size, initial community partition strength, and the type of perturbation. Notably, node2vec and LLE consistently demonstrate higher robustness for community detection across different scenarios, including networks with degree and community size heterogeneity. These findings highlight the importance of selecting an appropriate graph embedding method based on the specific characteristics of the network and the task at hand, particularly in scenarios where robustness to perturbations is crucial.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Linear and Sinusoidal: Rethinking the Time Encoder in Dynamic Graph Learning</title>
<link>https://arxiv.org/abs/2504.08129</link>
<guid>https://arxiv.org/abs/2504.08129</guid>
<content:encoded><![CDATA[
<div> linear time encoder, temporal relationships, dynamic graph learning, self-attention mechanism, parameter savings
Summary:
This paper presents a study on the effectiveness of linear time encoders compared to sinusoidal time encoders in dynamic graph learning models. The linear time encoder, a simpler alternative, avoids temporal information loss and reduces the need for high-dimensional encoders. By using the self-attention mechanism to compute time spans between events from linear time encodings, relevant temporal patterns can be extracted. Experimental results on six dynamic graph datasets show that the linear time encoder improves the performance of existing models such as TGAT and DyGFormer. Additionally, significant parameter savings can be achieved with the linear time encoder, with minimal loss in performance. This study underscores the advantages of using linear time features in dynamic graph learning architectures, which can benefit various applications such as recommender systems, communication networks, and traffic forecasting.<br /><br />Summary: <div>
arXiv:2504.08129v2 Announce Type: replace-cross 
Abstract: Dynamic graph learning is essential for applications involving temporal networks and requires effective modeling of temporal relationships. Seminal attention-based models like TGAT and DyGFormer rely on sinusoidal time encoders to capture temporal dependencies between edge events. Prior work justified sinusoidal encodings because their inner products depend on the time spans between events, which are crucial features for modeling inter-event relations. However, sinusoidal encodings inherently lose temporal information due to their many-to-one nature and therefore require high dimensions. In this paper, we rigorously study a simpler alternative: the linear time encoder, which avoids temporal information loss caused by sinusoidal functions and reduces the need for high-dimensional time encoders. We show that the self-attention mechanism can effectively learn to compute time spans between events from linear time encodings and extract relevant temporal patterns. Through extensive experiments on six dynamic graph datasets, we demonstrate that the linear time encoder improves the performance of TGAT and DyGFormer in most cases. Moreover, the linear time encoder can lead to significant savings in model parameters with minimal performance loss. For example, compared to a 100-dimensional sinusoidal time encoder, TGAT with a 2-dimensional linear time encoder saves 43% of parameters and achieves higher average precision on five datasets. While both encoders can be used simultaneously, our study highlights the often-overlooked advantages of linear time features in modern dynamic graph models. These findings can positively impact the design choices of various dynamic graph learning architectures and eventually benefit temporal network applications such as recommender systems, communication networks, and traffic forecasting.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Individuals to Crowds: Dual-Level Public Response Prediction in Social Media</title>
<link>https://arxiv.org/abs/2508.00497</link>
<guid>https://arxiv.org/abs/2508.00497</guid>
<content:encoded><![CDATA[
<div> Keywords: Public response prediction, SocialAlign, micro-level personalization, macro-level sentiment distribution, SentiWeibo <br />
<br />
Summary: 
SocialAlign is a new framework for predicting public responses in social contexts. It addresses the limitations of existing works by offering micro-level personalization through specialized modules for content analysis and response generation. At the macro level, SocialAlign models group sentiment distributions to align predictions with real-world sentiment trends. The framework is evaluated using a large-scale dataset called SentiWeibo. Experimental results demonstrate that SocialAlign outperforms strong baselines in terms of accuracy, interpretability, and generalization in public response prediction. This work contributes to advancing research in public response prediction and computational social science. <div>
arXiv:2508.00497v1 Announce Type: new 
Abstract: Public response prediction is critical for understanding how individuals or groups might react to specific events, policies, or social phenomena, making it highly valuable for crisis management, policy-making, and social media analysis. However, existing works face notable limitations. First, they lack micro-level personalization, producing generic responses that ignore individual user preferences. Moreover, they overlook macro-level sentiment distribution and only deal with individual-level sentiment, constraining them from analyzing broader societal trends and group sentiment dynamics. To address these challenges, we propose SocialAlign, a unified framework that predicts real-world responses at both micro and macro levels in social contexts. At the micro level, SocialAlign employs SocialLLM with an articulate Personalized Analyze-Compose LoRA (PAC-LoRA) structure, which deploys specialized expert modules for content analysis and response generation across diverse topics and user profiles, enabling the generation of personalized comments with corresponding sentiments. At the macro level, it models group sentiment distributions and aligns predictions with real-world sentiment trends derived from social media data. To evaluate SocialAlign in real-world scenarios, we introduce SentiWeibo, a large-scale dataset curated from authentic social interactions on the Weibo platform. Experimental results on our SentiWeibo and related LaMP benchmark demonstrate that SocialAlign surpasses strong baselines, showing improved accuracy, interpretability, and generalization in public response prediction. We hope our work inspires further research in public response prediction and computational social science: https://github.com/Znull-1220/SocialAlign.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web</title>
<link>https://arxiv.org/abs/2507.23585</link>
<guid>https://arxiv.org/abs/2507.23585</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithm-driven interfaces, user agency, hypertextual friction, comparative analysis, reclaiming agency

Summary: 
This paper discusses the impact of algorithm-driven interfaces on user agency and introduces the concept of "Hypertextual Friction" as a way to reclaim control in these environments. Through a comparison of real-world interfaces such as Wikipedia and Instagram Explore, as well as Are.na and GenAI image tools, the authors highlight how different systems structure user experience and navigation. Hypertext systems prioritize provenance, associative thinking, and user-driven meaning-making, while algorithmic systems tend to diminish user participation and obscure processes. The study provides a comparative analysis of interface structures in user-driven versus agent-driven systems and proposes hypertextual values as design commitments for increasing agency in a digital landscape dominated by algorithms.

<br /><br />Summary: <div>
arXiv:2507.23585v1 Announce Type: cross 
Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI tools, often prioritize engagement and efficiency at the expense of user agency. As systems take on more decision-making, users have less control over what they see and how meaning or relationships between content are constructed. This paper introduces "Hypertextual Friction," a conceptual design stance that repositions classical hypertext principles--friction, traceability, and structure--as actionable values for reclaiming agency in algorithmically mediated environments. Through a comparative analysis of real-world interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image tools--we examine how different systems structure user experience, navigation, and authorship. We show that hypertext systems emphasize provenance, associative thinking, and user-driven meaning-making, while algorithmic systems tend to obscure process and flatten participation. We contribute: (1) a comparative analysis of how interface structures shape agency in user-driven versus agent-driven systems, and (2) a conceptual stance that offers hypertextual values as design commitments for reclaiming agency in an increasingly algorithmic web.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Bias in Human Mobility is a Universal Phenomenon but is Highly Location-specific</title>
<link>https://arxiv.org/abs/2508.00149</link>
<guid>https://arxiv.org/abs/2508.00149</guid>
<content:encoded><![CDATA[
<div> bias, human mobility data, data production, GPS, prediction

Summary:
- Large-scale human mobility datasets are crucial in many fields but have biases that impact analyses and predictions.
- Data production is studied to understand how individuals are represented in datasets and how much data they produce.
- Analysis of GPS mobility data from smartphones in ten US cities shows inequality in data distribution, with strong effects of wealth, ethnicity, and education.
- Bias is universal in all cities, but each city has its own manifestation of bias, requiring location-specific models.
- This study highlights the need for further research to address biases in human mobility data. 

<br /><br />Summary: <div>
arXiv:2508.00149v1 Announce Type: cross 
Abstract: Large-scale human mobility datasets play increasingly critical roles in many algorithmic systems, business processes and policy decisions. Unfortunately there has been little focus on understanding bias and other fundamental shortcomings of the datasets and how they impact downstream analyses and prediction tasks. In this work, we study `data production', quantifying not only whether individuals are represented in big digital datasets, but also how they are represented in terms of how much data they produce. We study GPS mobility data collected from anonymized smartphones for ten major US cities and find that data points can be more unequally distributed between users than wealth. We build models to predict the number of data points we can expect to be produced by the composition of demographic groups living in census tracts, and find strong effects of wealth, ethnicity, and education on data production. While we find that bias is a universal phenomenon, occurring in all cities, we further find that each city suffers from its own manifestation of it, and that location-specific models are required to model bias for each city. This work raises serious questions about general approaches to debias human mobility data and urges further research.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Dynamic Epidemic Model for Successive Opinion Diffusion in Social Networks</title>
<link>https://arxiv.org/abs/2504.01718</link>
<guid>https://arxiv.org/abs/2504.01718</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic epidemic model, opinion diffusion, social networks, rumor spread, echo chamber effect

Summary: 
This paper presents a novel dynamic epidemic model that addresses successive opinion diffusion in social networks, building upon the SHIMR model. The model incorporates dynamic decision-making influenced by social distances and accounts for accumulative opinion diffusion resulting from interconnected rumors. By considering the impact of rumor spread on social network structures, the model offers valuable insights into phenomena such as the echo chamber effect. Through simulations, the effectiveness of the model in explaining various dynamics of opinion diffusion is validated. The findings provide a deeper understanding of social polarization and network evolution, highlighting the intricate interplay between opinion diffusion and social network dynamics. <div>
arXiv:2504.01718v2 Announce Type: replace 
Abstract: This paper proposes a dynamic epidemic model for successive opinion diffusion in social networks, extending the SHIMR model. It incorporates dynamic decision-making influenced by social distances and captures accumulative opinion diffusion caused by interrelated rumors. The model reflects the impact of rumor spread on social network structures. Simulations validate its effectiveness in explaining phenomena like the echo chamber effect and provide insights into opinion diffusion dynamics, with implications for understanding social polarization and network evolution.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Between the Nodes: Community Discovery Beyond Vectors</title>
<link>https://arxiv.org/abs/2507.22955</link>
<guid>https://arxiv.org/abs/2507.22955</guid>
<content:encoded><![CDATA[
<div> Keyword: Community detection, Social network graphs, Large Language Models, Prompt-based reasoning, Graph-aware strategies

Summary:
Community detection in social network graphs is crucial for understanding group dynamics and information spread. This paper explores the integration of Large Language Models (LLMs) in identifying communities within social graphs. The proposed CommLLM framework combines the GPT-4o model with prompt-based reasoning to incorporate semantic and contextual information. Evaluations on real-world datasets show that LLMs, especially when guided by graph-aware strategies, can effectively detect communities in small to medium-sized graphs. Integration of instruction-tuned models and carefully crafted prompts improves accuracy and coherence of detected communities. These findings emphasize the potential of LLMs in graph-based research and highlight the importance of tailoring model interactions to the structure of graph data. 

<br /><br />Summary: <div>
arXiv:2507.22955v1 Announce Type: new 
Abstract: Community detection in social network graphs plays a vital role in uncovering group dynamics, influence pathways, and the spread of information. Traditional methods focus primarily on graph structural properties, but recent advancements in Large Language Models (LLMs) open up new avenues for integrating semantic and contextual information into this task. In this paper, we present a detailed investigation into how various LLM-based approaches perform in identifying communities within social graphs. We introduce a two-step framework called CommLLM, which leverages the GPT-4o model along with prompt-based reasoning to fuse language model outputs with graph structure. Evaluations are conducted on six real-world social network datasets, measuring performance using key metrics such as Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Variation of Information (VOI), and cluster purity. Our findings reveal that LLMs, particularly when guided by graph-aware strategies, can be successfully applied to community detection tasks in small to medium-sized graphs. We observe that the integration of instruction-tuned models and carefully engineered prompts significantly improves the accuracy and coherence of detected communities. These insights not only highlight the potential of LLMs in graph-based research but also underscore the importance of tailoring model interactions to the specific structure of graph data.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing and Sampling Directed Graphs with Linearly Rescaled Degree Matrices</title>
<link>https://arxiv.org/abs/2507.23025</link>
<guid>https://arxiv.org/abs/2507.23025</guid>
<content:encoded><![CDATA[
<div> framework, directed graphs, sampling algorithm, Joint Degree Matrix, Degree Correlation Matrix
Summary:
The article proposes a new framework for sampling directed graphs to speed up the analysis of large networks. By rescaling the Joint Degree Matrix (JDM) and Degree Correlation Matrix (DCM), a sample graph can be constructed while preserving important graph properties. Experiments on real-world datasets show that the number of non-zero entries in JDM and DCM is small compared to the number of edges and nodes. The proposed algorithm can preserve in-degree and out-degree distributions, as well as joint degree distribution and degree correlation distribution. The algorithm's performance is expected to exceed theoretical expectations due to the negative correlation between deviations and the sparsity of JDM and DCM. The framework offers a promising approach for analyzing large directed networks efficiently.<br /><br />Summary: <div>
arXiv:2507.23025v1 Announce Type: new 
Abstract: In recent years, many large directed networks such as online social networks are collected with the help of powerful data engineering and data storage techniques. Analyses of such networks attract significant attention from both the academics and industries. However, analyses of large directed networks are often time-consuming and expensive because the complexities of a lot of graph algorithms are often polynomial with the size of the graph. Hence, sampling algorithms that can generate graphs preserving properties of original graph are of great importance because they can speed up the analysis process. We propose a promising framework to sample directed graphs: Construct a sample graph with linearly rescaled Joint Degree Matrix (JDM) and Degree Correlation Matrix (DCM). Previous work shows that graphs with the same JDM and DCM will have a range of very similar graph properties. We also conduct experiments on real-world datasets to show that the numbers of non-zero entries in JDM and DCM are quite small compared to the number of edges and nodes. Adopting this framework, we propose a novel graph sampling algorithm that can provably preserves in-degree and out-degree distributions, which are two most fundamental properties of a graph. We also prove the upper bound for deviations in the joint degree distribution and degree correlation distribution, which correspond to JDM and DCM. Besides, we prove that the deviations in these distributions are negatively correlated with the sparsity of the JDM and DCM. Considering that these two matrices are always quite sparse, we believe that proposed algorithm will have a better-than-theory performance on real-world large directed networks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Countering the Forgetting of Novel Health Information with 'Social Boosting'</title>
<link>https://arxiv.org/abs/2507.23148</link>
<guid>https://arxiv.org/abs/2507.23148</guid>
<content:encoded><![CDATA[
<div> Keywords: intervention techniques, social structure, knowledge retention, social interactions, health knowledge 

Summary: 
The study investigates the impact of social structure on the retention of knowledge interventions in isolated Honduran villages. By focusing on maternal and child health care information delivery, the researchers found that individuals with friendship ties within a social network experienced enhanced effectiveness of knowledge interventions. This was attributed to the opportunities for social interactions, such as discussing and reinforcing information with others, leading to deeper cognitive processing and memory retention. The concept of "social boosting" emerged, where well-connected individuals could internalize and retain information better due to increased social interactions. The findings emphasize the significant role of social interactions in reinforcing health knowledge interventions over the long term. This study has implications for health policy, global health workforce, healthcare professionals working with disadvantaged populations, and UN missions addressing infodemics.<br /><br />Summary: <div>
arXiv:2507.23148v1 Announce Type: new 
Abstract: To mitigate the adverse effects of low-quality or false information, studies have shown the effectiveness of various intervention techniques through debunking or so-called pre-bunking. However, the effectiveness of such interventions can decay. Here, we investigate the role of the detailed social structure of the local villages within which the intervened individuals live, which provides opportunities for the targeted individuals to discuss and internalize new knowledge. We evaluated this with respect to a critically important topic, information about maternal and child health care, delivered via a 22-month in-home intervention. Specifically, we examined the effect of having friendship ties on the retention of knowledge interventions among targeted individuals in 110 isolated Honduran villages. We hypothesize that individuals who receive specific knowledge can internalize and consolidate this information by engaging in social interactions where, for instance, they have an opportunity to discuss it with others in the process. The opportunity to explain information to others (knowledge sharing) promotes deeper cognitive processing and elaborative encoding, which ultimately enhances memory retention. We found that well-connected individuals within a social network experience an enhanced effectiveness of knowledge interventions. These individuals may be more likely to internalize and retain the information and reinforce it in others, due to increased opportunities for social interaction where they teach others or learn from them, a mechanism we refer to as "social boosting". These findings underscore the role of social interactions in reinforcing health knowledge interventions over the long term. We believe these findings would be of interest to the health policy, the global health workforce, and healthcare professionals focusing on disadvantaged populations and UN missions on infodemics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical cross-system meta-analysis of long-term transmission grid evolution</title>
<link>https://arxiv.org/abs/2507.23546</link>
<guid>https://arxiv.org/abs/2507.23546</guid>
<content:encoded><![CDATA[
<div> Keywords: grid-side flexibility, transmission network, reconfiguration, real-world grids, empirical studies 

Summary: 
Grid-side flexibility, the ability to reconfigure transmission network topology, is not fully utilized in real-world grids due to limited empirical studies. The potential for grid-side flexibility remains underexplored, hindering its full implementation. The lack of empirical research hinders understanding on how real-world grids evolve over time. By examining real-world grids, insights can be gained into the practical implications of implementing grid-side flexibility. Analyzing the evolution of transmission network topology can provide valuable knowledge for enhancing grid efficiency and resilience. Empirical studies on real-world grids are crucial for unlocking the full potential of grid-side flexibility and optimizing grid operations for the future. 

Summary: <div>
arXiv:2507.23546v1 Announce Type: new 
Abstract: The potential of grid-side flexibility, the latent ability to reconfigure transmission network topology remains under-used partly because of the lack of empirical studies on how real-world grids evolve.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Left-Wing Extremism on the Decentralized Web: An Analysis of Lemmygrad.ml</title>
<link>https://arxiv.org/abs/2507.23699</link>
<guid>https://arxiv.org/abs/2507.23699</guid>
<content:encoded><![CDATA[
<div> extremism, Lemmy, social media, toxicity, authoritarian

Summary:<br />
This study examines left-wing extremism on the Lemmygrad.ml instance of the social media platform Lemmy. The research covers user activity and toxicity levels, particularly after the migration of certain subreddits. It uncovers an increase in user engagement and harmful content, including support for authoritarian regimes and anti-Zionist and antisemitic posts. By using a transformer-based topic modeling approach, the study offers insights into the nature of content on Lemmygrad.ml. The findings highlight the need for a comprehensive analysis of political extremism on decentralized social networks, stressing the importance of studying extremism across the political spectrum. <div>
arXiv:2507.23699v1 Announce Type: new 
Abstract: This study investigates the presence of left-wing extremism on the Lemmygrad.ml instance of the decentralized social media platform Lemmy, from its launch in 2019 up to a month after the bans of the subreddits r/GenZedong and r/GenZhou. We conduct a temporal analysis on Lemmygrad.ml's user activity, with also measuring the degree of highly abusive or hateful content. Furthermore, we explore the content of their posts using a transformer-based topic modeling approach. Our findings reveal a substantial increase in user activity and toxicity levels following the migration of these subreddits to Lemmygrad.ml. We also identify posts that support authoritarian regimes, endorse the Russian invasion of Ukraine, and feature anti-Zionist and antisemitic content. Overall, our findings contribute to a more nuanced understanding of political extremism within decentralized social networks and emphasize the necessity of analyzing both ends of the political spectrum in research.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting stock prices with ChatGPT-annotated Reddit sentiment</title>
<link>https://arxiv.org/abs/2507.22922</link>
<guid>https://arxiv.org/abs/2507.22922</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, sentiment analysis, stock market, predictive power, retail investors

Summary:
- The study examines the relationship between social media sentiment, particularly from Reddit's r/wallstreetbets, and stock market movements for GameStop and AMC Entertainment.
- Three sentiment analysis methods are employed, including a model fine-tuned for interpreting informal language and emojis on social media.
- Surprisingly, social media sentiment shows only a weak correlation with stock prices, with simpler metrics like comment volume and Google search trends offering stronger predictive signals.
- The results suggest that traditional sentiment analysis may not fully capture the complexity of retail investor behavior and market dynamics.
- The study underscores the need to consider various factors beyond sentiment analysis when predicting stock market movements. 

<br /><br />Summary: <div>
arXiv:2507.22922v1 Announce Type: cross 
Abstract: The surge of retail investor activity on social media, exemplified by the 2021 GameStop short squeeze, raised questions about the influence of online sentiment on stock prices. This paper explores whether sentiment derived from social media discussions can meaningfully predict stock market movements. We focus on Reddit's r/wallstreetbets and analyze sentiment related to two companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's role, we employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model designed to better interpret the informal language and emojis prevalent in social media discussions. We use correlation and causality metrics to determine these models' predictive power. Surprisingly, our findings suggest that social media sentiment has only a weak correlation with stock prices. At the same time, simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals. These results highlight the complexity of retail investor behavior and suggest that traditional sentiment analysis may not fully capture the nuances of market-moving online discussions.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection</title>
<link>https://arxiv.org/abs/2507.22930</link>
<guid>https://arxiv.org/abs/2507.22930</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, Personal Information Identifiers (PIIs), synthetic dataset, privacy risks, online social media<br />
Summary:<br />
- The study focuses on the risk of privacy breaches on social platforms like Reddit due to users' self-disclosures of Personal Information Identifiers (PIIs).
- A lack of open-source labeled datasets hinders research into identifying and retrieving PII-revealing text.
- The researchers developed a methodology to create synthetic PII-labeled datasets from large language models to address this issue.
- They created a taxonomy of 19 PII-revealing categories for vulnerable populations.
- The evaluation of the synthetic dataset was based on reproducibility equivalence, unlinkability to original users, and indistinguishability from the original data.
- The dataset and code were released to facilitate further research into PII privacy risks on online social media platforms. <br />Summary: <div>
arXiv:2507.22930v1 Announce Type: cross 
Abstract: Social platforms such as Reddit have a network of communities of shared interests, with a prevalence of posts and comments from which one can infer users' Personal Information Identifiers (PIIs). While such self-disclosures can lead to rewarding social interactions, they pose privacy risks and the threat of online harms. Research into the identification and retrieval of such risky self-disclosures of PIIs is hampered by the lack of open-source labeled datasets. To foster reproducible research into PII-revealing text detection, we develop a novel methodology to create synthetic equivalents of PII-revealing data that can be safely shared. Our contributions include creating a taxonomy of 19 PII-revealing categories for vulnerable populations and the creation and release of a synthetic PII-labeled multi-text span dataset generated from 3 text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and zephyr-7b-beta, with sequential instruction prompting to resemble the original Reddit posts. The utility of our methodology to generate this synthetic dataset is evaluated with three metrics: First, we require reproducibility equivalence, i.e., results from training a model on the synthetic data should be comparable to those obtained by training the same models on the original posts. Second, we require that the synthetic data be unlinkable to the original users, through common mechanisms such as Google Search. Third, we wish to ensure that the synthetic data be indistinguishable from the original, i.e., trained humans should not be able to tell them apart. We release our dataset and code at https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster reproducible research into PII privacy risks in online social media.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion formation in Wikipedia Ising networks</title>
<link>https://arxiv.org/abs/2507.22254</link>
<guid>https://arxiv.org/abs/2507.22254</guid>
<content:encoded><![CDATA[
<div> opinion formation, Wikipedia Ising Networks, competition, political leaders, social concepts

Summary:
The study focuses on opinion formation on Wikipedia Ising Networks, where each node represents a Wikipedia article and links are generated by citations. Ising spins at each node determine their orientation based on a majority vote of connected neighbors, leading to a spin polarized steady-state phase with stable opinion polarization. The model explores competition between political leaders, world countries, and social concepts, using examples like Donald Trump, Vladimir Putin, and Xi Jinping. The approach is extended to three groups with different opinions represented by colors within English, Russian, and Chinese editions of Wikipedia. The research suggests that this Ising Network Opinion Formation model offers a generic description of opinion formation in complex networks. <br /><br />Summary: <div>
arXiv:2507.22254v1 Announce Type: new 
Abstract: We study properties of opinion formation
  on Wikipedia Ising Networks. Each Wikipedia article
  is represented as a node and links are formed by citations of
  one article to another generating a directed network
  of a given language edition with millions of nodes.
  Ising spins are placed at each node
  and their orientation up or down is determined by a majority vote
  of connected neighbors. At the initial stage there are only
  a few nodes from two groups with fixed competing opinions up and down
  while other nodes are assumed to have no initial opinion with no
  effect on the vote. The competition of two opinions is modeled by
  an asynchronous Monte Carlo process converging to a spin polarized
  steady-state phase.
  This phase remains stable with respect to small fluctuations
  induced by an effective temperature of the Monte Carlo process.
  The opinion polarization at the steady-state provides
  opinion (spin) preferences for each node. In the framework of
  this Ising Network
  Opinion Formation model we analyze the influence and competition between
  political leaders, world countries and social concepts.
  This approach is also generalized to the competition between
  three groups of
  different opinions described by three colors, for example
  Donald Trump, Vladimir Putin, Xi Jinping or USA, Russia, China
  within English, Russian and Chinese editions of Wikipedia of March 2025.
  We argue that this approach provides a generic description of
  opinion formation in various complex networks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Influence Maximization on Temporal Networks: A Guide to Make the Best Choice</title>
<link>https://arxiv.org/abs/2507.22589</link>
<guid>https://arxiv.org/abs/2507.22589</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal networks, influence maximization, diffusion models, seed selection, network settings

Summary: 
This article presents a comprehensive guide for selecting the most suitable diffusion model for influence maximization on temporal networks. The categorization of existing models based on underlying mechanisms and effectiveness in different network settings aids in making informed decisions. The analysis of seed selection strategies emphasizes the development of efficient algorithms for finding near-optimal influential node sets. The comparison of key advancements, challenges, and practical applications provides a roadmap for researchers and practitioners to navigate the temporal influence maximization landscape effectively. Overall, this structured guide offers valuable insights into maximizing influence in dynamic communication systems and online social platforms. 

<br /><br />Summary:  <div>
arXiv:2507.22589v1 Announce Type: new 
Abstract: The increasing prominence of temporal networks in online social platforms and dynamic communication systems has made influence maximization a critical research area. Various diffusion models have been proposed to capture the spread of information, yet selecting the most suitable model for a given scenario remains challenging. This article provides a structured guide to making the best choice among diffusion models for influence maximization on temporal networks. We categorize existing models based on their underlying mechanisms and assess their effectiveness in different network settings. We analyze seed selection strategies, highlighting how the inherent properties of influence spread enable the development of efficient algorithms that can find near-optimal sets of influential nodes. By comparing key advancements, challenges, and practical applications, we offer a comprehensive roadmap for researchers and practitioners to navigate the landscape of temporal influence maximization effectively.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility in Epidemic Modeling</title>
<link>https://arxiv.org/abs/2507.22799</link>
<guid>https://arxiv.org/abs/2507.22799</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, epidemic modeling, contact tracing, data-driven methodologies, disease transmission dynamics

Summary: 
This review explores the integration of high-resolution human mobility data into epidemic modeling, highlighting the importance of considering the complex and heterogeneous nature of real-world human interactions. The review covers diverse sources and representations of human mobility data, and examines the behavioral and structural roles of mobility in shaping disease transmission dynamics. It discusses various epidemic modeling approaches, including compartmental models, network-based models, agent-based models, and machine learning models. The review also addresses how integrating mobility data improves risk management and response strategies during epidemics. By synthesizing these insights, the review serves as a foundational resource for researchers and practitioners, bridging the gap between epidemiological theory and the dynamic complexities of human interaction while providing clear directions for future research.

<br /><br />Summary: <div>
arXiv:2507.22799v1 Announce Type: new 
Abstract: Human mobility forms the backbone of contact patterns through which infectious diseases propagate, fundamentally shaping the spatio-temporal dynamics of epidemics and pandemics. While traditional models are often based on the assumption that all individuals have the same probability of infecting every other individual in the population, a so-called random homogeneous mixing, they struggle to capture the complex and heterogeneous nature of real-world human interactions. Recent advancements in data-driven methodologies and computational capabilities have unlocked the potential of integrating high-resolution human mobility data into epidemic modeling, significantly improving the accuracy, timeliness, and applicability of epidemic risk assessment, contact tracing, and intervention strategies. This review provides a comprehensive synthesis of the current landscape in human mobility-informed epidemic modeling. We explore diverse sources and representations of human mobility data, and then examine the behavioral and structural roles of mobility and contact in shaping disease transmission dynamics. Furthermore, the review spans a wide range of epidemic modeling approaches, ranging from classical compartmental models to network-based, agent-based, and machine learning models. And we also discuss how mobility integration enhances risk management and response strategies during epidemics. By synthesizing these insights, the review can serve as a foundational resource for researchers and practitioners, bridging the gap between epidemiological theory and the dynamic complexities of human interaction while charting clear directions for future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The role of media memorability in facilitating startups' access to venture capital funding</title>
<link>https://arxiv.org/abs/2507.22201</link>
<guid>https://arxiv.org/abs/2507.22201</guid>
<content:encoded><![CDATA[
<div> memedia, memorability, venture capital, investment, startup  
Summary:  
- Media memorability, related to imprinting a startup's name in investor memory, influences venture capital investment in startups, with detailed cues such as distinctiveness and connectivity key factors.  
- Venture capitalists consider nuanced aspects of media content, beyond general exposure, when making funding decisions.  
- Data from 197 UK micro and nanotechnology startups funded between 1995 and 2004 supports the significant impact of media memorability on investment outcomes.  
- Startups should focus on strengthening brand memorability through targeted, meaningful media coverage highlighting uniqueness and industry relevance to attract venture capital.  
- The study contributes to understanding how media legitimation influences entrepreneurial finance and emphasizes the importance of strategic media engagement for startups seeking investment.<br /><br /> <div>
arXiv:2507.22201v1 Announce Type: cross 
Abstract: Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From News Source Sharers to Post Viewers: How Topic Diversity and Conspiracy Theories Shape Engagement With Misinformation During a Health Crisis</title>
<link>https://arxiv.org/abs/2401.08832</link>
<guid>https://arxiv.org/abs/2401.08832</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, COVID-19, social media, conspiracy theories, engagement

Summary:
The study examines online engagement with misinformation during the COVID-19 pandemic on social media platform X. False news with conspiracy theories is found to have higher topic diversity than true news. False news also has a longer lifespan and receives more engagement on X, especially when conspiracy theories are involved. News source sharers' engagement is not significantly influenced by topic diversity. In contrast, post viewers engage more with posts that have diverse topics, with misinformation containing conspiracy theories receiving significantly more reposts, likes, and replies. These findings highlight distinct engagement patterns between news source sharers and post viewers on X, providing insights for refining interventions against misinformation at both user levels. <div>
arXiv:2401.08832v3 Announce Type: replace 
Abstract: Online engagement with misinformation threatens societal well-being, particularly during health crises when susceptibility to misinformation is heightened in a multi-topic context. Here, we focus on the COVID-19 pandemic and address a critical gap in understanding engagement with multi-topic misinformation on social media at two user levels: news source sharers (who post news items) and post viewers (who engage with news posts). To this end, we analyze 7273 fact-checked source news items and their associated posts on X through the lens of topic diversity and conspiracy theories. We find that false news, especially those containing conspiracy theories, exhibits higher topic diversity than true news. At news source sharer level, false news has a longer lifetime and receives more posts on X than true news, with conspiracy theories further extending its longevity. However, topic diversity does not significantly influence news source sharers' engagement. At post viewer level, contrary to news source sharer level, posts characterized by heightened topic diversity receive more reposts, likes, and replies. Notably, post viewers tend to engage more with misinformation containing conspiracy narratives: false news posts that contain conspiracy theories, on average, receive 40.8% more reposts, 45.2% more likes, and 44.1% more replies compared to those without conspiracy theories. Our findings suggest that news source sharers and post viewers exhibit distinct engagement patterns on X, offering valuable insights into refining misinformation interventions at these two user levels.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weighted-Median Model of Opinion Dynamics on Networks</title>
<link>https://arxiv.org/abs/2406.17552</link>
<guid>https://arxiv.org/abs/2406.17552</guid>
<content:encoded><![CDATA[
<div> consensus opinion, opinion fragmentation, social network, opinion model, echo chambers  
Summary:  
The study explores how social interactions in a network influence individuals' opinions, leading to either a consensus opinion or opinion fragmentation forming echo chambers. Unlike traditional models that rely on mean opinion, the study considers an update rule based on weighted median opinion for a more realistic approach. Numerical simulations investigate how the limit opinion distribution is impacted by network structure. For configuration-model networks, a mean-field approximation for the opinion distribution dynamics with infinitely many individuals is derived. The research sheds light on the dynamics of opinions in social networks and provides insights into the formation of echo chambers and consensus opinions. <div>
arXiv:2406.17552v2 Announce Type: replace-cross 
Abstract: Social interactions influence people's opinions. In some situations, these interactions result in a consensus opinion; in others, they result in opinion fragmentation and the formation of different opinion groups in the form of "echo chambers". Consider a social network of individuals, who hold continuous-valued scalar opinions and change their opinions when they interact with each other. In such an opinion model, it is common for an opinion-update rule to depend on the mean opinion of interacting individuals. However, we consider an alternative update rule - which may be more realistic in some situations - that instead depends on a weighted median opinion of interacting individuals. Through numerical simulations of our opinion model, we investigate how the limit opinion distribution depends on network structure. For configuration-model networks, we also derive a mean-field approximation for the asymptotic dynamics of the opinion distribution when there are infinitely many individuals in a network.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Half-life of Youtube News Videos: Diffusion Dynamics and Predictive Factors</title>
<link>https://arxiv.org/abs/2507.21187</link>
<guid>https://arxiv.org/abs/2507.21187</guid>
<content:encoded><![CDATA[
<div> diffusion patterns, dispersion rate, 24-hour half-life, prediction models, Explainable AI

Summary:
The study investigates the early-stage diffusion patterns and dispersion rate of news videos on YouTube within the first 24 hours. It analyzes a dataset of over 50,000 videos from 75 countries and six continents, revealing the average 24-hour half-life of YouTube news videos to be around 7 hours, with variability across regions. The research also delves into predicting the 24-hour half-lives of news videos using 6 different models based on statistical and Deep Learning techniques. The performance differences and the importance of video- and channel-related predictors are examined through Explainable AI techniques. The dataset, analysis codebase, and trained models are made publicly available to support further research in this field. 

<br /><br />Summary: <div>
arXiv:2507.21187v1 Announce Type: new 
Abstract: Consumption of YouTube news videos significantly shapes public opinion and political narratives. While prior works have studied the longitudinal dissemination dynamics of YouTube News videos across extended periods, limited attention has been paid to the short-term trends. In this paper, we investigate the early-stage diffusion patterns and dispersion rate of news videos on YouTube, focusing on the first 24 hours. To this end, we introduce and analyze a rich dataset of over 50,000 videos across 75 countries and six continents. We provide the first quantitative evaluation of the 24-hour half-life of YouTube news videos as well as identify their distinct diffusion patterns. According to the findings, the average 24-hour half-life is approximately 7 hours, with substantial variance both within and across countries, ranging from as short as 2 hours to as long as 15 hours. Additionally, we explore the problem of predicting the latency of news videos' 24-hour half-lives. Leveraging the presented datasets, we train and contrast the performance of 6 different models based on statistical as well as Deep Learning techniques. The difference in prediction results across the models is traced and analyzed. Lastly, we investigate the importance of video- and channel-related predictors through Explainable AI (XAI) techniques. The dataset, analysis codebase and the trained models are released at http://bit.ly/3ILvTLU to facilitate further research in this area.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Growing Toxicity Manifests: A Topic Trajectory Analysis of U.S. Immigration Discourse on Social Media</title>
<link>https://arxiv.org/abs/2507.21418</link>
<guid>https://arxiv.org/abs/2507.21418</guid>
<content:encoded><![CDATA[
<div> Keywords: online public sphere, immigration discourse, toxicity, topic discovery, trajectory analysis

Summary: 
The research examines how shifts in toxicity in discussions about U.S. immigration correspond to changes in topics. By analyzing 4 million online posts over six months, the study identifies 157 fine-grained subtopics within the immigration discourse. Users with increasing toxicity tend to adopt alarmist and fear-based perspectives, while those with decreasing toxicity gravitate towards legal and policy-focused themes. These patterns are statistically significantly different from two reference groups with stable toxicity levels. The study employs a novel method that combines hierarchical topic discovery with trajectory analysis to understand dynamic conversations around social issues in a scalable way. This approach can provide valuable insights into understanding polarization and toxicity in online discussions about immigration. 

<br /><br />Summary: <div>
arXiv:2507.21418v1 Announce Type: new 
Abstract: In the online public sphere, discussions about immigration often become increasingly fractious, marked by toxic language and polarization. Drawing on 4 million X posts over six months, we combine a user- and topic-centric approach to study how shifts in toxicity manifest as topical shifts. Our topic discovery method, which leverages instruction-based embeddings and recursive HDBSCAN, uncovers 157 fine-grained subtopics within the U.S. immigration discourse. We focus on users in four groups: (1) those with increasing toxicity, (2) those with decreasing toxicity, and two reference groups with no significant toxicity trend but matched toxicity levels. Treating each posting history as a trajectory through a five-dimensional topic space, we compare average group trajectories using permutational MANOVA. Our findings show that users with increasing toxicity drift toward alarmist, fear-based frames, whereas those with decreasing toxicity pivot toward legal and policy-focused themes. Both patterns diverge statistically significantly from their reference groups. This pipeline, which combines hierarchical topic discovery with trajectory analysis, offers a replicable method for studying dynamic conversations around social issues at scale.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation</title>
<link>https://arxiv.org/abs/2507.21903</link>
<guid>https://arxiv.org/abs/2507.21903</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Graphical methods, news summarization, stakeholders, events<br />
Summary:<br />
- Existing news summarization methods focusing only on textual content of articles, lacking analysis on parties involved.
- SUnSET framework introduced for Timeline Summarization, leveraging Large Language Models to build SET triplets.
- Stakeholder-based ranking used to construct Relevancy metric, outperforming prior baselines and becoming State-of-the-Art.
- Importance of stakeholders highlighted in news articles, impacting the understanding of events.
- SUnSET provides a novel approach to tracking events across sources, considering parties involved and connection of related events. <br />
Summary: <div>
arXiv:2507.21903v1 Announce Type: new 
Abstract: As news reporting becomes increasingly global and decentralized online, tracking related events across multiple sources presents significant challenges. Existing news summarization methods typically utilizes Large Language Models and Graphical methods on article-based summaries. However, this is not effective since it only considers the textual content of similarly dated articles to understand the gist of the event. To counteract the lack of analysis on the parties involved, it is essential to come up with a novel framework to gauge the importance of stakeholders and the connection of related events through the relevant entities involved. Therefore, we present SUnSET: Synergistic Understanding of Stakeholder, Events and Time for the task of Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs) to build SET triplets and introduced the use of stakeholder-based ranking to construct a $Relevancy$ metric, which can be extended into general situations. Our experimental results outperform all prior baselines and emerged as the new State-of-the-Art, highlighting the impact of stakeholder information within news article.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models</title>
<link>https://arxiv.org/abs/2507.21055</link>
<guid>https://arxiv.org/abs/2507.21055</guid>
<content:encoded><![CDATA[
<div> Keywords: news media, comprehension gaps, large language models, agent-based framework, diverse audiences

Summary:
Large language models (LLMs) are used in an agent-based framework to address comprehension gaps in news media among diverse audiences. The framework simulates communication behaviors among agents representing experts from various occupations or different age groups. Through iterative discussions, the framework identifies confusion and misunderstandings in news content for the agents. Supplemental materials specific to these gaps are then designed and provided to the agents, leading to significantly improved news comprehension. This study demonstrates the utility and efficiency of the framework in enhancing news understanding for audiences with varying levels of expertise and age. <div>
arXiv:2507.21055v1 Announce Type: cross 
Abstract: In the interconnected world, news media are critical in conveying information to public across diverse domains including technology, finance, and agriculture. Journalists make efforts to present accurate information, however, the interpretation of news often varies significantly among different audiences due to their specific expertise and age. In this work, we investigate how to identify these comprehension gaps and provide solutions to improve audiences understanding of news content, particular to the aspects of articles outside their primary domains of knowledge. We propose a agent-based framework using large language models (LLMs) to simulate society communication behaviors, where several agents can discuss news. These agents can be designed to be experts from various occupation, or from different age group. Our results indicate that this framework can identify confusions or even misunderstanding of news for the agent through the iterative discussion process. Based on these accurate identification, the framework can design a supplement material specific to these agents on the news. Our results show that agents exhibit significantly improved news understanding after receiving this material. These findings highlight our framework's utility and efficiency in enhancing news comprehension for diverse audiences by directly addressing their understanding gap.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Data Retrieval and Comparative Bias Analysis of Recommendation Algorithms for YouTube Shorts and Long-Form Videos</title>
<link>https://arxiv.org/abs/2507.21467</link>
<guid>https://arxiv.org/abs/2507.21467</guid>
<content:encoded><![CDATA[
<div> algorithm, recommendation, YouTube, bias, content diversity

Summary:
This study explores the impact of recommendation algorithms on user engagement and content diversity on YouTube, focusing on short-form and long-form videos. By developing a novel data collection framework, the researchers analyze the distinct behavioral patterns of the algorithms in recommending content. They find that short-form videos lead to quicker shifts towards engaging but less diverse content compared to long-form videos. Moreover, the study investigates biases in politically sensitive topics like the South China Sea dispute, shedding light on how these algorithms shape narratives and amplify specific viewpoints. The research emphasizes the importance of responsible AI practices in creating equitable and transparent recommendation systems to address concerns about biases, echo chambers, and content diversity in digital media platforms. This study provides actionable insights for designing recommendation algorithms that promote fairness and transparency, highlighting the need for responsible AI practices in the evolving digital landscape. 

<br /><br />Summary: <div>
arXiv:2507.21467v1 Announce Type: cross 
Abstract: The growing popularity of short-form video content, such as YouTube Shorts, has transformed user engagement on digital platforms, raising critical questions about the role of recommendation algorithms in shaping user experiences. These algorithms significantly influence content consumption, yet concerns about biases, echo chambers, and content diversity persist. This study develops an efficient data collection framework to analyze YouTube's recommendation algorithms for both short-form and long-form videos, employing parallel computing and advanced scraping techniques to overcome limitations of YouTube's API. The analysis uncovers distinct behavioral patterns in recommendation algorithms across the two formats, with short-form videos showing a more immediate shift toward engaging yet less diverse content compared to long-form videos. Furthermore, a novel investigation into biases in politically sensitive topics, such as the South China Sea dispute, highlights the role of these algorithms in shaping narratives and amplifying specific viewpoints. By providing actionable insights for designing equitable and transparent recommendation systems, this research underscores the importance of responsible AI practices in the evolving digital media landscape.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Tight Bounds for Estimating Degree Distribution in Streaming and Query Models</title>
<link>https://arxiv.org/abs/2507.21784</link>
<guid>https://arxiv.org/abs/2507.21784</guid>
<content:encoded><![CDATA[
<div> degree distribution, complementary cumulative degree histogram, approximation algorithm, lower bounds, sublinear models

Summary:
An algorithm is proposed to approximate the complementary cumulative degree histogram (ccdh) of a graph by obtaining suitable vertex and edge samples, independent of any sublinear model. Efficient methods for obtaining these samples in streaming and query models are discussed. The complexity of the problem across sublinear models is nearly settled with the first lower bounds established in both query and streaming scenarios. This work addresses an open problem posed in a previous conference, providing insights into approximating ccdh and advancing the understanding of graph structure analysis. <div>
arXiv:2507.21784v1 Announce Type: cross 
Abstract: The degree distribution of a graph $G=(V,E)$, $|V|=n$, $|E|=m$ is one of the most fundamental objects of study in the analysis of graphs as it embodies relationship among entities. In particular, an important derived distribution from degree distribution is the complementary cumulative degree histogram (ccdh). The ccdh is a fundamental summary of graph structure, capturing, for each threshold $d$, the number of vertices with degree at least $d$. For approximating ccdh, we consider the $(\varepsilon_D,\varepsilon_R)$-BiCriteria Multiplicative Approximation, which allows for controlled multiplicative slack in both the domain and the range. The exact complexity of the problem was not known and had been posed as an open problem in WOLA 2019 [Sublinear.info, Problem 98].
  In this work, we first design an algorithm that can approximate ccdh if a suitable vertex sample and an edge sample can be obtained and thus, the algorithm is independent of any sublinear model. Next, we show that in the streaming and query models, these samples can be obtained efficiently. On the other end, we establish the first lower bounds for this problem in both query and streaming models, and (almost) settle the complexity of the problem across both the sublinear models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Misconceptions in Social Bots Research</title>
<link>https://arxiv.org/abs/2303.17251</link>
<guid>https://arxiv.org/abs/2303.17251</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots research, biases, misconceptions, online manipulation, methodological issues <br />
Summary: This article discusses the prevalent biases, hyped results, and misconceptions that plague social bots research, leading to ambiguities and unrealistic expectations. It highlights the need for rigorous, unbiased, and responsible discussions on online disinformation and manipulation. The analysis addresses methodological and conceptual issues affecting current research and refutes common fallacious arguments used by proponents and opponents alike. By demystifying misconceptions and providing guidelines for future research, the article aims to ensure reliable solutions and reaffirm the validity of the scientific method. <div>
arXiv:2303.17251v4 Announce Type: replace 
Abstract: Research on social bots aims at advancing knowledge and providing solutions to one of the most debated forms of online manipulation. Yet, social bot research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. Here, we discuss a broad set of consequential methodological and conceptual issues that affect current social bots research, illustrating each with examples drawn from recent studies. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss research about online disinformation and manipulation in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research, as well as providing directions toward sound methodologies for future research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</title>
<link>https://arxiv.org/abs/2503.20797</link>
<guid>https://arxiv.org/abs/2503.20797</guid>
<content:encoded><![CDATA[
<div> social media, ideology classification, Large Language Models, in-context learning, metadata

Summary:
Large Language Models (LLMs) show promise in classifying political ideology in online content within the context of the two-party US political spectrum through in-context learning (ICL). The study conducted experiments on news articles and YouTube videos, demonstrating that the approach outperforms zero-shot and traditional supervised methods. The influence of metadata, such as content source and descriptions, on ideological classification was also evaluated. Providing source information for political and non-political content was found to impact the LLM's classification accuracy. The research addresses concerns of radicalization, filter bubbles, and content bias in the rapidly expanding social media landscape. <div>
arXiv:2503.20797v2 Announce Type: replace-cross 
Abstract: The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Green building blocks reveal the complex anatomy of climate change mitigation technologies</title>
<link>https://arxiv.org/abs/2504.06834</link>
<guid>https://arxiv.org/abs/2504.06834</guid>
<content:encoded><![CDATA[
<div> Keywords: Green Building Blocks, innovation, climate change, technology, international collaboration

Summary: 
The study focuses on identifying "Green Building Blocks" (GBBs) as modular components that can be incorporated into existing technologies to reduce their carbon footprint, thus facilitating the transition to net-zero emissions. By comparing green and nongreen patents, the researchers construct a network that connects nongreen technologies to GBBs, highlighting areas with varying potential for climate-change mitigating innovation. The unequal distribution of node degrees in the network underscores the differing opportunities for innovation across domains. Additionally, the study reveals the importance of international collaboration in driving innovation, with a significant proportion of firms in the US, Germany, and China relying on foreign partners for optimal development of green technologies. The findings emphasize the critical role of global cooperation in advancing sustainable technological solutions and warn against the risks of economic nationalism hindering progress towards achieving climate goals. 

<br /><br />Summary: <div>
arXiv:2504.06834v2 Announce Type: replace-cross 
Abstract: Achieving net-zero emissions requires rapid innovation, yet the necessary technological knowhow is scattered across industries and countries. Comparing functionally similar green and nongreen patents, we identify "Green Building Blocks" (GBBs): modular components that can be added to reduce existing technologies' carbon footprints. These GBBs depict the anatomy of the green transition as a network that connects problems -- nongreen technologies -- to GBBs that mitigate their climate-change impact. Node degrees in this network are highly unequal, showing that the scope for climate-change mitigating innovation varies substantially across domains. The network also helps predict which green technologies firms develop themselves, and which alliances they form to do so. This reveals a critical dependence on international collaboration: optimal innovation partners for 84% of US, 87% of German, and 92% of Chinese firms are foreign, providing quantitative evidence that rising economic nationalism threatens the pace of innovation required to meet global climate goals.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dual Personas of Social Media Bots</title>
<link>https://arxiv.org/abs/2504.12498</link>
<guid>https://arxiv.org/abs/2504.12498</guid>
<content:encoded><![CDATA[
<div> Keywords: social media bots, personas, bot detection regulation, good-bad duality, metrics

Summary:
Social media bots, AI agents that engage in online interactions, have various personas tailored for specific behaviors or content types. This article introduces fifteen agent personas categorized into Content-Based and Behavior-Based Bot Personas. The bots can serve both positive and negative purposes, challenging the common perception that all bots are malicious. To better understand bot behavior, metrics for assessing the good and bad aspects of bot agents are outlined. The research highlights the importance of considering how bots are utilized rather than labeling them universally as harmful. This guideline aims to inform bot detection regulation and underscores the need for nuanced approaches to assessing and addressing the impacts of social media bots. <div>
arXiv:2504.12498v2 Announce Type: replace-cross 
Abstract: Social media bots are AI agents that participate in online conversations. Most studies focus on the general bot and the malicious nature of these agents. However, bots have many different personas, each specialized towards a specific behavioral or content trait. Neither are bots singularly bad, because they are used for both good and bad information dissemination. In this article, we introduce fifteen agent personas of social media bots. These personas have two main categories: Content-Based Bot Persona and Behavior-Based Bot Persona. We also form yardsticks of the good-bad duality of the bots, elaborating on metrics of good and bad bot agents. Our work puts forth a guideline to inform bot detection regulation, emphasizing that policies should focus on how these agents are employed, rather than collectively terming bot agents as bad.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Deep Learning-based Model for Ranking Influential Nodes in Complex Networks</title>
<link>https://arxiv.org/abs/2507.19702</link>
<guid>https://arxiv.org/abs/2507.19702</guid>
<content:encoded><![CDATA[
<div> 1D-CGS; influential nodes; complex networks; node ranking; deep learning
<br />
<br />
Summary: 
The article introduces 1D-CGS, a hybrid model combining 1D convolutional neural networks (1D-CNN) with GraphSAGE for efficient node ranking in complex networks. It utilizes node degree and average neighbor degree as input features, processed through 1D convolutions and GraphSAGE layers. The model is trained on synthetic networks and tested on real-world networks, outperforming traditional centrality measures and deep learning models in accuracy while maintaining fast runtime. Results show a substantial improvement in ranking accuracy compared to baselines, with high correlation and Jaccard Similarity scores. The model exhibits unique and discriminative rankings with near-perfect rank distributions and high Monotonicity Index scores. Importantly, 1D-CGS operates in a highly reasonable time frame, making it suitable for large-scale applications. <div>
arXiv:2507.19702v1 Announce Type: new 
Abstract: Identifying influential nodes in complex networks is a critical task with a wide range of applications across different domains. However, existing approaches often face trade-offs between accuracy and computational efficiency. To address these challenges, we propose 1D-CGS, a lightweight and effective hybrid model that integrates the speed of one-dimensional convolutional neural networks (1D-CNN) with the topological representation power of GraphSAGE for efficient node ranking. The model uses a lightweight input representation built on two straightforward and significant topological features: node degree and average neighbor degree. These features are processed through 1D convolutions to extract local patterns, followed by GraphSAGE layers to aggregate neighborhood information. We formulate the node ranking task as a regression problem and use the Susceptible-Infected-Recovered (SIR) model to generate ground truth influence scores. 1D-CGS is initially trained on synthetic networks generated by the Barabasi-Albert model and then applied to real world networks for identifying influential nodes. Experimental evaluations on twelve real world networks demonstrate that 1D-CGS significantly outperforms traditional centrality measures and recent deep learning models in ranking accuracy, while operating in very fast runtime. The proposed model achieves an average improvement of 4.73% in Kendall's Tau correlation and 7.67% in Jaccard Similarity over the best performing deep learning baselines. It also achieves an average Monotonicity Index (MI) score 0.99 and produces near perfect rank distributions, indicating highly unique and discriminative rankings. Furthermore, all experiments confirm that 1D-CGS operates in a highly reasonable time, running significantly faster than existing deep learning methods, making it suitable for large scale applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the Closed Loop Dynamics Between a Social Media Recommender System and Users' Opinions</title>
<link>https://arxiv.org/abs/2507.19792</link>
<guid>https://arxiv.org/abs/2507.19792</guid>
<content:encoded><![CDATA[
<div> Keywords: Recommender System, User Opinions, Monte Carlo simulations, Polarisation, Radicalisation

Summary:
The paper presents a mathematical model that analyzes the interaction between a Recommender System (RS) algorithm and content consumers. The model considers a large user population with varying opinions, consuming personalized content recommended by the RS. Through Monte Carlo simulations, the study explores how RS impacts user opinions and the subsequent content recommendations based on user engagement. The research delves into the performance of the RS in influencing user engagement and how user opinions evolve, particularly focusing on polarization and radicalization. It identifies certain opinion distributions as more prone to polarization, highlights ineffective content stances in changing opinions, and underscores the effectiveness of viral content in combating polarization. The findings provide insights into the dynamics of RS-user interactions and the role of viral content in mitigating opinion polarization.<br /><br />Summary: <div>
arXiv:2507.19792v1 Announce Type: new 
Abstract: This paper proposes a mathematical model to study the coupled dynamics of a Recommender System (RS) algorithm and content consumers (users). The model posits that a large population of users, each with an opinion, consumes personalised content recommended by the RS. The RS can select from a range of content to recommend, based on users' past engagement, while users can engage with the content (like, watch), and in doing so, users' opinions evolve. This occurs repeatedly to capture the endless content available for user consumption on social media. We employ a campaign of Monte Carlo simulations using this model to study how recommender systems influence users' opinions, and in turn how users' opinions shape the subsequent recommended content. We take an interest in both the performance of the RS (e.g., how users engage with the content) and the user's opinions, focusing on polarisation and radicalisation of opinions. We find that different opinion distributions are more susceptible to becoming polarised than others, many content stances are ineffective in changing user opinions, and creating viral content is an effective measure in combating polarisation of opinions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying Disinformation Narratives on Social Media with LLMs and Semantic Similarity</title>
<link>https://arxiv.org/abs/2507.20066</link>
<guid>https://arxiv.org/abs/2507.20066</guid>
<content:encoded><![CDATA[
<div> scale measurement, similarity, disinformation, narratives, detection

Summary:
The thesis presents a continuous scale measurement of similarity to disinformation narratives for detecting and capturing partial truths. Two tools, a tracing tool and a narrative synthesis tool, are developed to analyze tweets and target narratives. The tracing tool rates similarities and graphs them over time, while the narrative synthesis tool clusters tweets and identifies dominant narratives. The tools are integrated into a Tweet Narrative Analysis Dashboard. Validation on the GLUE STS-B benchmark is followed by case studies on "The 2020 election was stolen" narrative using Donald Trump's tweets and "Transgender people are harmful to society" narrative using tweets from media outlets. The empirical findings support semantic similarity for nuanced disinformation detection, tracing, and characterization. Access to the tools is available upon request to the author. 

<br /><br />Summary: <div>
arXiv:2507.20066v1 Announce Type: new 
Abstract: This thesis develops a continuous scale measurement of similarity to disinformation narratives that can serve to detect disinformation and capture the nuanced, partial truths that are characteristic of it. To do so, two tools are developed and their methodologies are documented. The tracing tool takes tweets and a target narrative, rates the similarities of each to the target narrative, and graphs it as a timeline. The second narrative synthesis tool clusters tweets above a similarity threshold and generates the dominant narratives within each cluster. These tools are combined into a Tweet Narrative Analysis Dashboard. The tracing tool is validated on the GLUE STS-B benchmark, and then the two tools are used to analyze two case studies for further empirical validation. The first case study uses the target narrative "The 2020 election was stolen" and analyzes a dataset of Donald Trump's tweets during 2020. The second case study uses the target narrative, "Transgender people are harmful to society" and analyzes tens of thousands of tweets from the media outlets The New York Times, The Guardian, The Gateway Pundit, and Fox News. Together, the empirical findings from these case studies demonstrate semantic similarity for nuanced disinformation detection, tracing, and characterization.
  The tools developed in this thesis are hosted and can be accessed through the permission of the author. Please explain your use case in your request. The HTML friendly version of this paper is at https://chaytanc.github.io/projects/disinfo-research (Inman, 2025).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Blockchain-Based Quality Control Model for Online Collaboration Systems</title>
<link>https://arxiv.org/abs/2507.20265</link>
<guid>https://arxiv.org/abs/2507.20265</guid>
<content:encoded><![CDATA[
<div> blockchain, quality control, collaborative content generation, decentralized trust, informetrics

Summary: 
The manuscript introduces a blockchain-based quality control model for collaborative content generation (CCG), addressing challenges such as citation manipulation, transparency, and decentralized trust in metric computation. The model uses a semi-iterative algorithm to compute quality scores of artifacts and reputation of nodes. It is agile in processing latency and shows comparable performance to PageRank and HITS baselines in evaluating quality scores. The model also demonstrates throughput, latency, and robustness against malicious nodes, confirming its reliability. Theoretical comparison with recent studies validates its feasibility for real-world informetric application. <div>
arXiv:2507.20265v1 Announce Type: new 
Abstract: Collaborative content generation (CCG) enables collective creation of artifacts like scientific articles. Quality is a paramount concern in CCG, and a multitude of methods have been proposed to evaluate the quality of artifacts. Nevertheless, the majority of these methods are reliant on centralized architectures, which present challenges pertaining to security, privacy, and availability. Blockchain technology proffers a potential resolution to these challenges, by furnishing a decentralized and immutable ledger of quality scores. In this manuscript, we introduce a blockchain-based quality control model for CCG that uses a semi-iterative algorithm to interdependently compute quality scores of artifacts and reputation of nodes. Our model addresses critical challenges in academic informetrics, such as citation manipulation, transparency in collaborative scholarship, and decentralized trust in metric computation. Our model also exhibits sensitivity to processing latency, rendering it more agile in the presence of delays. Our model's quality scores, evaluated against PageRank and HITS baselines, show comparable performance, with additional assessments of throughput, latency, and robustness against malicious nodes confirming its reliability. A theoretical comparison with recent studies validates its feasibility for real world informetric application.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural-Aware Key Node Identification in Hypergraphs via Representation Learning and Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.20682</link>
<guid>https://arxiv.org/abs/2507.20682</guid>
<content:encoded><![CDATA[
<div> Autoencoder, Hypergraph neural network, Active learning, Node importance, Node identification

Summary:
The article introduces a new framework, AHGA, for evaluating key nodes in hypergraphs, capturing polyadic interactions in real-world systems. AHGA combines an Autoencoder for higher-order structural features, a HyperGraph neural network for pre-training, and an Active learning-based fine-tuning process. The fine-tuning step improves robustness and generalization across diverse hypergraph topologies. Experimental results on empirical hypergraphs show AHGA outperforms centrality-based baselines by 37.4%. Nodes identified by AHGA exhibit high influence and strong structural disruption capability, highlighting their ability to detect multifunctional nodes. <div>
arXiv:2507.20682v1 Announce Type: new 
Abstract: Evaluating node importance is a critical aspect of analyzing complex systems, with broad applications in digital marketing, rumor suppression, and disease control. However, existing methods typically rely on conventional network structures and fail to capture the polyadic interactions intrinsic to many real-world systems. To address this limitation, we study key node identification in hypergraphs, where higher-order interactions are naturally modeled as hyperedges. We propose a novel framework, AHGA, which integrates an Autoencoder for extracting higher-order structural features, a HyperGraph neural network-based pre-training module (HGNN), and an Active learning-based fine-tuning process. This fine-tuning step plays a vital role in mitigating the gap between synthetic and real-world data, thereby enhancing the model's robustness and generalization across diverse hypergraph topologies. Extensive experiments on eight empirical hypergraphs show that AHGA outperforms classical centrality-based baselines by approximately 37.4%. Furthermore, the nodes identified by AHGA exhibit both high influence and strong structural disruption capability, demonstrating their superiority in detecting multifunctional nodes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wardropian Cycles make traffic assignment both optimal and fair by eliminating price-of-anarchy with Cyclical User Equilibrium for compliant connected autonomous vehicles</title>
<link>https://arxiv.org/abs/2507.19675</link>
<guid>https://arxiv.org/abs/2507.19675</guid>
<content:encoded><![CDATA[
<div> Keywords: Connected and Autonomous Vehicles, Wardropian cycles, System Optimal, User Equilibrium, Traffic Performance

Summary:
In this study, the concept of Wardropian cycles is proposed to achieve fair and optimal traffic assignment in the context of Connected and Autonomous Vehicles (CAVs). These cycles ensure both fairness and efficiency in routing, satisfying Wardrop's principles while equalizing average travel times among users. The researchers develop exact methods and a greedy heuristic to compute and optimize these cycles efficiently. They introduce the concept of Cyclical User Equilibrium for stability under deviations. Large-scale simulations in Barcelona, Berlin, Anaheim, and Sioux Falls show significant reductions in traffic inefficiencies and inequities with the implementation of Wardropian cycles, with the approach displaying more social acceptability. In Barcelona, up to 670 vehicle-hours of inefficiencies were eliminated, while in other cities, initial inequities were significantly reduced within a short timeframe. Overall, the study demonstrates the potential of Wardropian cycles in improving traffic performance and equality in CAV systems.<br /><br />Summary: <div>
arXiv:2507.19675v1 Announce Type: cross 
Abstract: Connected and Autonomous Vehicles (CAVs) open the possibility for centralised routing with full compliance, making System Optimal traffic assignment attainable. However, as System Optimum makes some drivers better off than others, voluntary acceptance seems dubious. To overcome this issue, we propose a new concept of Wardropian cycles, which, in contrast to previous utopian visions, makes the assignment fair on top of being optimal, which amounts to satisfaction of both Wardrop's principles. Such cycles, represented as sequences of permutations to the daily assignment matrices, always exist and equalise, after a limited number of days, average travel times among travellers (like in User Equilibrium) while preserving everyday optimality of path flows (like in System Optimum). We propose exact methods to compute such cycles and reduce their length and within-cycle inconvenience to the users. As identification of optimal cycles turns out to be NP-hard in many aspects, we introduce a greedy heuristic efficiently approximating the optimal solution. Finally, we introduce and discuss a new paradigm of Cyclical User Equilibrium, which ensures stability of optimal Wardropian Cycles under unilateral deviations.
  We complement our theoretical study with large-scale simulations. In Barcelona, 670 vehicle-hours of Price-of-Anarchy are eliminated using cycles with a median length of 11 days-though 5% of cycles exceed 90 days. However, in Berlin, just five days of applying the greedy assignment rule significantly reduces initial inequity. In Barcelona, Anaheim, and Sioux Falls, less than 7% of the initial inequity remains after 10 days, demonstrating the effectiveness of this approach in improving traffic performance with more ubiquitous social acceptability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashGuard: Novel Method in Evaluating Differential Characteristics of Visual Stimuli for Deterring Seizure Triggers in Photosensitive Epilepsy</title>
<link>https://arxiv.org/abs/2507.19692</link>
<guid>https://arxiv.org/abs/2507.19692</guid>
<content:encoded><![CDATA[
<div> photosensitive epilepsy, FlashGuard, color analysis, seizure prevention, digital media<br />
<br />
Summary: <br />
Individuals with photosensitive epilepsy (PSE) face challenges in the virtual realm due to unpredictable seizure-causing visual stimuli. Current solutions detect flashes asynchronously, lacking real-time and computational efficiency. FlashGuard, a novel approach, assesses color change rates in frames to mitigate stimuli in real-time. It uses CIELAB color space analysis to analyze color differences, reducing luminance and smoothing transitions. This study highlights how color properties impact flashing perception for PSE individuals, advocating for broader WCAG guidelines. Implementing these insights can better protect individuals with PSE from digital media triggers, enhancing accessibility and safety. <div>
arXiv:2507.19692v1 Announce Type: cross 
Abstract: In the virtual realm, individuals with photosensitive epilepsy (PSE) encounter challenges when using devices, resulting in exposure to unpredictable seizure-causing visual stimuli. The current norm for preventing epileptic flashes in media is to detect asynchronously when a flash will occur in a video, then notifying the user. However, there is a lack of a real-time and computationally efficient solution for dealing with this issue. To address this issue and enhance accessibility for photosensitive viewers, FlashGuard, a novel approach, was devised to assess the rate of change of colors in frames across the user's screen and appropriately mitigate stimuli, based on perceptually aligned color space analysis in the CIELAB color space. The detection system is built on analyzing differences in color, and the mitigation system works by reducing luminance and smoothing color transitions. This study provides novel insight into how intrinsic color properties contribute to perceptual differences in flashing for PSE individuals, calling for the adoption of broadened WCAG guidelines to better account for risk. These insights and implementations pave the way for stronger protections for individuals with PSE from dangerous triggers in digital media, both in policy and in software.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Bidding in Service-Oriented Combinatorial Spectrum Forward Auctions</title>
<link>https://arxiv.org/abs/2507.19720</link>
<guid>https://arxiv.org/abs/2507.19720</guid>
<content:encoded><![CDATA[
<div> auctions, spectrum, combinatorial, bidding mechanism, social welfare

Summary:
The article introduces a novel combinatorial forward auction scheme that allows for flexible bidding in dynamic spectrum sharing environments. Participants can submit bids consisting of base spectrum demand and adjustable demand ranges, improving resource efficiency and maximizing social welfare. A Spectrum Equivalent Mapping (SEM) coefficient is used to standardize valuation across frequency bands. A greedy matching algorithm sorts buyers by equivalent unit bid prices to determine winning bids and allocate resources within supply constraints. Simulation results show that the proposed flexible bidding mechanism outperforms existing methods, achieving higher social welfare in dynamic spectrum sharing scenarios. <div>
arXiv:2507.19720v1 Announce Type: cross 
Abstract: Traditional combinatorial spectrum auctions mainly rely on fixed bidding and matching processes, which limit participants' ability to adapt their strategies and often result in suboptimal social welfare in dynamic spectrum sharing environments. To address these limitations, we propose a novel approximately truthful combinatorial forward auction scheme with a flexible bidding mechanism aimed at enhancing resource efficiency and maximizing social welfare. In the proposed scheme, each buyer submits a combinatorial bid consisting of the base spectrum demand and adjustable demand ranges, enabling the auctioneer to dynamically optimize spectrum allocation in response to market conditions. To standardize the valuation across heterogeneous frequency bands, we introduce a Spectrum Equivalent Mapping (SEM) coefficient. A greedy matching algorithm is employed to determine winning bids by sorting buyers based on their equivalent unit bid prices and allocating resources within supply constraints. Simulation results demonstrate that the proposed flexible bidding mechanism significantly outperforms existing benchmark methods, achieving notably higher social welfare in dynamic spectrum sharing scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democracy for DAOs: An Empirical Study of Decentralized Governance and Dynamic (Case Study Internet Computer SNS Ecosystem)</title>
<link>https://arxiv.org/abs/2507.20234</link>
<guid>https://arxiv.org/abs/2507.20234</guid>
<content:encoded><![CDATA[
<div> Decentralized Autonomous Organizations, DAOs, governance mechanism, user behavior, Internet Computer Protocol, SNS DAO framework<br />
Summary: 
This empirical study explores user behavior in decentralized autonomous organizations (DAOs) using the Internet Computer Protocol DAO framework SNS. The analysis includes participation rates, proposal submission frequency, voter approval rates, decision duration times, and metric shifts. Over 3,000 proposals from 14 SNS DAOs were evaluated, showing high approval rates and alignment in governance mechanisms. SNS DAOs demonstrate higher activity, lower costs, and faster decisions compared to other blockchain platforms. Importantly, SNS DAOs exhibit sustained or increasing engagement levels over time, contrary to declines seen in other frameworks. This study highlights the effectiveness of SNS governance mechanisms in promoting user engagement and agility in decision-making processes. <br /><br /> <div>
arXiv:2507.20234v1 Announce Type: cross 
Abstract: Decentralized autonomous organizations (DAOs) rely on governance mechanism without centralized leadership. This paper presents an empirical study of user behavior in governance for a variety of DAOs, ranging from DeFi to gaming, using the Internet Computer Protocol DAO framework called SNS (Service Nervous System). To analyse user engagement, we measure participation rates and frequency of proposals submission and voter approval rates. We evaluate decision duration times to determine DAO agility. To investigate dynamic aspects, we also measure metric shifts in time. We evaluate over 3,000 proposals submitted in a time frame of 20 months from 14 SNS DAOs. The selected DAO have been existing between 6 and 20 months and cover a wide spectrum of use cases, treasury sizes, and number of participants. We also compare our results for SNS DAOs with DAOs from other blockchain platforms. While approval rates are generally high for all DAOs studied, SNS DAOs show slightly more alignment. We observe that the SNS governance mechanisms and processes in ICP lead to higher activity, lower costs and faster decisions. Most importantly, in contrast to studies which report a decline in participation over time for other frameworks, SNS DAOs exhibit sustained or increasing engagement levels over time.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Community Detection in Academic Networks by Handling Publication Bias</title>
<link>https://arxiv.org/abs/2507.20449</link>
<guid>https://arxiv.org/abs/2507.20449</guid>
<content:encoded><![CDATA[
<div> Keywords: research collaboration, topic-based network, BERTopic, SciBERT model, interdisciplinary links

Summary: 
The research article introduces a novel approach to identifying potential research collaborators based on publication content rather than traditional methods like co-authorships and citations. By utilizing BERTopic with a fine-tuned SciBERT model, a topic-based research network is built to connect researchers across disciplines who share topical interests. The main challenge addressed is publication imbalance, where some researchers publish more frequently across various topics, making their less frequent interests less visible. To overcome this, a cloning strategy is proposed, clustering a researcher's publications to treat each cluster as a separate node, allowing researchers to be part of multiple communities and improving the detection of interdisciplinary links. The evaluation of this method demonstrates that the cloned network structure leads to more meaningful communities and reveals a wider range of collaboration opportunities.<br /><br />Summary: <div>
arXiv:2507.20449v1 Announce Type: cross 
Abstract: Finding potential research collaborators is a challenging task, especially in today's fast-growing and interdisciplinary research landscape. While traditional methods often rely on observable relationships such as co-authorships and citations to construct the research network, in this work, we focus solely on publication content to build a topic-based research network using BERTopic with a fine-tuned SciBERT model that connects and recommends researchers across disciplines based on shared topical interests. A major challenge we address is publication imbalance, where some researchers publish much more than others, often across several topics. Without careful handling, their less frequent interests are hidden under dominant topics, limiting the network's ability to detect their full research scope. To tackle this, we introduce a cloning strategy that clusters a researcher's publications and treats each cluster as a separate node. This allows researchers to be part of multiple communities, improving the detection of interdisciplinary links. Evaluation on the proposed method shows that the cloned network structure leads to more meaningful communities and uncovers a broader set of collaboration opportunities.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2507.20924</link>
<guid>https://arxiv.org/abs/2507.20924</guid>
<content:encoded><![CDATA[
<div> Identification, classification, sexism, social media, models
Summary:
The paper discusses the fifth Sexism Identification in Social Networks (EXIST) challenge at CLEF 2025, focusing on identifying and classifying sexism in social media posts. Three subtasks are addressed through the implementation of three models: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses adjectives as bottleneck concepts for interpretability, while SCBMT combines this with transformer contextual embeddings for improved performance. The models offer insights at both instance and class levels. Additional metadata like annotators' profiles are explored for leveraging in the classification task. Results show competitive performance, with SCBMT ranking 7th and 6th for English and Spanish, respectively, in Subtask 1.1. In comparison, the fine-tuned XLM-RoBERTa model achieves 6th and 4th for English in the Soft-Soft evaluation of Subtask 1.1. This research contributes to combating sexism on social media through advanced modeling techniques and data analysis. 
<br /><br />Summary: <div>
arXiv:2507.20924v1 Announce Type: cross 
Abstract: Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Bridge Spatial and Temporal Heterogeneity in Link Prediction? A Contrastive Method</title>
<link>https://arxiv.org/abs/2411.00612</link>
<guid>https://arxiv.org/abs/2411.00612</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal Heterogeneous Networks, Link Prediction, Contrastive Learning, Spatial Heterogeneity, Temporal Heterogeneity

Summary:
This study introduces a novel Contrastive Learning-based Link Prediction model (CLP) designed to address the shortcomings of existing methods in capturing spatial and temporal heterogeneity in Temporal Heterogeneous Networks. The model employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. It includes a spatial feature modeling layer for capturing fine-grained topological distribution patterns and a temporal information modeling layer to perceive evolutionary dependencies. By encoding spatial and temporal distribution heterogeneity from a contrastive learning perspective, the model enables comprehensive self-supervised hierarchical relation modeling for link prediction. Experimental results on four real-world dynamic heterogeneous network datasets show that CLP consistently outperforms state-of-the-art models, demonstrating significant improvements in AUC and AP metrics. This research showcases the effectiveness of CLP in capturing complex system dynamics and heterogeneity in network link prediction tasks.

<br /><br />Summary: <div>
arXiv:2411.00612v2 Announce Type: replace 
Abstract: Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel \textbf{C}ontrastive Learning-based \textbf{L}ink \textbf{P}rediction model, \textbf{CLP}, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a spatial feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the spatial and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our \mymodel consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10\%, 13.44\% in terms of AUC and AP, respectively.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fixed points of Personalized PageRank centrality: From irreducible to reducible networks</title>
<link>https://arxiv.org/abs/2507.18652</link>
<guid>https://arxiv.org/abs/2507.18652</guid>
<content:encoded><![CDATA[
<div> PageRank, complex network, personalization vector, fixed points, strongly connected components <br />
Summary: <br />
- The study focuses on analyzing the PageRank of a complex network based on its personalization vector.
- The research provides a comprehensive understanding of the existence and uniqueness of fixed points of PageRank in a graph.
- The number and nature of strongly connected components play a crucial role in determining the fixed points of PageRank.
- A feedback-PageRank method is introduced to accurately compute the fixed points using Power's Method and the Perron vector of each strongly connected component.
- The approach presented in the paper offers a systematic way to analyze the PageRank behavior in complex networks. <div>
arXiv:2507.18652v1 Announce Type: new 
Abstract: In this paper we analyze the PageRank of a complex network as a function of its personalization vector. By using this approach, a complete characterization of the existence and uniqueness of fixed points of PageRank of a graph is given in terms of the number and nature of its strongly connected components. The method presented includes the use of a feedback-PageRank in order to compute exactly the fixed points following the classic Power's Method in terms of the (left-hand) Perron vector of each strongly connected components.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negative news posts are less prevalent and generate lower user engagement than non-negative news posts across six countries</title>
<link>https://arxiv.org/abs/2507.19300</link>
<guid>https://arxiv.org/abs/2507.19300</guid>
<content:encoded><![CDATA[
<div> Facebook, news posts, negativity, engagement, multilingual classifiers  
Summary:  
Negative news posts on Facebook make up a small fraction (12.6%) of all posts, with political news posts not being more negative than non-political ones. In the U.S., political news posts are relatively less negative compared to other countries. Negative news posts receive fewer likes and comments compared to non-negative posts. Only a small proportion (10.2% to 13.1%) of user engagement with news posts comes from negative posts by analyzed news organizations. This comparative study sheds light on the prevalence of negative news on social media and its impact on user engagement, suggesting that negativity does not always correlate with higher engagement levels. <br /><br />Summary: <div>
arXiv:2507.19300v1 Announce Type: new 
Abstract: Although news negativity is often studied, missing is comparative evidence on the prevalence of and engagement with negative political and non-political news posts on social media. We use 6,081,134 Facebook posts published between January 1, 2020, and April 1, 2024, by 97 media organizations in six countries (U.S., UK, Ireland, Poland, France, Spain) and develop two multilingual classifiers for labeling posts as (non-)political and (non-)negative. We show that: (1) negative news posts constitute a relatively small fraction (12.6%); (2) political news posts are neither more nor less negative than non-political news posts; (3) U.S. political news posts are less negative relative to the other countries on average (40% lower odds); (4) Negative news posts get 15% fewer likes and 13% fewer comments than non-negative news posts. Lastly, (5) we provide estimates of the proportion of the total volume of user engagement with negative news posts and show that only between 10.2% to 13.1% of engagement is linked to negative posts by the analyzed news organizations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Changes to the Facebook Algorithm Decreased News Visibility Between 2021-2024</title>
<link>https://arxiv.org/abs/2507.19373</link>
<guid>https://arxiv.org/abs/2507.19373</guid>
<content:encoded><![CDATA[
<div> Facebook, news, algorithm, visibility, reactions <br />
Summary: The study examines the impact of Meta's algorithm changes on news visibility on Facebook between 2016 and 2025. Data from news and non-news pages show a significant decline in user reactions to news, particularly between 2021 and 2024, indicating targeted suppression. Low-quality news sources were disproportionately affected. However, the end of the "War on News" in 2025 led to an increase in user reactions to news, especially low-quality sources. This suppression of news visibility did not align with a decrease in news supply, Facebook user base, or interest in news. The findings suggest that Meta's algorithm changes had a substantial impact on the visibility and engagement with news content on the platform. <div>
arXiv:2507.19373v1 Announce Type: new 
Abstract: Platforms, especially Facebook, are primary news sources in the US. In its widely criticized "War on News," Meta algorithmically deprioritized news and political content. We use data from 40 news organizations (5,243,302 Facebook posts, 7,875,372,958 user reactions) and 21 non-news pages (396,468 posts; 1,909,088,308 reactions) between January 1, 2016 and February 13, 2025 to examine how these changes influenced news visibility on the platform. Reactions to news declined by 78% between 2021 and 2024 while reactions to non-news pages increased, indicating targeted suppression of news visibility. Low-quality sources were especially suppressed, yet the 2025 end to "War on News" increased user reactions to news, especially low-quality ones. These changes do not reflect decreased news supply, Facebook user base, or interest in news over this period.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityHood: An Explainable Travel Recommender System for Cities and Neighborhoods</title>
<link>https://arxiv.org/abs/2507.18778</link>
<guid>https://arxiv.org/abs/2507.18778</guid>
<content:encoded><![CDATA[
<div> recommendation system, CityHood, interactive, explainable, personalized

Summary:
CityHood is an interactive and explainable recommendation system that suggests cities and neighborhoods based on users' interests. It leverages large-scale Google Places reviews enriched with various indicators to provide personalized recommendations at city and neighborhood levels. The system uses the LIME technique for explainability and offers natural-language explanations for each suggestion. Users can explore recommendations based on their preferences and understand the reasoning behind each suggestion through a visual interface. CityHood combines interest modeling, multi-scale analysis, and explainability to make travel recommendations transparent and engaging. It bridges gaps in location-based recommendations by considering spatial similarity, cultural alignment, and user interests. <div>
arXiv:2507.18778v1 Announce Type: cross 
Abstract: We present CityHood, an interactive and explainable recommendation system that suggests cities and neighborhoods based on users' areas of interest. The system models user interests leveraging large-scale Google Places reviews enriched with geographic, socio-demographic, political, and cultural indicators. It provides personalized recommendations at city (Core-Based Statistical Areas - CBSAs) and neighborhood (ZIP code) levels, supported by an explainable technique (LIME) and natural-language explanations. Users can explore recommendations based on their stated preferences and inspect the reasoning behind each suggestion through a visual interface. The demo illustrates how spatial similarity, cultural alignment, and interest understanding can be used to make travel recommendations transparent and engaging. This work bridges gaps in location-based recommendation by combining a kind of interest modeling, multi-scale analysis, and explainability in a user-facing system.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributed Approach for Agile Supply Chain Decision-Making Based on Network Attributes</title>
<link>https://arxiv.org/abs/2507.19038</link>
<guid>https://arxiv.org/abs/2507.19038</guid>
<content:encoded><![CDATA[
<div> network attributes, supply chain, disruption mitigation, distributed decision-making, performance evaluation

Summary:
This paper addresses the issue of disruptions in global supply chains and the need for agile decision-making strategies to mitigate their impact. The study characterizes supply chains from both a capability and network topological perspective and explores the use of distributed decision-making approaches to improve supply chain resilience. A comprehensive case study is conducted, evaluating the performance of a distributed framework in response to disruptions based on the network structure and agent attributes. Comparisons with a centralized decision-making approach highlight trade-offs between performance, computation time, and network communication. The findings provide valuable insights for practitioners looking to design effective response strategies that leverage agent capabilities, network attributes, and desired supply chain performance. <div>
arXiv:2507.19038v1 Announce Type: cross 
Abstract: In recent years, the frequent occurrence of disruptions has had a negative impact on global supply chains. To stay competitive, enterprises strive to remain agile through the implementation of efficient and effective decision-making strategies in reaction to disruptions. A significant effort has been made to develop these agile disruption mitigation approaches, leveraging both centralized and distributed decision-making strategies. Though trade-offs of centralized and distributed approaches have been analyzed in existing studies, no related work has been found on understanding supply chain performance based on the network attributes of the disrupted supply chain entities. In this paper, we characterize supply chains from a capability and network topological perspective and investigate the use of a distributed decision-making approach based on classical multi-agent frameworks. The performance of the distributed framework is evaluated through a comprehensive case study that investigates the performance of the supply chain as a function of the network structure and agent attributes within the network in the presence of a disruption. Comparison to a centralized decision-making approach highlights trade-offs between performance, computation time, and network communication based on the decision-making strategy and network architecture. Practitioners can use the outcomes of our studies to design response strategies based on agent capabilities, network attributes, and desired supply chain performance.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epidemiology-informed Network for Robust Rumor Detection</title>
<link>https://arxiv.org/abs/2411.12949</link>
<guid>https://arxiv.org/abs/2411.12949</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, rumor detection, information cascade, epidemiology, social media<br />
<br />
Summary:<br />
The article discusses the challenges posed by the rapid spread of rumors on social media and the limitations of existing graph-based rumor detection models. It introduces a novel approach called Epidemiology-informed Network (EIN) that integrates epidemiological knowledge to improve performance by addressing issues related to data quality and varying depths of propagation trees. The EIN model utilizes large language models to generate stance labels from user responses, eliminating the need for manual annotation. Experimental results show that EIN outperforms state-of-the-art methods on real-world datasets and demonstrates enhanced robustness across different tree depths. By incorporating epidemiological principles into rumor detection, the EIN approach offers a more effective and efficient way to detect and monitor the spread of rumors on social media platforms. <br /> <div>
arXiv:2411.12949v3 Announce Type: replace 
Abstract: The rapid spread of rumors on social media has posed significant challenges to maintaining public trust and information integrity. Since an information cascade process is essentially a propagation tree, recent rumor detection models leverage graph neural networks to additionally capture information propagation patterns, thus outperforming text-only solutions. Given the variations in topics and social impact of the root node, different source information naturally has distinct outreach capabilities, resulting in different heights of propagation trees. This variation, however, impedes the data-driven design of existing graph-based rumor detectors. Given a shallow propagation tree with limited interactions, it is unlikely for graph-based approaches to capture sufficient cascading patterns, questioning their ability to handle less popular news or early detection needs. In contrast, a deep propagation tree is prone to noisy user responses, and this can in turn obfuscate the predictions. In this paper, we propose a novel Epidemiology-informed Network (EIN) that integrates epidemiological knowledge to enhance performance by overcoming data-driven methods sensitivity to data quality. Meanwhile, to adapt epidemiology theory to rumor detection, it is expected that each users stance toward the source information will be annotated. To bypass the costly and time-consuming human labeling process, we take advantage of large language models to generate stance labels, facilitating optimization objectives for learning epidemiology-informed representations. Our experimental results demonstrate that the proposed EIN not only outperforms state-of-the-art methods on real-world datasets but also exhibits enhanced robustness across varying tree depths.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the Climate Change Debate in Italy through Information Supply and Demand</title>
<link>https://arxiv.org/abs/2503.17026</link>
<guid>https://arxiv.org/abs/2503.17026</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, social media, information circulation, information voids, Italian climate transition discourse

Summary: 
The study focuses on the dynamics of information circulation and the emergence of information voids in the context of the Italian climate-transition discourse. It highlights the importance of public understanding of climate issues and the role of social media platforms in disseminating information. The model proposed in the study takes into account the supply and demand of information on platforms like Facebook, Instagram, and GDELT, as well as Google searches to capture information demand. The findings reveal responsiveness and temporal coupling between supply and demand, especially during moments of heightened public attention due to significant external events. The study identifies an adaptive information ecosystem but also notes persistent information voids that could limit public understanding and hinder meaningful engagement with climate policy. <div>
arXiv:2503.17026v2 Announce Type: replace 
Abstract: Climate change is one of the most critical challenges of the twenty-first century. Public understanding of climate issues and of the goals regarding the climate transition is essential to translate awareness into concrete actions. In this context, social media platforms play a crucial role in disseminating information about climate change and climate policy. To better understand the dynamics of information circulation and the emergence of information voids we propose a model that takes into account the supply and demand of information related to the Italian climate-transition discourse. We conceptualise information supply as the production of content on Facebook, Instagram and GDELT (an online news database) while leveraging Google searches to capture information demand. Our findings highlight responsiveness and temporal coupling between supply and demand, particularly during moments of heightened public attention triggered by significant external events. These responsive interactions reveal an overall adaptive information ecosystem. However, we also observe persistent information voids which may limit public understanding and delay meaningful engagement.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image</title>
<link>https://arxiv.org/abs/2507.17755</link>
<guid>https://arxiv.org/abs/2507.17755</guid>
<content:encoded><![CDATA[
<div> Keywords: Douyin, WeChat, body image, male adolescents, social media <br />
Summary: <br />
This study explores the impact of two Chinese social media platforms, Douyin and WeChat, on body image perceptions among male adolescents. Through surveys with 395 participants, it was found that Douyin usage was significantly linked to appearance evaluation and body area satisfaction, while WeChat usage showed no significant correlation with any body image dimensions. The results suggest that Douyin's algorithm-driven, video-centric environment may contribute to heightened exposure to idealized body standards, influencing users at a cognitive level. The study emphasizes the need to consider platform-specific characteristics when examining social media's influence on body image. By highlighting how technological design and content modalities shape psychological outcomes, the research provides valuable insights for addressing body image concerns among male adolescents in China. <br /> <div>
arXiv:2507.17755v1 Announce Type: cross 
Abstract: In the digital era, social media platforms play a pivotal role in shaping adolescents' body image perceptions. This study examines how Douyin and WeChat, two contrasting Chinese social media platforms, influence body image among Chinese male adolescents. Employing a platformization perspective, we surveyed 395 male adolescents aged 10 to 24 using the Multidimensional Body-Self Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation and body satisfaction. Our findings reveal that Douyin usage is significantly correlated with appearance evaluation and body area satisfaction, while WeChat usage shows no significant correlation with any body image dimensions. These results suggest that Douyin's algorithm-driven, video-centric environment intensifies exposure to idealized body standards, impacting users at a cognitive level. This study underscores the importance of considering platform-specific characteristics in understanding social media's impact on body image. It contributes to the broader discourse on how technological design and content modalities mediate psychological outcomes, offering insights for addressing body image concerns among male adolescents in China.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Uniqueness and Divisiveness of Presidential Discourse</title>
<link>https://arxiv.org/abs/2401.01405</link>
<guid>https://arxiv.org/abs/2401.01405</guid>
<content:encoded><![CDATA[
<div> Keywords: American presidents, speech patterns, divisive language, uniqueness, language models

Summary:
This study analyzes the speaking styles of American presidents using a unique metric based on language models. The research examines whether presidents speak differently from each other and in what ways, particularly focusing on Donald Trump's distinctiveness. The findings suggest that Trump's speech patterns diverge significantly from other major party nominees, with an emphasis on divisive and antagonistic language towards political opponents. These differences are consistent across various measurement strategies, occurring both in campaign speeches and official addresses. The study concludes that Trump's uniqueness in speech is not simply a result of changes in presidential communications over time. Overall, this research sheds light on the distinct ways in which presidents communicate and highlights Trump's particularly unique and divisive language compared to his predecessors.<br /><br />Summary: <div>
arXiv:2401.01405v2 Announce Type: replace-cross 
Abstract: Do American presidents speak discernibly different from each other? If so, in what ways? And are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for assessing the distinctive ways in which presidents speak about their political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Donald Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values appear closer to those of the Democrats. Contributing to these differences is Trump's employment of divisive and antagonistic language, particularly when targeting his political opponents. These differences hold across a variety of measurement strategies, arise on both the campaign trail and in official presidential addresses, and do not appear to be an artifact of secular changes in presidential communications.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature</title>
<link>https://arxiv.org/abs/2507.16820</link>
<guid>https://arxiv.org/abs/2507.16820</guid>
<content:encoded><![CDATA[
<div> countries, institutions, authors, collaboration networks, topics<br />
<br />
Summary: 
Countries most impacted by COVID-19 were highly active in disaster informatics research, each with specific interests. Regional and language-based collaborations were common among countries and institutions. Top authors tended to partner closely with a few key collaborators, focused on specific topics, with institutions having diverse interests. The pandemic shifted research priorities towards public health in disaster informatics. The field is emphasizing multidimensional resilience strategies and data-sharing collaborations, reflecting global vulnerability awareness. Strategies, practices, and tools used in this study can be applied to similar datasets. The analysis provides insights for policymakers, practitioners, and scholars looking to enhance disaster informatics capacities in complex risk landscapes. <br /><br /> <div>
arXiv:2507.16820v1 Announce Type: new 
Abstract: This study presents a comprehensive bibliometric and topic analysis of the disaster informatics literature published between January 2020 to September 2022. Leveraging a large-scale corpus and advanced techniques such as pre-trained language models and generative AI, we identify the most active countries, institutions, authors, collaboration networks, emergent topics, patterns among the most significant topics, and shifts in research priorities spurred by the COVID-19 pandemic. Our findings highlight (1) countries that were most impacted by the COVID-19 pandemic were also among the most active, with each country having specific research interests, (2) countries and institutions within the same region or share a common language tend to collaborate, (3) top active authors tend to form close partnerships with one or two key partners, (4) authors typically specialized in one or two specific topics, while institutions had more diverse interests across several topics, and (5) the COVID-19 pandemic has influenced research priorities in disaster informatics, placing greater emphasis on public health. We further demonstrate that the field is converging on multidimensional resilience strategies and cross-sectoral data-sharing collaborations or projects, reflecting a heightened awareness of global vulnerability and interdependency. Collecting and quality assurance strategies, data analytic practices, LLM-based topic extraction and summarization approaches, and result visualization tools can be applied to comparable datasets or solve similar analytic problems. By mapping out the trends in disaster informatics, our analysis offers strategic insights for policymakers, practitioners, and scholars aiming to enhance disaster informatics capacities in an increasingly uncertain and complex risk landscape.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVOLVE-X: Embedding Fusion and Language Prompting for User Evolution Forecasting on Social Media</title>
<link>https://arxiv.org/abs/2507.16847</link>
<guid>https://arxiv.org/abs/2507.16847</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media, user behavior, prediction, open-source models, cross-modal configurations

Summary: 
This study explores the evolution of user behavior on social media platforms over their lifetime using a novel approach. By leveraging open-source models and advanced language processing techniques like GPT-2, BERT, and RoBERTa, the research aims to predict future stages of user social evolution, including network changes, future connections, and shifts in activities. Experimental results show that GPT-2 outperforms other models in a Cross-modal configuration, emphasizing the importance of this approach for superior performance. The study addresses critical challenges in social media, such as friend recommendations and activity predictions, providing insights into user behavior trajectories. By anticipating future interactions and activities, the research aims to offer early warnings about potential negative outcomes, enabling users to make informed decisions and mitigate risks in the long term.<br /><br />Summary: <div>
arXiv:2507.16847v1 Announce Type: new 
Abstract: Social media platforms serve as a significant medium for sharing personal emotions, daily activities, and various life events, ensuring individuals stay informed about the latest developments. From the initiation of an account, users progressively expand their circle of friends or followers, engaging actively by posting, commenting, and sharing content. Over time, user behavior on these platforms evolves, influenced by demographic attributes and the networks they form. In this study, we present a novel approach that leverages open-source models Llama-3-Instruct, Mistral-7B-Instruct, Gemma-7B-IT through prompt engineering, combined with GPT-2, BERT, and RoBERTa using a joint embedding technique, to analyze and predict the evolution of user behavior on social media over their lifetime. Our experiments demonstrate the potential of these models to forecast future stages of a user's social evolution, including network changes, future connections, and shifts in user activities. Experimental results highlight the effectiveness of our approach, with GPT-2 achieving the lowest perplexity (8.21) in a Cross-modal configuration, outperforming RoBERTa (9.11) and BERT, and underscoring the importance of leveraging Cross-modal configurations for superior performance. This approach addresses critical challenges in social media, such as friend recommendations and activity predictions, offering insights into the trajectory of user behavior. By anticipating future interactions and activities, this research aims to provide early warnings about potential negative outcomes, enabling users to make informed decisions and mitigate risks in the long term.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Simulation Framework for Disinformation Dissemination and Correction With Social Bots</title>
<link>https://arxiv.org/abs/2507.16848</link>
<guid>https://arxiv.org/abs/2507.16848</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots, disinformation dissemination, multi-agent framework, correction strategies, network modeling <br />
<br />
Summary: 
The study focuses on understanding the influence of social bots in spreading and correcting disinformation in the human-bot symbiotic information ecosystem. The proposed Multi Agent based framework for Disinformation Dissemination (MADD) aims to address the limitations of current studies by utilizing a more realistic propagation network model that integrates scale-free topology and community structures. By incorporating both malicious and legitimate bots with controlled dynamic participation, MADD enables quantitative analysis of correction strategies. The framework was evaluated using individual and group-level metrics, and experiments verified the consistency of user attributes and network structure with real-world data. The simulation of disinformation dissemination showcased the varying effects of fact-based and narrative-based correction strategies. MADD provides a valuable tool for better understanding and managing the impact of social bots in the dissemination of disinformation. <br /> <div>
arXiv:2507.16848v1 Announce Type: new 
Abstract: In the human-bot symbiotic information ecosystem, social bots play key roles in spreading and correcting disinformation. Understanding their influence is essential for risk control and better governance. However, current studies often rely on simplistic user and network modeling, overlook the dynamic behavior of bots, and lack quantitative evaluation of correction strategies. To fill these gaps, we propose MADD, a Multi Agent based framework for Disinformation Dissemination. MADD constructs a more realistic propagation network by integrating the Barabasi Albert Model for scale free topology and the Stochastic Block Model for community structures, while designing node attributes based on real world user data. Furthermore, MADD incorporates both malicious and legitimate bots, with their controlled dynamic participation allows for quantitative analysis of correction strategies. We evaluate MADD using individual and group level metrics. We experimentally verify the real world consistency of MADD user attributes and network structure, and we simulate the dissemination of six disinformation topics, demonstrating the differential effects of fact based and narrative based correction strategies.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Subreddit Behavior as Open-Source Indicators of Coordinated Influence: A Case Study of r/Sino &amp; r/China</title>
<link>https://arxiv.org/abs/2507.16857</link>
<guid>https://arxiv.org/abs/2507.16857</guid>
<content:encoded><![CDATA[
<div> Keywords: coordinated influence activity, Reddit communities, sentiment analysis, behavioral profiling, online influence behavior

Summary: 
This study examines indicators of coordinated influence activity in users of the Reddit communities r/Sino and r/China, which focus on Chinese political discourse. The research uses topic modeling and sentiment analysis to analyze posts from users who participate in both communities, creating a user-topic sentiment matrix. Behavioral profiling is conducted using various measures such as lexical diversity, language consistency, and posting frequency. Users with anomalous behavior are identified and examined within a subreddit co-participation network. The study integrates linguistic and behavioral analysis to identify patterns indicative of inauthentic or strategically structured participation. The findings showcase the importance of combining content and activity-based signals in analyzing online influence behavior, particularly within contested information environments. <div>
arXiv:2507.16857v1 Announce Type: new 
Abstract: This study investigates potential indicators of coordinated influence activity among users participating in both r/Sino and r/China, two ideologically divergent Reddit communities focused on Chinese political discourse. Topic modeling and sentiment analysis are applied to all posts and comments authored by dual-subreddit users to construct a user-topic sentiment matrix. Individual sentiment patterns are compared to global topic baselines derived from the broader r/Sino and r/China populations. Behavioral profiling is performed using full user activity histories and metadata, incorporating measures such as lexical diversity, language consistency, account age, posting frequency, and karma distribution. Users exhibiting multiple behavioral anomalies are identified and examined within a subreddit co-participation network to assess structural overlap. The combined linguistic and behavioral analysis enables the identification of patterns consistent with inauthentic or strategically structured participation. These findings demonstrate the utility of integrating content and activity-based signals in the analysis of online influence behavior within contested information environments.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky</title>
<link>https://arxiv.org/abs/2507.16858</link>
<guid>https://arxiv.org/abs/2507.16858</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, Congressional Democrats, social network analysis, decentralized platforms, political messaging

Summary:
In the wake of the 2024 U.S. presidential election, Democratic lawmakers shifted from mainstream social media to Bluesky, a decentralized platform. A study on 182 verified Democratic members of Congress analyzes their use of Bluesky to form influence networks and disseminate political messaging. Party leaders like Hakeem Jeffries and Elizabeth Warren are prominent, but lesser-known figures like Marcy Kaptur and Donald Beyer hold significant influence positions. Analysis reveals homophily based on ideology, state, and leadership lines, with Senate leaders less connected. Topic modeling uncovers shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad standing out. The study highlights how decentralized platforms reshape intra-party communication and emphasizes the importance of computational research on elite political behavior in digital environments. <br /><br />Summary: <div>
arXiv:2507.16858v1 Announce Type: new 
Abstract: Following the 2024 U.S. presidential election, Democratic lawmakers and their supporters increasingly migrated from mainstream social media plat-forms like X (formerly Twitter) to decentralized alternatives such as Bluesky. This study investigates how Congressional Democrats use Bluesky to form networks of influence and disseminate political messaging in a platform environment that lacks algorithmic amplification. We employ a mixed-methods approach that combines social network analysis, expo-nential random graph modeling (ERGM), and transformer-based topic mod-eling (BERTopic) to analyze follows, mentions, reposts, and discourse pat-terns among 182 verified Democratic members of Congress. Our findings show that while party leaders such as Hakeem Jeffries and Elizabeth War-ren dominate visibility metrics, overlooked figures like Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central positions, suggesting latent influence within the digital party ecosystem. ERGM re-sults reveal significant homophily along ideological, state, and leadership lines, with Senate leadership exhibiting lower connectivity. Topic analysis identifies both shared themes (e.g., reproductive rights, foreign conflicts) and subgroup-specific issues, with The Squad showing the most distinct discourse profile. These results demonstrate the potential of decentralized platforms to reshape intra-party communication dynamics and highlight the need for continued computational research on elite political behavior in emerging digital environments.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Links in LinkedIn: Enhancing Fake Profile Detection in the Age of LLMs</title>
<link>https://arxiv.org/abs/2507.16860</link>
<guid>https://arxiv.org/abs/2507.16860</guid>
<content:encoded><![CDATA[
<div> Language Models, Fake Profile Detection, Adversarial Training, Robustness, GPT<br />
<br />
Summary: 
The study evaluates the robustness of existing fake profile detectors against Large Language Models (LLMs) like GPT. While current detectors are effective against manually created fake profiles, they struggle to detect LLM-generated profiles. The proposed solution involves GPT-assisted adversarial training, which reduces the False Accept Rate significantly without affecting the False Reject Rates. Ablation studies show that detectors utilizing combined numerical and textual embeddings perform the best. Analysis also highlights the importance of automated detectors over prompt-based language models like GPT-4Turbo and human evaluators for detecting fake profiles effectively. The study emphasizes the necessity to enhance fake profile detection mechanisms to combat the risk posed by LLMs in creating realistic fake profiles on platforms like LinkedIn.<br /> <div>
arXiv:2507.16860v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made it easier to create realistic fake profiles on platforms like LinkedIn. This poses a significant risk for text-based fake profile detectors. In this study, we evaluate the robustness of existing detectors against LLM-generated profiles. While highly effective in detecting manually created fake profiles (False Accept Rate: 6-7%), the existing detectors fail to identify GPT-generated profiles (False Accept Rate: 42-52%). We propose GPT-assisted adversarial training as a countermeasure, restoring the False Accept Rate to between 1-7% without impacting the False Reject Rates (0.5-2%). Ablation studies revealed that detectors trained on combined numerical and textual embeddings exhibit the highest robustness, followed by those using numerical-only embeddings, and lastly those using textual-only embeddings. Complementary analysis on the ability of prompt-based GPT-4Turbo and human evaluators affirms the need for robust automated detectors such as the one proposed in this study.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of temporal influence in polarised networks</title>
<link>https://arxiv.org/abs/2507.17177</link>
<guid>https://arxiv.org/abs/2507.17177</guid>
<content:encoded><![CDATA[
<div> influential users, social networks, polarised networks, temporal dynamics, community structure

Summary: 
This study investigates the dynamics of users' influence in polarised social networks, where information is often segregated within communities. It highlights the importance of identifying the most influential users across different communities and acknowledges that users' influence can change over time as opinions evolve. The research compares the stability of influence rankings using temporal centrality measures, considering community structure and network evolution behaviors. The study successfully groups nodes into influence bands and demonstrates how centrality scores can be aggregated to analyze community influence over time. The modified temporal independent cascade model and temporal degree centrality prove to be effective in isolating nodes into their respective influence bands, providing valuable insights into the changing dynamics of user influence in polarized social networks. <div>
arXiv:2507.17177v1 Announce Type: new 
Abstract: In social networks, it is often of interest to identify the most influential users who can successfully spread information to others. This is particularly important for marketing (e.g., targeting influencers for a marketing campaign) and to understand the dynamics of information diffusion (e.g., who is the most central user in the spreading of a certain type of information). However, different opinions often split the audience and make the network polarised. In polarised networks, information becomes soiled within communities in the network, and the most influential user within a network might not be the most influential across all communities. Additionally, influential users and their influence may change over time as users may change their opinion or choose to decrease or halt their engagement on the subject. In this work, we aim to study the temporal dynamics of users' influence in a polarised social network. We compare the stability of influence ranking using temporal centrality measures, while extending them to account for community structure across a number of network evolution behaviours. We show that we can successfully aggregate nodes into influence bands, and how to aggregate centrality scores to analyse the influence of communities over time. A modified version of the temporal independent cascade model and the temporal degree centrality perform the best in this setting, as they are able to reliably isolate nodes into their bands.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quotegraph: A Social Network Extracted from Millions of News Quotations</title>
<link>https://arxiv.org/abs/2507.17626</link>
<guid>https://arxiv.org/abs/2507.17626</guid>
<content:encoded><![CDATA[
<div> network, Quotegraph, social, news articles, computational social scientists  
Summary:  
Quotegraph is a new large-scale social network created from speaker-attributed quotations in English news articles from 2008 to 2020. It comprises 528 thousand unique nodes and 8.63 million directed edges, connecting speakers to the individuals they mention. Nodes are linked to Wikidata entries, providing detailed biographic entity information like nationality, gender, and political affiliation. Derived from Quotebank, Quotegraph includes context information, enriching its relations. The network construction process is language-agnostic, facilitating the creation of similar datasets from non-English news sources. Quotegraph offers valuable insights for computational social scientists, complementing online social networks and offering new perspectives on public figure behavior as portrayed in news media.  

Summary: <div>
arXiv:2507.17626v1 Announce Type: new 
Abstract: We introduce Quotegraph, a novel large-scale social network derived from speaker-attributed quotations in English news articles published between 2008 and 2020. Quotegraph consists of 528 thousand unique nodes and 8.63 million directed edges, pointing from speakers to persons they mention. The nodes are linked to their corresponding items in Wikidata, thereby endowing the dataset with detailed biographic entity information, including nationality, gender, and political affiliation. Being derived from Quotebank, a massive corpus of quotations, relations in Quotegraph are additionally enriched with the information about the context in which they are featured. Each part of the network construction pipeline is language agnostic, enabling the construction of similar datasets based on non-English news corpora. We believe Quotegraph is a compelling resource for computational social scientists, complementary to online social networks, with the potential to yield novel insights into the behavior of public figures and how it is captured in the news.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triadic First-Order Logic Queries in Temporal Networks</title>
<link>https://arxiv.org/abs/2507.17215</link>
<guid>https://arxiv.org/abs/2507.17215</guid>
<content:encoded><![CDATA[
<div> Thresholded First Order Logic, Motif Analysis, Temporal Networks, Algorithm, Triadic Queries<br />
<br />Summary:
In the study of motif counting in network analysis, the concept of thresholded First Order Logic (FOL) Motif Analysis has been introduced for massive temporal networks. This approach combines ideas from logic and database theory to extract richer information from networks by incorporating temporal constraints into motif queries. The FOLTY algorithm was developed to efficiently mine thresholded FOL triadic queries, with theoretical running time matching the best known for temporal triangle counting in sparse graphs. The algorithm's implementation using specialized temporal data structures allows for fast processing, enabling the analysis of networks with millions of edges in a short time frame on standard hardware. This work has the potential to open up new avenues for research in the established field of motif analysis. <div>
arXiv:2507.17215v1 Announce Type: cross 
Abstract: Motif counting is a fundamental problem in network analysis, and there is a rich literature of theoretical and applied algorithms for this problem. Given a large input network $G$, a motif $H$ is a small "pattern" graph indicative of special local structure. Motif/pattern mining involves finding all matches of this pattern in the input $G$. The simplest, yet challenging, case of motif counting is when $H$ has three vertices, often called a "triadic" query. Recent work has focused on "temporal graph mining", where the network $G$ has edges with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of "thresholded First Order Logic (FOL) Motif Analysis" for massive temporal networks. A typical triadic motif query asks for the existence of three vertices that form a desired temporal pattern. An "FOL" motif query is obtained by having both existential and thresholded universal quantifiers. This allows for query semantics that can mine richer information from networks. A typical triadic query would be "find all triples of vertices $u,v,w$ such that they form a triangle within one hour". A thresholded FOL query can express "find all pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$ also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic queries. The theoretical running time of FOLTY matches the best known running time for temporal triangle counting in sparse graphs. We give an efficient implementation of FOLTY using specialized temporal data structures. FOLTY has excellent empirical behavior, and can answer triadic FOL queries on graphs with nearly 70M edges is less than hour on commodity hardware. Our work has the potential to start a new research direction in the classic well-studied problem of motif analysis.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Maximization in Temporal Networks with Persistent and Reactive Behaviors</title>
<link>https://arxiv.org/abs/2412.20936</link>
<guid>https://arxiv.org/abs/2412.20936</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence maximization, temporal social networks, dynamic interactions, active-inactive transitions, seed selection

Summary:<br />
Influence maximization in temporal social networks is challenging due to dynamic interactions and active-inactive transitions among nodes. The Continuous Persistent Susceptible-Infected Model with Reinforcement and Re-activation (cpSI-R) addresses this by incorporating active-inactive transitions and node reinforcement, improving influence spread understanding. This model leads to a submodular and monotone objective function, allowing for efficient seed selection optimization. A temporal snapshot sampling method simplifies network analysis, enhancing seed selection efficiency. Adapting prior seed selection algorithms to the cpSI-R model and sampling strategy leads to reduced computational costs and improved performance. Experimental evaluations on various datasets show significant performance enhancements over baseline methods, proving the effectiveness of cpSI-R for real-world temporal networks.<br /><br />Summary: <div>
arXiv:2412.20936v2 Announce Type: replace 
Abstract: Influence maximization in temporal social networks presents unique challenges due to the dynamic interactions that evolve over time. Traditional diffusion models often fall short in capturing the real-world complexities of active-inactive transitions among nodes, obscuring the true behavior of influence spread. In dynamic networks, nodes do not simply transition to an active state once; rather, they can oscillate between active and inactive states, with the potential for reactivation and reinforcement over time. This reactivation allows previously influenced nodes to regain influence potency, enhancing their ability to spread influence to others and amplifying the overall diffusion process. Ignoring these transitions can thus conceal the cumulative impact of influence, making it essential to account for them in any effective diffusion model. To address these challenges, we introduce the Continuous Persistent Susceptible-Infected Model with Reinforcement and Re-activation (cpSI-R), which explicitly incorporates active-inactive transitions, capturing the progressive reinforcement that makes nodes more potent spreaders upon reactivation. This model naturally leads to a submodular and monotone objective function, which supports efficient optimization for seed selection in influence maximization tasks. Alongside cpSI-R, we propose an efficient temporal snapshot sampling method, simplifying the analysis of evolving networks. We then adapt the prior algorithms of seed selection to our model and sampling strategy, resulting in reduced computational costs and enhanced seed selection efficiency. Experimental evaluations on diverse datasets demonstrate substantial improvements in performance over baseline methods, underscoring the effectiveness of cpSI-R for real-world temporal networks
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Matching in Correlated Networks with Node Attributes for Improved Community Recovery</title>
<link>https://arxiv.org/abs/2501.02851</link>
<guid>https://arxiv.org/abs/2501.02851</guid>
<content:encoded><![CDATA[
<div> community detection, multiple networks, correlated node attributes, Stochastic Block Model, Gaussian Mixture Models

Summary:
The study investigates community detection in multiple networks with correlated node attributes and edges, such as social platforms. They introduce the correlated Contextual Stochastic Block Model (CSBM) to incorporate both structural and attribute correlations across graphs. By analyzing correlated Gaussian Mixture Models, they establish conditions for exact node matching using attribute distances. Their two-step algorithm combines edge information and attribute distances through $k$-core matching to accurately identify node correspondence and merge correlated edges for improved community detection. The research shows how incorporating side information from correlated graphs can make community detection feasible even in cases where it may be impossible in a single graph. This approach leverages the interplay between graph matching and community recovery to enhance performance in attribute-based community detection across multiple networks. <br /><br />Summary: <div>
arXiv:2501.02851v2 Announce Type: replace 
Abstract: We study community detection in multiple networks with jointly correlated node attributes and edges. This setting arises naturally in applications such as social platforms, where a shared set of users may exhibit both correlated friendship patterns and correlated attributes across different platforms. Extending the classical Stochastic Block Model (SBM) and its contextual counterpart (Contextual SBM or CSBM), we introduce the correlated CSBM, which incorporates structural and attribute correlations across graphs. To build intuition, we first analyze correlated Gaussian Mixture Models, wherein only correlated node attributes are available without edges, and identify the conditions under which an estimator minimizing the distance between attributes achieves exact matching of nodes across the two databases. For the correlated CSBMs, we develop a two-step procedure that first applies $k$-core matching to most nodes using edge information, then refines the matching for the remaining unmatched nodes by leveraging their attributes with a distance-based estimator. We identify the conditions under which the algorithm recovers the exact node correspondence, enabling us to merge the correlated edges and average the correlated attributes for enhanced community detection. Crucially, by aligning and combining graphs, we identify regimes in which community detection is impossible in a single graph but becomes feasible when side information from correlated graphs is incorporated. Our results illustrate how the interplay between graph matching and community recovery can boost performance, broadening the scope of multi-graph, attribute-based community detection.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image memorability predicts social media virality and externally-associated commenting</title>
<link>https://arxiv.org/abs/2409.14659</link>
<guid>https://arxiv.org/abs/2409.14659</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, image memorability, virality, neural network, online engagement <br />
<br />
Summary: <br />
This study explores the connection between image memorability and social media virality. By analyzing over 1,200 Reddit image posts, the researchers found that memorable images tend to attract more engagement, specifically in the form of comments. They used a neural network called ResMem to predict image memorability and correlated these scores with virality metrics. Memorable images were linked to neutral-affect comments, indicating a unique pathway to virality compared to emotion-driven content. The study also revealed that visually consistent, memorable posts prompted a variety of externally-associated comments. Analysis of ResMem's layers emphasized the importance of semantic distinctiveness in both memorability and virality, even when accounting for image categories. Overall, the findings suggest that image memorability can be a valuable predictor of online engagement on social media platforms. <br /> <div>
arXiv:2409.14659v2 Announce Type: replace-cross 
Abstract: Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images are consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional contents. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found that semantic distinctiveness was key to both memorability and virality even after accounting for image category effects. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief Alignment vs Opinion Leadership: Understanding Cross-linguistic Digital Activism in K-pop and BLM Communities</title>
<link>https://arxiv.org/abs/2507.16046</link>
<guid>https://arxiv.org/abs/2507.16046</guid>
<content:encoded><![CDATA[
<div> activism, internet, K-pop fans, Black Lives Matter, belief alignment
Summary: 
- The study explores the motivations behind cross-cultural activism, focusing on the interaction between K-pop fans and the Black Lives Matter movement on Twitter after George Floyd's murder.
- It suggests that belief alignment, where individuals resonate with shared beliefs, drives cross-cultural interactions in digital activism.
- The study indicates that influential online opinion leaders, such as K-pop entertainers, may amplify activism through their actions, but are not the direct cause of activism.
- The findings show a slight increase in belief similarity between BLM and K-pop fans following their interaction on Twitter.
- Overall, the research highlights the importance of belief resonance in driving global movements, transcending geographical boundaries through online platforms. 
<br /><br />Summary: <div>
arXiv:2507.16046v1 Announce Type: new 
Abstract: The internet has transformed activism, giving rise to more organic, diverse, and dynamic social movements that transcend geo-political boundaries. Despite extensive research on the role of social media and the internet in cross-cultural activism, the fundamental motivations driving these global movements remain poorly understood. This study examines two plausible explanations for cross-cultural activism: first, that it is driven by influential online opinion leaders, and second, that it results from individuals resonating with emergent sets of beliefs, values, and norms. We conduct a case study of the interaction between K-pop fans and the Black Lives Matter (BLM) movement on Twitter following the murder of George Floyd. Our findings provide strong evidence that belief alignment, where people resonate with common beliefs, is a primary driver of cross-cultural interactions in digital activism. We also demonstrate that while the actions of potential opinion leaders--in this case, K-pop entertainers--may amplify activism and lead to further expressions of love and admiration from fans, they do not appear to be a direct cause of activism. Finally, we report some initial evidence that the interaction between BLM and K-pop led to slight increases in their overall belief similarity.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections</title>
<link>https://arxiv.org/abs/2507.16298</link>
<guid>https://arxiv.org/abs/2507.16298</guid>
<content:encoded><![CDATA[
<div> WhatsApp tiplines, misinformation, fact-checkers, Indian assembly elections, content analysis

Summary:
The study examines WhatsApp tiplines used during the 2021 Indian assembly elections to combat misinformation. It analyzes 580 claims from 451 users in English, Hindi, and Telugu, categorizing them into election, COVID-19, and other topics. Similarities in claims are found across languages, with some users submitting tips in multiple languages. Fact-checkers take a couple of days on average to debunk claims. Users do not submit claims to multiple fact-checking organizations, indicating unique audiences. Practical recommendations for using tiplines during elections with ethical considerations for users' information are provided. <div>
arXiv:2507.16298v1 Announce Type: new 
Abstract: WhatsApp tiplines, first launched in 2019 to combat misinformation, enable users to interact with fact-checkers to verify misleading content. This study analyzes 580 unique claims (tips) from 451 users, covering both high-resource languages (English, Hindi) and a low-resource language (Telugu) during the 2021 Indian assembly elections using a mixed-method approach. We categorize the claims into three categories, election, COVID-19, and others, and observe variations across languages. We compare content similarity through frequent word analysis and clustering of neural sentence embeddings. We also investigate user overlap across languages and fact-checking organizations. We measure the average time required to debunk claims and inform tipline users. Results reveal similarities in claims across languages, with some users submitting tips in multiple languages to the same fact-checkers. Fact-checkers generally require a couple of days to debunk a new claim and share the results with users. Notably, no user submits claims to multiple fact-checking organizations, indicating that each organization maintains a unique audience. We provide practical recommendations for using tiplines during elections with ethical consideration of users' information.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SASH: Decoding Community Structure in Graphs</title>
<link>https://arxiv.org/abs/2507.16583</link>
<guid>https://arxiv.org/abs/2507.16583</guid>
<content:encoded><![CDATA[
<div> Algorithm, Community detection, Graph, Error correcting codes, Simulations
Summary:
The paper introduces an encoding method for community structure in graphs and presents a decoding algorithm called SASH to estimate communities from observed data. The problem of detecting communities in a graph is important and has various applications. The approach views a graph as a noisy version of the underlying communities and utilizes error correcting codes. SASH is demonstrated through simulations on an assortative planted partition model and Zachary's Karate Club dataset, showing its performance in identifying densely connected clusters of vertices. Overall, the study provides a novel perspective on community detection in graphs and offers a promising algorithm for accurately decoding community structures from noisy data.<br /><br />Summary: <div>
arXiv:2507.16583v1 Announce Type: new 
Abstract: Detection of communities in a graph entails identifying clusters of densely connected vertices; the area has a variety of important applications and a rich literature. The problem has previously been situated in the realm of error correcting codes by viewing a graph as a noisy version of the assumed underlying communities. In this paper, we introduce an encoding of community structure along with the resulting code's parameters. We then present a novel algorithm, SASH, to decode to estimated communities given an observed dataset. We demonstrate the performance of SASH via simulations on an assortative planted partition model and on the Zachary's Karate Club dataset.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Data-centric Overview of Federated Graph Learning</title>
<link>https://arxiv.org/abs/2507.16541</link>
<guid>https://arxiv.org/abs/2507.16541</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Graph Learning, Data Characteristics, Data Utilization, Pretrained Large Models, Realistic Applications

Summary: 
Federated Graph Learning (FGL) is a solution for reconciling decentralized datasets while preserving sensitive information. This survey introduces a two-level data-centric taxonomy for FGL research, focusing on Data Characteristics and Data Utilization. Data Characteristics categorize studies based on dataset properties, while Data Utilization analyzes training procedures to overcome data-centric challenges. The survey also examines FGL integration with Pretrained Large Models, showcases realistic applications, and suggests future research directions aligned with Graph Machine Learning trends. Ultimately, this comprehensive review aims to organize FGL research around data-centric constraints to enhance model performance. 

<br /><br />Summary: <div>
arXiv:2507.16541v1 Announce Type: cross 
Abstract: In the era of big data applications, Federated Graph Learning (FGL) has emerged as a prominent solution that reconcile the tradeoff between optimizing the collective intelligence between decentralized datasets holders and preserving sensitive information to maximum. Existing FGL surveys have contributed meaningfully but largely focus on integrating Federated Learning (FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that emphasis on methodology and simulated scenarios. Notably, a data centric perspective, which systematically examines FGL methods through the lens of data properties and usage, remains unadapted to reorganize FGL research, yet it is critical to assess how FGL studies manage to tackle data centric constraints to enhance model performances. This survey propose a two-level data centric taxonomy: Data Characteristics, which categorizes studies based on the structural and distributional properties of datasets used in FGL, and Data Utilization, which analyzes the training procedures and techniques employed to overcome key data centric challenges. Each taxonomy level is defined by three orthogonal criteria, each representing a distinct data centric configuration. Beyond taxonomy, this survey examines FGL integration with Pretrained Large Models, showcases realistic applications, and highlights future direction aligned with emerging trends in GML.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Analytics for Anti-Money Laundering -- A Systematic Literature Review and Experimental Evaluation</title>
<link>https://arxiv.org/abs/2405.19383</link>
<guid>https://arxiv.org/abs/2405.19383</guid>
<content:encoded><![CDATA[
<div> Keywords: money laundering, network analytics, anti-money laundering, deep learning, fraud analytics

Summary: 
This study reviews existing research on using network analytics for anti-money laundering (AML) to combat the pervasive issue of money laundering funding illegal activities. The literature review covers 97 papers and presents a taxonomy following a fraud analytics framework. The research found that while most studies rely on expert-based rules and manual features, deep learning methods are gaining popularity. A comprehensive framework for evaluating and comparing the performance of different methods was also developed. The study compared manual feature engineering, random walk-based, and deep learning methods on two public datasets, concluding that network analytics enhances predictive power but caution is needed in applying Graph Neural Networks (GNNs) due to class imbalance and network topology issues. The study warns against relying on synthetic data for overly optimistic results. An open-source implementation is provided to facilitate further research and standardize the analysis and evaluation of network analytics for AML. 

<br /><br />Summary: <div>
arXiv:2405.19383v4 Announce Type: replace 
Abstract: Money laundering presents a pervasive challenge, burdening society by financing illegal activities. The use of network information is increasingly being explored to effectively combat money laundering, given it involves connected parties. This led to a surge in research on network analytics for anti-money laundering (AML). The literature is, however, fragmented and a comprehensive overview of existing work is missing. This results in limited understanding of the methods to apply and their comparative detection power. This paper presents an extensive and unique literature review, based on 97 papers from Web of Science and Scopus, resulting in a taxonomy following a recently proposed fraud analytics framework. We conclude that most research relies on expert-based rules and manual features, while deep learning methods have been gaining traction. This paper also presents a comprehensive framework to evaluate and compare the performance of prominent methods in a standardized setup. We compare manual feature engineering, random walk-based, and deep learning methods on two publicly available data sets. We conclude that (1) network analytics increases the predictive power, but caution is needed when applying GNNs in the face of class imbalance and network topology, and that (2) care should be taken with synthetic data as this can give overly optimistic results. The open-source implementation facilitates researchers and practitioners to extend this work on proprietary data, promoting a standardised approach for the analysis and evaluation of network analytics for AML.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why We Experience Society Differently: Intrinsic Dispositions as Drivers of Ideological Complexity in Adaptive Social Networks</title>
<link>https://arxiv.org/abs/2504.07848</link>
<guid>https://arxiv.org/abs/2504.07848</guid>
<content:encoded><![CDATA[
<div> behavioral tendencies, opinion dynamics, complexity, social networks, inequality

Summary:
- The study focuses on understanding how inequality emerges in complex systems by considering both structural dynamics and intrinsic heterogeneity in opinion dynamics.
- Traditional models overlook how diverse cognitive dispositions influence belief evolution, instead assuming homogeneous agent behavior.
- An adaptive social network model is analyzed, where agents exhibit one of three behavioral tendencies - homophily, neophily, or social conformity.
- The study measures individual opinion trajectories using normalized Lempel-Ziv complexity and finds counterintuitive patterns such as homophilic agents becoming unpredictable, neophilic agents stabilizing, and conformic agents showing a U-shaped trajectory.
- The results show that internal behavioral dispositions primarily govern long-term opinion unpredictability rather than external environmental factors.
- Individuals' experiences of ideological volatility or stability are self-structured through their own cognitive tendencies, leading to persistent disparities in dynamical experience within social systems. <div>
arXiv:2504.07848v2 Announce Type: replace 
Abstract: Understanding the emergence of inequality in complex systems requires attention to both structural dynamics and intrinsic heterogeneity. In the context of opinion dynamics, traditional models relied on static snapshots or assumed homogeneous agent behavior, overlooking how diverse cognitive dispositions shape belief evolution. While some recent models introduce behavioral heterogeneity, they typically focus on macro-level patterns, neglecting the unequal and individualized dynamics that unfold at the agent level. In this study, we analyze an adaptive social network model where each agent exhibits one of three behavioral tendencies-homophily, neophily (attention to novelty), or social conformity-and measure the complexity of individual opinion trajectories using normalized Lempel-Ziv (nLZ) complexity. We find that the resulting dynamics are often counterintuitive-homophilic agents, despite seeking similarity, become increasingly unpredictable; neophilic agents, despite pursuing novelty, stabilize; and conformic agents follow a U-shaped trajectory, transitioning from early stability to later unpredictability. More fundamentally, these patterns remain robust across diverse network settings, showing that internal behavioral dispositions - not external environment - primarily govern long-term opinion unpredictability. The broader implication is that individuals' experiences of ideological volatility, uncertainty, or stability are not merely environmental, but endogenously self-structured through their own cognitive tendencies. These results establish a novel individual-level lens on opinion dynamics, where the behavioral identity of agents serves as a dynamical fingerprint in the evolution of belief systems, and gives rise to persistent disparities in dynamical experience within self-organizing social systems, even in structurally similar environments.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study</title>
<link>https://arxiv.org/abs/2502.02451</link>
<guid>https://arxiv.org/abs/2502.02451</guid>
<content:encoded><![CDATA[
<div> machine translation, local lexicon, multilingual language models, large language models, moral foundations

Summary:<br />
- The study examines computational methods for analyzing moral foundations (MFs) in non-English texts, using Chinese as a test case.
- Machine translation and local lexicons are not effective for capturing cultural nuances in moral assessments.
- Multilingual language models and large language models (LLMs) show promise in cross-language MF measurement with transfer learning.
- LLMs are particularly efficient in data usage for this task.
- It is crucial to have human validation in automated MF assessment to ensure cultural nuances are not overlooked.
- The study emphasizes the potential of LLMs for cross-language MF measurements and other complex multilingual coding tasks. 

Summary: <div>
arXiv:2502.02451v3 Announce Type: replace-cross 
Abstract: This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discipline and Resistance: The Construction of a Digital Home for TikTok Refugees on Xiaohongshu</title>
<link>https://arxiv.org/abs/2507.14465</link>
<guid>https://arxiv.org/abs/2507.14465</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, Xiaohongshu, heterotopia, cross-cultural discourse, digital migration

Summary: 
This study explores the migration of TikTok users to Xiaohongshu following a potential ban of TikTok in the US, utilizing Foucault's concept of heterotopia to analyze the platform as a crisis space for cross-cultural discussions. Through Critical Discourse Analysis of 586 user comments, the study uncovers how both Chinese and international users collaborated in constructing and challenging a new online order through language negotiation, identity positioning, and playful platform policing. The research highlights unique discursive tactics employed by domestic and overseas users, showcasing a blend of cultural resistance and adaptation. By shedding light on digital migration, heterotopic spaces in social media, and evolving dynamics of cross-cultural discourse amid geopolitical turmoil, this study offers valuable insights into the complexities of online interaction in a globalized world. <div>
arXiv:2507.14465v1 Announce Type: new 
Abstract: This study examines how TikTok refugees moved to Xiaohongshu after TikTok was about to be banned in the United States. It utilizes Foucault's idea of heterotopia to demonstrate how Xiaohongshu became a crisis space for cross-cultural discussions across the Great Firewall. Through Critical Discourse Analysis of 586 user comments, the study reveals how Chinese and international users collaboratively constructed and contested a new online order through language negotiation, identity positioning, and playful platform policing. The findings highlight distinct discursive strategies between domestic and overseas users, reflecting both cultural resistance and adaptation. This research contributes to the understanding of digital migration, heterotopic spaces in social media, and emerging dynamics of cross-cultural discourse during geopolitical crises.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rejection or Inclusion in the Emotion-Identity Dynamics of TikTok Refugees on RedNote</title>
<link>https://arxiv.org/abs/2507.14623</link>
<guid>https://arxiv.org/abs/2507.14623</guid>
<content:encoded><![CDATA[
<div> sentiment analysis, cross-cultural interactions, social media, Chinese users, TikTok Refugees  

Summary:  
- This study explores cross-cultural interactions between Chinese users and "TikTok Refugees" on RedNote after TikTok's U.S. ban, using sentiment analysis and topic modeling.  
- Analysis of 1,862 posts and 403,054 comments reveals emotional asymmetry with Chinese users showing pride and praise in cultural discussions, while political topics evoke contempt and anger, particularly from Pro-China users.  
- Pro-Foreign users exhibit strong negative emotions across all topics, while neutral users express curiosity and joy within mainstream norms.  
- Appearance-related content fosters emotionally balanced interactions, while political discussions lead to high polarization.  
- The findings highlight distinct emotion-stance structures in Sino-foreign online interactions, shedding light on identity negotiation in transnational digital publics.  

<br /><br /> <div>
arXiv:2507.14623v1 Announce Type: new 
Abstract: This study examines cross-cultural interactions between Chinese users and self-identified "TikTok Refugees"(foreign users who migrated to RedNote after TikTok's U.S. ban). Based on a dataset of 1,862 posts and 403,054 comments, we use large language model-based sentiment classification and BERT-based topic modelling to explore how both groups engage with the TikTok refugee phenomenon. We analyse what themes foreign users express, how Chinese users respond, how stances (Pro-China, Neutral, Pro-Foreign) shape emotional expression, and how affective responses differ across topics and identities. Results show strong affective asymmetry: Chinese users respond with varying emotional intensities across topics and stances: pride and praise dominate cultural threads, while political discussions elicit high levels of contempt and anger, especially from Pro-China commenters. Pro-Foreign users exhibit the strongest negative emotions across all topics, whereas neutral users express curiosity and joy but still reinforce mainstream discursive norms. Cross-topic comparisons reveal that appearance-related content produces the most emotionally balanced interactions, while politics generates the highest polarization. Our findings reveal distinct emotion-stance structures in Sino-foreign online interactions and offer empirical insights into identity negotiation in transnational digital publics.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Faculty Placement from Patterns in Co-authorship Networks</title>
<link>https://arxiv.org/abs/2507.14696</link>
<guid>https://arxiv.org/abs/2507.14696</guid>
<content:encoded><![CDATA[
<div> faculty hiring, academic placement, co-authorship networks, institutional prestige, predictive accuracy

Summary:
- Faculty hiring plays a crucial role in shaping academic progress and individual career paths.
- Traditional indicators like doctoral department prestige and publication record are not sufficient predictors of faculty placement outcomes.
- The study introduces a novel approach by considering faculty placement as an individual-level prediction task.
- Analysis of temporal co-authorship networks reveals a significant improvement in predictive accuracy, especially for placements at top-tier departments.
- The findings highlight the importance of social networks, professional endorsements, and implicit advocacy in faculty hiring decisions, beyond traditional measures of productivity and prestige.

<br /><br />Summary: <div>
arXiv:2507.14696v1 Announce Type: new 
Abstract: Faculty hiring shapes the flow of ideas, resources, and opportunities in academia, influencing not only individual career trajectories but also broader patterns of institutional prestige and scientific progress. While traditional studies have found strong correlations between faculty hiring and attributes such as doctoral department prestige and publication record, they rarely assess whether these associations generalize to individual hiring outcomes, particularly for future candidates outside the original sample. Here, we consider faculty placement as an individual-level prediction task. Our data consist of temporal co-authorship networks with conventional attributes such as doctoral department prestige and bibliometric features. We observe that using the co-authorship network significantly improves predictive accuracy by up to 10% over traditional indicators alone, with the largest gains observed for placements at the most elite (top-10) departments. Our results underscore the role that social networks, professional endorsements, and implicit advocacy play in faculty hiring beyond traditional measures of scholarly productivity and institutional prestige. By introducing a predictive framing of faculty placement and establishing the benefit of considering co-authorship networks, this work provides a new lens for understanding structural biases in academia that could inform targeted interventions aimed at increasing transparency, fairness, and equity in academic hiring practices.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Algorithms for Relevant Quantities of Friedkin-Johnsen Opinion Dynamics Model</title>
<link>https://arxiv.org/abs/2507.14864</link>
<guid>https://arxiv.org/abs/2507.14864</guid>
<content:encoded><![CDATA[
<div> Keywords: Online social networks, Friedkin-Johnsen model, equilibrium opinion vector, polarization, disagreement

Summary: 
The paper introduces a computational approach for efficiently determining the equilibrium opinion vector, polarization, and disagreement in online social networks. The Friedkin-Johnsen model is utilized as the basis for modeling opinion formation dynamics in both directed and undirected networks. A deterministic local algorithm with relative error guarantees is proposed, capable of scaling to networks with over ten million nodes. Integration with successive over-relaxation techniques further accelerates the computation process by optimizing convergence rates. Extensive experiments conducted on real-world networks demonstrate the practical effectiveness of the proposed approaches, showcasing significant improvements in computational efficiency and scalability compared to traditional methods. The strategies developed in this work have the potential to enhance our understanding of opinion dynamics in complex social networks and contribute to the development of more effective strategies for managing and analyzing online discourse. 

<br /><br />Summary: <div>
arXiv:2507.14864v1 Announce Type: new 
Abstract: Online social networks have become an integral part of modern society, profoundly influencing how individuals form and exchange opinions across diverse domains ranging from politics to public health. The Friedkin-Johnsen model serves as a foundational framework for modeling opinion formation dynamics in such networks. In this paper, we address the computational task of efficiently determining the equilibrium opinion vector and associated metrics including polarization and disagreement, applicable to both directed and undirected social networks. We propose a deterministic local algorithm with relative error guarantees, scaling to networks exceeding ten million nodes. Further acceleration is achieved through integration with successive over-relaxation techniques, where a relaxation factor optimizes convergence rates. Extensive experiments on diverse real-world networks validate the practical effectiveness of our approaches, demonstrating significant improvements in computational efficiency and scalability compared to conventional methods.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehensive Privacy Risk Assessment in Social Networks Using User Attributes Social Graphs and Text Analysis</title>
<link>https://arxiv.org/abs/2507.15124</link>
<guid>https://arxiv.org/abs/2507.15124</guid>
<content:encoded><![CDATA[
<div> framework, privacy risk, social networking, user attributes, content 

Summary:
The Comprehensive Privacy Risk Scoring (CPRS) framework quantifies privacy risk by combining user attributes, social graph structures, and user-generated content. It calculates risk scores based on sensitivity, visibility, structural similarity, and entity-level analysis, culminating in a unified risk score. Validation on real-world datasets showed an average CPRS of 0.478, with graph-based risks posing a higher threat than content or profile attributes. Attributes such as email, date of birth, and mobile number were identified as high-risk factors. A user study demonstrated that 85% found the CPRS dashboard clear and actionable, affirming its practicality. This framework offers personalized privacy risk insights and a comprehensive approach to privacy management, with future plans to incorporate temporal dynamics and multimodal content for wider applicability.<br /><br />Summary: <div>
arXiv:2507.15124v1 Announce Type: new 
Abstract: The rise of social networking platforms has amplified privacy threats as users increasingly share sensitive information across profiles, content, and social connections. We present a Comprehensive Privacy Risk Scoring (CPRS) framework that quantifies privacy risk by integrating user attributes, social graph structures, and user-generated content. Our framework computes risk scores across these dimensions using sensitivity, visibility, structural similarity, and entity-level analysis, then aggregates them into a unified risk score. We validate CPRS on two real-world datasets: the SNAP Facebook Ego Network (4,039 users) and the Koo microblogging dataset (1M posts, 1M comments). The average CPRS is 0.478 with equal weighting, rising to 0.501 in graph-sensitive scenarios. Component-wise, graph-based risks (mean 0.52) surpass content (0.48) and profile attributes (0.45). High-risk attributes include email, date of birth, and mobile number. Our user study with 100 participants shows 85% rated the dashboard as clear and actionable, confirming CPRS's practical utility. This work enables personalized privacy risk insights and contributes a holistic, scalable methodology for privacy management. Future directions include incorporating temporal dynamics and multimodal content for broader applicability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Multimodal News Recommendation through Federated Learning</title>
<link>https://arxiv.org/abs/2507.15460</link>
<guid>https://arxiv.org/abs/2507.15460</guid>
<content:encoded><![CDATA[
<div> Multimodal, federated learning, news recommendation, privacy preservation, user interests <br />
<br />
Summary: This paper introduces a novel multimodal federated learning-based approach for personalized news recommendation. It addresses challenges faced by traditional systems by integrating textual and visual features of news items, balancing users' long-term and short-term interests, and enhancing privacy through a federated learning framework. The approach combines a multimodal model for more comprehensive content representation with a time-aware model using multi-head self-attention networks to improve recommendation accuracy. The federated learning framework divides the recommendation model into a server-maintained news model and a user model shared between the server and clients, ensuring collaborative model training without sharing user data. Additionally, a secure aggregation algorithm based on Shamir's secret sharing is employed to further safeguard user privacy. Experimental results on a real-world news dataset demonstrate significantly improved performance compared to existing systems, making it a substantial advancement in privacy-preserving personalized news recommendation. <br /> <div>
arXiv:2507.15460v1 Announce Type: new 
Abstract: Personalized News Recommendation systems (PNR) have emerged as a solution to information overload by predicting and suggesting news items tailored to individual user interests. However, traditional PNR systems face several challenges, including an overreliance on textual content, common neglect of short-term user interests, and significant privacy concerns due to centralized data storage. This paper addresses these issues by introducing a novel multimodal federated learning-based approach for news recommendation. First, it integrates both textual and visual features of news items using a multimodal model, enabling a more comprehensive representation of content. Second, it employs a time-aware model that balances users' long-term and short-term interests through multi-head self-attention networks, improving recommendation accuracy. Finally, to enhance privacy, a federated learning framework is implemented, enabling collaborative model training without sharing user data. The framework divides the recommendation model into a large server-maintained news model and a lightweight user model shared between the server and clients. The client requests news representations (vectors) and a user model from the central server, then computes gradients with user local data, and finally sends their locally computed gradients to the server for aggregation. The central server aggregates gradients to update the global user model and news model. The updated news model is further used to infer news representation by the server. To further safeguard user privacy, a secure aggregation algorithm based on Shamir's secret sharing is employed. Experiments on a real-world news dataset demonstrate strong performance compared to existing systems, representing a significant advancement in privacy-preserving personalized news recommendation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apology of Green Digitalization in the Context of Information and Climate Feedback Theory</title>
<link>https://arxiv.org/abs/2507.14162</link>
<guid>https://arxiv.org/abs/2507.14162</guid>
<content:encoded><![CDATA[
<div> digitalization, energy consumption, climate feedback, sustainable digitalization, Green Digital Accord
Summary:
The paper introduces the theory of information and climate feedback (ICF), which examines the impact of digitalization on the environment. It highlights the overlooked influence of information and communication technologies on the biosphere's thermal and energy balance. The ICF model, represented by differential equations, explores the interconnected relationship between digitalization, energy consumption, thermal footprint, climatic response, and infrastructure vulnerability. Through numerical analysis and thermal mapping, critical scenarios like digital overheating and infrastructural collapse are identified. The paper concludes with a call for the Green Digital Accord, an international agreement promoting sustainable digitalization. This interdisciplinary work combines climatology, information technologies, and political economy to address the environmental challenges posed by accelerating digitalization. 
<br /><br />Summary: <div>
arXiv:2507.14162v1 Announce Type: cross 
Abstract: Amid accelerated digitalization, not only is the scale of data processing and storage increasing, but so too is the associated infrastructure load on the climate. Current climate models and environmental protocols almost entirely overlook the impact of information and communication technologies on the thermal and energy balance of the biosphere.
  This paper proposes the theory of information and climate feedback (ICF) as a new nonlinear model describing the loop of digitalization, energy consumption, the thermal footprint, the climatic response, and the vulnerability of digital infrastructure. The system is formalized via differential equations with delays and parameters of sensitivity, greenness, and phase stability.
  A multiscenario numerical analysis, phase reconstructions, and thermal cartography were conducted. Critical regimes, including digital overheating, fluctuational instability, and infrastructural collapse in the absence of adaptive measures, were identified.
  The paper concludes with the proposal of an international agreement titled the Green Digital Accord and a set of metrics for sustainable digitalization. This work integrates climatology, information technologies, and the political economy of sustainability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module</title>
<link>https://arxiv.org/abs/2507.14612</link>
<guid>https://arxiv.org/abs/2507.14612</guid>
<content:encoded><![CDATA[
<div> keywords: POI recommendation, time information, graph representation, weighting factors, performance improvement
<br />
Summary: 
The paper proposes a novel framework, GDPW, for next point of interest (POI) recommendation that considers POI category information and multiple weighting factors. The framework utilizes global category and category-time graphs to learn category and time representations, disentangling them through contrastive learning. It incorporates weighting information such as POI popularity, transition relationships, and distances between POIs to improve prediction accuracy. By jointly considering POI categories and time information, GDPW addresses existing limitations in capturing user tendencies and time continuity. Experimental results on real-world datasets show that GDPW outperforms other models, achieving a performance improvement of 3% to 11%. <div>
arXiv:2507.14612v1 Announce Type: cross 
Abstract: Next point of interest (POI) recommendation primarily predicts future activities based on users' past check-in data and current status, providing significant value to users and service providers. We observed that the popular check-in times for different POI categories vary. For example, coffee shops are crowded in the afternoon because people like to have coffee to refresh after meals, while bars are busy late at night. However, existing methods rarely explore the relationship between POI categories and time, which may result in the model being unable to fully learn users' tendencies to visit certain POI categories at different times. Additionally, existing methods for modeling time information often convert it into time embeddings or calculate the time interval and incorporate it into the model, making it difficult to capture the continuity of time. Finally, during POI prediction, various weighting information is often ignored, such as the popularity of each POI, the transition relationships between POIs, and the distances between POIs, leading to suboptimal performance. To address these issues, this paper proposes a novel next POI recommendation framework called Graph Disentangler with POI Weighted Module (GDPW). This framework aims to jointly consider POI category information and multiple POI weighting factors. Specifically, the proposed GDPW learns category and time representations through the Global Category Graph and the Global Category-Time Graph. Then, we disentangle category and time information through contrastive learning. After prediction, the final POI recommendation for users is obtained by weighting the prediction results based on the transition weights and distance relationships between POIs. We conducted experiments on two real-world datasets, and the results demonstrate that the proposed GDPW outperforms other existing models, improving performance by 3% to 11%.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model</title>
<link>https://arxiv.org/abs/2507.15067</link>
<guid>https://arxiv.org/abs/2507.15067</guid>
<content:encoded><![CDATA[
<div> transformer-based classification model, bad actor detection, robustness, adversarial attacks, sequence embedding  
Summary:  
- The study focuses on developing a robust deep learning model, ROBAD, for detecting bad actors on internet platforms.  
- ROBAD utilizes transformer encoder and decoder blocks to capture local and global information from user posts.  
- The model leverages sequence embeddings to detect potential modifications in input sequences.  
- By incorporating mimicked behaviors of bad actors in training, ROBAD enhances its knowledge and robustness against adversarial attacks.  
- Experimental results on Yelp and Wikipedia datasets demonstrate ROBAD's effectiveness in detecting bad actors under state-of-the-art adversarial attacks.  

<br /><br />Summary: <div>
arXiv:2507.15067v1 Announce Type: cross 
Abstract: Detecting bad actors is critical to ensure the safety and integrity of internet platforms. Several deep learning-based models have been developed to identify such users. These models should not only accurately detect bad actors, but also be robust against adversarial attacks that aim to evade detection. However, past deep learning-based detection models do not meet the robustness requirement because they are sensitive to even minor changes in the input sequence. To address this issue, we focus on (1) improving the model understanding capability and (2) enhancing the model knowledge such that the model can recognize potential input modifications when making predictions. To achieve these goals, we create a novel transformer-based classification model, called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection model), which uses the sequence of user posts to generate user embedding to detect bad actors. Particularly, ROBAD first leverages the transformer encoder block to encode each post bidirectionally, thus building a post embedding to capture the local information at the post level. Next, it adopts the transformer decoder block to model the sequential pattern in the post embeddings by using the attention mechanism, which generates the sequence embedding to obtain the global information at the sequence level. Finally, to enrich the knowledge of the model, embeddings of modified sequences by mimicked attackers are fed into a contrastive-learning-enhanced classification layer for sequence prediction. In essence, by capturing the local and global information (i.e., the post and sequence information) and leveraging the mimicked behaviors of bad actors in training, ROBAD can be robust to adversarial attacks. Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can effectively detect bad actors when under state-of-the-art adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Homophily and Heterophily in Multimodal Graph Clustering</title>
<link>https://arxiv.org/abs/2507.15253</link>
<guid>https://arxiv.org/abs/2507.15253</guid>
<content:encoded><![CDATA[
<div> framework, multimodal graph clustering, Disentangled Multimodal Graph Clustering, dual-frequency fusion, self-supervised alignment  
Summary:  
The article introduces the Disentangled Multimodal Graph Clustering (DMGC) framework for unsupervised learning on multimodal graphs. These graphs combine structured interconnections with heterogeneous data, exhibiting both homophilic and heterophilic relationships. The DMGC framework decomposes the hybrid graph into homophily-enhanced and heterophily-aware graphs, integrating them through a Multimodal Dual-frequency Fusion mechanism. This approach effectively combines different views while preventing category confusion. Self-supervised alignment objectives guide the learning process without requiring labels. Empirical analysis on various datasets shows that DMGC outperforms existing methods, demonstrating its effectiveness and generalizability. The code for DMGC is available on GitHub at https://github.com/Uncnbb/DMGC. 

<br /><br />Summary: <div>
arXiv:2507.15253v1 Announce Type: cross 
Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with structured interconnections, offer substantial real-world utility but remain insufficiently explored in unsupervised learning. In this work, we initiate the study of multimodal graph clustering, aiming to bridge this critical gap. Through empirical analysis, we observe that real-world multimodal graphs often exhibit hybrid neighborhood patterns, combining both homophilic and heterophilic relationships. To address this challenge, we propose a novel framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which decomposes the original hybrid graph into two complementary views: (1) a homophily-enhanced graph that captures cross-modal class consistency, and (2) heterophily-aware graphs that preserve modality-specific inter-class distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism that jointly filters these disentangled graphs through a dual-pass strategy, enabling effective multimodal integration while mitigating category confusion. Our self-supervised alignment objectives further guide the learning process without requiring labels. Extensive experiments on both multimodal and multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art performance, highlighting its effectiveness and generalizability across diverse settings. Our code is available at https://github.com/Uncnbb/DMGC.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflicting narratives and polarization on social media</title>
<link>https://arxiv.org/abs/2507.15600</link>
<guid>https://arxiv.org/abs/2507.15600</guid>
<content:encoded><![CDATA[
<div> conflicting narratives, political reality, polarization, issue alignment, discursive mechanisms

Summary: 
This study examines how conflicting narratives in the public sphere contribute to political polarization and issue alignment. By analyzing tweets from opposing opinion groups in the German Twittersphere between 2021 and 2023, the researchers identify conflicting narratives on key issues such as the war in Ukraine, Covid, and climate change. They find that these conflicting narratives stem from differing attributions of actantial roles and the emplotment of different actants for the same events. Additionally, the study uncovers patterns of narrative alignment, wherein political actors strategically align opinions across different issues. These findings highlight the role of narratives as a valuable analytical lens for understanding discursive polarization mechanisms in political discourse. 

<br /><br />Summary: <div>
arXiv:2507.15600v1 Announce Type: cross 
Abstract: Narratives are key interpretative devices by which humans make sense of political reality. In this work, we show how the analysis of conflicting narratives, i.e. conflicting interpretive lenses through which political reality is experienced and told, provides insight into the discursive mechanisms of polarization and issue alignment in the public sphere. Building upon previous work that has identified ideologically polarized issues in the German Twittersphere between 2021 and 2023, we analyze the discursive dimension of polarization by extracting textual signals of conflicting narratives from tweets of opposing opinion groups. Focusing on a selection of salient issues and events (the war in Ukraine, Covid, climate change), we show evidence for conflicting narratives along two dimensions: (i) different attributions of actantial roles to the same set of actants (e.g. diverging interpretations of the role of NATO in the war in Ukraine), and (ii) emplotment of different actants for the same event (e.g. Bill Gates in the right-leaning Covid narrative). Furthermore, we provide first evidence for patterns of narrative alignment, a discursive strategy that political actors employ to align opinions across issues. These findings demonstrate the use of narratives as an analytical lens into the discursive mechanisms of polarization.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work</title>
<link>https://arxiv.org/abs/2507.15823</link>
<guid>https://arxiv.org/abs/2507.15823</guid>
<content:encoded><![CDATA[
<div> Keywords: AI for Good, collaboration, deployment, resource-constrained, continuous performance updates 

Summary: 
This work focuses on the deployment of AI models in collaboration with a humanitarian-to-humanitarian organization, emphasizing the importance of real-world impact in the AI for Good space. The paper discusses the process of deploying AI models in resource-constrained environments, highlighting the challenges and successes of maintaining the models for continuous performance updates. The close collaboration with the partner organization is key to ensuring the AI model's success and impact in high-impact applications. Practitioners can learn valuable insights from this work on how to effectively collaborate with organizations in deploying AI models and maintaining them for long-term success. Overall, this study sheds light on the importance of not only developing AI models for high-impact applications but also on the crucial process of deploying and collaborating with partner organizations for real-world impact. 

<br /><br />Summary: <div>
arXiv:2507.15823v1 Announce Type: cross 
Abstract: Publications in the AI for Good space have tended to focus on the research and model development that can support high-impact applications. However, very few AI for Good papers discuss the process of deploying and collaborating with the partner organization, and the resulting real-world impact. In this work, we share details about the close collaboration with a humanitarian-to-humanitarian (H2H) organization and how to not only deploy the AI model in a resource-constrained environment, but also how to maintain it for continuous performance updates, and share key takeaways for practitioners.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection for directed networks revisited using bimodularity</title>
<link>https://arxiv.org/abs/2502.04777</link>
<guid>https://arxiv.org/abs/2502.04777</guid>
<content:encoded><![CDATA[
<div> Keywords: Community structure, Directed networks, Bimodularity, Edge-based clustering, Convex relaxation

Summary:
Community structure, a common feature in networks, has been extensively studied for undirected graphs but not as satisfactorily for directed graphs. This study introduces the concept of bimodularity to represent directed communities by mapping sending and receiving communities. Using convex relaxation and the singular value decomposition of the directed modularity matrix, bimodularity can be optimized. An edge-based clustering approach is then proposed to uncover directed communities and their mappings. The effectiveness of this new framework is demonstrated on a synthetic model and applied to the neuronal network of \textit{C. elegans}, revealing meaningful feedforward loops in the motion systems of the head and body. This novel approach lays a foundation for identifying and understanding community structures in directed networks. 

<br /><br />Summary: Community structure in directed networks is addressed through the concept of bimodularity, optimizing the directed modularity matrix using convex relaxation. An edge-based clustering method reveals directed communities and their mappings, showcased on a synthetic model and the neuronal network of \textit{C. elegans}. This framework advances the detection and comprehension of community structures in directed networks. <div>
arXiv:2502.04777v2 Announce Type: replace 
Abstract: Community structure is a key feature omnipresent in real-world network data. Plethora of methods have been proposed to reveal subsets of densely interconnected nodes using criteria such as the modularity index. These approaches have been successful for undirected graphs, but directed edge information has not yet been dealt with in a satisfactory way. Here, we revisit the concept of directed communities as a mapping between sending and receiving communities. This translates into a new definition that we term bimodularity. Using convex relaxation, bimodularity can be optimized with the singular value decomposition of the directed modularity matrix. Subsequently, we propose an edge-based clustering approach to reveal the directed communities including their mappings. The feasibility of the new framework is illustrated on a synthetic model and further applied to the neuronal wiring diagram of the \textit{C. elegans}, for which it yields meaningful feedforward loops of the head and body motion systems. This framework sets the ground for the understanding and detection of community structures in directed networks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion</title>
<link>https://arxiv.org/abs/2507.13366</link>
<guid>https://arxiv.org/abs/2507.13366</guid>
<content:encoded><![CDATA[
<div> framework, trajectory, synthesis, privacy-preserving, mobility <br />
Summary: <br />
The paper introduces Cardiff, a novel trajectory synthesizing framework for generating fine-grained and privacy-preserving urban mobility data. Cardiff utilizes a cascaded hybrid diffusion-based approach, decomposing the generation process into segment-level and GPS-level synthesis for realistic trajectories. The segment-level encodes road segments into latent embeddings and employs a denoising network for synthesis, while the GPS-level network uses noise augmentation for high-fidelity generation. Cardiff offers a balance between privacy preservation and utility, outperforming existing methods on real-world trajectory datasets in various metrics. <div>
arXiv:2507.13366v1 Announce Type: new 
Abstract: Urban mobility data has significant connections with economic growth and plays an essential role in various smart-city applications. However, due to privacy concerns and substantial data collection costs, fine-grained human mobility trajectories are difficult to become publicly available on a large scale. A promising solution to address this issue is trajectory synthesizing. However, existing works often ignore the inherent structural complexity of trajectories, unable to handle complicated high-dimensional distributions and generate realistic fine-grained trajectories. In this paper, we propose Cardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory synthesizing framework for fine-grained and privacy-preserving mobility generation. By leveraging the hierarchical nature of urban mobility, Cardiff decomposes the generation process into two distinct levels, i.e., discrete road segment-level and continuous fine-grained GPS-level: (i) In the segment-level, to reduce computational costs and redundancy in raw trajectories, we first encode the discrete road segments into low-dimensional latent embeddings and design a diffusion transformer-based latent denoising network for segment-level trajectory synthesis. (ii) Taking the first stage of generation as conditions, we then design a fine-grained GPS-level conditional denoising network with a noise augmentation mechanism to achieve robust and high-fidelity generation. Additionally, the Cardiff framework not only progressively generates high-fidelity trajectories through cascaded denoising but also flexibly enables a tunable balance between privacy preservation and utility. Experimental results on three large real-world trajectory datasets demonstrate that our method outperforms state-of-the-art baselines in various metrics.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiatio</title>
<link>https://arxiv.org/abs/2507.13368</link>
<guid>https://arxiv.org/abs/2507.13368</guid>
<content:encoded><![CDATA[
<div> neighborhood search, multiple views, graph clustering, attribute-missing, performance improvement

Summary: 
The paper presents a new method called CMV-ND for deep graph clustering (DGC) on large-scale attribute-missing graphs. It preprocesses structural information into multiple non-redundant views through recursive neighborhood search and neighborhood differentiation strategies. By constructing complementary views from differential hop representations and node features, CMV-ND effectively enhances the performance of existing multi-view clustering and DGC methods. Experimental results on six graph datasets validate the significant performance improvement achieved by CMV-ND in various industrial scenarios like community detection and recommendation.<br /><br />Summary: <div>
arXiv:2507.13368v1 Announce Type: new 
Abstract: Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes in an attribute graph into different clusters, has seen substantial potential in various industrial scenarios like community detection and recommendation. However, the real-world attribute graphs, e.g., social networks interactions, are usually large-scale and attribute-missing. To solve these two problems, we propose a novel DGC method termed \underline{\textbf{C}}omplementary \underline{\textbf{M}}ulti-\underline{\textbf{V}}iew \underline{\textbf{N}}eighborhood \underline{\textbf{D}}ifferentiation (\textit{CMV-ND}), which preprocesses graph structural information into multiple views in a complete but non-redundant manner. First, to ensure completeness of the structural information, we propose a recursive neighborhood search that recursively explores the local structure of the graph by completely expanding node neighborhoods across different hop distances. Second, to eliminate the redundancy between neighborhoods at different hops, we introduce a neighborhood differential strategy that ensures no overlapping nodes between the differential hop representations. Then, we construct $K+1$ complementary views from the $K$ differential hop representations and the features of the target node. Last, we apply existing multi-view clustering or DGC methods to the views. Experimental results on six widely used graph datasets demonstrate that CMV-ND significantly improves the performance of various methods.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance</title>
<link>https://arxiv.org/abs/2507.13370</link>
<guid>https://arxiv.org/abs/2507.13370</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion evolution, consensus, non-intrusive, multi-agent reinforcement learning

Summary: 
The article introduces the H-NeiFi framework, a hierarchical and non-intrusive approach to guiding opinion evolution on social media towards global consensus. By considering social roles and behavioral characteristics, the framework aims to optimize information propagation paths without directly modifying user views or enforcing connections. Through a dynamic two-layer model and non-intrusive neighbor filtering method, H-NeiFi adaptsively controls user communication channels to enhance consensus speed by 22.0% to 30.7% and maintain global convergence even in the absence of experts. Utilizing multi-agent reinforcement learning, the framework offers a long-term reward function to guide opinions naturally and efficiently while preserving user interaction autonomy. This approach presents a new paradigm for social network governance, addressing challenges in promoting global consensus without intruding on individual opinions. 

Summary: <div>
arXiv:2507.13370v1 Announce Type: new 
Abstract: The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patterns, Models, and Challenges in Online Social Media: A Survey</title>
<link>https://arxiv.org/abs/2507.13379</link>
<guid>https://arxiv.org/abs/2507.13379</guid>
<content:encoded><![CDATA[
<div> keywords: digital platforms, individual behavior, collective behavior, online social systems, model validation
Summary:
The article discusses the impact of digital platforms on observing individual and collective behavior through interaction data. It highlights the need for consolidation in the field due to methodological heterogeneity and weak integration across domains. The survey systematically synthesizes empirical findings and formal models to understand platform-level regularities and methodological architectures. It emphasizes the importance of current modeling frameworks in accounting for observed dynamics in online social systems. The goal is to establish a shared empirical baseline and identify structural constraints for more robust analyses. The article aims to pave the way for improved, comparable, and actionable analyses of online social systems.<br /><br />Summary: <div>
arXiv:2507.13379v1 Announce Type: new 
Abstract: The rise of digital platforms has enabled the large scale observation of individual and collective behavior through high resolution interaction data. This development has opened new analytical pathways for investigating how information circulates, how opinions evolve, and how coordination emerges in online environments. Yet despite a growing body of research, the field remains fragmented and marked by methodological heterogeneity, limited model validation, and weak integration across domains. This survey offers a systematic synthesis of empirical findings and formal models. We examine platform-level regularities, assess the methodological architectures that generate them, and evaluate the extent to which current modeling frameworks account for observed dynamics. The goal is to consolidate a shared empirical baseline and clarify the structural constraints that shape inference in this domain, laying the groundwork for more robust, comparable, and actionable analyses of online social systems.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing the Dynamics of Conspiracy Related German Telegram Conversations during COVID-19</title>
<link>https://arxiv.org/abs/2507.13398</link>
<guid>https://arxiv.org/abs/2507.13398</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, Telegram chats, COVID-19, misinformation, information flow

Summary:
Conspiracy theories have seen a surge on German-language Telegram chats during the COVID-19 pandemic, raising concerns about their impact on trust, democracy, and public health. The study analyzed the structure of these chats, revealing spikes in activity during key pandemic events and highlighting the role of societal stressors in amplifying conspiratorial beliefs. Information flow was observed from larger national or transnational discourse to localized discussions, with a small number of key actors driving the dissemination of content. The top 10% of chats accounted for a majority of forwarded content, but they operated independently with minimal interconnection. A concerning finding was that a significant proportion of shared links pointed to untrustworthy sources, indicating Telegram's role in spreading misinformation. This research sheds light on how conspiracy-related discussions on the platform serve as vectors for the spread of false information.<br /><br />Summary: <div>
arXiv:2507.13398v1 Announce Type: new 
Abstract: Conspiracy theories have long drawn public attention, but their explosive growth on platforms like Telegram during the COVID-19 pandemic raises pressing questions about their impact on societal trust, democracy, and public health. We provide a geographical, temporal and network analysis of the structure of of conspiracy-related German-language Telegram chats in a novel large-scale data set. We examine how information flows between regional user groups and influential broadcasting channels, revealing the interplay between decentralized discussions and content spread driven by a small number of key actors.
  Our findings reveal that conspiracy-related activity spikes during major COVID-19-related events, correlating with societal stressors and mirroring prior research on how crises amplify conspiratorial beliefs. By analysing the interplay between regional, national and transnational chats, we uncover how information flows from larger national or transnational discourse to localised, community-driven discussions. Furthermore, we find that the top 10% of chats account for 94% of all forwarded content, portraying the large influence of a few actors in disseminating information. However, these chats operate independently, with minimal interconnection between each other, primarily forwarding messages to low-traffic groups. Notably, 43% of links shared in the data set point to untrustworthy sources as identified by NewsGuard, a proportion far exceeding their share on other platforms and in other discourse contexts, underscoring the role of conspiracy-related discussions on Telegram as vector for the spread of misinformation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking Multi-Site Sex Ad Data at the Individual Level to Aid Counter-Trafficking Efforts</title>
<link>https://arxiv.org/abs/2507.13477</link>
<guid>https://arxiv.org/abs/2507.13477</guid>
<content:encoded><![CDATA[
<div> Keywords: sex trafficking, adult service websites, counter-trafficking efforts, data linking, artificial intelligence<br />
<br />
Summary: 
The study focuses on the use of adult service websites (ASWs) for sex trafficking and the challenges in collecting and linking data from these sites. The closure of Backpage.com has led to the expansion of ASWs outside US jurisdiction, making it difficult to gather intelligence for counter-trafficking efforts. The researchers have developed an end-to-end process using network science, information systems, and artificial intelligence to link and filter sex ad data efficiently. This process has been successful in identifying over 60 potential victims of sex trafficking, helping them transition out of the exploitative life. The key component of the process is an edge filtering procedure that removes erroneous links in the data. Compared to existing approaches, their process is more computationally efficient and provides more actionable intelligence. The proposed process is a valuable tool in transforming disparate sex ad data into actionable intelligence to combat sex trafficking and save lives.<br /><br /> <div>
arXiv:2507.13477v1 Announce Type: new 
Abstract: The Internet facilitates sex trafficking through adult service websites (ASWs) that host online advertisements for sexual services (sex ads). Since the closure of the popular site Backpage.com, the ecosystem of ASWs has expanded to include multiple competing sites that are hosted outside US jurisdiction. Gaining intelligence for counter-trafficking efforts requires collecting, linking, and cleaning the data from multiple sites. However, high ad volumes, disparate data types, and the existence of generic and misappropriated data make this process challenging. We present an end-to-end process for linking sex ad data and filtering potentially erroneous links. Outputs of the developed process have been used to inform counter-trafficking operations that have helped identify more than 60 potential victims of sex trafficking, some of whom are getting help to transition out of the life. Our process leverages concepts and techniques from network science, information systems, and artificial intelligence to link ads across sites at the level of an individual or unique posting entity. Our approach is computationally efficient, allowing millions of ads to be processed in under an hour. A key component of our process is an edge filtering procedure that identifies and removes potentially erroneous links in a graph representation of sex ad data. A comparison of the proposed process to an existing approach shows that our process is typically more computationally efficient and yields substantial increases in the number of individuals for which we can derive actionable intelligence. The proposed process is an efficient and effective approach for transforming the high volumes of disparate data from sex ads into intelligence that can save lives. It has been refined over years of collaboration with practitioners and represents a strong foundation upon which further counter-trafficking tools can be built.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Community Surveys for Operational Decision Making in Interconnected Utility Infrastructures</title>
<link>https://arxiv.org/abs/2507.13577</link>
<guid>https://arxiv.org/abs/2507.13577</guid>
<content:encoded><![CDATA[
<div> Keywords: interdependent infrastructure systems, hetero-functional graph, community preferences, Large Language Model, learning algorithms 

Summary:
Interdependent infrastructure systems and communities are represented using a hetero-functional graph (HFG) to encode dependencies between functionalities. This graph establishes a partial order of functionalities to guide repair decisions during disasters. However, integrating community preferences is crucial to refine this order and enhance resilience. To accomplish this, a Large Language Model (LLM) is used as a proxy survey tool to gather community feedback on preferred repair sequences. Simulated personas representing diverse disaster experiences provide input on prioritizing infrastructure repair needs across communities. Learning algorithms are then employed to generate a global order based on aggregated responses from these LLM-generated personas. This approach combines technical criteria with community input to optimize repair strategies and enhance disaster resilience.<br /><br />Summary: Interdependent infrastructure systems are represented using a HFG, allowing for a partial order of repair functionalities. Community preferences are integrated via LLM proxy surveys to refine this order and improve resilience. Simulated personas provide diverse input on prioritizing repair needs, culminating in a global order generated by learning algorithms. <div>
arXiv:2507.13577v1 Announce Type: new 
Abstract: We represent interdependent infrastructure systems and communities alike with a hetero-functional graph (HFG) that encodes the dependencies between functionalities. This graph naturally imposes a partial order of functionalities that can inform the sequence of repair decisions to be made during a disaster across affected communities. However, using such technical criteria alone provides limited guidance at the point where the functionalities directly impact the communities, since these can be repaired in any order without violating the system constraints. To address this gap and improve resilience, we integrate community preferences to refine this partial order from the HFG into a total order. Our strategy involves getting the communities' opinions on their preferred sequence for repair crews to address infrastructure issues, considering potential constraints on resources. Due to the delay and cost associated with real-world survey data, we utilize a Large Language Model (LLM) as a proxy survey tool. We use the LLM to craft distinct personas representing individuals, each with varied disaster experiences. We construct diverse disaster scenarios, and each simulated persona provides input on prioritizing infrastructure repair needs across various communities. Finally, we apply learning algorithms to generate a global order based on the aggregated responses from these LLM-generated personas.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Duplicating Deceit: Inauthentic Behavior Among Indian Misinformation Duplicators on X/Twitter</title>
<link>https://arxiv.org/abs/2507.13636</link>
<guid>https://arxiv.org/abs/2507.13636</guid>
<content:encoded><![CDATA[
<div> Keywords: inauthentic duplication, social media, misinformation, TweeXster, duplication campaigns

Summary:
This paper examines the issue of inauthentic duplication on social media, specifically focusing on the spread of misinformation through multiple accounts sharing identical false tweets. The study utilizes a dataset from AltNews, an Indian fact-checking organization, containing over 12 million posts from 5,493 accounts known to be involved in duplicating such content. Surprisingly, the research reveals that less than 1% of these accounts exhibit bot-like behavior, dispelling the common misconception that bots are solely responsible for the dissemination of false information. The authors introduce TweeXster, a framework designed to detect and analyze duplication campaigns, which uncovers clusters of accounts engaged in the repeated and sometimes revived sharing of deceptive or harmful content. This research sheds light on the complex dynamics of misinformation spread on social media and offers a novel approach to combatting such campaigns. 

<br /><br />Summary: <div>
arXiv:2507.13636v1 Announce Type: new 
Abstract: This paper investigates inauthentic duplication on social media, where multiple accounts share identical misinformation tweets. Leveraging a dataset of misinformation verified by AltNews, an Indian fact-checking organization, we analyze over 12 million posts from 5,493 accounts known to have duplicated such content. Contrary to common assumptions that bots are primarily responsible for spreading false information, fewer than 1\% of these accounts exhibit bot-like behavior. We present TweeXster, a framework for detecting and analyzing duplication campaigns, revealing clusters of accounts involved in repeated and sometimes revived dissemination of false or abusive content.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Route-based Conflation Between Linear Referencing System Maps And OpenStreetMap Using Open-source Tools</title>
<link>https://arxiv.org/abs/2507.13939</link>
<guid>https://arxiv.org/abs/2507.13939</guid>
<content:encoded><![CDATA[
<div> conflation, basemaps, LRS, OpenStreetMap, automated<br />
<br />
Summary: 
This paper introduces an automated open-source process for conflation of two basemaps - the Virginia Department of Transportation's linear reference system (LRS) basemap and OpenStreetMap (OSM) for Virginia. The process involves loading one LRS route at a time, determining the direction of travel, interpolating to fill gaps, and using Valhalla's map-matching algorithm with a Hidden Markov Model (HMM) and Viterbi search to find corresponding points on OSM. Key contributions include successful conflation of Virginia's LRS map with OSM using an automated method, a replicable open-source processing pipeline, and achieving over 98% successful matches. This approach improves upon existing automated conflation processes and eliminates the need for proprietary licenses. <br /><br />Summary: <div>
arXiv:2507.13939v1 Announce Type: new 
Abstract: Transportation researchers and planners utilize a wide range of roadway metrics that are usually associated with different basemaps. Conflation is an important process for transferring these metrics onto a single basemap. However, conflation is often an expensive and time-consuming process based on proprietary algorithms that require manual verification.
  In this paper, an automated open-source process is used to conflate two basemaps: the linear reference system (LRS) basemap produced by the Virginia Department of Transportation and the OpenStreetMap (OSM) basemap for Virginia. This process loads one LRS route at a time, determines the correct direction of travel, interpolates to fill gaps larger than 12 meters, and then uses Valhalla's map-matching algorithm to find the corresponding points along OSM's segments. Valhalla's map-matching process uses a Hidden Markov Model (HMM) and Viterbi search-based approach to find the most likely OSM segments matching the LRS route.
  This work has three key contributions. First, it conflates the Virginia roadway network LRS map with OSM using an automated conflation method based on HMM and Viterbi search. Second, it demonstrates a novel open-source processing pipeline that could be replicated without the need for proprietary licenses. Finally, the overall conflation process yields over 98% successful matches, which is an improvement over most automated processes currently available for this type of conflation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rumour Spreading Depends on the Latent Geometry and Degree Distribution in Social Network Models</title>
<link>https://arxiv.org/abs/2408.01268</link>
<guid>https://arxiv.org/abs/2408.01268</guid>
<content:encoded><![CDATA[
<div> power-law distribution, rumour spreading, ultra-small-world models, geometric inhomogeneous random graphs, metric geometry 

Summary: 
- The study investigates rumour spreading in ultra-small-world models with power-law distribution degrees, focusing on Geometric Inhomogeneous Random Graphs (GIRGs).
- Rumour spreading speed varies in GIRGs with Euclidean geometry, showing slow, fast (polylogarithmic), or ultra-fast rates based on power law exponent and geometry strength.
- Rumour spreading in GIRGs doesn't align with graph distance rates, indicating that faster spreading may occur even with short graph distances.
- Non-metric geometry always leads to at least fast rumour spreading, depicting social connections based on single attributes like familial kinship.
- In Euclidean geometry, efficient rumour transmission pathways differ from known paths, showcasing chains of vertices with specific degree structures. <div>
arXiv:2408.01268v3 Announce Type: replace-cross 
Abstract: We study push-pull rumour spreading in ultra-small-world models for social networks where the degrees follow a power-law distribution. In a non-geometric setting, Fountoulakis, Panagiotou and Sauerwald have shown that rumours always spread ultra-fast (SODA 2012), i.e. in doubly logarithmic time. On the other hand, Janssen and Mehrabian have found that rumours spread slowly (polynomial time) in a spatial preferential attachment model (SIDMA 2017). We study the question systematically for the model of Geometric Inhomogeneous Random Graphs (GIRGs). Our results are two-fold: first, with Euclidean geometry slow, fast (polylogarithmic) and ultra-fast rumour spreading may occur, depending on the exponent of the power law and the strength of the geometry in the networks, and we fully characterise the phase boundaries in between. The regimes do not coincide with the graph distance regimes, i.e., polylogarithmic or even polynomial rumour spreading may occur even if graph distances are doubly logarithmic. We expect these results to hold with little effort for related models, e.g. Scale-Free Percolation. Second, we show that rumour spreading is always (at least) fast in a non-metric geometry. The considered non-metric geometry allows to model social connections where resemblance of vertices in a single attribute, such as familial kinship, already strongly indicates the presence of an edge. Euclidean geometry fails to capture such ties.
  For some regimes in the Euclidean setting, the efficient pathways for spreading rumours differ from previously identified paths. For example, a vertex of degree $d$ can transmit the rumour to a vertex of larger degree by a chain of length $3$, where one of the two intermediaries has constant degree, and the other has degree $d^{c}$ for some constant $c<1$. Similar but longer chains of vertices, all having non-constant degree, turn out to be useful as well.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification of Authoritative Nodes and Dismantling of Illicit Networks Using a Novel Metric for Measuring Strength of a Graph</title>
<link>https://arxiv.org/abs/2507.12711</link>
<guid>https://arxiv.org/abs/2507.12711</guid>
<content:encoded><![CDATA[
<div> metrics, criminal networks, node removal, human perception, network strength 
Summary: 
The study introduces a new metric for evaluating the strength of networks after node removal in scenarios like dismantling criminal networks or containing epidemics. Traditional metrics focus on structural properties of the graph, neglecting human perception. The proposed metric combines structural properties with human perception, aligning more closely with human judgment. Validation through human subject surveys shows that the new metric outperforms existing methods in identifying authoritative nodes and effectively dismantling networks. The metric's effectiveness is demonstrated through dismantling both synthetic and real-world networks. By integrating structural properties and human perception, the new metric offers a more comprehensive evaluation of network strength post-node removal. <div>
arXiv:2507.12711v1 Announce Type: new 
Abstract: Dismantling criminal networks or containing epidemics or misinformation through node removal is a well-studied problem. To evaluate the effectiveness of such efforts, one must measure the strength of the network before and after node removal. Process P1 is considered more effective than P2 if the strength of the residual network after removing k nodes via P1 is smaller than that from P2. This leads to the central question: How should network strength be measured?
  Existing metrics rely solely on structural properties of the graph, such as connectivity. However, in real-world scenarios, particularly in law enforcement, the perception of agents regarding network strength can differ significantly from structural assessments. These perceptions are often ignored in traditional metrics.
  We propose a new strength metric that integrates both structural properties and human perception. Using human subject surveys, we validate our approach against existing metrics. Our metric not only aligns more closely with human judgment but also outperforms traditional methods in identifying authoritative nodes and effectively dismantling both synthetic and real-world networks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T3MAL: Test-Time Fast Adaptation for Robust Multi-Scale Information Diffusion Prediction</title>
<link>https://arxiv.org/abs/2507.12880</link>
<guid>https://arxiv.org/abs/2507.12880</guid>
<content:encoded><![CDATA[
<div> adaptation, information diffusion prediction, test-time training, distribution shifts, self-supervised auxiliary task <br />
<br />
Summary: 
The paper introduces T3MAL, a framework for multi-scale diffusion prediction that addresses distribution shifts within IDP tasks. T3MAL utilizes a test-time training approach to adapt a trained model to the distribution of each test instance, improving prediction accuracy in social networks with uncertain user behavior. The framework includes a self-supervised auxiliary network inspired by BYOL, facilitating instance-specific adaptation during testing. T3MAL also incorporates a meta-auxiliary learning scheme and a lightweight adaptor for efficient test-time adaptation and better weight initialization to prevent catastrophic forgetting. Extensive experiments on public datasets demonstrate T3MAL's superiority over existing methods in information diffusion prediction tasks. <div>
arXiv:2507.12880v1 Announce Type: new 
Abstract: Information diffusion prediction (IDP) is a pivotal task for understanding how information propagates among users. Most existing methods commonly adhere to a conventional training-test paradigm, where models are pretrained on training data and then directly applied to test samples. However, the success of this paradigm hinges on the assumption that the data are independently and identically distributed, which often fails in practical social networks due to the inherent uncertainty and variability of user behavior. In the paper, we address the novel challenge of distribution shifts within IDP tasks and propose a robust test-time training (TTT)-based framework for multi-scale diffusion prediction, named T3MAL. The core idea is to flexibly adapt a trained model to accommodate the distribution of each test instance before making predictions via a self-supervised auxiliary task. Specifically, T3MAL introduces a BYOL-inspired self-supervised auxiliary network that shares a common feature extraction backbone with the primary diffusion prediction network to guide instance-specific adaptation during testing. Furthermore, T3MAL enables fast and accurate test-time adaptation by incorporating a novel meta-auxiliary learning scheme and a lightweight adaptor, which together provide better weight initialization for TTT and mitigate catastrophic forgetting. Extensive experiments on three public datasets demonstrate that T3MAL outperforms various state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Centrality Paradox: Why Your Friends Are Always More Important</title>
<link>https://arxiv.org/abs/2507.13059</link>
<guid>https://arxiv.org/abs/2507.13059</guid>
<content:encoded><![CDATA[
<div> friendship paradox, network centrality, graph theory, eigenvector centrality, PageRank <br />
<br />
Summary: 
The article revisits the classical friendship paradox in the context of network centrality measures. It shows that in any irreducible, undirected graph, various centrality measures such as degree, eigenvector-centrality, walk-count, Katz, and PageRank centralities follow the friendship paradox principle, where one's friends, on average, have more connections than oneself. The study connects this result to the variational characterization of the eigenvector corresponding to the Perron eigenvalue. By generalizing the friendship paradox, the research demonstrates that different centrality measures showcase similar patterns, exceeding the global average. This finding contributes to a better understanding of how network structures influence the distribution of connections within a graph, highlighting the importance of considering various centrality measures in network analysis. <div>
arXiv:2507.13059v1 Announce Type: new 
Abstract: We revisit the classical friendship paradox which states that on an average one's friends have at least as many friends as oneself and generalize it to a variety of network centrality measures. In particular, we show that for any irreducible, undirected graph $G$, the "friends-average" of degree, eigenvector-centrality, walk-count, Katz, and PageRank centralities exceeds the global average. We show that the result follows from the variational characterisation of the eigenvector corresponding to the Perron eigenvalue.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interacting Hosts with Microbiome Exchange: An Extension of Metacommunity Theory for Discrete Interactions</title>
<link>https://arxiv.org/abs/2507.11958</link>
<guid>https://arxiv.org/abs/2507.11958</guid>
<content:encoded><![CDATA[
<div> Metacommunity theory, microbiome model, interactions, dispersal, living hosts<br />
<br />
Summary: <br />
Microbiomes, collections of interacting microbes in environments, have significant effects on their surroundings. Metacommunity theory, which incorporates interactions across multiple scales, is commonly used in microbiome models. Current metacommunity models assume continuous dispersal of microbiomes between environments, but this does not reflect the discrete interactions between microbiomes in living hosts. A new modeling framework is developed in this paper, considering discrete interactions and using parameters to control interaction frequencies and microbiome exchange amounts. Analytical approximations in three parameter regimes are derived and proven to be accurate. Both parameters are crucial in determining microbiome dynamics, with microbiome convergence depending on the interplay between interaction frequency and strength. The study emphasizes the importance of incorporating discrete interactions in microbiome models for a better understanding of microbiome dynamics in living hosts. <div>
arXiv:2507.11958v1 Announce Type: cross 
Abstract: Microbiomes, which are collections of interacting microbes in an environment, often substantially impact the environmental patches or living hosts that they occupy. In microbiome models, it is important to consider both the local dynamics within an environment and exchanges of microbiomes between environments. One way to incorporate these and other interactions across multiple scales is to employ metacommunity theory. Metacommunity models commonly assume continuous microbiome dispersal between the environments in which local microbiome dynamics occur. Under this assumption, a single parameter between each pair of environments controls the dispersal rate between those environments. This metacommunity framework is well-suited to abiotic environmental patches, but it fails to capture an essential aspect of the microbiomes of living hosts, which generally do not interact continuously with each other. Instead, living hosts interact with each other in discrete time intervals. In this paper, we develop a modeling framework that encodes such discrete interactions and uses two parameters to separately control the interaction frequencies between hosts and the amount of microbiome exchange during each interaction. We derive analytical approximations of models in our framework in three parameter regimes and prove that they are accurate in those regimes. We compare these approximations to numerical simulations for an illustrative model. We demonstrate that both parameters in our modeling framework are necessary to determine microbiome dynamics. Key features of the dynamics, such as microbiome convergence across hosts, depend sensitively on the interplay between interaction frequency and strength.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better Understand Public Concerns about Vaccines</title>
<link>https://arxiv.org/abs/2507.12840</link>
<guid>https://arxiv.org/abs/2507.12840</guid>
<content:encoded><![CDATA[
<div> Vaccine hesitancy, social media, public concerns, Language Models, VaxPulse Query Corner <br />
Summary: <br />
The article discusses the threat of vaccine hesitancy to public health and the role of social media in understanding public concerns. Traditional methods like topic modelling may struggle to capture nuanced opinions, while large Language Models (LLMs) can miss current events and community concerns. Hallucinations in LLMs further complicate public health communication. To address these challenges, the authors developed a tool called VaxPulse Query Corner using the Retrieval Augmented Generation technique. This tool helps answer complex queries about public vaccine concerns on online platforms, enabling public health administrators and stakeholders to better understand public concerns and implement targeted interventions to increase vaccine confidence. An analysis of 35,103 Shingrix social media posts showed high levels of answer faithfulness and relevance, demonstrating the tool's effectiveness in addressing public vaccine concerns. <div>
arXiv:2507.12840v1 Announce Type: cross 
Abstract: Vaccine hesitancy threatens public health, leading to delayed or rejected vaccines. Social media is a vital source for understanding public concerns, and traditional methods like topic modelling often struggle to capture nuanced opinions. Though trained for query answering, large Language Models (LLMs) often miss current events and community concerns. Additionally, hallucinations in LLMs can compromise public health communication. To address these limitations, we developed a tool (VaxPulse Query Corner) using the Retrieval Augmented Generation technique. It addresses complex queries about public vaccine concerns on various online platforms, aiding public health administrators and stakeholders in understanding public concerns and implementing targeted interventions to boost vaccine confidence. Analysing 35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and relevance (0.94).
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Life Finds A Way: Emergence of Cooperative Structures in Adaptive Threshold Networks</title>
<link>https://arxiv.org/abs/2507.13253</link>
<guid>https://arxiv.org/abs/2507.13253</guid>
<content:encoded><![CDATA[
<div> cooperation, competition, network dynamics, evolution, threshold-directed network

Summary:
The study explores the evolution of new levels of organization in systems with varying biases towards cooperation and antagonism. Using a random threshold-directed network model, the researchers simulate the dynamic formation of directed links based on node-specific traits and threshold rules. The model generates a multi-digraph with signed edges reflecting support or antagonism, leading to two interdependent threshold graphs. By incorporating temporal growth and node turnover, the researchers observe phase transitions in connectivity and resilience within communities. The findings offer insights into adaptive systems in biological and economic contexts and have applications in Collective Affordance Sets. The framework may be valuable for predicting outcomes in ongoing experiments on microbial communities in soil. <br /><br />Summary: <div>
arXiv:2507.13253v1 Announce Type: cross 
Abstract: There has been a long debate on how new levels of organization have evolved. It might seem unlikely, as cooperation must prevail over competition. One well-studied example is the emergence of autocatalytic sets, which seem to be a prerequisite for the evolution of life. Using a simple model, we investigate how varying bias toward cooperation versus antagonism shapes network dynamics, revealing that higher-order organization emerges even amid pervasive antagonistic interactions. In general, we observe that a quantitative increase in the number of elements in a system leads to a qualitative transition.
  We present a random threshold-directed network model that integrates node-specific traits with dynamic edge formation and node removal, simulating arbitrary levels of cooperation and competition. In our framework, intrinsic node values determine directed links through various threshold rules. Our model generates a multi-digraph with signed edges (reflecting support/antagonism, labeled ``help''/``harm''), which ultimately yields two parallel yet interdependent threshold graphs. Incorporating temporal growth and node turnover in our approach allows exploration of the evolution, adaptation, and potential collapse of communities and reveals phase transitions in both connectivity and resilience.
  Our findings extend classical random threshold and Erd\H{o}s-R\'enyi models, offering new insights into adaptive systems in biological and economic contexts, with emphasis on the application to Collective Affordance Sets. This framework should also be useful for making predictions that will be tested by ongoing experiments of microbial communities in soil.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling the spillover from online engagement to offline protest: stochastic dynamics and mean-field approximations on networks</title>
<link>https://arxiv.org/abs/2507.13310</link>
<guid>https://arxiv.org/abs/2507.13310</guid>
<content:encoded><![CDATA[
<div> social media, offline protest, coupled modelling framework, online social network, transmission rate

Summary:
The study explores how engagement on social media topics influences offline protest activities using a coupled modelling framework. Various stochastic and mean-field models are developed to estimate the reproductive number and predict surge times in activity. The transmission rate between online and offline realms is crucial, needing to be within a critical range for offline outbursts to occur. Network structure impacts model accuracy, with low-density networks requiring more complexity. However, when tested on real-world networks, increased complexity did not improve accuracy. The research highlights the importance of considering online engagement when studying offline actions and how network density affects model accuracy. <div>
arXiv:2507.13310v1 Announce Type: cross 
Abstract: Social media is transforming various aspects of offline life, from everyday decisions such as dining choices to the progression of conflicts. In this study, we propose a coupled modelling framework with an online social network layer to analyse how engagement on a specific topic spills over into offline protest activities. We develop a stochastic model and derive several mean-field models of varying complexity. These models allow us to estimate the reproductive number and anticipate when surges in activity are likely to occur. A key factor is the transmission rate between the online and offline domains; for offline outbursts to emerge, this rate must fall within a critical range, neither too low nor too high. Additionally, using synthetic networks, we examine how network structure influences the accuracy of these approximations. Our findings indicate that low-density networks need more complex approximations, whereas simpler models can effectively represent higher-density networks. When tested on two real-world networks, however, increased complexity did not enhance accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DomainDemo: a dataset of domain-sharing activities among different demographic groups on Twitter</title>
<link>https://arxiv.org/abs/2501.09035</link>
<guid>https://arxiv.org/abs/2501.09035</guid>
<content:encoded><![CDATA[
<div> social media, web content, demographic factors, information sharing, Twitter data

Summary:
The study introduces DomainDemo, a dataset linking Twitter domains with user demographics from 2011 to 2022, providing insights into information sharing during elections. The dataset includes age, gender, race, political affiliation, and geolocation of over 1.5 million users, allowing for a decade of analysis. By aggregating user demographics onto domains, five metrics are derived, including localness and partisan audience, offering a better understanding of information flows. The metrics align with existing classifications, validating DomainDemo's approach. This dataset enhances understanding of social media dynamics and trends in political discourse among registered U.S. voters from different sociodemographic groups. <div>
arXiv:2501.09035v2 Announce Type: replace 
Abstract: Social media play a pivotal role in disseminating web content, particularly during elections, yet our understanding of the association between demographic factors and information sharing online remains limited. Here, we introduce a unique dataset, DomainDemo, linking domains shared on Twitter (X) with the demographic characteristics of associated users, including age, gender, race, political affiliation, and geolocation, from 2011 to 2022. This new resource was derived from a panel of over 1.5 million Twitter users matched against their U.S. voter registration records, facilitating a better understanding of a decade of information flows on one of the most prominent social media platforms and trends in political and public discourse among registered U.S. voters from different sociodemographic groups. By aggregating user demographic information onto the domains, we derive five metrics that provide critical insights into over 129,000 websites. In particular, the localness and partisan audience metrics quantify the domains' geographical reach and ideological orientation, respectively. These metrics show substantial agreement with existing classifications, suggesting the effectiveness and reliability of DomainDemo's approach.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Link Prediction via Hyperedge Copying</title>
<link>https://arxiv.org/abs/2502.02386</link>
<guid>https://arxiv.org/abs/2502.02386</guid>
<content:encoded><![CDATA[
<div> generative model, hypergraphs, noisy copying, likelihood, empirical hypergraphs <br /> 
<br />Summary: 
The article proposes a generative model for temporally-evolving hypergraphs where hyperedges are formed by noisy copying of previous hyperedges. This model is able to replicate several patterns observed in empirical hypergraphs and can be learned from data. It defines a likelihood over complete hypergraphs rather than subsets. The model allows for analysis of node degree, edge size, and edge intersection size distributions in terms of its parameters. It successfully captures certain characteristics of empirical hypergraphs but also has limitations. The article presents a scalable stochastic expectation maximization algorithm for fitting the model to large hypergraph datasets. Furthermore, the model is evaluated in a hypergraph link prediction task and shows competitive performance with a minimal number of parameters compared to large neural networks. <div>
arXiv:2502.02386v2 Announce Type: replace 
Abstract: We propose a generative model of temporally-evolving hypergraphs in which hyperedges form via noisy copying of previous hyperedges. Our proposed model reproduces several stylized facts from many empirical hypergraphs, is learnable from data, and defines a likelihood over a complete hypergraph rather than ego-based or other sub-hypergraphs. Analyzing our model, we derive descriptions of node degree, edge size, and edge intersection size distributions in terms of the model parameters. We also show several features of empirical hypergraphs which are and are not successfully captured by our model. We provide a scalable stochastic expectation maximization algorithm with which we can fit our model to hypergraph data sets with millions of nodes and edges. Finally, we assess our model on a hypergraph link prediction task, finding that an instantiation of our model with just 11 parameters can achieve competitive predictive performance with large neural networks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models</title>
<link>https://arxiv.org/abs/2503.12989</link>
<guid>https://arxiv.org/abs/2503.12989</guid>
<content:encoded><![CDATA[
<div> job data, occupation classification, large language models, taxonomies, taxonomy-guided reasoning 

Summary:
The study evaluates large language models' ability to accurately classify job data using occupational taxonomies, highlighting their limitations for smaller models. A multi-stage framework is proposed, incorporating taxonomy-guided reasoning examples to improve performance by aligning outputs with taxonomic knowledge. Evaluations on a large dataset demonstrate that the framework not only enhances occupation and skill classification tasks but also offers a cost-effective alternative to advanced models like GPT-4o, reducing computational costs while maintaining strong performance. This framework proves to be practical and scalable for occupation classification and related tasks across large language models. 

<br /><br />Summary: <div>
arXiv:2503.12989v2 Announce Type: replace-cross 
Abstract: Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations, especially for smaller models. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show that our framework not only enhances occupation and skill classification tasks, but also provides a cost-effective alternative to frontier models like GPT-4o, significantly reducing computational costs while maintaining strong performance. This makes it a practical and scalable solution for occupation classification and related tasks across LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictable Drifts in Collective Cultural Attention: Evidence from Nation-Level Library Takeout Data</title>
<link>https://arxiv.org/abs/2507.12007</link>
<guid>https://arxiv.org/abs/2507.12007</guid>
<content:encoded><![CDATA[
<div> Keywords: consumer attention, cultural products, popularity distributions, demographic groups, cultural drifts

Summary:
Consumer attention for cultural products, such as books, movies, and songs, is challenging to predict due to intrinsic limits. This study analyzed nationwide library loan data for over 660,000 unique books and discovered that culture drifts continuously, leading to a growing divergence over time. Different book genres exhibit varying drift rates. The influence of demographic factors like age, sex, education, and location on cultural drift was also examined, revealing diverse effects across demographic groups. These findings have implications for market forecasting and recommender systems, emphasizing the importance of considering specific drift dynamics for different types of items and demographic segments.
<br /><br />Summary: <div>
arXiv:2507.12007v1 Announce Type: new 
Abstract: Predicting changes in consumer attention for cultural products, such as books, movies, and songs, is notoriously difficult. Past research on predicting the popularity of individual products suggests the existence of intrinsic prediction limits. However, little is known about the limits for predicting collective attention across cultural products. Here, we analyze four years of nationwide library loan data for approximately 2 million individuals, comprising over 100 million loans of more than 660,000 unique books. We find that culture, as measured by popularity distributions of loaned books, drifts continually from month to month at a near-constant rate, leading to a growing divergence over time, and that drifts vary between different book genres. By linking book loans to registry data, we investigate the influence of age, sex, educational level, and geographical area on cultural drift, finding heterogeneous effects from the different demographic groups. Our findings have important implications for market forecasting and developing robust recommender systems, highlighting the need to account for specific drift dynamics for different types of items and demographic groups.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Cascade Graph Learning for Classifying Real and Synthetic Information Diffusion Patterns</title>
<link>https://arxiv.org/abs/2507.12063</link>
<guid>https://arxiv.org/abs/2507.12063</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, cascade graph mining, Contrastive Cascade Graph Learning (CCGL), information diffusion analysis, cascade classification

Summary: 
CCGL is a promising approach for analyzing cascade graphs in social media data to curb harmful content spread and promote reliable information dissemination. The study evaluates CCGL's performance in cascade classification tasks, showcasing its ability to capture specific structural patterns in cascade graphs across different platforms and models. The findings suggest CCGL's strong effectiveness in understanding the dynamics of information diffusion, making it a valuable tool for various downstream analyses in this domain.<br /><br />Summary: <div>
arXiv:2507.12063v1 Announce Type: new 
Abstract: A wide variety of information is disseminated through social media, and content that spreads at scale can have tangible effects on the real world. To curb the spread of harmful content and promote the dissemination of reliable information, research on cascade graph mining has attracted increasing attention. A promising approach in this area is Contrastive Cascade Graph Learning (CCGL). One important task in cascade graph mining is cascade classification, which involves categorizing cascade graphs based on their structural characteristics. Although CCGL is expected to be effective for this task, its performance has not yet been thoroughly evaluated. This study aims to investigate the effectiveness of CCGL for cascade classification. Our findings demonstrate the strong performance of CCGL in capturing platform- and model-specific structural patterns in cascade graphs, highlighting its potential for a range of downstream information diffusion analysis tasks.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Coordinated Online Behavior: Trade-offs and Strategies</title>
<link>https://arxiv.org/abs/2507.12108</link>
<guid>https://arxiv.org/abs/2507.12108</guid>
<content:encoded><![CDATA[
<div> multimodal, coordinated behavior, digital ecosystem, detection, online platforms
Summary:<br /><br />The study explores the detection of multimodal coordinated online behavior, comparing monomodal and multimodal approaches. It highlights the trade-off between weakly and strongly integrated models, emphasizing the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. The research assesses the unique contributions of different data modalities and the impact of varying implementations of multimodality on detection outcomes. Findings indicate that a multimodal approach provides a more comprehensive understanding of coordination dynamics. The study enhances the ability to detect and analyze coordinated behavior online, offering new perspectives for safeguarding the integrity of digital platforms. <div>
arXiv:2507.12108v1 Announce Type: new 
Abstract: Coordinated online behavior, which spans from beneficial collective actions to harmful manipulation such as disinformation campaigns, has become a key focus in digital ecosystem analysis. Traditional methods often rely on monomodal approaches, focusing on single types of interactions like co-retweets or co-hashtags, or consider multiple modalities independently of each other. However, these approaches may overlook the complex dynamics inherent in multimodal coordination. This study compares different ways of operationalizing the detection of multimodal coordinated behavior. It examines the trade-off between weakly and strongly integrated multimodal models, highlighting the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. By comparing monomodal and multimodal approaches, we assess the unique contributions of different data modalities and explore how varying implementations of multimodality impact detection outcomes. Our findings reveal that not all the modalities provide distinct insights, but that with a multimodal approach we can get a more comprehensive understanding of coordination dynamics. This work enhances the ability to detect and analyze coordinated online behavior, offering new perspectives for safeguarding the integrity of digital platforms.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RSD-15K: A Large-Scale User-Level Annotated Dataset for Suicide Risk Detection on Social Media</title>
<link>https://arxiv.org/abs/2507.11559</link>
<guid>https://arxiv.org/abs/2507.11559</guid>
<content:encoded><![CDATA[
<div> Dataset, Suicide risk assessment, Machine learning, Deep learning, Privacy protection
Summary:<br /><br />This paper introduces a large-scale dataset of 15,000 user-level social media posts related to cognitive and mental health (CMH) disorders and suicide risk. The dataset includes complete user posting time sequence information and has undergone rigorous annotations. Various machine learning methods, deep learning models, and large language models were evaluated for automatic assessment of suicide risk using this dataset. Results show promising performance in this task. Privacy protection and ethical use of the dataset were discussed, along with potential applications in mental health testing and clinical psychiatric treatment. The study provides valuable insights for future research in this area. <div>
arXiv:2507.11559v1 Announce Type: cross 
Abstract: In recent years, cognitive and mental health (CMH) disorders have increasingly become an important challenge for global public health, especially the suicide problem caused by multiple factors such as social competition, economic pressure and interpersonal relationships among young and middle-aged people. Social media, as an important platform for individuals to express emotions and seek help, provides the possibility for early detection and intervention of suicide risk. This paper introduces a large-scale dataset containing 15,000 user-level posts. Compared with existing datasets, this dataset retains complete user posting time sequence information, supports modeling the dynamic evolution of suicide risk, and we have also conducted comprehensive and rigorous annotations on these datasets. In the benchmark experiment, we systematically evaluated the performance of traditional machine learning methods, deep learning models, and fine-tuned large language models. The experimental results show that our dataset can effectively support the automatic assessment task of suicide risk. Considering the sensitivity of mental health data, we also discussed the privacy protection and ethical use of the dataset. In addition, we also explored the potential applications of the dataset in mental health testing, clinical psychiatric auxiliary treatment, etc., and provided directional suggestions for future research work.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peer Review and the Diffusion of Ideas</title>
<link>https://arxiv.org/abs/2507.11825</link>
<guid>https://arxiv.org/abs/2507.11825</guid>
<content:encoded><![CDATA[
<div> exposure, peer review, knowledge, citations, scientific knowledge 
Summary: 
The study investigates the role of peer review in exposing reviewers to new ideas, utilizing a natural experiment with over 500,000 peer review invitations. Exposure to a manuscript's core ideas impacts the future referencing behavior and knowledge of reviewers who decline the review invite. Reviewers who view manuscript summaries increase citations to the manuscript and demonstrate enhanced breadth, depth, diversity, and prominence in citing the submitting author's work. This underscores how peer review influences the dissemination of scientific knowledge. Despite debates on the costs and burdens of peer review, the study highlights its role as a powerful engine for idea diffusion on a massive scale, driving scientific advancements and scholarly communication. <div>
arXiv:2507.11825v1 Announce Type: cross 
Abstract: This study examines a fundamental yet overlooked function of peer review: its role in exposing reviewers to new and unexpected ideas. Leveraging a natural experiment involving over half a million peer review invitations covering both accepted and rejected manuscripts, and integrating high-scale bibliographic and editorial records for 37,279 submitting authors, we find that exposure to a manuscript's core ideas significantly influences the future referencing behavior and knowledge of reviewer invitees who decline the review invite. Specifically, declining reviewer invitees who could view concise summaries of the manuscript's core ideas not only increase their citations to the manuscript itself but also demonstrate expanded breadth, depth, diversity, and prominence of citations to the submitting author's broader body of work. Overall, these results suggest peer review substantially influences the spread of scientific knowledge. Ironically, while the massive scale of peer review, entailing millions of reviews annually, often drives policy debates about its costs and burdens, our findings demonstrate that precisely because of this scale, peer review serves as a powerful yet previously unrecognized engine for idea diffusion, which is central to scientific advances and scholarly communication.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Freshness, Persistence and Success of Scientific Teams</title>
<link>https://arxiv.org/abs/2507.12255</link>
<guid>https://arxiv.org/abs/2507.12255</guid>
<content:encoded><![CDATA[
<div> team science, academic teams, success, collaboration, network-driven approach <br /> 
Summary: <br /> 
The study explores the factors contributing to the success of academic teams in scientific knowledge production. By analyzing data on publications and authors, the research highlights the importance of team freshness and persistence in team success. Contrary to popular belief, success is not solely driven by team persistence but also by the freshness of new collaborations built on prior experience. The study suggests that high-impact research tends to emerge early in a team's lifespan, emphasizing the significance of new collaborative ties. Teams open to new collaborations consistently produce better science, and team re-combinations introducing freshness impulses sustain success. Additionally, experienced teams with persistence impulses are linked to earlier impact. Together, the balance of freshness and persistence influences team success at different stages of collaboration. <div>
arXiv:2507.12255v1 Announce Type: cross 
Abstract: Team science dominates scientific knowledge production, but what makes academic teams successful? Using temporal data on 25.2 million publications and 31.8 million authors, we propose a novel network-driven approach to identify and study the success of persistent teams. Challenging the idea that persistence alone drives success, we find that team freshness - new collaborations built on prior experience - is key to success. High impact research tends to emerge early in a team's lifespan. Analyzing complex team overlap, we find that teams open to new collaborative ties consistently produce better science. Specifically, team re-combinations that introduce new freshness impulses sustain success, while persistence impulses from experienced teams are linked to earlier impact. Together, freshness and persistence shape team success across collaboration stages.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fediverse Sharing: Cross-Platform Interaction Dynamics between Threads and Mastodon Users</title>
<link>https://arxiv.org/abs/2502.17926</link>
<guid>https://arxiv.org/abs/2502.17926</guid>
<content:encoded><![CDATA[
<div> federation, social media platforms, Mastodon, Threads, cross-platform interactions <br />
Summary:
Traditional social media platforms are facing criticism, leading users to seek alternatives like Mastodon and Threads. However, this diversification has caused user dispersion. Federation protocols like ActivityPub have been adopted to address these issues, with Mastodon taking the lead in building decentralized networks. Threads joined the federation in March 2024 with its Fediverse Sharing service, enabling interactions between Threads and Mastodon users. A study of interactions between 20,000+ Threads users and 20,000+ Mastodon users over ten months was conducted. This research lays the groundwork for further exploration of cross-platform interactions and integration driven by federation protocols. <br /><br /> <div>
arXiv:2502.17926v2 Announce Type: replace 
Abstract: Traditional social media platforms, once envisioned as digital town squares, now face growing criticism over corporate control, content moderation, and privacy concerns. Events such as Twitter's acquisition (now X) and major policy changes have pushed users toward alternative platforms like Mastodon and Threads. However, this diversification has led to user dispersion and fragmented discussions across the walled gardens of social media platforms. To address these issues, federation protocols like ActivityPub have been adopted, with Mastodon leading efforts to build decentralized yet interconnected networks. In March 2024, Threads joined this federation by introducing its Fediverse Sharing service, which enables interactions such as posts, replies, and likes between Threads and Mastodon users as if on a unified platform. Building on this development, we study the interactions between 20,000+ Threads users and 20,000+ Mastodon users over a ten-month period. Our work lays the foundation for research on cross-platform interactions and federation-driven platform integration.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Clustering in Hypergraphs through Higher-Order Motifs</title>
<link>https://arxiv.org/abs/2507.10570</link>
<guid>https://arxiv.org/abs/2507.10570</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, clustering, higher-order motifs, local structures, computational efficiency

Summary:
This article introduces a novel approach for local clustering in hypergraphs by utilizing higher-order motifs. Traditional graph-based clustering methods often fail to capture higher-order interactions, leading to subpar clustering results. The proposed method leverages hypergraph-specific higher-order motifs to better characterize local structures and optimize motif conductance. Two strategies are presented for identifying local clusters around a seed hyperedge: a core-based approach using hypergraph core decomposition and a BFS-based method involving breadth-first exploration. An auxiliary hypergraph is constructed to facilitate efficient partitioning, along with a framework for local motif-based clustering. Extensive experiments on real-world datasets showcase the effectiveness of the framework, offering a comparative analysis of the two clustering strategies in terms of clustering quality and computational efficiency.
<br /><br />Summary: <div>
arXiv:2507.10570v1 Announce Type: new 
Abstract: Hypergraphs provide a powerful framework for modeling complex systems and networks with higher-order interactions beyond simple pairwise relationships. However, graph-based clustering approaches, which focus primarily on pairwise relations, fail to represent higher-order interactions, often resulting in low-quality clustering outcomes. In this work, we introduce a novel approach for local clustering in hypergraphs based on higher-order motifs, small connected subgraphs in which nodes may be linked by interactions of any order, extending motif-based techniques previously applied to standard graphs. Our method exploits hypergraph-specific higher-order motifs to better characterize local structures and optimize motif conductance. We propose two alternative strategies for identifying local clusters around a seed hyperedge: a core-based method utilizing hypergraph core decomposition and a BFS-based method based on breadth-first exploration. We construct an auxiliary hypergraph to facilitate efficient partitioning and introduce a framework for local motif-based clustering. Extensive experiments on real-world datasets demonstrate the effectiveness of our framework and provide a comparative analysis of the two proposed clustering strategies in terms of clustering quality and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns</title>
<link>https://arxiv.org/abs/2507.10608</link>
<guid>https://arxiv.org/abs/2507.10608</guid>
<content:encoded><![CDATA[
<div> Keywords: anti-money laundering, network theory, behavioral consistency, laundering patterns, pattern fragility

Summary:
The article challenges the conventional approach to anti-money laundering systems by proposing a network-theoretic perspective that focuses on detecting predefined laundering patterns in transaction networks. It emphasizes the importance of behavioral consistency in identifying money laundering activities and argues that patterns are better captured through subgraph structures expressing semantic and functional roles. The concept of pattern fragility is introduced, highlighting the sensitivity of laundering patterns to small attribute changes and their semantic robustness under significant topological transformations. The article suggests that detecting money laundering should not rely on statistical outliers but on the preservation of behavioral essence. This philosophical shift in approach has significant implications for how anti-money laundering systems model, scan, and interpret networks to combat financial crime.

<br /><br />Summary: <div>
arXiv:2507.10608v1 Announce Type: new 
Abstract: Conventional anti-money laundering (AML) systems predominantly focus on identifying anomalous entities or transactions, flagging them for manual investigation based on statistical deviation or suspicious behavior. This paradigm, however, misconstrues the true nature of money laundering, which is rarely anomalous but often deliberate, repeated, and concealed within consistent behavioral routines. In this paper, we challenge the entity-centric approach and propose a network-theoretic perspective that emphasizes detecting predefined laundering patterns across directed transaction networks. We introduce the notion of behavioral consistency as the core trait of laundering activity, and argue that such patterns are better captured through subgraph structures expressing semantic and functional roles - not solely geometry. Crucially, we explore the concept of pattern fragility: the sensitivity of laundering patterns to small attribute changes and, conversely, their semantic robustness even under drastic topological transformations. We claim that laundering detection should not hinge on statistical outliers, but on preservation of behavioral essence, and propose a reconceptualization of pattern similarity grounded in this insight. This philosophical and practical shift has implications for how AML systems model, scan, and interpret networks in the fight against financial crime.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilayer Artificial Benchmark for Community Detection (mABCD)</title>
<link>https://arxiv.org/abs/2507.10795</link>
<guid>https://arxiv.org/abs/2507.10795</guid>
<content:encoded><![CDATA[
<div> random graph model, community detection, power-law distribution, LFR model, multilayer networks

Summary:
The article introduces the Artificial Benchmark for Community Detection (ABCD) model, a random graph model that incorporates community structure and power-law distribution for both degrees and community sizes. This model, similar to the LFR model but faster and more analytically accessible, is used to generate graphs. The article then proposes a variant of the ABCD model for multilayer networks, known as mABCD. This variant leverages the underlying elements of the ABCD model to address community detection in multilayer networks. The mABCD model offers interpretability and efficiency in generating multilayer networks with community structure and power-law distribution. Overall, the introduction of the mABCD model extends the applicability of the ABCD model to the realm of multilayer networks, providing a valuable tool for community detection research. 

<br /><br />Summary: <div>
arXiv:2507.10795v1 Announce Type: new 
Abstract: The Artificial Benchmark for Community Detection (ABCD) model is a random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs similar to the well-known LFR model but it is faster, more interpretable, and can be investigated analytically. In this paper, we use the underlying ingredients of the ABCD model and introduce its variant for multilayer networks, mABCD.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toxicity in State Sponsored Information Operations</title>
<link>https://arxiv.org/abs/2507.10936</link>
<guid>https://arxiv.org/abs/2507.10936</guid>
<content:encoded><![CDATA[
<div> Keywords: state-sponsored information operations, toxic language, social media, engagement metrics, Russian influence operations

Summary: 
This study analyzes toxic language deployment in state-sponsored information operations on social media platforms, specifically focusing on 56 million posts from over 42 thousand accounts linked to 18 geopolitical entities on Twitter. Six categories of toxic content were systematically detected and quantified using Google's Perspective API, revealing that toxic content only makes up 1.53% of all posts but garners high engagement. The distribution of toxic content varied across national origins and linguistic structures, indicating strategic deployment in specific geopolitical contexts. Russian influence operations stood out for receiving significantly higher user engagement compared to other countries in the dataset. The findings provide valuable insights into the emotional and rhetorical strategies used in state-sponsored information operations on social media platforms and highlight the importance of understanding these patterns for effective countermeasures. 

Summary:<br />
1. Analysis of toxic language deployment in state-sponsored information operations on social media platforms<br />
2. Detection and quantification of six categories of toxic content from 56 million posts<br />
3. Toxic content comprises 1.53% of all posts but shows high engagement rates<br />
4. Strategic deployment of toxic content in specific geopolitical contexts<br />
5. Russian influence operations garner significantly higher user engagement compared to other countries.<br /> <div>
arXiv:2507.10936v1 Announce Type: new 
Abstract: State-sponsored information operations (IOs) increasingly influence global discourse on social media platforms, yet their emotional and rhetorical strategies remain inadequately characterized in scientific literature. This study presents the first comprehensive analysis of toxic language deployment within such campaigns, examining 56 million posts from over 42 thousand accounts linked to 18 distinct geopolitical entities on X/Twitter. Using Google's Perspective API, we systematically detect and quantify six categories of toxic content and analyze their distribution across national origins, linguistic structures, and engagement metrics, providing essential information regarding the underlying patterns of such operations. Our findings reveal that while toxic content constitutes only 1.53% of all posts, they are associated with disproportionately high engagement and appear to be strategically deployed in specific geopolitical contexts. Notably, toxic content originating from Russian influence operations receives significantly higher user engagement compared to influence operations from any other country in our dataset. Our code is available at https://github.com/shafin191/Toxic_IO.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban delineation through the lens of commute networks: Leveraging graph embeddings to distinguish socioeconomic groups in cities</title>
<link>https://arxiv.org/abs/2507.11057</link>
<guid>https://arxiv.org/abs/2507.11057</guid>
<content:encoded><![CDATA[
<div> Keywords: urban delineation, commute networks, Graph Neural Network, community detection, socioeconomic disparities 

Summary: 
This study focuses on using commute networks from census data to delineate urban areas, utilizing a Graph Neural Network (GNN) model to create low-dimensional representations of urban nodes. The nodes' embeddings are clustered to identify cohesive communities within urban regions. The research demonstrates the efficacy of network embeddings in capturing socioeconomic disparities, particularly in median household income, across various U.S. cities. The role of census mobility data in regional delineation is highlighted, showcasing the utility of GNNs in urban community detection as a powerful alternative to existing methods. The results emphasize the valuable insights provided by commute networks in shaping meaningful representations of urban regions. 

<br /><br />Summary: <div>
arXiv:2507.11057v1 Announce Type: new 
Abstract: Delineating areas within metropolitan regions stands as an important focus among urban researchers, shedding light on the urban perimeters shaped by evolving population dynamics. Applications to urban science are numerous, from facilitating comparisons between delineated districts and administrative divisions to informing policymakers of the shifting economic and labor landscapes. In this study, we propose using commute networks sourced from the census for the purpose of urban delineation, by modeling them with a Graph Neural Network (GNN) architecture. We derive low-dimensional representations of granular urban areas (nodes) using GNNs. Subsequently, nodes' embeddings are clustered to identify spatially cohesive communities in urban areas. Our experiments across the U.S. demonstrate the effectiveness of network embeddings in capturing significant socioeconomic disparities between communities in various cities, particularly in factors such as median household income. The role of census mobility data in regional delineation is also noted, and we establish the utility of GNNs in urban community detection, as a powerful alternative to existing methods in this domain. The results offer insights into the wider effects of commute networks and their use in building meaningful representations of urban regions.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhance Stability of Network by Edge Anchor</title>
<link>https://arxiv.org/abs/2507.11090</link>
<guid>https://arxiv.org/abs/2507.11090</guid>
<content:encoded><![CDATA[
<div> influential relationships, community stability, anchor trussness reinforcement problem, greedy framework, real-world networks 

Summary: 
- The study focuses on identifying influential relationships in online social networks to enhance community stability.
- The anchor trussness reinforcement problem is introduced to boost user engagement by anchoring specific edges.
- The problem of identifying edges for maximum trussness gain while staying within a given budget is proven to be NP-hard.
- A greedy framework is proposed to iteratively select the best edges for reinforcement, with a focus on efficiency.
- Methods such as the upward-route approach and classification tree structure are introduced to efficiently compute trussness gain and minimize redundant computations in the process.
- Extensive experiments on real-world networks validate the efficiency and effectiveness of the proposed model and methods. <div>
arXiv:2507.11090v1 Announce Type: new 
Abstract: With the rapid growth of online social networks, strengthening their stability has emerged as a key research focus. This study aims to identify influential relationships that significantly impact community stability. In this paper, we introduce and explore the anchor trussness reinforcement problem to reinforce the overall user engagement of networks by anchoring some edges. Specifically, for a given graph $G$ and a budget $b$, we aim to identify $b$ edges whose anchoring maximizes the trussness gain, which is the cumulative increment of trussness across all edges in $G$. We establish the NP-hardness of the problem. To address this problem, we introduce a greedy framework that iteratively selects the current best edge. To scale for larger networks, we first propose an upward-route method to constrain potential trussness increment edges. Augmented with a support check strategy, this approach enables the efficient computation of the trussness gain for anchoring one edge. Then, we design a classification tree structure to minimize redundant computations in each iteration by organizing edges based on their trussness. We conduct extensive experiments on 8 real-world networks to validate the efficiency and effectiveness of the proposed model and methods.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services</title>
<link>https://arxiv.org/abs/2507.10605</link>
<guid>https://arxiv.org/abs/2507.10605</guid>
<content:encoded><![CDATA[
<div> Keywords: social networking services, large language models, RedOne, content management, interaction quality improvement 

Summary:
RedOne is a domain-specific large language model (LLM) developed to address the challenges faced by social networking services (SNS) in content management and interaction quality improvement. It utilizes a three-stage training strategy involving continue pretraining, supervised fine-tuning, and preference optimization using a large-scale real-world dataset. Through extensive experiments, RedOne demonstrates strong capabilities, outperforming base models by up to 14.02% in SNS tasks and 7.56% in a bilingual evaluation benchmark. In online testing, RedOne reduces exposure to harmful content by 11.23% and improves post-view search click rates by 14.95% compared to baseline models. These results establish RedOne as a robust LLM for SNS, showcasing its versatility across various tasks and potential for real-world applications. 

<br /><br />Summary: <div>
arXiv:2507.10605v1 Announce Type: cross 
Abstract: As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler</title>
<link>https://arxiv.org/abs/2507.10810</link>
<guid>https://arxiv.org/abs/2507.10810</guid>
<content:encoded><![CDATA[
<div> Keywords: online hate, social approval theory, Parler, hate speech, social media platforms <br />
Summary: 
This paper explores the relationship between social approval and online hate speech on the Parler platform from 2018 to 2021. The study specifically looks at whether receiving social approval influences the production and extremity of hate speech messages. Contrary to Walther's social approval theory, the study found that the number of upvotes on hate speech posts did not predict future hate speech production. Between-person effects showed a mixed relationship between social approval and hate speech at different time intervals. The findings suggest that social approval reinforcement mechanisms for online hate may vary on niche social media platforms like Parler. Overall, the study challenges existing theories on the role of social approval in motivating online hate speech production. <br /><br /> <div>
arXiv:2507.10810v1 Announce Type: cross 
Abstract: In this paper, we explored how online hate is motivated by receiving social approval from others. We specifically examined two central tenets of Walther's (2024) social approval theory of online hate: (H1a) more signals of social approval on hate messages predicts more subsequent hate messages, and (H1b) as social approval increases, hate speech messages become more extreme. Using over 110 million posts from Parler (2018-2021), we observed that the number of upvotes a person received on a hate speech post was unassociated with the amount of hate speech in their next post and posts during the next week, month, three months, and six months. Between-person effects revealed an average negative relationship between social approval and hate speech production at the post level, but this relationship was mixed at other time intervals. Social approval reinforcement mechanisms of online hate may operate differently on niche social media platforms.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Potential Impact of Disruptive AI Innovations on U.S. Occupations</title>
<link>https://arxiv.org/abs/2507.11403</link>
<guid>https://arxiv.org/abs/2507.11403</guid>
<content:encoded><![CDATA[
<div> disruption index, AI patents, job tasks, consolidating AI, disruptive AI
<br />
Summary:<br />
The study examines the impact of AI innovations on the labor market through a disruption index of U.S. AI patents. It distinguishes between consolidating AI that reinforces existing structures and disruptive AI that alters them. The analysis reveals that consolidating AI targets physical, routine, and solo tasks in manufacturing and construction, mainly in the Midwest and central states. On the other hand, disruptive AI affects unpredictable and mental tasks, particularly in coastal science and technology sectors. Surprisingly, disruptive AI disproportionately affects areas with skilled labor shortages, potentially accelerating change where workers are scarce. Overall, consolidating AI extends current automation trends, while disruptive AI is poised to transform complex mental work, with an exception for collaborative tasks. <div>
arXiv:2507.11403v1 Announce Type: cross 
Abstract: The rapid rise of AI is poised to disrupt the labor market. However, AI is not a monolith; its impact depends on both the nature of the innovation and the jobs it affects. While computational approaches are emerging, there is no consensus on how to systematically measure an innovation's disruptive potential. Here, we calculate the disruption index of 3,237 U.S. AI patents (2015-2022) and link them to job tasks to distinguish between "consolidating" AI innovations that reinforce existing structures and "disruptive" AI innovations that alter them. Our analysis reveals that consolidating AI primarily targets physical, routine, and solo tasks, common in manufacturing and construction in the Midwest and central states. By contrast, disruptive AI affects unpredictable and mental tasks, particularly in coastal science and technology sectors. Surprisingly, we also find that disruptive AI disproportionately affects areas already facing skilled labor shortages, suggesting disruptive AI technologies may accelerate change where workers are scarce rather than replacing a surplus. Ultimately, consolidating AI appears to extend current automation trends, while disruptive AI is set to transform complex mental work, with a notable exception for collaborative tasks.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIF: The hypergraph interchange format for higher-order networks</title>
<link>https://arxiv.org/abs/2507.11520</link>
<guid>https://arxiv.org/abs/2507.11520</guid>
<content:encoded><![CDATA[
<div> Hypergraph, Interchange Format, Higher-order networks, Software packages, Data exchange<br />
Summary: <br />
The article introduces the Hypergraph Interchange Format (HIF), a standardized format for storing higher-order network data. It aims to bring together various software packages used for analyzing complex interactions in systems such as chemical reactions, social groups, and ecological dependencies. HIF supports different types of higher-order networks, including undirected and directed hypergraphs, as well as simplicial complexes. It also includes metadata support for attributes associated with nodes, edges, and incidences. The initiative is a collaborative effort involving authors and contributors from prominent hypergraph software packages. The project provides a JSON schema, documentation, unit tests, example datasets, and tutorials demonstrating HIF's compatibility with popular higher-order network analysis software packages. This standardization of data format will facilitate seamless data exchange and enhance the interoperability of different higher-order network analysis tools. <br /> 
Summary: <div>
arXiv:2507.11520v1 Announce Type: cross 
Abstract: Many empirical systems contain complex interactions of arbitrary size, representing, for example, chemical reactions, social groups, co-authorship relationships, and ecological dependencies. These interactions are known as higher-order interactions and the collection of these interactions comprise a higher-order network, or hypergraph. Hypergraphs have established themselves as a popular and versatile mathematical representation of such systems and a number of software packages written in various programming languages have been designed to analyze these networks. However, the ecosystem of higher-order network analysis software is fragmented due to specialization of each software's programming interface and compatible data representations. To enable seamless data exchange between higher-order network analysis software packages, we introduce the Hypergraph Interchange Format (HIF), a standardized format for storing higher-order network data. HIF supports multiple types of higher-order networks, including undirected hypergraphs, directed hypergraphs, and simplicial complexes, while actively exploring extensions to represent multiplex hypergraphs, temporal hypergraphs, and ordered hypergraphs. To accommodate the wide variety of metadata used in different contexts, HIF also includes support for attributes associated with nodes, edges, and incidences. This initiative is a collaborative effort involving authors, maintainers, and contributors from prominent hypergraph software packages. This project introduces a JSON schema with corresponding documentation and unit tests, example HIF-compliant datasets, and tutorials demonstrating the use of HIF with several popular higher-order network analysis software packages.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-based models to randomize real-world hypergraphs</title>
<link>https://arxiv.org/abs/2207.12123</link>
<guid>https://arxiv.org/abs/2207.12123</guid>
<content:encoded><![CDATA[
<div> Keywords: Network theory, hypergraphs, Exponential Random Hypergraphs, entropy-based approach, real-world data <br />
Summary: 
The article focuses on the importance of considering many-body relationships in network theory through the use of hypergraphs to capture polyadic interactions. By using the representation of hypergraphs based on incidence matrices, the study introduces Exponential Random Hypergraphs (ERHs) as a framework to analyze higher-order structures. The research explores the thresholds and asymptotic behavior of ERHs in comparison to traditional percolation thresholds. Key network metrics are generalized to hypergraphs, allowing for the computation of expected values and comparison against empirical data to identify deviations from random behaviors. The method presented is analytically tractable, scalable, and effective in detecting structural patterns in real-world hypergraphs that differ significantly from simpler constraints. <div>
arXiv:2207.12123v3 Announce Type: replace 
Abstract: Network theory has often disregarded many-body relationships, solely focusing on pairwise interactions: neglecting them, however, can lead to misleading representations of complex systems. Hypergraphs represent a suitable framework for describing polyadic interactions. Here, we leverage the representation of hypergraphs based on the incidence matrix for extending the entropy-based approach to higher-order structures: in analogy with the Exponential Random Graphs, we introduce the Exponential Random Hypergraphs (ERHs). After exploring the asymptotic behaviour of thresholds generalising the percolation one, we apply ERHs to study real-world data. First, we generalise key network metrics to hypergraphs; then, we compute their expected value and compare it with the empirical one, in order to detect deviations from random behaviours. Our method is analytically tractable, scalable and capable of revealing structural patterns of real-world hypergraphs that differ significantly from those emerging as a consequence of simpler constraints.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verified authors shape X/Twitter discursive communities</title>
<link>https://arxiv.org/abs/2405.04896</link>
<guid>https://arxiv.org/abs/2405.04896</guid>
<content:encoded><![CDATA[
<div> Keywords: discursive communities, Twitter, verified users, political events, community detection 

Summary: 
The study investigates the detection of discursive communities on Twitter during key Italian political events in 2022. It focuses on the role of verified users as content creators before paid account verification was introduced. Two novel methodologies, MonoDC and BiDC, were proposed and compared to identify community partitions based on the retweet network and shared audience similarity. Leveraging verified users as indicators of prestige and authority resulted in clear community partitions reflecting actual political affiliations. This approach outperformed standard algorithms applied to the entire retweet network. The study highlights the significant influence of verified users in shaping online discourse, emphasizing the importance of platform governance, especially in light of recent changes to paid verification. <div>
arXiv:2405.04896v2 Announce Type: replace 
Abstract: In this study, we address the challenge of detecting ``discursive communities'' on X/Twitter by focusing on the role of verified users as the main content creators in online political debates. The analysis centers on three major Italian political events in 2022 - the Presidential election, a governmental crisis, and the general elections - occurring before the introduction of paid account verification. We propose and compare two novel methodologies, MonoDC and BiDC, which exploit, respectively, the retweet network among users and a similarity network based on shared audiences, while integrating a maximum entropy null model to filter out the inherent noise in online social networks. Our results demonstrate that leveraging verified users-considered as indicators of prestige and authority-leads to significantly clear community partitions that closely reflect the actual political affiliations, outperforming standard community detection algorithms applied to the entire retweet network. Moreover, the comparison of different methodologies and user sets suggests that the status conferred by the blue verification tick plays a dominant role in shaping online discourse, with important implications for platform governance, especially in light of the recent shift to paid verification.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowd: A Social Network Simulation Framework</title>
<link>https://arxiv.org/abs/2412.10781</link>
<guid>https://arxiv.org/abs/2412.10781</guid>
<content:encoded><![CDATA[
<div> Agent-based modeling, social networks, simulation, Crowd, Python
<br />
Summary:
Crowd is a social network simulator that utilizes agent-based modeling to simulate real-world phenomena within a network environment. It offers easy setup through YAML configuration, customization options, and features like no-code simulations, interactive visualizations, and data aggregation. Developed in Python, Crowd supports generative agents and integrates easily with data analysis and machine learning libraries. The framework is demonstrated through three case studies showcasing its application in modeling generative agents in epidemics, influence maximization, and networked trust games. Crowd addresses the limitations of general-purpose ABMS frameworks by providing specialized tools for social networks and simplifying complex simulations. <div>
arXiv:2412.10781v3 Announce Type: replace 
Abstract: To observe how individual behavior shapes a larger community's actions, agent-based modeling and simulation (ABMS) has been widely adopted by researchers in social sciences, economics, and epidemiology. While simulations can be run on general-purpose ABMS frameworks, these tools are not specifically designed for social networks and, therefore, provide limited features, increasing the effort required for complex simulations. In this paper, we introduce Crowd, a social network simulator that adopts the agent-based modeling methodology to model real-world phenomena within a network environment. Designed to facilitate easy and quick modeling, Crowd supports simulation setup through YAML configuration and enables further customization with user-defined methods. Other features include no-code simulations for diffusion tasks, interactive visualizations, data aggregation, and chart drawing facilities. Designed in Python, Crowd also supports generative agents and connects easily with Python's libraries for data analysis and machine learning. Finally, we include three case studies to illustrate the use of the framework, including generative agents in epidemics, influence maximization, and networked trust games.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Dynamics in Online Public Discourse: A Case Study of Universal Basic Income Discussions on Reddit</title>
<link>https://arxiv.org/abs/2312.09611</link>
<guid>https://arxiv.org/abs/2312.09611</guid>
<content:encoded><![CDATA[
<div> opinion change, online platforms, Universal Basic Income (UBI), Reddit, public discourse
<br />
Summary:
The study explores how public opinion on Universal Basic Income (UBI) has changed on Reddit, an online platform. By analyzing shifts in user cohorts, community affluence levels, and partisan leanings, the researchers found that support for UBI on Reddit initially declined but saw a significant increase starting in mid-2019. The study highlights the potential of online platforms to capture nuanced shifts in public opinion and offers a conceptual model to understand opinion change at a large scale. The findings illustrate the dynamic nature of public discourse and the impact of online discussions on shaping societal attitudes towards policy proposals. This research can be applied to other important issues and policies to gain insights into the diverse range of opinions within a heterogeneous population.
<br /> <div>
arXiv:2312.09611v2 Announce Type: replace-cross 
Abstract: Societal change is often driven by shifts in public opinion. As citizens evolve in their norms, beliefs, and values, public policies change too. While traditional opinion polling and surveys can outline the broad strokes of whether public opinion on a particular topic is changing, they usually cannot capture the full multi-dimensional richness and diversity of opinion present in a large heterogeneous population. However, an increasing fraction of public discourse about public policy issues is now occurring on online platforms, which presents an opportunity to measure public opinion change at a qualitatively different scale of resolution and context.
  In this paper, we present a conceptual model of observed opinion change on online platforms and apply it to study public discourse on Universal Basic Income (UBI) on Reddit throughout its history. UBI is a periodic, no-strings-attached cash payment given to every citizen of a population. We study UBI as it is a clearly-defined policy proposal that has recently experienced a surge of interest through trends like automation and events like the COVID-19 pandemic. We find that overall stance towards UBI on Reddit significantly declined until mid-2019, when this historical trend suddenly reversed and Reddit became substantially more supportive. Using our model, we find the most significant drivers of this overall stance change were shifts within different user cohorts, within communities that represented similar affluence levels, and within communities that represented similar partisan leanings. Our method identifies nuanced social drivers of opinion change in the large-scale public discourse that now regularly occurs online, and could be applied to a broad set of other important issues and policies.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks</title>
<link>https://arxiv.org/abs/2507.09055</link>
<guid>https://arxiv.org/abs/2507.09055</guid>
<content:encoded><![CDATA[
<div> influence centrality, health misinformation vulnerability centrality, propagation centrality, online social networks, misinformation influencers 

Summary: 
- The study introduces three novel centrality metrics, DIC, MVC, and PC, to understand information flow in online social networks during crises like the COVID-19 pandemic.
- These advanced metrics incorporate temporal dynamics, susceptibility, and multilayered network interactions to identify influential nodes, propagation pathways, and misinformation influencers.
- Comparison with traditional metrics showed a 44.83% increase in the identification of influential nodes using the new metrics.
- Implementing interventions based on the new metrics led to a 25% improvement in reducing health misinformation spread compared to baseline interventions.
- The validation on a different dataset confirmed the generalizability of the advanced metrics in identifying influential actors in diverse health misinformation discussions beyond COVID-19. 

<br /><br /> <div>
arXiv:2507.09055v1 Announce Type: new 
Abstract: The rapid spread of health misinformation on online social networks (OSNs) during global crises such as the COVID-19 pandemic poses challenges to public health, social stability, and institutional trust. Centrality metrics have long been pivotal in understanding the dynamics of information flow, particularly in the context of health misinformation. However, the increasing complexity and dynamism of online networks, especially during crises, highlight the limitations of these traditional approaches. This study introduces and compares three novel centrality metrics: dynamic influence centrality (DIC), health misinformation vulnerability centrality (MVC), and propagation centrality (PC). These metrics incorporate temporal dynamics, susceptibility, and multilayered network interactions. Using the FibVID dataset, we compared traditional and novel metrics to identify influential nodes, propagation pathways, and misinformation influencers. Traditional metrics identified 29 influential nodes, while the new metrics uncovered 24 unique nodes, resulting in 42 combined nodes, an increase of 44.83%. Baseline interventions reduced health misinformation by 50%, while incorporating the new metrics increased this to 62.5%, an improvement of 25%. To evaluate the broader applicability of the proposed metrics, we validated our framework on a second dataset, Monant Medical Misinformation, which covers a diverse range of health misinformation discussions beyond COVID-19. The results confirmed that the advanced metrics generalised successfully, identifying distinct influential actors not captured by traditional methods. In general, the findings suggest that a combination of traditional and novel centrality measures offers a more robust and generalisable framework for understanding and mitigating the spread of health misinformation in different online network contexts.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)</title>
<link>https://arxiv.org/abs/2507.09149</link>
<guid>https://arxiv.org/abs/2507.09149</guid>
<content:encoded><![CDATA[
<div> ELM, misinformation detection, CNN, LSTM, social media <br />
Summary: <br />
- Study uses ELM to improve detection of health misinformation on social media.
- Model combines CNN and LSTM to enhance accuracy and reliability of classification.
- Integrates ELM-based features like text readability and sentiment polarity.
- Achieves high performance metrics: accuracy 97.37%, F1-score 97.41%.
- Feature engineering further boosts precision, recall, and ROC-AUC metrics.
- Demonstrates practical application of psychological theories in ML algorithms for misinformation detection.<br /> <div>
arXiv:2507.09149v1 Announce Type: new 
Abstract: Health misinformation during the COVID-19 pandemic has significantly challenged public health efforts globally. This study applies the Elaboration Likelihood Model (ELM) to enhance misinformation detection on social media using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model. The model aims to enhance the detection accuracy and reliability of misinformation classification by integrating ELM-based features such as text readability, sentiment polarity, and heuristic cues (e.g., punctuation frequency). The enhanced model achieved an accuracy of 97.37%, precision of 96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined model incorporating feature engineering further improved performance, achieving a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of 99.80%. These findings highlight the value of ELM features in improving detection performance, offering valuable contextual information. This study demonstrates the practical application of psychological theories in developing advanced machine learning algorithms to address health misinformation effectively.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks</title>
<link>https://arxiv.org/abs/2507.09657</link>
<guid>https://arxiv.org/abs/2507.09657</guid>
<content:encoded><![CDATA[
<div> Keywords: generative agents, social network, temperature decisions, personality traits, happiness

Summary:
This study utilizes generative agents powered by large language models to simulate a social network within a residential building, where they make temperature decisions for a central heating system. The agents, categorized as Family Members and Representatives, take into account personal preferences, traits, connections, and weather conditions. Daily simulations involve reaching consensus at the family level before making building-wide decisions among representatives. The research explores three distributions of personality traits (positive, mixed, and negative) and establishes a correlation between positive traits and increased happiness and stronger friendships. Results show that temperature preferences, assertiveness, and selflessness play a vital role in both happiness and decision-making processes. Overall, this work highlights the effectiveness of using LLM-driven agents to model complex human behavior in scenarios where real-life simulations are challenging. 

<br /><br />Summary: <div>
arXiv:2507.09657v1 Announce Type: new 
Abstract: We use generative agents powered by large language models (LLMs) to simulate a social network in a shared residential building, driving the temperature decisions for a central heating system. Agents, divided into Family Members and Representatives, consider personal preferences, personal traits, connections, and weather conditions. Daily simulations involve family-level consensus followed by building-wide decisions among representatives. We tested three personality traits distributions (positive, mixed, and negative) and found that positive traits correlate with higher happiness and stronger friendships. Temperature preferences, assertiveness, and selflessness have a significant impact on happiness and decisions. This work demonstrates how LLM-driven agents can help simulate nuanced human behavior where complex real-life human simulations are difficult to set.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experimental Analysis and Evaluation of Cohesive Subgraph Discovery</title>
<link>https://arxiv.org/abs/2507.10262</link>
<guid>https://arxiv.org/abs/2507.10262</guid>
<content:encoded><![CDATA[
<div> cohesive subgraphs, networks, social network analysis, graph data management, evaluation

Summary: 
This study evaluates different cohesive subgraph models for their performance in retrieving cohesive subgraphs in networks. The research includes task-based evaluations on synthetic and real-world networks to provide insights into the efficiency and applicability of these models. The findings highlight the balance between interpretability and cohesion of subgraphs, guiding the selection of suitable models for specific analytical needs and applications. The study not only offers a comprehensive evaluation of current models but also sets the foundation for future research in this area. The systematic comparison of performance across varied network configurations fills a gap in the existing literature and provides valuable insights for marketers and recommendation systems. This research contributes to advancing the understanding of cohesive subgraphs in network analysis. 

<br /><br />Summary: <div>
arXiv:2507.10262v1 Announce Type: new 
Abstract: Retrieving cohesive subgraphs in networks is a fundamental problem in social network analysis and graph data management. These subgraphs can be used for marketing strategies or recommendation systems. Despite the introduction of numerous models over the years, a systematic comparison of their performance, especially across varied network configurations, remains unexplored. In this study, we evaluated various cohesive subgraph models using task-based evaluations and conducted extensive experimental studies on both synthetic and real-world networks. Thus, we unveil the characteristics of cohesive subgraph models, highlighting their efficiency and applicability. Our findings not only provide a detailed evaluation of current models but also lay the groundwork for future research by shedding light on the balance between the interpretability and cohesion of the subgraphs. This research guides the selection of suitable models for specific analytical needs and applications, providing valuable insights.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowsDT: A Geospatial Digital Twin for Navigating Urban Flood Dynamics</title>
<link>https://arxiv.org/abs/2507.08850</link>
<guid>https://arxiv.org/abs/2507.08850</guid>
<content:encoded><![CDATA[
<div> Keywords: flood analysis, hydrodynamic modeling, digital twin, urban resilience, Galveston City

Summary:
The study focuses on using advanced hydrodynamic modeling and digital twin technology to enhance flood analysis and forecasting in Galveston City. By developing a geospatial digital twin (GDT) and validating it with historical data, the researchers were able to model hyperlocal flood conditions under various rainfall scenarios. The model, FlowsDT-Galveston, accurately simulates flood depth, extent, duration, and velocity in a 4-D environment, identifying at-risk zones in the city. Results show an increase in building and road inundations with higher return period rainfall scenarios. This innovative approach can support proactive flood management and urban planning in Galveston, informing disaster resilience efforts and guiding sustainable infrastructure development. The framework established in this study can be applied to other communities facing similar flood hazard challenges.<br /><br />Summary: <div>
arXiv:2507.08850v1 Announce Type: cross 
Abstract: Communities worldwide increasingly confront flood hazards intensified by climate change, urban expansion, and environmental degradation. Addressing these challenges requires real-time flood analysis, precise flood forecasting, and robust risk communications with stakeholders to implement efficient mitigation strategies. Recent advances in hydrodynamic modeling and digital twins afford new opportunities for high-resolution flood modeling and visualization at the street and basement levels. Focusing on Galveston City, a barrier island in Texas, U.S., this study created a geospatial digital twin (GDT) supported by 1D-2D coupled hydrodynamic models to strengthen urban resilience to pluvial and fluvial flooding. The objectives include: (1) developing a GDT (FlowsDT-Galveston) incorporating topography, hydrography, and infrastructure; (2) validating the twin using historical flood events and social sensing; (3) modeling hyperlocal flood conditions under 2-, 10-, 25-, 50-, and 100-year return period rainfall scenarios; and (4) identifying at-risk zones under different scenarios. This study employs the PCSWMM to create dynamic virtual replicas of urban landscapes and accurate flood modeling. By integrating LiDAR data, land cover, and storm sewer geometries, the model can simulate flood depth, extent, duration, and velocity in a 4-D environment across different historical and design storms. Results show buildings inundated over one foot increased by 5.7% from 2- to 100-year flood. Road inundations above 1 foot increased by 6.7% from 2- to 100-year floods. The proposed model can support proactive flood management and urban planning in Galveston; and inform disaster resilience efforts and guide sustainable infrastructure development. The framework can be extended to other communities facing similar challenges.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions</title>
<link>https://arxiv.org/abs/2507.08881</link>
<guid>https://arxiv.org/abs/2507.08881</guid>
<content:encoded><![CDATA[
<div> consistency, acceptability, large language model, judicial systems, governance framework
Summary: 
The article discusses the integration of large language models (LLM) in judicial systems and highlights the emerging paradox of "consistency-acceptability divergence." This refers to the gap between the technical consistency of LLMs and their social acceptance. The study analyzes data from 2023-2025 and identifies the need to balance technical efficiency with social legitimacy in LLM judicial applications. The proposed Dual-Track Deliberative Multi-Role LLM Judicial Governance Framework (DTDMR-LJGF) aims to address this challenge by enabling intelligent task classification and fostering meaningful interactions among stakeholders. By understanding both the task and stakeholder dimensions, this framework provides theoretical insights and practical guidance for building a more balanced and effective LLM judicial ecosystem. <br /><br /> <div>
arXiv:2507.08881v1 Announce Type: cross 
Abstract: The integration of large language model (LLM) technology into judicial systems is fundamentally transforming legal practice worldwide. However, this global transformation has revealed an urgent paradox requiring immediate attention. This study introduces the concept of ``consistency-acceptability divergence'' for the first time, referring to the gap between technical consistency and social acceptance. While LLMs achieve high consistency at the technical level, this consistency demonstrates both positive and negative effects. Through comprehensive analysis of recent data on LLM judicial applications from 2023--2025, this study finds that addressing this challenge requires understanding both task and stakeholder dimensions. This study proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance Framework (DTDMR-LJGF), which enables intelligent task classification and meaningful interaction among diverse stakeholders. This framework offers both theoretical insights and practical guidance for building an LLM judicial ecosystem that balances technical efficiency with social legitimacy.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Te Ahorr\'e Un Click: A Revised Definition of Clickbait and Detection in Spanish News</title>
<link>https://arxiv.org/abs/2507.09777</link>
<guid>https://arxiv.org/abs/2507.09777</guid>
<content:encoded><![CDATA[
<div> clickbait, curiosity gap, sensationalism, headlines, clickbait detection

Summary:
- The definition of clickbait is revised to highlight the creation of a curiosity gap as the distinguishing factor.
- Clickbait is defined as a technique that deliberately omits information to raise curiosity and entice clicks.
- A new dataset, TA1C, for clickbait detection in Spanish is introduced, consisting of 3,500 tweets from 18 media sources.
- The dataset has reached a high inter-annotator agreement of 0.825 Fleiss' K.
- Strong baselines are implemented, achieving a F1-score of 0.84 in clickbait detection.

Summary: <div>
arXiv:2507.09777v1 Announce Type: cross 
Abstract: We revise the definition of clickbait, which lacks current consensus, and argue that the creation of a curiosity gap is the key concept that distinguishes clickbait from other related phenomena such as sensationalism and headlines that do not deliver what they promise or diverge from the article. Therefore, we propose a new definition: clickbait is a technique for generating headlines and teasers that deliberately omit part of the information with the goal of raising the readers' curiosity, capturing their attention and enticing them to click. We introduce a new approach to clickbait detection datasets creation, by refining the concept limits and annotations criteria, minimizing the subjectivity in the decision as much as possible. Following it, we created and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the first open source dataset for clickbait detection in Spanish. It consists of 3,500 tweets coming from 18 well known media sources, manually annotated and reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong baselines that achieve 0.84 in F1-score.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantification of Interdependent Emotion Dynamics in Online Interactions</title>
<link>https://arxiv.org/abs/2408.05700</link>
<guid>https://arxiv.org/abs/2408.05700</guid>
<content:encoded><![CDATA[
<div> Keywords: online interactions, emotions, Hawkes self-exciting point process, peer interactions, emotional manipulation <br />
Summary: <br />
- The study focuses on understanding the dynamics of emotions in online interactions, particularly in YouTube Live chats.
- A multivariate Hawkes self-exciting point process is used to model the expression of six basic emotions, considering both external video content and social feedback.
- Emotional expressions in YouTube Live chats are influenced more by peer interactions than video content, with positivity being more contagious and negativity lingering longer.
- Negative emotions often trigger positive ones in a pattern consistent with trolling dynamics, suggesting the risks of emotional manipulation in online interactions.
- The findings emphasize the significant role of social interaction in shaping emotional dynamics online, particularly as human-chatbot interactions become more realistic. <br /> <br /> <div>
arXiv:2408.05700v3 Announce Type: replace 
Abstract: A growing share of human interactions now occurs online, where the expression and perception of emotions are often amplified and distorted. Yet, the interplay between different emotions and the extent to which they are driven by external stimuli or social feedback remains poorly understood. We calibrate a multivariate Hawkes self-exciting point process to model the temporal expression of six basic emotions in YouTube Live chats. This framework captures both temporal and cross-emotional dependencies while allowing us to disentangle the influence of video content (exogenous) from peer interactions (endogenous). We find that emotional expressions are up to four times more strongly driven by peer interaction than by video content. Positivity is more contagious, spreading three times more readily, whereas negativity is more memorable, lingering nearly twice as long. Moreover, we observe asymmetric cross-excitation, with negative emotions frequently triggering positive ones, a pattern consistent with trolling dynamics, but not the reverse. These findings highlight the central role of social interaction in shaping emotional dynamics online and the risks of emotional manipulation as human-chatbot interactions become increasingly realistic.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introduction to correlation networks: Interdisciplinary approaches beyond thresholding</title>
<link>https://arxiv.org/abs/2311.09536</link>
<guid>https://arxiv.org/abs/2311.09536</guid>
<content:encoded><![CDATA[
<div> networks, correlation, thresholding, weighted networks, regularization

Summary:
This article addresses the challenge of constructing and analyzing correlation networks in various fields such as psychology, neuroscience, genomics, and finance. It highlights the limitations of the traditional method of thresholding on correlation values and explores alternative approaches such as weighted networks, regularization, dynamic correlation networks, and threshold-free methods. The article also discusses the importance of comparing networks with null models and presents key open questions in the field. Overall, the review emphasizes the need for cross-disciplinary collaboration and recommends best practices for transforming correlational data into meaningful networks. <div>
arXiv:2311.09536v3 Announce Type: replace-cross 
Abstract: Many empirical networks originate from correlational data, arising in domains as diverse as psychology, neuroscience, genomics, microbiology, finance, and climate science. Specialized algorithms and theory have been developed in different application domains for working with such networks, as well as in statistics, network science, and computer science, often with limited communication between practitioners in different fields. This leaves significant room for cross-pollination across disciplines. A central challenge is that it is not always clear how to best transform correlation matrix data into networks for the application at hand, and probably the most widespread method, i.e., thresholding on the correlation value to create either unweighted or weighted networks, suffers from multiple problems. In this article, we review various methods of constructing and analyzing correlation networks, ranging from thresholding and its improvements to weighted networks, regularization, dynamic correlation networks, threshold-free approaches, comparison with null models, and more. Finally, we propose and discuss recommended practices and a variety of key open questions currently confronting this field.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media</title>
<link>https://arxiv.org/abs/2504.12355</link>
<guid>https://arxiv.org/abs/2504.12355</guid>
<content:encoded><![CDATA[
<div> Keywords: drug overdose, social media, AI-driven NLP framework, multi-class classification, personalized intervention

Summary: 
The study focuses on the use of social media data and AI technology to detect drug misuse and overdose symptoms. By training a framework using a combination of large language models (LLMs) and human annotators, the researchers achieved high accuracy rates in classifying commonly used drugs and associated symptoms. The framework outperformed baseline models, showcasing its potential for real-time public health surveillance. With a 98% accuracy in multi-class classification and 97% in multi-label classification, the AI-driven approach offers new opportunities for personalized intervention strategies in combating drug overdose. This research demonstrates the effectiveness of leveraging AI and social media for timely insights and targeted interventions in addressing the global health challenge of substance misuse. 

<br /><br />Summary: <div>
arXiv:2504.12355v2 Announce Type: replace-cross 
Abstract: Drug overdose remains a critical global health issue, often driven by misuse of opioids, painkillers, and psychiatric medications. Traditional research methods face limitations, whereas social media offers real-time insights into self-reported substance use and overdose symptoms. This study proposes an AI-driven NLP framework trained on annotated social media data to detect commonly used drugs and associated overdose symptoms. Using a hybrid annotation strategy with LLMs and human annotators, we applied traditional ML models, neural networks, and advanced transformer-based models. Our framework achieved 98% accuracy in multi-class and 97% in multi-label classification, outperforming baseline models by up to 8%. These findings highlight the potential of AI for supporting public health surveillance and personalized intervention strategies.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Propaganda in Tweets From Politically Biased Sources</title>
<link>https://arxiv.org/abs/2507.08169</link>
<guid>https://arxiv.org/abs/2507.08169</guid>
<content:encoded><![CDATA[
arXiv:2507.08169v1 Announce Type: new 
Abstract: News outlets are well known to have political associations, and many national outlets cultivate political biases to cater to different audiences. Journalists working for these news outlets have a big impact on the stories they cover. In this work, we present a methodology to analyze the role of journalists, affiliated with popular news outlets, in propagating their bias using some form of propaganda-like language. We introduce JMBX(Journalist Media Bias on X), a systematically collected and annotated dataset of 1874 tweets from Twitter (now known as X). These tweets are authored by popular journalists from 10 news outlets whose political biases range from extreme left to extreme right. We extract several insights from the data and conclude that journalists who are affiliated with outlets with extreme biases are more likely to use propaganda-like language in their writings compared to those who are affiliated with outlets with mild political leans. We compare eight different Large Language Models (LLM) by OpenAI and Google. We find that LLMs generally performs better when detecting propaganda in social media and news article compared to BERT-based model which is fine-tuned for propaganda detection. While the performance improvements of using large language models (LLMs) are significant, they come at a notable monetary and environmental cost. This study provides an analysis of both the financial costs, based on token usage, and the environmental impact, utilizing tools that estimate carbon emissions associated with LLM operations.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing overlapping communities in multiple-source detection: An edge clustering approach for complex networks</title>
<link>https://arxiv.org/abs/2507.08265</link>
<guid>https://arxiv.org/abs/2507.08265</guid>
<content:encoded><![CDATA[
arXiv:2507.08265v1 Announce Type: new 
Abstract: The source detection problem in network analysis involves identifying the origins of diffusion processes, such as disease outbreaks or misinformation propagation. Traditional methods often focus on single sources, whereas real-world scenarios frequently involve multiple sources, complicating detection efforts. This study addresses the multiple-source detection (MSD) problem by integrating edge clustering algorithms into the community-based label propagation framework, effectively handling mixed-membership issues where nodes belong to multiple communities.
  The proposed approach applies the automated latent space edge clustering model to a network, partitioning infected networks into edge-based clusters to identify multiple sources. Simulation studies on ADD HEALTH social network datasets demonstrate that this method achieves superior accuracy, as measured by the F1-Measure, compared to state-of-the-art clustering algorithms. The results highlight the robustness of edge clustering in accurately detecting sources, particularly in networks with complex and overlapping source regions. This work advances the applicability of clustering-based methods to MSD problems, offering improved accuracy and adaptability for real-world network analyses.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering High-Order Cohesive Structures: Efficient (k,g)-Core Computation and Decomposition for Large Hypergraphs</title>
<link>https://arxiv.org/abs/2507.08328</link>
<guid>https://arxiv.org/abs/2507.08328</guid>
<content:encoded><![CDATA[
arXiv:2507.08328v1 Announce Type: new 
Abstract: Hypergraphs, increasingly utilised to model complex and diverse relationships in modern networks, have gained significant attention for representing intricate higher-order interactions. Among various challenges, cohesive subgraph discovery is one of the fundamental problems and offers deep insights into these structures, yet the task of selecting appropriate parameters is an open question. To address this question, we aim to design an efficient indexing structure to retrieve cohesive subgraphs in an online manner. The main idea is to enable the discovery of corresponding structures within a reasonable time without the need for exhaustive graph traversals. Our method enables faster and more effective retrieval of cohesive structures, which supports decision-making in applications that require online analysis of large-scale hypergraphs. Through extensive experiments on real-world networks, we demonstrate the superiority of our proposed indexing technique.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Evolutionary Graph Theory</title>
<link>https://arxiv.org/abs/2507.08363</link>
<guid>https://arxiv.org/abs/2507.08363</guid>
<content:encoded><![CDATA[
arXiv:2507.08363v1 Announce Type: new 
Abstract: The stability of communities - whether biological, social, economic, technological or ecological depends on the balance between cooperation and cheating. While cooperation strengthens communities, selfish individuals, or "cheaters," exploit collective benefits without contributing. If cheaters become too prevalent, they can trigger the collapse of cooperation and of the community, often in an abrupt manner. A key challenge is determining whether the risk of such a collapse can be detected in advance. To address this, we use a combination of evolutionary graph theory and machine learning to examine how one can predict the unravel of cooperation on complex networks. By introducing few cheaters into a structured population, we employ machine learning to detect and anticipate the spreading of cheaters and cooperation collapse. Using temporal and structural data, the presented results show that prediction accuracy improves with stronger selection strength and larger observation windows, with CNN-Seq-LSTM and Seq-LSTM best performing models. Moreover, the accuracy for the predictions depends crucially on the type of game played between cooperators and cheaters (i.e., accuracy improves when it is more advantageous to defect) and on the community structure. Overall, this work introduces a machine learning approach into detecting abrupt shifts in evolutionary graph theory and offer potential strategies for anticipating and preventing cooperation collapse in complex social networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Feedback Enhances Community-Based Content Moderation through Engagement with Counterarguments</title>
<link>https://arxiv.org/abs/2507.08110</link>
<guid>https://arxiv.org/abs/2507.08110</guid>
<content:encoded><![CDATA[
arXiv:2507.08110v1 Announce Type: cross 
Abstract: Today, social media platforms are significant sources of news and political communication, but their role in spreading misinformation has raised significant concerns. In response, these platforms have implemented various content moderation strategies. One such method, Community Notes on X, relies on crowdsourced fact-checking and has gained traction, though it faces challenges such as partisan bias and delays in verification. This study explores an AI-assisted hybrid moderation framework in which participants receive AI-generated feedback -supportive, neutral, or argumentative -on their notes and are asked to revise them accordingly. The results show that incorporating feedback improves the quality of notes, with the most substantial gains resulting from argumentative feedback. This underscores the value of diverse perspectives and direct engagement in human-AI collective intelligence. The research contributes to ongoing discussions about AI's role in political content moderation, highlighting the potential of generative AI and the importance of informed design.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Motifs for Financial Networks: A Study on Mercari, JPMC, and Venmo Platforms</title>
<link>https://arxiv.org/abs/2301.07791</link>
<guid>https://arxiv.org/abs/2301.07791</guid>
<content:encoded><![CDATA[
arXiv:2301.07791v2 Announce Type: replace 
Abstract: Understanding the dynamics of financial transactions among people is critical for various applications such as fraud detection. One important aspect of financial transaction networks is temporality. The order and repetition of transactions can offer new insights when considered within the graph structure. Temporal motifs, defined as a set of nodes that interact with each other in a short time period, are a promising tool in this context. In this work, we study three unique temporal financial networks: transactions in Mercari, an online marketplace, payments in a synthetic network generated by J.P. Morgan Chase, and payments and friendships among Venmo users. We consider the fraud detection problem on the Mercari and J.P. Morgan Chase networks, for which the ground truth is available. We show that temporal motifs offer superior performance to several baselines, including a previous method that considers simple graph features and two node embedding techniques (LINE and node2vec), while being practical in terms of runtime performance. For the Venmo network, we investigate the interplay between financial and social relations on three tasks: friendship prediction, vendor identification, and analysis of temporal cycles. For friendship prediction, temporal motifs yield better results than general heuristics, such as Jaccard and Adamic-Adar measures. We are also able to identify vendors with high accuracy and observe interesting patterns in rare motifs, such as temporal cycles. We believe that the analysis, datasets, and lessons from this work will be beneficial for future research on financial transaction networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signed Diverse Multiplex Networks: Clustering and Inference</title>
<link>https://arxiv.org/abs/2402.10242</link>
<guid>https://arxiv.org/abs/2402.10242</guid>
<content:encoded><![CDATA[
arXiv:2402.10242v3 Announce Type: replace 
Abstract: The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG) model, which is a variant of the Generalized Random Dot Product Graph (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models, including GRDPG, as its particular cases.
  By employing novel methodologies, our paper ensures strongly consistent clustering of layers and highly accurate subspace estimation, which are significant improvements over the results of Pensky and Wang (2024). All algorithms and theoretical results in the paper remain true for both signed and binary networks. In addition, the paper shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and clustering and, hence, is beneficial for tackling real world problems such as, for example, analysis of brain networks.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incivility and Contentiousness Spillover in Public Engagement with Public Health and Climate Science</title>
<link>https://arxiv.org/abs/2502.05255</link>
<guid>https://arxiv.org/abs/2502.05255</guid>
<content:encoded><![CDATA[
arXiv:2502.05255v2 Announce Type: replace 
Abstract: Affective polarization and political sorting drive public antagonism around issues at the science-policy nexus. Looking at the COVID-19 period, we study cross-domain spillover of incivility and contentiousness in public engagements with climate change and public health on Twitter and Reddit. We find strong evidence of the signatures of affective polarization surrounding COVID-19 spilling into the climate change domain. Across different social media systems, COVID-19 content is associated with incivility and contentiousness in climate discussions. These patterns of increased antagonism were responsive to pandemic events that made the link between science and public policy more salient. The observed spillover activated along pre-pandemic political cleavages, specifically anti-internationalist populist beliefs, that linked climate policy opposition to vaccine hesitancy. Our findings show how affective polarization in public engagement with science becomes entrenched across science policy domains.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Probabilistic Propagation in Graphs by Adding Edges</title>
<link>https://arxiv.org/abs/2407.02624</link>
<guid>https://arxiv.org/abs/2407.02624</guid>
<content:encoded><![CDATA[
arXiv:2407.02624v3 Announce Type: replace-cross 
Abstract: Probabilistic graphs are an abstraction that allow us to study randomized propagation in graphs. In a probabilistic graph, each edge is "active" with a certain probability, independent of the other edges. For two vertices $u,v$, a classic quantity of interest, that we refer to as the proximity $\mathcal{P}_{G}(u, v)$, is the probability that there exists a path between $u$ and $v$ all of whose edges are active. For a given subset of vertices $V_s$, the reach of $V_s$ is defined as the minimum over pairs $u \in V_s$ and $v \in V$ of the proximity $\mathcal{P}_{G}(u,v)$. This quantity has been studied in the context of multicast in unreliable communication networks and in social network analysis.
  We study the problem of improving the reach in a probabilistic graph via edge augmentation. Formally, given a budget $k$ of edge additions and a set of source vertices $V_s$, the goal of Reach Improvement is to maximize the reach of $V_s$ by adding at most $k$ new edges to the graph. The problem was introduced in earlier empirical work in the algorithmic fairness community. We provide the first approximation guarantees and hardness results for Reach Improvement.
  We prove that the existence of a good augmentation implies a cluster structure for the graph. We use this structural result to analyze a novel algorithm that outputs a $k$-edge augmentation with an objective value that is poly($\beta^*$), where $\beta^*$ is the objective value for the optimal augmentation. We also give an algorithm that adds $O(k \log n)$ edges and yields a multiplicative approximation to $\beta^*$. Our arguments rely on new probabilistic tools for analyzing proximity, inspired by techniques in percolation theory; these tools may be of broader interest. Finally, we show that significantly better approximations are unlikely, under known hardness assumptions related to gap variants of the classic Set Cover problem.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Signed Exponential Random Graph Models under Local Dependence</title>
<link>https://arxiv.org/abs/2507.07660</link>
<guid>https://arxiv.org/abs/2507.07660</guid>
<content:encoded><![CDATA[
<div> Keywords: network analysis, signed interactions, Stochastic Block Models, Exponential Family Random Graph Models, structural balance theory

Summary: 
Traditional network analysis typically focuses on binary relationships, but real-world interactions can be more complex, involving cooperation, neutrality, and conflict. The emergence of negative interactions in social media has led to increased interest in analyzing signed interactions in polarized debates. However, analyzing large digital networks poses challenges for traditional methods like Stochastic Block Models (SBM) and Exponential Family Random Graph Models (ERGM). This new method combines the strengths of SBM and ERGM while addressing their weaknesses, incorporating local dependence based on non-overlapping blocks. The approach involves decomposing the network into sub-networks using SBM and then estimating parameters using ERGM. Validation on synthetic networks and application to a signed Wikipedia network of editors reveal patterns that align with structural balance theory. <div>
arXiv:2507.07660v1 Announce Type: new 
Abstract: Traditional network analysis focuses on binary edges, while real-world relationships are more nuanced, encompassing cooperation, neutrality, and conflict. The rise of negative edges in social media discussions spurred interest in analyzing signed interactions, especially in polarized debates. However, the vast data generated by digital networks presents challenges for traditional methods like Stochastic Block Models (SBM) and Exponential Family Random Graph Models (ERGM), particularly due to the homogeneity assumption and global dependence, which become increasingly unrealistic as network size grows. To address this, we propose a novel method that combines the strengths of SBM and ERGM while mitigating their weaknesses by incorporating local dependence based on non-overlapping blocks. Our approach involves a two-step process: first, decomposing the network into sub-networks using SBM approximation, and then estimating parameters using ERGM methods. We validate our method on large synthetic networks and apply it to a signed Wikipedia network of thousands of editors. Through the use of local dependence, we find patterns consistent with structural balance theory.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Connectivity: Higher-Order Network Framework for Capturing Memory-Driven Mobility Dynamics</title>
<link>https://arxiv.org/abs/2507.07727</link>
<guid>https://arxiv.org/abs/2507.07727</guid>
<content:encoded><![CDATA[
<div> Framework, higher-order network, transportation systems, memory-dependent dynamics, predictive accuracy

Summary:
The study introduces a novel higher-order network framework for modeling memory-dependent dynamics in transportation systems. This framework extends traditional graph representations to capture sequential dependencies in real-world mobility patterns. By incorporating higher-order Markov chains and de Bruijn graph structures, the framework encodes spatial and temporal ordering of paths, improving the analysis of critical components in transportation networks. The study generalizes network analytics such as betweenness centrality and PageRank to this higher-order setting and validates the approach on the Sioux Falls transportation network. Experimental results show that higher-order models outperform first-order baselines for tasks such as next-step prediction. The third-order model strikes a balance between predictive accuracy and model complexity, demonstrating the importance of incorporating memory effects in transportation analysis. This scalable, data-driven methodology offers insights into complex mobility behaviors in infrastructure systems. 

<br /><br /> <div>
arXiv:2507.07727v1 Announce Type: new 
Abstract: Understanding and predicting mobility dynamics in transportation networks is critical for infrastructure planning, resilience analysis, and traffic management. Traditional graph-based models typically assume memoryless movement, limiting their ability to capture sequential dependencies inherent in real-world mobility patterns. In this study, we introduce a novel higher-order network framework for modeling memory-dependent dynamics in transportation systems. By extending classical graph representations through higher-order Markov chains and de Bruijn graph structures, our framework encodes the spatial and temporal ordering of traversed paths, enabling the analysis of structurally and functionally critical components with improved fidelity. We generalize key network analytics, including betweenness centrality, PageRank, and next-step prediction, to this higher-order setting and validate our approach on the Sioux Falls transportation network using agent-based trajectory data generated with MATSim. Experimental results demonstrate that higher-order models outperform first-order baselines across multiple tasks, with the third-order model achieving an optimal balance between predictive accuracy and model complexity. These findings highlight the importance of incorporating memory effects into network-based transportation analysis and offer a scalable, data-driven methodology for capturing complex mobility behaviors in infrastructure systems.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conspiracy to Commit: Information Pollution, Artificial Intelligence, and Real-World Hate Crime</title>
<link>https://arxiv.org/abs/2507.07884</link>
<guid>https://arxiv.org/abs/2507.07884</guid>
<content:encoded><![CDATA[
<div> conspiracy theories, online search trends, hate crimes, 1D-CNN, machine learning

Summary:
This study looks at the relationship between online search trends for racially and politically charged conspiracy theories in Michigan from 2015 to 2019 and hate crime occurrences offline. Using a one-dimensional convolutional neural network (1D-CNN), the researchers found that specific conspiracy theories like the Rothschilds family, Q-Anon, and The Great Replacement were linked to an increase in hate crimes two to three weeks after search spikes. However, most theories did not show a clear connection to offline hate crimes. The findings support neutralization and differential association theories, suggesting that certain conspiracy theories can contribute to real-world violence. Additionally, the study highlights the potential for machine learning to identify harmful online patterns and advance social science research. <div>
arXiv:2507.07884v1 Announce Type: new 
Abstract: Is demand for conspiracy theories online linked to real-world hate crimes? By analyzing online search trends for 36 racially and politically-charged conspiracy theories in Michigan (2015-2019), we employ a one-dimensional convolutional neural network (1D-CNN) to predict hate crime occurrences offline. A subset of theories including the Rothschilds family, Q-Anon, and The Great Replacement improves prediction accuracy, with effects emerging two to three weeks after fluctuations in searches. However, most theories showed no clear connection to offline hate crimes. Aligning with neutralization and differential association theories, our findings provide a partial empirical link between specific racially charged conspiracy theories and real-world violence. Just as well, this study underscores the potential for machine learning to be used in identifying harmful online patterns and advancing social science research.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Adaptive Estimation of Local Triadic Coefficients</title>
<link>https://arxiv.org/abs/2507.07536</link>
<guid>https://arxiv.org/abs/2507.07536</guid>
<content:encoded><![CDATA[
<div> local triadic coefficients, graph properties, graph partitioning, sampling, collaboration networks

Summary: 
The article introduces the concept of local triadic coefficients, such as the local clustering coefficient and local closure coefficient, which are crucial in analyzing networked systems. The focus is on efficiently computing the average local triadic coefficients for a partitioned graph. Due to the infeasibility of exact computation for large networks, the Triad algorithm is developed based on sampling to provide highly accurate probabilistic estimates. Triad utilizes unbiased estimators and offers non-trivial bounds on sample complexity, enabling efficient computation. The algorithm is demonstrated to be effective in capturing high-order patterns in collaboration networks. The study highlights the importance of local triadic coefficients in understanding graph structures and node properties, offering insights for various applications ranging from graph embeddings to network analysis.<br /><br />Summary: <div>
arXiv:2507.07536v1 Announce Type: cross 
Abstract: Characterizing graph properties is fundamental to the analysis and to our understanding of real-world networked systems. The local clustering coefficient, and the more recently introduced, local closure coefficient, capture powerful properties that are essential in a large number of applications, ranging from graph embeddings to graph partitioning. Such coefficients capture the local density of the neighborhood of each node, considering incident triadic structures and paths of length two. For this reason, we refer to these coefficients collectively as local triadic coefficients.
  In this work, we consider the novel problem of computing efficiently the average of local triadic coefficients, over a given partition of the nodes of the input graph into a set of disjoint buckets. The average local triadic coefficients of the nodes in each bucket provide a better insight into the interplay of graph structure and the properties of the nodes associated to each bucket. Unfortunately, exact computation, which requires listing all triangles in a graph, is infeasible for large networks. Hence, we focus on obtaining highly-accurate probabilistic estimates.
  We develop Triad, an adaptive algorithm based on sampling, which can be used to estimate the average local triadic coefficients for a partition of the nodes into buckets. Triad is based on a new class of unbiased estimators, and non-trivial bounds on its sample complexity, enabling the efficient computation of highly accurate estimates. Finally, we show how Triad can be efficiently used in practice on large networks, and we present a case study showing that average local triadic coefficients can capture high-order patterns over collaboration networks.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion of complex contagions is shaped by a trade-off between reach and reinforcement</title>
<link>https://arxiv.org/abs/2411.07907</link>
<guid>https://arxiv.org/abs/2411.07907</guid>
<content:encoded><![CDATA[
<div> social network structure, behavior diffusion, clustered networks, random networks, social reinforcement

Summary:<br /><br />Existing theory suggests that behavior diffusion is influenced by social reinforcement and network structure. A new model was developed to analyze the impact of clustered and random networks on behavior spread. The study found that random networks often outperform clustered networks in spreading behavior, especially when social reinforcement increases adoption. Clustered networks may have an advantage only in specific conditions, such as when adoption is nearly deterministic. Additionally, clustered networks are less advantageous when individuals remain influential after adopting, have more neighbors, or require more neighbors for social reinforcement. The research highlights a tradeoff between random ties, which enhance reach, and clustered ties, which enhance social reinforcement. Ultimately, clustered networks outperform random networks by a small margin in only a minority of scenarios, showcasing the complexity of behavior diffusion on social networks. <div>
arXiv:2411.07907v2 Announce Type: replace 
Abstract: How does social network structure amplify or stifle behavior diffusion? Existing theory suggests that when social reinforcement makes the adoption of behavior more likely, it should spread more -- both farther and faster -- on clustered networks with redundant ties. Conversely, if adoption does not benefit from social reinforcement, it should spread more on random networks which avoid such redundancies. We develop a novel model of behavior diffusion with tunable probabilistic adoption and social reinforcement parameters to systematically evaluate the conditions under which clustered networks spread behavior better than random networks. Using simulations and analytical methods, we identify precise boundaries in the parameter space where one network type outperforms the other or they perform equally. We find that, in most cases, random networks spread behavior as far or farther than clustered networks, even when social reinforcement increases adoption. Although we find that probabilistic, socially reinforced behaviors can spread farther on clustered networks in some cases, this is not the dominant pattern. Clustered networks are even less advantageous when individuals remain influential for longer after adopting, have more neighbors, or need more neighbors before social reinforcement takes effect. Under such conditions, clustering tends to help only when adoption is nearly deterministic, which is not representative of socially reinforced behaviors more generally. Clustered networks outperform random networks by a 5% margin in only 22% of the parameter space under its most favorable conditions. This pattern reflects a fundamental tradeoff: random ties enhance reach, while clustered ties enhance social reinforcement.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dialects Collide: How Socioeconomic Mixing Affects Language Use</title>
<link>https://arxiv.org/abs/2307.10016</link>
<guid>https://arxiv.org/abs/2307.10016</guid>
<content:encoded><![CDATA[
<div> Keywords: sociolinguistics, language variation, socioeconomic background, geotagged tweets, agent-based model

Summary: 
This study explores the relationship between language variation and socioeconomic background using geotagged tweets in England and Wales. By analyzing deviations from standard English in different areas and correlating them with income levels, the researchers found that in areas with greater socioeconomic diversity, the relationship between language variation and income becomes less pronounced. This suggests that interactions between different socioeconomic classes may influence how people use language. Additionally, an agent-based model was developed to explain the observed patterns, providing insights into the mechanisms behind linguistic variety adoption. Overall, the study highlights the complex interplay between socioeconomic factors and language use, shedding light on how social dynamics can shape linguistic patterns at a large scale. 

Summary: <div>
arXiv:2307.10016v2 Announce Type: replace-cross 
Abstract: The socioeconomic background of people and how they use standard forms of language are not independent, as demonstrated in various sociolinguistic studies. However, the extent to which these correlations may be influenced by the mixing of people from different socioeconomic classes remains relatively unexplored from a quantitative perspective. In this work we leverage geotagged tweets and transferable computational methods to map deviations from standard English on a large scale, in seven thousand administrative areas of England and Wales. We combine these data with high-resolution income maps to assign a proxy socioeconomic indicator to home-located users. Strikingly, across eight metropolitan areas we find a consistent pattern suggesting that the more different socioeconomic classes mix, the less interdependent the frequency of their departures from standard grammar and their income become. Further, we propose an agent-based model of linguistic variety adoption that sheds light on the mechanisms that produce the observations seen in the data.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Motif Participation Profiles for Analyzing Node Similarity in Temporal Networks</title>
<link>https://arxiv.org/abs/2507.06465</link>
<guid>https://arxiv.org/abs/2507.06465</guid>
<content:encoded><![CDATA[
<div> Temporal networks, temporal motifs, TMPPs, node clustering, militarized interstate disputes <br />
Summary: <br />
Temporal networks track interactions between nodes over time. Temporal motifs capture higher-order patterns like directed triangles. TMPPs represent node behavior in these motifs, serving as interpretable node embeddings. Nodes with similar TMPPs have similar roles in motifs. Clustering TMPPs reveals node groups with similar roles. Simulation experiments and a study on militarized interstate disputes demonstrate the effectiveness of TMPPs in uncovering node roles in temporal networks. <div>
arXiv:2507.06465v1 Announce Type: new 
Abstract: Temporal networks consisting of timestamped interactions between a set of nodes provide a useful representation for analyzing complex networked systems that evolve over time. Beyond pairwise interactions between nodes, temporal motifs capture patterns of higher-order interactions such as directed triangles over short time periods. We propose temporal motif participation profiles (TMPPs) to capture the behavior of nodes in temporal motifs. Two nodes with similar TMPPs take similar positions within temporal motifs, possibly with different nodes. TMPPs serve as unsupervised embeddings for nodes in temporal networks that are directly interpretable, as each entry denotes the frequency at which a node participates in a particular position in a specific temporal motif. We demonstrate that clustering TMPPs reveals groups of nodes with similar roles in a temporal network through simulation experiments and a case study on a network of militarized interstate disputes.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Fake Account Detection: A Survey</title>
<link>https://arxiv.org/abs/2507.06541</link>
<guid>https://arxiv.org/abs/2507.06541</guid>
<content:encoded><![CDATA[
<div> Keywords: fake account detection, online social networks, graph-based techniques, topological features, detection time

Summary:
This survey paper discusses the current state of algorithms for detecting fake accounts in online social networks, with a focus on graph-based techniques that use social graph topology. Different methods are categorized based on techniques, input data, and detection time, with strengths and limitations discussed. The paper also explores available datasets, including real-world and synthesized data models. Potential areas for future research are suggested. <div>
arXiv:2507.06541v1 Announce Type: new 
Abstract: In recent years, there has been a growing effort to develop effective and efficient algorithms for fake account detection in online social networks. This survey comprehensively reviews existing methods, with a focus on graph-based techniques that utilise topological features of social graphs (in addition to account information, such as their shared contents and profile data) to distinguish between fake and real accounts. We provide several categorisations of these methods (for example, based on techniques used, input data, and detection time), discuss their strengths and limitations, and explain how these methods connect in the broader context. We also investigate the available datasets, including both real-world data and synthesised models. We conclude the paper by proposing several potential avenues for future research.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Heterogeneity across Varying Spatial Extents: Discovering Linkages between Sea Ice Retreat and Ice Shelve Melt in the Antarctic</title>
<link>https://arxiv.org/abs/2507.07036</link>
<guid>https://arxiv.org/abs/2507.07036</guid>
<content:encoded><![CDATA[
<div> Keywords: spatial heterogeneity, sea ice retreat, Antarctic ice shelf melt, graph-based framework, climate adaptation<br />
Summary:<br />
- The study explores the complex linkages between sea ice retreat and Antarctic ice shelf (AIS) melt, focusing on the spatial heterogeneity in dynamic regions like ice shelves and sea ice.<br />
- Traditional models treat sea ice and AIS as separate systems, limiting their ability to capture localized linkages and cascading feedback.<br />
- The proposed Spatial-Link framework is a novel graph-based approach that quantifies spatial heterogeneity to capture linkages between sea ice retreat and AIS melt, revealing non-local coupling patterns.<br />
- Results show that sea ice loss can initiate or amplify downstream AIS melt, establishing a direct linkage between sea ice retreat and AIS mass loss.<br />
- Spatial-Link offers a scalable, data-driven tool to improve sea-level rise projections and inform climate adaptation strategies. <br /> 

Summary: <div>
arXiv:2507.07036v1 Announce Type: new 
Abstract: Spatial phenomena often exhibit heterogeneity across spatial extents and in proximity, making them complex to model-especially in dynamic regions like ice shelves and sea ice. In this study, we address this challenge by exploring the linkages between sea ice retreat and Antarctic ice shelf (AIS) melt. Although atmospheric forcing and basal melting have been widely studied, the direct impact of sea ice retreat on AIS mass loss remains underexplored. Traditional models treat sea ice and AIS as separate systems. It limits their ability to capture localized linkages and cascading feedback. To overcome this, we propose Spatial-Link, a novel graph-based framework that quantifies spatial heterogeneity to capture linkages between sea ice retreat and AIS melt. Our method constructs a spatial graph using Delaunay triangulation of satellite-derived ice change matrices, where nodes represent regions of significant change and edges encode proximity and directional consistency. We extract and statistically validate linkage paths using breadth-first search and Monte Carlo simulations. Results reveal non-local, spatially heterogeneous coupling patterns, suggesting sea ice loss can initiate or amplify downstream AIS melt. Our analysis shows how sea ice retreat evolves over an oceanic grid and progresses toward ice shelves-establishing a direct linkage. To our knowledge, this is the first proposed methodology linking sea ice retreat to AIS melt. Spatial-Link offers a scalable, data-driven tool to improve sea-level rise projections and inform climate adaptation strategies.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning</title>
<link>https://arxiv.org/abs/2507.06469</link>
<guid>https://arxiv.org/abs/2507.06469</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph representation learning, fraud detection, topology, class imbalance, dual-view graph

Summary: 
Graph representation learning has become a popular method for fraud detection, but the imbalance in global topological information transmission poses a challenge. In this paper, the authors propose a novel method, MimbFD, to address this issue. MimbFD includes a topological message reachability module for improved node representation learning and a local confounding debiasing module to enhance the association between node representations and labels. These components help mitigate the imbalance in supervisory messages caused by fraudsters' topological behavior obfuscation and identity feature concealment. Experimental results on three public fraud datasets demonstrate the effectiveness of MimbFD in fraud detection. <div>
arXiv:2507.06469v1 Announce Type: cross 
Abstract: Graph representation learning has become a mainstream method for fraud detection due to its strong expressive power, which focuses on enhancing node representations through improved neighborhood knowledge capture. However, the focus on local interactions leads to imbalanced transmission of global topological information and increased risk of node-specific information being overwhelmed during aggregation due to the imbalance between fraud and benign nodes. In this paper, we first summarize the impact of topology and class imbalance on downstream tasks in GNN-based fraud detection, as the problem of imbalanced supervisory messages is caused by fraudsters' topological behavior obfuscation and identity feature concealment. Based on statistical validation, we propose a novel dual-view graph representation learning method to mitigate Message imbalance in Fraud Detection(MimbFD). Specifically, we design a topological message reachability module for high-quality node representation learning to penetrate fraudsters' camouflage and alleviate insufficient propagation. Then, we introduce a local confounding debiasing module to adjust node representations, enhancing the stable association between node representations and labels to balance the influence of different classes. Finally, we conducted experiments on three public fraud datasets, and the results demonstrate that MimbFD exhibits outstanding performance in fraud detection.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Designing Social Interventions for Online Climate Change Denialism Discussions</title>
<link>https://arxiv.org/abs/2507.06561</link>
<guid>https://arxiv.org/abs/2507.06561</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, climate change denialism, Reddit, intervention strategies, social media

Summary: 
This study explores intervention strategies for combating conspiracy theory ideology in climate change denialism on Reddit. The researchers used insider language to engage with users in two Reddit communities - climate change deniers and supporters. By crafting evidence-based intervention messages and deploying them through bot accounts, the study found that neutral language interventions sparked positive engagement and open discussions among climate change deniers. Climate change supporters responded actively, contributing additional evidence to the discussions. The study sheds light on the challenges and processes involved in delivering interventions in conspiracy theory communities on social media, offering valuable insights for future research on social media interventions.

<br /><br />Summary: <div>
arXiv:2507.06561v1 Announce Type: cross 
Abstract: As conspiracy theories gain traction, it has become crucial to research effective intervention strategies that can foster evidence and science-based discussions in conspiracy theory communities online. This study presents a novel framework using insider language to contest conspiracy theory ideology in climate change denialism on Reddit. Focusing on discussions in two Reddit communities, our research investigates reactions to pro-social and evidence-based intervention messages for two cohorts of users: climate change deniers and climate change supporters. Specifically, we combine manual and generative AI-based methods to craft intervention messages and deploy the interventions as replies on Reddit posts and comments through transparently labeled bot accounts. On the one hand, we find that evidence-based interventions with neutral language foster positive engagement, encouraging open discussions among believers of climate change denialism. On the other, climate change supporters respond positively, actively participating and presenting additional evidence. Our study contributes valuable insights into the process and challenges of automatically delivering interventions in conspiracy theory communities on social media, and helps inform future research on social media interventions.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DICE: Data Influence Cascade in Decentralized Learning</title>
<link>https://arxiv.org/abs/2507.06931</link>
<guid>https://arxiv.org/abs/2507.06931</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralized learning, Fair incentive mechanism, Data Influence Cascade, Peer-to-peer networks, Tractable approximations

Summary: 
The article introduces a novel method called Data Influence Cascade Estimation (DICE) for decentralized learning in peer-to-peer networks. The focus is on creating fair incentives for participating nodes by accurately attributing contributions. DICE addresses the challenge of influence cascading in decentralized networks by considering data, communication topology, and loss landscape curvature. The framework provides tractable approximations for influence cascades across neighbor hops, facilitating the selection of collaborators and detection of malicious behaviors. The theoretical foundations of DICE suggest that influence cascades are determined by a complex interplay of factors. This innovative approach aims to encourage participation and improve the efficiency of decentralized learning systems.<br /><br />Summary: <div>
arXiv:2507.06931v1 Announce Type: cross 
Abstract: Decentralized learning offers a promising approach to crowdsource data consumptions and computational workloads across geographically distributed compute interconnected through peer-to-peer networks, accommodating the exponentially increasing demands. However, proper incentives are still in absence, considerably discouraging participation. Our vision is that a fair incentive mechanism relies on fair attribution of contributions to participating nodes, which faces non-trivial challenges arising from the localized connections making influence ``cascade'' in a decentralized network. To overcome this, we design the first method to estimate \textbf{D}ata \textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized environment. Theoretically, the framework derives tractable approximations of influence cascade over arbitrary neighbor hops, suggesting the influence cascade is determined by an interplay of data, communication topology, and the curvature of loss landscape. DICE also lays the foundations for applications including selecting suitable collaborators and identifying malicious behaviors. Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage</title>
<link>https://arxiv.org/abs/2507.07045</link>
<guid>https://arxiv.org/abs/2507.07045</guid>
<content:encoded><![CDATA[
<div> design, Large Language Models, prompt, framework, efficiency
Summary:
The article discusses the shift from traditional prompt engineering to a more structured prompt design approach in human-Large Language Model (LLM) interactions. It introduces the 5C Prompt Contract framework, comprising Character, Cause, Constraint, Contingency, and Calibration components, aimed at simplifying prompt design for better AI interactions. The framework integrates fallback and output optimization directives, enhancing reliability and interpretability while maintaining creativity. Experimental results show that the 5C framework improves input token efficiency and ensures consistent outputs across various LLM architectures. It is particularly beneficial for individuals and Small-to-Medium Enterprises (SMEs) with limited AI resources. <br /><br />Summary: <div>
arXiv:2507.07045v1 Announce Type: cross 
Abstract: The progression from traditional prompt engineering to a more rigorous discipline of prompt design marks a pivotal shift in human-LLM interaction. As Large Language Models (LLMs) become increasingly embedded in mission-critical applications, there emerges a pressing need for frameworks that are not only explicit and systematic but also minimal enough to remain practical and broadly accessible. While many existing approaches address prompt structuring through elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such methods can impose significant token and cognitive overhead, potentially constraining the model's creative capacity. In this context, we propose the 5C Prompt Contract, a framework that distills prompt design into five intuitive components: Character, Cause, Constraint, Contingency, and Calibration. This minimal cognitive schema explicitly integrates fallback and output optimization directives, fostering reliable, interpretable, and creatively flexible AI interactions. Experimental results demonstrate that the 5C framework consistently achieves superior input token efficiency while maintaining rich and consistent outputs across diverse LLM architectures (OpenAI, Anthropic, DeepSeek, and Gemini), making it particularly suited for individuals and Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Public Perceptions of Generative AI in Libraries: A Social Media Analysis of X Discussions</title>
<link>https://arxiv.org/abs/2507.07047</link>
<guid>https://arxiv.org/abs/2507.07047</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, libraries, social network analysis, sentiment analysis, public perception
<br />
Summary: 
<br />
This study examines public perceptions of generative artificial intelligence (GenAI) in libraries by analyzing posts on X (formerly Twitter). The research combines temporal trend analysis, sentiment classification, and social network analysis to understand how discourse around GenAI and libraries has evolved. The analysis shows that discussions are mostly negative, particularly focused on ethical and intellectual property concerns. Social network analysis reveals the importance of institutional authority and individual bridge users in facilitating cross-domain engagement. Overall, the study contributes to the existing literature on GenAI in the library and GLAM sectors, offering a real-time insight into the opportunities and challenges presented by GenAI. <div>
arXiv:2507.07047v1 Announce Type: cross 
Abstract: This study investigates public perceptions of generative artificial intelligence (GenAI) in libraries through a large-scale analysis of posts on X (formerly Twitter). Using a mixed-method approach that combines temporal trend analysis, sentiment classification, and social network analysis, this paper explores how public discourse around GenAI and libraries has evolved over time, the emotional tones that dominate the conversation, and the key users or organizations driving engagement. The findings reveal that discussions are predominantly negative in tone, with surges linked to concerns about ethics and intellectual property. Furthermore, social network analysis identifies both institutional authority and individual bridge users who facilitate cross-domain engagement. The results in this paper contribute to the growing body of literature on GenAI in the library and GLAM (Galleries, Libraries, Archives, and Museums) sectors and offer a real-time, public-facing perspective on the emerging opportunities and concerns GenAI presents.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Ranking for Deliberation in the Public Sphere</title>
<link>https://arxiv.org/abs/2503.18962</link>
<guid>https://arxiv.org/abs/2503.18962</guid>
<content:encoded><![CDATA[
<div> comment sections, online platforms, algorithmic ranking, diverse viewpoints, social choice

Summary: 
Platforms with online comment sections often struggle with toxic exchanges, hindering productive public deliberation. To address this issue, algorithmic ranking is used to promote higher-quality discussions, but this can inadvertently suppress legitimate viewpoints, reducing the representation of diverse perspectives. To combat this, the concept of justified representation (JR) is introduced, drawing from social choice theory. By incorporating a JR constraint into comment ranking systems, platforms can ensure a more inclusive representation of diverse viewpoints while still optimizing for user engagement or conversational quality. This approach strikes a balance between fostering quality discussions and upholding the importance of diverse perspectives in online deliberation. <div>
arXiv:2503.18962v2 Announce Type: replace 
Abstract: Online comment sections, such as those on news sites or social media, have the potential to foster informal public deliberation, However, this potential is often undermined by the frequency of toxic or low-quality exchanges that occur in these settings. To combat this, platforms increasingly leverage algorithmic ranking to facilitate higher-quality discussions, e.g., by using civility classifiers or forms of prosocial ranking. Yet, these interventions may also inadvertently reduce the visibility of legitimate viewpoints, undermining another key aspect of deliberation: representation of diverse views. We seek to remedy this problem by introducing guarantees of representation into these methods. In particular, we adopt the notion of justified representation (JR) from the social choice literature and incorporate a JR constraint into the comment ranking setting. We find that enforcing JR leads to greater inclusion of diverse viewpoints while still being compatible with optimizing for user engagement or other measures of conversational quality.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Displacement and disconnection: the impact of violence on migration networks and highway traffic in Mexico</title>
<link>https://arxiv.org/abs/2301.12743</link>
<guid>https://arxiv.org/abs/2301.12743</guid>
<content:encoded><![CDATA[
<div> migration flows, violence impact, network strength, oil theft, Mexican municipalities
Summary:<br />
- The study examines how violence affects migration flows and reshapes the strength of migration networks by using a novel network algorithm and Mexican census data from 2005 to 2020. 
- Rising violence, driven by increased oil theft activities following government crackdowns on drug trafficking organizations, led to higher emigration flows within Mexico and towards the United States. 
- The study found that violence increased emigration by at least 1.12 million people domestically and reduced the return of 50,200 Mexicans from the US. 
- Additionally, violence eroded regional connectivity, causing a decline in daily vehicle traffic on highways linking violent areas to the rest of the country. 
- The findings highlight the complex dynamics between violence, migration, and network strength in shaping population movements within and outside Mexico. 
<br /><br />Summary: <div>
arXiv:2301.12743v2 Announce Type: replace-cross 
Abstract: We examine how violence affects migration flows and, crucially, how it reshapes the strength of migration networks -- measured by the intensity of migration between areas, accounting for the fact that some routes become more prominent or fade over time -- an aspect traditional studies overlook. Using a novel network algorithm and Mexican census data from 2005 to 2020, we first quantify changes in the strength of domestic and international migration networks across all Mexican municipalities. We exploit variation in local homicide rates, using exogenous fuel price increases and municipalities' proximity to oil pipelines as instruments, to estimate the causal impact of violence on migration. During our study period, following intensified government crackdowns on drug trafficking organizations, many criminal groups fragmented and turned toward large-scale oil theft, driving sharp increases in violence in areas with oil pipelines, particularly when fuel prices rose. The findings show that rising violence increased emigration flows, predominantly within Mexico, and strengthened the intensity of emigration networks both domestically and toward the United States. Although violent municipalities continued to receive new residents, the rise in emigration was larger. Increasing homicide rates led to at least an additional 1.12 million people emigrating domestically and 50,200 fewer Mexicans returning from the United States. Violence also eroded regional connectivity, causing a long-term decline in daily vehicle traffic on highways linking violent areas to the rest of the country.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The most influential philosophers in Wikipedia: a multicultural analysis</title>
<link>https://arxiv.org/abs/2507.06034</link>
<guid>https://arxiv.org/abs/2507.06034</guid>
<content:encoded><![CDATA[
<div> PageRank, CheiRank, philosophers, Wikipedia knowledge network, presocratic philosophers  
Summary:  
- The study examines the influence and interconnectivity of philosophers in the Wikipedia knowledge network across different language editions.  
- 237 philosopher articles in nine languages were analyzed using PageRank and CheiRank algorithms to determine their relative ranking and influence.  
- A comparison with entries from the Stanford and Internet Encyclopedia of Philosophy was conducted to highlight differences between general and specialized knowledge networks.  
- The study focuses on a sub-network of 21 presocratic philosophers grouped into traditional schools and uses the reduced Google matrix method to reveal both direct and hidden links between them.  
- The analysis provides new insights into the intellectual relationships and influence of early philosophers within the Western philosophical tradition.  

<br /><br />Summary: <div>
arXiv:2507.06034v1 Announce Type: new 
Abstract: We explore the influence and interconnectivity of philosophical thinkers within the Wikipedia knowledge network. Using a dataset of 237 articles dedicated to philosophers across nine different language editions (Arabic, Chinese, English, French, German, Japanese, Portuguese, Russian, and Spanish), we apply the PageRank and CheiRank algorithms to analyze their relative ranking and influence in each linguistic context. Furthermore, we compare our results with entries from the Stanford Encyclopedia of Philosophy and the Internet Encyclopedia of Philosophy, providing insight into the differences between general knowledge networks like Wikipedia and specialized philosophical databases. A key focus of our analysis is the sub-network of 21 presocratic philosophers, grouped into four traditional schools: Italic (Pythagorean + Eleatic), Ionian, Abderian (Atomist), and Sophist. Using the reduced Google matrix method, we uncover both direct and hidden links between these early thinkers, offering new perspectives on their intellectual relationships and influence within the Western philosophical tradition.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuHE: Optimizing Utility-Cost in Quantum Key Distribution and Homomorphic Encryption Enabled Secure Edge Computing Networks</title>
<link>https://arxiv.org/abs/2507.06086</link>
<guid>https://arxiv.org/abs/2507.06086</guid>
<content:encoded><![CDATA[
<div> Quantum Key Distribution, Homomorphic Encryption, Mobile Edge Computing, Optimization, Resource Allocation<br />
<br />
Summary: 
This paper introduces a novel framework for secure and efficient data processing in mobile edge computing (MEC) systems by integrating Quantum Key Distribution (QKD), transciphering, and Homomorphic Encryption (HE). The framework addresses the trade-offs among QKD utility, HE security, and system costs. An optimization problem is formulated to balance these factors, but it is non-convex and NP-hard. To efficiently solve it, the Quantum-enhanced Homomorphic Encryption resource allocation (QuHE) algorithm is proposed. The algorithm is proven to be convergent and optimal through theoretical analysis and is shown to be effective in simulations across various performance metrics. <div>
arXiv:2507.06086v1 Announce Type: new 
Abstract: Ensuring secure and efficient data processing in mobile edge computing (MEC) systems is a critical challenge. While quantum key distribution (QKD) offers unconditionally secure key exchange and homomorphic encryption (HE) enables privacy-preserving data processing, existing research fails to address the comprehensive trade-offs among QKD utility, HE security, and system costs. This paper proposes a novel framework integrating QKD, transciphering, and HE for secure and efficient MEC. QKD distributes symmetric keys, transciphering bridges symmetric encryption, and HE processes encrypted data at the server. We formulate an optimization problem balancing QKD utility, HE security, processing and wireless transmission costs. However, the formulated optimization is non-convex and NPhard. To solve it efficiently, we propose the Quantum-enhanced Homomorphic Encryption resource allocation (QuHE) algorithm. Theoretical analysis proves the proposed QuHE algorithm's convergence and optimality, and simulations demonstrate its effectiveness across multiple performance metrics.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Nodes Identification in Complex Networks: A Survey</title>
<link>https://arxiv.org/abs/2507.06164</link>
<guid>https://arxiv.org/abs/2507.06164</guid>
<content:encoded><![CDATA[
<div> Complex networks, critical node identification, centrality, dynamic networks, higher-order networks <br />
Summary: <br />
This paper offers a comprehensive review of critical node identification techniques in complex networks, categorizing them into seven main classes. It addresses the challenges posed by the complexity and heterogeneity of real-world networks, particularly in dynamic and higher-order structures. The review highlights various methods, such as centrality, critical node deletion problem, influence maximization, network control, artificial intelligence, and higher-order and dynamic approaches. It emphasizes the strengths, limitations, and applicability of these methods across different network types. Key challenges identified include algorithmic universality, real-time evaluation in dynamic networks, analysis of higher-order structures, and computational efficiency in large-scale networks. The structured synthesis consolidates current progress and underscores open questions in modeling temporal dynamics, advancing efficient algorithms, integrating machine learning, and developing scalable and interpretable metrics for complex systems. <div>
arXiv:2507.06164v1 Announce Type: new 
Abstract: Complex networks have become essential tools for understanding diverse phenomena in social systems, traffic systems, biomolecular systems, and financial systems. Identifying critical nodes is a central theme in contemporary research, serving as a vital bridge between theoretical foundations and practical applications. Nevertheless, the intrinsic complexity and structural heterogeneity characterizing real-world networks, with particular emphasis on dynamic and higher-order networks, present substantial obstacles to the development of universal frameworks for critical node identification. This paper provides a comprehensive review of critical node identification techniques, categorizing them into seven main classes: centrality, critical nodes deletion problem, influence maximization, network control, artificial intelligence, higher-order and dynamic methods. Our review bridges the gaps in existing surveys by systematically classifying methods based on their methodological foundations and practical implications, and by highlighting their strengths, limitations, and applicability across different network types. Our work enhances the understanding of critical node research by identifying key challenges, such as algorithmic universality, real-time evaluation in dynamic networks, analysis of higher-order structures, and computational efficiency in large-scale networks. The structured synthesis consolidates current progress and highlights open questions, particularly in modeling temporal dynamics, advancing efficient algorithms, integrating machine learning approaches, and developing scalable and interpretable metrics for complex systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are Introvert</title>
<link>https://arxiv.org/abs/2507.05638</link>
<guid>https://arxiv.org/abs/2507.05638</guid>
<content:encoded><![CDATA[
<div> transformed information dissemination, social media, generative AI, misinformation, information propagation dynamics <br />
<br />Summary: The article discusses the impact of social media and generative AI on information dissemination and the spread of misinformation. Traditional models like SIR are insufficient in capturing the complexities of online interactions. Advanced methods like attention mechanisms and graph neural networks overlook user psychology and behavioral dynamics. Large language models (LLMs) offer potential for simulating psychological aspects of information spread but have limitations in capturing authentic human dynamics. The article introduces the SIP-CoT mechanism enhanced by emotion-guided memory to address these limitations. Experimental results confirm that SIP-CoT-enhanced LLM agents process social information more effectively, demonstrating behaviors closer to real human interactions. Overall, the research identifies critical limitations in current LLM-based propagation simulations and shows how integrating SIP-CoT and emotional memory enhances the social intelligence and realism of LLM agents. <br /> <div>
arXiv:2507.05638v1 Announce Type: cross 
Abstract: The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity isn't everything -- how far do demographics take us towards self-identified party ID?</title>
<link>https://arxiv.org/abs/2507.06193</link>
<guid>https://arxiv.org/abs/2507.06193</guid>
<content:encoded><![CDATA[
<div> party identification, demographics, identity strength, predictive framework, Black Democrats

Summary: 
Demographics alone may not fully explain party identification, as individuals may choose to construct a political identity independent of their demographic group. The study examines the role of identity strength alongside demographics in predicting party identification. While demographics are highly predictive for some groups, such as Black Democrats, others, like Hispanic Republicans, benefit from considering identity strength as well. This suggests that individuals may prioritize certain group affiliations in shaping their political identity, and that a deeper understanding of identity strength can enhance the accuracy of predicting party identification. <div>
arXiv:2507.06193v1 Announce Type: cross 
Abstract: How well do demographics explain party identification? Demographics are related to party identification in political polls, news articles, and academic publications. Yet, there is a diversity of party identification even within demographic groups which have historically been attached to one party. And some groups lack a clear connection to either party. It may be that demographics on their own fail to account for the fact that people generally belong to a variety of groups. They must select the groups which are most important to them when shaping a political identity, and may choose to construct an identity relatively unattached to any specific demographic group to which they belong. This prompts the question, do we need to consider measures of identity strength when using demographics to explain party identification? We utilize a predictive framework to address these questions and find that demographics are highly predictive for some groups (e.g., Black Democrats), while others benefit from the inclusion of identity strength (e.g., Hispanic Republicans).
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.12149</link>
<guid>https://arxiv.org/abs/2503.12149</guid>
<content:encoded><![CDATA[
<div> Keywords: LVLMs, multimodal sarcasm, interpretive variations, subjectivity, uncertainty-aware modeling

Summary:
LVLMs, or large vision-language models, are increasingly being used to understand multimodal sarcasm. This study evaluates 12 LVLMs using a systematic framework on existing sarcasm datasets, analyzing interpretive variations and subjective perspectives. The results show discrepancies in how different models interpret sarcasm, especially when given varied prompts. While classification prompts show higher internal consistency, models diverge in interpretive reasoning. These findings challenge traditional labeling paradigms by highlighting sarcasm's subjectivity. The study suggests moving towards uncertainty-aware modeling to better understand multimodal sarcasm comprehension. This research provides valuable insights for developing more nuanced and deeper comprehension of sarcasm in LVLMs.<br /><br />Summary: <div>
arXiv:2503.12149v2 Announce Type: replace-cross 
Abstract: With the advent of large vision-language models (LVLMs) demonstrating increasingly human-like abilities, a pivotal question emerges: do different LVLMs interpret multimodal sarcasm differently, and can a single model grasp sarcasm from multiple perspectives like humans? To explore this, we introduce an analytical framework using systematically designed prompts on existing multimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409 samples, we examine interpretive variations within and across models, focusing on confidence levels, alignment with dataset labels, and recognition of ambiguous "neutral" cases. Our findings reveal notable discrepancies -- across LVLMs and within the same model under varied prompts. While classification-oriented prompts yield higher internal consistency, models diverge markedly when tasked with interpretive reasoning. These results challenge binary labeling paradigms by highlighting sarcasm's subjectivity. We advocate moving beyond rigid annotation schemes toward multi-perspective, uncertainty-aware modeling, offering deeper insights into multimodal sarcasm comprehension. Our code and data are available at: https://github.com/CoderChen01/LVLMSarcasmAnalysis
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dominance or Fair Play in Social Networks? A Model of Influencer Popularity Dynamic</title>
<link>https://arxiv.org/abs/2507.03448</link>
<guid>https://arxiv.org/abs/2507.03448</guid>
<content:encoded><![CDATA[
<div> Keywords: data-driven, mean-field approach, popularity dynamics, influencers, social networks

Summary: 
This paper introduces a data-driven mean-field model to study the dynamics of influencer popularity on social media platforms. The model incorporates individual activity patterns, content virality, external events, and platform visibility to predict the success of influencers. By deriving conditions for system ergodicity, the model can forecast popularity distributions among influencers. A sensitivity analysis examines different system setups to identify factors that can lead to either dominance or fair play in the influencer ecosystem. The findings provide insights into the potential evolution of social networks towards more equitable or biased influence dynamics. <div>
arXiv:2507.03448v1 Announce Type: new 
Abstract: This paper presents a data-driven mean-field approach to model the popularity dynamics of users seeking public attention, i.e., influencers. We propose a novel analytical model that integrates individual activity patterns, expertise in producing viral content, exogenous events, and the platform's role in visibility enhancement, ultimately determining each influencer's success. We analytically derive sufficient conditions for system ergodicity, enabling predictions of popularity distributions. A sensitivity analysis explores various system configurations, highlighting conditions favoring either dominance or fair play among influencers. Our findings offer valuable insights into the potential evolution of social networks towards more equitable or biased influence ecosystems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaxPulse: Active Global Vaccine Infodemic Risk Assessment</title>
<link>https://arxiv.org/abs/2507.04222</link>
<guid>https://arxiv.org/abs/2507.04222</guid>
<content:encoded><![CDATA[
<div> AI, social listening, vaccine infodemics, misinformation, public health

Summary: 
The paper presents VaxPulse VIRAL, an AI-powered social listening platform to monitor and assess vaccine-related infodemic risks. It utilizes machine learning methods like deep learning and active learning to analyze public sentiments, misinformation trends, and social bot activity in real-time. The platform's dynamic dashboards offer tailored insights for immunization programs and combatting misinformation. Iterative feedback from experts and stakeholders guides continuous improvements. Collaboration with an international network and community leaders ensures ongoing enhancements to VaxPulse. This innovative approach aims to address the challenges posed by vaccine infodemics and support global public health initiatives. <br /><br /> <div>
arXiv:2507.04222v1 Announce Type: new 
Abstract: Vaccine infodemics, driven by misinformation, disinformation, and inauthentic online behaviours, pose significant threats to global public health. This paper presents our response to this challenge, demonstrating how we developed VaxPulse Vaccine Infodemic Risk Assessment Lifecycle (VIRAL), an AI-powered social listening platform designed to monitor and assess vaccine-related infodemic risks. Leveraging interdisciplinary expertise and international collaborations, VaxPulse VIRAL integrates machine learning methods, including deep learning, active learning, and data augmentation, to provide real-time insights into public sentiments, misinformation trends, and social bot activity. Iterative feedback from domain experts and stakeholders has guided the development of dynamic dashboards that offer tailored, actionable insights to support immunisation programs and address information disorder. Ongoing improvements to VaxPulse will continue through collaboration with our international network and community leaders.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating User Watch-Time to Investigate Bias in YouTube Shorts Recommendations</title>
<link>https://arxiv.org/abs/2507.04534</link>
<guid>https://arxiv.org/abs/2507.04534</guid>
<content:encoded><![CDATA[
<div> YouTube Shorts, engagement-driven algorithms, content exposure, viewer behaviors, relevance shift

Summary: 
This study explores the impact of viewer behaviors, such as fast scrolling or skipping, on the relevance and topical continuity of recommended videos on short-form video platforms like YouTube Shorts. Analyzing a dataset of over 404,000 videos on geopolitical themes and conflicts, including Russia, China, the Russia-Ukraine War, and the South China Sea dispute, the research simulates viewer interactions and assesses how relevance changes across recommendation chains under different watch-time conditions. Using GPT-4o to measure semantic alignment between videos, the study uncovers patterns of amplification, drift, and topic generalization that have significant implications for content diversity and platform accountability. By combining insights from computer science, media studies, and political communication, this interdisciplinary work enhances our understanding of how engagement cues shape algorithmic pathways in short-form content ecosystems. 

Summary: <div>
arXiv:2507.04534v1 Announce Type: new 
Abstract: Short-form video platforms such as YouTube Shorts increasingly shape how information is consumed, yet the effects of engagement-driven algorithms on content exposure remain poorly understood. This study investigates how different viewing behaviors, including fast scrolling or skipping, influence the relevance and topical continuity of recommended videos. Using a dataset of over 404,000 videos, we simulate viewer interactions across both broader geopolitical themes and more narrowly focused conflicts, including topics related to Russia, China, the Russia-Ukraine War, and the South China Sea dispute. We assess how relevance shifts across recommendation chains under varying watch-time conditions, using GPT-4o to evaluate semantic alignment between videos. Our analysis reveals patterns of amplification, drift, and topic generalization, with significant implications for content diversity and platform accountability. By bridging perspectives from computer science, media studies, and political communication, this work contributes a multidisciplinary understanding of how engagement cues influence algorithmic pathways in short-form content ecosystems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Algorithmic Bias in YouTube Shorts</title>
<link>https://arxiv.org/abs/2507.04605</link>
<guid>https://arxiv.org/abs/2507.04605</guid>
<content:encoded><![CDATA[
<div> YouTube Shorts, algorithmic bias, content visibility, drift, generative AI models<br />
<br />
Summary: 
The study explores algorithmic bias in YouTube Shorts' recommendation system, focusing on watch-time duration, topic sensitivity, and engagement metrics. Analyzing over 685,000 videos across different content domains, the research finds a drift away from politically sensitive content towards entertainment-focused videos. Emotion analysis reveals a preference for joyful or neutral content, while highly viewed and liked videos are disproportionately promoted, reinforcing popularity bias. These findings highlight how algorithm design influences content exposure on YouTube Shorts, impacting information diversity and transparency on the platform. <div>
arXiv:2507.04605v1 Announce Type: new 
Abstract: The rapid growth of YouTube Shorts, now serving over 2 billion monthly users, reflects a global shift toward short-form video as a dominant mode of online content consumption. This study investigates algorithmic bias in YouTube Shorts' recommendation system by analyzing how watch-time duration, topic sensitivity, and engagement metrics influence content visibility and drift. We focus on three content domains: the South China Sea dispute, the 2024 Taiwan presidential election, and general YouTube Shorts content. Using generative AI models, we classified 685,842 videos across relevance, topic category, and emotional tone. Our results reveal a consistent drift away from politically sensitive content toward entertainment-focused videos. Emotion analysis shows a systematic preference for joyful or neutral content, while engagement patterns indicate that highly viewed and liked videos are disproportionately promoted, reinforcing popularity bias. This work provides the first comprehensive analysis of algorithmic drift in YouTube Shorts based on textual content, emotional tone, topic categorization, and varying watch-time conditions. These findings offer new insights into how algorithmic design shapes content exposure, with implications for platform transparency and information diversity.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaxPulse: Monitoring of Online Public Concerns to Enhance Post-licensure Vaccine Surveillance</title>
<link>https://arxiv.org/abs/2507.04656</link>
<guid>https://arxiv.org/abs/2507.04656</guid>
<content:encoded><![CDATA[
<div> Keywords: vaccine safety, misinformation management, sentiment analysis, vaccine hesitancy, public opinion

Summary: 
The article discusses the enhancement of Victoria's vaccine safety service, SAEFVIC's reporting surveillance system to address the recent vaccine-related infodemic. By incorporating new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online, the system aims to proactively manage misinformation. The framework, VaxPulse, integrates adverse events following immunization (AEFI) with sentiment analysis, emphasizing the importance of contextualizing public concerns. Furthermore, the article highlights the need to address non-English languages to stratify concerns across ethno-lingual communities, providing insights for vaccine uptake strategies. Real-world examples and a case study on women's vaccine hesitancy demonstrate the framework's benefits and adaptability in identifying public opinion from online media.<br /><br />Summary: The article discusses the enhancement of Victoria's vaccine safety service, SAEFVIC's reporting surveillance system to address the recent vaccine-related infodemic. By incorporating new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online, the system aims to proactively manage misinformation. The framework, VaxPulse, integrates adverse events following immunization (AEFI) with sentiment analysis, emphasizing the importance of contextualizing public concerns. Furthermore, the article highlights the need to address non-English languages to stratify concerns across ethno-lingual communities, providing insights for vaccine uptake strategies. Real-world examples and a case study on women's vaccine hesitancy demonstrate the framework's benefits and adaptability in identifying public opinion from online media. <div>
arXiv:2507.04656v1 Announce Type: new 
Abstract: The recent vaccine-related infodemic has amplified public concerns, highlighting the need for proactive misinformation management. We describe how we enhanced the reporting surveillance system of Victoria's vaccine safety service, SAEFVIC, through the incorporation of new information sources for public sentiment analysis, topics of discussion, and hesitancies about vaccinations online. Using VaxPulse, a multi-step framework, we integrate adverse events following immunisation (AEFI) with sentiment analysis, demonstrating the importance of contextualising public concerns. Additionally, we emphasise the need to address non-English languages to stratify concerns across ethno-lingual communities, providing valuable insights for vaccine uptake strategies and combating mis/disinformation. The framework is applied to real-world examples and a case study on women's vaccine hesitancy, showcasing its benefits and adaptability by identifying public opinion from online media.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancement of Circular Economy Through Interdisciplinary Collaboration: A Bibliometric Approach</title>
<link>https://arxiv.org/abs/2507.04923</link>
<guid>https://arxiv.org/abs/2507.04923</guid>
<content:encoded><![CDATA[
<div> Circular Economy, Research, Interdisciplinary Collaboration, Business and Management, Engineering<br />
<br />
Summary: 
The study analyzes over 25,000 Circular Economy (CE) publications to understand the interdisciplinary nature and researcher dynamics of the field. It identifies 16 research clusters and visualizes collaboration patterns among researchers. Business and management research attract significant attention, while engineering research tends to secure higher funding success. Collaborative CE papers from different disciplines demonstrate higher research impact compared to intradisciplinary work, emphasizing the value of interdisciplinary efforts. Case studies highlight the benefits of collaborations between business-oriented and engineering-oriented disciplines. The findings suggest a positive dynamic where attention drawn by business research contributes to securing economic resources for realizing CE goals. The study provides insights for guiding future cross-disciplinary engagement in the CE field. <br /><br /> <div>
arXiv:2507.04923v1 Announce Type: new 
Abstract: Since the European Union introduced its Circular Economy (CE) Action Plan in 2015, CE research has expanded rapidly. However, the structure of this emerging field - both in terms of its constituent disciplines and researcher dynamics - remains poorly understood. To address this gap, we analyze over 25,000 CE-related publications from Scopus by combining conventional bibliometric approaches with advanced machine learning techniques, including text embeddings and clustering. This hybrid method enables both a macro-level mapping of research domains and a micro-level investigation of individual researchers' disciplinary backgrounds and collaborations.
  We classify CE research into 16 distinct clusters, identifying the original disciplines of researchers and visualizing patterns of interdisciplinary collaboration. Building on this foundation, we ask: Which CE-related research domains receive the most attention in academic and policy contexts? And how are different types of interdisciplinary collaboration associated with research impact?
  Our findings show that research in business and management attracts substantial academic and policy attention, while engineering research - though less visible - tends to achieve higher funding success. This suggests a positive dynamic in which the former draws attention to CE issues and the latter secures the economic resources necessary to realize them.
  We further demonstrate that CE papers co-authored by researchers from different disciplines tend to show higher research impact than intradisciplinary work. Qualitative case analyses also highlight this tendency. Centered particularly on collaborations between business-oriented and engineering-oriented disciplines, our findings underscore the importance of interdisciplinary efforts in CE research and offer insights for guiding future cross-disciplinary engagement in the field.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interest Networks (iNETs) for Cities: Cross-Platform Insights and Urban Behavior Explanations</title>
<link>https://arxiv.org/abs/2507.04995</link>
<guid>https://arxiv.org/abs/2507.04995</guid>
<content:encoded><![CDATA[
<div> Interest Networks, LBSNs, spatial granularities, recommendation system, explainable AI<br />
Summary: Location-Based Social Networks (LBSNs) provide valuable insights into urban behavior through Interest Networks (iNETs), with this study comparing iNETs on Google Places and Foursquare at different spatial levels. It reveals that user interests are mainly influenced by proximity and venue similarity, with socioeconomic and political factors playing a smaller role. The research develops a multi-level recommendation system that caters to different user behaviors, leveraging explainable AI techniques to offer personalized urban recommendations with natural-language explanations. The study introduces h3-cities for spatial analysis and releases a public demo for interactive exploration. This approach contributes to urban mobility research by delivering scalable, context-aware, and interpretable recommendation systems.<br /><br />Summary: <div>
arXiv:2507.04995v1 Announce Type: new 
Abstract: Location-Based Social Networks (LBSNs) provide a rich foundation for modeling urban behavior through iNETs (Interest Networks), which capture how user interests are distributed throughout urban spaces. This study compares iNETs across platforms (Google Places and Foursquare) and spatial granularities, showing that coarser levels reveal more consistent cross-platform patterns, while finer granularities expose subtle, platform-specific behaviors. Our analysis finds that, in general, user interest is primarily shaped by geographic proximity and venue similarity, while socioeconomic and political contexts play a lesser role. Building on these insights, we develop a multi-level, explainable recommendation system that predicts high-interest urban regions for different user types. The model adapts to behavior profiles -- such as explorers, who are driven by proximity, and returners, who prefer familiar venues -- and provides natural-language explanations using explainable AI (XAI) techniques. To support our approach, we introduce h3-cities, a tool for multi-scale spatial analysis, and release a public demo for interactively exploring personalized urban recommendations. Our findings contribute to urban mobility research by providing scalable, context-aware, and interpretable recommendation systems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Location Disclosure Fails to Deter Overseas Criticism but Amplifies Regional Divisions on Chinese Social Media</title>
<link>https://arxiv.org/abs/2507.03238</link>
<guid>https://arxiv.org/abs/2507.03238</guid>
<content:encoded><![CDATA[
<div> location disclosure policy, Sina Weibo, user behavior, censorship, online participation <br />
Summary: The study analyzes the impact of a user location disclosure policy on Sina Weibo, China's largest microblogging platform. It finds that the policy, implemented to deter overseas users from spreading harmful information, did not reduce their engagement. Instead, it significantly decreased domestic users' willingness to comment on local issues outside their provinces, particularly affecting out-of-province commenters and criticisms. The policy led to a rise in regionally discriminatory replies and reshaped online participation norms. The findings suggest that authoritarian regimes can use social cleavages, such as regional divisions, to reinforce censorship, suppress dissent, and fragment public discourse. <div>
arXiv:2507.03238v1 Announce Type: cross 
Abstract: We examine the behavioral impact of a user location disclosure policy implemented on Sina Weibo, China's largest microblogging platform, using a high-frequency, real-time dataset of uncensored user engagement with 165 leading government and media accounts. Leveraging a natural experiment result from the platform's sudden rollout of location tagging on April 28, 2022, we compare millions of time-stamped observations of user behavior in the comment sections of these accounts before and after the policy change. Although the policy appeared intended to deter overseas users from spreading information deemed harmful by the regime, we find no reduction in their engagement. Instead, the policy sharply reduced domestic users' willingness to comment on posts about local issues outside their own provinces. This effect was especially pronounced among out-of-province commenters and disproportionately curtailed criticisms. Using large language models, we further show that location disclosure triggered a rise in regionally discriminatory replies, which in turn heightened the perceived risk of cross-provincial engagement and reshaped the norms of online participation. Our findings suggest that authoritarian regimes can reinforce censorship not only through top-down control, but by mobilizing social cleavages, here, regional divisions, to suppress dissent and fragment public discourse.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products</title>
<link>https://arxiv.org/abs/2507.04364</link>
<guid>https://arxiv.org/abs/2507.04364</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine learning, health sentiments, Large Language Models, sentiment classification, public health

Summary: 
Machine learning methods are being used to analyze health-related public discourse, but their accuracy in detecting different health sentiments is not well understood. This research focused on three Large Language Models (LLMs) – GPT, Gemini, and LLAMA – to classify risk-promoting versus health-supporting sentiments on HPV vaccination and heated tobacco products. Results showed that all three LLMs were generally accurate in classifying sentiments, with some variations based on platform, health issue, and model type. Higher accuracy was observed for risk-promoting sentiment on Facebook, while health-supporting messages were better detected on Twitter. However, LLMs faced challenges in accurately detecting neutral messages. This study emphasizes the importance of validating language models for public health analysis and being aware of potential biases in training data influencing the results. 

<br /><br />Summary: <div>
arXiv:2507.04364v1 Announce Type: cross 
Abstract: Machine learning methods are increasingly applied to analyze health-related public discourse based on large-scale data, but questions remain regarding their ability to accurately detect different types of health sentiments. Especially, Large Language Models (LLMs) have gained attention as a powerful technology, yet their accuracy and feasibility in capturing different opinions and perspectives on health issues are largely unexplored. Thus, this research examines how accurate the three prominent LLMs (GPT, Gemini, and LLAMA) are in detecting risk-promoting versus health-supporting sentiments across two critical public health topics: Human Papillomavirus (HPV) vaccination and heated tobacco products (HTPs). Drawing on data from Facebook and Twitter, we curated multiple sets of messages supporting or opposing recommended health behaviors, supplemented with human annotations as the gold standard for sentiment classification. The findings indicate that all three LLMs generally demonstrate substantial accuracy in classifying risk-promoting and health-supporting sentiments, although notable discrepancies emerge by platform, health issue, and model type. Specifically, models often show higher accuracy for risk-promoting sentiment on Facebook, whereas health-supporting messages on Twitter are more accurately detected. An additional analysis also shows the challenges LLMs face in reliably detecting neutral messages. These results highlight the importance of carefully selecting and validating language models for public health analyses, particularly given potential biases in training data that may lead LLMs to overestimate or underestimate the prevalence of certain perspectives.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Social Media Network Effects</title>
<link>https://arxiv.org/abs/2507.04545</link>
<guid>https://arxiv.org/abs/2507.04545</guid>
<content:encoded><![CDATA[
<div> local network effects, social media platforms, consumer surplus, incentive-compatible, online choice experiments

Summary:
Local network effects play a significant role in the value generated by social media platforms, with 20-34% of the total value derived from these effects. Different platforms show varying levels of value, with stronger ties being more valuable on Facebook and Instagram, while weaker ties are preferred on LinkedIn and X. Gender and race also play a role in how connections are valued, with men valuing connections to women more on specific platforms. Additionally, consumer behavior differs based on whether they are looking for work or not, with LinkedIn being more valuable for job seekers. Overall, social media platforms generate between $53B and $215B in consumer surplus per year in the US, highlighting the significant impact of local network effects on platform value. <div>
arXiv:2507.04545v1 Announce Type: cross 
Abstract: We use representative, incentive-compatible online choice experiments involving 19,923 Facebook, Instagram, LinkedIn, and X users in the US to provide the first large-scale, empirical measurement of local network effects in the digital economy. Our analysis reveals social media platform value ranges from $78 to $101 per consumer, per month, on average, and that 20-34% of that value is explained by local network effects. We also find 1) stronger ties are more valuable on Facebook and Instagram, while weaker ties are more valuable on LinkedIn and X; 2) connections known through work are most valuable on LinkedIn and least valuable on Facebook, and people looking for work value LinkedIn significantly more and Facebook significantly less than people not looking for work; 3) men value connections to women on social media significantly more than they value connections to other men, particularly on Instagram, Facebook and X, while women value connections to men and women equally; 4) white consumers value relationships with other white consumers significantly more than they value relationships with non-white consumers on Facebook while, on Instagram, connections to alters eighteen years old or younger are valued significantly more than any other age group-two patterns not seen on any other platforms. Social media platforms individually generate between $53B and $215B in consumer surplus per year in the US alone. These results suggest social media generates significant value, local network effects drive a substantial fraction of that value and that these effects vary across platforms, consumers, and connections.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Tweet Posting Behavior on Citizen Security: A Hawkes Point Process Analysis</title>
<link>https://arxiv.org/abs/2402.03378</link>
<guid>https://arxiv.org/abs/2402.03378</guid>
<content:encoded><![CDATA[
<div> Keywords: Perception of Security, Social Network Data, Predictive Modeling, External Factors, Proactive Security Planning 

Summary: 
The article introduces a novel approach to understanding the Perception of Security (PoS) using social network data. By analyzing social network content, the model aims to offer real-time monitoring and predictive insights into security perceptions. The model incorporates external factors that influence the publication and reposting of security-related content, achieving competitive predictive performance while maintaining interpretability. The research highlights the importance of temporal patterns and external factors in anticipating security perceptions, providing valuable insights for proactive security planning. Overall, the innovative approach presented in the article contributes to improving the measurement and understanding of security perceptions in short time frames, enhancing the ability to anticipate and address security concerns effectively. 

<br /><br />Summary: <div>
arXiv:2402.03378v2 Announce Type: replace 
Abstract: The Perception of Security (PoS) refers to people's opinions about security or insecurity in a place or situation. While surveys have traditionally been the primary means to capture such perceptions, they need to be improved in their ability to offer real-time monitoring or predictive insights into future security perceptions. Recent evidence suggests that social network content can provide complementary insights into quantifying these perceptions. However, the challenge of accurately predicting these perceptions, with the capacity to anticipate them, still needs to be explored. This article introduces an innovative approach to PoS within short time frames using social network data. Our model incorporates external factors that influence the publication and reposting of content related to security perceptions. Our results demonstrate that this proposed model achieves competitive predictive performance and maintains a high degree of interpretability regarding the factors influencing security perceptions. This research contributes to understanding how temporal patterns and external factors impact the anticipation of security perceptions, providing valuable insights for proactive security planning.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Social Sphere Model: Heuristic Influence Prediction in Evolving Networks</title>
<link>https://arxiv.org/abs/2402.03522</link>
<guid>https://arxiv.org/abs/2402.03522</guid>
<content:encoded><![CDATA[
<div> link prediction, influencers, influence maximization, deep learning, social network analysis

Summary:
The study explores admissions in a university program for influencers, focusing on influence maximization and link prediction in social network analysis. It introduces The Social Sphere Model, an algorithm that combines path-based link prediction metrics and heuristic influence maximization strategies to identify future key nodes in weighted networks. Testing on contagion models shows promising results with lower computational requirements. This advancement enhances understanding of network dynamics and offers a more efficient approach to network management and influence strategy development. <div>
arXiv:2402.03522v3 Announce Type: replace 
Abstract: How would admissions look like in a university program for influencers? In the realm of social network analysis, influence maximization and link prediction stand out as pivotal challenges. Influence maximization focuses on identifying a set of key nodes to maximize information dissemination, while link prediction aims to foresee potential connections within the network. These strategies, primarily deep learning link prediction methods and greedy algorithms, have been previously used in tandem to identify future influencers. However, given the complexity of these tasks, especially in large-scale networks, we propose an algorithm, The Social Sphere Model, which uniquely utilizes expected value in its future graph prediction and combines specifically path-based link prediction metrics and heuristic influence maximization strategies to effectively identify future vital nodes in weighted networks. Our approach is tested on two distinct contagion models, offering a promising solution with lower computational demands. This advancement not only enhances our understanding of network dynamics but also opens new avenues for efficient network management and influence strategy development.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHyPar: A Spectral Coarsening Approach to Hypergraph Partitioning</title>
<link>https://arxiv.org/abs/2410.10875</link>
<guid>https://arxiv.org/abs/2410.10875</guid>
<content:encoded><![CDATA[
<div> effective resistances, flow-based community detection, spectral framework, hypergraph partitioning, VLSI designs

Summary: 
The paper introduces SHyPar, a multilevel spectral framework for partitioning hypergraphs that considers structural features. It leverages hyperedge effective resistances and flow-based community detection techniques to guide the partitioning process. SHyPar aims to decompose hypergraphs into subgraphs with minimal inter-partition hyperedges. A key component is a flow-based local clustering scheme for hypergraph coarsening, which improves conductance. The framework also uses an effective resistance-based rating function for merging strongly connected nodes. Experimental results on VLSI designs show that SHyPar outperforms existing methods in terms of solution quality. <div>
arXiv:2410.10875v3 Announce Type: replace 
Abstract: State-of-the-art hypergraph partitioners utilize a multilevel paradigm to construct progressively coarser hypergraphs across multiple layers, guiding cut refinements at each level of the hierarchy. Traditionally, these partitioners employ heuristic methods for coarsening and do not consider the structural features of hypergraphs. In this work, we introduce a multilevel spectral framework, SHyPar, for partitioning large-scale hypergraphs by leveraging hyperedge effective resistances and flow-based community detection techniques. Inspired by the latest theoretical spectral clustering frameworks, such as HyperEF and HyperSF, SHyPar aims to decompose large hypergraphs into multiple subgraphs with few inter-partition hyperedges (cut size). A key component of SHyPar is a flow-based local clustering scheme for hypergraph coarsening, which incorporates a max-flow-based algorithm to produce clusters with substantially improved conductance. Additionally, SHyPar utilizes an effective resistance-based rating function for merging nodes that are strongly connected (coupled). Compared with existing state-of-the-art hypergraph partitioning methods, our extensive experimental results on real-world VLSI designs demonstrate that SHyPar can more effectively partition hypergraphs, achieving state-of-the-art solution quality.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children with Autism Spectrum Disorders: A Systematic Review</title>
<link>https://arxiv.org/abs/2409.18162</link>
<guid>https://arxiv.org/abs/2409.18162</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, augmented reality, user interface, autism spectrum disorder, therapy<br />
<br />
Summary: 
This review study delves into the integration of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children with autism spectrum disorder (ASD). Through a comprehensive literature review, the study identifies three key areas of focus: the impact of AR on social and learning outcomes, the role of LLMs in communication support, and the importance of effective UI/UX design in enhancing the efficacy of these technologies. Findings indicate that LLMs offer personalized learning and communication assistance, while AR shows potential in improving social skills, motivation, and attention for children with ASD. However, there is a dearth of robotics-based educational programs tailored specifically for autistic children. To optimize the benefits of these technologies, further research is needed to address issues surrounding customization, accessibility, and integration in ASD therapies and immersive education.<br /><br /> <div>
arXiv:2409.18162v2 Announce Type: replace-cross 
Abstract: The emergence of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children, especially with disorders like autism spectrum disorder (ASD), is studied in detail in this review study. 150 publications were collected by a thorough literature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 60 of them were chosen based on their methodological rigor and relevance to the focus area. Three of the primary areas are studied and covered in this review: how AR can improve social and learning results, how LLMs can support communication, and how UI/UX design affects how effective these technologies can be. Results show that while LLMs can provide individualized learning and communication support, AR has shown promise in enhancing social skills, motivation, and attention. For children with ASD, accessible and engaging interventions rely heavily on effective UI/UX design, but there is still a significant lack of robotics-based education and therapeutic programs specifically tailored for autistic children. To optimize the benefits of these technologies in ASD therapies and immersive education, the study emphasizes the need for additional research to address difficulties related to customization, accessibility, and integration.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Census-Based Genetic Algorithm for Target Set Selection Problem in Social Networks</title>
<link>https://arxiv.org/abs/2410.02011</link>
<guid>https://arxiv.org/abs/2410.02011</guid>
<content:encoded><![CDATA[
<div> Genetic Algorithm, Target Set Selection Problem, Social Networks, Census, Viral Marketing<br />
<br />
Summary: This paper proposes a novel approach, the census-based genetic algorithm, to solve the Target Set Selection (TSS) Problem in social networks for viral marketing. The algorithm uses census information to maintain diversity and prevent premature convergence to local optima. It tracks the number of times individuals are identified and nodes are included in solutions. The algorithm can self-adjust by varying the aggressiveness parameter in reproduction methods and runs efficiently in a parallelized environment. Experimental results on random and real-life social network graphs demonstrate the algorithm's ability to find optimal solutions, outperforming previous studies by reducing solution size and including more network vertices. The novel approach shows promising results for efficiently solving the TSS problem in social networks. <br /><br />Summary: <div>
arXiv:2410.02011v2 Announce Type: replace-cross 
Abstract: This paper considers the Target Set Selection (TSS) Problem in social networks, a fundamental problem in viral marketing. In the TSS problem, a graph and a threshold value for each vertex of the graph are given. We need to find a minimum size vertex subset to "activate" such that all graph vertices are activated at the end of the propagation process. Specifically, we propose a novel approach called "a census-based genetic algorithm" for the TSS problem. In our algorithm, we use the idea of a census to gather and store information about each individual in a population and collect census data from the individuals constructed during the algorithm's execution so that we can achieve greater diversity and avoid premature convergence at locally optimal solutions. We use two distinct census information: (a) for each individual, the algorithm stores how many times it has been identified during the execution (b) for each network node, the algorithm counts how many times it has been included in a solution. The proposed algorithm can also self-adjust by using a parameter specifying the aggressiveness employed in each reproduction method. Additionally, the algorithm is designed to run in a parallelized environment to minimize the computational cost and check each individual's feasibility. Moreover, our algorithm finds the optimal solution in all cases while experimenting on random graphs. Furthermore, we execute the proposed algorithm on 14 large graphs of real-life social network instances from the literature, improving around 9.57 solution size (on average) and 134 vertices (in total) compared to the best solutions obtained in previous studies.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Coefficient Reflecting Pairwise Relationships within Hyperedges</title>
<link>https://arxiv.org/abs/2410.23799</link>
<guid>https://arxiv.org/abs/2410.23799</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, clustering coefficients, weighted graphs, higher-order motifs, complex networks 

Summary: 
This study introduces a novel clustering coefficient for hypergraphs that considers intra-hyperedge pairwise relationships and accurately quantifies local link density. By transforming hypergraphs into weighted graphs reflecting relationship strength between nodes based on hyperedge connections, the proposed coefficient addresses limitations of existing approaches. The new definition ensures values in the range [0,1], aligns with simple graph clustering coefficients, and effectively captures intra-hyperedge relationships. Theoretical evaluation on higher-order motifs demonstrates superior performance compared to existing definitions, particularly on motifs III, IV-a, IV-b of order 3. Empirical evaluation on real-world datasets confirms similar overall clustering tendencies with more detailed measurements, especially for hypergraphs with larger hyperedges. The proposed clustering coefficient provides a more accurate representation of structural characteristics in complex networks, particularly in systems where group membership implies connections between members, such as social communities and co-authorship networks.  

<br /><br />Summary: <div>
arXiv:2410.23799v2 Announce Type: replace-cross 
Abstract: Hypergraphs are generalizations of simple graphs that allow for the representation of complex group interactions beyond pairwise relationships. Clustering coefficients quantify local link density in networks and have been widely studied for both simple graphs and hypergraphs. However, existing clustering coefficients for hypergraphs treat each hyperedge as a distinct unit rather than a collection of potentially related node pairs, failing to capture intra-hyperedge pairwise relationships and incorrectly assigning zero values to nodes with meaningful clustering patterns. We propose a novel clustering coefficient that addresses this fundamental limitation by transforming hypergraphs into weighted graphs, where edge weights reflect relationship strength between nodes based on hyperedge connections. Our definition satisfies three key conditions: values in the range $[0,1]$, consistency with simple graph clustering coefficients, and effective capture of intra-hyperedge pairwise relationships -- a capability absent from existing approaches. Theoretical evaluation on higher-order motifs demonstrates that our definition correctly assigns values to motifs where existing definitions fail (motifs III, IV-a, IV-b of order 3), while empirical evaluation on three real-world datasets shows similar overall clustering tendencies with more detailed measurements, especially for hypergraphs with larger hyperedges. The proposed clustering coefficient enables accurate quantification of local density in complex networks, revealing structural characteristics missed by existing definitions in systems where group membership implies connections between members, such as social communities and co-authorship networks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypermodularity and community detection in hypergraphs</title>
<link>https://arxiv.org/abs/2412.06935</link>
<guid>https://arxiv.org/abs/2412.06935</guid>
<content:encoded><![CDATA[
<div> community detection, higher-order networks, hypermodularity, spectral methods, hidden information<br />
<br />
Summary: 
This article introduces a formalism for detecting community structures in networks with higher-order interactions, such as hypergraphs. The concept of hypermodularity is utilized to apply spectral methods for community detection in these complex networks. The approach is tested on both synthetic random networks and real-world data, demonstrating its effectiveness in capturing the dynamics and nature of interactions within the networks. The study emphasizes the importance of considering higher-order interactions in network analysis and presents a valuable tool for extracting hidden information from intricate higher-order data sets. The results show that the proposed method can reveal nontrivial communities in various types of networked systems, including biological, social, and technological networks. This novel approach offers insights into the modular organization of networks, enabling a deeper understanding of their underlying structure and functionality. <div>
arXiv:2412.06935v2 Announce Type: replace-cross 
Abstract: Numerous networked systems feature a structure of nontrivial communities, which often correspond to their functional modules. Such communities have been detected in real-world biological, social and technological systems, as well as in synthetic models thereof. While much effort has been devoted to developing methods for community detection in traditional networks, the study of community structure in networks with higher-order interactions is still not as extensively explored. In this article, we introduce a formalism for the hypermodularity of higher-order networks that allows us to use spectral methods to detect community structures in hypergraphs. We apply this approach to synthetic random networks as well as to real-world data, showing that it produces results that reflect the nature and the dynamics of the interactions modelled, thereby constituting a valuable tool for the extraction of hidden information from complex higher-order data sets.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks</title>
<link>https://arxiv.org/abs/2502.02197</link>
<guid>https://arxiv.org/abs/2502.02197</guid>
<content:encoded><![CDATA[
<div> Signed networks, community detection, polarization, trust dynamics, local search <br />
<br />
Summary:
This paper presents a novel method for identifying polarized communities in signed networks, where edges are labeled as positive or negative to represent friendly or antagonistic interactions. The key challenge addressed by the proposed method is to detect internally cohesive and externally antagonistic communities while allowing for neutral vertices. By introducing a new optimization objective that avoids highly size-imbalanced solutions, the method outperforms existing approaches in solution quality. In addition, the paper introduces the first local search algorithm that extends to the setting with neutral vertices, providing a scalable solution for large networks. The approach is connected to block-coordinate Frank-Wolfe optimization, ensuring a linear convergence rate. Experimental results on real-world and synthetic datasets demonstrate the superiority of the proposed method in both solution quality and computational efficiency compared to state-of-the-art baselines. <br /><br />Summary: <div>
arXiv:2502.02197v2 Announce Type: replace-cross 
Abstract: Signed networks, where edges are labeled as positive or negative to represent friendly or antagonistic interactions, offer a natural framework for analyzing polarization, trust, and conflict in social systems. Detecting meaningful group structures in such networks is crucial for understanding online discourse, political divisions, and trust dynamics. A key challenge is to identify communities that are internally cohesive and externally antagonistic, while allowing for neutral or unaligned vertices. In this paper, we propose a method for identifying $k$ polarized communities that addresses a major limitation of prior methods: their tendency to produce highly size-imbalanced solutions. We introduce a novel optimization objective that avoids such imbalance. In addition, it is well known that approximation algorithms based on local search are highly effective for clustering signed networks when neutral vertices are not allowed. We build on this idea and design the first local search algorithm that extends to the setting with neutral vertices while scaling to large networks. By connecting our approach to block-coordinate Frank-Wolfe optimization, we prove a linear convergence rate, enabled by the structure of our objective. Experiments on real-world and synthetic datasets demonstrate that our method consistently outperforms state-of-the-art baselines in solution quality, while remaining competitive in computational efficiency.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommendation Algorithms on Social Media: Unseen Drivers of Political Opinion</title>
<link>https://arxiv.org/abs/2507.01978</link>
<guid>https://arxiv.org/abs/2507.01978</guid>
<content:encoded><![CDATA[
<div> Facebook, X, social media platforms, algorithms, political interest
<br />
Summary: 
This study examines the impact of social media platforms on political interest among users, analyzing data from over 3,300 participants. The research finds that moderate Facebook users show decreased political engagement, while minimal engagement with X boosts political interest. Demographic variations play a significant role, with males, older individuals, Black or African American users, and those with higher incomes exhibiting greater political interest. Republicans are particularly active on social media, potentially influencing engagement patterns. However, a key limitation is the lack of data regarding the content users are exposed to. Future research should explore these influences and consider additional platforms to enhance understanding. Addressing these gaps can provide insights into digital political mobilization, benefiting policymakers, educators, and platform designers in fostering healthier democratic engagement.
<br /><br /> <div>
arXiv:2507.01978v1 Announce Type: new 
Abstract: Social media broadly refers to digital platforms and applications that simulate social interactions online. This study investigates the impact of social media platforms and their algorithms on political interest among users. As social media usage continues to rise, platforms like Facebook and X (formerly Twitter) play increasingly pivotal roles in shaping political discourse. By employing statistical analyses on data collected from over 3,300 participants, this research identifies significant differences in how various social media platforms influence political interest. Findings reveal that moderate Facebook users demonstrate decreased political engagement, whereas even minimal engagement with X significantly boosts political interest. The study further identifies demographic variations, noting that males, older individuals, Black or African American users, those with higher incomes show greater political interest. The demographic analysis highlights that Republicans are particularly active on social media - potentially influencing their social media engagement patterns. However, the study acknowledges a crucial limitation - the lack of direct data regarding the content users are exposed to which is shaping their social media experiences. Future research should explore these influences and consider additional popular platforms to enhance the understanding of social media's political impact. Addressing these gaps can provide deeper insights into digital political mobilization, aiding policymakers, educators, and platform designers in fostering healthier democratic engagement.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Large Semi-Synthetic Graphs of Any Size</title>
<link>https://arxiv.org/abs/2507.02166</link>
<guid>https://arxiv.org/abs/2507.02166</guid>
<content:encoded><![CDATA[
<div> Keywords: graph generation, deep learning, Graph Neural Networks, Latent Graph Sampling Generation (LGSG), diffusion models

Summary: 
The article introduces Latent Graph Sampling Generation (LGSG), a new framework for generating graphs that addresses limitations of current models. LGSG leverages diffusion models and node embeddings to generate graphs of varying sizes without the need for retraining. By eliminating the dependency on node IDs and capturing the distribution of node embeddings and subgraph structures, LGSG enables scalable and flexible graph generation. Experimental results show that LGSG performs comparably to baseline models for standard metrics and outperforms them in metrics such as the tendency of nodes to form clusters. Additionally, LGSG maintains consistent structural characteristics across graphs of different sizes, demonstrating robustness and scalability. This framework represents a significant advancement in data-driven graph generation and has the potential to impact various applications in network science. 

<br /><br />Summary: <div>
arXiv:2507.02166v1 Announce Type: new 
Abstract: Graph generation is an important area in network science. Traditional approaches focus on replicating specific properties of real-world graphs, such as small diameters or power-law degree distributions. Recent advancements in deep learning, particularly with Graph Neural Networks, have enabled data-driven methods to learn and generate graphs without relying on predefined structural properties. Despite these advances, current models are limited by their reliance on node IDs, which restricts their ability to generate graphs larger than the input graph and ignores node attributes. To address these challenges, we propose Latent Graph Sampling Generation (LGSG), a novel framework that leverages diffusion models and node embeddings to generate graphs of varying sizes without retraining. The framework eliminates the dependency on node IDs and captures the distribution of node embeddings and subgraph structures, enabling scalable and flexible graph generation. Experimental results show that LGSG performs on par with baseline models for standard metrics while outperforming them in overlooked ones, such as the tendency of nodes to form clusters. Additionally, it maintains consistent structural characteristics across graphs of different sizes, demonstrating robustness and scalability.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features</title>
<link>https://arxiv.org/abs/2507.01984</link>
<guid>https://arxiv.org/abs/2507.01984</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation detection, multimodal features, early fusion approach, social media, COVID-19 pandemic

Summary:
The study explores the integration of text, images, and social features in detecting misinformation on social media during the COVID-19 pandemic and election periods. By analyzing 1,529 tweets, the researchers found that combining unsupervised and supervised machine learning models improved classification performance by 15% compared to unimodal models. Additionally, incorporating techniques like object detection and OCR for extracting visual features further enhanced the accuracy of the classification model. The study also delves into the propagation patterns of misinformation, shedding light on the characteristics of misinformation tweets and the users responsible for spreading them. Overall, the findings highlight the importance of leveraging multimodal feature combinations for effectively detecting misinformation and understanding its dissemination dynamics on social media platforms. 

<br /><br />Summary: <div>
arXiv:2507.01984v1 Announce Type: cross 
Abstract: Amid a tidal wave of misinformation flooding social media during elections and crises, extensive research has been conducted on misinformation detection, primarily focusing on text-based or image-based approaches. However, only a few studies have explored multimodal feature combinations, such as integrating text and images for building a classification model to detect misinformation. This study investigates the effectiveness of different multimodal feature combinations, incorporating text, images, and social features using an early fusion approach for the classification model. This study analyzed 1,529 tweets containing both text and images during the COVID-19 pandemic and election periods collected from Twitter (now X). A data enrichment process was applied to extract additional social features, as well as visual features, through techniques such as object detection and optical character recognition (OCR). The results show that combining unsupervised and supervised machine learning models improves classification performance by 15% compared to unimodal models and by 5% compared to bimodal models. Additionally, the study analyzes the propagation patterns of misinformation based on the characteristics of misinformation tweets and the users who disseminate them.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>