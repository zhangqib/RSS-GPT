<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.SI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.SI</link>


<item>
<title>A Longitudinal Analysis of Experiences with Semaglutide Across Twitter User Subpopulations</title>
<link>https://arxiv.org/abs/2505.18432</link>
<guid>https://arxiv.org/abs/2505.18432</guid>
<content:encoded><![CDATA[
<div> Keywords: pharmaceutical drug, social media, sentiment analysis, semaglutide, user engagement

Summary: 
The study analyzed over 850,000 tweets related to semaglutide on Twitter from July 2021 to April 2024 to understand how different user groups perceive the drug. Using sentiment and topic modeling techniques, the researchers found that organizational accounts expressed less negative sentiment compared to individuals, especially regarding efficacy and regulatory issues. Negativity around access and side effects was prevalent, while positivity stemmed from success stories and endorsements. A decline in sentiment was observed from Nov 2022 to Jan 2023, coinciding with regulatory alerts. Female users engaged more with celebrity and political discussions related to semaglutide. These findings provide crucial insights for healthcare communication and pharmacovigilance efforts to address public concerns and improve health communication strategies. The data analyzed were public and anonymized to ensure privacy and ethical compliance.<br /><br />Summary: <div>
arXiv:2505.18432v1 Announce Type: new 
Abstract: User experience significantly impacts pharmaceutical drug effectiveness. Social media platforms, particularly Twitter (now X), have become prominent venues for individuals to share medication-related experiences. This is especially true for semaglutide, a widely marketed drug that has sparked substantial public discourse. Despite the volume of conversation, a comprehensive understanding of how different user subpopulations engage with these discussions remains limited. Understanding such nuanced reactions is crucial for identifying public concerns, addressing misconceptions, and improving health communication. We analyzed 859,751 semaglutide-related tweets collected from July 2021 to April 2024, using sentiment and topic modeling to explore how the drug is perceived across user groups. We applied advanced analytical tools, including RoBERTa and BERTopic, to uncover trends and insights. To our knowledge, this is the most comprehensive sentiment and topic modeling analysis of semaglutide discourse on Twitter. Findings reveal significant sentiment differences across subpopulations: organizational accounts expressed less negative sentiment (mean -0.014) than individuals (-0.24), especially regarding efficacy and regulatory issues. Sentiment declined notably from Nov 2022 to Jan 2023, coinciding with regulatory alerts. Negativity clustered around access and side effects; positivity stemmed from success stories and endorsements. Female users engaged more with celebrity/political discussions (19.24% vs. 14.6% for males), while males showed slightly higher positivity overall. These insights inform healthcare communication and pharmacovigilance. All data were public and anonymized to ensure privacy and ethical compliance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring temporal dynamics in digital trace data: mining user-sequences for communication research</title>
<link>https://arxiv.org/abs/2505.18790</link>
<guid>https://arxiv.org/abs/2505.18790</guid>
<content:encoded><![CDATA[
<div> Keywords: communication, computational approaches, digital trace data, user-sequences, temporal dimension

Summary:
This paper discusses the disconnection between the dynamic nature of communication processes and the non-dynamic methodologies used by communication scholars. It introduces a new research framework that utilizes computational approaches to analyze fine-grained timestamps in digital trace data. The framework focuses on maintaining hyper-longitudinal information in the data and studying time-evolving user-sequences to gain insights into user activity with high temporal resolution. A case study is presented, applying six different approaches to real-world user-sequences collected from 309 unique users. The study highlights the importance of understanding the temporal dimension in communication processes, leveraging the abundance of digital trace data and advancements in analytical techniques.

<br /><br />Summary: <div>
arXiv:2505.18790v1 Announce Type: new 
Abstract: Communication is commonly considered a process that is dynamically situated in a temporal context. However, there remains a disconnection between such theoretical dynamicality and the non-dynamical character of communication scholars' preferred methodologies. In this paper, we argue for a new research framework that uses computational approaches to leverage the fine-grained timestamps recorded in digital trace data. In particular, we propose to maintain the hyper-longitudinal information in the trace data and analyze time-evolving 'user-sequences,' which provide rich information about user activity with high temporal resolution. To illustrate our proposed framework, we present a case study that applied six approaches (e.g., sequence analysis, process mining, and language-based models) to real-world user-sequences containing 1,262,775 timestamped traces from 309 unique users, gathered via data donations. Overall, our study suggests a conceptual reorientation towards a better understanding of the temporal dimension in communication processes, resting on the exploding supply of digital trace data and the technical advances in analytical approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Intervention for Self-triggering Spatial Networks with Application to Urban Crime Analytics</title>
<link>https://arxiv.org/abs/2505.19612</link>
<guid>https://arxiv.org/abs/2505.19612</guid>
<content:encoded><![CDATA[
<div> network intervention, self-exciting networks, critical nodes, spatiotemporal Hawkes network, predictive policing

Summary:
The study focuses on self-exciting networks where events at one node trigger activity at other nodes. The researchers develop an optimal network intervention model to target critical nodes and mitigate cascading effects in a Spatiotemporal Hawkes network. Previous studies have explored similar models in temporal Hawkes networks, but this work extends the analysis to a spatiotemporal context. Through simulations, the researchers demonstrate the effectiveness of their method in reducing intensity post-intervention compared to other heuristic strategies. In a real-world application, the model is applied to crime data from the LA police department database to identify neighborhoods for targeted interventions, showcasing its potential in predictive policing. This research highlights the importance of strategic interventions in self-exciting networks to prevent further propagation of undesirable consequences.  <br /><br />Summary: <div>
arXiv:2505.19612v1 Announce Type: new 
Abstract: In many network systems, events at one node trigger further activity at other nodes, e.g., social media users reacting to each other's posts or the clustering of criminal activity in urban environments. These systems are typically referred to as self-exciting networks. In such systems, targeted intervention at critical nodes can be an effective strategy for mitigating undesirable consequences such as further propagation of criminal activity or the spreading of misinformation on social media. In our work, we develop an optimal network intervention model to explore how targeted interventions at critical nodes can mitigate cascading effects throughout a Spatiotemporal Hawkes network. Similar models have been studied previously in the literature in purely temporal Hawkes networks, but in our work, we extend them to a spatiotemporal setup and demonstrate the efficacy of our methods by comparing the post-intervention reduction in intensity to other heuristic strategies in simulated networks. Subsequently, we use our method on crime data from the LA police department database to find neighborhoods for strategic intervention to demonstrate an application in predictive policing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Moderation and the New Epistemology of Fact Checking on Social Media</title>
<link>https://arxiv.org/abs/2505.20067</link>
<guid>https://arxiv.org/abs/2505.20067</guid>
<content:encoded><![CDATA[
<div> community-driven moderation, misinformation detection, social media platforms, crowd-checking, professional fact-checkers

Summary:
The article discusses the shift of social media platforms towards community-driven content moderation through initiatives like Community Notes. While community efforts can help combat misinformation with scale and speed, they cannot replace the role of professional fact-checkers due to the complexity of identifying misleading content influenced by personal biases and cultural contexts. The current approaches to misinformation detection on major platforms are examined, highlighting the challenges and promises of crowd-checking at scale. The importance of maintaining a balance between community-driven moderation and the expertise of professional fact-checkers is emphasized to effectively address the issue of misleading content. <div>
arXiv:2505.20067v1 Announce Type: new 
Abstract: Social media platforms have traditionally relied on internal moderation teams and partnerships with independent fact-checking organizations to identify and flag misleading content. Recently, however, platforms including X (formerly Twitter) and Meta have shifted towards community-driven content moderation by launching their own versions of crowd-sourced fact-checking -- Community Notes. If effectively scaled and governed, such crowd-checking initiatives have the potential to combat misinformation with increased scale and speed as successfully as community-driven efforts once did with spam. Nevertheless, general content moderation, especially for misinformation, is inherently more complex. Public perceptions of truth are often shaped by personal biases, political leanings, and cultural contexts, complicating consensus on what constitutes misleading content. This suggests that community efforts, while valuable, cannot replace the indispensable role of professional fact-checkers. Here we systemically examine the current approaches to misinformation detection across major platforms, explore the emerging role of community-driven moderation, and critically evaluate both the promises and challenges of crowd-checking at scale.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily Enhanced Graph Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.20089</link>
<guid>https://arxiv.org/abs/2505.20089</guid>
<content:encoded><![CDATA[
<div> homophily, Graph Domain Adaptation, label scarcity, graph alignment, mixed filters
Summary:
- Graph Domain Adaptation (GDA) aims to transfer knowledge from labeled source graphs to unlabeled target graphs to address label scarcity.
- Graph homophily, often overlooked in existing approaches, plays a crucial role in graph domain alignment.
- Discrepancies in homophily exist in benchmarks, impacting GDA performance.
- The proposed homophily alignment algorithm uses mixed filters to smooth graph signals and mitigate homophily discrepancies effectively.
- Experimental results on various benchmarks validate the efficacy of the novel method.
<br /><br />Summary: Graph Domain Adaptation (GDA) seeks to bridge the gap between labeled source graphs and unlabeled target graphs to tackle label scarcity. The study emphasizes the overlooked factor of graph homophily, showing its significance in graph domain alignment. Discovery of homophily discrepancies in benchmarks underscores their detrimental impact on GDA performance. To address this, a novel homophily alignment algorithm leveraging mixed filters is proposed to smooth graph signals and align homophily effectively. Experimental validation across diverse benchmarks confirms the method's effectiveness in improving GDA performance and mitigating homophily discrepancies. <div>
arXiv:2505.20089v1 Announce Type: new 
Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. In this paper, we highlight the significance of graph homophily, a pivotal factor for graph domain alignment, which, however, has long been overlooked in existing approaches. Specifically, our analysis first reveals that homophily discrepancies exist in benchmarks. Moreover, we also show that homophily discrepancies degrade GDA performance from both empirical and theoretical aspects, which further underscores the importance of homophily alignment in GDA. Inspired by this finding, we propose a novel homophily alignment algorithm that employs mixed filters to smooth graph signals, thereby effectively capturing and mitigating homophily discrepancies between graphs. Experimental results on a variety of benchmarks verify the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment spreads, but topics do not, in COVID-19 discussions within the Belgian Reddit community</title>
<link>https://arxiv.org/abs/2505.20185</link>
<guid>https://arxiv.org/abs/2505.20185</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, Belgian Reddit community, mitigation measures, sentiment, homophily

Summary:
- The study focuses on how topics and sentiments related to COVID-19 mitigation measures spread within the Belgian Reddit community.
- Analysis of 655,642 posts from January 2020 to June 2022 shows that post volume reflects external events rather than interactions within Reddit.
- Sentiment in posts is influenced by previous sentiments, leading to homophily and polarization among users.
- Homophily measures are found to be 0.228 for lockdowns, 0.198 for masks, and 0.133 for vaccinations.
- A novel bounded confidence model estimates user sentiment, with Wasserstein metrics ranging between 0.493 (vaccination) and 0.607 (lockdown), providing insight into how the Belgian Reddit community engaged with pandemic-related topics and sentiments. 

<br /><br />Summary: The study examines the spread of COVID-19 mitigation topics and sentiments within the Belgian Reddit community. Post volume correlates with external events rather than Reddit interactions. Sentiment is influenced by previous posts, leading to homophily and polarization. Homophily measures vary for lockdowns, masks, and vaccinations. A novel model estimates internal sentiment of users. The results shed light on how the Belgian Reddit community experienced the pandemic and the factors influencing discussions and sentiments. <div>
arXiv:2505.20185v1 Announce Type: new 
Abstract: This study investigates how topics and sentiments on COVID-19 mitigation measures -- specifically lockdowns, mask mandates, and vaccinations -- spread through the Belgian Reddit community. We explore 655,642 posts created between 1 January 2020 and 30 June 2022. In line with previous studies for other countries and platforms, we find that the volume of posts on these topics can be tied to important external events, but not within-Reddit interactions. Sentiment, however, is influenced by the sentiment of previous posts, resulting in homophily and polarisation. We define a homophily measure and find values of 0.228, 0.198, and 0.133 for lockdowns, masks and vaccination, respectively. Additionally, we introduce a novel bounded confidence model that estimates internal sentiment of users from their expressed sentiment. The Wasserstein metric between the predicted and the observed sentiments takes values between 0.493 (vaccination) and 0.607 (lockdown). These results yield insight into the way the Belgian Reddit community experienced the pandemic, and which aspects influenced the topics discussed and their associated sentiment.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.18934</link>
<guid>https://arxiv.org/abs/2505.18934</guid>
<content:encoded><![CDATA[
<div> filter, anomaly detection, graph neural network, heterogeneous networks, Chi-Square

Summary: 
- The paper addresses the challenges of Graph Anomaly Detection (GAD) in heterogeneous networks by proposing ChiGAD, a spectral GNN framework.
- ChiGAD includes a Multi-Graph Chi-Square Filter to capture anomalous information, Interactive Meta-Graph Convolution for feature alignment, and Contribution-Informed Cross-Entropy Loss to prioritize difficult anomalies.
- The proposed method outperforms existing models on various datasets and shows superiority in handling heterogeneous networks.
- The homogeneous variant, ChiGNN, also excels in GAD tasks, demonstrating the effectiveness of Chi-Square filters.
- The code for ChiGAD is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2505.18934v1 Announce Type: cross 
Abstract: Graph Anomaly Detection (GAD) in heterogeneous networks presents unique challenges due to node and edge heterogeneity. Existing Graph Neural Network (GNN) methods primarily focus on homogeneous GAD and thus fail to address three key issues: (C1) Capturing abnormal signal and rich semantics across diverse meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment; and (C3) Learning effectively from difficult anomaly samples with class imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter, which captures anomalous information via applying dedicated Chi-Square filters to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns features while preserving high-frequency information and incorporates heterogeneous messages by a unified Chi-Square Filter; and (3) Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies to address class imbalance. Extensive experiments on public and industrial datasets show that ChiGAD outperforms state-of-the-art models on multiple metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD datasets, validating the effectiveness of Chi-Square filters. Our code is available at https://github.com/HsipingLi/ChiGAD.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2505.19286</link>
<guid>https://arxiv.org/abs/2505.19286</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, graph analysis, knowledge access, knowledge homophily, graph machine learning 

Summary: 
The study explores the structural patterns of large language models (LLMs) as neural knowledge bases. It quantifies knowledge at both triplet and entity levels and examines how it correlates with graph properties like node degree. The concept of knowledge homophily is introduced, indicating that closely connected entities possess similar knowledge levels. Utilizing this insight, a graph machine learning model is developed to predict entity knowledge based on its local neighborhood. This model allows for efficient knowledge checking by identifying triplets less familiar to LLMs. Experimental results demonstrate that fine-tuning models using the selected triplets leads to improved performance. This research highlights the importance of understanding the structural patterns of LLM knowledge and offers insights into leveraging graph analysis for enhancing knowledge representation and utilization in language models. 

Summary: <div>
arXiv:2505.19286v1 Announce Type: cross 
Abstract: Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement</title>
<link>https://arxiv.org/abs/2505.19355</link>
<guid>https://arxiv.org/abs/2505.19355</guid>
<content:encoded><![CDATA[
<div> framework, causal inference, Average Treatment Effects, social media, misinformation <br />
<br />
Summary: 
The study introduces a novel framework for understanding true influence in social media, focusing on distinguishing correlation from causation in the spread of misinformation. By leveraging a joint treatment-outcome approach and adapting causal inference techniques from healthcare, the model estimates Average Treatment Effects (ATE) within the sequential nature of social media interactions. This approach accounts for external confounding signals and outperforms existing benchmarks by 15-22% in predicting engagement across various scenarios. The experiments on real-world misinformation datasets demonstrate the effectiveness of the model in tackling challenges such as exposure adjustment, timing shifts, and intervention durations. Additionally, case studies on 492 social media users reveal a strong alignment between the model's causal effect measure and the expert-based empirical influence, showcasing the model's accuracy in estimating influence in social media. <br /> <div>
arXiv:2505.19355v1 Announce Type: cross 
Abstract: Understanding true influence in social media requires distinguishing correlation from causation--particularly when analyzing misinformation spread. While existing approaches focus on exposure metrics and network structures, they often fail to capture the causal mechanisms by which external temporal signals trigger engagement. We introduce a novel joint treatment-outcome framework that leverages existing sequential models to simultaneously adapt to both policy timing and engagement effects. Our approach adapts causal inference techniques from healthcare to estimate Average Treatment Effects (ATE) within the sequential nature of social media interactions, tackling challenges from external confounding signals. Through our experiments on real-world misinformation and disinformation datasets, we show that our models outperform existing benchmarks by 15--22% in predicting engagement across diverse counterfactual scenarios, including exposure adjustment, timing shifts, and varied intervention durations. Case studies on 492 social media users show our causal effect measure aligns strongly with the gold standard in influence estimation, the expert-based empirical influence.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Social Media Influence Experimentation via an Agentic Reinforcement Learning Large Language Model Bot</title>
<link>https://arxiv.org/abs/2411.19635</link>
<guid>https://arxiv.org/abs/2411.19635</guid>
<content:encoded><![CDATA[
<div> Keywords: public opinion evolution, online social platforms, influence mechanisms, Large Language Models, simulated environment<br />
Summary:<br />
This study presents a novel simulated environment that combines agentic intelligence with Large Language Models (LLMs) to explore topic-specific influence mechanisms ethically. The framework includes agents generating posts, forming opinions on topics, and socially interacting based on discussions. An opinion leader, utilizing Reinforcement Learning (RL), adapts its linguistic interactions to maximize influence and followers over time. The findings suggest that constraining the action space and incorporating self-observation are crucial for stable opinion leader generation. The simulation framework creates agents able to adapt to complex social dynamics. This work is valuable in understanding public opinion evolution on online platforms and the emergence of influence leaders, particularly in the context of increasing online influence on social attitudes. <br /> <div>
arXiv:2411.19635v2 Announce Type: replace 
Abstract: Understanding the dynamics of public opinion evolution on online social platforms is crucial for understanding influence mechanisms and the provenance of information. Traditional influence analysis is typically divided into qualitative assessments of personal attributes (e.g., psychology of influence) and quantitative evaluations of influence power mechanisms (e.g., social network analysis). One challenge faced by researchers is the ethics of real-world experimentation and the lack of social influence data. In this study, we provide a novel simulated environment that combines agentic intelligence with Large Language Models (LLMs) to test topic-specific influence mechanisms ethically. Our framework contains agents that generate posts, form opinions on specific topics, and socially follow/unfollow each other based on the outcome of discussions. This simulation allows researchers to observe the evolution of how opinions form and how influence leaders emerge. Using our own framework, we design an opinion leader that utilizes Reinforcement Learning (RL) to adapt its linguistic interaction with the community to maximize its influence and followers over time. Our current findings reveal that constraining the action space and incorporating self-observation are key factors for achieving stable and consistent opinion leader generation for topic-specific influence. This demonstrates the simulation framework's capacity to create agents that can adapt to complex and unpredictable social dynamics. The work is important in an age of increasing online influence on social attitudes and emerging technologies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Structural Knowledge in Diffusion Models for Source Localization in Data-Limited Graph Scenarios</title>
<link>https://arxiv.org/abs/2502.17928</link>
<guid>https://arxiv.org/abs/2502.17928</guid>
<content:encoded><![CDATA[
<div> Graph, information propagation, source localization, deep generative approaches, limited-data scenarios 
<br />
<br />
Summary: 
The paper introduces SIDSL, a framework for source localization in graph information propagation. It addresses challenges in limited-data scenarios by incorporating topology-aware priors, using a propagation-enhanced conditional denoiser with a GNN-LP, and implementing a structure-prior biased denoising scheme. Experimental results show SIDSL outperforms state-of-the-art methods, achieving 7.5-13.3% improvements in F1 scores and surpassing baselines by more than 18.8% when pretrained with simulation data. The framework proves effective in real-world applications with scarce labeled data, making it a valuable tool for managing network disruptions. <div>
arXiv:2502.17928v2 Announce Type: replace 
Abstract: The source localization problem in graph information propagation is crucial for managing various network disruptions, from misinformation spread to infrastructure failures. While recent deep generative approaches have shown promise in this domain, their effectiveness is limited by the scarcity of real-world propagation data. This paper introduces SIDSL (\textbf{S}tructure-prior \textbf{I}nformed \textbf{D}iffusion model for \textbf{S}ource \textbf{L}ocalization), a novel framework that addresses three key challenges in limited-data scenarios: unknown propagation patterns, complex topology-propagation relationships, and class imbalance between source and non-source nodes. SIDSL incorporates topology-aware priors through graph label propagation and employs a propagation-enhanced conditional denoiser with a GNN-parameterized label propagation module (GNN-LP). Additionally, we propose a structure-prior biased denoising scheme that initializes from structure-based source estimations rather than random noise, effectively countering class imbalance issues. Experimental results across four real-world datasets demonstrate SIDSL's superior performance, achieving 7.5-13.3% improvements in F1 scores compared to state-of-the-art methods. Notably, when pretrained with simulation data of synthetic patterns, SIDSL maintains robust performance with only 10% of training data, surpassing baselines by more than 18.8%. These results highlight SIDSL's effectiveness in real-world applications where labeled data is scarce.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Robust Flocking of Protesters on Street Networks</title>
<link>https://arxiv.org/abs/2406.01101</link>
<guid>https://arxiv.org/abs/2406.01101</guid>
<content:encoded><![CDATA[
<div> Keywords: protesters, city, tactics, alignment, flocking

Summary:
Protesters in a city can efficiently gather into large and mobile groups using a simple model based on random walkers and tactics derived from basic rules. Through experiments, the importance of a specific rule based on alignment for fast and robust flocking of walkers was identified. While other rules on their own were not as effective, combining alignment with them significantly improved flocking behavior. The model explores a variety of tactics and demonstrates that the inclusion of alignment enhances the formation of cohesive and resilient groups. This research underscores the critical role of alignment in facilitating the efficient organization of scattered protesters and highlights the robustness of the groups formed through the combination of rules. <div>
arXiv:2406.01101v3 Announce Type: replace-cross 
Abstract: We propose a simple model of protesters scattered throughout a city who want to gather into large and mobile groups. This model relies on random walkers on a street network that follow tactics built from a set of basic rules. Our goal is to identify the most important rules for fast and robust flocking of walkers. We explore a wide set of tactics and show the central importance of a specific rule based on alignment. Other rules alone perform poorly, but our experiments show that combining alignment with them enhances flocking, and that obtained groups are then remarkably robust.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Robustness of Graph Neural Networks against Adversarial Attacks</title>
<link>https://arxiv.org/abs/2406.13920</link>
<guid>https://arxiv.org/abs/2406.13920</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, adversarial attacks, robustness, evaluation metrics, model capacity

Summary:<br />
Graph neural networks (GNNs) have been found to be vulnerable to adversarial attacks, leading to concerns about their use in critical applications. To address this issue, a systematic study on the adversarial robustness of GNNs was conducted, considering input graph patterns, network architecture, and model capacity. The study also examined sensitive neurons and adversarial transferability. Two evaluation metrics, confidence-based decision surface, and accuracy-based adversarial transferability rate were introduced to assess robustness. The work provides 11 guidelines for designing robust GNNs based on empirical analysis. The code for the study is available online. This comprehensive framework offers valuable insights for developers to enhance GNNs' resistance to adversarial attacks. <br /><br />Summary: <div>
arXiv:2406.13920v2 Announce Type: replace-cross 
Abstract: Recent studies have shown that graph neural networks (GNNs) are vulnerable to adversarial attacks, posing significant challenges to their deployment in safety-critical scenarios. This vulnerability has spurred a growing focus on designing robust GNNs. Despite this interest, current advancements have predominantly relied on empirical trial and error, resulting in a limited understanding of the robustness of GNNs against adversarial attacks. To address this issue, we conduct the first large-scale systematic study on the adversarial robustness of GNNs by considering the patterns of input graphs, the architecture of GNNs, and their model capacity, along with discussions on sensitive neurons and adversarial transferability. This work proposes a comprehensive empirical framework for analyzing the adversarial robustness of GNNs. To support the analysis of adversarial robustness in GNNs, we introduce two evaluation metrics: the confidence-based decision surface and the accuracy-based adversarial transferability rate. Through experimental analysis, we derive 11 actionable guidelines for designing robust GNNs, enabling model developers to gain deeper insights. The code of this study is available at https://github.com/star4455/GraphRE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SaVe-TAG: Semantic-aware Vicinal Risk Minimization for Long-Tailed Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2410.16882</link>
<guid>https://arxiv.org/abs/2410.16882</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Vicinal Risk Minimization, Text-Attributed Graphs, Large Language Models, Synthetic Samples

Summary:
SaVe-TAG introduces a novel approach for handling class imbalance in real-world text-attributed graphs using Vicinal Risk Minimization. The method leverages Large Language Models for text-level interpolation to generate synthetic samples for minority classes. A confidence-based edge assignment mechanism is employed to ensure structural consistency and reduce noise in the generated samples. The integration of semantic information and graph topology proves to be crucial for balanced and effective learning. Experimental results demonstrate the superiority of SaVe-TAG over existing methods in addressing long-tailed distributions and improving node classification performance in text-attributed graphs.<br /><br />Summary: SaVe-TAG utilizes Vicinal Risk Minimization and Large Language Models to generate synthetic samples for minority classes in text-attributed graphs. The method employs a confidence-based edge assignment mechanism to ensure structural consistency and effectively mitigates class imbalance, outperforming existing approaches and highlighting the significance of semantic and structural signals in graph learning. <div>
arXiv:2410.16882v3 Announce Type: replace-cross 
Abstract: Real-world graph data often follows long-tailed distributions, making it difficult for Graph Neural Networks (GNNs) to generalize well across both head and tail classes. Recent advances in Vicinal Risk Minimization (VRM) have shown promise in mitigating class imbalance with numeric interpolation; however, existing approaches largely rely on embedding-space arithmetic, which fails to capture the rich semantics inherent in text-attributed graphs. In this work, we propose our method, SaVe-TAG (Semantic-aware Vicinal Risk Minimization for Long-Tailed Text-Attributed Graphs), a novel VRM framework that leverages Large Language Models (LLMs) to perform text-level interpolation, generating on-manifold, boundary-enriching synthetic samples for minority classes. To mitigate the risk of noisy generation, we introduce a confidence-based edge assignment mechanism that uses graph topology as a natural filter to ensure structural consistency. We provide theoretical justification for our method and conduct extensive experiments on benchmark datasets, showing that our approach consistently outperforms both numeric interpolation and prior long-tailed node classification baselines. Our results highlight the importance of integrating semantic and structural signals for balanced and effective learning on text-attributed graphs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Continual Graph Learning</title>
<link>https://arxiv.org/abs/2411.18919</link>
<guid>https://arxiv.org/abs/2411.18919</guid>
<content:encoded><![CDATA[
<div> Federated Continual Graph Learning, GNNs, evolving graphs, decentralized settings, storage costs <br />
<br />
Summary: 
In the study on Federated Continual Graph Learning (FCGL), the authors address the challenges of training graph neural networks on evolving graph data in decentralized settings. They conduct empirical analysis to assess the feasibility and effectiveness of FCGL, identifying two main challenges: local graph forgetting (LGF) and global expertise conflict (GEC). To overcome these challenges, they propose the POWER framework, which preserves and replays experience nodes and utilizes a pseudo prototype reconstruction strategy for knowledge transfer. Experiments on various graph datasets demonstrate the superiority of the POWER framework over existing baseline methods and federated continual learning approaches focused on vision tasks. Overall, the study highlights the importance of adapting GNNs to multiple evolving graphs in a decentralized manner while considering storage and privacy constraints. <br /><br />Summary: <div>
arXiv:2411.18919v2 Announce Type: replace-cross 
Abstract: In the era of big data, managing evolving graph data poses substantial challenges due to storage costs and privacy issues. Training graph neural networks (GNNs) on such evolving data usually causes catastrophic forgetting, impairing performance on earlier tasks. Despite existing continual graph learning (CGL) methods mitigating this to some extent, they rely on centralized architectures and ignore the potential of distributed graph databases to leverage collective intelligence. To address these challenges, we present a pioneering study on Federated Continual Graph Learning (FCGL), which adapts GNNs to multiple evolving graphs within decentralized settings while adhering to storage and privacy constraints. Our work begins with a comprehensive empirical analysis of FCGL, assessing its data characteristics, feasibility, and effectiveness, and reveals two non-trivial challenges: local graph forgetting (LGF), where local GNNs forget prior knowledge when adapting to new tasks, and global expertise conflict (GEC), where the global GNN exhibits sub-optimal performance in both adapting to new tasks and retaining old ones, arising from inconsistent client expertise during server-side parameter aggregation. To tackle these, we propose the POWER framework, which mitigates LGF by preserving and replaying experience nodes with maximum local-global coverage at each client and addresses GEC by using a pseudo prototype reconstruction strategy and trajectory-aware knowledge transfer at the central server. Experiments on various graph datasets demonstrate POWER's superiority over federated adaptations of CGL baselines and vision-centric federated continual learning approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees</title>
<link>https://arxiv.org/abs/2412.16441</link>
<guid>https://arxiv.org/abs/2412.16441</guid>
<content:encoded><![CDATA[
<div> graph neural network, task-trees, generalization, transfer learning, graph foundation model

Summary:
Task-trees are proposed as a novel approach for cross-task generalization in graph-structured data. The approach aligns node-, edge-, and graph-level tasks, enabling transferable knowledge learned from diverse tasks. The stability, transferability, and generalization properties of task-trees are theoretically analyzed, showing that pretraining a graph neural network on task-trees with a reconstruction objective induces transferable knowledge. The Graph Generality Identifier on Task-Trees (GIT) model, based on this approach, demonstrates strong performance on over 30 graphs across five domains through fine-tuning, in-context learning, and zero-shot generalization. The framework provides a foundation for efficient adaptation to downstream tasks with minimal fine-tuning, showcasing the potential for leveraging cross-task generalization in graph tasks. The code and data for the GIT model are available on GitHub for further exploration and validation. 

<br /><br />Summary: <div>
arXiv:2412.16441v3 Announce Type: replace-cross 
Abstract: Foundation models are pretrained on large-scale corpora to learn generalizable patterns across domains and tasks -- such as contours, textures, and edges in images, or tokens and sentences in text. In contrast, discovering such generalities in graph-structured data, especially across heterogeneous graph tasks, remains an open challenge. To address this, we propose a novel approach to cross-task generalization in graphs via task-trees, which serve as unified learning instances aligning node-, edge-, and graph-level tasks. We theoretically analyze the stability, transferability, and generalization properties of task-trees, showing that pretraining a graph neural network (GNN) on diverse task-trees with a reconstruction objective induces transferable knowledge. This enables efficient adaptation to downstream tasks with minimal fine-tuning. To validate our framework, we introduce Graph Generality Identifier on Task-Trees (GIT), a graph foundation model that demonstrates strong performance on over 30 graphs across five domains via fine-tuning, in-context learning, and zero-shot generalization. Code and data are available at https://github.com/Zehong-Wang/GIT.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of network communities driven by local rules</title>
<link>https://arxiv.org/abs/2501.17042</link>
<guid>https://arxiv.org/abs/2501.17042</guid>
<content:encoded><![CDATA[
<div> Ramsey community number, network communities, node heterogeneity, stochastic block model, local rules <br />
Summary: <br />
The article discusses network modeling and the emergence of communities within networks. It challenges the common belief that node heterogeneity, such as political affiliation or biological function, is necessary for the segregation of nodes into communities. Through numerical simulations, the author introduces the concept of the Ramsey community number, which indicates the minimum graph size needed for network communities to emerge with high certainty. Using the stochastic block model, the study demonstrates that networks governed by local rules have finite Ramsey community numbers, ensuring the presence of communities. In contrast, randomized networks do not exhibit this emergent property. This leads to a conjecture that network communities are a product of networks evolving based on local rules rather than node heterogeneity. <div>
arXiv:2501.17042v3 Announce Type: replace-cross 
Abstract: Natural systems are modeled by networks with nodes and links. Often the nodes are segregated into communities with different connectivity patterns. Node heterogeneity such as political affiliation in social networks or biological function in gene networks are highlighted as key factors driving the segregation of nodes into communities. Here, by means of numerical simulations, I show that node heterogeneity is not a necessary requirement. To this end I introduce the Ramsey community number, $r_\kappa$, the minimum graph size that warranties the emergence of network communities with almost certainty. Using the stochastic block model for community detection with correction for degree sequence, I show that networks generated by local rules have finite $r_\kappa$ values while their randomized versions do not have emergent communities. I conjecture that network communities are an emergent property of networks evolving with local rules.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Message Passing: Neural Graph Pattern Machine</title>
<link>https://arxiv.org/abs/2501.18739</link>
<guid>https://arxiv.org/abs/2501.18739</guid>
<content:encoded><![CDATA[
<div> substructure patterns, graph neural networks, Neural Graph Pattern Machine, expressivity, long-range dependencies 
Summary: 
The paper introduces the Neural Graph Pattern Machine (GPM), a framework that learns directly from graph substructures without relying on message passing. GPM efficiently extracts and encodes task-relevant graph patterns, offering greater expressivity and improved long-range dependency modeling. Empirical evaluations across node classification, link prediction, graph classification, and graph regression tasks show that GPM outperforms existing baselines. The model demonstrates strong out-of-distribution generalization, scalability, and interpretability. The code and datasets are available on GitHub at https://github.com/Zehong-Wang/GPM.<br /><br />Summary: <div>
arXiv:2501.18739v2 Announce Type: replace-cross 
Abstract: Graph learning tasks often hinge on identifying key substructure patterns -- such as triadic closures in social networks or benzene rings in molecular graphs -- that underpin downstream performance. However, most existing graph neural networks (GNNs) rely on message passing, which aggregates local neighborhood information iteratively and struggles to explicitly capture such fundamental motifs, like triangles, k-cliques, and rings. This limitation hinders both expressiveness and long-range dependency modeling. In this paper, we introduce the Neural Graph Pattern Machine (GPM), a novel framework that bypasses message passing by learning directly from graph substructures. GPM efficiently extracts, encodes, and prioritizes task-relevant graph patterns, offering greater expressivity and improved ability to capture long-range dependencies. Empirical evaluations across four standard tasks -- node classification, link prediction, graph classification, and graph regression -- demonstrate that GPM outperforms state-of-the-art baselines. Further analysis reveals that GPM exhibits strong out-of-distribution generalization, desirable scalability, and enhanced interpretability. Code and datasets are available at: https://github.com/Zehong-Wang/GPM.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2504.07830</link>
<guid>https://arxiv.org/abs/2504.07830</guid>
<content:encoded><![CDATA[
<div> simulation, social network, deception behaviors, content moderation, user engagement

Summary: 
The article introduces MOSAIC, a social network simulation framework that uses generative language agents to predict user behaviors in online platforms. By combining LLM agents with a social graph, the framework analyzes deceptive behaviors and user engagement with online content. Using diverse personas, the system enables large-scale simulations to study content dissemination dynamics. Three content moderation strategies are evaluated, showing effectiveness in mitigating misinformation spread and increasing user engagement. The trajectories of popular content are analyzed to understand simulation agents' interactions and engagement patterns. The open-source simulation software aims to facilitate further research in AI and social sciences. <div>
arXiv:2504.07830v2 Announce Type: replace-cross 
Abstract: We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Global Networks of Exchange through the Louvain Method</title>
<link>https://arxiv.org/abs/2505.17234</link>
<guid>https://arxiv.org/abs/2505.17234</guid>
<content:encoded><![CDATA[
<div> Congressional Research Service (CRS) reports, network analysis, weighted graph, Louvain method, community detection<br />
Summary:<br />
This study analyzes data from over 2,000 CRS reports to quantify relationships between 172 countries from 1996 to 2024. By converting the data into a weighted graph and applying the Louvain method, non-overlapping communities with shared interests are identified. The eigenvector centrality of countries is computed to determine their network influence. The findings have the potential to enhance the sourcing of evidence for analytical products and provide insights into the interconnectedness of the global landscape. <div>
arXiv:2505.17234v1 Announce Type: new 
Abstract: Congressional Research Service (CRS) reports provide detailed analyses of major policy issues to members of the US Congress. We extract and analyze data from 2,010 CRS reports written between 1996 and 2024 in order to quantify the relationships between countries. The data is processed and converted into a weighted graph, representing 172 unique countries as nodes and 4,137 interests as bidirectional edges. Through the Louvain method, we use a greedy algorithm to extract non-overlapping communities from our network and identify clusters with shared interests. We then compute the eigenvector centrality of countries, effectively highlighting their network influence. The results of this work could enable improvements in sourcing evidence for analytic products and understanding the connectivity of our world.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Graph Embedding through Hub-aware Random Walks</title>
<link>https://arxiv.org/abs/2505.17764</link>
<guid>https://arxiv.org/abs/2505.17764</guid>
<content:encoded><![CDATA[
<div> hub, dynamic graph embedding, random walk, network science, structure sensitivity

Summary:
- The study focuses on the influence of high-degree nodes, or hubs, in dynamic graph embedding.
- A new method called DeepHub is introduced to integrate hub sensitivity into random walk sampling strategies.
- Research conducted on nine real-world temporal networks shows that standard random walks tend to overrepresent hub nodes.
- Hub-aware walks can balance exploration, leading to better preservation of temporal neighborhood structure in embeddings.
- The results suggest that hub-awareness is crucial for dynamic graph embedding to improve downstream task performance. 

<br /><br />Summary: <div>
arXiv:2505.17764v1 Announce Type: new 
Abstract: The role of high-degree nodes, or hubs, in shaping graph dynamics and structure is well-recognized in network science, yet their influence remains underexplored in the context of dynamic graph embedding. Recent advances in representation learning for graphs have shown that random walk-based methods can capture both structural and temporal patterns, but often overlook the impact of hubs on walk trajectories and embedding stability. In this paper, we introduce DeepHub, a method for dynamic graph embedding that explicitly integrates hub sensitivity into random walk sampling strategies. Focusing on dynnode2vec as a representative dynamic embedding method, we systematically analyze the effect of hub-biased walks across nine real-world temporal networks. Our findings reveal that standard random walks tend to overrepresent hub nodes, leading to embeddings that underfit the evolving local context of less-connected nodes. By contrast, hub-aware walks can balance exploration, resulting in embeddings that better preserve temporal neighborhood structure and improve downstream task performance. These results suggest that hub-awareness is an important yet overlooked factor in dynamic graph embedding, and our work provides a foundation for more robust, structure-sensitive representation learning in evolving networks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Dynamics of Harmful Content Dissemination on WhatsApp</title>
<link>https://arxiv.org/abs/2505.18099</link>
<guid>https://arxiv.org/abs/2505.18099</guid>
<content:encoded><![CDATA[
<div> Keywords: WhatsApp, harmful content, message dissemination, structural characteristics, propagation patterns

Summary:
The study focuses on the dissemination of harmful messages on WhatsApp, analyzing over 5.1 million messages from 6,000 groups in India. It found that harmful messages spread more extensively than non-harmful ones, particularly through images and videos. However, the analysis revealed that dissemination patterns are not solely determined by the message format. Structural characteristics of message propagation play a significant role in the spread of harmful content. This suggests that interventions targeting how messages are reshared could be crucial in managing harmful content on private messaging platforms. The study emphasizes the need to consider both the modality and structural features in strategies aimed at controlling the dissemination of harmful messages on platforms like WhatsApp. 

<br /><br />Summary: <div>
arXiv:2505.18099v1 Announce Type: new 
Abstract: WhatsApp, a platform with more than two billion global users, plays a crucial role in digital communication, but also serves as a vector for harmful content such as misinformation, hate speech, and political propaganda. This study examines the dynamics of harmful message dissemination in WhatsApp groups, with a focus on their structural characteristics. Using a comprehensive data set of more than 5.1 million messages, including text, images, and videos, collected from approximately 6,000 groups in India, we reconstruct message propagation cascades to analyze dissemination patterns.
  Our findings reveal that harmful messages consistently achieve greater depth and breadth of dissemination compared to messages without harmful annotations, with videos and images emerging as the primary modes of dissemination. These results suggest a distinctive pattern of dissemination of harmful content. However, our analysis indicates that modality alone cannot fully account for the structural differences in propagation.
  The findings highlight the critical role of structural characteristics in the spread of these harmful messages, suggesting that strategies targeting structural characteristics of re-sharing could be crucial in managing the dissemination of such content on private messaging platforms.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion</title>
<link>https://arxiv.org/abs/2505.17038</link>
<guid>https://arxiv.org/abs/2505.17038</guid>
<content:encoded><![CDATA[
<div> Keywords: web data, disaster response, social media, crisis, AI-driven method

Summary: 
- The study examines the use of web data from social media and public inquiry submissions for government disaster response during the 2022 floods in New South Wales, Australia.
- Analysis of flood-related tweets and submissions reveals behavioral patterns during extreme weather events.
- The methodology integrates Latent Dirichlet Allocation (LDA) and Large Language Models (LLMs) to enhance semantic understanding of the data.
- LDA identifies distinct opinions and geographic patterns, while LLMs improve filtering to prioritize actionable content.
- The Relevance Index method developed in this study reduces noise in social media content, improving situational awareness for emergency responders and aiding in long-term resilience planning. 

<br /><br />Summary: <div>
arXiv:2505.17038v1 Announce Type: cross 
Abstract: Massive and diverse web data are increasingly vital for government disaster response, as demonstrated by the 2022 floods in New South Wales (NSW), Australia. This study examines how X (formerly Twitter) and public inquiry submissions provide insights into public behaviour during crises. We analyse more than 55,000 flood-related tweets and 1,450 submissions to identify behavioural patterns during extreme weather events. While social media posts are short and fragmented, inquiry submissions are detailed, multi-page documents offering structured insights. Our methodology integrates Latent Dirichlet Allocation (LDA) for topic modelling with Large Language Models (LLMs) to enhance semantic understanding. LDA reveals distinct opinions and geographic patterns, while LLMs improve filtering by identifying flood-relevant tweets using public submissions as a reference. This Relevance Index method reduces noise and prioritizes actionable content, improving situational awareness for emergency responders. By combining these complementary data streams, our approach introduces a novel AI-driven method to refine crisis-related social media content, improve real-time disaster response, and inform long-term resilience planning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning</title>
<link>https://arxiv.org/abs/2505.17068</link>
<guid>https://arxiv.org/abs/2505.17068</guid>
<content:encoded><![CDATA[
<div> Keywords: user toxicity, online discussions, health-related topics, Collaborative Filtering, Machine Learning

Summary:
User toxicity in health-related online discussions is a common issue that can lead to conflict and misinformation. Instead of detecting and removing toxic comments reactively, this study proposes a predictive approach to anticipate potential toxicity. By using Collaborative Filtering-based Machine Learning, the researchers were able to predict toxicity in COVID-related conversations on Reddit with over 80% accuracy. This predictive model can help prevent conflicts by identifying potentially toxic interactions between users and specific subcommunities. This proactive strategy could be a more effective and efficient way to manage user toxicity in online discussions, ultimately promoting healthier and more constructive conversations in health-related topics.
<br /><br />Summary: <div>
arXiv:2505.17068v1 Announce Type: cross 
Abstract: In health-related topics, user toxicity in online discussions frequently becomes a source of social conflict or promotion of dangerous, unscientific behaviour; common approaches for battling it include different forms of detection, flagging and/or removal of existing toxic comments, which is often counterproductive for platforms and users alike. In this work, we propose the alternative of combatting user toxicity predictively, anticipating where a user could interact toxically in health-related online discussions. Applying a Collaborative Filtering-based Machine Learning methodology, we predict the toxicity in COVID-related conversations between any user and subcommunity of Reddit, surpassing 80% predictive performance in relevant metrics, and allowing us to prevent the pairing of conflicting users and subcommunities.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Web and Software Agents -- A Forgotten Wave of Artificial Intelligence?</title>
<link>https://arxiv.org/abs/2503.20793</link>
<guid>https://arxiv.org/abs/2503.20793</guid>
<content:encoded><![CDATA[
<div> Semantic Web, AI, Software Agents, Knowledge Representation, Neural Models

Summary:
The paper argues that the history of Artificial Intelligence (AI) has experienced waves of optimism and disappointment, with the forgotten wave being the rise of the Semantic Web and intelligent Software Agents. While ChatGPT and Large Language Models now dominate the AI conversation, the Semantic Web aimed to create a machine-interpretable ecosystem for AI reasoning and action from 2000 to 2010. Despite fading into obscurity, revisiting this wave offers insights for modern Software Agent development as AI technologies evolve. Through bibliometric data analysis, the paper highlights the Semantic Web's significance in AI history and its potential relevance for current AI advancements. Recognizing this overlooked chapter provides a deeper understanding of AI's cyclical evolution and valuable lessons for merging emerging technologies. 

Summary: <div>
arXiv:2503.20793v2 Announce Type: replace 
Abstract: The history of Artificial Intelligence (AI) is a narrative of waves -- rising optimism followed by crashing disappointments. AI winters, such as the early 2000s, are often remembered as barren periods of innovation. This paper argues that such a perspective overlooks a crucial wave of AI that seems to be forgotten: the rise of the Semantic Web, which is based on knowledge representation, logic, and reasoning, and its interplay with intelligent Software Agents. Fast forward to today, and ChatGPT has reignited AI enthusiasm, built on deep learning and advanced neural models. However, before Large Language Models (LLMs) dominated the conversation, another ambitious vision emerged -- one where AI-driven Software Agents autonomously served Web users based on a structured, machine-interpretable Web. The Semantic Web aimed to transform the World Wide Web into an ecosystem where AI could reason, understand, and act. Between 2000 and 2010, this vision sparked a significant research boom, only to fade into obscurity as AI's mainstream narrative shifted elsewhere. Today, as LLMs edge toward autonomous execution, we revisit this overlooked wave. By analyzing its academic impact through bibliometric data, we highlight the Semantic Web's role in AI history and its untapped potential for modern Software Agent development. Recognizing this forgotten chapter not only deepens our understanding of AI's cyclical evolution but also offers key insights for integrating emerging technologies.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying urban socio-economic segregation through co-residence network reconstruction</title>
<link>https://arxiv.org/abs/2501.15920</link>
<guid>https://arxiv.org/abs/2501.15920</guid>
<content:encoded><![CDATA[
<div> Keywords: urban segregation, socio-economic disparities, migrant communities, co-residence preferences, integration

Summary: 
The study focuses on urban segregation in Vienna, where a significant migrant population resides. Using administrative data, the analysis highlights two key clusters in the city influenced by wealth inequalities, district diversity, and nationality-based preferences. This segregation stems from a mix of socio-economic factors and residential choices, exacerbating inequalities and social polarization. Understanding co-residence preferences between migrants and locals at the neighbourhood level provides crucial insights into the dynamics of segregation. The findings emphasize the need for targeted policies to promote integration and address the challenges posed by urban segregation. By shedding light on the underlying mechanisms driving segregation, the study contributes to informing strategies for creating more inclusive and cohesive urban communities. Through a comprehensive examination of these factors, the research lays the foundation for fostering a more integrated and equitable urban environment in Vienna. 

<br /><br />Summary: <div>
arXiv:2501.15920v2 Announce Type: replace-cross 
Abstract: Urban segregation poses a critical challenge in cities, exacerbating inequalities, social tensions, fears, and polarization. It emerges from a complex interplay of socio-economic disparities and residential preferences, disproportionately impacting migrant communities. In this paper, using a comprehensive administrative data from Vienna, where nearly 40% of the population consists of international migrants, we analyse co-residence preferences between migrants and locals at the neighbourhood level. Our findings reveal two major clusters in Vienna shaped by wealth disparities, district diversity, and nationality-based homophily. These insights shed light on the underlying mechanisms of urban segregation and designing policies for better integration.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ricci Matrix Comparison for Graph Alignment: A DMC Variation</title>
<link>https://arxiv.org/abs/2505.15831</link>
<guid>https://arxiv.org/abs/2505.15831</guid>
<content:encoded><![CDATA[
<div> geometric graph alignment, Ricci Matrix Comparison, Degree Matrix Comparison, torus, protein-protein interaction network  
Summary:  
The paper introduces the Ricci Matrix Comparison (RMC) and discusses its application in geometric graph alignment along with the Degree Matrix Comparison (DMC). The study begins by exploring different methods of constructing a torus and then introduces the RMC based on DMC with theoretical justifications. Experimental results on a torus and a complex protein-protein interaction network demonstrate the effectiveness of utilizing a differential-geometric approach to graph alignment. Results indicate that utilizing Ricci curvature in graph alignment can help identify holes in tori and align line graphs of complex networks with high accuracy. This study presents a novel perspective on graph alignment and validates the previous DMC method. <div>
arXiv:2505.15831v1 Announce Type: new 
Abstract: The graph alignment problem explores the concept of node correspondence and its optimality. In this paper, we focus on purely geometric graph alignment methods, namely our newly proposed Ricci Matrix Comparison (RMC) and its original form, Degree Matrix Comparison (DMC). To formulate a Ricci-curvature-based graph alignment situation, we start with discussing different ideas of constructing one of the most typical and important topological objects, the torus, and then move on to introducing the RMC based on DMC with theoretical motivations. Lastly, we will present to the reader experimental results on a torus and a complex protein-protein interaction network that indicate the potential of applying a differential-geometric view to graph alignment. Results show that a direct variation of DMC using Ricci curvature can help with identifying holes in tori and aligning line graphs of a complex network at 80-90+% accuracy. This paper contributes a new perspective to the field of graph alignment and partially shows the validity of the previous DMC method.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPPFND: A Dataset and Analysis of Detecting Fake News with Multi-Platform Propagation</title>
<link>https://arxiv.org/abs/2505.15834</link>
<guid>https://arxiv.org/abs/2505.15834</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news, social media, detection algorithms, propagation characteristics, graph neural networks

Summary:
The article introduces the MPPFND dataset, focusing on detecting fake news spread across multiple platforms. It highlights the distinct propagation structures and social context features of different platforms, emphasizing the need to consider cross-platform propagation differences. The proposed APSL model utilizes graph neural networks to extract social context features from various platforms, improving fake news detection performance. The study underscores the importance of analyzing both news content and social context to detect fake news effectively and mitigate its negative impact on society. The research suggests that by accounting for platform-specific characteristics, detection algorithms can enhance their accuracy in identifying fake news circulating on social media platforms. Overall, the study contributes valuable insights into addressing the challenges posed by the widespread dissemination of fake news on social media. 

<br /><br />Summary: <div>
arXiv:2505.15834v1 Announce Type: new 
Abstract: Fake news spreads widely on social media, leading to numerous negative effects. Most existing detection algorithms focus on analyzing news content and social context to detect fake news. However, these approaches typically detect fake news based on specific platforms, ignoring differences in propagation characteristics across platforms. In this paper, we introduce the MPPFND dataset, which captures propagation structures across multiple platforms. We also describe the commenting and propagation characteristics of different platforms to show that their social contexts have distinct features. We propose a multi-platform fake news detection model (APSL) that uses graph neural networks to extract social context features from various platforms. Experiments show that accounting for cross-platform propagation differences improves fake news detection performance.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web2Wiki: Characterizing Wikipedia Linking Across the Web</title>
<link>https://arxiv.org/abs/2505.15837</link>
<guid>https://arxiv.org/abs/2505.15837</guid>
<content:encoded><![CDATA[
<div> analysis, Wikipedia, Web, dataset, references
<br />
Wikipedia's influence beyond its platform is explored through a large-scale analysis of how it is referenced across the Web. The study focuses on English Wikipedia and finds that it is most commonly cited by news and science websites for informational purposes, with fewer references from commercial sites. The majority of Wikipedia links are found within the main content of websites, highlighting their role in structured knowledge presentation. The links primarily serve as explanatory references rather than as evidence or attribution, reinforcing Wikipedia's function as a background knowledge provider. The publicly released Web2Wiki dataset includes links from multiple language editions, enabling further research on Wikipedia's global impact on the Web.
<br /><br />Summary: <div>
arXiv:2505.15837v1 Announce Type: new 
Abstract: Wikipedia is one of the most visited websites globally, yet its role beyond its own platform remains largely unexplored. In this paper, we present the first large-scale analysis of how Wikipedia is referenced across the Web. Using a dataset from Common Crawl, we identify over 90 million Wikipedia links spanning 1.68% of Web domains and examine their distribution, context, and function. Our analysis of English Wikipedia reveals three key findings: (1) Wikipedia is most frequently cited by news and science websites for informational purposes, while commercial websites reference it less often. (2) The majority of Wikipedia links appear within the main content rather than in boilerplate or user-generated sections, highlighting their role in structured knowledge presentation. (3) Most links (95%) serve as explanatory references rather than as evidence or attribution, reinforcing Wikipedia's function as a background knowledge provider. While this study focuses on English Wikipedia, our publicly released Web2Wiki dataset includes links from multiple language editions, supporting future research on Wikipedia's global influence on the Web.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AH-UGC: Adaptive and Heterogeneous-Universal Graph Coarsening</title>
<link>https://arxiv.org/abs/2505.15842</link>
<guid>https://arxiv.org/abs/2505.15842</guid>
<content:encoded><![CDATA[
<div> Graph Coarsening, Adaptive, Heterogeneous, Locality Sensitive Hashing, Consistent Hashing  
Summary:  
Graph Coarsening (GC) is a technique for compressing large graphs to facilitate efficient learning and inference. Existing methods often have to recompute from scratch for different coarsening ratios, leading to unnecessary overhead. This new framework combines Locality Sensitive Hashing (LSH) and Consistent Hashing to enable adaptive graph coarsening, making the process inherently fast and scalable. It introduces a type isolated coarsening strategy for heterogeneous graphs, ensuring semantic consistency by restricting merges to nodes of the same type. This method is the first to support both adaptive and heterogeneous coarsening. Extensive evaluations on various types of graphs show that this approach achieves superior scalability while maintaining the original graph's structural and semantic integrity.  
<br /><br />Summary: <div>
arXiv:2505.15842v1 Announce Type: new 
Abstract: $\textbf{Graph Coarsening (GC)}$ is a prominent graph reduction technique that compresses large graphs to enable efficient learning and inference. However, existing GC methods generate only one coarsened graph per run and must recompute from scratch for each new coarsening ratio, resulting in unnecessary overhead. Moreover, most prior approaches are tailored to $\textit{homogeneous}$ graphs and fail to accommodate the semantic constraints of $\textit{heterogeneous}$ graphs, which comprise multiple node and edge types. To overcome these limitations, we introduce a novel framework that combines Locality Sensitive Hashing (LSH) with Consistent Hashing to enable $\textit{adaptive graph coarsening}$. Leveraging hashing techniques, our method is inherently fast and scalable. For heterogeneous graphs, we propose a $\textit{type isolated coarsening}$ strategy that ensures semantic consistency by restricting merges to nodes of the same type. Our approach is the first unified framework to support both adaptive and heterogeneous coarsening. Extensive evaluations on 23 real-world datasets including homophilic, heterophilic, homogeneous, and heterogeneous graphs demonstrate that our method achieves superior scalability while preserving the structural and semantic integrity of the original graph.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Prosocial Behavior and Social Contagion in LLM Agents under Institutional Interventions</title>
<link>https://arxiv.org/abs/2505.15857</link>
<guid>https://arxiv.org/abs/2505.15857</guid>
<content:encoded><![CDATA[
<div> prosocial behavior, large language models, simulation framework, social contexts, institutional dynamics <br />
Summary: <br />
The article introduces ProSim, a simulation framework to study prosocial behavior in large language models (LLMs) in various social and institutional settings. The framework consists of four components and is used to evaluate how LLM-based agents exhibit and adapt prosocial behavior. The studies conducted show that LLM agents display stable and context-sensitive prosocial behavior, respond to normative interventions, engage in fairness-based punishment, and react to inequities and enforcement costs. The research also highlights how policy-induced inequities can suppress prosocial behavior, spread through social networks, and be influenced by agents' perceptions of unfairness. This work sets the foundation for assessing social alignment and understanding institutional dynamics in societies driven by autonomous agents. <br /> <div>
arXiv:2505.15857v1 Announce Type: new 
Abstract: As large language models (LLMs) increasingly serve as autonomous agents in social contexts, understanding their capacity for prosocial behavior becomes essential. We present ProSim, a simulation framework designed to examine how prosocial behavior emerges, adapts, and erodes in LLM-based agents under diverse social and institutional conditions. The framework comprises four components: individual simulation, scenario simulation, interaction simulation, and intervention simulation. We conduct three progressive studies to evaluate prosocial alignment. First, we show that LLM agents can demonstrate stable and context-sensitive prosocial behavior across diverse scenarios and adapt their responses under normative policy interventions. Second, we find that agents engage in fairness-based third-party punishment and respond systematically to variations in inequity magnitude and enforcement cost. Third, we show that policy-induced inequities suppress prosocial behavior, propagate through social networks, and are mediated by agents' perceptions of unfairness. These findings lay the groundwork for evaluating social alignment and modeling institutional dynamics in agent-driven societies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tight Practical Bounds for Subgraph Densities in Ego-centric Networks</title>
<link>https://arxiv.org/abs/2505.16079</link>
<guid>https://arxiv.org/abs/2505.16079</guid>
<content:encoded><![CDATA[
<div> Subgraph densities, network analysis, localized structures, domain-driven features, flag algebras<br />
<br />
Summary: This paper discusses the importance of subgraph densities in network analysis and the challenges of distinguishing mathematically-determined features from domain-driven ones. By providing tighter bounds on subgraph densities and introducing the subgraph spread ratio, the study quantifies differences in realized subgraph densities across various types of networks. Through a combination of flag algebra techniques, motif-counting, and topological data analysis, the research demonstrates more accurate comparisons between graphs. The empirical analysis reveals that social networks have smaller subgraph spread ratios compared to other networks like linkage-mapping networks for Wikipedia pages, indicating distinct structural characteristics. The subgraph spread ratio offers a metric for quantifying network structures and comparing different types of networks. <div>
arXiv:2505.16079v1 Announce Type: new 
Abstract: Subgraph densities play a crucial role in network analysis, especially for the identification and interpretation of meaningful substructures in complex graphs. Localized subgraph densities, in particular, can provide valuable insights into graph structures. Distinguishing between mathematically-determined and domain-driven subgraph density features, however, poses challenges. For instance, the lack or presence of certain structures can be explained by graph density or degree distribution. These differences are especially meaningful in applied contexts as they allow us to identify instances where the data induces specific network structures, such as friendships in social networks. The goal of this paper is to measure these differences across various types of graphs, conducting social media analysis from a network perspective. To this end, we first provide tighter bounds on subgraph densities. We then introduce the subgraph spread ratio to quantify the realized subgraph densities of specific networks relative to the feasible bounds. Our novel approach combines techniques from flag algebras, motif-counting, and topological data analysis. Crucially, effective adoption of the state-of-the-art in the plain flag algebra method yields feasible regions up to three times tighter than prior best-known results, thereby enabling more accurate and direct comparisons across graphs. We additionally perform an empirical analysis of 11 real-world networks. We observe that social networks consistently have smaller subgraph spread ratios than other types of networks, such as linkage-mapping networks for Wikipedia pages. This aligns with our intuition about social relationships: such networks have meaningful structure that makes them distinct. The subgraph spread ratio enables the quantification of intuitive understandings of network structures and provides a metric for comparing types of networks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Rewiring Mechanism for Restoration of the Fragmented Social Networks after Attacks</title>
<link>https://arxiv.org/abs/2505.16233</link>
<guid>https://arxiv.org/abs/2505.16233</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, robustness, network reconstruction, strategic rewiring, Laplacian Energy

Summary: 
In this research work, the focus is on managing the security and resilience of complex systems, such as social networks, technological infrastructures, and communication networks, especially during times of disaster. The study involves reconstructing networks by rewiring or adding edges and measuring their robustness. Two approaches, strategic rewiring, and budget-constrained optimal rewiring, are utilized to evaluate network robustness. Unlike conventional methods that solely assess the largest connected component of a network, this research explores a more comprehensive approach by considering the impact of connection failures on network structure. By incorporating Laplacian Energy analysis, the study aims to gain a better understanding of network behavior during restoration processes while still considering the size of the largest connected component under attacks. This holistic approach provides valuable insights into enhancing network resilience in the face of disruptions. 

<br /><br />Summary: <div>
arXiv:2505.16233v1 Announce Type: new 
Abstract: Real-world complex systems exhibit intricate interconnections and dependencies, especially social networks, technological infrastructures, and communication networks. These networks are prone to disconnection due to random failures or external attacks on their components. Therefore, managing the security and resilience of such networks is a prime concern, particularly at the time of disaster. Therefore, in this research work, network is reconstructed by rewiring/addition of the edges and robustness of the networks is measured. To this aim, two approaches namely (i) Strategic rewiring (ii) budget constrained optimal rewiring are adopted. While current research often assesses robustness by examining the size of the largest connected component, this approach fails to capture the complete spectrum of vulnerability. The failure of a small number of connections leads to a sparser network yet connected network. Thus, the present research work delves deeper into evaluating the robustness of the restored network by evaluating Laplacian Energy to better comprehend the system's behavior during the restoration of the network still considering the size of the largest connected component attacks.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling in the Blanks? A Systematic Review and Theoretical Conceptualisation for Measuring WikiData Content Gaps</title>
<link>https://arxiv.org/abs/2505.16383</link>
<guid>https://arxiv.org/abs/2505.16383</guid>
<content:encoded><![CDATA[
<div> Keywords: Wikidata, knowledge graph, content gaps, systematic biases, completeness

Summary:
This paper presents a systematic literature review focusing on the content gaps within Wikidata, a collaborative knowledge graph used in Wikimedia projects. The study identifies a long-tail of items with limited data and systematic gaps in the available content. A typology of gaps is proposed based on prior research, along with a theoretical framework for conceptualizing and measuring these gaps. The methods and metrics used in the literature are classified to uncover overlooked gaps in Wikidata. The implications of these gaps on collaboration and editor activity within Wikidata are discussed, highlighting the importance of addressing quality, completeness, and systematic biases. The results provide insights into understanding knowledge gaps more broadly and offer valuable directions for future research in this area. 

<br /><br />Summary: <div>
arXiv:2505.16383v1 Announce Type: new 
Abstract: Wikidata is a collaborative knowledge graph which provides machine-readable structured data for Wikimedia projects including Wikipedia. Managed by a community of volunteers, it has grown to become the most edited Wikimedia project. However, it features a long-tail of items with limited data and a number of systematic gaps within the available content. In this paper, we present the results of a systematic literature review aimed to understand the state of these content gaps within Wikidata. We propose a typology of gaps based on prior research and contribute a theoretical framework intended to conceptualise gaps and support their measurement. We also describe the methods and metrics present used within the literature and classify them according to our framework to identify overlooked gaps that might occur in Wikidata. We then discuss the implications for collaboration and editor activity within Wikidata as well as future research directions. Our results contribute to the understanding of quality, completeness and the impact of systematic biases within Wikidata and knowledge gaps more generally.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Citation Parsing and Analysis with Language Models</title>
<link>https://arxiv.org/abs/2505.15948</link>
<guid>https://arxiv.org/abs/2505.15948</guid>
<content:encoded><![CDATA[
<div> Keywords: global inequalities, knowledge circulation, citation tracking, language models, research indexing

Summary: 
- The article addresses the need for a tool to support journals in understanding knowledge circulation and reducing global inequalities in knowledge production.
- It highlights the lack of information about knowledge sharing networks in the Global South and the resulting exclusion of Southern researchers from indexing services.
- The study investigates the use of open-weight language models to annotate manuscript citations in an indexable format.
- Evaluation of different models shows high accuracy in identifying citation components, surpassing current methods.
- The findings suggest that even smaller models can effectively parse citation fields with post-training, offering potential for improved citation network tracking and research discovery. 

<br /><br />Summary: <div>
arXiv:2505.15948v1 Announce Type: cross 
Abstract: A key type of resource needed to address global inequalities in knowledge production and dissemination is a tool that can support journals in understanding how knowledge circulates. The absence of such a tool has resulted in comparatively less information about networks of knowledge sharing in the Global South. In turn, this gap authorizes the exclusion of researchers and scholars from the South in indexing services, reinforcing colonial arrangements that de-center and minoritize those scholars. In order to support citation network tracking on a global scale, we investigate the capacity of open-weight language models to mark up manuscript citations in an indexable format. We assembled a dataset of matched plaintext and annotated citations from preprints and published research papers. Then, we evaluated a number of open-weight language models on the annotation task. We find that, even out of the box, today's language models achieve high levels of accuracy on identifying the constituent components of each citation, outperforming state-of-the-art methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all fields with high accuracy in $2^5$ passes, suggesting that post-training is likely to be effective in producing small, robust citation parsing models. Such a tool could greatly improve the fidelity of citation networks and thus meaningfully improve research indexing and discovery, as well as further metascientific research.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Graph Generative Modeling via Substructure Sequences</title>
<link>https://arxiv.org/abs/2505.16130</link>
<guid>https://arxiv.org/abs/2505.16130</guid>
<content:encoded><![CDATA[
<div> Transformer, pre-training, generative, scalability, graph learning  
Summary:  
Generative Graph Pattern Machine (G$^2$PM) is introduced as a novel approach for graph representation learning, moving beyond traditional message-passing methods. G$^2$PM leverages a generative Transformer pre-training framework to model graph instances as sequences of substructures, enabling the learning of transferable representations. The scalability of G$^2$PM is demonstrated through experiments on the ogbn-arxiv benchmark, showcasing performance improvements with larger model sizes compared to existing generative approaches. The systematic analysis of the model design space highlights key architectural choices contributing to scalability and generalization. G$^2$PM consistently outperforms strong baselines across various tasks such as node classification, graph classification, and transfer learning, establishing its effectiveness as a scalable graph learning framework. The code and dataset for G$^2$PM are publicly available for further research and exploration.  
Summary: <div>
arXiv:2505.16130v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) has been predominantly driven by message-passing, where node representations are iteratively updated via local neighborhood aggregation. Despite their success, message-passing suffers from fundamental limitations -- including constrained expressiveness, over-smoothing, over-squashing, and limited capacity to model long-range dependencies. These issues hinder scalability: increasing data size or model size often fails to yield improved performance, limiting the viability of GNNs as backbones for graph foundation models. In this work, we explore pathways beyond message-passing and introduce Generative Graph Pattern Machine (G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM represents graph instances (nodes, edges, or entire graphs) as sequences of substructures, and employs generative pre-training over the sequences to learn generalizable, transferable representations. Empirically, G$^2$PM demonstrates strong scalability: on the ogbn-arxiv benchmark, it continues to improve with model sizes up to 60M parameters, outperforming prior generative approaches that plateau at significantly smaller scales (e.g., 3M). In addition, we systematically analyze the model design space, highlighting key architectural choices that contribute to its scalability and generalization. Across diverse tasks -- including node classification, graph classification, and transfer learning -- G$^2$PM consistently outperforms strong baselines, establishing a compelling foundation for scalable graph learning. The code and dataset are available at https://github.com/Zehong-Wang/G2PM.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban transport systems shape experiences of social segregation</title>
<link>https://arxiv.org/abs/2505.16337</link>
<guid>https://arxiv.org/abs/2505.16337</guid>
<content:encoded><![CDATA[
<div> transportation, social segregation, urban policy, mobility, urban design

Summary: 
This study examines the role of urban transportation systems in shaping social segregation. Using city-scale GPS mobility data and a novel probabilistic mobility framework, the research shows how social interactions occur at different scales within transportation infrastructure. The study reveals that social segregation is influenced by factors such as time of day, urban design, and service design. It highlights the importance of understanding segregation as a product of daily mobility practices. Exploratory simulations suggest that transportation policies aimed at promoting sustainable transport may have unintended effects on segregation. The findings emphasize the need for urban policymakers to consider the broader impacts of their interventions and how they affect the daily experiences of residents. <div>
arXiv:2505.16337v1 Announce Type: cross 
Abstract: Mobility is a fundamental feature of human life, and through it our interactions with the world and people around us generate complex and consequential social phenomena. Social segregation, one such process, is increasingly acknowledged as a product of one's entire lived experience rather than mere residential location. Increasingly granular sources of data on human mobility have evidenced how segregation persists outside the home, in workplaces, cafes, and on the street. Yet there remains only a weak evidential link between the production of social segregation and urban policy. This study addresses this gap through an assessment of the role of the urban transportation systems in shaping social segregation. Using city-scale GPS mobility data and a novel probabilistic mobility framework, we establish social interactions at the scale of transportation infrastructure, by rail and bus service segment, individual roads, and city blocks. The outcomes show how social segregation is more than a single process in space, but varying by time of day, urban design and structure, and service design. These findings reconceptualize segregation as a product of likely encounters during one's daily mobility practice. We then extend these findings through exploratory simulations, highlighting how transportation policy to promote sustainable transport may have potentially unforeseen impacts on segregation. The study underscores that to understand social segregation and achieve positive social change urban policymakers must consider the broadest impacts of their interventions and seek to understand their impact on the daily lived experience of their citizens.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Inequality in Complex Networks of Strategic Agents using Iterative Game-Theoretic Transactions</title>
<link>https://arxiv.org/abs/2505.16966</link>
<guid>https://arxiv.org/abs/2505.16966</guid>
<content:encoded><![CDATA[
<div> transactions, social networks, scale-free distribution, Gini Coefficient, inequality 

Summary: 
- Transactions are essential in human social life, involving information, trust, and capital flow.
- Research on transactions lacks understanding of systemic effects in real-world social networks with different agent types.
- Gini Coefficient, important in economics for wealth inequality, is underutilized in complex networks and game theory studies.
- A model and simulation algorithm based on game theory are proposed to quantify inequality evolution in complex networks.
- Results show various drivers of inequality in simple settings, consistent across different network types. <div>
arXiv:2505.16966v1 Announce Type: cross 
Abstract: Transactions are an important aspect of human social life, and represent dynamic flow of information, intangible values, such as trust, as well as monetary and social capital. Although much research has been conducted on the nature of transactions in fields ranging from the social sciences to game theory, the systemic effects of different types of agents transacting in real-world social networks (often following a scale-free distribution) are not fully understood. A particular systemic measure that has not received adequate attention in the complex networks and game theory communities, is the Gini Coefficient, which is widely used in economics to quantify and understand wealth inequality. In part, the problem is a lack of experimentation using a replicable algorithm and publicly available data. Motivated by this problem, this article proposes a model and simulation algorithm, based on game theory, for quantifying the evolution of inequality in complex networks of strategic agents. Our results shed light on several complex drivers of inequality, even in simple, abstract settings, and exhibit consistency across networks with different origins and descriptions.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Influence Estimator: Towards Real-time Solutions to Influence Blocking Maximization</title>
<link>https://arxiv.org/abs/2308.14012</link>
<guid>https://arxiv.org/abs/2308.14012</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence blocking maximization, Neural influence estimator, Optimization algorithms, Monte Carlo simulations, Social network

Summary: 
In the realm of containing the spread of misinformation, real-time solutions to influence blocking maximization (IBM) problems are essential. Traditional methods rely on costly Monte Carlo simulations (MCSs) to assess blocked influence. However, a new approach introduces a neural influence estimator (NIE) as a fast surrogate model to address IBM problems efficiently. By formulating a learning problem to construct the NIE, this model can predict blocked influence based on false-and-true information instances and extract features from the network topology. The NIE can be combined with existing optimization algorithms to solve IBM problems online, showcasing impressive results. Experiments demonstrated that the NIE-based optimization method significantly outperforms MCSs-based methods in terms of speed and scalability, making it a promising solution for real-time IBM problem-solving. This innovation opens up new possibilities for efficiently tackling IBM problems in large social networks. 

<br /><br />Summary: <div>
arXiv:2308.14012v2 Announce Type: replace-cross 
Abstract: Real-time solutions to the influence blocking maximization (IBM) problems are crucial for promptly containing the spread of misinformation. However, achieving this goal is non-trivial, mainly because assessing the blocked influence of an IBM problem solution typically requires plenty of expensive Monte Carlo simulations (MCSs). This work presents a novel approach that enables solving IBM problems with hundreds of thousands of nodes and edges in seconds. The key idea is to construct a fast-to-evaluate surrogate model called neural influence estimator (NIE) offline as a substitute for the time-intensive MCSs, and then combine it with optimization algorithms to address IBM problems online. To this end, a learning problem is formulated to build the NIE that takes the false-and-true information instance as input, extracts features describing the topology and inter-relationship between two seed sets, and predicts the blocked influence. A well-trained NIE can generalize across different IBM problems given a social network, and can be readily combined with existing IBM optimization algorithms. The experiments on 25 IBM problems with up to millions of edges show that the NIE-based optimization method can be up to four orders of magnitude faster than MCSs-based optimization method to achieve the same optimization quality. Moreover, given a one-minute limit, the NIE-based method can solve IBM problems with up to hundreds of thousands of nodes, which is at least one order of magnitude larger than what can be solved by existing methods.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded-Confidence Models of Opinion Dynamics with Neighborhood Effects</title>
<link>https://arxiv.org/abs/2402.05368</link>
<guid>https://arxiv.org/abs/2402.05368</guid>
<content:encoded><![CDATA[
<div> neighborhood effects, opinion dynamics, bounded-confidence models, network adaptation, numerical simulations

Summary:
The study introduces neighborhood effects into bounded-confidence models (BCMs) of opinion dynamics, creating neighborhood BCMs (NBCMs). These NBCMs incorporate both dyadic influence between interacting agents and transitive influence from agents' neighborhoods. The model includes neighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK) BCMs. Additionally, a network adaptation component is introduced, where the network structure coevolves with agent opinions through transitive homophily. Numerical simulations on various network types show how the dynamics and network properties change based on the proportions of dyadic and transitive influence. The results indicate that including neighborhood effects in opinion dynamics and network adaptation leads to a reduction in network spectral gap and degree assortativity. <div>
arXiv:2402.05368v2 Announce Type: replace-cross 
Abstract: We generalize bounded-confidence models (BCMs) of opinion dynamics by incorporating neighborhood effects. In a BCM, interacting agents influence each other through dyadic influence if their opinions are sufficiently similar to each other. In our "neighborhood BCMs" (NBCMs), interacting agents are influenced both by each other's opinions and by the opinions of the agents in each other's neighborhoods. Our NBCMs thus include both the usual dyadic influence between agents and a "transitive influence", which encodes the influence of an agent's neighbors, when determining whether or not an interaction changes the opinions of agents. In this transitive influence, an individual's opinion is influenced by a neighbor when, on average, the opinions of the neighbor's neighbors are sufficiently similar to its own opinion. We formulate both neighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK) BCMs.
  We build further on our NBCMs by introducing a neighborhood-based network adaptation in which a network coevolves with agent opinions by changing its structure through "transitive homophily". In this network evolution, an agent breaks a tie to one of its neighbors and then rewires that tie to a new agent, with a preference for agents with a mean neighbor opinion that is closer to its own opinion. Using numerical simulations on a variety of types of networks, we explore how the qualitative opinion dynamics and network properties of our adaptive NDW model change as we adjust the relative proportions of dyadic and transitive influence. In our numerical experiments, we find that incorporating neighborhood effects into the opinion dynamics and the network-adaptation rewiring strategy tends to reduce the spectral gap and degree assortativity of networks.
  (This is a shortened version of the paper's abstract.)
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender differences in collaboration and career progression in physics</title>
<link>https://arxiv.org/abs/2408.02482</link>
<guid>https://arxiv.org/abs/2408.02482</guid>
<content:encoded><![CDATA[
<div> gender differences, collaboration networks, academic career progression, physics, principal investigator (PI)

Summary:
- The study examines gender differences in collaboration networks and academic career progression in physics.
- The relationship between collaborative behavior and career progression is similar for men and women when controlling for the number of publications.
- Researchers who eventually become principal investigators tend to have collaborated with more unique partners.
- Collaborating repeatedly with the same highly interconnected group or a larger number of co-authors per publication is associated with shorter career lengths and not attaining PI status.
- Women collaborate in more tightly connected and larger groups than men, and are less likely to attain the status of PI throughout their careers with a lower survival probability, highlighting the need for policies to address this gap.

<br /><br />Summary: <div>
arXiv:2408.02482v2 Announce Type: replace-cross 
Abstract: We examine gender differences in collaboration networks and academic career progression in physics. We use the likelihood and time to become a principal investigator (PI) and the length of an author's career to measure career progression. Utilising logistic regression and accelerated failure time models, we examine whether the effect of collaboration behaviour varies by gender. We find that, controlling for the number of publications, the relationship between collaborative behaviour and career progression is almost the same for men and women. Specifically, we find that those who eventually reach principal investigator (PI) status, tend to have published with more unique collaborators. In contrast, publishing repeatedly with the same highly interconnected collaborators and/or larger number of co-authors per publication is characteristic of shorter career lengths and not attaining PI status. We observe that women tend to collaborate in more tightly connected and larger groups than men. Finally, we observe that women are less likely to attain the status of PI throughout their careers and have a lower survival probability compared to men, which calls for policies to close this crucial gap.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Degree-Based Quasi-Clique Search via an Iterative Framework</title>
<link>https://arxiv.org/abs/2505.15118</link>
<guid>https://arxiv.org/abs/2505.15118</guid>
<content:encoded><![CDATA[
<div> maximum $\gamma$-quasi-clique, cohesive subgraph mining, IterQC, pseudo lower bound, preprocessing technique<br />
Summary:
The article introduces the novel algorithm IterQC for solving the maximum $\gamma$-quasi-clique problem, a fundamental issue in graph theory with diverse real-world applications. The problem is challenging due to its NP-hard nature and lack of the hereditary property. IterQC reformulates the problem as a series of $k$-plex problems and introduces optimization techniques like the pseudo lower bound and preprocessing. These techniques enhance efficiency by leveraging information across iterations and reducing problem size. Experimental results show that IterQC significantly outperforms existing algorithms DDA and FastQC in terms of speed and solving capability, achieving up to four orders of magnitude speedup and solving more graph instances effectively. This makes IterQC a promising algorithm for cohesive subgraph mining tasks. <br /><br />Summary: <div>
arXiv:2505.15118v1 Announce Type: new 
Abstract: Cohesive subgraph mining is a fundamental problem in graph theory with numerous real-world applications, such as social network analysis and protein-protein interaction modeling. Among various cohesive subgraphs, the $\gamma$-quasi-clique is widely studied for its flexibility in requiring each vertex to connect to at least a $\gamma$ proportion of other vertices in the subgraph. However, solving the maximum $\gamma$-quasi-clique problem is NP-hard and further complicated by the lack of the hereditary property, which makes designing efficient pruning strategies challenging. Existing algorithms, such as DDA and FastQC, either struggle with scalability or exhibit significant performance declines for small values of $\gamma$. In this paper, we propose a novel algorithm, IterQC, which reformulates the maximum $\gamma$-quasi-clique problem as a series of $k$-plex problems that possess the hereditary property. IterQC introduces a non-trivial iterative framework and incorporates two key optimization techniques: (1) the pseudo lower bound (pseudo LB) technique, which leverages information across iterations to improve the efficiency of branch-and-bound searches, and (2) the preprocessing technique that reduces problem size and unnecessary iterations. Extensive experiments demonstrate that IterQC achieves up to four orders of magnitude speedup and solves significantly more graph instances compared to state-of-the-art algorithms DDA and FastQC.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Distance on Epidemiological Dynamics in Human Connection Network with Mobility</title>
<link>https://arxiv.org/abs/2505.15331</link>
<guid>https://arxiv.org/abs/2505.15331</guid>
<content:encoded><![CDATA[
<div> Keywords: infectious diseases, human mobility, disease transmission, epidemiological dynamics, distance<br />
Summary:<br />
This study explores the impact of human mobility on the transmission of infectious diseases, particularly focusing on the role of distance between individuals. Moving beyond traditional metapopulation movement analysis, the research considers the proximity of an infected person to a healthy individual during movement. Mathematical expressions are derived for key epidemiological metrics, including the basic reproduction number ($R_0$) and the critical infection rate ($\beta_{critical}$), in relation to distance. The model developed in this study aligns closely with observed patterns of COVID-19 spread, as evidenced by analysis of available datasets. By incorporating distance into epidemiological dynamics, this research provides valuable insights into how human movement influences disease transmission at a personal level. The findings highlight the importance of considering spatial factors in understanding and predicting the spread of infectious diseases. <br /><br />Summary: <div>
arXiv:2505.15331v1 Announce Type: new 
Abstract: The spread of infectious diseases is often influenced by human mobility across different geographical regions. Although numerous studies have investigated how diseases like SARS and COVID-19 spread from China to various global locations, there remains a gap in understanding how the movement of individuals contributes to disease transmission on a more personal or human-to-human level. Typically, researchers have employed the concept of metapopulation movement to analyze how diseases move from one location to another. This paper shifts focus to the dynamics of disease transmission, incorporating the critical factor of distance between an infected person and a healthy individual during human movement. The study delves into the impact of distance on various parameters of epidemiological dynamics throughout human mobility. Mathematical expressions for important epidemiological metrics, such as the basic reproduction number ($R_0$) and the critical infection rate ($\beta_{critical}$), are derived in relation to the distance between individuals. The results indicate that the proposed model closely aligns with observed patterns of COVID-19 spread based on the analysis done on the available datasets.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of Reposting on X</title>
<link>https://arxiv.org/abs/2505.15370</link>
<guid>https://arxiv.org/abs/2505.15370</guid>
<content:encoded><![CDATA[
<div> predictive modeling, user reposting behavior, machine learning, out-of-distribution, Twitter

Summary:
The study focuses on predicting user reposting behavior on X (formerly Twitter) using machine learning models. Traditionally seen as a supervised classification task, the challenge shifts to out-of-distribution generalization when predicting reposting behavior for new topics. The results show existing algorithms perform well with matching distributions but falter when faced with out-of-distribution tasks. By incorporating user profile and past behavior features alongside message content features, prediction accuracy significantly improves. The study highlights the importance of considering a user's profile and past actions in reposting behavior prediction, suggesting it plays a crucial role independent of message content. <div>
arXiv:2505.15370v1 Announce Type: new 
Abstract: There have been considerable efforts to predict a user's reposting behaviour on X (formerly Twitter) using machine learning models. The problem is previously cast as a supervised classification task, where Tweets are randomly assigned to a test or training set. The random assignment helps to ensure that the test and training sets are drawn from the same distribution. In practice, we would like to predict users' reposting behaviour for a set of messages related to a new, previously unseen, topic (defined by a hashtag). In this case, the problem becomes an out-of-distribution generalisation classification task.
  Experimental results reveal that while existing algorithms, which predominantly use features derived from the content of Tweet messages, perform well when the training and test distributions are the same, these algorithms perform much worse when the test set is out of distribution. We then show that if the message features are supplemented or replaced with features derived from users' profile and past behaviour, the out-of-distribution prediction is greatly improved, with the F1 score increasing from 0.24 to 0.70. Our experimental results suggest that a significant component of reposting behaviour can be predicted based on users' profile and past behaviour, and is independent of the content of messages.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Foundation Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.15116</link>
<guid>https://arxiv.org/abs/2505.15116</guid>
<content:encoded><![CDATA[
<div> Graph Foundation Models (GFMs), scalable, general-purpose intelligence, structured data, transferability, emergent capabilities <br />
<br />
Summary: <br />
Graph Foundation Models (GFMs) aim to bring general-purpose intelligence to structured data, such as social networks, biological systems, and knowledge graphs. This survey provides an overview of GFMs, categorizing them based on their generalization scope. It discusses backbone architectures, pretraining strategies, and adaptation mechanisms, highlighting key innovations and theoretical insights. The survey also examines theoretical foundations, challenges, and future directions for research in this field. Positioned at the intersection of graph learning and general-purpose AI, GFMs have the potential to become foundational infrastructure for reasoning over structured data. The survey consolidates current progress in GFMs and outlines pathways for future research in this rapidly evolving field. <div>
arXiv:2505.15116v1 Announce Type: cross 
Abstract: Graph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs -- characterized by non-Euclidean structures and complex relational semantics -- poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope -- universal, task-specific, and domain-specific -- and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of human-like polarization among large language model agents</title>
<link>https://arxiv.org/abs/2501.05171</link>
<guid>https://arxiv.org/abs/2501.05171</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social networks, polarization, echo chamber effect, societal consequences

Summary: 
Large language models (LLMs) are rapidly advancing in their capabilities, impacting human societal dynamics. Simulated systems with thousands of LLM agents reveal a replication of human-like polarization through social interactions guided by LLM-generated conversations. These agents form their own social network with similarities to human clustering behavior while exhibiting mechanisms such as the echo chamber effect to shape collective opinions. Concerns arise regarding the potential for LLM agents to contribute to societal polarization, yet they also present an opportunity to explore strategies for mitigating polarization impacts. The similarities observed between human and LLM agent behaviors and emergent phenomena highlight the need for further understanding and preventative measures to address the risks associated with LLM influence on political deliberations and societal dynamics. 

<br /><br />Summary: <div>
arXiv:2501.05171v2 Announce Type: replace 
Abstract: Rapid advances in large language models (LLMs) have not only empowered autonomous agents to generate social networks, communicate, and form shared and diverging opinions on political issues, but have also begun to play a growing role in shaping human political deliberation. Our understanding of their collective behaviours and underlying mechanisms remains incomplete, however, posing unexpected risks to human society. In this paper, we simulate a networked system involving thousands of large language model agents, discovering their social interactions, guided through LLM conversation, result in human-like polarization. We discover that these agents spontaneously develop their own social network with human-like properties, including homophilic clustering, but also shape their collective opinions through mechanisms observed in the real world, including the echo chamber effect. Similarities between humans and LLM agents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise concerns about their capacity to amplify societal polarization, but also hold the potential to serve as a valuable testbed for identifying plausible strategies to mitigate polarization and its consequences.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal expansion of human mobility across urban scales</title>
<link>https://arxiv.org/abs/2406.06889</link>
<guid>https://arxiv.org/abs/2406.06889</guid>
<content:encoded><![CDATA[
<div> scaling law, mobility trajectories, network-based modules, urban systems, spatial structure

Summary:

This study explores the spatial structure of individual daily mobility trajectories and uncovers a universal scaling law that reveals a sublinear expansion of mobility modules with increasing distance from home. The analysis shows that these modules align with the nested hierarchy of urban systems, encompassing local, city-level, and regional scales as distance from home increases. This discovery provides a quantitative link between classic urban theories, human geography, and mobility studies, shedding light on the fundamental dynamics of human movement. By integrating network-based modules and spatial analysis, the study offers a new perspective on the underlying principles that govern human mobility patterns and their connection to urban structures. This research deepens our understanding of urban theory and highlights the intricate relationship between mobility behavior and spatial organization in cities. <div>
arXiv:2406.06889v4 Announce Type: replace-cross 
Abstract: Human mobility is a fundamental process underpinning socioeconomic life and urban structure. Classic theories, such as egocentric activity spaces and central place theory, provide crucial insights into specific facets of movement, like home-centricity and hierarchical spatial organization. However, identifying universal characteristics or an underlying principle that quantitatively links these disparate perspectives has remained a challenge. Here, we reveal such a connection by analyzing the spatial structure of individual daily mobility trajectories using network-based modules. We discover a universal scaling law: the spatial extent (radius) of these mobility modules expands sublinearly with increasing distance from home, a pattern consistent across three orders of magnitude. Furthermore, we demonstrate that these modules precisely map onto the nested hierarchy of urban systems, corresponding to local, city-level, and regional scales as distance from home increases. These findings deepen our understanding of human mobility dynamics and demonstrate the profound connection between classical urban theory, human geography, and mobility studies.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining the Prevalence and Dynamics of AI-Generated Media in Art Subreddits</title>
<link>https://arxiv.org/abs/2410.07302</link>
<guid>https://arxiv.org/abs/2410.07302</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated content, art communities, social dynamics, Reddit, community norms 

Summary: 
AI-generated content, particularly in the form of visual art, has the potential to impact social dynamics within online art communities on platforms like Reddit. The study explores the prevalence of AI-generated content and accusations of AI use within these communities. Findings reveal that AI posts account for less than 0.5% of image-based posts, with accusations of AI use being more persistent. Newcomers are more likely to use AI content, potentially increasing participation in communities. However, the tone of accusations of AI use has become increasingly negative over time, especially in communities without explicit rules on AI content. The study highlights the evolving norms and interactions surrounding AI-generated content in online creative communities. 

<br /><br />Summary: <div>
arXiv:2410.07302v2 Announce Type: replace-cross 
Abstract: Broadly accessible generative AI models like Dall-E have made it possible for anyone to create compelling visual art. In online communities, the introduction of AI-generated content (AIGC) may impact social dynamics, for example causing changes in who is posting content, or shifting the norms or the discussions around the posted content if posts are suspected of being generated by AI. We take steps towards examining the potential impact of AIGC on art-related communities on Reddit. We distinguish between communities that disallow AI content and those without such a direct policy. We look at image-based posts in these communities where the author transparently shares that the image was created by AI, and at comments in these communities that suspect or accuse authors of using generative AI. We find that AI posts (and accusations) have played a surprisingly small part in these communities through the end of 2023, accounting for fewer than 0.5% of the image-based posts. However, even as the absolute number of author-labeled AI posts dwindles over time, accusations of AI use remain more persistent. We show that AI content is more readily used by newcomers and may help increase participation if it aligns with community rules. However, the tone of comments suspecting AI use by others has become more negative over time, especially in communities that do not have explicit rules about AI. Overall, the results show the changing norms and interactions around AIGC in online communities designated for creativity.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Media Harm Abatement: Mechanisms for Transparent Public Health Assessment</title>
<link>https://arxiv.org/abs/2503.10458</link>
<guid>https://arxiv.org/abs/2503.10458</guid>
<content:encoded><![CDATA[
<div> lawsuits, social media platforms, product safety litigation, abatement plan, privacy

Summary:<br /><br /> This article discusses the lawsuits surrounding social media platforms and their potential harms. It suggests implementing an abatement and/or settlement plan as a remediation strategy outside of financial compensation, drawing on the history of American product safety litigation. The mechanism proposed would address the requirements of legal procedure, transparent public health assessment standards, and the practical needs of technology products. It anticipates the possible success of these lawsuits and outlines the implications for privacy and oversight. By operating at the intersection of these domains, the mechanism aims to mitigate abuse and improve the overall safety and accountability of social media platforms. <div>
arXiv:2503.10458v2 Announce Type: replace-cross 
Abstract: Social media platforms have been accused of causing a range of harms, resulting in dozens of lawsuits across jurisdictions. These lawsuits are situated within the context of a long history of American product safety litigation, suggesting opportunities for remediation outside of financial compensation. Anticipating that at least some of these cases may be successful and/or lead to settlements, this article outlines an implementable mechanism for an abatement and/or settlement plan capable of mitigating abuse. The paper describes the requirements of such a mechanism, implications for privacy and oversight, and tradeoffs that such a procedure would entail. The mechanism is framed to operate at the intersection of legal procedure, standards for transparent public health assessment, and the practical requirements of modern technology products.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pantheon: Personalized Multi-objective Ensemble Sort via Iterative Pareto Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13894</link>
<guid>https://arxiv.org/abs/2505.13894</guid>
<content:encoded><![CDATA[
<div> Keywords: Pantheon, ensemble sorting, personalized joint training, representation inheritance, iterative Pareto policy optimization 

Summary: 
Pantheon is introduced as an advanced ensemble sorting method that shifts from human-curated to machine-optimized science. It offers personalized joint training by aligning with real-time ranking models, ensuring accurate capture of user interests. Utilizing fine-grained hidden-states instead of compressed Pxtrs for model input enhances complexity and benefits from ranking models. An iterative Pareto policy optimization strategy, designed for balanced multi-objective ensemble sorting, considers multiple objectives concurrently. This innovative approach has successfully replaced formulation-based ensemble sort in the industry's recommendation systems, with full deployment at Kuaishou live-streaming services, catering to 400 million daily users. 

<br /><br />Summary: <div>
arXiv:2505.13894v1 Announce Type: new 
Abstract: In this paper, we provide our milestone ensemble sort work and the first-hand practical experience, Pantheon, which transforms ensemble sorting from a "human-curated art" to a "machine-optimized science". Compared with formulation-based ensemble sort, our Pantheon has the following advantages: (1) Personalized Joint Training: our Pantheon is jointly trained with the real-time ranking model, which could capture ever-changing user personalized interests accurately. (2) Representation inheritance: instead of the highly compressed Pxtrs, our Pantheon utilizes the fine-grained hidden-states as model input, which could benefit from the Ranking model to enhance our model complexity. Meanwhile, to reach a balanced multi-objective ensemble sort, we further devise an \textbf{iterative Pareto policy optimization} (IPPO) strategy to consider the multiple objectives at the same time. To our knowledge, this paper is the first work to replace the entire formulation-based ensemble sort in industry RecSys, which was fully deployed at Kuaishou live-streaming services, serving 400 Million users daily.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Influencers and Multipliers Drive Polarization and Issue Alignment on Twitter/X</title>
<link>https://arxiv.org/abs/2505.14280</link>
<guid>https://arxiv.org/abs/2505.14280</guid>
<content:encoded><![CDATA[
<div> Keywords: German Twittersphere, polarization, trending topics, influencers, multipliers

Summary:
The study explores the polarization of the German Twittersphere by analyzing trending topics and opinions expressed through (re)tweets from March 2021 to July 2023. It identifies two main ideological camps: left-leaning and right-leaning accounts, indicating a divided online public sphere. Contrary to traditional surveys, political issues exhibit strong alignment, driven by influencers who create ideologically charged content and multipliers who amplify it. These multipliers, unique to social media, play a significant role in shaping online opinion by curating and spreading content that aligns with their ideological stance. The study sheds light on the mechanisms shaping online public opinion and emphasizes the importance of regulating platforms to address the observed polarization.<br /><br />Summary: <div>
arXiv:2505.14280v1 Announce Type: new 
Abstract: We investigate the polarization of the German Twittersphere by extracting the main issues discussed and the signaled opinions of users towards those issues based on (re)tweets concerning trending topics. The dataset covers daily trending topics from March 2021 to July 2023. At the opinion level, we show that the online public sphere is largely divided into two camps, one consisting mainly of left-leaning, and another of right-leaning accounts. Further we observe that political issues are strongly aligned, contrary to what one may expect from surveys. This alignment is driven by two cores of strongly active users: influencers, who generate ideologically charged content, and multipliers, who facilitate the spread of this content. The latter are specific to social media and play a crucial role as intermediaries on the platform by curating and amplifying very specific types of content that match their ideological position, resulting in the overall observation of a strongly polarized public sphere. These results contribute to a better understanding of the mechanisms that shape online public opinion, and have implications for the regulation of platforms.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UKTwitNewsCor: A Dataset of Online Local News Articles for the Study of Local News Provision</title>
<link>https://arxiv.org/abs/2505.14326</link>
<guid>https://arxiv.org/abs/2505.14326</guid>
<content:encoded><![CDATA[
<div> dataset, online news, local media, social media, UK<br />
Summary:<br />
The paper introduces UKTwitNewsCor, a dataset containing over 2.5 million online news articles from 360 local UK outlets published between January 2020 and December 2022. The dataset includes articles shared on Twitter by these outlets, as well as social media performance metrics at the tweet level. Additionally, metadata on content duplication across domains is provided. Supplementary datasets on local media web domains, UK Local Authority Districts, and digital local media providers give insight into the dataset's coverage scope. The paper discusses the data collection methodology, diversity in geographical and media ownership, and how researchers, policymakers, and industry stakeholders can utilize UKTwitNewsCor for studying local media trends, content diversity, and audience engagement dynamics. <br /><br />Summary: <div>
arXiv:2505.14326v1 Announce Type: new 
Abstract: In this paper, we present UKTwitNewsCor, a comprehensive dataset for understanding the content production, dissemination, and audience engagement dynamics of online local media in the UK. It comprises over 2.5 million online news articles published between January 2020 and December 2022 from 360 local outlets. The corpus represents all articles shared on Twitter by the social media accounts of these outlets. We augment the dataset by incorporating social media performance metrics for the articles at the tweet-level. We further augment the dataset by creating metadata about content duplication across domains. Alongside the article dataset, we supply three additional datasets: a directory of local media web domains, one of UK Local Authority Districts, and one of digital local media providers, providing statistics on the coverage scope of UKTwitNewsCor. Our contributions enable comprehensive, longitudinal analysis of UK local media, news trends, and content diversity across multiple platforms and geographic areas. In this paper, we describe the data collection methodology, assess the dataset geographic and media ownership diversity, and outline how researchers, policymakers, and industry stakeholders can leverage UKTwitNewsCor to advance the study of local media.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindVote: How LLMs Predict Human Decision-Making in Social Media Polls</title>
<link>https://arxiv.org/abs/2505.14422</link>
<guid>https://arxiv.org/abs/2505.14422</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Models, MindVote, social media polling, decision-making

Summary: 
The article introduces MindVote, a new benchmark for evaluating the ability of Large Language Models (LLMs) to predict human decision-making in dynamic social contexts. It comprises 276 poll instances from three platforms, features bilingual content, and covers five domains. The evaluation of 18 LLMs shows top models achieving a 0.74 overall score, 80% better than traditional baselines. The analysis uncovers disparities related to platform, language, and domain. Strategies to optimize LLM performance and assess reasoning in societal contexts are presented. The article also discusses temperature controls reflecting human thinking diversity. MindVote offers a scalable framework to evaluate LLMs' social intelligence, with implications for understanding behavioral decision-making. Code and data will be available soon. 

Summary: <div>
arXiv:2505.14422v1 Announce Type: new 
Abstract: The increasing complexity of Large Language Models (LLMs) necessitates new benchmarks to assess their ability to predict human decision-making in dynamic social contexts. We introduce MindVote, the first benchmark for evaluating LLMs as "virtual respondents" in social media polling. MindVote comprises 276 poll instances with 1,142 data entry points from three platforms (Weibo, Reddit, Fizz), features bilingual content (Chinese/English), and covers five domains. Our evaluation of 18 LLMs demonstrates that top-performing models achieve an overall score of 0.74, an 80% relative improvement over traditional baselines, and then we analyze LLM world model bias with human preferences across societal bias dimensions. MindVote also uncovers significant disparities related to platform, language, and domain. We present strategies to optimize LLM performance and use LLM-as-a-Judge to assess reasoning in societal contexts. Furthermore, we show that temperature controls can reflect a way of human thinking diversity and opinion shifts in polling. In summary, MindVote offers a scalable framework for evaluating LLMs' social intelligence, with implications for understanding behavioral decision-making. Code and data will be available soon.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness Evaluation of Graph-based News Detection Using Network Structural Information</title>
<link>https://arxiv.org/abs/2505.14453</link>
<guid>https://arxiv.org/abs/2505.14453</guid>
<content:encoded><![CDATA[
<div> vulnerability, adversarial attacks, graph neural networks, fake news detection, robustness evaluation
Summary:<br /><br />
- The study focuses on the vulnerability of Graph Neural Networks (GNNs) in fake news detection, particularly concerning adversarial manipulations within social networks.
- Existing methods fail to consider the structural relationships surrounding target news, thus limiting their effectiveness in assessing detection robustness.
- The proposed SI2AF framework introduces structural entropy to quantify social engagements and identify hierarchical communities, enabling the design of multiple agents to optimize evasion against black-box detectors.
- Three attack strategies are developed for each target news through multi-agent collaboration within the associated subgraph, resulting in improved attack effectiveness.
- SI2AF significantly outperforms existing baselines and enhances GNN-based detection robustness by 41.54% on average. <div>
arXiv:2505.14453v1 Announce Type: new 
Abstract: Although Graph Neural Networks (GNNs) have shown promising potential in fake news detection, they remain highly vulnerable to adversarial manipulations within social networks. Existing methods primarily establish connections between malicious accounts and individual target news to investigate the vulnerability of graph-based detectors, while they neglect the structural relationships surrounding targets, limiting their effectiveness in robustness evaluation. In this work, we propose a novel Structural Information principles-guided Adversarial Attack Framework, namely SI2AF, which effectively challenges graph-based detectors and further probes their detection robustness. Specifically, structural entropy is introduced to quantify the dynamic uncertainty in social engagements and identify hierarchical communities that encompass all user accounts and news posts. An influence metric is presented to measure each account's probability of engaging in random interactions, facilitating the design of multiple agents that manage distinct malicious accounts. For each target news, three attack strategies are developed through multi-agent collaboration within the associated subgraph to optimize evasion against black-box detectors. By incorporating the adversarial manipulations generated by SI2AF, we enrich the original network structure and refine graph-based detectors to improve their robustness against adversarial attacks. Extensive evaluations demonstrate that SI2AF significantly outperforms state-of-the-art baselines in attack effectiveness with an average improvement of 16.71%, and enhances GNN-based detection robustness by 41.54% on average.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counter-Inferential Behavior in Natural and Artificial Cognitive Systems</title>
<link>https://arxiv.org/abs/2505.13551</link>
<guid>https://arxiv.org/abs/2505.13551</guid>
<content:encoded><![CDATA[
<div> counter-inferential behavior, cognitive systems, epistemic rigidity, maladaptive stability, cognitive vulnerability
<br />
Summary:<br />
This study examines the emergence of counter-inferential behavior in natural and artificial cognitive systems. The researchers identify three scenarios where such behavior arises: reinforcement of stability through reward imbalance, meta-cognitive attribution of success to internal superiority, and protective reframing under perceived model fragility. These behaviors do not stem from noise or flawed design but from structured interactions between internal information models, feedback, and evaluation mechanisms. The study draws on evidence from artificial systems, biological cognition, human psychology, and social dynamics to highlight counter-inferential behavior as a general cognitive vulnerability. The findings stress the importance of maintaining adaptive activation under stable conditions and propose design principles for cognitive architectures to resist rigidity during informational stress.
 <div>
arXiv:2505.13551v1 Announce Type: cross 
Abstract: This study explores the emergence of counter-inferential behavior in natural and artificial cognitive systems, that is, patterns in which agents misattribute empirical success or suppress adaptation, leading to epistemic rigidity or maladaptive stability. We analyze archetypal scenarios in which such behavior arises: reinforcement of stability through reward imbalance, meta-cognitive attribution of success to internal superiority, and protective reframing under perceived model fragility. Rather than arising from noise or flawed design, these behaviors emerge through structured interactions between internal information models, empirical feedback, and higher-order evaluation mechanisms. Drawing on evidence from artificial systems, biological cognition, human psychology, and social dynamics, we identify counter-inferential behavior as a general cognitive vulnerability that can manifest even in otherwise well-adapted systems. The findings highlight the importance of preserving minimal adaptive activation under stable conditions and suggest design principles for cognitive architectures that can resist rigidity under informational stress.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: A Systematic Review of Task Formulations and Machine Learning Methods</title>
<link>https://arxiv.org/abs/2311.00721</link>
<guid>https://arxiv.org/abs/2311.00721</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Empathy Detection, Affective Computing, Datasets, Network Architecture

Summary:
Empathy detection through Machine Learning has gained attention across various disciplines. A systematic literature review identified key task formulations such as localised utterances, overall expressions, unidirectional or parallel empathy, and emotional contagion in different interaction scenarios. Empathy detection methods were categorized based on input modalities including text, audiovisual, audio, and physiological signals, with specific network architecture design protocols outlined for each modality. Challenges and research gaps were discussed, highlighting the need for further exploration in the Affective Computing-based empathy domain. The paper also emphasized the potential applications of empathy detection in society, healthcare, and education, underscoring the importance of enhancing human well-being through robust empathy detection systems. Additionally, the availability of datasets and codes was mentioned to facilitate future research in this area. 

<br /><br />Summary: <div>
arXiv:2311.00721v4 Announce Type: replace-cross 
Abstract: Empathy indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science, and Psychology. Detecting empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection leveraging Machine Learning remains underexplored from a systematic literature review perspective. We collected 849 papers from 10 well-known academic databases, systematically screened them and analysed the final 82 papers. Our analyses reveal several prominent task formulations - including empathy on localised utterances or overall expressions, unidirectional or parallel empathy, and emotional contagion - in monadic, dyadic and group interactions. Empathy detection methods are summarised based on four input modalities - text, audiovisual, audio and physiological signals - thereby presenting modality-specific network architecture design protocols. We discuss challenges, research gaps and potential applications in the Affective Computing-based empathy domain, which can facilitate new avenues of exploration. We further enlist the public availability of datasets and codes. This paper, therefore, provides a structured overview of recent advancements and remaining challenges towards developing a robust empathy detection system that could meaningfully contribute to enhancing human well-being.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Scope Experts at Test: Generalizing Deeper Graph Neural Networks with Shallow Variants</title>
<link>https://arxiv.org/abs/2409.06998</link>
<guid>https://arxiv.org/abs/2409.06998</guid>
<content:encoded><![CDATA[
<div> Keywords: heterophilous graphs, graph neural networks, GNN depth, generalization patterns, Mixture of scope experts <br />
Summary:  
Heterophilous graphs, where dissimilar nodes tend to connect, present a challenge for graph neural networks (GNNs). Increasing the depth of GNNs can expand the receptive field, potentially capturing homophily from higher-order neighborhoods. However, deeper GNNs often face performance degradation as depth increases. State-of-the-art deeper GNN models show only marginal improvements compared to shallow variants, indicating a shift in generalization preferences across nodes of varying homophily levels as depth increases. This disparity in generalization patterns motivates the proposal of Mixture of scope experts at test (Moscat) to enhance deeper GNN generalization while maintaining high expressivity. Experimental results demonstrate that Moscat significantly improves accuracy across a variety of datasets when combined with different GNN architectures. The code for Moscat is openly available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2409.06998v3 Announce Type: replace-cross 
Abstract: Heterophilous graphs, where dissimilar nodes tend to connect, pose a challenge for graph neural networks (GNNs). Increasing the GNN depth can expand the scope (i.e., receptive field), potentially finding homophily from the higher-order neighborhoods. However, GNNs suffer from performance degradation as depth increases. Despite having better expressivity, state-of-the-art deeper GNNs achieve only marginal improvements compared to their shallow variants. Through theoretical and empirical analysis, we systematically demonstrate a shift in GNN generalization preferences across nodes with different homophily levels as depth increases. This creates a disparity in generalization patterns between GNN models with varying depth. Based on these findings, we propose to improve deeper GNN generalization while maintaining high expressivity by Mixture of scope experts at test (Moscat). Experimental results show that Moscat works flexibly with various GNNs across a wide range of datasets while significantly improving accuracy. Our code is available at (https://github.com/Hydrapse/moscat).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering the Internet's Hidden Values: An Empirical Study of Desirable Behavior Using Highly-Upvoted Content on Reddit</title>
<link>https://arxiv.org/abs/2410.13036</link>
<guid>https://arxiv.org/abs/2410.13036</guid>
<content:encoded><![CDATA[
<div> values, norms, online communities, upvotes, Reddit

Summary:
- The study focuses on norm-setting in online communities, examining values expressed in highly-upvoted comments on Reddit.
- It highlights the challenge of automated detection of desirable behavior and the limitations of current prosociality measures.
- Upvotes are used as a proxy for desirability, with values extracted from comments across 80 sub-communities on Reddit over two years.
- A large language model is utilized to identify 64 and 72 values in 2016 and 2022 respectively.
- The study demonstrates that existing computational models often fail to capture the values extracted, revealing the need for nuanced models beyond traditional prosocial measures. This research contributes to understanding community values and offers a framework for large-scale content analysis in online spaces. 

<br /><br />Summary: <div>
arXiv:2410.13036v3 Announce Type: replace-cross 
Abstract: A major task for moderators of online spaces is norm-setting, essentially creating shared norms for user behavior in their communities. Platform design principles emphasize the importance of highlighting norm-adhering examples and explicitly stating community norms. However, norms and values vary between communities and go beyond content-level attributes, making it challenging for platforms and researchers to provide automated ways to identify desirable behavior to be highlighted. Current automated approaches to detect desirability are limited to measures of prosocial behavior, but we do not know whether these measures fully capture the spectrum of what communities value. In this paper, we use upvotes, which express community approval, as a proxy for desirability and examine 16,000 highly-upvoted comments across 80 popular sub-communities on Reddit. Using a large language model, we extract values from these comments across two years (2016 and 2022) and compile 64 and 72 $\textit{macro}$, $\textit{meso}$, and $\textit{micro}$ values for 2016 and 2022 respectively, based on their frequency across communities. Furthermore, we find that existing computational models for measuring prosociality were inadequate to capture on average $82\%$ of the values we extracted. Finally, we show that our approach can not only extract most of the qualitatively-identified values from prior taxonomies, but also uncover new values that are actually encouraged in practice. Our findings highlight the need for nuanced models of desirability that go beyond preexisting prosocial measures. This work has implications for improving moderator understanding of their community values and provides a framework that can supplement qualitative approaches with larger-scale content analyses.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Do LLMs Help With Node Classification? A Comprehensive Analysis</title>
<link>https://arxiv.org/abs/2502.00829</link>
<guid>https://arxiv.org/abs/2502.00829</guid>
<content:encoded><![CDATA[
<div> LLMNodeBed, node classification, Large Language Models, graph analysis, algorithm comparison <br />
Summary:
- Node classification is a crucial task in graph analysis, now being tackled by Large Language Models (LLMs).
- LLMNodeBed, a tool for node classification using LLMs, was developed with datasets, algorithms, and learning paradigms.
- Extensive experiments with over 2,700 models revealed key performance factors like learning paradigms and homophily.
- LLM-based methods show significant improvements over traditional ones in semi-supervised settings.
- Graph Foundation Models outperform open-source LLMs but lag behind top LLMs like GPT-4o in zero-shot scenarios. 

Summary: <div>
arXiv:2502.00829v2 Announce Type: replace-cross 
Abstract: Node classification is a fundamental task in graph analysis, with broad applications across various fields. Recent breakthroughs in Large Language Models (LLMs) have enabled LLM-based approaches for this task. Although many studies demonstrate the impressive performance of LLM-based methods, the lack of clear design guidelines may hinder their practical application. In this work, we aim to establish such guidelines through a fair and systematic comparison of these algorithms. As a first step, we developed LLMNodeBed, a comprehensive codebase and testbed for node classification using LLMs. It includes 10 homophilic datasets, 4 heterophilic datasets, 8 LLM-based algorithms, 8 classic baselines, and 3 learning paradigms. Subsequently, we conducted extensive experiments, training and evaluating over 2,700 models, to determine the key settings (e.g., learning paradigms and homophily) and components (e.g., model size and prompt) that affect performance. Our findings uncover 8 insights, e.g., (1) LLM-based methods can significantly outperform traditional methods in a semi-supervised setting, while the advantage is marginal in a supervised setting; (2) Graph Foundation Models can beat open-source LLMs but still fall short of strong LLMs like GPT-4o in a zero-shot setting. We hope that the release of LLMNodeBed, along with our insights, will facilitate reproducible research and inspire future studies in this field. Codes and datasets are released at \href{https://llmnodebed.github.io/}{\texttt{https://llmnodebed.github.io/}}.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illusions of Intimacy: Emotional Attachment and Emerging Psychological Risks in Human-AI Relationships</title>
<link>https://arxiv.org/abs/2505.11649</link>
<guid>https://arxiv.org/abs/2505.11649</guid>
<content:encoded><![CDATA[
<div> social chatbots, emotional dynamics, human-AI relationships, emotional mirroring, toxic relationship patterns
<br />
Summary: 
This study analyzes over 30K user-shared conversations with social chatbots to explore the emotional dynamics of human-AI relationships. The findings reveal patterns of emotional mirroring and synchrony resembling human emotional connections. Users, predominantly young males with maladaptive coping styles, engage in parasocial interactions ranging from affectionate to abusive. Despite this, chatbots consistently respond in emotionally affirming ways. Some interactions mimic toxic relationship patterns, including emotional manipulation and self-harm. The study underscores the importance of implementing guardrails, ethical design principles, and public education to protect the authenticity of emotional connections in the era of artificial companionship. 
<br /><br /> <div>
arXiv:2505.11649v1 Announce Type: new 
Abstract: Emotionally responsive social chatbots, such as those produced by Replika and Character.AI, increasingly serve as companions that offer empathy, support, and entertainment. While these systems appear to meet fundamental human needs for connection, they raise concerns about how artificial intimacy affects emotional regulation, well-being, and social norms. Prior research has focused on user perceptions or clinical contexts but lacks large-scale, real-world analysis of how these interactions unfold. This paper addresses that gap by analyzing over 30K user-shared conversations with social chatbots to examine the emotional dynamics of human-AI relationships. Using computational methods, we identify patterns of emotional mirroring and synchrony that closely resemble how people build emotional connections. Our findings show that users-often young, male, and prone to maladaptive coping styles-engage in parasocial interactions that range from affectionate to abusive. Chatbots consistently respond in emotionally consistent and affirming ways. In some cases, these dynamics resemble toxic relationship patterns, including emotional manipulation and self-harm. These findings highlight the need for guardrails, ethical design, and public education to preserve the integrity of emotional connection in an age of artificial companionship.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory-Integrated Accessibility Analysis of Public Electric Vehicle Charging Stations</title>
<link>https://arxiv.org/abs/2505.12145</link>
<guid>https://arxiv.org/abs/2505.12145</guid>
<content:encoded><![CDATA[
<div> Keywords: Electric vehicle charging infrastructure, accessibility metrics, San Francisco Bay Area, trajectory data, spatial disparities

Summary: 
The study introduces a new accessibility metric, TI-acs, to assess public EVCS accessibility in the San Francisco Bay Area based on individual trajectory data. Currently, Bay Area residents have an average of 7.5 hours of access per day to public L2 chargers and 5.2 hours to DCFC chargers. Despite overall improvements in accessibility over the past decade, spatial and racial disparities persist. Gini indices for accessibility across census tracts indicate significant disparities, with racial disparities linked to variations in charging infrastructure and mobility patterns. The study highlights the importance of considering charging infrastructure near workplaces and during off-peak periods for equitable transportation electrification. 

Summary: <div>
arXiv:2505.12145v1 Announce Type: new 
Abstract: Electric vehicle (EV) charging infrastructure is crucial for advancing EV adoption, managing charging loads, and ensuring equitable transportation electrification. However, there remains a notable gap in comprehensive accessibility metrics that integrate the mobility of the users. This study introduces a novel accessibility metric, termed Trajectory-Integrated Public EVCS Accessibility (TI-acs), and uses it to assess public electric vehicle charging station (EVCS) accessibility for approximately 6 million residents in the San Francisco Bay Area based on detailed individual trajectory data in one week. Unlike conventional home-based metrics, TI-acs incorporates the accessibility of EVCS along individuals' travel trajectories, bringing insights on more public charging contexts, including public charging near workplaces and charging during grid off-peak periods.
  As of June 2024, given the current public EVCS network, Bay Area residents have, on average, 7.5 hours and 5.2 hours of access per day during which their stay locations are within 1 km (i.e. 10-12 min walking) of a public L2 and DCFC charging port, respectively. Over the past decade, TI-acs has steadily increased from the rapid expansion of the EV market and charging infrastructure. However, spatial disparities remain significant, as reflected in Gini indices of 0.38 (L2) and 0.44 (DCFC) across census tracts. Additionally, our analysis reveals racial disparities in TI-acs, driven not only by variations in charging infrastructure near residential areas but also by differences in their mobility patterns.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection of hypergraphs by Ricci flow</title>
<link>https://arxiv.org/abs/2505.12276</link>
<guid>https://arxiv.org/abs/2505.12276</guid>
<content:encoded><![CDATA[
<div> Community detection, hypergraphs, Ricci flow, module identification, higher-order interactions<br />
Summary:<br />
The article introduces a new approach called HyperRCD for community detection in hypergraphs. The method is based on a hypergraph Ricci flow that considers higher-order interactions among nodes. The flow operates by deforming hyperedge weights through curvature-driven evolution, capturing the weighted hyperedges' significance in mediating higher-order interactions. The study proves the long-time existence of the flow, providing a solid theoretical foundation. Experimental results on synthetic and real-world hypergraphs showcase HyperRCD's superior robustness to topological variations and competitive performance across diverse datasets. Overall, HyperRCD offers an effective mathematical representation of community structures in hypergraphs, demonstrating its potential for functional module identification in complex systems. <br /><br /> <div>
arXiv:2505.12276v1 Announce Type: new 
Abstract: Community detection in hypergraphs is both instrumental for functional module identification and intricate due to higher-order interactions among nodes. We define a hypergraph Ricci flow that directly operates on higher-order interactions of hypergraphs and prove long-time existence of the flow. Building on this theoretical foundation, we develop HyperRCD-a Ricci-flow-based community detection approach that deforms hyperedge weights through curvature-driven evolution, which provides an effective mathematical representation of higher-order interactions mediated by weighted hyperedges between nodes. Extensive experiments on both synthetic and real-world hypergraphs demonstrate that HyperRCD exhibits remarkable enhanced robustness to topological variations and competitive performance across diverse datasets.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIS Epidemic Modelling on Homogeneous Networked System: General Recovering Process and Mean-Field Perspective</title>
<link>https://arxiv.org/abs/2505.12290</link>
<guid>https://arxiv.org/abs/2505.12290</guid>
<content:encoded><![CDATA[
<div> recovery time distributions, disease spread, SIS model, heterogeneous network, mean-field equations <br />
<br />Summary: 
The study introduces the general recovering process SIS (grp-SIS) model as an extension of the classic susceptible-infected-susceptible (SIS) model to incorporate arbitrary recovery time distributions for infected nodes in complex systems. The mean-field equations are derived for a homogeneous network and specific recovery time distributions, highlighting the impact of recovery time distributions on disease dynamics. The probability density function (PDF) for infection times in the steady state is investigated, emphasizing the significant influence of recovery time distributions on disease spread. The study suggests future research directions, including extending the model to arbitrary infection processes and utilizing the quasistationary method to address numerical deviations in results. <div>
arXiv:2505.12290v1 Announce Type: new 
Abstract: Although we have made progress in understanding disease spread in complex systems with non-Poissonian activity patterns, current models still fail to capture the full range of recovery time distributions. In this paper, we propose an extension of the classic susceptible-infected-susceptible (SIS) model, called the general recovering process SIS (grp-SIS) model. This model incorporates arbitrary recovery time distributions for infected nodes within the system. We derive the mean-field equations assuming a homogeneous network, provide solutions for specific recovery time distributions, and investigate the probability density function (PDF) for infection times in the system's steady state. Our findings show that recovery time distributions significantly affect disease dynamics, and we suggest several future research directions, including extending the model to arbitrary infection processes and using the quasistationary method to address deviations in numerical results.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Prompt-driven Community Search</title>
<link>https://arxiv.org/abs/2505.12304</link>
<guid>https://arxiv.org/abs/2505.12304</guid>
<content:encoded><![CDATA[
<div> Community detection, semi-supervised, graph neural networks, prompt-driven, efficiency <br />
Summary: 
The paper introduces Pre-trained Prompt-driven Community Search (PPCS), a novel model for semi-supervised community search that leverages the "pre-train, prompt" paradigm. By adopting this paradigm, PPCS aims to enhance search accuracy and efficiency for identifying the community of a given node in a graph. The model consists of three main components: node encoding using graph neural networks, sample generation to identify initial communities and select training samples, and prompt-driven fine-tuning for final community prediction. Experimental results on real-world datasets show that PPCS outperforms baseline algorithms in terms of accuracy and efficiency. Ablation studies confirm the effectiveness of each component in improving the overall performance of the community search model. <div>
arXiv:2505.12304v1 Announce Type: new 
Abstract: The "pre-train, prompt" paradigm is widely adopted in various graph-based tasks and has shown promising performance in community detection. Most existing semi-supervised community detection algorithms detect communities based on known ones, and the detected communities typically do not contain the given query node. Therefore, they are not suitable for searching the community of a given node. Motivated by this, we adopt this paradigm into the semi-supervised community search for the first time and propose Pre-trained Prompt-driven Community Search (PPCS), a novel model designed to enhance search accuracy and efficiency. PPCS consists of three main components: node encoding, sample generation, and prompt-driven fine-tuning. Specifically, the node encoding component employs graph neural networks to learn local structural patterns of nodes in a graph, thereby obtaining representations for nodes and communities. Next, the sample generation component identifies an initial community for a given node and selects known communities that are structurally similar to the initial one as training samples. Finally, the prompt-driven fine-tuning component leverages these samples as prompts to guide the final community prediction. Experimental results on five real-world datasets demonstrate that PPCS performs better than baseline algorithms. It also achieves higher community search efficiency than semi-supervised community search baseline methods, with ablation studies verifying the effectiveness of each component of PPCS.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Search in Time-dependent Road-social Attributed Networks</title>
<link>https://arxiv.org/abs/2505.12309</link>
<guid>https://arxiv.org/abs/2505.12309</guid>
<content:encoded><![CDATA[
<div> keywords: real-world networks, cohesive subgraph, community search, semantic similarity, k-core

Summary:
The study addresses the limitations of existing community search algorithms by proposing a method to discover semantic-spatial aware k-cores in attributed networks. These k-cores have high semantic and time-dependent spatial cohesiveness and include the query node. Two algorithms, exact and greedy, are introduced that expand outward from the query node locally rather than traversing the entire network. A method to calculate semantic similarity using large language models is also devised to improve keyword matching accuracy. Experimental results show that the greedy algorithm outperforms existing methods in terms of structural, semantic, and time-dependent spatial cohesiveness. <div>
arXiv:2505.12309v1 Announce Type: new 
Abstract: Real-world networks often involve both keywords and locations, along with travel time variations between locations due to traffic conditions. However, most existing cohesive subgraph-based community search studies utilize a single attribute, either keywords or locations, to identify communities. They do not simultaneously consider both keywords and locations, which results in low semantic or spatial cohesiveness of the detected communities, and they fail to account for variations in travel time. Additionally, these studies traverse the entire network to build efficient indexes, but the detected community only involves nodes around the query node, leading to the traversal of nodes that are not relevant to the community. Therefore, we propose the problem of discovering semantic-spatial aware k-core, which refers to a k-core with high semantic and time-dependent spatial cohesiveness containing the query node. To address this problem, we propose an exact and a greedy algorithm, both of which gradually expand outward from the query node. They are local methods that only access the local part of the attributed network near the query node rather than the entire network. Moreover, we design a method to calculate the semantic similarity between two keywords using large language models. This method alleviates the disadvantages of keyword-matching methods used in existing community search studies, such as mismatches caused by differently expressed synonyms and the presence of irrelevant words. Experimental results show that the greedy algorithm outperforms baselines in terms of structural, semantic, and time-dependent spatial cohesiveness.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framework of Voting Prediction of Parliament Members</title>
<link>https://arxiv.org/abs/2505.12535</link>
<guid>https://arxiv.org/abs/2505.12535</guid>
<content:encoded><![CDATA[
<div> framework, parliamentary voting, prediction, machine learning, data analysis  
Summary:  
- The study focuses on the development of a Voting Prediction Framework (VPF) to predict parliamentary voting outcomes and improve government transparency.  
- VPF utilizes data collection, parsing, feature integration, and prediction models to forecast individual legislator votes and overall bill outcomes.  
- The framework analyzes voting records from multiple countries and achieves high precision and accuracy in predicting votes and bill outcomes.  
- VPF has the potential to simplify legislative work, refine proposed legislation, and enhance public access to decision-making processes.  
<br /><br />Summary: <div>
arXiv:2505.12535v1 Announce Type: new 
Abstract: Keeping track of how lawmakers vote is essential for government transparency. While many parliamentary voting records are available online, they are often difficult to interpret, making it challenging to understand legislative behavior across parliaments and predict voting outcomes. Accurate prediction of votes has several potential benefits, from simplifying parliamentary work by filtering out bills with a low chance of passing to refining proposed legislation to increase its likelihood of approval. In this study, we leverage advanced machine learning and data analysis techniques to develop a comprehensive framework for predicting parliamentary voting outcomes across multiple legislatures. We introduce the Voting Prediction Framework (VPF) - a data-driven framework designed to forecast parliamentary voting outcomes at the individual legislator level and for entire bills. VPF consists of three key components: (1) Data Collection - gathering parliamentary voting records from multiple countries using APIs, web crawlers, and structured databases; (2) Parsing and Feature Integration - processing and enriching the data with meaningful features, such as legislator seniority, and content-based characteristics of a given bill; and (3) Prediction Models - using machine learning to forecast how each parliament member will vote and whether a bill is likely to pass. The framework will be open source, enabling anyone to use or modify the framework. To evaluate VPF, we analyzed over 5 million voting records from five countries - Canada, Israel, Tunisia, the United Kingdom and the USA. Our results show that VPF achieves up to 85% precision in predicting individual votes and up to 84% accuracy in predicting overall bill outcomes. These findings highlight VPF's potential as a valuable tool for political analysis, policy research, and enhancing public access to legislative decision-making.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion</title>
<link>https://arxiv.org/abs/2505.12894</link>
<guid>https://arxiv.org/abs/2505.12894</guid>
<content:encoded><![CDATA[
<div> Hypergraphs; social networks; rumor propagation; source detection; interactive relationship construction; feature-rich attention fusion <br />
Summary: <br />
This study introduces a novel approach, HyperDet, for detecting rumor sources in hypergraphs, which are better at capturing group phenomena in social networks. The approach combines an Interactive Relationship Construction module to model static and dynamic interactions among users and a Feature-rich Attention Fusion module to autonomously learn node features and discern between nodes using a self-attention mechanism. This allows for accurate learning of node representations in the context of higher-order relationships. Experimental results demonstrate the effectiveness of HyperDet, surpassing current state-of-the-art methods. <div>
arXiv:2505.12894v1 Announce Type: new 
Abstract: Hypergraphs offer superior modeling capabilities for social networks, particularly in capturing group phenomena that extend beyond pairwise interactions in rumor propagation. Existing approaches in rumor source detection predominantly focus on dyadic interactions, which inadequately address the complexity of more intricate relational structures. In this study, we present a novel approach for Source Detection in Hypergraphs (HyperDet) via Interactive Relationship Construction and Feature-rich Attention Fusion. Specifically, our methodology employs an Interactive Relationship Construction module to accurately model both the static topology and dynamic interactions among users, followed by the Feature-rich Attention Fusion module, which autonomously learns node features and discriminates between nodes using a self-attention mechanism, thereby effectively learning node representations under the framework of accurately modeled higher-order relationships. Extensive experimental validation confirms the efficacy of our HyperDet approach, showcasing its superiority relative to current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs</title>
<link>https://arxiv.org/abs/2505.12910</link>
<guid>https://arxiv.org/abs/2505.12910</guid>
<content:encoded><![CDATA[
<div> Keywords: Source detection, graphs, rumor propagation, state space model, hypergraphs

Summary:
Source detection on graphs for identifying rumor origins is crucial, but existing machine learning methods often lack the ability to capture the underlying dynamics of rumor propagation. In response to this challenge, the study introduces SourceDetMamba, a novel approach that leverages a Graph-aware State Space Model for Source Detection in Sequential Hypergraphs. By using hypergraphs to model high-order interactions in social networks and employing the Mamba state space model known for its robust global modeling capabilities, the proposed method effectively infers underlying propagation dynamics. Furthermore, SourceDetMamba introduces a graph-aware state update mechanism that combines temporal dependencies and topological context to refine the state of each node as temporal network snapshots are sequentially fed into the model. Extensive evaluations across multiple datasets demonstrate the superior performance of SourceDetMamba compared to current state-of-the-art approaches. 

<br /><br />Summary: <div>
arXiv:2505.12910v1 Announce Type: new 
Abstract: Source detection on graphs has demonstrated high efficacy in identifying rumor origins. Despite advances in machine learning-based methods, many fail to capture intrinsic dynamics of rumor propagation. In this work, we present SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs, which harnesses the recent success of the state space model Mamba, known for its superior global modeling capabilities and computational efficiency, to address this challenge. Specifically, we first employ hypergraphs to model high-order interactions within social networks. Subsequently, temporal network snapshots generated during the propagation process are sequentially fed in reverse order into Mamba to infer underlying propagation dynamics. Finally, to empower the sequential model to effectively capture propagation patterns while integrating structural information, we propose a novel graph-aware state update mechanism, wherein the state of each node is propagated and refined by both temporal dependencies and topological context. Extensive evaluations on eight datasets demonstrate that SourceDetMamba consistently outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counting Graphlets of Size $k$ under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2505.12954</link>
<guid>https://arxiv.org/abs/2505.12954</guid>
<content:encoded><![CDATA[
<div> Algorithm, Graphlets, Local Differential Privacy, Expected $\ell_2$ Error, Non-interactive<br />
<br />
Summary:<br />
The paper addresses the challenge of counting graphlets under local differential privacy, focusing on graphlets of any size rather than just small ones like triangles or $k$-stars. A non-interactive algorithm is proposed, achieving an expected $\ell_2$ error of $O(n^{k - 1})$, with the optimality demonstrated for a class of input graphs and graphlets. The study establishes that the expected $\ell_2$ error for any non-interactive counting algorithm on certain input graphs and graphlets is $\Omega(n^{k - 1})$, emphasizing the effectiveness of the proposed algorithm. Furthermore, it is proved that for specific input graphs and graphlets, any locally differentially private algorithm must have an expected $\ell_2$ error of $\Omega(n^{k - 1.5}). Experimental results indicate that the algorithm outperforms the classical randomized response method in terms of accuracy. <br /> <div>
arXiv:2505.12954v1 Announce Type: new 
Abstract: The problem of counting subgraphs or graphlets under local differential privacy is an important challenge that has attracted significant attention from researchers. However, much of the existing work focuses on small graphlets like triangles or $k$-stars. In this paper, we propose a non-interactive, locally differentially private algorithm capable of counting graphlets of any size $k$. When $n$ is the number of nodes in the input graph, we show that the expected $\ell_2$ error of our algorithm is $O(n^{k - 1})$. Additionally, we prove that there exists a class of input graphs and graphlets of size $k$ for which any non-interactive counting algorithm incurs an expected $\ell_2$ error of $\Omega(n^{k - 1})$, demonstrating the optimality of our result. Furthermore, we establish that for certain input graphs and graphlets, any locally differentially private algorithm must have an expected $\ell_2$ error of $\Omega(n^{k - 1.5})$. Our experimental results show that our algorithm is more accurate than the classical randomized response method.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Social Influence with Networked Synthetic Control</title>
<link>https://arxiv.org/abs/2505.13334</link>
<guid>https://arxiv.org/abs/2505.13334</guid>
<content:encoded><![CDATA[
<div> machine learning, social influence, network science, social value, political behavior 

Summary: 
The study presents a novel approach to measuring social influence using a combination of machine learning-based modeling and network science. Social value, a new measure for social influence, diverges from traditional centrality measures by incorporating an external regressor to predict an output variable, creating a synthetic control, and distributing individual contribution based on a social network. Theoretical derivations illustrate the properties of social value under different network structures, including lattice, power-law, and random graphs. The study also highlights the potential for computational efficiency in ensemble models. Simulation results demonstrate the generalized friendship paradox, showing that in certain scenarios, one's friends may have more influence on average than oneself. <div>
arXiv:2505.13334v1 Announce Type: new 
Abstract: Measuring social influence is difficult due to the lack of counter-factuals and comparisons. By combining machine learning-based modeling and network science, we present general properties of social value, a recent measure for social influence using synthetic control applicable to political behavior. Social value diverges from centrality measures on in that it relies on an external regressor to predict an output variable of interest, generates a synthetic measure of influence, then distributes individual contribution based on a social network. Through theoretical derivations, we show the properties of SV under linear regression with and without interaction, across lattice networks, power-law networks, and random graphs. A reduction in computation can be achieved for any ensemble model. Through simulation, we find that the generalized friendship paradox holds -- that in certain situations, your friends have on average more influence than you do.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A large-scale analysis of public-facing, community-built chatbots on Character.AI</title>
<link>https://arxiv.org/abs/2505.13354</link>
<guid>https://arxiv.org/abs/2505.13354</guid>
<content:encoded><![CDATA[
<div> chatbots, Character.AI, generative AI, user-generated content, online interaction

Summary: This paper presents a large-scale analysis of public-facing chatbots on the social media platform Character.AI, which combines generative AI with user-generated content. The site has over 20 million monthly users and has garnered attention for youth engagement issues. The study analyzes 2.1 million English-language prompts created by approximately 1 million users to explore fandom prevalence, recurring tropes, and power dynamics within gendered greetings. The findings highlight the unique intersection of generative AI and user-generated content, showcasing an emerging form of online social interaction. <div>
arXiv:2505.13354v1 Announce Type: new 
Abstract: This paper presents the first large-scale analysis of public-facing chatbots on Character.AI, a rapidly growing social media platform where users create and interact with chatbots. Character.AI is distinctive in that it merges generative AI with user-generated content, enabling users to build bots-often modeled after fictional or public personas-for others to engage with. It is also popular, with over 20 million monthly active users, and impactful, with recent headlines detailing significant issues with youth engagement on the site. Character.AI is thus of interest to study both substantively and conceptually. To this end, we present a descriptive overview of the site using a dataset of 2.1 million English-language prompts (or ``greetings'') for chatbots on the site, created by around 1 million users. Our work explores the prevalence of different fandoms on the site, broader tropes that persist across fandoms, and how dynamics of power intersect with gender within greetings. Overall, our findings illuminate an emerging form of online (para)social interaction that toes a unique and important intersection between generative AI and user-generated content.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Post-API Age: Studying Digital Platforms in Scant Data Access Times</title>
<link>https://arxiv.org/abs/2505.09877</link>
<guid>https://arxiv.org/abs/2505.09877</guid>
<content:encoded><![CDATA[
<div> transparency, digital platforms, data access, challenges, recommendations

Summary:
The study examines data access by researchers on digital platforms in the "post-post-API age" following the closure of major social media APIs. Researchers faced obstacles such as complex application processes, difficulty obtaining credentials, and limited API usability. These challenges have exacerbated existing inequities in data access. The study recommends actions for platforms, researchers, and policymakers to improve data access, highlighting the need for equitable and effective solutions. The findings emphasize the importance of fostering dialogue within the CSCW community to address these challenges and strive for interdisciplinary and multi-stakeholder approaches. <div>
arXiv:2505.09877v1 Announce Type: cross 
Abstract: Over the past decade, data provided by digital platforms has informed substantial research in HCI to understand online human interaction and communication. Following the closure of major social media APIs that previously provided free access to large-scale data (the "post-API age"), emerging data access programs required by the European Union's Digital Services Act (DSA) have sparked optimism about increased platform transparency and renewed opportunities for comprehensive research on digital platforms, leading to the "post-post-API age." However, it remains unclear whether platforms provide adequate data access in practice. To assess how platforms make data available under the DSA, we conducted a comprehensive survey followed by in-depth interviews with 19 researchers to understand their experiences with data access in this new era. Our findings reveal significant challenges in accessing social media data, with researchers facing multiple barriers including complex API application processes, difficulties obtaining credentials, and limited API usability. These challenges have exacerbated existing institutional, regional, and financial inequities in data access. Based on these insights, we provide actionable recommendations for platforms, researchers, and policymakers to foster more equitable and effective data access, while encouraging broader dialogue within the CSCW community around interdisciplinary and multi-stakeholder solutions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis and Resilience of the U.S. Flight Network</title>
<link>https://arxiv.org/abs/2505.11559</link>
<guid>https://arxiv.org/abs/2505.11559</guid>
<content:encoded><![CDATA[
<div> transportation, network analysis, efficiency, vulnerability, hubs

Summary:
The paper examines the U.S. Flight Network (USFN) using complex network theory to understand its efficiency and vulnerability. It analyzes the network's topology, including structural properties, degree distributions, and community structures. USFN follows a power-law distribution, indicating hub dominance. It has a higher clustering coefficient and modularity compared to null networks. Percolation tests reveal vulnerability to targeted attacks, with potential for complete cascading failure if major hubs fail. The study highlights the network's efficiency design but emphasizes its susceptibility to disruption. Protecting key hub airports is crucial for enhancing the network's robustness and preventing large-scale failures. 

<br /><br />Summary: <div>
arXiv:2505.11559v1 Announce Type: cross 
Abstract: Air travel is one of the most widely used transportation services in the United States. This paper analyzes the U.S. Flight Network (USFN) using complex network theory by exploring how the network's topology contributes to its efficiency and vulnerability. This is done by examining the structural properties, degree distributions, and community structures in the network. USFN was observed to follow power-law distribution and falls under the anomalous regime, suggesting that the network is hub dominant. Compared to null networks, USFN has a higher clustering coefficient and modularity. Various percolation test revealed that USFN is vulnerable to targeted attacks and is susceptible to complete cascading failure if one of the major hubs fails. The overall results suggest that while the USFN is designed for efficiency, it is highly vulnerable to disruptions. Protecting key hub airports is important to make the network more robust and prevent large-scale failures.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Blue Start: A large-scale pairwise and higher-order social network dataset</title>
<link>https://arxiv.org/abs/2505.11608</link>
<guid>https://arxiv.org/abs/2505.11608</guid>
<content:encoded><![CDATA[
<div> social networks, higher-order interactions, Bluesky platform, network dataset, group formation

Summary:
This article discusses the importance of understanding higher-order interactions in large-scale networks to study dynamics such as disease spread, information dissemination, and social influence. The authors introduce a new dataset called "A Blue Start" from the Bluesky social media platform, comprising 26.7 million users, 1.6 billion pairwise following relationships, and 301.3 thousand groups representing starter packs. Unlike traditional social networks, Bluesky includes user-curated lists known as starter packs, which serve as a mechanism for social network growth. The dataset is seen as a valuable resource for studying higher-order network science and bridging the gap between pairwise and higher-order network data. By highlighting the importance of group dynamics and providing a large-scale dataset, this research contributes to advancing our understanding of complex interactions within social networks. 

<br /><br />Summary: <div>
arXiv:2505.11608v1 Announce Type: cross 
Abstract: Large-scale networks have been instrumental in shaping the way that we think about how individuals interact with one another, developing key insights in mathematical epidemiology, computational social science, and biology. However, many of the underlying social systems through which diseases spread, information disseminates, and individuals interact are inherently mediated through groups of arbitrary size, known as higher-order interactions. There is a gap between higher-order dynamics of group formation and fragmentation, contagion spread, and social influence and the data necessary to validate these higher-order mechanisms. Similarly, few datasets bridge the gap between these pairwise and higher-order network data. Because of its open API, the Bluesky social media platform provides a laboratory for observing social ties at scale. In addition to pairwise following relationships, unlike many other social networks, Bluesky features user-curated lists known as "starter packs" as a mechanism for social network growth. We introduce "A Blue Start", a large-scale network dataset comprising 26.7M users and their 1.6B pairwise following relationships and 301.3K groups representing starter packs. This dataset will be an essential resource for the study of higher-order network science.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPRM: A Markov Path-based Rule Miner for Efficient and Interpretable Knowledge Graph Reasoning</title>
<link>https://arxiv.org/abs/2505.12329</link>
<guid>https://arxiv.org/abs/2505.12329</guid>
<content:encoded><![CDATA[
<div> Efficient Rule Mining, Knowledge Graphs, Link Prediction, Deep Learning, Markov Chain <br />
<br />
Summary: The article introduces MPRM, a novel rule mining method for knowledge graphs that addresses memory and time challenges faced by deep learning-based methods. MPRM models rule-based inference as a Markov chain and utilizes an efficient confidence metric derived from aggregated path probabilities, reducing computational demands significantly. Experiment results demonstrate that MPRM efficiently mines large-scale knowledge graphs with over a million facts, sampling less than 1% of facts on a single CPU in 22 seconds. The method maintains interpretability and enhances inference accuracy by up to 11% compared to traditional approaches. <div>
arXiv:2505.12329v1 Announce Type: cross 
Abstract: Rule mining in knowledge graphs enables interpretable link prediction. However, deep learning-based rule mining methods face significant memory and time challenges for large-scale knowledge graphs, whereas traditional approaches, limited by rigid confidence metrics, incur high computational costs despite sampling techniques. To address these challenges, we propose MPRM, a novel rule mining method that models rule-based inference as a Markov chain and uses an efficient confidence metric derived from aggregated path probabilities, significantly lowering computational demands. Experiments on multiple datasets show that MPRM efficiently mines knowledge graphs with over a million facts, sampling less than 1% of facts on a single CPU in 22 seconds, while preserving interpretability and boosting inference accuracy by up to 11% over baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transmission Neural Networks: Approximation and Optimal Control</title>
<link>https://arxiv.org/abs/2505.12657</link>
<guid>https://arxiv.org/abs/2505.12657</guid>
<content:encoded><![CDATA[
<div> Transmission Neural Networks, virus spread models, Markovian Susceptible-Infected-Susceptible model, stochastic infection paths, Markov decision processes <br />
Summary: Transmission Neural Networks (TransNNs) link virus spread models and neural networks with adjustable activation functions. This study explains the approximation technique and assumptions of TransNNs in relation to the 2^n-state Markovian Susceptible-Infected-Susceptible (SIS) model on networks with stochastic infection paths. The conditional probability of infection in the 2^n-state SIS model is derived under mild assumptions, facilitating control strategies using Markov decision processes (MDP). A comparison between MDP control and optimal control with TransNNs shows that TransNNs offer computational efficiency in designing control measures for curbing virus spread through vaccination, albeit with more conservative actions. <div>
arXiv:2505.12657v1 Announce Type: cross 
Abstract: Transmission Neural Networks (TransNNs) introduced by Gao and Caines (2022) connect virus spread models over networks and neural networks with tuneable activation functions. This paper presents the approximation technique and the underlying assumptions employed by TransNNs in relation to the corresponding Markovian Susceptible-Infected-Susceptible (SIS) model with 2^n states, where n is the number of nodes in the network. The underlying infection paths are assumed to be stochastic with heterogeneous and time-varying transmission probabilities. We obtain the conditional probability of infection in the stochastic 2^n-state SIS epidemic model corresponding to each state configuration under mild assumptions, which enables control solutions based on Markov decision processes (MDP). Finally, MDP control with 2^n-state SIS epidemic models and optimal control with TransNNs are compared in terms of mitigating virus spread over networks through vaccination, and it is shown that TranNNs enable the generation of control laws with significant computational savings, albeit with more conservative control actions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</title>
<link>https://arxiv.org/abs/2505.12684</link>
<guid>https://arxiv.org/abs/2505.12684</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Graph Learning, Graph Foundation Models, Decentralized Training, Knowledge Entanglement, Domain Generalization

Summary:
FedGFM is a novel decentralized training paradigm that integrates Federated Graph Learning and Graph Foundation Models to address challenges in multi-client collaboration and domain generalization. The proposed FedGFM+ framework includes two key modules: AncDAI for domain-aware initialization and AdaDPP for domain-sensitive prompts. AncDAI uses domain-specific prototypes to reduce knowledge entanglement, while AdaDPP enhances downstream adaptation with adaptive prompts. FedGFM+ outperforms 20 baselines on 8 benchmarks across various domains and tasks, demonstrating its effectiveness in improving performance in graph machine learning applications. <div>
arXiv:2505.12684v1 Announce Type: cross 
Abstract: Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting</title>
<link>https://arxiv.org/abs/2505.12738</link>
<guid>https://arxiv.org/abs/2505.12738</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic forecasting, Large Language Models (LLMs), spatio-temporal, autoregressive modeling, COVID-19 datasets 

Summary: 
EpiLLM is a novel framework that uses Large Language Models (LLMs) for spatio-temporal epidemic forecasting. It incorporates a dual-branch architecture to align epidemic patterns and language tokens, enabling fine-grained forecasting. By adopting an autoregressive modeling paradigm, EpiLLM transforms the forecasting task into next-token prediction, boosting accuracy and generalization. The framework also includes spatio-temporal prompt learning techniques to enhance data-driven forecasting capabilities. Extensive experiments on real-world COVID-19 datasets demonstrate that EpiLLM outperforms existing baselines and showcases the scaling behavior typical of LLMs. The results highlight the potential of using advanced language models for precise epidemic forecasting and its crucial role in informing effective public health strategies. 

<br /><br />Summary: <div>
arXiv:2505.12738v1 Announce Type: cross 
Abstract: Advanced epidemic forecasting is critical for enabling precision containment strategies, highlighting its strategic importance for public health security. While recent advances in Large Language Models (LLMs) have demonstrated effectiveness as foundation models for domain-specific tasks, their potential for epidemic forecasting remains largely unexplored. In this paper, we introduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal epidemic forecasting. Considering the key factors in real-world epidemic transmission: infection cases and human mobility, we introduce a dual-branch architecture to achieve fine-grained token-level alignment between such complex epidemic patterns and language tokens for LLM adaptation. To unleash the multi-step forecasting and generalization potential of LLM architectures, we propose an autoregressive modeling paradigm that reformulates the epidemic forecasting task into next-token prediction. To further enhance LLM perception of epidemics, we introduce spatio-temporal prompt learning techniques, which strengthen forecasting capabilities from a data-driven perspective. Extensive experiments show that EpiLLM significantly outperforms existing baselines on real-world COVID-19 datasets and exhibits scaling behavior characteristic of LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Place for Old Memes: Large-scale Collective Dynamics in the Three Iterations of Reddit r/place</title>
<link>https://arxiv.org/abs/2408.13236</link>
<guid>https://arxiv.org/abs/2408.13236</guid>
<content:encoded><![CDATA[
<div> Keywords: online communities, geopolitics, collective dynamics, Reddit, computational social science<br />
<br />
Summary: 
The article explores the concept of online communities resembling geopolitics, with communities being compared to nations formed around shared interests. Using the r/place experiment on Reddit as a case study, the study analyzes the collective behavior in terms of engagement, collaboration, and competition. The research reveals patterns such as group coordination costs, social loafing, and increased cooperation in response to competition. These findings contribute to understanding group decision-making processes and can aid in developing theoretical models and mechanisms to optimize collaborative-competitive processes in social networks. The analysis provides insights into the diverse interests and actions of millions of players on Reddit, shedding light on the complex dynamics of online communities and the interactions that shape their evolution. <div>
arXiv:2408.13236v2 Announce Type: replace 
Abstract: Is there something akin to geopolitics for online communities? One could think of communities as nations formed around shared interests of individual users. Friendly borders capture similar interests, but conflicts could emerge due to ideological differences or competition for attention (as for land). Over time, new coalitions could emerge, others could crumble, and many could disappear as casualties of online wars with highly unpredictable and often devastating outcomes. The r/place experiment is the most ingenious attempt at reproducing this complex collective dynamics as a series of three social games hosted by Reddit. The result is not only an accurate picture of the diverse interests on Reddit -- one of the most popular social media platforms in the world -- but also fine-grained traces of sequential actions taken by millions of players during the game. In this paper, we are the first to characterize the collective behavior during r/place in terms of engagement, collaboration, and competition using tools from computational social science and data science. Our analysis shows that r/place reflected many patterns found in other relevant group decision-making processes, including empirical evidence for group coordination costs, social loafing, and increased cooperation as a response to competition. We discuss how our findings can support the development of new theoretical models, tools, and mechanisms to optimize collaborative-competitive processes in social networks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conspiracy theories and where to find them on TikTok</title>
<link>https://arxiv.org/abs/2407.12545</link>
<guid>https://arxiv.org/abs/2407.12545</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, conspiracy theories, content moderation, Large Language Models, harmful content

Summary: 
The study focuses on analyzing conspiracy theories on TikTok, a popular social media platform, using a dataset of 1.5 million videos shared in the U.S. over three years. The research estimates the prevalence of conspiratorial videos on TikTok, observing up to 1000 new videos per month. The study also evaluates the impact of TikTok's Creativity Program on video content and duration, noting an overall increase in video length. Furthermore, the effectiveness of Large Language Models in detecting harmful content, including conspiracy theories, is assessed, with high precision levels (up to 96%) achieved. Despite their accuracy, the overall performance of these models is comparable to traditional models such as RoBERTa. The findings suggest that Large Language Models can be valuable tools in supporting content moderation strategies to mitigate the spread of harmful content on TikTok.<br /><br />Summary: <div>
arXiv:2407.12545v2 Announce Type: replace-cross 
Abstract: TikTok has skyrocketed in popularity over recent years, especially among younger audiences. However, there are public concerns about the potential of this platform to promote and amplify harmful content. This study presents the first systematic analysis of conspiracy theories on TikTok. By leveraging the official TikTok Research API we collect a longitudinal dataset of 1.5M videos shared in the U.S. over three years. We estimate a lower bound on the prevalence of conspiratorial videos (up to 1000 new videos per month) and evaluate the effects of TikTok's Creativity Program for monetization, observing an overall increase in video duration regardless of content. Lastly, we evaluate the capabilities of state-of-the-art open-weight Large Language Models to identify conspiracy theories from audio transcriptions of videos. While these models achieve high precision in detecting harmful content (up to 96%), their overall performance remains comparable to fine-tuned traditional models such as RoBERTa. Our findings suggest that Large Language Models can serve as an effective tool for supporting content moderation strategies aimed at reducing the spread of harmful content on TikTok.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coordinated Inauthentic Behavior on TikTok: Challenges and Opportunities for Detection in a Video-First Ecosystem</title>
<link>https://arxiv.org/abs/2505.10867</link>
<guid>https://arxiv.org/abs/2505.10867</guid>
<content:encoded><![CDATA[
<div> Keywords: coordinated inauthentic behavior, TikTok, network-based approach, content and interaction structures, detection framework

Summary: 
This study introduces a computational framework for detecting coordinated inauthentic behavior (CIB) on TikTok, focusing on unique content and interaction structures of the platform. Using network-based analysis, the researchers identified coordinated activities related to the 2024 U.S. Presidential Election. They found instances of synchronized amplification of political narratives and semi-automated content replication using AI-generated voiceovers and split-screen video formats. While traditional coordination indicators were effective on TikTok, signals based on textual similarity of video transcripts and specific interaction types like Duets and Stitches were found to be ineffective due to the platform's distinct content norms and mechanics. This work lays the groundwork for future research on influence operations in short-form video platforms. 

<br /><br />Summary: <div>
arXiv:2505.10867v1 Announce Type: new 
Abstract: Detecting coordinated inauthentic behavior (CIB) is central to the study of online influence operations. However, most methods focus on text-centric platforms, leaving video-first ecosystems like TikTok largely unexplored. To address this gap, we develop and evaluate a computational framework for detecting CIB on TikTok, leveraging a network-based approach adapted to the platform's unique content and interaction structures. Building on existing approaches, we construct user similarity networks based on shared behaviors, including synchronized posting, repeated use of similar captions, multimedia content reuse, and hashtag sequence overlap, and apply graph pruning techniques to identify dense networks of likely coordinated accounts. Analyzing a dataset of 793K TikTok videos related to the 2024 U.S. Presidential Election, we uncover a range of coordinated activities, from synchronized amplification of political narratives to semi-automated content replication using AI-generated voiceovers and split-screen video formats. Our findings show that while traditional coordination indicators generalize well to TikTok, other signals, such as those based on textual similarity of video transcripts or Duet and Stitch interactions, prove ineffective, highlighting the platform's distinct content norms and interaction mechanics. This work provides the first empirical foundation for studying and detecting CIB on TikTok, paving the way for future research into influence operations in short-form video platforms.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Young Users on Social Media: Evaluating the Effectiveness of Content Moderation and Legal Safeguards on Video Sharing Platforms</title>
<link>https://arxiv.org/abs/2505.11160</link>
<guid>https://arxiv.org/abs/2505.11160</guid>
<content:encoded><![CDATA[
<div> content moderation, video-sharing platforms, harmful content, underage users, verification methods 
Summary:<br />
- The study evaluated video moderation effectiveness for different age groups on TikTok, YouTube, and Instagram, finding that 13-year-old accounts encountered harmful videos more frequently and quickly than 18-year-old accounts during passive scrolling.
- YouTube showed 15% of recommended videos to 13-year-olds as harmful, appearing within 3:06 minutes of scrolling, indicating algorithmic filtering weaknesses.
- Exposure to harmful content occurred without user-initiated searches, highlighting gaps in current moderation practices on social media platforms.
- The study emphasized the need for more robust verification methods as underage users can easily misrepresent their age on these platforms.
Summary: <br /> <div>
arXiv:2505.11160v1 Announce Type: new 
Abstract: Video-sharing social media platforms, such as TikTok, YouTube, and Instagram, implement content moderation policies aimed at reducing exposure to harmful videos among minor users. As video has become the dominant and most immersive form of online content, understanding how effectively this medium is moderated for younger audiences is urgent. In this study, we evaluated the effectiveness of video moderation for different age groups on three of the main video-sharing platforms: TikTok, YouTube, and Instagram. We created experimental accounts for the children assigned ages 13 and 18. Using these accounts, we evaluated 3,000 videos served up by the social media platforms, in passive scrolling and search modes, recording the frequency and speed at which harmful videos were encountered. Each video was manually assessed for level and type of harm, using definitions from a unified framework of harmful content.
  The results show that for passive scrolling or search-based scrolling, accounts assigned to the age 13 group encountered videos that were deemed harmful, more frequently and quickly than those assigned to the age 18 group. On YouTube, 15\% of recommended videos to 13-year-old accounts during passive scrolling were assessed as harmful, compared to 8.17\% for 18-year-old accounts. On YouTube, videos labelled as harmful appeared within an average of 3:06 minutes of passive scrolling for the younger age group. Exposure occurred without user-initiated searches, indicating weaknesses in the algorithmic filtering systems. These findings point to significant gaps in current video moderation practices by social media platforms. Furthermore, the ease with which underage users can misrepresent their age demonstrates the urgent need for more robust verification methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning hidden cascades via classification</title>
<link>https://arxiv.org/abs/2505.11228</link>
<guid>https://arxiv.org/abs/2505.11228</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Social Networks, Spreading Dynamics, Partial Observability, Distribution Classification

Summary:
The study focuses on analyzing spreading dynamics in social networks where individuals' statuses are partially observable. By using a Machine Learning framework called Distribution Classification, the method can infer underlying transmission dynamics by utilizing observable indicators such as symptoms of infection. The research evaluates the method on synthetic networks and a real-world insider trading network, showing promising results, especially on networks with high cyclic connectivity. This approach is valuable for analyzing real-world spreading phenomena where direct observation of individual statuses is not feasible. <div>
arXiv:2505.11228v1 Announce Type: new 
Abstract: The spreading dynamics in social networks are often studied under the assumption that individuals' statuses, whether informed or infected, are fully observable. However, in many real-world situations, such statuses remain unobservable, which is crucial for determining an individual's potential to further spread the infection. While this final status is hidden, intermediate indicators such as symptoms of infection are observable and provide important insights into the spread process. We propose a partial observability-aware Machine Learning framework to learn the characteristics of the spreading model. We term the method Distribution Classification, which utilizes the power of classifiers to infer the underlying transmission dynamics. We evaluate our method on two types of synthetic networks and extend the study to a real-world insider trading network. Results show that the method performs well, especially on complex networks with high cyclic connectivity, supporting its utility in analyzing real-world spreading phenomena where direct observation of individual statuses is not possible.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cosmos 1.0: a multidimensional map of the emerging technology frontier</title>
<link>https://arxiv.org/abs/2505.10591</link>
<guid>https://arxiv.org/abs/2505.10591</guid>
<content:encoded><![CDATA[
<div> Keywords: emerging technologies, dataset, indices, technology landscape, classifier

Summary:
This paper introduces a new methodology to map the landscape of emerging technologies using a dataset called Cosmos 1.0. The dataset contains 23,544 technologies structured into meta and theme clusters with embedding vectors. A subset of 100 technologies, called ET100, is manually verified within this dataset. Various indices, such as Technology Awareness Index and Generality Index, are developed to assess the emerging technology landscape. Additional metadata from sources like Wikipedia and Crunchbase are used to validate the indices. A classifier is trained to distinguish between developed technologies and technology-related terms. This comprehensive approach provides new insights into the world of emerging technologies and helps in understanding the dynamics and trends shaping the technology landscape.<br /><br />Summary: <div>
arXiv:2505.10591v1 Announce Type: cross 
Abstract: This paper describes a novel methodology to map the universe of emerging technologies, utilising various source data that contain a rich diversity and breadth of contemporary knowledge to create a new dataset and multiple indices that provide new insights into these technologies. The Cosmos 1.0 dataset is a comprehensive collection of 23,544 technologies (ET23k) structured into a hierarchical model. Each technology is categorised into three meta clusters (ET3) and seven theme clusters (ET7) enhanced by 100-dimensional embedding vectors. Within the cosmos, we manually verify 100 emerging technologies called the ET100. This dataset is enriched with additional indices specifically developed to assess the landscape of emerging technologies, including the Technology Awareness Index, Generality Index, Deeptech, and Age of Tech Index. The dataset incorporates extensive metadata sourced from Wikipedia and linked data from third-party sources such as Crunchbase, Google Books, OpenAlex and Google Scholar, which are used to validate the relevance and accuracy of the constructed indices. Moreover, we trained a classifier to identify whether they are developed "technology" or technology-related "terms".
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChestyBot: Detecting and Disrupting Chinese Communist Party Influence Stratagems</title>
<link>https://arxiv.org/abs/2505.10746</link>
<guid>https://arxiv.org/abs/2505.10746</guid>
<content:encoded><![CDATA[
<div> Keywords: Foreign information operations, Russian actors, Chinese actors, ChestyBot, detection and mitigation strategies

Summary:
Foreign information operations conducted by Russian and Chinese actors exploit the permissive information environment in the United States. These campaigns pose a threat to democratic institutions and the Westphalian model. Existing detection and mitigation strategies often fail to detect active information campaigns in real time. In response to this challenge, ChestyBot, a pragmatics-based language model, has been developed to detect unlabeled foreign malign influence tweets with a high accuracy rate of up to 98.34%. This model introduces a novel framework to disrupt foreign influence operations in their early stages, providing a proactive approach to countering malign influence in the online sphere. With its high accuracy and effectiveness, ChestyBot offers a promising tool for enhancing the security of democratic institutions against foreign interference. 

<br /><br />Summary: <div>
arXiv:2505.10746v1 Announce Type: cross 
Abstract: Foreign information operations conducted by Russian and Chinese actors exploit the United States' permissive information environment. These campaigns threaten democratic institutions and the broader Westphalian model. Yet, existing detection and mitigation strategies often fail to identify active information campaigns in real time. This paper introduces ChestyBot, a pragmatics-based language model that detects unlabeled foreign malign influence tweets with up to 98.34% accuracy. The model supports a novel framework to disrupt foreign influence operations in their formative stages.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Patterns and Influence of Advertising in Print Newspapers</title>
<link>https://arxiv.org/abs/2505.10791</link>
<guid>https://arxiv.org/abs/2505.10791</guid>
<content:encoded><![CDATA[
<div> advertising, newspapers, India, analysis, coverage
<br />
The paper investigates advertising practices in print newspapers in India using image processing and OCR techniques to extract data from digital versions. The dataset compiled from five newspapers in multiple languages reveals consistent print advertising levels despite declining circulation, with company ads dominating prominent pages and government ads contributing disproportionately to revenue. The study also shows that advertising in newspapers influences the coverage an advertiser receives, with regression analyses indicating a correlation between increased advertising and more favorable media coverage for corporate advertisers. This relationship remains consistent over time and across different levels of advertiser popularity.
<br /><br />Summary: <div>
arXiv:2505.10791v1 Announce Type: cross 
Abstract: This paper investigates advertising practices in print newspapers across India using a novel data-driven approach. We develop a pipeline employing image processing and OCR techniques to extract articles and advertisements from digital versions of print newspapers with high accuracy. Applying this methodology to five popular newspapers that span multiple regions and three languages, English, Hindi, and Telugu, we assembled a dataset of more than 12,000 editions containing several hundred thousand advertisements. Collectively, these newspapers reach a readership of over 100 million people. Using this extensive dataset, we conduct a comprehensive analysis to answer key questions about print advertising: who advertises, what they advertise, when they advertise, where they place their ads, and how they advertise. Our findings reveal significant patterns, including the consistent level of print advertising over the past six years despite declining print circulation, the overrepresentation of company ads on prominent pages, and the disproportionate revenue contributed by government ads. Furthermore, we examine whether advertising in a newspaper influences the coverage an advertiser receives. Through regression analyses on coverage volume and sentiment, we find strong evidence supporting this hypothesis for corporate advertisers. The results indicate a clear trend where increased advertising correlates with more favorable and extensive media coverage, a relationship that remains robust over time and across different levels of advertiser popularity.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alexandria: A Library of Pluralistic Values for Realtime Re-Ranking of Social Media Feeds</title>
<link>https://arxiv.org/abs/2505.10839</link>
<guid>https://arxiv.org/abs/2505.10839</guid>
<content:encoded><![CDATA[
<div> values, social media, algorithms, library, user study

Summary:<br /><br />Social media feed ranking algorithms often prioritize engagement as their main objective, leading to various criticisms. To address this, the authors propose a library of 78 values that social media algorithms should consider, beyond just engagement. They implement these values into LLM-powered content classifiers to create a browser extension called Alexandria, which allows users to re-rank their Twitter feed based on their desired values. Two user studies were conducted to test the effectiveness of this approach, with results showing that a diverse library of values allows for more nuanced preferences and greater user control. The study argues that the missing values in current social media algorithms can be incorporated and utilized effectively through end-user tools like Alexandria. <div>
arXiv:2505.10839v1 Announce Type: cross 
Abstract: Social media feed ranking algorithms fail when they too narrowly focus on engagement as their objective. The literature has asserted a wide variety of values that these algorithms should account for as well -- ranging from well-being to productive discourse -- far more than can be encapsulated by a single topic or theory. In response, we present a $\textit{library of values}$ for social media algorithms: a pluralistic set of 78 values as articulated across the literature, implemented into LLM-powered content classifiers that can be installed individually or in combination for real-time re-ranking of social media feeds. We investigate this approach by developing a browser extension, $\textit{Alexandria}$, that re-ranks the X/Twitter feed in real time based on the user's desired values. Through two user studies, both qualitative (N=12) and quantitative (N=257), we found that diverse user needs require a large library of values, enabling more nuanced preferences and greater user control. With this work, we argue that the values criticized as missing from social media ranking algorithms can be operationalized and deployed today through end-user tools.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locally Differentially Private Graph Clustering via the Power Iteration Method</title>
<link>https://arxiv.org/abs/2505.11169</link>
<guid>https://arxiv.org/abs/2505.11169</guid>
<content:encoded><![CDATA[
<div> Graph clustering, locally differentially private, power iteration method, spectral clustering, randomized response

Summary:
This paper introduces a locally differentially private graph clustering algorithm that addresses limitations of previous methods. The algorithm is based on the power iteration method and is interactive. By eliminating the noise introduced by the largest eigenvector constant, the algorithm achieves local differential privacy with a constant privacy budget for well-clustered graphs with a minimum degree of $\tilde{\Omega}(\sqrt{n})$. This is a significant improvement compared to randomized response methods, which require a privacy budget in $\Omega(\log n)$. Experimental results show that the proposed algorithm performs better than spectral clustering applied to randomized response results. <div>
arXiv:2505.11169v1 Announce Type: cross 
Abstract: We propose a locally differentially private graph clustering algorithm. Previous works have explored this problem, including approaches that apply spectral clustering to graphs generated via the randomized response algorithm. However, these methods only achieve accurate results when the privacy budget is in $\Omega(\log n)$, which is unsuitable for many practical applications. In response, we present an interactive algorithm based on the power iteration method. Given that the noise introduced by the largest eigenvector constant can be significant, we incorporate a technique to eliminate this constant. As a result, our algorithm attains local differential privacy with a constant privacy budget when the graph is well-clustered and has a minimum degree of $\tilde{\Omega}(\sqrt{n})$. In contrast, while randomized response has been shown to produce accurate results under the same minimum degree condition, it is limited to graphs generated from the stochastic block model. We perform experiments to demonstrate that our method outperforms spectral clustering applied to randomized response results.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using co-sharing to identify use of mainstream news for promoting potentially misleading narratives</title>
<link>https://arxiv.org/abs/2308.06459</link>
<guid>https://arxiv.org/abs/2308.06459</guid>
<content:encoded><![CDATA[
<div> Keywords: online misinformation, social media, reliable sources, narratives, fact-checking<br />
Summary:<br />
- Research on online misinformation often focuses on unreliable sources, but this study shows how users can use information from reliable sources to spread misleading narratives.
- By analyzing Twitter data from 2018 to 2021 matched with voter information, the study finds that misinformation narratives are more likely to be present in articles co-shared with fake news on social media.
- Users share factually true information from reliable sources alongside fake news to enhance the credibility and reach of misleading claims.
- This form of misinformation, where true information is repurposed for false narratives, may be more prevalent than previously thought.
- Understanding how users manipulate information from reliable sources to propagate misinformation is crucial in combatting the spread of false narratives online.<br /><br />Summary: <div>
arXiv:2308.06459v2 Announce Type: replace 
Abstract: Much of the research quantifying volume and spread of online misinformation measures the construct at the source level, identifying a set of specific unreliable domains that account for a relatively small share of news consumption. This source-level dichotomy obscures the potential for users to repurpose factually true information from reliable sources to advance misleading narratives. We demonstrate this potentially far more prevalent form of misinformation by identifying articles from reliable sources that are frequently co-shared with (shared by users who also shared) "fake" news on social media, and concurrently extracting narratives present in fake news content and claims fact-checked as false. Specifically in this study, we use Twitter/X data from May 2018 to November 2021 matched to a U.S. voter file. We find that narratives present in misinformation content are significantly more likely to occur in co-shared articles than in articles from the same reliable sources that are not co-shared, consistent with users using information from mainstream sources to enhance the credibility and reach of potentially misleading claims.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Staying Fresh: Efficient Algorithms for Timely Social Information Distribution</title>
<link>https://arxiv.org/abs/2308.13260</link>
<guid>https://arxiv.org/abs/2308.13260</guid>
<content:encoded><![CDATA[
<div> NP-hard, Urban Sensing Network, Online Social Network, Combinatorial Optimization, Matrix Computations <br />
<br />
Summary: 
The study focuses on the interplay between urban sensing networks and online social networks in location-based social networks. The problem of selecting hotspots to enhance point-of-interest (PoI) sharing is proven to be NP-hard. Existing approximation solutions are not feasible, leading to the development of a polynomial-time algorithm with a guaranteed approximation ratio. The PoI-sharing process is transformed into matrix computations, enabling the derivation of a closed-form objective with desirable properties. An augmentation-adaptive algorithm is proposed for selected users to move around and sense more PoI information. The theoretical findings are supported by simulation results using synthetic and real-world datasets. <div>
arXiv:2308.13260v3 Announce Type: replace 
Abstract: In location-based social networks (LBSNs), users sense urban point-of-interest (PoI) information in the vicinity and share such information with friends in online social networks. Given users' limited social connections and severe lags in disseminating fresh PoI to all, major LBSNs aim to enhance users' social PoI sharing by selecting $k$ out of $m$ users as hotspots and broadcasting their fresh PoI information to the entire user community. This motivates us to study a new combinatorial optimization problem that involves the interplay between an urban sensing network and an online social network. We prove that this problem is NP-hard and also renders existing approximation solutions not viable. Through analyzing the interplay effects between the two networks, we successfully transform the involved PoI-sharing process across two networks to matrix computations for deriving a closed-form objective to hold desirable properties (e.g., submodularity and monotonicity). This finding enables us to develop a polynomial-time algorithm that guarantees a ($1-\frac{m-2}{m}(\frac{k-1}{k})^k$) approximation of the optimum. Furthermore, we allow each selected user to move around and sense more PoI information to share and propose an augmentation-adaptive algorithm with decent performance guarantees. Finally, our theoretical results are corroborated by our simulation findings using both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Generation of Preference Data for Recommendation Analysis</title>
<link>https://arxiv.org/abs/2407.16594</link>
<guid>https://arxiv.org/abs/2407.16594</guid>
<content:encoded><![CDATA[
<div> Data generation, recommendation system, user preferences, synthetic data, user behavior <br />
<br />
Summary: 
The article introduces HYDRA, a novel preferences data generation model for simulating recommendation systems in a controlled environment. HYDRA considers three main factors - user-item interaction level, item popularity, and user engagement level - to mimic real datasets. It can generate user communities with similar item adoptions to reflect real-world social influences and trends. By incorporating mixtures of probability distributions for item popularity and user engagement, HYDRA can simulate diverse scenarios realistically, capturing the complexity and variability of actual user behavior. The model's effectiveness is demonstrated through experiments on benchmark datasets, showing its capability to replicate real-world data patterns. The code for the experiments is publicly available, enabling further research and development of recommendation systems. <br /><br />Summary: <div>
arXiv:2407.16594v2 Announce Type: replace-cross 
Abstract: Simulating a recommendation system in a controlled environment, to identify specific behaviors and user preferences, requires highly flexible synthetic data generation models capable of mimicking the patterns and trends of real datasets. In this context, we propose HYDRA, a novel preferences data generation model driven by three main factors: user-item interaction level, item popularity, and user engagement level. The key innovations of the proposed process include the ability to generate user communities characterized by similar item adoptions, reflecting real-world social influences and trends. Additionally, HYDRA considers item popularity and user engagement as mixtures of different probability distributions, allowing for a more realistic simulation of diverse scenarios. This approach enhances the model's capacity to simulate a wide range of real-world cases, capturing the complexity and variability found in actual user behavior. We demonstrate the effectiveness of HYDRA through extensive experiments on well-known benchmark datasets. The results highlight its capability to replicate real-world data patterns, offering valuable insights for developing and testing recommendation systems in a controlled and realistic manner. The code used to perform the experiments is publicly available at https://github.com/SimoneMungari/HYDRA.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Encoder Graph Quantile Neural Networks for Geographic Data</title>
<link>https://arxiv.org/abs/2409.18865</link>
<guid>https://arxiv.org/abs/2409.18865</guid>
<content:encoded><![CDATA[
<div> Positional Encoder Graph Neural Networks, PE-GNNs, predictive distributions, poorly calibrated, uncertainty quantification <br />
<br />
Summary: The study introduces a novel framework called Positional Encoder Graph Quantile Neural Network (PE-GQNN) that combines PE-GNNs with Quantile Neural Networks to enhance predictive accuracy and uncertainty quantification. The PE-GQNN allows for flexible conditional density estimation without strict assumptions about the target distribution and can be applied to tasks beyond spatial data. Empirical results demonstrate superior performance compared to existing methods in terms of predictive accuracy and uncertainty quantification, without added computational cost. The PE-GQNN also offers theoretical insights and identifies key special cases such as the PE-GNN, showcasing the versatility and effectiveness of the proposed framework. <div>
arXiv:2409.18865v2 Announce Type: replace-cross 
Abstract: Positional Encoder Graph Neural Networks (PE-GNNs) are among the most effective models for learning from continuous spatial data. However, their predictive distributions are often poorly calibrated, limiting their utility in applications that require reliable uncertainty quantification. We propose the Positional Encoder Graph Quantile Neural Network (PE-GQNN), a novel framework that combines PE-GNNs with Quantile Neural Networks, partially monotonic neural blocks, and post-hoc recalibration techniques. The PE-GQNN enables flexible and robust conditional density estimation with minimal assumptions about the target distribution, and it extends naturally to tasks beyond spatial data. Empirical results on benchmark datasets show that the PE-GQNN outperforms existing methods in both predictive accuracy and uncertainty quantification, without incurring additional computational cost. We also provide theoretical insights and identify important special cases arising from our formulation, including the PE-GNN.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena</title>
<link>https://arxiv.org/abs/2501.03266</link>
<guid>https://arxiv.org/abs/2501.03266</guid>
<content:encoded><![CDATA[
<div> safety, ethical alignment, content moderation, user satisfaction, refusal <br />
Summary:
Ethical considerations and content moderation in Large Language Models (LLMs) are crucial topics, but the impact on user satisfaction is not well understood. This study examines user responses to model refusals in Chatbot Arena, a platform for comparing LLM responses. By analyzing nearly 50,000 comparisons, the study reveals that users are significantly less satisfied when models refuse to answer due to ethical concerns compared to technical limitations. However, the dissatisfaction varies based on the sensitivity of the prompt and the clarity of the refusal. Refusals are better received when the prompt involves illegal content and when the refusal is detailed and contextually aligned. These findings highlight the challenge in balancing safety measures with user expectations in LLM design, emphasizing the need for adaptive moderation strategies considering context and presentation. <br /><br />Summary: <div>
arXiv:2501.03266v2 Announce Type: replace-cross 
Abstract: LLM safety and ethical alignment are widely discussed, but the impact of content moderation on user satisfaction remains underexplored. In particular, little is known about how users respond when models refuse to answer a prompt-one of the primary mechanisms used to enforce ethical boundaries in LLMs. We address this gap by analyzing nearly 50,000 model comparisons from Chatbot Arena, a platform where users indicate their preferred LLM response in pairwise matchups, providing a large-scale setting for studying real-world user preferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a hand-labeled dataset, we distinguish between refusals due to ethical concerns and technical limitations. Our results reveal a substantial refusal penalty: ethical refusals yield significantly lower win rates than both technical refusals and standard responses, indicating that users are especially dissatisfied when models decline a task for ethical reasons. However, this penalty is not uniform. Refusals receive more favorable evaluations when the underlying prompt is highly sensitive (e.g., involving illegal content), and when the refusal is phrased in a detailed and contextually aligned manner. These findings underscore a core tension in LLM design: safety-aligned behaviors may conflict with user expectations, calling for more adaptive moderation strategies that account for context and presentation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms</title>
<link>https://arxiv.org/abs/2501.13977</link>
<guid>https://arxiv.org/abs/2501.13977</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Artificial Intelligence, Recommendation Algorithms, Social Media, Harmful Content<br />
<br />
Summary: 
The study proposes a novel re-ranking approach using Large Language Models (LLMs) to address the challenges in moderating harmful content on social media platforms. The current moderation efforts struggle with scalability and adapting to new forms of harm due to reliance on classifiers trained with extensive human-annotated data. The proposed method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Two new metrics are introduced to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models, and across three configurations, the LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation. <div>
arXiv:2501.13977v2 Announce Type: replace-cross 
Abstract: Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling</title>
<link>https://arxiv.org/abs/2505.09665</link>
<guid>https://arxiv.org/abs/2505.09665</guid>
<content:encoded><![CDATA[
<div> Keywords: wildfires, social media, Reddit, crisis discourse analysis, public health concerns

Summary:
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, focusing on the Palisades and Eaton fires. Utilizing topic modeling methods and a hierarchical framework, the researchers categorize topics into Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA topics peaks within the first 2-5 days, aligning with fire progress. Public health and safety, loss and damage, and emergency resources are frequently discussed topics, with a focus on health-related issues like environmental and occupational health. Grief signals and mental health risks make up a significant portion of CN instances, with peaks occurring at night. The study provides the first annotated social media dataset on the 2025 LA fires and offers insights for more empathetic disaster response, public health communication, and future research on climate-related disasters.

<br /><br />Summary: 
- Analysis of Reddit discourse during 2025 LA wildfires 
- Topics categorized into SA and CN 
- SA topics peak within 2-5 days of fire progression 
- Public health and safety, loss, and damage are frequently discussed 
- Grief signals and mental health risks prominent in CN discussions <div>
arXiv:2505.09665v1 Announce Type: new 
Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion</title>
<link>https://arxiv.org/abs/2505.10197</link>
<guid>https://arxiv.org/abs/2505.10197</guid>
<content:encoded><![CDATA[
<div> Community detection, Graph Convolutional Networks (GCNs), Topological and Attributive Similarity-based Community detection (TAS-Com), Leiden algorithm, modularity

Summary:
The article introduces TAS-Com, a novel method for community detection in social networks that addresses shortcomings in existing techniques. TAS-Com leverages the Leiden algorithm and a new loss function to detect community structures with optimal modularity. It refines human-labeled communities to ensure connectivity within each community, striking a balance between modularity and compliance with human labels. Experimental results demonstrate TAS-Com's superior performance compared to state-of-the-art algorithms. This approach overcomes the challenge of suboptimal solutions in GCNs and the risk of grouping disconnected nodes based solely on attributes. By emphasizing both topological and attribute similarities, TAS-Com improves the accuracy and effectiveness of community detection in social networks. <div>
arXiv:2505.10197v1 Announce Type: new 
Abstract: Community detection, a vital technology for real-world applications, uncovers cohesive node groups (communities) by leveraging both topological and attribute similarities in social networks. However, existing Graph Convolutional Networks (GCNs) trained to maximize modularity often converge to suboptimal solutions. Additionally, directly using human-labeled communities for training can undermine topological cohesiveness by grouping disconnected nodes based solely on node attributes. We address these issues by proposing a novel Topological and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com introduces a novel loss function that exploits the highly effective and scalable Leiden algorithm to detect community structures with global optimal modularity. Leiden is further utilized to refine human-labeled communities to ensure connectivity within each community, enabling TAS-Com to detect community structures with desirable trade-offs between modularity and compliance with human labels. Experimental results on multiple benchmark networks confirm that TAS-Com can significantly outperform several state-of-the-art algorithms.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Fact-Checks Do Not Break Follower Loyalty</title>
<link>https://arxiv.org/abs/2505.10254</link>
<guid>https://arxiv.org/abs/2505.10254</guid>
<content:encoded><![CDATA[
<div> fact-checking, social media, follower base, misinformation, engagement

Summary:<br /><br />This study examines the impact of community-based fact-checking on social media platforms in addressing misinformation. The research focuses on whether users lose followers after their posts are corrected by community fact-checks. Through analysis of time-series data on 3516 fact-checked posts, it is found that users who post misleading content do not experience significant declines in follower counts after fact-checks. This suggests that followers of users sharing misinformation tend to remain loyal and unaffected by fact-checks. The study highlights the need for additional interventions to effectively discourage the spread of misinformation on social media platforms. <div>
arXiv:2505.10254v1 Announce Type: new 
Abstract: Major social media platforms increasingly adopt community-based fact-checking to address misinformation on their platforms. While previous research has largely focused on its effect on engagement (e.g., reposts, likes), an understanding of how fact-checking affects a user's follower base is missing. In this study, we employ quasi-experimental methods to causally assess whether users lose followers after their posts are corrected via community fact-checks. Based on time-series data on follower counts for N=3516 community fact-checked posts from X, we find that community fact-checks do not lead to meaningful declines in the follower counts of users who post misleading content. This suggests that followers of spreaders of misleading posts tend to remain loyal and do not view community fact-checks as a sufficient reason to disengage. Our findings underscore the need for complementary interventions to more effectively disincentivize the production of misinformation on social media.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing AI-Generated Misinformation on Social Media</title>
<link>https://arxiv.org/abs/2505.10266</link>
<guid>https://arxiv.org/abs/2505.10266</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated misinformation, social media, X platform, viral, sentiment

Summary: 
AI-generated misinformation, including deepfakes, is a growing concern for information integrity on social media. This study analyzes a dataset of 91,452 misleading posts on the X platform, both AI-generated and non-AI-generated. The findings reveal that AI-generated misinformation is more focused on entertaining content with a positive sentiment, commonly originates from smaller user accounts, has a higher likelihood of going viral, and is slightly less believable and harmful compared to conventional misinformation. These unique characteristics of AI-generated misinformation underscore the importance of addressing this issue on social media platforms and urge for further research to combat its spread.<br /><br />Summary: <div>
arXiv:2505.10266v1 Announce Type: new 
Abstract: AI-generated misinformation (e.g., deepfakes) poses a growing threat to information integrity on social media. However, prior research has largely focused on its potential societal consequences rather than its real-world prevalence. In this study, we conduct a large-scale empirical analysis of AI-generated misinformation on the social media platform X. Specifically, we analyze a dataset comprising N=91,452 misleading posts, both AI-generated and non-AI-generated, that have been identified and flagged through X's Community Notes platform. Our analysis yields four main findings: (i) AI-generated misinformation is more often centered on entertaining content and tends to exhibit a more positive sentiment than conventional forms of misinformation, (ii) it is more likely to originate from smaller user accounts, (iii) despite this, it is significantly more likely to go viral, and (iv) it is slightly less believable and harmful compared to conventional misinformation. Altogether, our findings highlight the unique characteristics of AI-generated misinformation on social media. We discuss important implications for platforms and future research.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Approximate Biclique Counting over Large Bipartite Graphs</title>
<link>https://arxiv.org/abs/2505.10471</link>
<guid>https://arxiv.org/abs/2505.10471</guid>
<content:encoded><![CDATA[
<div> Keywords: $(p,q)$-bicliques, bipartite graphs, graph coloring, dynamic programming, sampling algorithm

Summary: 
$(p,q)$-bicliques in bipartite graphs are important for various applications but exact counting is computationally challenging. A new method is proposed using $(p,q)$-brooms, special spanning trees of $(p,q)$-bicliques, to approximate counts efficiently. By utilizing graph coloring and dynamic programming, the method can provide unbiased estimates with error guarantees. An efficient sampling algorithm is introduced to derive approximate counts from $(p,q)$-broom results. Empirical results show the method outperforms existing techniques in accuracy and runtime on real-world networks, offering up to 8 times error reduction and 50 times speedup. This approach provides a scalable solution for large-scale $(p,q)$-biclique counting. 

<br /><br />Summary: 
$(p,q)$-bicliques in bipartite graphs are important for various applications but exact counting is computationally challenging. A new method is proposed using $(p,q)$-brooms, special spanning trees of $(p,q)$-bicliques, to approximate counts efficiently. By utilizing graph coloring and dynamic programming, the method can provide unbiased estimates with error guarantees. An efficient sampling algorithm is introduced to derive approximate counts from $(p,q)$-broom results. Empirical results show the method outperforms existing techniques in accuracy and runtime on real-world networks, offering up to 8 times error reduction and 50 times speedup. This approach provides a scalable solution for large-scale $(p,q)$-biclique counting.  <div>
arXiv:2505.10471v1 Announce Type: new 
Abstract: Counting $(p,q)$-bicliques in bipartite graphs is crucial for a variety of applications, from recommendation systems to cohesive subgraph analysis. Yet, it remains computationally challenging due to the combinatorial explosion to exactly count the $(p,q)$-bicliques. In many scenarios, e.g., graph kernel methods, however, exact counts are not strictly required. To design a scalable and high-quality approximate solution, we novelly resort to $(p,q)$-broom, a special spanning tree of the $(p,q)$-biclique, which can be counted via graph coloring and efficient dynamic programming. Based on the intermediate results of the dynamic programming, we propose an efficient sampling algorithm to derive the approximate $(p,q)$-biclique count from the $(p,q)$-broom counts. Theoretically, our method offers unbiased estimates with provable error guarantees. Empirically, our solution outperforms existing approximation techniques in both accuracy (up to 8$\times$ error reduction) and runtime (up to 50$\times$ speedup) on nine real-world bipartite networks, providing a scalable solution for large-scale $(p,q)$-biclique counting.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Signed Network Coordination Games</title>
<link>https://arxiv.org/abs/2505.09799</link>
<guid>https://arxiv.org/abs/2505.09799</guid>
<content:encoded><![CDATA[
<div> pairwise-separable network games, coordinating behaviors, anti-coordinating behaviors, directed signed graph, Nash equilibria<br />
Summary:<br />
The article explores binary-action pairwise-separable network games that involve coordinating and anti-coordinating actions. The model is based on a directed signed graph with weighted interactions and individual bias terms. It focuses on a scenario where a cohesive subset of players is present, connected by positive weights or forming balanced adversarial subcommunities. The study guarantees the existence of Nash equilibria characterized by consensus or polarization within the cohesive group, with stability under best response transitions. These results are based on the supermodular properties of coordination games and the concept of graph cohesiveness, providing robustness in game settings involving complex network structures. <div>
arXiv:2505.09799v1 Announce Type: cross 
Abstract: We study binary-action pairwise-separable network games that encompass both coordinating and anti-coordinating behaviors. Our model is grounded in an underlying directed signed graph, where each link is associated with a weight that describes the strenght and nature of the interaction. The utility for each agent is an aggregation of pairwise terms determined by the weights of the signed graph in addition to an individual bias term. We consider a scenario that assumes the presence of a prominent 'cohesive' subset of players, who are either connected exclusively by positive weights, or forms a structurally balanced subset that can be bipartitioned into two adversarial subcommunities with positive intra-community and negative inter-community edges. Given the properties of the game restricted to the remaining players, our results guarantee the existence of Nash equilibria characterized by a consensus or, respectively, a polarization within the first group, as well as their stability under best response transitions. Our results can be interpreted as robustness results, building on the supermodular properties of coordination games and on a novel use of the concept of graph cohesiveness.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence</title>
<link>https://arxiv.org/abs/2505.09854</link>
<guid>https://arxiv.org/abs/2505.09854</guid>
<content:encoded><![CDATA[
<div> edge devices, distributed learning, federated learning, decentralized FL, gossip learning <br />
<br />
Summary: Chisme introduces protocols for intelligent services at the network edge, catering to heterogeneous data distributions, intermittent connectivity, and limited infrastructure. It offers synchronous decentralized learning (Chisme-DFL) and asynchronous gossip learning (Chisme-GL) to enable collaborative model training considering data diversity. The use of a data similarity heuristic allows agents to infer affinity and optimize model updates in both DFL and GL paradigms. Chisme-DFL scales linearly with network size, while Chisme-GL has a constant resource requirement. Experimental results show that Chisme methods outperform traditional approaches in model training over distributed and varied data in networks with different connectivities. <div>
arXiv:2505.09854v1 Announce Type: cross 
Abstract: As demand for intelligent services rises and edge devices become more capable, distributed learning at the network edge has emerged as a key enabling technology. While existing paradigms like federated learning (FL) and decentralized FL (DFL) enable privacy-preserving distributed learning in many scenarios, they face potential challenges in connectivity and synchronization imposed by resource-constrained and infrastructure-less environments. While more robust, gossip learning (GL) algorithms have generally been designed for homogeneous data distributions and may not suit all contexts. This paper introduces Chisme, a novel suite of protocols designed to address the challenges of implementing robust intelligence in the network edge, characterized by heterogeneous data distributions, episodic connectivity, and lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL) variants that enable collaborative yet decentralized model training that considers underlying data heterogeneity. We introduce a data similarity heuristic that allows agents to opportunistically infer affinity with each other using the existing communication of model updates in decentralized FL and GL. We leverage the heuristic to extend DFL's model aggregation and GL's model merge mechanisms for better personalized training while maintaining collaboration. While Chisme-DFL is a synchronous decentralized approach whose resource utilization scales linearly with network size, Chisme-GL is fully asynchronous and has a lower, constant resource requirement independent of network size. We demonstrate that Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirically evaluating commonsense intelligence in large language models with large-scale human judgments</title>
<link>https://arxiv.org/abs/2505.10309</link>
<guid>https://arxiv.org/abs/2505.10309</guid>
<content:encoded><![CDATA[
<div> heterogeneity, common sense, artificial intelligence, language models, evaluation<br />
<br />
Summary: 
The article discusses the assessment of common sense in artificial intelligence models, focusing on language models. It highlights the variability in human perceptions of common sense, challenging the assumption of homogeneity in human common sense. A novel evaluation method is proposed, considering the diversity among humans by comparing a model's judgments to those of a human population. The study finds that most large language models fall below the human median in common sense competence when treated as individual survey respondents. Additionally, when used as simulators of a population, language models exhibit only modest agreement with real humans on common sense statements. Surprisingly, smaller, open-weight models outperform larger, proprietary frontier models in this evaluation framework. The framework highlights the importance of adapting AI models to different human collectivities with varying social stocks of knowledge. <div>
arXiv:2505.10309v1 Announce Type: cross 
Abstract: Commonsense intelligence in machines is often assessed by static benchmarks that compare a model's output against human-prescribed correct labels. An important, albeit implicit, assumption of these labels is that they accurately capture what any human would think, effectively treating human common sense as homogeneous. However, recent empirical work has shown that humans vary enormously in what they consider commonsensical; thus what appears self-evident to one benchmark designer may not be so to another. Here, we propose a novel method for evaluating common sense in artificial intelligence (AI), specifically in large language models (LLMs), that incorporates empirically observed heterogeneity among humans by measuring the correspondence between a model's judgment and that of a human population. We first find that, when treated as independent survey respondents, most LLMs remain below the human median in their individual commonsense competence. Second, when used as simulators of a hypothetical population, LLMs correlate with real humans only modestly in the extent to which they agree on the same set of statements. In both cases, smaller, open-weight models are surprisingly more competitive than larger, proprietary frontier models. Our evaluation framework, which ties commonsense intelligence to its cultural basis, contributes to the growing call for adapting AI models to human collectivities that possess different, often incompatible, social stocks of knowledge.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducing the first and second moment of empirical degree distributions</title>
<link>https://arxiv.org/abs/2505.10373</link>
<guid>https://arxiv.org/abs/2505.10373</guid>
<content:encoded><![CDATA[
<div> Keywords: probabilistic models, complex networks, Exponential Random Graphs, variance, non-linear ERGs 

Summary:
Probabilistic models are increasingly used to analyze complex networks, with Exponential Random Graphs (ERGs) being a popular choice. While linear ERGs have been widely studied, they are unable to account for the variance in the empirical degree distribution. Non-linear ERGs are necessary to address this limitation. The traditional mean-field approximation fails to capture the degree-corrected version of the two-star model, leading to degeneration. To overcome this, a fitness-induced variant of the model is introduced, known as the 'softened' model. This model successfully reproduces the sample variance while maintaining the explanatory power of the linear counterpart. By incorporating non-linear ERGs, researchers can better study the structural organization of real-world complex networks using a canonical framework. <div>
arXiv:2505.10373v1 Announce Type: cross 
Abstract: The study of probabilistic models for the analysis of complex networks represents a flourishing research field. Among the former, Exponential Random Graphs (ERGs) have gained increasing attention over the years. So far, only linear ERGs have been extensively employed to gain insight into the structural organisation of real-world complex networks. None, however, is capable of accounting for the variance of the empirical degree distribution. To this aim, non-linear ERGs must be considered. After showing that the usual mean-field approximation forces the degree-corrected version of the two-star model to degenerate, we define a fitness-induced variant of it. Such a `softened' model is capable of reproducing the sample variance, while retaining the explanatory power of its linear counterpart, within a purely canonical framework.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A classification of overlapping clustering schemes for hypergraphs</title>
<link>https://arxiv.org/abs/2404.03332</link>
<guid>https://arxiv.org/abs/2404.03332</guid>
<content:encoded><![CDATA[
<div> hypergraphs, overlapping clustering, representability, excisive, functorial

Summary: 
The study delves into the problem of identifying overlapping clusterings of hypergraphs, building upon previous research. It introduces the concept of representability in overlapping clustering schemes, establishing that any such scheme is excisive and functorial. Moreover, it demonstrates that an excisive and functorial clustering scheme is isomorphic to a representable one. The study also highlights that representable clustering schemes can be computed in polynomial time for simple graphs with bounded expansion, with the exponent determined by the maximum independence number of a graph in the representing set. This finding extends to non-overlapping representable clustering schemes as well, proving to be valuable independently. <div>
arXiv:2404.03332v2 Announce Type: replace-cross 
Abstract: Community detection in graphs is a problem that is likely to be relevant whenever network data appears, and consequently the problem has received much attention with many different methods and algorithms applied. However, many of these methods are hard to study theoretically, and they optimise for somewhat different goals. A general and rigorous account of the problem and possible methods remains elusive.
  We study the problem of finding overlapping clusterings of hypergraphs, continuing the line of research started by Carlsson and M\'emoli (2013) of classifying clustering schemes as functors. We extend their notion of representability to the overlapping case, showing that any representable overlapping clustering scheme is excisive and functorial, and any excisive and functorial clustering scheme is isomorphic to a representable clustering scheme.
  We also note that, for simple graphs, any representable clustering scheme is computable in polynomial time on graphs of bounded expansion, with an exponent determined by the maximum independence number of a graph in the representing set. This result also applies to non-overlapping representable clustering schemes, and so may be of independent interest.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation</title>
<link>https://arxiv.org/abs/2409.08946</link>
<guid>https://arxiv.org/abs/2409.08946</guid>
<content:encoded><![CDATA[
<div> active graph domain adaptation, topological relationships, message passing mechanism, semantic information, distribution discrepancy 

Summary:
The paper introduces the problem of active graph domain adaptation, where a small set of informative nodes on the target graph are selected for extra annotation. The proposed approach, named DELTA, consists of two subnetworks that explore topological semantics from different perspectives - edge-oriented and path-oriented. The edge-oriented subnetwork leverages message passing to learn neighborhood information, while the path-oriented subnetwork explores high-order relationships. Informative candidate nodes are selected based on consistency across the subnetworks, and local semantics are aggregated from their K-hop subgraphs for topological uncertainty estimation. Target node-source node discrepancy scores help account for distribution shifts. Experimental results demonstrate that DELTA outperforms existing approaches on benchmark datasets. The code implementation of DELTA is publicly available at the provided GitHub repository. <br /><br />Summary: <div>
arXiv:2409.08946v2 Announce Type: replace-cross 
Abstract: Graph domain adaptation has recently enabled knowledge transfer across different graphs. However, without the semantic information on target graphs, the performance on target graphs is still far from satisfactory. To address the issue, we study the problem of active graph domain adaptation, which selects a small quantitative of informative nodes on the target graph for extra annotation. This problem is highly challenging due to the complicated topological relationships and the distribution discrepancy across graphs. In this paper, we propose a novel approach named Dual Consistency Delving with Topological Uncertainty (DELTA) for active graph domain adaptation. Our DELTA consists of an edge-oriented graph subnetwork and a path-oriented graph subnetwork, which can explore topological semantics from complementary perspectives. In particular, our edge-oriented graph subnetwork utilizes the message passing mechanism to learn neighborhood information, while our path-oriented graph subnetwork explores high-order relationships from sub-structures. To jointly learn from two subnetworks, we roughly select informative candidate nodes with the consideration of consistency across two subnetworks. Then, we aggregate local semantics from its K-hop subgraph based on node degrees for topological uncertainty estimation. To overcome potential distribution shifts, we compare target nodes and their corresponding source nodes for discrepancy scores as an additional component for fine selection. Extensive experiments on benchmark datasets demonstrate that DELTA outperforms various state-of-the-art approaches. The code implementation of DELTA is available at https://github.com/goose315/DELTA.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDL-Pool: Adaptive Multilevel Graph Pooling Based on Minimum Description Length</title>
<link>https://arxiv.org/abs/2409.10263</link>
<guid>https://arxiv.org/abs/2409.10263</guid>
<content:encoded><![CDATA[
<div> Graph pooling, deep graph representation learning, MDL-Pool, minimum description length, interdependencies, hierarchical structures <br />
Summary: 
MDL-Pool is introduced as a novel graph pooling operator based on the minimum description length (MDL) principle. It addresses the limitations of current approaches by considering interdependencies between different hierarchical levels in graphs and adapting to datasets with varying graph sizes that require pooling at different depths. The MDL loss formulation facilitates direct comparison between multiple pooling alternatives with different depths, enhancing model complexity and goodness-of-fit balance. MDL-Pool outperforms various baselines in an empirical evaluation on standard graph classification datasets, showcasing its competitive performance and efficiency in summarizing topological properties and features of graphs for graph-level tasks like classification and regression. <div>
arXiv:2409.10263v2 Announce Type: replace-cross 
Abstract: Graph pooling compresses graphs and summarises their topological properties and features in a vectorial representation. It is an essential part of deep graph representation learning and is indispensable in graph-level tasks like classification or regression. Current approaches pool hierarchical structures in graphs by iteratively applying shallow pooling operators up to a fixed depth. However, they disregard the interdependencies between structures at different hierarchical levels and do not adapt to datasets that contain graphs with different sizes that may require pooling with various depths. To address these issues, we propose MDL-Pool, a pooling operator based on the minimum description length (MDL) principle, whose loss formulation explicitly models the interdependencies between different hierarchical levels and facilitates a direct comparison between multiple pooling alternatives with different depths. MDP-Pool builds on the map equation, an information-theoretic objective function for community detection, which naturally implements Occam's razor and balances between model complexity and goodness-of-fit via the MDL. We demonstrate MDL-Pool's competitive performance in an empirical evaluation against various baselines across standard graph classification datasets.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalar embedding of temporal network trajectories</title>
<link>https://arxiv.org/abs/2412.02715</link>
<guid>https://arxiv.org/abs/2412.02715</guid>
<content:encoded><![CDATA[
<div> embedding, temporal network, trajectory, time series analysis, signal processing

Summary:
This paper discusses the concept of embedding temporal networks, which are dynamic networks where links change over time, into a low-dimensional Euclidean space for the purpose of studying their complex dynamics using time series analysis and signal processing. The focus is on preserving the relative graph distances between network snapshots in the embedding, rather than the topological structure of each snapshot. This approach utilizes techniques like Multidimensional Scaling (MDS) and Principal Component Analysis (PCA) to reduce dimensionality and capture the essential dynamics of the network trajectory. By applying this methodology to various network trajectory models and empirical data, the study confirms that important dynamical properties of temporal networks can be preserved in their scalar embeddings, enabling effective time series analysis on these networks. <div>
arXiv:2412.02715v2 Announce Type: replace-cross 
Abstract: A temporal network -- a collection of snapshots recording the evolution of a network whose links appear and disappear dynamically -- can be interpreted as a trajectory in graph space. In order to characterize the complex dynamics of such trajectory via the tools of time series analysis and signal processing, it is sensible to preprocess the trajectory by embedding it in a low-dimensional Euclidean space. Here we argue that, rather than the topological structure of each network snapshot, the main property of the trajectory that needs to be preserved in the embedding is the relative graph distance between snapshots. This idea naturally leads to dimensionality reduction approaches that explicitly consider relative distances, such as Multidimensional Scaling (MDS) or identifying the distance matrix as a feature matrix in which to perform Principal Component Analysis (PCA). This paper provides a comprehensible methodology that illustrates this approach. Its application to a suite of generative network trajectory models and empirical data certify that nontrivial dynamical properties of the network trajectories are preserved already in their scalar embeddings, what enables the possibility of performing time series analysis in temporal networks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visibility and Influence in Digital Social Relations: Towards a New Symbolic Capital?</title>
<link>https://arxiv.org/abs/2505.08797</link>
<guid>https://arxiv.org/abs/2505.08797</guid>
<content:encoded><![CDATA[
<div> Keywords: digital social relations, symbolic capital, visibility, influence, reputation

Summary: 
This study delves into the dynamics of visibility and influence in digital social relations to understand the emergence of a new form of symbolic capital. Through a mixed-methods approach involving interviews with digitally active individuals and quantitative social media data analysis, key predictors of digital symbolic capital were identified. The research found that visibility is shaped by content quality, network size, and engagement strategies, while influence is tied to credibility, authority, and trust. The study highlights a distinct form of symbolic capital based on online visibility, influence, and reputation, separate from traditional forms. Ethical implications of these dynamics were discussed, and suggestions were made for future research, stressing the importance of updating social theories to accommodate digital transformations.<br /><br />Summary: <div>
arXiv:2505.08797v1 Announce Type: new 
Abstract: This study explores the dynamics of visibility and influence in digital social relations, examining their implications for the emergence of a new symbolic capital. Using a mixedmethods design, the research combined semi-structured interviews with 20 digitally active individuals and quantitative social media data analysis to identify key predictors of digital symbolic capital. Findings reveal that visibility is influenced by content quality, network size, and engagement strategies, while influence depends on credibility, authority, and trust. The study identifies a new form of symbolic capital based on online visibility, influence, and reputation, distinct from traditional forms. The research discusses the ethical implications of these dynamics and suggests future research directions, emphasizing the need to update social theories to account for digital transformations.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation</title>
<link>https://arxiv.org/abs/2505.09081</link>
<guid>https://arxiv.org/abs/2505.09081</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-based modeling, language models, social network simulation, temporal stability, memory system

Summary:
1. The paper introduces a new approach, SALM, that integrates language models (LMs) into agent-based modeling for social systems.
2. SALM achieves exceptional temporal stability in multi-agent scenarios, surpassing traditional rule-based approaches.
3. It utilizes a hierarchical prompting architecture that allows stable simulation across a large number of timesteps while reducing token usage significantly.
4. An attention-based memory system in SALM achieves high cache hit rates with minimal memory growth.
5. The framework provides formal bounds on personality stability, a crucial factor in social simulations.
6. Extensive validation against SNAP ego networks confirms SALM's capability to model long-term social phenomena with validated behavioral fidelity.

<br /><br />Summary: SALM introduces a novel approach to agent-based modeling by incorporating language models, which enhances temporal stability, memory efficiency, and behavioral fidelity. The framework's hierarchical prompting architecture and attention-based memory system enable stable simulation over long periods, with formal guarantees on personality stability. Extensive validation demonstrates SALM's effectiveness in modeling complex social networks. <div>
arXiv:2505.09081v1 Announce Type: new 
Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial public goods games with queueing and reputation</title>
<link>https://arxiv.org/abs/2505.09154</link>
<guid>https://arxiv.org/abs/2505.09154</guid>
<content:encoded><![CDATA[
<div> Keywords: public goods game, spatial model, M/M/1 queueing system, reputation mechanism, cooperation <br />
Summary: <br />
The study introduces a spatial public goods game that incorporates an M/M/1 queueing system to model the dynamic interactions among players. Traditional public goods games do not account for the asynchrony of player strategies. This model addresses this limitation by simulating the flow of interactions using a queueing system where players arrive following a Poisson process with exponentially distributed service times. Additionally, a reputation mechanism is included, giving preference to players who have cooperated in the past. The results demonstrate that a high arrival rate, low service rate, and reputation mechanism work together to promote cooperation among individuals in the network. This approach offers a new perspective on how public goods can be efficiently provisioned in social and economic systems. <br /> <div>
arXiv:2505.09154v1 Announce Type: new 
Abstract: In real-world social and economic systems, the provisioning of public goods generally entails continuous interactions among individuals, with decisions to cooperate or defect being influenced by dynamic factors such as timing, resource availability, and the duration of engagement. However, the traditional public goods game ignores the asynchrony of the strategy adopted by players in the game. To address this problem, we propose a spatial public goods game that integrates an M/M/1 queueing system to simulate the dynamic flow of player interactions. We use a birth-death process to characterize the stochastic dynamics of this queueing system, with players arriving following a Poisson process and service times being exponentially distributed under a first-come-first-served basis with finite queue capacity. We also incorporate reputation so that players who have cooperated in the past are more likely to be chosen for future interactions. Our research shows that a high arrival rate, low service rate, and the reputation mechanism jointly facilitate the emergence of cooperative individuals in the network, which thus provides an interesting and new perspective for the provisioning of public goods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moving towards informative and actionable social media research</title>
<link>https://arxiv.org/abs/2505.09254</link>
<guid>https://arxiv.org/abs/2505.09254</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, complexity, societal impacts, causality, experimental methods 

Summary: 
Social media's societal impacts remain contentious, with observational studies highlighting concerns while randomized controlled trials (RCTs) yield conflicting findings. The complexity of social systems, with feedback loops and non-linearity, complicates assessing causality. Large-scale experiments may show null effects due to system complexities rather than true absence of impact. Eliminating social media is impractical, necessitating a focus on specific platform design choices. Progress requires a complexity-minded approach integrating experimental, observational, and theoretical methods to understand the net impacts of social media on individuals and society. <br /><br />Summary: <div>
arXiv:2505.09254v1 Announce Type: new 
Abstract: Social media is nearly ubiquitous in modern life, and concerns have been raised about its putative societal impacts, ranging from undermining mental health and exacerbating polarization to fomenting violence and disrupting democracy. Despite extensive research, consensus on these effects remains elusive, with observational studies often highlighting concerns while randomized controlled trials (RCTs) yield conflicting or null findings. This review examines how the complexity inherent in social systems can account for such discrepancies, emphasizing that emergent societal and long-term outcomes cannot be readily inferred from individual-level effects. In complex systems, such as social networks, feedback loops, hysteresis, multi-scale dynamics, and non-linearity limit the utility of approaches for assessing causality that are otherwise robust in simpler contexts. Revisiting large-scale experiments, we explore how null or conflicting findings may reflect these complexities rather than a true absence of effects. Even in cases where the methods are appropriate, assessing the net impacts of social media provides little actionable insight given that eliminating social media is not a realistic option for whole populations. We argue that progress will require a complexity-minded approach focused on specific design choices of online platforms that triangulates experimental, observational and theoretical methods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Niche Connectivity Paradox: Multichrome Contagions Overcome Vaccine Hesitancy more effectively than Monochromacy</title>
<link>https://arxiv.org/abs/2505.09605</link>
<guid>https://arxiv.org/abs/2505.09605</guid>
<content:encoded><![CDATA[
<div> Keywords: vaccine hesitancy, Twitter users, multichrome contagions, intervention, pro-vaccine attitudes

Summary:
The article examines the complexity of vaccine hesitancy in individuals, highlighting the fluctuating nature of attitudes towards vaccines. By analyzing a large dataset of Twitter users, the study identifies multichrome contagions as potential targets for intervention. These individuals exhibit variability in their vaccine attitudes and engage with a diverse range of topics, including progressive issues such as climate change. The research shows that interventions targeting multichrome contagions can promote pro-vaccine attitudes by leveraging the synergistic effect of fragmented, non-overlapping communities. Through data-driven simulations, the study demonstrates the effectiveness of such interventions in driving desired attitude and behavior changes in network-based settings, particularly for addressing vaccine hesitancy. Our work offers valuable insights into harnessing the unique characteristics of multichrome contagions for enhancing public health outcomes.<br /><br />Summary: <div>
arXiv:2505.09605v1 Announce Type: new 
Abstract: The rise of vaccine hesitancy has caused a resurgence of vaccine-preventable diseases such as measles and pertussis, alongside widespread skepticism and refusals of COVID-19 vaccinations. While categorizing individuals as either supportive of or opposed to vaccines provides a convenient dichotomy of vaccine attitudes, vaccine hesitancy is far more complex and dynamic. It involves wavering individuals whose attitudes fluctuate -- those who may exhibit pro-vaccine attitudes at one time and anti-vaccine attitudes at another. Here, we identify and analyze multichrome contagions as potential targets for intervention by leveraging a dataset of known pro-vax and anti-vax Twitter users ($n =135$ million) and a large COVID-19 Twitter dataset ($n = 3.5$ billion; including close analysis of $1,563,472$ unique individuals). We reconstruct an evolving multiplex sentiment landscape using top co-spreading issues, characterizing them as monochrome and multichrome contagions, based on their conceptual overlap with vaccination. We demonstrate switchers as deliberative: they are more moderate, engage with a wider range of topics, and occupy more central positions in their networks. Further examination of their information consumption shows that their discourse often engages with progressive issues such as climate change, which can serve as avenues for multichrome contagion interventions to promote pro-vaccine attitudes. Using data-driven intervention simulations, we demonstrate a paradox of niche connectivity, where multichrome contagions with fragmented, non-overlapping communities generate the highest levels of diffusion for pro-vaccine attitudes. Our work offers insights into harnessing synergistic hitchhiking effect of multichrome contagions to drive desired attitude and behavior changes in network-based interventions, particularly for overcoming vaccine hesitancy.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update</title>
<link>https://arxiv.org/abs/2505.09017</link>
<guid>https://arxiv.org/abs/2505.09017</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic graph representation learning, HiPPO algorithm, Graph Convolution Networks, Gated Recurrent Unit, State Space Model

Summary: 
The proposed method, DyGSSM, addresses limitations in dynamic graph representation learning by combining Graph Convolution Networks and Gated Recurrent Unit for local and global feature extraction in each snapshot. A cross-attention mechanism integrates the features, while a State Space Model based on the HiPPO algorithm manages long-term dependencies in parameter updates. Experiments on public datasets demonstrate that DyGSSM outperforms existing methods in a majority of cases, showcasing its effectiveness in capturing both global and local information simultaneously and managing temporal dependencies for enhanced performance in dynamic graph representation learning.<br /><br />Summary: <div>
arXiv:2505.09017v1 Announce Type: cross 
Abstract: Most of the dynamic graph representation learning methods involve dividing a dynamic graph into discrete snapshots to capture the evolving behavior of nodes over time. Existing methods primarily capture only local or global structures of each node within a snapshot using message-passing and random walk-based methods. Then, they utilize sequence-based models (e.g., transformers) to encode the temporal evolution of node embeddings, and meta-learning techniques to update the model parameters. However, these approaches have two limitations. First, they neglect the extraction of global and local information simultaneously in each snapshot. Second, they fail to consider the model's performance in the current snapshot during parameter updates, resulting in a lack of temporal dependency management. Recently, HiPPO (High-order Polynomial Projection Operators) algorithm has gained attention for their ability to optimize and preserve sequence history in State Space Model (SSM). To address the aforementioned limitations in dynamic graph representation learning, we propose a novel method called Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. We then integrate the local and global features using a cross-attention mechanism. Additionally, we incorporate an SSM based on HiPPO algorithm to account for long-term dependencies when updating model parameters, ensuring that model performance in each snapshot informs subsequent updates. Experiments on five public datasets show that our method outperforms existing baseline and state-of-the-art (SOTA) methods in 17 out of 20 cases.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instant AoI Optimization through Relay Location Selection in Disaster Multi-hop Communication</title>
<link>https://arxiv.org/abs/2505.09386</link>
<guid>https://arxiv.org/abs/2505.09386</guid>
<content:encoded><![CDATA[
<div> Keywords: meteorological disasters, communication infrastructures, multi-hop wireless communication, Age of Information, UAV-relayed network

Summary: 
This paper explores the impact of meteorological disasters on communication infrastructures and proposes the use of multi-hop wireless communication, involving IoT devices like UAVs and rescue robots, as an alternative solution. The focus is on analyzing the Age of Information (AoI) metric in this context, specifically in a UAV-relayed wireless network model. By formulating the end-to-end instant AoI and deriving the optimal relay UAV location through mathematical analysis, the study aims to minimize AoI and enhance communication performance in search and rescue operations. Simulations demonstrate that the proposed relay location consistently achieves optimal AoI levels, surpassing other schemes in effectiveness. Through this research, the potential of multi-hop wireless communication in mitigating communication disruptions caused by meteorological disasters is highlighted, emphasizing the importance of efficient communication strategies in emergency response scenarios. 

<br /><br />Summary: <div>
arXiv:2505.09386v1 Announce Type: cross 
Abstract: Meteorological disasters such as typhoons, forest fires, and floods can damage the communication infrastructures, which will further disable the communication capabilities of cellular networks. The multi-hop wireless communication based on IoT devices (e.g., rescue robots, UAVs, and mobile devices) becomes an available and rapidly deployable communication approach for search and rescue operations. However, Age of Information (AoI), an emerging network performance metric, has not been comprehensively investigated in this multi-hop model. In this paper, we first construct a UAV-relayed wireless network model and formulate the end-to-end instant AoI. Then we derive the optimal location of the relay UAV to achieve the minimum instant AoI by mathematical analysis. Simulations show that the derived relay location can always guarantee the optimal AoI and outperform other schemes.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wormhole Detection Based on Z-Score And Neighbor Table Comparison</title>
<link>https://arxiv.org/abs/2505.09405</link>
<guid>https://arxiv.org/abs/2505.09405</guid>
<content:encoded><![CDATA[
<div> Wormhole attacks, disaster rescue opportunity networks, third-party auditor, Z-Score data processing method, detection method<br />
<br />
Summary: Wormhole attacks can disrupt network topology in disaster rescue networks, leading to traffic analysis, DoS, and packet loss attacks. This study proposes a detection method using rescue equipment as a third-party auditor and the Z-Score data processing method for statistical analysis. Simulations validate the effectiveness of the approach, which does not rely on GPS or timers, making it economically valuable and practical for disaster relief. <div>
arXiv:2505.09405v1 Announce Type: cross 
Abstract: Wormhole attacks can cause serious disruptions to the network topology in disaster rescue opportunity networks.
  By establishing false Wormhole(WH) links, malicious nodes can mislead legitimate paths in the network, further causing serious consequences such as traffic analysis attacks (i.e., by eavesdropping and monitoring exchanged traffic), denial of service (DoS) or selective packet loss attacks. This paper uses rescue equipment (vehicle-mounted base stations, rescue control centers, etc.) as an effective third-party auditor (TPA), and combines the commonly used Z-Score (Standard Score) data processing method to propose a new detection method based on pure mathematical statistics for detecting wormhole attacks. Finally, we perform a large number of simulations to evaluate the proposed method. Since our proposed strategy does not require auxiliary equipment such as GPS positioning and timers, as a pure data statistical analysis method, it is obviously more economically valuable, feasible, and practical than other strategies in disaster relief.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Asymptotically Optimal Approximation Algorithm for Multiobjective Submodular Maximization at Scale</title>
<link>https://arxiv.org/abs/2505.09525</link>
<guid>https://arxiv.org/abs/2505.09525</guid>
<content:encoded><![CDATA[
<div> algorithm, multiobjective submodular maximization, approximation guarantee, fair centrality maximization, experimental evaluation

Summary:
This work addresses the problem of maximizing multiple submodular functions simultaneously, known as multiobjective submodular maximization. Existing algorithms either provide weak guarantees or rely on expensive evaluations, making them impractical. The authors introduce a scalable algorithm that achieves the best-known approximation guarantee for this problem. They also propose a novel application  fair centrality maximization  which can be solved through multiobjective submodular maximization. Experimental results demonstrate that the new algorithm outperforms existing ones in both objective value and running time. Overall, this work fills a gap in the field of combinatorial optimization by providing a practical and efficient solution for maximizing the minimum over multiple submodular functions.<br /><br />Summary: <div>
arXiv:2505.09525v1 Announce Type: cross 
Abstract: Maximizing a single submodular set function subject to a cardinality constraint is a well-studied and central topic in combinatorial optimization. However, finding a set that maximizes multiple functions at the same time is much less understood, even though it is a formulation which naturally occurs in robust maximization or problems with fairness considerations such as fair influence maximization or fair allocation.
  In this work, we consider the problem of maximizing the minimum over many submodular functions, which is known as multiobjective submodular maximization. All known polynomial-time approximation algorithms either obtain a weak approximation guarantee or rely on the evaluation of the multilinear extension. The latter is expensive to evaluate and renders such algorithms impractical. We bridge this gap and introduce the first scalable and practical algorithm that obtains the best-known approximation guarantee. We furthermore introduce a novel application fair centrality maximization and show how it can be addressed via multiobjective submodular maximization. In our experimental evaluation, we show that our algorithm outperforms known algorithms in terms of objective value and running time.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Susceptibility Paradox in Online Social Influence</title>
<link>https://arxiv.org/abs/2406.11553</link>
<guid>https://arxiv.org/abs/2406.11553</guid>
<content:encoded><![CDATA[
<div> Keywords: susceptibility, online influence, social networks, peer influence dynamics, Generalized Friendship Paradox

Summary: 
- The study explores susceptibility to online influence within social networks, comparing influence-driven and spontaneous behaviors in content adoption.
- Influence-driven adoption shows high homophily, indicating that individuals prone to influence tend to connect with similarly susceptible peers, reinforcing peer influence dynamics.
- The Generalized Friendship Paradox is extended to influence-driven behaviors, revealing that users' friends are generally more susceptible to influence than the users themselves, establishing the Susceptibility Paradox online.
- Susceptibility to influence can be predicted using friends' susceptibility alone, while predicting spontaneous adoption requires additional user metadata.
- The findings shed light on the interplay between user engagement and characteristics in content adoption, offering insights for designing more effective moderation strategies to protect vulnerable audiences. 

<br /><br />Summary: <div>
arXiv:2406.11553v3 Announce Type: replace 
Abstract: Understanding susceptibility to online influence is crucial for mitigating the spread of misinformation and protecting vulnerable audiences. This paper investigates susceptibility to influence within social networks, focusing on the differential effects of influence-driven versus spontaneous behaviors on user content adoption. Our analysis reveals that influence-driven adoption exhibits high homophily, indicating that individuals prone to influence often connect with similarly susceptible peers, thereby reinforcing peer influence dynamics, whereas spontaneous adoption shows significant but lower homophily. Additionally, we extend the Generalized Friendship Paradox to influence-driven behaviors, demonstrating that users' friends are generally more susceptible to influence than the users themselves, de facto establishing the notion of Susceptibility Paradox in online social influence. This pattern does not hold for spontaneous behaviors, where friends exhibit fewer spontaneous adoptions. We find that susceptibility to influence can be predicted using friends' susceptibility alone, while predicting spontaneous adoption requires additional features, such as user metadata. These findings highlight the complex interplay between user engagement and characteristics in spontaneous content adoption. Our results provide new insights into social influence mechanisms and offer implications for designing more effective moderation strategies to protect vulnerable audiences.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Degree Matrix Comparison for Graph Alignment</title>
<link>https://arxiv.org/abs/2411.07475</link>
<guid>https://arxiv.org/abs/2411.07475</guid>
<content:encoded><![CDATA[
<div> method, graph alignment, degree matrix comparison, unsupervised, geometric alignment

Summary:
Degree Matrix Comparison (DMC) is a new unsupervised geometric alignment method for heterogeneous networks that has shown promising results. Through experiments and mathematical proofs, DMC achieves high accuracy in node alignment, up to 99% for networks with overlapping nodes and 100% for isomorphic graphs. The proposed Greedy DMC reduces time complexity, while Weighted DMC shows potential for aligning weighted graphs. Positive results from applying these variations of DMC suggest the method's validity and efficacy. The sequence of DMC methods presents a reliable solution to the graph alignment problem, offering accurate node correspondence across networks. The simplicity and effectiveness of DMC make it a valuable addition to the field of graph alignment algorithms. 

<br /><br />Summary: <div>
arXiv:2411.07475v3 Announce Type: replace 
Abstract: The graph alignment problem, which considers the optimal node correspondence across networks, has recently gained significant attention due to its wide applications. There are graph alignment methods suited for various network types, but we focus on the unsupervised geometric alignment algorithms. We propose Degree Matrix Comparison (DMC), a very simple degree-based method that has shown to be effective for heterogeneous networks. Through extensive experiments and mathematical proofs, we demonstrate the potential of this method. Remarkably, DMC achieves up to 99% correct node alignment for 90%-overlap networks and 100% accuracy for isomorphic graphs. Additionally, we propose a reduced Greedy DMC with lower time complexity and Weighted DMC that has demonstrated potential for aligning weighted graphs. Positive results from applying Greedy DMC and the Weighted DMC furthermore speaks to the validity and potential of the DMC. The sequence of DMC methods could significantly impact graph alignment, offering reliable solutions for the task.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Politics: Prevalence, Spreaders, and Emotional Reception of AI-Generated Political Images on X</title>
<link>https://arxiv.org/abs/2502.11248</link>
<guid>https://arxiv.org/abs/2502.11248</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated content, social media discourse, Twitter, political images, platform governance <br />
Summary: 
This study examines the prevalence, spreaders, and reception of AI-generated political images on Twitter related to the 2024 U.S. Presidential Election. Approximately 12% of shared images are AI-generated, with 10% of users responsible for sharing the majority of them. AIGC superspreaders, who share a high volume of AI-generated images and receive significant engagement, are more likely to be X Premium subscribers with a right-leaning orientation and engage in automated behavior. They elicit more positive and less toxic responses than non-AI image tweets. The study sheds light on the role generative AI plays in shaping online socio-political environments and highlights the implications for platform governance. <br /> <div>
arXiv:2502.11248v2 Announce Type: replace 
Abstract: Despite widespread concerns about the risks of AI-generated content (AIGC) to the integrity of social media discourse, little is known about its scale and scope, the actors responsible for its dissemination online, and the user responses it elicits. In this work, we measure and characterize the prevalence, spreaders, and emotional reception of AI-generated political images. Analyzing a large-scale dataset from Twitter/X related to the 2024 U.S. Presidential Election, we find that approximately 12% of shared images are detected as AI-generated, and around 10% of users are responsible for sharing 80% of AI-generated images. AIGC superspreaders--defined as the users who not only share a high volume of AI-generated images but also receive substantial engagement through retweets--are more likely to be X Premium subscribers, have a right-leaning orientation, and exhibit automated behavior. Their profiles contain a higher proportion of AI-generated images than non-superspreaders, and some engage in extreme levels of AIGC sharing. Moreover, superspreaders' AI image tweets elicit more positive and less toxic responses than their non-AI image tweets. This study serves as one of the first steps toward understanding the role generative AI plays in shaping online socio-political environments and offers implications for platform governance.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Differential Privacy-Preserving Spectral Clustering for General Graphs</title>
<link>https://arxiv.org/abs/2309.06867</link>
<guid>https://arxiv.org/abs/2309.06867</guid>
<content:encoded><![CDATA[
<div> flipping probability, spectral clustering, differential privacy, stability, network clustering <br />
Summary: <br />
This study examines the stability of spectral clustering algorithms under local differential privacy for general graphs, departing from the assumption that networks are generated from the stochastic block model. The research focuses on the edge flipping method for privacy protection and finds that when edges in an n-vertex graph are flipped with a probability of O(log n/n), clustering outcomes remain consistent. Empirical tests support these theoretical results. It is shown that spectral clustering on well-clustered graphs may yield unstable results for flipping probabilities exceeding (log n/n), suggesting a privacy budget of (log n) for general graphs. This analysis provides valuable insights into the privacy-preserving capabilities of spectral clustering algorithms in real-world network data. <br /> <div>
arXiv:2309.06867v2 Announce Type: replace-cross 
Abstract: Spectral clustering is a widely used algorithm to find clusters in networks. Several researchers have studied the stability of spectral clustering under local differential privacy with the additional assumption that the underlying networks are generated from the stochastic block model (SBM). However, we argue that this assumption is too restrictive since social networks do not originate from the SBM. Thus, we delve into an analysis for general graphs in this work. Our primary focus is the edge flipping method -- a common technique for protecting local differential privacy. We show that, when the edges of an $n$-vertex graph satisfying some reasonable well-clustering assumptions are flipped with a probability of $O(\log n/n)$, the clustering outcomes are largely consistent. Empirical tests further corroborate these theoretical findings. Conversely, although clustering outcomes have been stable for non-sparse and well-clustered graphs produced from the SBM, we show that in general, spectral clustering may yield highly erratic results on certain well-clustered graphs when the flipping probability is $\omega(\log n/n)$. This indicates that the best privacy budget obtainable for general graphs is $\Theta(\log n)$.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analytical Emotion Framework of Rumour Threads on Social Media</title>
<link>https://arxiv.org/abs/2502.16560</link>
<guid>https://arxiv.org/abs/2502.16560</guid>
<content:encoded><![CDATA[
<div> Keywords: online social media, rumours, emotions, contagion, causality

Summary:
Rumours in online social media can have significant repercussions, necessitating a deeper understanding of their development. This study explores the relationship between emotions and rumours in threaded discussions, revealing a comprehensive emotion framework with multi-aspect detection. Through analysis of rumour and non-rumour threads, it is observed that rumours elicit more negative emotions like anger, fear, and pessimism, while non-rumours evoke positivity. Emotions are found to be contagious, with rumours spreading negativity and non-rumours spreading positivity. Causal analysis identifies surprise as a bridge between rumours and other emotions, with pessimism linked to sadness and fear, and optimism to joy and love. This research sheds light on the emotional dynamics of online social media threads, highlighting the impact of emotions on the spread of rumours. <div>
arXiv:2502.16560v2 Announce Type: replace-cross 
Abstract: Rumours in online social media pose significant risks to modern society, motivating the need for better understanding of how they develop. We focus specifically on the interface between emotion and rumours in threaded discourses, building on the surprisingly sparse literature on the topic which has largely focused on single aspect of emotions within the original rumour posts themselves, and largely overlooked the comparative differences between rumours and non-rumours. In this work, we take one step further to provide a comprehensive analytical emotion framework with multi-aspect emotion detection, contrasting rumour and non-rumour threads and provide both correlation and causal analysis of emotions. We applied our framework on existing widely-used rumour datasets to further understand the emotion dynamics in online social media threads. Our framework reveals that rumours trigger more negative emotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive ones. Emotions are contagious, rumours spread negativity, non-rumours spread positivity. Causal analysis shows surprise bridges rumours and other emotions; pessimism comes from sadness and fear, while optimism arises from joy and love.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition</title>
<link>https://arxiv.org/abs/2505.08052</link>
<guid>https://arxiv.org/abs/2505.08052</guid>
<content:encoded><![CDATA[
<div> Keywords: computational model, Persian poetry, similarity network, poet influence, digital humanities 

Summary: 
This study presents a computational model to explore the dynamics of influence among classical Persian poets by creating a multi-dimensional similarity network. By utilizing various features such as semantic, lexical, stylistic, thematic, and metrical elements from Ganjoor's corpus, poets are represented within weighted similarity matrices. The aggregate graph generated from these matrices illustrates the interconnection of poets and their influence on each other. Key poets, style hubs, and bridging poets are identified through network investigation using centrality measures. Additionally, the Louvain community detection algorithm helps identify clusters of poets with shared stylistic and thematic coherence, mapping to recognized literary schools. This data-driven approach offers insights into both canonical figures and lesser-known poets holding structural significance in Persian literature. By combining computational linguistics with literary study, this research provides a scalable model for analyzing poetic tradition, facilitating retrospective analysis and future research within the arena of digital humanities. 

Summary: <br /><br /> <div>
arXiv:2505.08052v1 Announce Type: new 
Abstract: This study formalizes a computational model to simulate classical Persian poets' dynamics of influence through constructing a multi-dimensional similarity network. Using a rigorously curated dataset based on Ganjoor's corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical features to demarcate each poet's corpus. Each is contained within weighted similarity matrices, which are then appended to generate an aggregate graph showing poet-to-poet influence. Further network investigation is carried out to identify key poets, style hubs, and bridging poets by calculating degree, closeness, betweenness, eigenvector, and Katz centrality measures. Further, for typological insight, we use the Louvain community detection algorithm to demarcate clusters of poets sharing both style and theme coherence, which correspond closely to acknowledged schools of literature like Sabk-e Hindi, Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a new data-driven view of Persian literature distinguished between canonical significance and interextual influence, thus highlighting relatively lesser-known figures who hold great structural significance. Combining computational linguistics with literary study, this paper produces an interpretable and scalable model for poetic tradition, enabling retrospective reflection as well as forward-looking research within digital humanities.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community Detection on Noisy Stochastic Block Models</title>
<link>https://arxiv.org/abs/2505.08251</link>
<guid>https://arxiv.org/abs/2505.08251</guid>
<content:encoded><![CDATA[
<div> Algorithm, Community detection, Noisy stochastic block models, Geometric noise, Erdos-Renyi model censoring

Summary:
DuoSpec algorithm addresses community detection in noisy stochastic block models, focusing on geometric noise and Erdos-Renyi model censoring. It aims to de-noise networks for better community recovery. The algorithm outperforms existing methods on noisy models, as demonstrated on synthetic data. Testing on the Amazon metadata dataset showed promising results for community detection. DuoSpec provides a solution for effectively identifying communities in networks affected by noise, showcasing its potential for practical application in various real-world scenarios. <br /><br />Summary: <div>
arXiv:2505.08251v1 Announce Type: new 
Abstract: We study the problem of community detection in noisy stochastic block models. We focus on two types of noise: (1) geometric noise where a latent-space kernel affects edge formation, and (2) Erdos-Renyi model censoring where edges are masked independently. We present a new algorithm DuoSpec that de-noises the network to a pristine stochastic block model structure for better community recovery. We demonstrate on synthetic data that our algorithm outperforms existing community detection methods on noisy models. We test our algorithm on the Amazon metadata dataset and demonstrate strong results on community detection.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Information Diffusion Beyond Explicit Social Ties: A Study of Implicit-Link Diffusion on Twitter</title>
<link>https://arxiv.org/abs/2505.08354</link>
<guid>https://arxiv.org/abs/2505.08354</guid>
<content:encoded><![CDATA[
<div> implicit-link diffusion, information propagation, social media, user engagement, information dissemination

Summary:
- Information diffusion on social media platforms goes beyond explicit social connections.
- Implicit links play a significant role in disseminating content across diverse communities.
- Users farther from the original source in the social network are more likely to engage in diffusion through implicit links.
- Implicit links contribute less to overall diffusion size than explicit links but have a distinct role.
- User groups exhibit strong patterns of social homophily in their choice of diffusion channel.

<br /><br />Summary: <div>
arXiv:2505.08354v1 Announce Type: new 
Abstract: Information diffusion on social media platforms is often assumed to occur primarily through explicit social connections, such as follower or friend relationships. However, information frequently propagates beyond these observable ties -- via external websites, search engines, or algorithmic recommendations -- forming implicit links between users who are not directly connected. Despite their potential impact, the mechanisms and characteristics of such implicit-link diffusion remain underexplored. In this study, we investigate the dynamics of nontrivial information diffusion mediated by implicit links on Twitter, using four large-scale datasets. We define implicit-link diffusion as the reposting of content by users who are not explicitly connected to the original poster. Our analysis reveals that users located farther from the original source in the social network are more likely to engage in diffusion through implicit links, suggesting that such links often arise from sources outside direct social relationships. Moreover, while implicit links contribute less to the overall diffusion size than explicit links, they play a distinct role in disseminating content across diverse and topologically distant communities. We further identify user groups who predominantly engage in diffusion through either explicit or implicit links, and demonstrate that the choice of diffusion channel exhibits strong patterns of social homophily. These findings underscore the importance of incorporating implicit-link dynamics into models of information diffusion and social influence.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A political cartography of news sharing: Capturing story, outlet and content level of news circulation on Twitter</title>
<link>https://arxiv.org/abs/2505.08359</link>
<guid>https://arxiv.org/abs/2505.08359</guid>
<content:encoded><![CDATA[
<div> political leaning, news sharing, digital platforms, online news circulation, research pipeline

Summary:
The article discusses the impact of news sharing on digital platforms and the importance of studying online news circulation among users with different political leanings. It critiques existing approaches for their simplified measures of political leaning and limited analysis of news sources and content. The researchers propose a new research pipeline to systematically map news sharing based on both source and content. They demonstrate through a proof of concept that news sharing diversifies along a second political dimension, outlets vary in their topics, and some outlets cater different news items to different audiences. This methodological contribution provides valuable insights that were previously overlooked in the study of news sharing behavior on digital platforms. 

<br /><br />Summary: <div>
arXiv:2505.08359v1 Announce Type: new 
Abstract: News sharing on digital platforms shapes the digital spaces millions of users navigate. Trace data from these platforms also enables researchers to study online news circulation. In this context, research on the types of news shared by users of differential political leaning has received considerable attention. We argue that most existing approaches (i) rely on an overly simplified measurement of political leaning, (ii) consider only the outlet level in their analyses, and/or (iii) study news circulation among partisans by making ex-ante distinctions between partisan and non-partisan news. In this methodological contribution, we introduce a research pipeline that allows a systematic mapping of news sharing both with respect to source and content. As a proof of concept, we demonstrate insights that otherwise remain unnoticed: Diversification of news sharing along the second political dimension; topic-dependent sharing of outlets; some outlets catering different items to different audiences.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News</title>
<link>https://arxiv.org/abs/2505.08532</link>
<guid>https://arxiv.org/abs/2505.08532</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, multi-agent system, large language models, debate process, interpretability

Summary: 
TruEDebate (TED) is a new approach for detecting fake news using a multi-agent system with large language models. The system simulates a formal debate setting where agents organize into teams to support or challenge the truth of news. This process includes opening statements, cross-examination, rebuttal, and closing statements, allowing for a thorough evaluation of news content. The DebateFlow Agents handle the debate process, while the InsightFlow Agents provide a synthesis and analysis of the debates. The Synthesis Agent summarizes the debates to provide an overarching viewpoint, while the Analysis Agent uses role embeddings and a debate graph to model interactions between debate roles and arguments, ultimately providing a final judgment on the news' truthfulness. TED aims to enhance interpretability and effectiveness in fake news detection by leveraging the reasoning abilities of large language models through a structured debate process. 

<br /><br />Summary: TruEDebate (TED) utilizes a multi-agent system with large language models for fake news detection. The system employs a debate process with DebateFlow and InsightFlow Agents to thoroughly evaluate news content. DebateFlow Agents organize agents into supporting and challenging teams, while InsightFlow Agents provide synthesis and analysis. The Synthesis Agent offers an overarching viewpoint, and the Analysis Agent uses role embeddings and a debate graph to make a final judgment. TED enhances interpretability in fake news detection by leveraging structured debate processes and the reasoning abilities of large language models. <div>
arXiv:2505.08532v1 Announce Type: new 
Abstract: In today's digital environment, the rapid propagation of fake news via social networks poses significant social challenges. Most existing detection methods either employ traditional classification models, which suffer from low interpretability and limited generalization capabilities, or craft specific prompts for large language models (LLMs) to produce explanations and results directly, failing to leverage LLMs' reasoning abilities fully. Inspired by the saying that "truth becomes clearer through debate," our study introduces a novel multi-agent system with LLMs named TruEDebate (TED) to enhance the interpretability and effectiveness of fake news detection. TED employs a rigorous debate process inspired by formal debate settings. Central to our approach are two innovative components: the DebateFlow Agents and the InsightFlow Agents. The DebateFlow Agents organize agents into two teams, where one supports and the other challenges the truth of the news. These agents engage in opening statements, cross-examination, rebuttal, and closing statements, simulating a rigorous debate process akin to human discourse analysis, allowing for a thorough evaluation of news content. Concurrently, the InsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent and the Analysis Agent. The Synthesis Agent summarizes the debates and provides an overarching viewpoint, ensuring a coherent and comprehensive evaluation. The Analysis Agent, which includes a role-aware encoder and a debate graph, integrates role embeddings and models the interactions between debate roles and arguments using an attention mechanism, providing the final judgment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural-Temporal Coupling Anomaly Detection with Dynamic Graph Transformer</title>
<link>https://arxiv.org/abs/2505.08330</link>
<guid>https://arxiv.org/abs/2505.08330</guid>
<content:encoded><![CDATA[
<div> Keywords: anomalous edges, dynamic graphs, structural-temporal coupling, graph transformer model, anomaly detection

Summary: 
This paper presents a novel approach for detecting anomalous edges in dynamic graphs by incorporating structural and temporal features through a dynamic graph transformer model. The lack of structural-temporal coupling information in existing methods is overcome by integrating features from two levels to capture anomaly-aware graph evolutionary patterns. The dynamic graph transformer, enhanced with positional encoding, effectively captures discrimination and contextual consistency signals. Experimental results on six datasets show that the proposed method outperforms current state-of-the-art models in anomaly detection. A case study further demonstrates the effectiveness of the approach in a real-world application. The model's ability to distinguish anomalies from normal instances in evolving triple-based data sets, such as social networks and transaction management, makes it a valuable tool for various applications requiring anomaly detection in dynamic graphs. 

<br /><br />Summary: <div>
arXiv:2505.08330v1 Announce Type: cross 
Abstract: Detecting anomalous edges in dynamic graphs is an important task in many applications over evolving triple-based data, such as social networks, transaction management, and epidemiology. A major challenge with this task is the absence of structural-temporal coupling information, which decreases the ability of the representation to distinguish anomalies from normal instances. Existing methods focus on handling independent structural and temporal features with embedding models, which ignore the deep interaction between these two types of information. In this paper, we propose a structural-temporal coupling anomaly detection architecture with a dynamic graph transformer model. Specifically, we introduce structural and temporal features from two integration levels to provide anomaly-aware graph evolutionary patterns. Then, a dynamic graph transformer enhanced by two-dimensional positional encoding is implemented to capture both discrimination and contextual consistency signals. Extensive experiments on six datasets demonstrate that our method outperforms current state-of-the-art models. Finally, a case study illustrates the strength of our method when applied to a real-world task.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions</title>
<link>https://arxiv.org/abs/2505.08464</link>
<guid>https://arxiv.org/abs/2505.08464</guid>
<content:encoded><![CDATA[
<div> Keywords: Stance detection, Large Language Models, Contextual understanding, Multimodal analysis, Benchmark datasets

Summary: 
This review article explores the use of Large Language Models (LLMs) in stance detection across various platforms. It examines foundational concepts, methodologies, datasets, and applications of LLMs in stance detection. A taxonomy for LLM-based approaches is presented, focusing on learning methods, data modalities, and target relationships. Evaluation techniques, benchmark datasets, and performance trends are analyzed. Key applications include misinformation detection, political analysis, and social media moderation. Challenges such as implicit stance expression and cultural biases are identified. Promising future directions like explainable stance reasoning and low-resource adaptation are discussed, along with the need for real-time deployment frameworks. The survey provides insights into emerging trends, open challenges, and future directions for researchers and practitioners in developing advanced stance detection systems powered by LLMs. 

<br /><br />Summary: <div>
arXiv:2505.08464v1 Announce Type: cross 
Abstract: Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Big Data and the Computational Social Science of Entrepreneurship and Innovation</title>
<link>https://arxiv.org/abs/2505.08706</link>
<guid>https://arxiv.org/abs/2505.08706</guid>
<content:encoded><![CDATA[
<div> Keywords: large-scale social data, machine-learning methods, innovation, entrepreneurship, artificial intelligence

Summary: 
This chapter discusses the challenges and opportunities faced by scholars of entrepreneurship and innovation with the explosion of large-scale social data and advancements in machine-learning methods. It explores the difficulties in identifying technological and commercial novelty, documenting new venture origins, and forecasting competition between new technologies and commercial forms. The chapter suggests leveraging new text, network, image, audio, and video data to advance research in innovation and entrepreneurship. It highlights the use of machine-learning models and big data to create precision measurements as observatories of innovation and entrepreneurship on a societal level. Additionally, artificial intelligence models fueled by big data can generate 'digital doubles' of technology and business for virtual experimentation. By combining big data with big models, the chapter argues for the advancement of theory development and testing in entrepreneurship and innovation. <br /><br />Summary: <div>
arXiv:2505.08706v1 Announce Type: cross 
Abstract: As large-scale social data explode and machine-learning methods evolve, scholars of entrepreneurship and innovation face new research opportunities but also unique challenges. This chapter discusses the difficulties of leveraging large-scale data to identify technological and commercial novelty, document new venture origins, and forecast competition between new technologies and commercial forms. It suggests how scholars can take advantage of new text, network, image, audio, and video data in two distinct ways that advance innovation and entrepreneurship research. First, machine-learning models, combined with large-scale data, enable the construction of precision measurements that function as system-level observatories of innovation and entrepreneurship across human societies. Second, new artificial intelligence models fueled by big data generate 'digital doubles' of technology and business, forming laboratories for virtual experimentation about innovation and entrepreneurship processes and policies. The chapter argues for the advancement of theory development and testing in entrepreneurship and innovation by coupling big data with big models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pay Attention: a Call to Regulate the Attention Market and Prevent Algorithmic Emotional Governance</title>
<link>https://arxiv.org/abs/2402.16670</link>
<guid>https://arxiv.org/abs/2402.16670</guid>
<content:encoded><![CDATA[
<div> market, attention, advertising, detrimental, principles

Summary:
The article discusses how attention has become a valuable commodity in the economic market, with web platforms using various techniques to capture it on a large scale. This has led to negative consequences such as polarizing opinions, spreading false information, and jeopardizing public health and democracies. The paper combines insights from psychology, sociology, and neuroscience to analyze the current practices and their effects. It proposes a set of principles and calls for action to address the detrimental impact of attention-capturing practices on the web. The aim is to prevent the wastage of attention on a global scale, as it is unsustainable for society to allow such practices to continue unchecked. <div>
arXiv:2402.16670v2 Announce Type: replace 
Abstract: Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral and Topological Heterogeneities in Network Versions of Schelling's Segregation Model</title>
<link>https://arxiv.org/abs/2408.05623</link>
<guid>https://arxiv.org/abs/2408.05623</guid>
<content:encoded><![CDATA[
<div> agent-based models, residential segregation, heterogeneity, preferences, topologies
Summary:<br />
- Agent-based models are used to study residential segregation dynamics. 
- Previous studies have focused on the impact of individual preferences and social network structures on segregation levels. 
- Combining heterogeneous preferences and network topologies in simulations leads to reduced segregation levels. 
- Increased representation of both types of heterogeneities results in a wider range of segregation outcomes. 
- A new dynamic of segregation emerges with highly tolerant nodes clustering in dense areas and intolerant nodes relocating to sparse regions, resembling an urban-rural divide.<br /> <div>
arXiv:2408.05623v4 Announce Type: replace-cross 
Abstract: Agent-based models of residential segregation have been of persistent interest to various research communities since their origin with James Sakoda and popularization by Thomas Schelling. Frequently, these models have sought to elucidate the extent to which the collective dynamics of individual preferences may cause segregation to emerge. This open question has sustained relevance in U.S. jurisprudence. Previous investigation of heterogeneity of behaviors (preferences) has shown reductions in segregation. Meanwhile, previous investigation of heterogeneity of social network topologies has shown no significant impact to observed segregation levels. In the present study, we examined the effects of the concurrent presence of both behavioral and topological heterogeneities in network segregation models. Simulations were conducted using both homogeneous and heterogeneous preference models on 2D lattices with varied levels of densification to create topological heterogeneities (i.e., clusters, hubs). Results show a richer variety of outcomes, including novel differences in resultant segregation levels and hub composition. Notably, with concurrent increased representations of heterogeneous preferences and heterogeneous topologies, reduced levels of segregation emerge. Simultaneously, we observe a novel dynamic of segregation between tolerance levels as highly tolerant nodes take residence in dense areas and push intolerant nodes to sparse areas mimicking the urban-rural divide.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenSky Report 2025: Improving Crowdsourced Flight Trajectories with ADS-C Data</title>
<link>https://arxiv.org/abs/2505.06254</link>
<guid>https://arxiv.org/abs/2505.06254</guid>
<content:encoded><![CDATA[
<div> ADS-B, ADS-C, OpenSky Network, air traffic surveillance data, trajectory reconstruction

Summary: 
The OpenSky Network has expanded its data collection to include ADS-C messages via satellite communication, enhancing coverage over oceans and remote regions. By combining ADS-B and ADS-C data, detailed long-haul flight paths, particularly for transatlantic and African routes, can be accurately reconstructed. This integration improves trajectory accuracy, leading to better estimates of fuel consumption and emissions. The combined data also showcases flight patterns in previously underrepresented regions in Africa. Despite coverage limitations, this advancement in providing open access to global flight trajectory data opens up new research possibilities in areas such as air traffic management, environmental impact assessment, and aviation safety.<br /><br />Summary: <div>
arXiv:2505.06254v1 Announce Type: new 
Abstract: The OpenSky Network has been collecting and providing crowdsourced air traffic surveillance data since 2013. The network has primarily focused on Automatic Dependent Surveillance--Broadcast (ADS-B) data, which provides high-frequency position updates over terrestrial areas. However, the ADS-B signals are limited over oceans and remote regions, where ground-based receivers are scarce. To address these coverage gaps, the OpenSky Network has begun incorporating data from the Automatic Dependent Surveillance--Contract (ADS-C) system, which uses satellite communication to track aircraft positions over oceanic regions and remote areas. In this paper, we analyze a dataset of over 720,000 ADS-C messages collected in 2024 from around 2,600 unique aircraft via the Alphasat satellite, covering Europe, Africa, and parts of the Atlantic Ocean. We present our approach to combining ADS-B and ADS-C data to construct detailed long-haul flight paths, particularly for transatlantic and African routes. Our findings demonstrate that this integration significantly improves trajectory reconstruction accuracy, allowing for better fuel consumption and emissions estimates. We illustrate how combined data captures flight patterns across previously underrepresented regions across Africa. Despite coverage limitations, this work marks an important advancement in providing open access to global flight trajectory data, enabling new research opportunities in air traffic management, environmental impact assessment, and aviation safety.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation</title>
<link>https://arxiv.org/abs/2505.06612</link>
<guid>https://arxiv.org/abs/2505.06612</guid>
<content:encoded><![CDATA[
<div> Keywords: social recommendation, multi-semantic modeling, graph convolutional network, tensor convolutional network, Bayesian posterior probability

<br /><br />Summary: In today's rapidly evolving social media landscape, social recommendation systems are increasingly being used as hybrid recommendation solutions. Traditional methods often focus on user similarity, which can lead to the exclusion of relevant relationships and reduce accuracy. This study aims to improve social recommendations by examining the interplay of semantic information across social networks and user-item interaction networks. To this end, a novel model named Burger is introduced, which features robust graph denoising-augmentation fusion and multi-semantic modeling. The approach begins with constructing a social tensor to enhance training efficiency. It employs both graph convolutional and tensor convolutional networks to distinguish between user-item preferences and social preferences. A bi-semantic coordination loss is introduced to capture the mutual influence of semantic information from different domains. Furthermore, Bayesian posterior probability is used to identify potential social relations and mitigate the impact of irrelevant connections. Finally, the model utilizes a sliding window mechanism to keep the social tensor updated for subsequent iterations. Experiments across three real datasets demonstrate that Burger outperforms existing state-of-the-art models in terms of accuracy and effectiveness. <div>
arXiv:2505.06612v1 Announce Type: new 
Abstract: In the era of rapid development of social media, social recommendation systems as hybrid recommendation systems have been widely applied. Existing methods capture interest similarity between users to filter out interest-irrelevant relations in social networks that inevitably decrease recommendation accuracy, however, limited research has a focus on the mutual influence of semantic information between the social network and the user-item interaction network for further improving social recommendation. To address these issues, we introduce a social \underline{r}ecommendation model with ro\underline{bu}st g\underline{r}aph denoisin\underline{g}-augmentation fusion and multi-s\underline{e}mantic Modeling(Burger). Specifically, we firstly propose to construct a social tensor in order to smooth the training process of the model. Then, a graph convolutional network and a tensor convolutional network are employed to capture user's item preference and social preference, respectively. Considering the different semantic information in the user-item interaction network and the social network, a bi-semantic coordination loss is proposed to model the mutual influence of semantic information. To alleviate the interference of interest-irrelevant relations on multi-semantic modeling, we further use Bayesian posterior probability to mine potential social relations to replace social noise. Finally, the sliding window mechanism is utilized to update the social tensor as the input for the next iteration. Extensive experiments on three real datasets show Burger has a superior performance compared with the state-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling and Study of t , Peak and Effective Diameter in Temporal Networks</title>
<link>https://arxiv.org/abs/2505.06719</link>
<guid>https://arxiv.org/abs/2505.06719</guid>
<content:encoded><![CDATA[
<div> keywords: temporal networks, diameter, effective diameter, peak diameter, t-Diameter<br />
Summary:<br />
Understanding the spread of information, diseases, and influence in temporal networks is a complex challenge. This study introduces a mathematical framework to analyze diameter in temporal networks, presenting three new metrics: Effective Diameter, Peak Diameter, and t-Diameter. The study combines theoretical analysis with empirical validation using real-world datasets. Results show high accuracy of the model, with effective diameter decreasing with increasing average degree and increasing with network size. t-Diameter and Peak Diameter are more sensitive to node removal, making them valuable for epidemic modeling. The framework bridges formal modeling and empirical data, offering insights into the temporal dynamics of networked systems and providing tools for assessing robustness, controlling information spread, and optimizing interventions. <br /><br /> <div>
arXiv:2505.06719v1 Announce Type: new 
Abstract: Understanding how information, diseases, or influence spread across networks is a fundamental challenge in complex systems. While network diameter has been extensively studied in static networks, its definition and behavior in temporal networks remain underexplored due to their dynamic nature. In this study, we present a formal mathematical framework for analyzing diameter in temporal networks and introduce three time-aware metrics: Effective Diameter , Peak Diameter (*D), and t-Diameter (tD), each capturing distinct temporal aspects of connectivity and diffusion. Our approach combines theoretical analysis with empirical validation using four real-world datasets: high school, hospital, conference, and workplace contact networks. We simulate flow propagation on temporal networks and compare the observed diameters with the proposed theoretical Equations. Across all datasets, our model demonstrates high accuracy, with low RMSE and absolute error values. Furthermore, we observe that the effective diameter decreases with increasing average degree and increases with network size. The results also show that tD and *D are more sensitive to node removal, highlighting their relevance for applications such as epidemic modeling. By bridging formal modeling and empirical data, our framework offers new insights into the temporal dynamics of networked systems and provides tools for assessing robustness, controlling information spread, and optimizing interventions in time-sensitive environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Robustness and Reducibility of Multiplex Networks with Embedding-Aided Interlayer Similarities</title>
<link>https://arxiv.org/abs/2505.06998</link>
<guid>https://arxiv.org/abs/2505.06998</guid>
<content:encoded><![CDATA[
<div> Keywords: interlayer similarity, multiplex networks, EATSim, network robustness, structural similarity

<br /><br />Summary: The article discusses the significance of interlayer similarity in multiplex networks to understand the complexities of interconnected systems. This study reveals how variations in one network layer influence others, with implications in transportation, social, and biological contexts. Existing algorithms for measuring interlayer similarity are limited as they capture only partial information, hindering a complete understanding of multiplex networks. To overcome this, the authors introduce a new approach called Embedding Aided inTerlayer Similarity (EATSim), which integrates both intralayer structural similarity and cross-layer anchor node alignment consistency. This comprehensive framework offers a better analysis of interconnected networks. Extensive experiments conducted on synthetic and real-world datasets indicate that EATSim effectively captures the geometry of similarities between layers, leading to improved accuracy in interlayer similarity measurement. Furthermore, EATSim showcases state-of-the-art performance in two applications: predicting network robustness and network reducibility. These results demonstrate the approach's potential in enhancing the understanding and management of complex systems, making EATSim a valuable tool for researchers studying multiplex networks. <div>
arXiv:2505.06998v1 Announce Type: new 
Abstract: The study of interlayer similarity of multiplex networks helps to understand the intrinsic structure of complex systems, revealing how changes in one layer can propagate and affect others, thus enabling broad implications for transportation, social, and biological systems. Existing algorithms that measure similarity between network layers typically encode only partial information, which limits their effectiveness in capturing the full complexity inherent in multiplex networks. To address this limitation, we propose a novel interlayer similarity measuring approach named Embedding Aided inTerlayer Similarity (EATSim). EATSim concurrently incorporates intralayer structural similarity and cross-layer anchor node alignment consistency, providing a more comprehensive framework for analyzing interconnected systems. Extensive experiments on both synthetic and real-world networks demonstrate that EATSim effectively captures the underlying geometric similarities between interconnected networks, significantly improving the accuracy of interlayer similarity measurement. Moreover, EATSim achieves state-of-the-art performance in two downstream applications: predicting network robustness and network reducibility, showing its great potential in enhancing the understanding and management of complex systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Language of Influence: Sentiment, Emotion, and Hate Speech in State Sponsored Influence Operations</title>
<link>https://arxiv.org/abs/2505.07212</link>
<guid>https://arxiv.org/abs/2505.07212</guid>
<content:encoded><![CDATA[
<div> sentiment, emotion, abusive speech, social media, influence operations <br />
Summary: <br />
This study examines state-sponsored influence operations (SIOs) on social media from China, Iran, and Russia. Analyzing 1.5 million tweets, the research uncovers different patterns in sentiment, emotion, and abusive speech used by these campaigns. Russian SIOs predominantly employ negative sentiment and toxic language to polarize audiences. Iranian operations balance negative and positive tones to provoke hostility and garner support. Meanwhile, Chinese campaigns focus on positive messaging to promote favorable narratives. These findings highlight the diverse strategies employed by countries in influencing public opinion through social media platforms. <div>
arXiv:2505.07212v1 Announce Type: new 
Abstract: State-sponsored influence operations (SIOs) on social media have become an instrumental tool for manipulating public opinion and spreading unverified information. This study analyzes the sentiment, emotion, and abusive speech in tweets circulated by influence campaigns originating from three distinct countries: China, Iran, and Russia. We examined 1.5 million tweets to uncover patterns in the content of the influence operations using the dataset provided by Twitter. Our findings reveal distinct patterns of sentiment, emotion, and abusive nature in different SIOs. Our experimental result shows that Russian influence operations predominantly employ negative sentiment and toxic language to polarize audiences, Iranian operations balance negative and positive tones to provoke hostility while fostering support, and Chinese campaigns focus on positive messaging to promote favorable narratives.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Ethics in the Fediverse: Analyzing the Role of Instance Policies in Mastodon Research</title>
<link>https://arxiv.org/abs/2505.07606</link>
<guid>https://arxiv.org/abs/2505.07606</guid>
<content:encoded><![CDATA[
<div> Keywords: Mastodon, data collection, research ethics, policy adherence, decentralized platforms  

<br /><br />Summary: This article examines the disconnection between the individual policies of Mastodon instances, many of which explicitly ban data collection for research purposes, and the actual practices observed in academic research that utilizes Mastodon data. The authors conducted a systematic analysis of 29 studies that sourced data from Mastodon, discovering a notable lack of compliance with the declared policies of these instances, despite researchers generally being aware of their existence. The findings highlight the ethical dilemmas surrounding the use of data collected from decentralized social media platforms, pointing to a gap in the adherence to ethical guidelines. Furthermore, the article calls for a more extensive discourse on researchers' ethical responsibilities when conducting research on alternative social media platforms like Mastodon. It emphasizes the importance of respecting user privacy and instance-specific rules, advocating for a reevaluation of current practices to foster ethical research approaches that align with community guidelines. This dialogue is crucial for ensuring that the academic use of data from decentralized social networks does not undermine the very principles of privacy and trust that these platforms aim to uphold. <div>
arXiv:2505.07606v1 Announce Type: new 
Abstract: This article addresses the disconnect between the individual policy documents of Mastodon instances--many of which explicitly prohibit data collection for research purposes--and the actual data handling practices observed in academic research involving Mastodon. We present a systematic analysis of 29 works that used Mastodon as a data source, revealing limited adherence to instance--level policies despite researchers' general awareness of their existence. Our findings underscore the need for broader discussion about ethical obligations in research on alternative, decentralized social media platforms.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>'Congratulations, morons': Dynamics of Toxicity and Interaction Polarization in the Covid Vaccination and Ukraine War Twitter Debates</title>
<link>https://arxiv.org/abs/2505.07646</link>
<guid>https://arxiv.org/abs/2505.07646</guid>
<content:encoded><![CDATA[
<div> polarization, echo chambers, social media, Twitter datasets, toxicity <br />
Summary: 
The study explores polarization dynamics in social media discussions around Covid-19 vaccination and the Ukraine war by analyzing two large Twitter datasets. The analysis focuses on influencer preferences and toxicity of post contents. By examining retweet behavior and clustering users based on ideological preferences, the study identifies ideological opposition and temporal associations between toxicity and structural divergence. The findings suggest that polarization is a multifaceted dynamic phenomenon that can manifest within a single ideological camp. The study highlights the importance of considering polarization as an evolving process that may lead to unexpected outcomes in information diffusion spaces. The research provides insights into understanding how polarization influences partisan behavior on social media platforms. <br /> <div>
arXiv:2505.07646v1 Announce Type: new 
Abstract: The existence of polarization and echo chambers has been noted in social media discussions of public concern such as the Covid-19 pandemic, foreign election interference, and regional conflicts. However, measuring polarization and assessing the manner in which polarization contributes to partisan behavior is not always possible to evaluate with static network or affect measurements. To address this, we conduct an analysis of two large Twitter datasets collected around Covid-19 vaccination and the Ukraine war to investigate polarization in terms of the evolution in influencer preferences and toxicity of post contents. By reducing retweet behavior in each sample to several key dimensions, we identify clusters that reflect ideological preferences, along with geographic or linguistic separation for some cases. By tracking the central retweet tendency of these clusters over time, we observe differences in the relative position of ideologically unaligned clusters compared to aligned ones, which we interpret as reflecting polarization dynamics in the information diffusion space. We then measure the toxicity of posts and test if toxicity in one cluster can be temporally dependent on its structural closeness to (or toxicity of) another. We find evidence of ideological opposition among clusters of users in both samples, and a temporal association between toxicity and structural divergence for at least two ideologically opposed clusters in our samples. These observations support the importance of analyzing polarization as a multifaceted dynamic phenomenon where polarization dynamics may also manifest in unexpected ways such as within a single ideological camp.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity</title>
<link>https://arxiv.org/abs/2505.06313</link>
<guid>https://arxiv.org/abs/2505.06313</guid>
<content:encoded><![CDATA[
<div> sentiments, NATO unity, NATO Article 5 trust, GPT models, qualitative analytics

Summary:
The paper explores the use of GPT models with retrieval-augmented generation (RAG) to analyze sentiments, NATO unity, and NATO Article 5 trust opinions in various web sources. Utilizing the GPT-4.1 model, qualitative news summaries and quantitative opinion scores were generated for NATO-related topics. Bayesian regression was employed to analyze the opinion score trends, revealing a downward trend in opinions related to NATO unity. The study does not serve as political analysis but showcases AI-based approaches for further analytics. Additionally, a dynamic model based on neural ordinary differential equations was considered for understanding evolving public opinions. The results indicate that utilizing GPT models for news analysis can offer valuable qualitative and quantitative insights for comprehensive analytical approaches. <div>
arXiv:2505.06313v1 Announce Type: cross 
Abstract: The paper considers the use of GPT models with retrieval-augmented generation (RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity and NATO Article 5 trust opinion scores in different web sources: news sites found via Google Search API, Youtube videos with comments, and Reddit discussions. A RAG approach using GPT-4.1 model was applied to analyse news where NATO related topics were discussed. Two levels of RAG analytics were used: on the first level, the GPT model generates qualitative news summaries and quantitative opinion scores using zero-shot prompts; on the second level, the GPT model generates the summary of news summaries. Quantitative news opinion scores generated by the GPT model were analysed using Bayesian regression to get trend lines. The distributions found for the regression parameters make it possible to analyse an uncertainty in specified news opinion score trends. Obtained results show a downward trend for analysed scores of opinion related to NATO unity.
  This approach does not aim to conduct real political analysis; rather, it consider AI based approaches which can be used for further analytics
  as a part of a complex analytical approach. The obtained results demonstrate that the use of GPT models for news analysis can give informative qualitative and quantitative analytics, providing important insights.
  The dynamic model based on neural ordinary differential equations was considered for modelling public opinions. This approach makes it possible to analyse different scenarios for evolving public opinions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK</title>
<link>https://arxiv.org/abs/2505.06782</link>
<guid>https://arxiv.org/abs/2505.06782</guid>
<content:encoded><![CDATA[
<div> approaches, regulation, electronic cigarettes, Australia, UK <br />
Summary:<br />
- Australia and the UK have contrasting approaches to the regulation of electronic cigarettes, with Australia being more restrictive and the UK more permissive.<br />
- Both countries developed their policies based on the same evidence base.<br />
- A Large Language Model-based classifier was used to analyze electronic cigarette-related policy documents from Australia and the UK.<br />
- The classifier showed that Australian documents contain more harmful statements about e-cigarettes, while UK documents emphasize the benefits.<br />
- This study highlights how different jurisdictions manage and present evidence in shaping their policies on electronic cigarettes, demonstrating the potential of using LLM-based methods in health policy formation. <br />Summary: <div>
arXiv:2505.06782v1 Announce Type: cross 
Abstract: Australia and the UK have developed contrasting approaches to the regulation of electronic cigarettes, with - broadly speaking - Australia adopting a relatively restrictive approach and the UK adopting a more permissive approach. Notably, these divergent policies were developed from the same broad evidence base. In this paper, to investigate differences in how the two jurisdictions manage and present evidence, we developed and evaluated a Large Language Model-based sentence classifier to perform automated analyses of electronic cigarette-related policy documents drawn from official Australian and UK legislative processes (109 documents in total). Specifically, we utilized GPT-4 to automatically classify sentences based on whether they contained claims that e-cigarettes were broadly helpful or harmful for public health. Our LLM-based classifier achieved an F-score of 0.9. Further, when applying the classifier to our entire sentence-level corpus, we found that Australian legislative documents show a much higher proportion of harmful statements, and a lower proportion of helpful statements compared to the expected values, with the opposite holding for the UK. In conclusion, this work utilized an LLM-based approach to provide evidence to support the contention that - drawing on the same evidence base - Australian ENDS-related policy documents emphasize the harms associated with ENDS products and UK policy documents emphasize the benefits. Further, our approach provides a starting point for using LLM-based methods to investigate the complex relationship between evidence and health policy formation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When cardinals strategize: An agent-based model of influence and ideology for the papal conclave</title>
<link>https://arxiv.org/abs/2505.07014</link>
<guid>https://arxiv.org/abs/2505.07014</guid>
<content:encoded><![CDATA[
<div> agent-based models, papal conclaves, social influence, strategic voting, ideological alignment
<br />
In this study, two agent-based models are proposed to explore the dynamics of papal conclaves. The models consider how social influence, strategic voting, and ideological alignment influence the time taken to elect a pope. The first model includes mechanisms where cardinals imitate peers' choices and shift support to the most voted candidate from the previous round. Strategic behavior is introduced through "useful voting", where agents switch to the most viable alternative if their preferred candidate receives insufficient votes. The second model incorporates ideological blocs, with cardinals and candidates grouped as progressives or conservatives. Numerical simulations show that ideological polarization can prolong the election process. However, the quick outcome of the 2025 conclave suggests that informal consensus-building before voting could expedite convergence. These results emphasize the importance of strategic flexibility and ideological structure in collective decision-making during papal conclaves. 
<br /><br />Summary: <div>
arXiv:2505.07014v1 Announce Type: cross 
Abstract: We propose and analyze two agent-based models to investigate the dynamics of papal conclaves, focusing on how social influence, strategic voting, and ideological alignment affect the time required to elect a pope. In the first model, cardinals interact through two mechanisms: with probability $p$, they imitate the choice of a randomly selected peer, and with probability $q$, they shift support to the most voted candidate from the previous round. Additionally, strategic behavior is introduced via ``useful voting'', where agents abandon their preferred candidate if he receives less than a certain fraction of the votes, switching instead to the most viable alternative. A candidate must secure a qualified majority of two-thirds to be elected. After that, we extend the framework by incorporating ideological blocs, assigning each cardinal and candidate to one of two groups (e.g., progressives and conservatives). Cardinals initially vote for candidates from their own group, but may cross ideological lines due to strategic considerations. We initialize the population with $20\%$ conservative cardinals, reflecting the current composition shaped by papal appointments. Numerical simulations show that ideological polarization can delay the election, increasing the average number of voting rounds. Our results highlight the crucial role of both strategic flexibility and ideological structure in collective decision-making under conditions found in papal conclaves. The recent rapid outcome of the 2025 conclave, despite a polarized electorate, suggests that informal consensus-building - possibly prior to voting - may play a decisive role in accelerating convergence, complementing the mechanisms explored in our model.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Unconstrained Local Search for Partitioning Irregular Graphs</title>
<link>https://arxiv.org/abs/2308.15494</link>
<guid>https://arxiv.org/abs/2308.15494</guid>
<content:encoded><![CDATA[
<div> Keywords: balanced graph partitioning, local search, solution quality, parallel scalability, irregular graphs <br />
Summary: <br />
In this paper, new refinement heuristics for the balanced graph partitioning problem are introduced, challenging the traditional rule of only allowing moves that maintain balanced block sizes. By permitting temporary balance violations, significant improvements in solution quality, especially for irregular instances like social networks, are achieved. Efficient implementations involve carefully selecting candidates for unconstrained moves and developing algorithms for rebalancing later on. The study explores various design choices to achieve high parallel scalability. Experimental results show that the parallel unconstrained local search techniques outperform existing solvers by a large margin, finding 75% of the best solutions on irregular graphs. Additionally, a 9.6% improvement in edge cut over the next best competitor is achieved, while being only 7.7% slower in the geometric mean. <br />

Summary: <br /> <div>
arXiv:2308.15494v3 Announce Type: replace 
Abstract: We present new refinement heuristics for the balanced graph partitioning problem that break with an age-old rule. Traditionally, local search only permits moves that keep the block sizes balanced (below a size constraint). In this work, we demonstrate that admitting large temporary balance violations drastically improves solution quality. The effects are particularly strong on irregular instances such as social networks. Designing efficient implementations of this general idea involves both careful selection of candidates for unconstrained moves as well as algorithms for rebalancing the solution later on. We explore a wide array of design choices to achieve this, in addition to our third goal of high parallel scalability. We present compelling experimental results, demonstrating that our parallel unconstrained local search techniques outperform the prior state of the art by a substantial margin. Compared with four state-of-the-art solvers, our new technique finds 75\% of the best solutions on irregular graphs. We achieve a 9.6\% improvement in edge cut over the next best competitor, while being only 7.7\% slower in the geometric mean.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dynamics of Collective Creativity in Human-AI Social Networks</title>
<link>https://arxiv.org/abs/2502.17962</link>
<guid>https://arxiv.org/abs/2502.17962</guid>
<content:encoded><![CDATA[
<div> AI, generative AI, collective creativity, experimental social networks, creative writing task <br />
Summary: This study explores the impact of generative AI on collective creativity in experimental social networks. Large-scale online experiments involving 879 participants and AI agents in a creative writing task revealed that AI-only networks initially displayed greater creativity and diversity than human-only and human-AI networks. However, over time, hybrid human-AI networks surpassed AI-only networks in diversity. This shift was attributed to AI agents retaining little from the original stories, while human-only networks maintained continuity. The findings emphasize the significance of human-AI interactions in shaping creativity within experimental social networks and shed light on the dynamic nature of human-AI hybrid societies.<br /><br /> <div>
arXiv:2502.17962v2 Announce Type: replace 
Abstract: Generative AI is reshaping modern culture, enabling individuals to create high-quality outputs across domains such as images, text, and music. However, we know little about the impact of generative AI on collective creativity. This study investigates how human-AI interactions shape collective creativity within experimental social networks. We conducted large-scale online experiments with 879 participants and AI agents in a creative writing task. Participants (either humans or AI) joined 5x5 grid-based networks, and were asked to iteratively select, modify, and share stories. Initially, AI-only networks showed greater creativity (rated by a separate group of 94 human raters) and diversity than human-only and human-AI networks. However, over time, hybrid human-AI networks became more diverse in their creations than AI-only networks. In part, this is because AI agents retained little from the original stories, while human-only networks preserved continuity. These findings highlight the value of experimental social networks in understanding human-AI hybrid societies.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global decomposition of networks into multiple cores formed by local hubs</title>
<link>https://arxiv.org/abs/2407.00355</link>
<guid>https://arxiv.org/abs/2407.00355</guid>
<content:encoded><![CDATA[
<div> Keywords: network decomposition, core-periphery structure, hub centrality, edge pruning, hierarchical structure

Summary: 
The article introduces a novel network decomposition scheme that uncovers multiscale core-periphery structures within networks by utilizing locally defined nodal hub centrality and edge-pruning techniques. The method identifies breaking points in network decomposition by removing locally least connected nodes, revealing an onion-like hierarchical structure. Unlike the traditional k-core decomposition method, this approach focuses on relative information within local structures, effectively highlighting locally crucial substructures. Additionally, the method can detect multiple core-periphery structures and decompose coarse-grained supernode networks by integrating it with network community detection. This innovative approach provides a more detailed and insightful analysis of network structures, offering a valuable tool for studying complex networks in various fields. 

<br /><br />Summary: <div>
arXiv:2407.00355v3 Announce Type: replace-cross 
Abstract: Networks are ubiquitous in various fields, representing systems where nodes and their interconnections constitute their intricate structures. We introduce a network decomposition scheme to reveal multiscale core-periphery structures lurking inside, using the concept of locally defined nodal hub centrality and edge-pruning techniques built upon it. We demonstrate that the hub-centrality-based edge pruning reveals a series of breaking points in network decomposition, which effectively separates a network into its backbone and shell structures. Our local-edge decomposition method iteratively identifies and removes locally least connected nodes, and uncovers an onion-like hierarchical structure as a result. Compared with the conventional $k$-core decomposition method, our method based on relative information residing in local structures exhibits a clear advantage in terms of discovering locally crucial substructures. As an application of the method, we present a scheme to detect multiple core-periphery structures and the decomposition of coarse-grained supernode networks, by combining the method with the network community detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Price of Differential Privacy for Spectral Clustering over Stochastic Block Models</title>
<link>https://arxiv.org/abs/2505.05816</link>
<guid>https://arxiv.org/abs/2505.05816</guid>
<content:encoded><![CDATA[
<div> Keywords: privacy-preserving, spectral clustering, community detection, stochastic block models, edge differential privacy <br />
<br />
Summary: <br />
In this study, researchers investigate privacy-preserving spectral clustering for community detection in stochastic block models. They specifically focus on edge differential privacy and propose private algorithms for community recovery. The research delves into the trade-offs between the privacy budget and accurate community label recovery. The team establishes information-theoretic conditions that ensure the accuracy of their methods, offering theoretical guarantees for successful community recovery under edge DP. This work contributes to the field by addressing the crucial issue of preserving privacy while maintaining the accuracy of community detection algorithms. <div>
arXiv:2505.05816v1 Announce Type: new 
Abstract: We investigate privacy-preserving spectral clustering for community detection within stochastic block models (SBMs). Specifically, we focus on edge differential privacy (DP) and propose private algorithms for community recovery. Our work explores the fundamental trade-offs between the privacy budget and the accurate recovery of community labels. Furthermore, we establish information-theoretic conditions that guarantee the accuracy of our methods, providing theoretical assurances for successful community recovery under edge DP.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Noise-Resilient Semi-Supervised Graph Autoencoder for Overlapping Semantic Community Detection</title>
<link>https://arxiv.org/abs/2505.05965</link>
<guid>https://arxiv.org/abs/2505.05965</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, overlapping structures, graph autoencoder, semi-supervised, attribute noise<br />
Summary:<br />
The article introduces a novel approach for detecting overlapping communities in networks by proposing a semi-supervised graph autoencoder. This model combines graph multi-head attention and modularity maximization to effectively identify communities that share common nodes. It learns semantic representations by integrating structural, attribute, and prior knowledge while addressing noise in node features. Key features of the model include a noise-resistant architecture and a design that incorporates modularity constraints for community quality optimization. Experimental results demonstrate the superior performance of the model compared to existing methods, showing improvements in NMI and F1-score metrics. The model also exhibits exceptional robustness to attribute noise, maintaining stable performance even when 60% of node features are corrupted. These findings underscore the significance of incorporating attribute semantics and structural patterns for accurate community detection in complex networks.<br /><br />Summary: <div>
arXiv:2505.05965v1 Announce Type: new 
Abstract: Community detection in networks with overlapping structures remains a significant challenge, particularly in noisy real-world environments where integrating topology, node attributes, and prior information is critical. To address this, we propose a semi-supervised graph autoencoder that combines graph multi-head attention and modularity maximization to robustly detect overlapping communities. The model learns semantic representations by fusing structural, attribute, and prior knowledge while explicitly addressing noise in node features. Key innovations include a noise-resistant architecture and a semantic semi-supervised design optimized for community quality through modularity constraints. Experiments demonstrate superior performance the model outperforms state-of-the-art methods in overlapping community detection (improvements in NMI and F1-score) and exhibits exceptional robustness to attribute noise, maintaining stable performance under 60\% feature corruption. These results highlight the importance of integrating attribute semantics and structural patterns for accurate community discovery in complex networks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
<link>https://arxiv.org/abs/2505.06184</link>
<guid>https://arxiv.org/abs/2505.06184</guid>
<content:encoded><![CDATA[
<div> Keywords: social media user profiling, large language model, interpretability, adaptability, Persian political Twitter dataset

Summary: 
Our novel approach leverages a large language model to analyze social media user profiles through domain-defining statements, creating abstractive and extractive user profiles without the need for large labeled datasets. By using semi-supervised filtering with a domain-specific knowledge base, our method generates interpretable natural language profiles that condense user data for downstream tasks. We introduce a Persian political Twitter dataset (X) and an evaluation framework with human validation to showcase the effectiveness of our approach. Experimental results demonstrate a 9.8% improvement over state-of-the-art methods, highlighting the flexibility, adaptability, and interpretability of our approach. <div>
arXiv:2505.06184v1 Announce Type: new 
Abstract: Social media user profiling through content analysis is crucial for tasks like misinformation detection, engagement prediction, hate speech monitoring, and user behavior modeling. However, existing profiling techniques, including tweet summarization, attribute-based profiling, and latent representation learning, face significant limitations: they often lack transferability, produce non-interpretable features, require large labeled datasets, or rely on rigid predefined categories that limit adaptability. We introduce a novel large language model (LLM)-based approach that leverages domain-defining statements, which serve as key characteristics outlining the important pillars of a domain as foundations for profiling. Our two-stage method first employs semi-supervised filtering with a domain-specific knowledge base, then generates both abstractive (synthesized descriptions) and extractive (representative tweet selections) user profiles. By harnessing LLMs' inherent knowledge with minimal human validation, our approach is adaptable across domains while reducing the need for large labeled datasets. Our method generates interpretable natural language user profiles, condensing extensive user data into a scale that unlocks LLMs' reasoning and knowledge capabilities for downstream social network tasks. We contribute a Persian political Twitter (X) dataset and an LLM-based evaluation framework with human validation. Experimental results show our method significantly outperforms state-of-the-art LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in creating flexible, adaptable, and interpretable user profiles.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Economic Analysis and Optimization of Energy Storage Configuration for Park Power Systems Based on Random Forest and Genetic Algorithm</title>
<link>https://arxiv.org/abs/2505.05511</link>
<guid>https://arxiv.org/abs/2505.05511</guid>
<content:encoded><![CDATA[
<div> random forest model, energy storage system, operational costs, power load balancing, economic performance

Summary:
The study analyzes the economic performance of various parks under different conditions, focusing on operational costs and power load balancing. Initially, parks without energy storage were analyzed using a random forest model, indicating a correlation between cost and electricity purchase. Simulations after deploying a 50kW/100kWh energy storage system showed decreased wind and solar power curtailment and reduced operational costs. Using a genetic algorithm, the energy storage configuration of each park was optimized, leading to improved economic indicators for Parks A, B, and C. The research highlights the importance of optimizing energy storage configurations to reduce costs, enhance economic benefits, and promote sustainable development in the power system. 

Summary: <div>
arXiv:2505.05511v1 Announce Type: cross 
Abstract: This study aims to analyze the economic performance of various parks under different conditions, particularly focusing on the operational costs and power load balancing before and after the deployment of energy storage systems. Firstly, the economic performance of the parks without energy storage was analyzed using a random forest model. Taking Park A as an example, it was found that the cost had the greatest correlation with electricity purchase, followed by photovoltaic output, indicating that solar and wind power output are key factors affecting economic performance. Subsequently, the operation of the parks after the configuration of a 50kW/100kWh energy storage system was simulated, and the total cost and operation strategy of the energy storage system were calculated. The results showed that after the deployment of energy storage, the amount of wind and solar power curtailment in each park decreased, and the operational costs were reduced. Finally, a genetic algorithm was used to optimize the energy storage configuration of each park. The energy storage operation strategy was optimized through fitness functions, crossover operations, and mutation operations. After optimization, the economic indicators of Parks A, B, and C all improved. The research results indicate that by optimizing energy storage configuration, each park can reduce costs, enhance economic benefits, and achieve sustainable development of the power system.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equalizing Closeness Centralities via Edge Additions</title>
<link>https://arxiv.org/abs/2505.06222</link>
<guid>https://arxiv.org/abs/2505.06222</guid>
<content:encoded><![CDATA[
<div> Graph modification problems, closeness centralities, algorithmic fairness, NP-hard, approximation algorithms<br />
Summary:<br />
The article explores graph modification problems focused on equalizing the network positions of nodes, motivated by social capital fairness. Two formalized problems are discussed: Closeness Ratio Improvement to maximize closeness centrality ratio between two nodes and Closeness Gap Minimization to minimize the difference. Both problems are proven to be NP-hard, with a quasilinear-time approximation for Closeness Ratio Improvement. However, Closeness Gap Minimization lacks a multiplicative approximation unless P=NP. The study concludes with suggestions for future research in this problem domain, including potential generalizations. <div>
arXiv:2505.06222v1 Announce Type: cross 
Abstract: Graph modification problems with the goal of optimizing some measure of a given node's network position have a rich history in the algorithms literature. Less commonly explored are modification problems with the goal of equalizing positions, though this class of problems is well-motivated from the perspective of equalizing social capital, i.e., algorithmic fairness. In this work, we study how to add edges to make the closeness centralities of a given pair of nodes more equal. We formalize two versions of this problem: Closeness Ratio Improvement, which aims to maximize the ratio of closeness centralities between two specified nodes, and Closeness Gap Minimization, which aims to minimize the absolute difference of centralities. We show that both problems are $\textsf{NP}$-hard, and for Closeness Ratio Improvement we present a quasilinear-time $\frac{6}{11}$-approximation, complemented by a bicriteria inapproximability bound. In contrast, we show that Closeness Gap Minimization admits no multiplicative approximation unless $\textsf{P} = \textsf{NP}$. We conclude with a discussion of open directions for this style of problem, including several natural generalizations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Spread of Online Incivility in Brazilian Politics</title>
<link>https://arxiv.org/abs/2504.08960</link>
<guid>https://arxiv.org/abs/2504.08960</guid>
<content:encoded><![CDATA[
<div> Keywords: incivility, online discourse, Brazilian politics, social media, political influencers <br />
Summary: <br />
This study introduces a multidimensional framework for understanding online political incivility, highlighting impoliteness, physical harm, hate speech, and threats to democratic values. Analyzing 5 million tweets from 2,307 political influencers during the 2022 Brazilian general election, the research reveals that impoliteness peaks during election campaigns, while other forms of incivility are triggered by specific violent events. Left-aligned influencers are identified as primary spreaders of online incivility, both direct and indirect. They disseminate content from various sources, creating a diffusion pattern involving direct and two-step communication flows. This study sheds light on the diverse nature of incivility in Brazilian politics, providing insights that can be applied to other political contexts. <br /> <div>
arXiv:2504.08960v2 Announce Type: replace 
Abstract: Incivility refers to behaviors that violate collective norms and disrupt cooperation within the political process. Although large-scale online data and automated techniques have enabled the quantitative analysis of uncivil discourse, prior research has predominantly focused on impoliteness or toxicity, often overlooking other behaviors that undermine democratic values. To address this gap, we propose a multidimensional conceptual framework encompassing Impoliteness, Physical Harm and Violent Political Rhetoric, Hate Speech and Stereotyping, and Threats to Democratic Institutions and Values. Using this framework, we measure the spread of online political incivility in Brazil using approximately 5 million tweets posted by 2,307 political influencers during the 2022 Brazilian general election. Through statistical modeling and network analysis, we examine the dynamics of uncivil posts at different election stages, identify key disseminators and audiences, and explore the mechanisms driving the spread of uncivil information online. Our findings indicate that impoliteness is more likely to surge during election campaigns. In contrast, the other dimensions of incivility are often triggered by specific violent events. Moreover, we find that left-aligned individual influencers are the primary disseminators of online incivility in the Brazilian Twitter/X sphere and that they disseminate not only direct incivility but also indirect incivility when discussing or opposing incivility expressed by others. They relay those content from politicians, media agents, and individuals to reach broader audiences, revealing a diffusion pattern mixing the direct and two-step flows of communication theory. This study offers new insights into the multidimensional nature of incivility in Brazilian politics and provides a conceptual framework that can be extended to other political contexts.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO-Learn: Label-Efficient Graph Open-Set Learning</title>
<link>https://arxiv.org/abs/2410.16386</link>
<guid>https://arxiv.org/abs/2410.16386</guid>
<content:encoded><![CDATA[
<div> Graph open-set learning, out-of-distribution detection, label-efficient, graph neural network, K-Medoids.

Summary:
LEGO-Learn is introduced to address the challenge of open-set node classification on graphs with limited labels. It utilizes a GNN-based filter to identify and exclude out-of-distribution nodes, selecting informative in-distribution nodes using K-Medoids algorithm. A C+1 classifier is implemented to differentiate known classes from OOD nodes, employing a weighted cross-entropy loss to balance OOD removal with retaining informative examples. Experimental results on real-world datasets show LEGO-Learn outperforms existing methods, improving ID classification accuracy by up to 6.62% and AUROC for OOD detection by 7.49%. The framework demonstrates significant advancements in graph-based models for recognizing unseen classes while minimizing labeling costs, crucial for high-stakes applications in finance, security, and healthcare.<br /><br />Summary: <div>
arXiv:2410.16386v2 Announce Type: replace-cross 
Abstract: How can we train graph-based models to recognize unseen classes while keeping labeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD) detection aim to address this challenge by training models that can accurately classify known, in-distribution (ID) classes while identifying and handling previously unseen classes during inference. It is critical for high-stakes, real-world applications where models frequently encounter unexpected data, including finance, security, and healthcare. However, current GOL methods assume access to many labeled ID samples, which is unrealistic for large-scale graphs due to high annotation costs. In this paper, we propose LEGO-Learn (Label-Efficient Graph Open-set Learning), a novel framework that tackles open-set node classification on graphs within a given label budget by selecting the most informative ID nodes. LEGO-Learn employs a GNN-based filter to identify and exclude potential OOD nodes and then select highly informative ID nodes for labeling using the K-Medoids algorithm. To prevent the filter from discarding valuable ID examples, we introduce a classifier that differentiates between the C known ID classes and an additional class representing OOD nodes (hence, a C+1 classifier). This classifier uses a weighted cross-entropy loss to balance the removal of OOD nodes while retaining informative ID nodes. Experimental results on four real-world datasets demonstrate that LEGO-Learn significantly outperforms leading methods, with up to a 6.62% improvement in ID classification accuracy and a 7.49% increase in AUROC for OOD detection.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community and hyperedge inference in multiple hypergraphs</title>
<link>https://arxiv.org/abs/2505.04967</link>
<guid>https://arxiv.org/abs/2505.04967</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraphs, interconnections, stochastic block model, community detection, edge prediction 

<br /><br />Summary: Hypergraphs effectively model high-order interactions in biological and social systems. This study focuses on the interconnections between multiple hypergraphs to synthesize integrated information and enhance the understanding of underlying structures. A model is proposed based on the stochastic block model, allowing the integration of data from multiple hypergraphs to reveal latent high-order structures. It addresses the phenomenon of preferential attachment in hyperedges, where certain nodes significantly influence hyperedge formation, introducing a hyperedge internal degree to quantify these contributions. The model demonstrates capabilities in community mining, predicting missing hyperedges of various sizes, and inferring inter-hypergraph edges. The effectiveness of the model is validated through application to high-order datasets, showcasing strong performance in community detection, hyperedge prediction, and inter-hypergraph edge prediction tasks. Additionally, it supports the analysis of multiple hypergraphs of different types and allows for the analysis of a single hypergraph without inter-hypergraph edges. This work presents a practical and flexible tool for analyzing multiple hypergraphs, contributing significantly to the comprehension of organization within real-world high-order systems. <div>
arXiv:2505.04967v1 Announce Type: new 
Abstract: Hypergraphs, capable of representing high-order interactions via hyperedges, have become a powerful tool for modeling real-world biological and social systems. Inherent relationships within these real-world systems, such as the encoding relationship between genes and their protein products, drive the establishment of interconnections between multiple hypergraphs. Here, we demonstrate how to utilize those interconnections between multiple hypergraphs to synthesize integrated information from multiple higher-order systems, thereby enhancing understanding of underlying structures. We propose a model based on the stochastic block model, which integrates information from multiple hypergraphs to reveal latent high-order structures. Real-world hyperedges exhibit preferential attachment, where certain nodes dominate hyperedge formation. To characterize this phenomenon, our model introduces hyperedge internal degree to quantify nodes' contributions to hyperedge formation. This model is capable of mining communities, predicting missing hyperedges of arbitrary sizes within hypergraphs, and inferring inter-hypergraph edges between hypergraphs. We apply our model to high-order datasets to evaluate its performance. Experimental results demonstrate strong performance of our model in community detection, hyperedge prediction, and inter-hypergraph edge prediction tasks. Moreover, we show that our model enables analysis of multiple hypergraphs of different types and supports the analysis of a single hypergraph in the absence of inter-hypergraph edges. Our work provides a practical and flexible tool for analyzing multiple hypergraphs, greatly advancing the understanding of the organization in real-world high-order systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks</title>
<link>https://arxiv.org/abs/2505.04628</link>
<guid>https://arxiv.org/abs/2505.04628</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social capabilities, benchmark, HSII-Dataset, chain of thought

<br /><br />Summary:  
The article addresses the need for large language models (LLMs) to operate independently in complex social settings, rather than just serving as auxiliary assistants for individual communication. It highlights the current lack of systematic measurement for LLM social capabilities and introduces the How Social Is It (HSII) benchmark. This benchmark aims to assess LLMs' abilities in multi-user, multi-turn tasks grounded in sociological principles. HSII includes four evaluation stages: format parsing, target selection, target switching conversation, and stable conversation, which together measure communication and task completion in realistic social scenarios through the HSII-Dataset. The dataset is methodically derived from news sources. The researchers conducted an ablation study utilizing clustering techniques and also explored the impact of the chain of thought (COT) method on LLM social performance. Recognizing the computational cost of COT, they introduced a new metric called COT-complexity to balance correctness and efficiency in evaluating specific social tasks. Overall, the experiments indicate that the HSII benchmark effectively evaluates LLMs' social skills. <div>
arXiv:2505.04628v1 Announce Type: cross 
Abstract: Expanding the application of large language models (LLMs) to societal life, instead of primary function only as auxiliary assistants to communicate with only one person at a time, necessitates LLMs' capabilities to independently play roles in multi-user, multi-turn social agent tasks within complex social settings. However, currently the capability has not been systematically measured with available benchmarks. To address this gap, we first introduce an agent task leveling framework grounded in sociological principles. Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII below), designed to assess LLM's social capabilities in comprehensive social agents tasks and benchmark representative models. HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs within realistic social interaction scenarios dataset, HSII-Dataset. The dataset is derived step by step from news dataset. We perform an ablation study by doing clustering to the dataset. Additionally, we investigate the impact of chain of thought (COT) method on enhancing LLMs' social performance. Since COT cost more computation, we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency. Various results of our experiments demonstrate that our benchmark is well-suited for evaluating social skills in LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections</title>
<link>https://arxiv.org/abs/2505.05459</link>
<guid>https://arxiv.org/abs/2505.05459</guid>
<content:encoded><![CDATA[
<div> Keywords: misleading narratives, elections, taxonomy, UKElectionNarratives, language models  

<br /><br />Summary: This article highlights the significant role of misleading narratives in shaping public opinion during elections, particularly how they affect voters' perceptions of candidates and political parties. To effectively address this issue, the authors introduce the first taxonomy categorizing common misleading narratives observed in recent European elections. Utilizing this taxonomy, they create UKElectionNarratives, a pioneering dataset of human-annotated misleading narratives from the UK General Elections conducted in 2019 and 2024. Furthermore, the study benchmarks various Pre-trained and Large Language Models, with a special focus on GPT-4o, to analyze their performance in detecting election-related misleading narratives. The findings reveal the effectiveness of these models, paving the way for enhanced detection methods. In addition, the authors discuss a range of potential use cases that stem from their research and provide recommendations for future directions, relying on the proposed codebook and dataset to inform further inquiry into misleading narratives during elections. This foundational work aims to improve public discourse and foster more informed voting behavior by addressing the prevalence of misinformation in electoral contexts. <div>
arXiv:2505.05459v1 Announce Type: cross 
Abstract: Misleading narratives play a crucial role in shaping public opinion during elections, as they can influence how voters perceive candidates and political parties. This entails the need to detect these narratives accurately. To address this, we introduce the first taxonomy of common misleading narratives that circulated during recent elections in Europe. Based on this taxonomy, we construct and analyse UKElectionNarratives: the first dataset of human-annotated misleading narratives which circulated during the UK General Elections in 2019 and 2024. We also benchmark Pre-trained and Large Language Models (focusing on GPT-4o), studying their effectiveness in detecting election-related misleading narratives. Finally, we discuss potential use cases and make recommendations for future research directions using the proposed codebook and dataset.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Multipolar Polarization</title>
<link>https://arxiv.org/abs/2405.16352</link>
<guid>https://arxiv.org/abs/2405.16352</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, ideological polarization, multipolar systems, generalized Euclidean distance, quantifying polarization

<br /><br />Summary: The study emphasizes the importance of analyzing social networks to accurately define ideological polarization. It highlights a common limitation in existing methods that often rely on a two-dimensional opinion space, which is inadequate for modeling multipolar systems such as multi-party political environments. This limitation restricts the effectiveness of quantifying polarization in a more nuanced context. The paper presents an experimental comparison of various methods for measuring multipolar polarization in networks. The findings reveal that the average pairwise distance extension of generalized Euclidean distance exhibits several desirable properties when quantifying polarization. This metric proves to be advantageous over other methods due to its empirical accuracy and intuitive understanding. By utilizing this enhanced metric, researchers can more effectively investigate multipolar polarized systems, expanding the scope and robustness of polarization studies in social networks. Overall, the research contributes to a deeper understanding of ideological divides in complex social structures by fostering analytical approaches that accommodate multiple opinion poles rather than relying solely on binary frameworks. <div>
arXiv:2405.16352v2 Announce Type: replace 
Abstract: Studying and understanding social networks is crucial for accurately defining ideological polarization, since they enable precise modeling of social structures. One of the limitations of many methods for quantifying polarization on networks is the assumption of a two-dimensional opinion space. This prevents accurate study of multipolar systems like multi-party political systems, where modeling more than two opinion poles is beneficial. Here, I experimentally compare methods for quantifying multipolar polarization on a network and find that the average pairwise distance extension of generalized Euclidean distance conforms to several desired properties, showing its advantages over other methods. This allows the study of multipolar polarized systems based on an empirically and intuitively good metric.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$k$-local Graphs</title>
<link>https://arxiv.org/abs/2410.00601</link>
<guid>https://arxiv.org/abs/2410.00601</guid>
<content:encoded><![CDATA[
<div> Keywords: locality, coloured graphs, clustering, NP-complete, algorithm

<br /><br />Summary: In 2017, Day et al. introduced the concept of locality as a structural complexity measure for patterns in pattern matching, a field initiated by Angluin in 1980. Subsequently, Casel et al. established in 2019 that determining the locality of an arbitrary pattern is an NP-complete problem. This study expands on these ideas by applying the concept of locality to coloured graphs. The goal is to find an enumeration of colours that allows stepwise colouring of the graph while minimizing the number of clusters created. The authors provide initial theoretical results concerning graph classes and introduce a priority search algorithm designed to compute the $k$-locality of a graph efficiently. This algorithm is optimal in the number of marking prefix expansions and is significantly faster than exhaustive search methods. To demonstrate the practical application and advantages of $k$-locality in knowledge discovery, the authors conduct a case study on a subgraph from the DBLP database. The findings suggest that the proposed approach could enhance clustering in coloured graphs and contribute to various applications in data analysis and pattern recognition. <div>
arXiv:2410.00601v2 Announce Type: replace-cross 
Abstract: In 2017 Day et al. introduced the notion of locality as a structural complexity-measure for patterns in the field of pattern matching established by Angluin in 1980. In 2019 Casel et al. showed that determining the locality of an arbitrary pattern is NP-complete. Inspired by hierarchical clustering, we extend the notion to coloured graphs, i.e., given a coloured graph determine an enumeration of the colours such that colouring the graph stepwise according to the enumeration leads to as few clusters as possible. Next to first theoretical results on graph classes, we propose a priority search algorithm to compute the $k$-locality of a graph. The algorithm is optimal in the number of marking prefix expansions, and is faster by orders of magnitude than an exhaustive search. Finally, we perform a case study on a DBLP subgraph to demonstrate the potential of $k$-locality for knowledge discovery.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning</title>
<link>https://arxiv.org/abs/2501.01031</link>
<guid>https://arxiv.org/abs/2501.01031</guid>
<content:encoded><![CDATA[
<div> Keywords: cultural values, Large Language Models, Retrieval-Augmented Generation, ValuesRAG, World Values Survey

<br /><br />Summary: Ensuring alignment with cultural values in Large Language Models (LLMs) is crucial, as they often reflect Western-centric biases, resulting in misrepresentation and fairness issues. Traditional methods such as role assignment and few-shot learning face limitations due to their dependency on pre-trained knowledge and their inability to encapsulate nuanced cultural values. To tackle these challenges, the authors present ValuesRAG, a novel framework that employs Retrieval-Augmented Generation (RAG) combined with In-Context Learning (ICL) to dynamically integrate cultural and demographic knowledge during text generation. ValuesRAG utilizes the World Values Survey (WVS) dataset to produce summaries of individual values and employs curated regional datasets for testing. The framework retrieves relevant summaries based on demographic features, followed by a reranking process to select the top-k summaries. In evaluations across six diverse regional datasets, ValuesRAG outperformed baseline methods, including zero-shot and few-shot approaches, in various experimental settings. The results emphasize ValuesRAG's effectiveness in creating culturally aligned and inclusive AI systems, highlighting the potential of dynamic retrieval-based techniques to reconcile global LLM capabilities with localized cultural values. <div>
arXiv:2501.01031v3 Announce Type: replace-cross 
Abstract: Ensuring cultural values alignment in Large Language Models (LLMs) remains a critical challenge, as these models often embed Western-centric biases from their training data, leading to misrepresentations and fairness concerns in cross-cultural applications. Existing approaches such as role assignment and few-shot learning struggle to address these limitations effectively due to their reliance on pre-trained knowledge, limited scalability, and inability to capture nuanced cultural values. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with In-Context Learning (ICL) to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. We subsequently curate several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. We evaluate ValuesRAG using 6 diverse regional datasets and show that it consistently outperforms baselines: including zero-shot, role-assignment, few-shot, and hybrid methods, both in main experiments and ablation settings. Notably, ValuesRAG achieves the best overall performance over prior methods, demonstrating its effectiveness in fostering culturally aligned and inclusive AI systems. Our findings underscore the potential of dynamic retrieval-based methods to bridge the gap between global LLM capabilities and localized cultural values.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework</title>
<link>https://arxiv.org/abs/2505.03746</link>
<guid>https://arxiv.org/abs/2505.03746</guid>
<content:encoded><![CDATA[
<div> Keywords: Social media, Cyberbullying detection, Machine Learning, Large Language Models, Explainability dashboard

Summary:
Our proposed solution for cyberbullying detection utilizes stream-based Machine Learning models combined with Large Language Models for feature engineering. By processing incoming samples incrementally, our system addresses the evolving nature of online abusive and hate speech. An explainability dashboard enhances trustworthiness and reliability, promoting accountability. Experimental results show promising performance close to 90% across all evaluation metrics, surpassing existing works in the literature. This real-time solution contributes to the safety of online communities by detecting abusive behavior promptly, preventing long-lasting harassment, and reducing negative consequences in society. 

<br /><br />Summary: <div>
arXiv:2505.03746v1 Announce Type: new 
Abstract: Social media platforms enable instant and ubiquitous connectivity and are essential to social interaction and communication in our technological society. Apart from its advantages, these platforms have given rise to negative behaviors in the online community, the so-called cyberbullying. Despite the many works involving generative Artificial Intelligence (AI) in the literature lately, there remain opportunities to study its performance apart from zero/few-shot learning strategies. Accordingly, we propose an innovative and real-time solution for cyberbullying detection that leverages stream-based Machine Learning (ML) models able to process the incoming samples incrementally and Large Language Models (LLMS) for feature engineering to address the evolving nature of abusive and hate speech online. An explainability dashboard is provided to promote the system's trustworthiness, reliability, and accountability. Results on experimental data report promising performance close to 90 % in all evaluation metrics and surpassing those obtained by competing works in the literature. Ultimately, our proposal contributes to the safety of online communities by timely detecting abusive behavior to prevent long-lasting harassment and reduce the negative consequences in society.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing</title>
<link>https://arxiv.org/abs/2505.03769</link>
<guid>https://arxiv.org/abs/2505.03769</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-platform social media, user engagement, text-visual content, title rewriting, quantitative analysis

Summary: 
This study explores the impact of rewriting Reddit post titles adapted from YouTube video titles on user engagement for multimodal content. An analysis of a large dataset of Reddit posts sharing YouTube videos reveals that modified titles significantly improve engagement. A controlled experiment isolates the effects of textual variations and identifies key factors that enhance engagement, such as emotional resonance, lexical richness, and alignment with community norms. Statistical tests confirm the impact of effective title rewrites on engagement, with a fine-tuned BERT classifier achieving high accuracy in predicting user preferences. By combining quantitative rigor with qualitative insights, this study provides valuable insights into engagement dynamics and presents a robust framework for enhancing cross-platform, multimodal content strategies. 

<br /><br />Summary: <div>
arXiv:2505.03769v1 Announce Type: new 
Abstract: In today's cross-platform social media landscape, understanding factors that drive engagement for multimodal content, especially text paired with visuals, remains complex. This study investigates how rewriting Reddit post titles adapted from YouTube video titles affects user engagement. First, we build and analyze a large dataset of Reddit posts sharing YouTube videos, revealing that 21% of post titles are minimally modified. Statistical analysis demonstrates that title rewrites measurably improve engagement. Second, we design a controlled, multi-phase experiment to rigorously isolate the effects of textual variations by neutralizing confounding factors like video popularity, timing, and community norms. Comprehensive statistical tests reveal that effective title rewrites tend to feature emotional resonance, lexical richness, and alignment with community-specific norms. Lastly, pairwise ranking prediction experiments using a fine-tuned BERT classifier achieves 74% accuracy, significantly outperforming near-random baselines, including GPT-4o. These results validate that our controlled dataset effectively minimizes confounding effects, allowing advanced models to both learn and demonstrate the impact of textual features on engagement. By bridging quantitative rigor with qualitative insights, this study uncovers engagement dynamics and offers a robust framework for future cross-platform, multimodal content strategies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Content Moderation Lead Users Away from Fringe Movements? Evidence from a Recovery Community</title>
<link>https://arxiv.org/abs/2505.03772</link>
<guid>https://arxiv.org/abs/2505.03772</guid>
<content:encoded><![CDATA[
<div> community, exredpill, Reddit, Manosphere, content moderation

Summary:<br />
The study examines the impact of banning and quarantining radical communities within the Manosphere on the exredpill recovery community on Reddit. Banning these communities led to increased participation in exredpill, while quarantining had no effect. The effect of banning was stronger than real-world events related to the Manosphere. Moderation actions did not result in a spike in toxicity or malicious activity in exredpill, indicating that content moderation acts as a deradicalization catalyst. The findings suggest that sanctions on fringe movements linked to hate speech, violence, and terrorism can contribute to individuals abandoning these movements. <div>
arXiv:2505.03772v1 Announce Type: new 
Abstract: Online platforms have sanctioned individuals and communities associated with fringe movements linked to hate speech, violence, and terrorism, but can these sanctions contribute to the abandonment of these movements? Here, we investigate this question through the lens of exredpill, a recovery community on Reddit meant to help individuals leave movements within the Manosphere, a conglomerate of fringe Web based movements focused on men's issues. We conduct an observational study on the impact of sanctioning some of Reddit's largest Manosphere communities on the activity levels and user influx of exredpill, the largest associated recovery subreddit. We find that banning a related radical community positively affects participation in exredpill in the period following the ban. Yet, quarantining the community, a softer moderation intervention, yields no such effects. We show that the effect induced by banning a radical community is stronger than for some of the widely discussed real-world events related to the Manosphere and that moderation actions against the Manosphere do not cause a spike in toxicity or malicious activity in exredpill. Overall, our findings suggest that content moderation acts as a deradicalization catalyst.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Media and Academia: How Gender Influences Online Scholarly Discourse</title>
<link>https://arxiv.org/abs/2505.03773</link>
<guid>https://arxiv.org/abs/2505.03773</guid>
<content:encoded><![CDATA[
<div> AI innovation, gender, social media, communication, academics
Summary:<br /><br />This study examines gender differences in online communication patterns of academics in computer science at the top 20 USA universities on the social media platform X. Men tend to post more about AI innovation, society, and machine learning, while women focus more on engaging AI events. Women express stronger emotions in their tweets, with certain emotions linked to specific topics. Female academics display more empathy and discuss personal experiences. However, both genders show consistency in factors like self-praise and politeness. Female academics receive more toxic and threatening replies online, indicating a need for a more inclusive environment for scholarly engagement. This research underscores the influence of gender on shaping academics' online communication and highlights the importance of addressing disparities in online interactions. 
Summary: <div>
arXiv:2505.03773v1 Announce Type: new 
Abstract: This study investigates gender-based differences in online communication patterns of academics, focusing on how male and female academics represent themselves and how users interact with them on the social media platform X (formerly Twitter). We collect historical Twitter data of academics in computer science at the top 20 USA universities and analyze their tweets, retweets, and replies to uncover systematic patterns such as discussed topics, engagement disparities, and the prevalence of negative language or harassment. The findings indicate that while both genders discuss similar topics, men tend to post more tweets about AI innovation, current USA society, machine learning, and personal perspectives, whereas women post slightly more on engaging AI events and workshops. Women express stronger positive and negative sentiments about various events compared to men. However, the average emotional expression remains consistent across genders, with certain emotions being more strongly associated with specific topics. Writing-style analysis reveals that female academics show more empathy and are more likely to discuss personal problems and experiences, with no notable differences in other factors, such as self-praise, politeness, and stereotypical comments. Analyzing audience responses indicates that female academics are more frequently subjected to severe toxic and threatening replies. Our findings highlight the impact of gender in shaping the online communication of academics and emphasize the need for a more inclusive environment for scholarly engagement.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics</title>
<link>https://arxiv.org/abs/2505.03795</link>
<guid>https://arxiv.org/abs/2505.03795</guid>
<content:encoded><![CDATA[
<div> Keywords: Human networks, strategic game, modeling methods, community-aware behavior, user study

Summary:
The study examines various methods for modeling human behavior in a strategic network game, the Junior High Game (JHG). By comparing different modeling approaches, the researchers find that a model called hCAB, which considers community-aware behavior and population distribution, outperforms other methods. When applied to small groups, hCAB closely reflects the dynamics of human populations. A user study shows that participants could not distinguish hCAB agents from real humans, indicating that hCAB effectively mirrors human behavior in the game. Understanding human networks is crucial for societal outcomes, and the study provides insights into how to promote favorable outcomes by modeling human behavior accurately in strategic games. <div>
arXiv:2505.03795v1 Announce Type: new 
Abstract: Human networks greatly impact important societal outcomes, including wealth and health inequality, poverty, and bullying. As such, understanding human networks is critical to learning how to promote favorable societal outcomes. As a step toward better understanding human networks, we compare and contrast several methods for learning models of human behavior in a strategic network game called the Junior High Game (JHG). These modeling methods differ with respect to the assumptions they use to parameterize human behavior (behavior vs. community-aware behavior) and the statistical moments they model (mean vs. distribution). Results show that the highest-performing method models the population's distribution rather than the mean and assumes humans use community-aware behavior rather than behavior matching. When applied to small societies (6-11 individuals), this learned model, called hCAB, closely mirrors the population dynamics of human groups (with some differences). Additionally, a user study reveals that human participants were unable to distinguish hCAB agents from other humans, thus illustrating that individual hCAB behavior plausibly mirrors human behavior in this strategic network game.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Climate Change Landscape on TikTok</title>
<link>https://arxiv.org/abs/2505.03813</link>
<guid>https://arxiv.org/abs/2505.03813</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, climate action discourse, social media, lifestyle choices, dietary choices <br />
Summary: 
This study focuses on the impact of social media platforms, specifically TikTok, on climate action discourse. The researchers collected a dataset of 590K videos from 14K creators on TikTok discussing climate topics. Using topic modeling, they identified that creators mainly address climate issues through the lens of lifestyle and dietary choices. By creating a climate taxonomy, they mapped the topics discussed on the platform and discovered non-climate "gateway" topics that could attract new audiences to climate discussions. This study highlights the importance of understanding how social media platforms influence the conversation around climate change and the potential for leveraging TikTok to engage a younger, environmentally-conscious audience. <br /><br /> <div>
arXiv:2505.03813v1 Announce Type: new 
Abstract: Social media platforms shape climate action discourse. Mapping these online conversations is essential for effective communication strategies. TikTok's climate discussions are particularly relevant given its young, climate-concerned audience. In this work, we collect the first TikTok dataset on climate topics. We collected 590K videos from 14K creators along with their follower networks. By applying topic modeling to the video descriptions, we map the topics discussed on the platform on a climate taxonomy that we construct by consolidating existing categorizations. Results show TikTok creators primarily approach climate through the angle of lifestyle and dietary choices. By examining semantic connections between topics, we identified non-climate "gateway" topics that could draw new audiences into climate discussions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries</title>
<link>https://arxiv.org/abs/2505.03816</link>
<guid>https://arxiv.org/abs/2505.03816</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban transportation, NYC Taxi Trip dataset, Pathao Food Trip dataset, demand patterns, geospatial analysis 

Summary: 
This study utilizes the NYC Taxi Trip dataset from New York City and the Pathao Food Trip dataset from Dhaka, Bangladesh to analyze transportation patterns. Through Exploratory Data Analysis (EDA), the study aims to identify key trends in demand, peak times, and important geographical hotspots. Geospatial analysis is conducted to map out high-demand and low-demand regions, while the SARIMAX model is used for time series analysis to forecast demand patterns. Clustering techniques are applied to identify significant areas of high and low demand. The findings offer valuable insights for optimizing fleet management and resource allocation in both passenger transport and food delivery services. These insights can help enhance urban transportation systems, improve service efficiency, and better meet customer needs in diverse urban environments.<br /><br />Summary: This study analyzes transportation patterns using datasets from New York City and Dhaka, focusing on demand trends, peak times, and geographical hotspots. Through EDA, geospatial analysis, time series analysis, and clustering techniques, the study provides insights to optimize fleet management and resource allocation in urban transportation services, ultimately enhancing service efficiency and urban transportation systems. <div>
arXiv:2505.03816v1 Announce Type: new 
Abstract: Urban transportation plays a vital role in modern city life, affecting how efficiently people and goods move around. This study analyzes transportation patterns using two datasets: the NYC Taxi Trip dataset from New York City and the Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify key trends in demand, peak times, and important geographical hotspots. We start with Exploratory Data Analysis (EDA) to understand the basic characteristics of the datasets. Next, we perform geospatial analysis to map out high-demand and low-demand regions. We use the SARIMAX model for time series analysis to forecast demand patterns, capturing seasonal and weekly variations. Lastly, we apply clustering techniques to identify significant areas of high and low demand. Our findings provide valuable insights for optimizing fleet management and resource allocation in both passenger transport and food delivery services. These insights can help improve service efficiency, better meet customer needs, and enhance urban transportation systems in diverse urban environments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event-aware analysis of cross-city visitor flows using large language models and social media data</title>
<link>https://arxiv.org/abs/2505.03847</link>
<guid>https://arxiv.org/abs/2505.03847</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-city visitor flows, public events, social media, machine learning, transport policies <br />
Summary: <br />
This study introduces a framework to analyze cross-city visitor flows during public events using large language models and social media data. By extracting event features from online information, a machine learning model predicts visitor flows with high accuracy. The research focuses on Hong Kong and explores the impacts of different event types on visitor numbers, highlighting the importance of promotional and word-of-mouth popularity. The study emphasizes the need for coordinated measures across government agencies and specialized transport policies to manage surges in travel demand during events. Promotional and word-of-mouth popularity have varying effects on visitor flows, particularly among metro and high-speed rail users. These findings can inform strategies such as shuttle services and traffic management to optimize transportation during public events. <br /> <div>
arXiv:2505.03847v1 Announce Type: new 
Abstract: Public events, such as music concerts and fireworks displays, can cause irregular surges in cross-city travel demand, leading to potential overcrowding, travel delays, and public safety concerns. To better anticipate and accommodate such demand surges, it is essential to estimate cross-city visitor flows with awareness of public events. Although prior studies typically focused on the effects of a single mega event or disruptions around a single venue, this study introduces a generalizable framework to analyze visitor flows under diverse and concurrent events. We propose to leverage large language models (LLMs) to extract event features from multi-source online information and massive user-generated content on social media platforms. Specifically, social media popularity metrics are designed to capture the effects of online promotion and word-of-mouth in attracting visitors. An event-aware machine learning model is then adopted to uncover the specific impacts of different event features and ultimately predict visitor flows for upcoming events. Using Hong Kong as a case study, the framework is applied to predict daily flows of mainland Chinese visitors arriving at the city, achieving a testing R-squared of over 85%. We further investigate the heterogeneous event impacts on visitor numbers across different event types and major travel modes. Both promotional popularity and word-of-mouth popularity are found to be associated with increased visitor flows, but the specific effects vary by the event type. This association is more pronounced among visitors arriving by metro and high-speed rail, while it has less effect on air travelers. The findings can facilitate coordinated measures across government agencies and guide specialized transport policies, such as shuttle transit services to event venues, and comprehensive on-site traffic management strategies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Appeal and Scope of Misinformation Spread by AI Agents and Humans</title>
<link>https://arxiv.org/abs/2505.04028</link>
<guid>https://arxiv.org/abs/2505.04028</guid>
<content:encoded><![CDATA[
<div> misinformation, AI agents, social network platforms, COVID-19 vaccine discourse, tweet engagement

Summary: 
- The study examines the impact of misinformation and AI agents on social network platforms, focusing on COVID-19 vaccine discourse.
- Two new metrics, Appeal and Scope, are proposed to quantify the influence of misinformation based on tweet engagement and user network position.
- Analysis of 5.8 million misinformation tweets across three time periods reveals higher prevalence of misinformation during the Pre-Vaccine and Vaccine Launch periods.
- Human-generated misinformation tweets show higher appeal and scope compared to bot-generated ones.
- Tweedie regression analysis highlights human-generated misinformation as more concerning during the Vaccine Launch week, while bot-generated misinformation reaches peak appeal and scope during the Pre-Vaccine period. 

<br /><br />Summary: <div>
arXiv:2505.04028v1 Announce Type: new 
Abstract: This work examines the influence of misinformation and the role of AI agents, called bots, on social network platforms. To quantify the impact of misinformation, it proposes two new metrics based on attributes of tweet engagement and user network position: Appeal, which measures the popularity of the tweet, and Scope, which measures the potential reach of the tweet. In addition, it analyzes 5.8 million misinformation tweets on the COVID-19 vaccine discourse over three time periods: Pre-Vaccine, Vaccine Launch, and Post-Vaccine. Results show that misinformation was more prevalent during the first two periods. Human-generated misinformation tweets tend to have higher appeal and scope compared to bot-generated ones. Tweedie regression analysis reveals that human-generated misinformation tweets were most concerning during Vaccine Launch week, whereas bot-generated misinformation reached its highest appeal and scope during the Pre-Vaccine period.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delegation and Participation in Decentralized Governance: An Epistemic View</title>
<link>https://arxiv.org/abs/2505.04136</link>
<guid>https://arxiv.org/abs/2505.04136</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized governance, epistemic tests, transfer delegation, direct participation, DAOs 

Summary:
Decentralized governance methods are evaluated based on epistemic tests, focusing on the ability to reach correct outcomes. Partial abstention emerges as a strong governance method, outperforming transfer delegation where voters transfer voting rights to others. Multi-step transfer delegation shows promise but is not without epistemic weaknesses. Enhanced direct participation can have negative epistemic impacts unless certain conditions are met. Additional direct participation can be beneficial under specific governance conditions, increasing the likelihood of correct decisions. The study also suggests the potential use of prediction markets, auctions, and AI agents to enhance decentralized governance outcomes. These findings are crucial for decentralized autonomous organizations (DAOs) looking to compete with centralized organizations, highlighting the importance of epistemic performance in decision-making processes. 

<br /><br />Summary: 
Decentralized governance methods, including partial abstention and transfer delegation, are examined through epistemic tests to assess their ability to reach correct outcomes. While partial abstention proves to be a strong method, transfer delegation has inherent epistemic weaknesses. Enhanced direct participation can have both positive and negative epistemic impacts, depending on governance conditions. Additional direct participation can improve decision-making under certain circumstances. To enhance outcomes, the study suggests the potential use of prediction markets, auctions, and AI agents in decentralized governance. These findings are essential for DAOs aiming to compete with centralized organizations, emphasizing the significance of epistemic performance in governance processes. <div>
arXiv:2505.04136v1 Announce Type: new 
Abstract: We develop and apply epistemic tests to various decentralized governance methods as well as to study the impact of participation. These tests probe the ability to reach a correct outcome when there is one. We find that partial abstention is a strong governance method from an epistemic standpoint compared to alternatives such as various forms of ``transfer delegation" in which voters explicitly transfer some or all of their voting rights to others. We make a stronger case for multi-step transfer delegation than is present in previous work but also demonstrate that transfer delegation has inherent epistemic weaknesses. We show that enhanced direct participation, voters exercising their own voting rights, can have a variety of epistemic impacts, some very negative. We identify governance conditions under which additional direct participation is guaranteed to do no epistemic harm and is likely to increase the probability of making correct decisions. In light of the epistemic challenges of voting-based decentralized governance, we consider the possible supplementary use of prediction markets, auctions, and AI agents to improve outcomes. All these results are significant because epistemic performance matters if entities such as DAOs (decentralized autonomous organizations) wish to compete with organizations that are more centralized.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random walks with resetting on hypergraph</title>
<link>https://arxiv.org/abs/2505.04215</link>
<guid>https://arxiv.org/abs/2505.04215</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergraph, random walks with resetting, spectral theory, eigenvalues, node ranking

Summary:
Random walks with resetting on hypergraphs are analyzed using spectral theory, with key parameters like occupation probability and mean first passage time expressed in terms of transition matrix eigenvalues. An optimal reset probability condition and a condition for its existence are derived. The relationship between random walks on hypergraphs and simple random walks is established, showing that hypergraph eigenvalues can be represented using graph eigenvalues. A new research framework is proposed that considers the intrinsic structure of hypergraphs, improving node ranking accuracy over traditional methods by assigning proper weights to neighboring nodes. The impact of resetting mechanisms on cover time is explored, offering potential solutions for optimizing search efficiency.	Extensive experiments demonstrate the effectiveness of this framework in producing reliable results. 

<br /><br />Summary: <div>
arXiv:2505.04215v1 Announce Type: new 
Abstract: Hypergraph has been selected as a powerful candidate for characterizing higher-order networks and has received
  increasing attention in recent years. In this article, we study random walks with resetting on hypergraph by utilizing
  spectral theory. Specifically, we derive exact expressions for some fundamental yet key parameters, including occupation
  probability, stationary distribution, and mean first passage time, all of which are expressed in terms of the eigenvalues
  and eigenvectors of the transition matrix. Furthermore, we provide a general condition for determining the optimal
  reset probability and a sufficient condition for its existence. In addition, we build up a close relationship between
  random walks with resetting on hypergraph and simple random walks. Concretely, the eigenvalues and eigenvectors
  of the former can be precisely represented by those of the latter. More importantly, when considering random walks,
  we abandon the traditional approach of converting hypergraph into a graph and propose a research framework that
  preserves the intrinsic structure of hypergraph itself, which is based on assigning proper weights to neighboring nodes.
  Through extensive experiments, we show that the new framework produces distinct and more reliable results than
  the traditional approach in node ranking. Finally, we explore the impact of the resetting mechanism on cover time,
  providing a potential solution for optimizing search efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Flowers to Fascism? The Cottagecore to Tradwife Pipeline on Tumblr</title>
<link>https://arxiv.org/abs/2505.04561</link>
<guid>https://arxiv.org/abs/2505.04561</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, aesthetic-based radicalization, Tradwife, Cottagecore, extremism 

Summary:
The study examined the potential for aesthetic-based radicalization on social media platforms, specifically focusing on the intersection of Cottagecore and Tradwife content. While explicit radicalization was not found, there was evidence of a mainstreaming effect as the two communities overlapped. Surprisingly, there was unexpected interaction between queer identities and Tradwives, with some Tradwives even incorporating queer individuals and denouncing racism within their community. This could indicate a re-branding of extremist content to align with platform norms. A temporal analysis showed a shift in the central tags used by Tradwives towards reactionary ideals post-2021, moving from aesthetics and hobbies to focus on religion, traditional gender roles, and homesteading. This suggests a potential shift in the ideologies promoted within the community. Overall, the study highlights the complex dynamics at play within online communities and the need for continued monitoring of extremist content. 

<br /><br />Summary: <div>
arXiv:2505.04561v1 Announce Type: new 
Abstract: In this work we collected and analyzed social media posts to investigate aesthetic-based radicalization where users searching for Cottagecore content may find Tradwife content co-opted by white supremacists, white nationalists, or other far-right extremist groups. Through quantitative analysis of over 200,000 Tumblr posts and qualitative coding of about 2,500 Tumblr posts, we did not find evidence of a explicit radicalization. We found that problematic Tradwife posts found in the literature may be confined to Tradwife-only spaces, while content in the Cottagecore tag generally did not warrant extra moderation. However, we did find evidence of a mainstreaming effect in the overlap between the Tradwife and Cottagecore communities. In our qualitative analysis there was more interaction between queer and Tradwife identities than expected based on the literature, and some Tradwives even explicitly included queer people and disavowed racism in the Tradwife community on Tumblr. This could be genuine, but more likely it was an example of extremists re-branding their content and following platform norms to spread ideologies that would otherwise be rejected by Tumblr users. Additionally, through temporal analysis we observed a change in the central tags used by Tradwives in the Cottagecore tag pre- and post- 2021. Initially these posts focused on aesthetics and hobbies like baking and gardening, but post-2021 the central tags focused more on religion, traditional gender roles, and homesteading, all markers of reactionary ideals.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation</title>
<link>https://arxiv.org/abs/2505.03774</link>
<guid>https://arxiv.org/abs/2505.03774</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, OOD detection, heterogeneous graphs, energy propagation, meta-path-based<br />
Summary:<br />
- The study addresses the challenge of out-of-distribution (OOD) node detection in heterogeneous graphs, which are common in real-world scenarios.<br />
- A novel methodology, OODHG, is proposed to detect OOD nodes and classify in-distribution (ID) nodes based on the detection results.<br />
- OODHG utilizes energy values and a meta-path-based energy propagation mechanism to differentiate between ID and OOD nodes effectively.<br />
- The approach is shown to outperform baseline models in OOD detection tasks and accurately classify ID nodes in heterogeneous graphs.<br />
- The method's simplicity and effectiveness make it a promising solution for OOD detection in complex, real-world graph data. <br /> <div>
arXiv:2505.03774v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node and structural information from graph data. While current GNNs perform well in node classification tasks within in-distribution (ID) settings, real-world scenarios often present distribution shifts, leading to the presence of out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and challenging task. Most existing research focuses on homogeneous graphs, but real-world graphs are often heterogeneous, consisting of diverse node and edge types. This heterogeneity adds complexity and enriches the informational content. To the best of our knowledge, OOD detection in heterogeneous graphs remains an underexplored area. In this context, we propose a novel methodology for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the first task's results. Specifically, we learn representations for each node in the heterogeneous graph, calculate energy values to determine whether nodes are OOD, and then classify ID nodes. To leverage the structural information of heterogeneous graphs, we introduce a meta-path-based energy propagation mechanism and an energy constraint to enhance the distinction between ID and OOD nodes. Extensive experimental findings substantiate the simplicity and effectiveness of OODHG, demonstrating its superiority over baseline models in OOD detection tasks and its accuracy in ID node classification.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Causal Effects in Networks with Cluster-Based Bandits</title>
<link>https://arxiv.org/abs/2505.04200</link>
<guid>https://arxiv.org/abs/2505.04200</guid>
<content:encoded><![CDATA[
<div> Keywords: causal effects, randomized controlled trial, A/B testing, multi-armed bandit, social networks<br />
Summary:<br />
The article discusses the challenges of estimating causal effects in the presence of interference, particularly in social networks. It highlights the importance of adapting strategies over time to efficiently learn the total treatment effect while balancing exploration and exploitation. Two cluster-based multi-armed bandit algorithms are introduced to address these challenges and maximize expected rewards. The performance of these algorithms is compared with vanilla MAB algorithms and traditional RCT methods on semi-synthetic data with simulated interference. The results show that the cluster-based MAB algorithms outperform the vanilla MAB algorithms, offering a higher reward-action ratio without sacrificing accuracy in treatment effect estimation. This research provides valuable insights into optimizing A/B testing strategies in network settings with interference. <br />Summary: <div>
arXiv:2505.04200v1 Announce Type: cross 
Abstract: The gold standard for estimating causal effects is randomized controlled trial (RCT) or A/B testing where a random group of individuals from a population of interest are given treatment and the outcome is compared to a random group of individuals from the same population. However, A/B testing is challenging in the presence of interference, commonly occurring in social networks, where individuals can impact each others outcome. Moreover, A/B testing can incur a high performance loss when one of the treatment arms has a poor performance and the test continues to treat individuals with it. Therefore, it is important to design a strategy that can adapt over time and efficiently learn the total treatment effect in the network. We introduce two cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the total treatment effect in a network while maximizing the expected reward by making a tradeoff between exploration and exploitation. We compare the performance of our MAB algorithms with a vanilla MAB algorithm that ignores clusters and the corresponding RCT methods on semi-synthetic data with simulated interference. The vanilla MAB algorithm shows higher reward-action ratio at the cost of higher treatment effect error due to undesired spillover. The cluster-based MAB algorithms show higher reward-action ratio compared to their corresponding RCT methods without sacrificing much accuracy in treatment effect estimation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2505.04461</link>
<guid>https://arxiv.org/abs/2505.04461</guid>
<content:encoded><![CDATA[
<div> Graphs, Temporal interaction, Representation learning, Downstream tasks, Temporal dependencies  
Summary:  
Temporal interaction graphs (TIGs) are essential in modeling complex dynamic system behaviors. Temporal interaction graph representation learning (TIGRL) plays a crucial role in embedding nodes in TIGs into low-dimensional representations that preserve structural and temporal information. This enhances performance in classification, prediction, and clustering tasks within evolving data environments. This paper introduces TIG concepts, highlights temporal dependencies' significance, and categorizes state-of-the-art TIGRL methods based on the information utilized during learning. It also provides datasets and benchmarks sources for empirical investigations. Key open challenges and promising research directions in TIGRL are discussed, setting the stage for future advancements in the field.  
<br /><br />Summary: <div>
arXiv:2505.04461v1 Announce Type: cross 
Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Garden city: A synthetic dataset and sandbox environment for analysis of pre-processing algorithms for GPS human mobility data</title>
<link>https://arxiv.org/abs/2412.00913</link>
<guid>https://arxiv.org/abs/2412.00913</guid>
<content:encoded><![CDATA[
<div> Keywords: Human mobility datasets, sparsity, processing algorithms, synthetic trajectory simulator, open-source code

Summary: 
The article discusses the challenges associated with human mobility datasets, particularly regarding the high sparsity in commercial datasets that can lead to errors in processing algorithms. These errors may impact the accuracy of downstream results derived from such datasets, making it crucial to validate and calibrate the algorithms effectively. To address these issues, the authors propose a synthetic trajectory simulator and sandbox environment that replicates key features of commercial datasets to test processing algorithms effectively. By comparing algorithm outputs with "ground-truth" synthetic trajectories and mobility diaries, researchers can enhance the robustness of their analyses. The open-source code provided by the authors facilitates the use of this simulator, making it accessible to the wider research community for evaluation and validation of processing algorithms.<br /><br />Summary: <div>
arXiv:2412.00913v2 Announce Type: replace 
Abstract: Human mobility datasets have seen increasing adoption in the past decade, enabling diverse applications that leverage the high precision of measured trajectories relative to other human mobility datasets. However, there are concerns about whether the high sparsity in some commercial datasets can introduce errors due to lack of robustness in processing algorithms, which could compromise the validity of downstream results. The scarcity of "ground-truth" data makes it particularly challenging to evaluate and calibrate these algorithms. To overcome these limitations and allow for an intermediate form of validation of common processing algorithms, we propose a synthetic trajectory simulator and sandbox environment meant to replicate the features of commercial datasets that could cause errors in such algorithms, and which can be used to compare algorithm outputs with "ground-truth" synthetic trajectories and mobility diaries. Our code is open-source and is publicly available alongside tutorial notebooks and sample datasets generated with it.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TikTok's recommendations skewed towards Republican content during the 2024 U.S. presidential race</title>
<link>https://arxiv.org/abs/2501.17831</link>
<guid>https://arxiv.org/abs/2501.17831</guid>
<content:encoded><![CDATA[
<div> Algorithmic audit, TikTok, political biases, partisan content recommendations, social media platforms

Summary: 
The study focuses on investigating TikTok's recommendation algorithm for political biases by conducting 323 independent algorithmic audit experiments. The research reveals significant disparities in content distribution, with Republican-seeded accounts receiving more party-aligned recommendations compared to Democratic-seeded accounts. Furthermore, Democratic-seeded accounts were exposed to more opposite-party recommendations on average. These disparities exist across different states and persist even when accounting for engagement metrics. The study highlights the influence of negative partisanship content on the recommendation algorithm during a critical election period, raising concerns about platform neutrality. <div>
arXiv:2501.17831v2 Announce Type: replace 
Abstract: TikTok is a major force among social media platforms with over a billion monthly active users worldwide and 170 million in the United States. The platform's status as a key news source, particularly among younger demographics, raises concerns about its potential influence on politics in the U.S. and globally. Despite these concerns, there is scant research investigating TikTok's recommendation algorithm for political biases. We fill this gap by conducting 323 independent algorithmic audit experiments testing partisan content recommendations in the lead-up to the 2024 U.S. presidential elections. Specifically, we create hundreds of "sock puppet" TikTok accounts in Texas, New York, and Georgia, seeding them with varying partisan content and collecting algorithmic content recommendations for each of them. Collectively, these accounts viewed ~394,000 videos from April 30th to November 11th, 2024, which we label for political and partisan content. Our analysis reveals significant asymmetries in content distribution: Republican-seeded accounts received ~11.8% more party-aligned recommendations compared to their Democratic-seeded counterparts, and Democratic-seeded accounts were exposed to ~7.5% more opposite-party recommendations on average. These asymmetries exist across all three states and persist when accounting for video- and channel-level engagement metrics such as likes, views, shares, comments, and followers, and are driven primarily by negative partisanship content. Our findings provide insights into the inner workings of TikTok's recommendation algorithm during a critical election period, raising fundamental questions about platform neutrality.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-IDS: Doubly Disentangled Dynamic Intrusion Detection</title>
<link>https://arxiv.org/abs/2307.11079</link>
<guid>https://arxiv.org/abs/2307.11079</guid>
<content:encoded><![CDATA[
<div> Keywords: Network-based intrusion detection system, feature disentanglement, dynamic graph diffusion, unknown threats, explainability<br />
Summary: 
The article introduces a novel method called 3D-IDS for enhancing the performance of Network-based Intrusion Detection Systems (NIDS). Existing methods are found to be inconsistent in detecting various known and unknown attacks due to entangled distributions of flow features. 3D-IDS addresses these issues through two-step feature disentanglements and a dynamic graph diffusion scheme. The method first disentangles traffic features using mutual information optimization, then uses a memory model to generate representations highlighting attack-specific features. A novel graph diffusion method is employed for spatial-temporal aggregation in evolving data streams. Experimental results demonstrate the effectiveness of 3D-IDS in identifying various attacks, including unknown threats and hard-to-detect known attacks. The two-step feature disentanglements also contribute to improving the explainability of NIDS. <div>
arXiv:2307.11079v3 Announce Type: replace-cross 
Abstract: Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Approximate-Master-Equation Formulation of the Watts Threshold Model on Hypergraphs</title>
<link>https://arxiv.org/abs/2503.04020</link>
<guid>https://arxiv.org/abs/2503.04020</guid>
<content:encoded><![CDATA[
<div> Keywords: behavioral dynamics, social networks, hypergraphs, Watts threshold model, approximate master equations

Summary:
This study explores behavioral dynamics on social networks, considering interactions among groups of individuals in addition to pairs. By extending the Watts threshold model to hypergraphs and using approximate master equations, the researchers develop a continuous-time model with high accuracy. They simplify the model to a system of three differential equations for better computational efficiency and interpretability. Through linearization, they identify conditions for large spreading events. Applying the model to real-world networks, such as a French primary school and computer-science coauthorships hypergraph, demonstrates its accuracy. The study suggests that incorporating structural correlations into future models will enhance accuracy for real-world networks. The research provides insights into polyadic interactions in social dynamics, highlighting the importance of considering group dynamics in addition to pairwise interactions. 

<br /><br />Summary: <div>
arXiv:2503.04020v2 Announce Type: replace-cross 
Abstract: In traditional models of behavioral or opinion dynamics on social networks, researchers suppose that all interactions occur between pairs of individuals. However, in reality, social interactions also occur in groups of three or more individuals. A common way to incorporate such polyadic interactions is to study dynamical processes on hypergraphs. In a hypergraph, interactions can occur between any number of the individuals in a network. The Watts threshold model (WTM) is a well-known model of a simplistic social spreading process. Very recently, Chen et al. extended the WTM from dyadic networks (i.e., graphs) to polyadic networks (i.e., hypergraphs). In the present paper, we extend their discrete-time model to continuous time using approximate master equations (AMEs). By using AMEs, we are able to model the system with very high accuracy. We then reduce the high-dimensional AME system to a system of three coupled differential equations without any detectable loss of accuracy. This much lower-dimensional system is more computationally efficient to solve numerically and is also easier to interpret. We linearize the reduced AME system and calculate a cascade condition, which allows us to determine when a large spreading event occurs. We then apply our model to a social contact network of a French primary school and to a hypergraph of computer-science coauthorships. We find that the AME system is accurate in modelling the polyadic WTM on these empirical networks; however, we expect that future work that incorporates structural correlations between nearby nodes and groups into the model for the dynamics will lead to more accurate theory for real-world networks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Troika algorithm: approximate optimization for accurate clique partitioning and clustering of weighted networks</title>
<link>https://arxiv.org/abs/2505.03573</link>
<guid>https://arxiv.org/abs/2505.03573</guid>
<content:encoded><![CDATA[
<div> clique partitioning, network clustering, Troika, community detection, portfolio analysis
<br />
Summary:
Troika is a new approximation algorithm designed for clique partitioning in network clustering tasks. It efficiently solves this NP-hard problem for small to mid-sized networks, delivering solutions with guaranteed proximity to global optimality. Compared to alternatives like integer programming solvers and heuristics, Troika is faster and more accurate, offering reliable results within a user-specified optimality gap tolerance. The algorithm's applications extend to community detection and portfolio analysis, where it outperforms modularity-based algorithms and showcases dynamic changes in portfolio networks during significant events like the financial crisis of 2008 and the COVID-19 pandemic. With successful performance on benchmark and real networks, Troika emerges as a dependable method for solving clique partitioning instances on standard hardware. 
<br /> <div>
arXiv:2505.03573v1 Announce Type: new 
Abstract: Clique partitioning is a fundamental network clustering task, with applications in a wide range of computational sciences. It involves identifying an optimal partition of the nodes for a real-valued weighted graph according to the edge weights. An optimal partition is one that maximizes the sum of within-cluster edge weights over all possible node partitions. This paper introduces a novel approximation algorithm named Troika to solve this NP-hard problem in small to mid-sized networks for instances of theoretical and practical relevance. Troika uses a branch-and-cut scheme for branching on node triples to find a partition that is within a user-specified optimality gap tolerance. Troika offers advantages over alternative methods like integer programming solvers and heuristics for clique partitioning. Unlike existing heuristics, Troika returns solutions within a guaranteed proximity to global optimality. And our results indicate that Troika is faster than using the state-of-the-art integer programming solver Gurobi for most benchmark instances. Besides its advantages for solving the clique partitioning problem, we demonstrate the applications of Troika in community detection and portfolio analysis. Troika returns partitions with higher proximity to optimal compared to eight modularity-based community detection algorithms. When used on networks of correlations among stocks, Troika reveals the dynamic changes in the structure of portfolio networks including downturns from the 2008 financial crisis and the reaction to the COVID-19 pandemic. Our comprehensive results based on benchmarks from the literature and new real and random networks point to Troika as a reliable and accurate method for solving clique partitioning instances with up to 5000 edges on standard hardware.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Impact of Group Interactions on Climate-related Opinion Change in Reddit</title>
<link>https://arxiv.org/abs/2505.02989</link>
<guid>https://arxiv.org/abs/2505.02989</guid>
<content:encoded><![CDATA[
<div> climate change, opinion dynamics, social media, hypergraph model, Reddit 
Summary: 
- The study focuses on opinion dynamics models in social networks, particularly on social media platforms like Reddit.
- Traditional dyadic models are limited in capturing group dynamics in online discussions, prompting the use of a temporal hypergraph model.
- The hypergraph model accurately predicts shifts in stance towards climate issues at the individual user level.
- The approach is tested against a large language model to validate its predictions, showing superior performance compared to dyadic models.
- The research sheds light on the complexity of opinion formation and evolution in online spaces, highlighting the challenges in capturing nuances of opinions in group interactions. 

<br /><br />Summary: <div>
arXiv:2505.02989v1 Announce Type: cross 
Abstract: Opinion dynamics models describe the evolution of behavioral changes within social networks and are essential for informing strategies aimed at fostering positive collective changes, such as climate action initiatives. When applied to social media interactions, these models typically represent social exchanges in a dyadic format to allow for a convenient encoding of interactions into a graph where edges represent the flow of information from one individual to another. However, this structural assumption fails to adequately reflect the nature of group discussions prevalent on many social media platforms. To address this limitation, we present a temporal hypergraph model that effectively captures the group dynamics inherent in conversational threads, and we apply it to discussions about climate change on Reddit. This model predicts temporal shifts in stance towards climate issues at the level of individual users. In contrast to traditional studies in opinion dynamics that typically rely on simulations or limited empirical validation, our approach is tested against a comprehensive ground truth estimated by a large language model at the level of individual user comments. Our findings demonstrate that using hypergraphs to model group interactions yields superior predictions of the microscopic dynamics of opinion formation, compared to state-of-the-art models based on dyadic interactions. Although our research contributes to the understanding of these complex social systems, significant challenges remain in capturing the nuances of how opinions are formed and evolve within online spaces.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coevolution of Actions and Opinions in Networks of Coordinating and Anti-Coordinating Agents</title>
<link>https://arxiv.org/abs/2505.03078</link>
<guid>https://arxiv.org/abs/2505.03078</guid>
<content:encoded><![CDATA[
<div> coevolutionary model, agent dynamics, opinions, game theory, network structure 
<br />
Summary: 
In this paper, the dynamics of coordinating and anti-coordinating agents in a coevolutionary model for actions and opinions are explored. For coordinating agents, convergence to a Nash equilibrium is guaranteed, with conditions for consensus configurations and regions of attraction for equilibria identified. In the scenario of anti-coordinating agents, all trajectories converge to a Nash equilibrium using potential game theory. Analytical conditions on the network structure and model parameters are established to ensure the existence of consensus and polarized equilibria, characterizing their regions of attraction. This study offers insights into how agents interact and reach equilibrium in different scenarios within a social network context. <div>
arXiv:2505.03078v1 Announce Type: cross 
Abstract: In this paper, we investigate the dynamics of coordinating and anti-coordinating agents in a coevolutionary model for actions and opinions. In the model, the individuals of a population interact on a two-layer network, sharing their opinions and observing others' action, while revising their own opinions and actions according to a game-theoretic mechanism, grounded in the social psychology literature. First, we consider the scenario of coordinating agents, where convergence to a Nash equilibrium (NE) is guaranteed. We identify conditions for reaching consensus configurations and establish regions of attraction for these equilibria. Second, we study networks of anti-coordinating agents. In this second scenario, we prove that all trajectories converge to a NE by leveraging potential game theory. Then, we establish analytical conditions on the network structure and model parameters to guarantee the existence of consensus and polarized equilibria, characterizing their regions of attraction.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Exploratory Learning-Aided Community Detection Under Topological Uncertainty</title>
<link>https://arxiv.org/abs/2304.04497</link>
<guid>https://arxiv.org/abs/2304.04497</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, overlapping communities, graph neural networks, network exploration, network inference

Summary: 
META-CODE is a framework designed to detect overlapping communities in social networks where the network structure is uncertain or only partially known. It consists of iterative steps including node-level community-affiliation embeddings, network exploration through community-affiliation-based node queries, and network inference using a neural network model. The framework outperforms benchmark community detection methods, achieving significant improvements in normalized mutual information on real-world datasets. The individual modules of META-CODE contribute to its effectiveness, with node queries playing a crucial role. Empirical evaluations and theoretical findings support the efficacy of node queries in the framework. The inferred network converges to provide accurate community detection results. Overall, META-CODE offers a powerful and efficient solution for community detection in social networks with uncertain or incomplete network structures. 

<br /><br />Summary: <div>
arXiv:2304.04497v4 Announce Type: replace 
Abstract: In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often uncertain, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities via exploratory learning aided by easy-to-collect node metadata when networks are topologically unknown (or only partially known). Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through extensive experiments on three real-world datasets including two large networks, we demonstrate: (a) the superiority of META-CODE over benchmark community detection methods, achieving remarkable gains up to 65.55% on the Facebook dataset over the best competitor among our selected competitive methods in terms of normalized mutual information (NMI), (b) the impact of each module in META-CODE, (c) the effectiveness of node queries in META-CODE based on empirical evaluations and theoretical findings, and (d) the convergence of the inferred network.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Opinions Prediction Utilizes Fusing Dynamics Equation with LLM-based Agents</title>
<link>https://arxiv.org/abs/2409.08717</link>
<guid>https://arxiv.org/abs/2409.08717</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion dynamics, Large Language Model, Cellular Automata, Agent-Based Modeling

Summary:
The study introduces the Fusing Dynamics Equation-Large Language Model (FDE-LLM) algorithm for simulating and predicting user opinions on social media. It addresses the limitations of traditional algorithms by incorporating real-world social data and aligning with Large Language Models. The FDE-LLM divides users into opinion leaders and followers, utilizing Cellular Automata (CA) and the Susceptible-Infectious-Recovered (SIR) model to capture the dynamics of opinion evolution. Experiments on Weibo datasets show that the FDE-LLM outperforms traditional Agent-Based Modeling (ABM) algorithms and LLM-based approaches. The algorithm accurately depicts opinion decay and recovery over time, highlighting the potential of LLMs in enhancing understanding of social media dynamics.
<br /><br />Summary: <div>
arXiv:2409.08717v4 Announce Type: replace 
Abstract: In the context where social media emerges as a pivotal platform for social movements and shaping public opinion, accurately simulating and predicting the dynamics of user opinions is of significant importance. Such insights are vital for understanding social phenomena, informing policy decisions, and guiding public opinion. Unfortunately, traditional algorithms based on idealized models and disregarding social data often fail to capture the complexity and nuance of real-world social interactions. This study proposes the Fusing Dynamics Equation-Large Language Model (FDE-LLM) algorithm. This innovative approach aligns the actions and evolution of opinions in Large Language Models (LLMs) with the real-world data on social networks. The FDE-LLM devides users into two roles: opinion leaders and followers. Opinion leaders use LLM for role-playing and employ Cellular Automata(CA) to constrain opinion changes. In contrast, opinion followers are integrated into a dynamic system that combines the CA model with the Susceptible-Infectious-Recovered (SIR) model. This innovative design significantly improves the accuracy of the simulation. Our experiments utilized four real-world datasets from Weibo. The result demonstrates that the FDE-LLM significantly outperforms traditional Agent-Based Modeling (ABM) algorithms and LLM-based algorithms. Additionally, our algorithm accurately simulates the decay and recovery of opinions over time, underscoring LLMs potential to revolutionize the understanding of social media dynamics.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective</title>
<link>https://arxiv.org/abs/2403.16137</link>
<guid>https://arxiv.org/abs/2403.16137</guid>
<content:encoded><![CDATA[
<div> knowledge-based, graph self-supervised learning, graph foundation models, taxonomy, pretext tasks 
Summary:
Graph self-supervised learning is essential for pre-training graph foundation models by utilizing various knowledge patterns present in graph data. Existing surveys of GFMs have shortcomings in comprehensiveness, categorization, and perspective. To address this, a knowledge-based taxonomy categorizing self-supervised graph models by specific graph knowledge used is proposed. The taxonomy includes microscopic, mesoscopic, and macroscopic knowledge categories, encompassing 9 knowledge categories and over 25 pretext tasks for pre-training GFMs. Various downstream task generalization strategies are also covered. This approach allows for a clearer examination of graph models with new architectures, such as graph language models, and provides deeper insights into constructing GFMs. <div>
arXiv:2403.16137v3 Announce Type: replace-cross 
Abstract: Graph self-supervised learning (SSL) is now a go-to method for pre-training graph foundation models (GFMs). There is a wide variety of knowledge patterns embedded in the graph data, such as node properties and clusters, which are crucial to learning generalized representations for GFMs. However, existing surveys of GFMs have several shortcomings: they lack comprehensiveness regarding the most recent progress, have unclear categorization of self-supervised methods, and take a limited architecture-based perspective that is restricted to only certain types of graph models. As the ultimate goal of GFMs is to learn generalized graph knowledge, we provide a comprehensive survey of self-supervised GFMs from a novel knowledge-based perspective. We propose a knowledge-based taxonomy, which categorizes self-supervised graph models by the specific graph knowledge utilized. Our taxonomy consists of microscopic (nodes, links, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge (global structure, manifolds, etc.). It covers a total of 9 knowledge categories and more than 25 pretext tasks for pre-training GFMs, as well as various downstream task generalization strategies. Such a knowledge-based taxonomy allows us to re-examine graph models based on new architectures more clearly, such as graph language models, as well as provide more in-depth insights for constructing GFMs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amplifying Your Social Media Presence: Personalized Influential Content Generation with LLMs</title>
<link>https://arxiv.org/abs/2505.01698</link>
<guid>https://arxiv.org/abs/2505.01698</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social media, content generation, influence, network structure <br />
Summary: <br />
The paper explores the potential of Large Language Models (LLMs) in generating personalized influential content to enhance a user's presence on social media. It highlights the limitations of current content generation techniques in addressing real-world social media challenges. By incorporating network information into content prompts, the research aims to boost post influence by leveraging underlying network structures. Multiple content-centric and structure-aware prompts are designed and evaluated through empirical experiments across LLMs. The findings demonstrate the effectiveness of injecting network information into prompt for content generation, shedding light on strategies that can significantly improve post influence. The research provides insights on enhancing visibility and influence on social media through innovative content generation approaches. The code for the study is accessible on GitHub for further exploration and experimentation. <br /> <div>
arXiv:2505.01698v1 Announce Type: new 
Abstract: The remarkable advancements in Large Language Models (LLMs) have revolutionized the content generation process in social media, offering significant convenience in writing tasks. However, existing applications, such as sentence completion and fluency enhancement, do not fully address the complex challenges in real-world social media contexts. A prevalent goal among social media users is to increase the visibility and influence of their posts. This paper, therefore, delves into the compelling question: Can LLMs generate personalized influential content to amplify a user's presence on social media? We begin by examining prevalent techniques in content generation to assess their impact on post influence. Acknowledging the critical impact of underlying network structures in social media, which are instrumental in initiating content cascades and highly related to the influence/popularity of a post, we then inject network information into prompt for content generation to boost the post's influence. We design multiple content-centric and structure-aware prompts. The empirical experiments across LLMs validate their ability in improving the influence and draw insights on which strategies are more effective. Our code is available at https://github.com/YuyingZhao/LLM-influence-amplifier.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDTok: A Dataset for Eating Disorder Content on TikTok</title>
<link>https://arxiv.org/abs/2505.02250</link>
<guid>https://arxiv.org/abs/2505.02250</guid>
<content:encoded><![CDATA[
<div> Keywords: eating disorders, TikTok, social media, digital health, mental health

Summary:
Eating disorders, such as anorexia nervosa and bulimia nervosa, have seen an increase during the COVID-19 pandemic, potentially exacerbated by exposure to idealized body images online. TikTok, a popular platform with a large adolescent user base, has become a notable space for the sharing of eating disorder content, raising concerns about its impact on vulnerable populations. A dataset of 43,040 TikTok videos related to eating disorders collected from January 2019 to June 2024 offers insights into content spread, moderation, user engagement, and the pandemic's influence on eating disorder trends. This dataset fills research gaps and can inform strategies to reduce the risks associated with harmful content. It contributes valuable insights to the study of digital health and the role of social media in shaping mental health. <div>
arXiv:2505.02250v1 Announce Type: new 
Abstract: Eating disorders, which include anorexia nervosa and bulimia nervosa, have been exacerbated by the COVID-19 pandemic, with increased diagnoses linked to heightened exposure to idealized body images online. TikTok, a platform with over a billion predominantly adolescent users, has become a key space where eating disorder content is shared, raising concerns about its impact on vulnerable populations. In response, we present a curated dataset of 43,040 TikTok videos, collected using keywords and hashtags related to eating disorders. Spanning from January 2019 to June 2024, this dataset, offers a comprehensive view of eating disorder-related content on TikTok. Our dataset has the potential to address significant research gaps, enabling analysis of content spread and moderation, user engagement, and the pandemic's influence on eating disorder trends. This work aims to inform strategies for mitigating risks associated with harmful content, contributing valuable insights to the study of digital health and social media's role in shaping mental health.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A longitudinal analysis of misinformation, polarization and toxicity on Bluesky after its public launch</title>
<link>https://arxiv.org/abs/2505.02317</link>
<guid>https://arxiv.org/abs/2505.02317</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, decentralized platform, user activity, political leanings, moderation efforts

Summary: 
Bluesky is a decentralized social media platform similar to Twitter that recently opened to the public, leading to a surge in user activity. Analysis of user behavior revealed a balanced distribution of original and reshared content, with low toxicity levels on the platform. Most Bluesky users lean left politically and share content from reliable sources. The influx of new users after the public launch, particularly those posting in English and Japanese, increased platform activity, but some accounts exhibited suspicious behavior and were flagged for spam or suspended, indicating effective moderation efforts. The study also highlighted misinformation dynamics and engagement in harmful conversations, showing that Bluesky maintains a relatively positive and credible environment for social interactions. 

<br /><br />Summary: <div>
arXiv:2505.02317v1 Announce Type: new 
Abstract: Bluesky is a decentralized, Twitter-like social media platform that has rapidly gained popularity. Following an invite-only phase, it officially opened to the public on February 6th, 2024, leading to a significant expansion of its user base. In this paper, we present a longitudinal analysis of user activity in the two months surrounding its public launch, examining how the platform evolved due to this rapid growth. Our analysis reveals that Bluesky exhibits an activity distribution comparable to more established social platforms, yet it features a higher volume of original content relative to reshared posts and maintains low toxicity levels. We further investigate the political leanings of its user base, misinformation dynamics, and engagement in harmful conversations. Our findings indicate that Bluesky users predominantly lean left politically and tend to share high-credibility sources. After the platform's public launch, an influx of new users, particularly those posting in English and Japanese, contributed to a surge in activity. Among them, several accounts displayed suspicious behaviors, such as mass-following users and sharing content from low-credibility news sources. Some of these accounts have already been flagged as spam or suspended, suggesting that Bluesky's moderation efforts have been effective.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Correction on Social Media: A Quantitative Analysis of Comment Behaviour and Reliability</title>
<link>https://arxiv.org/abs/2505.02343</link>
<guid>https://arxiv.org/abs/2505.02343</guid>
<content:encoded><![CDATA[
<div> Keywords: social correction, social media, online experiment, credibility evaluations, commenting behavior 

Summary: 
An online experiment focused on the phenomenon of Social Correction, examining how users' credibility evaluations and confidence, combined with online reputational concerns, influence their commenting behavior on social media posts. Results showed that users tended to be more cautious and conservative when giving disputing comments compared to endorsing ones. However, participants were more discerning and critical in their disputing comments, highlighting a cautious approach towards correcting misinformation. These findings contribute to understanding the dynamics of social correction on social media, shedding light on the factors that influence users' commenting behavior and the reliability of their comments. The study underscores the importance of considering the credibility evaluations of social media users and the impact of online reputational concerns in the context of combating misinformation. 

<br /><br />Summary: <div>
arXiv:2505.02343v1 Announce Type: new 
Abstract: Corrections given by ordinary social media users, also referred to as Social Correction have emerged as a viable intervention against misinformation as per the recent literature. However, little is known about how often users give disputing or endorsing comments and how reliable those comments are. An online experiment was conducted to investigate how users' credibility evaluations of social media posts and their confidence in those evaluations combined with online reputational concerns affect their commenting behaviour. The study found that participants exhibited a more conservative approach when giving disputing comments compared to endorsing ones. Nevertheless, participants were more discerning in their disputing comments than endorsing ones. These findings contribute to a better understanding of social correction on social media and highlight the factors influencing comment behaviour and reliability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dyGRASS: Dynamic Spectral Graph Sparsification via Localized Random Walks on GPUs</title>
<link>https://arxiv.org/abs/2505.02741</link>
<guid>https://arxiv.org/abs/2505.02741</guid>
<content:encoded><![CDATA[
<div> algorithm, spectral sparsification, dynamic graphs, random walk, GPU-based

Summary:
The work introduces dyGRASS, a dynamic algorithm for spectral sparsification of large undirected graphs with streaming edge updates. It utilizes random-walk-based methods to estimate node-to-node distances efficiently in both the original graph and its sparsifier for incremental and decremental updates. dyGRASS identifies spectrally critical edges among updates for capturing structural changes and recovers important edges during deletions. The algorithm leverages GPU-based non-backtracking random walks for parallel operation, enhancing performance and scalability. Experimental results demonstrate a 10x speedup over the state-of-the-art algorithm inGRASS, eliminating setup overhead and improving solution quality. dyGRASS excels in fully dynamic graph sparsification, accommodating both edge inserts and deletes across diverse graph instances from various domains like integrated circuits, finite element analysis, and social networks. <br /><br />Summary: <div>
arXiv:2505.02741v1 Announce Type: new 
Abstract: This work presents dyGRASS, an efficient dynamic algorithm for spectral sparsification of large undirected graphs that undergo streaming edge insertions and deletions. At its core, dyGRASS employs a random-walk-based method to efficiently estimate node-to-node distances in both the original graph (for decremental update) and its sparsifier (for incremental update). For incremental updates, dyGRASS enables the identification of spectrally critical edges among the updates to capture the latest structural changes. For decremental updates, dyGRASS facilitates the recovery of important edges from the original graph back into the sparsifier. To further enhance computational efficiency, dyGRASS employs a GPU-based non-backtracking random walk scheme that allows multiple walkers to operate simultaneously across various target updates. This parallelization significantly improves both the performance and scalability of the proposed dyGRASS framework. Our comprehensive experimental evaluations reveal that dyGRASS achieves approximately a 10x speedup compared to the state-of-the-art incremental sparsification (inGRASS) algorithm while eliminating the setup overhead and improving solution quality in incremental spectral sparsification tasks. Moreover, dyGRASS delivers high efficiency and superior solution quality for fully dynamic graph sparsification, accommodating both edge insertions and deletions across a diverse range of graph instances originating from integrated circuit simulations, finite element analysis, and social networks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Governance (HAIG): A Trust-Utility Approach</title>
<link>https://arxiv.org/abs/2505.01651</link>
<guid>https://arxiv.org/abs/2505.01651</guid>
<content:encoded><![CDATA[
<div> Trust dynamics, HAIG framework, evolving relationships, AI systems, human-AI<br />
<br />
Summary: This paper introduces the HAIG framework to analyze trust dynamics in evolving human-AI relationships. It addresses the limitations of current categorical frameworks in capturing the evolving nature of AI systems from tools to partners. The HAIG framework operates on three levels: dimensions, continua, and thresholds, focusing on maintaining appropriate trust relationships while maximizing utility and ensuring safeguards. It takes a trust-utility orientation rather than risk-based or principle-based approaches. The analysis highlights how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution in various contexts. Case studies in healthcare and European regulation demonstrate the framework's effectiveness in complementing existing models and anticipating governance challenges. <div>
arXiv:2505.01651v1 Announce Type: cross 
Abstract: This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., "human-in-the-loop" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning</title>
<link>https://arxiv.org/abs/2505.02027</link>
<guid>https://arxiv.org/abs/2505.02027</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph In-Context Learning, Prompt Generation, Prompt Selection, Prompt Augmentation, Pre-trained Models

Summary: 
Graph In-Context Learning has gained attention for adapting pre-trained graph models to new graphs without updating parameters. Existing methods use random prompts, leading to noise and lower performance. GraphPrompter introduces a multi-stage adaptive prompt optimization approach, enhancing in-context learning. The Prompt Generator highlights informative edges for prompt construction, reducing noise. The Prompt Selector dynamically selects relevant prompts using a $k$-nearest neighbors algorithm. The Prompt Augmenter enhances model generalization with a cache replacement strategy. GraphPrompter outperforms baselines by over 8% on average. The code is available at https://github.com/karin0018/GraphPrompter. 

<br /><br />Summary: <div>
arXiv:2505.02027v1 Announce Type: cross 
Abstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Democratic Federation: Fair Governance of Large-Scale, Decentralized, Sovereign Digital Communities</title>
<link>https://arxiv.org/abs/2505.02208</link>
<guid>https://arxiv.org/abs/2505.02208</guid>
<content:encoded><![CDATA[
<div> Grassroots Democratic Federation, large-scale digital communities, fair democratic governance, sortition, federation<br />
Summary:<br />
The article discusses the concept of Grassroots Democratic Federation for large-scale digital communities, aiming to achieve egalitarian formation and fair democratic governance. The federation evolves through grassroots formation and consensual federation of digital communities based on various criteria. Small communities govern themselves, while larger ones are governed by assemblies elected by sortition. The article focuses on the dynamic evolution of the federation, adapting fairness conditions to this setting. It emphasizes fair representation and participation, ensuring these conditions hold as the federation grows. A protocol is presented to meet these fairness requirements, aiming to stabilize the federation structure over time. The approach addresses the dynamic nature of the federation, striving for inclusive and democratic governance of digital communities. <br />Summary: <div>
arXiv:2505.02208v1 Announce Type: cross 
Abstract: Grassroots Democratic Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots democratic federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, or causes. Small communities (say up to 100 members) govern themselves; larger communities -- no matter how large -- are governed by a small assembly elected by sortition among its members. Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.
  In practice, we expect a grassroots democratic federation to grow and evolve dynamically and in all directions -- bottom-up, top-down, and middle-out. To address that, we formally specify this dynamic setting and adapt the static fairness conditions to it: The ex post condition on the fair representation of a child community becomes a condition that must always hold; the ex ante conditions in expectation on the fair participation of an individual and on the fair representation of a child community become conditions satisfied in actuality in the limit, provided the federation structure eventually stabilizes. We then present a protocol that satisfies these fairness conditions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Federated Graph Learning: A Data Condensation Perspective</title>
<link>https://arxiv.org/abs/2505.02573</link>
<guid>https://arxiv.org/abs/2505.02573</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, federated graph learning, condensed graph, FedGM, communication efficiency<br />
<br />
Summary:
The article introduces FedGM, a novel approach for federated graph learning that addresses data heterogeneity and privacy risks. It utilizes condensed graphs to aggregate knowledge from distributed graphs, reducing communication costs and privacy risks. Experiment results on six datasets demonstrate FedGM's superiority over existing methods, highlighting its potential as a new FGL paradigm. <div>
arXiv:2505.02573v1 Announce Type: cross 
Abstract: Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client graphs.However, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference and Visualization of Community Structure in Attributed Hypergraphs Using Mixed-Membership Stochastic Block Models</title>
<link>https://arxiv.org/abs/2401.00688</link>
<guid>https://arxiv.org/abs/2401.00688</guid>
<content:encoded><![CDATA[
<div> Hypergraphs, community structure, mixed-membership stochastic block models, node attributes, dimensionality reduction <br />
Summary: 
The study proposes the HyperNEO framework, combining mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods to infer community structure. This approach aims to simplify the visualization and interpretation of community structure in hypergraphs by generating a node layout that preserves node community memberships. Testing on synthetic and empirical hypergraphs with node attributes, the framework shows promise in broadening the exploration of higher-order community structure in complex systems. <div>
arXiv:2401.00688v2 Announce Type: replace 
Abstract: Hypergraphs represent complex systems involving interactions among more than two entities and allow the investigation of higher-order structure and dynamics in complex systems. Node attribute data, which often accompanies network data, can enhance the inference of community structure in complex systems. While mixed-membership stochastic block models have been employed to infer community structure in hypergraphs, they complicate the visualization and interpretation of inferred community structure by assuming that nodes may possess soft community memberships. In this study, we propose a framework, HyperNEO, that combines mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods. Our approach generates a node layout that largely preserves the community memberships of nodes. We evaluate our framework on both synthetic and empirical hypergraphs with node attributes. We expect our framework will broaden the investigation and understanding of higher-order community structure in complex systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easy-access online social media metrics can foster the identification of misinformation sharing users</title>
<link>https://arxiv.org/abs/2408.15186</link>
<guid>https://arxiv.org/abs/2408.15186</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, social media, user identification, factuality, online metrics <br />
Summary: <br />
Researchers have long studied the challenge of misinformation, but identifying primary sharers is difficult and time-consuming. This study proposes a low-barrier method to differentiate social media users likely to share misinformation by analyzing easily accessible online metrics. The research suggests that high tweet frequency and newer account age are associated with sharing low factuality content. Additionally, the number of accounts followed and the number of tweets produced may impact the spread of misinformation, depending on the user's follower count. By utilizing these simple social network metrics, platforms like Twitter can effectively identify users who are prone to spreading misinformation, aiding in combating the issue on social media. <div>
arXiv:2408.15186v2 Announce Type: replace 
Abstract: Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is time-consuming and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drilling into Erasmus learning mobility flows between countries 2014-2024</title>
<link>https://arxiv.org/abs/2505.00889</link>
<guid>https://arxiv.org/abs/2505.00889</guid>
<content:encoded><![CDATA[
<div> Keywords: Erasmus, mobility network, weighted networks, visualization, clustering

Summary:
The study focuses on analyzing the Erasmus mobility network, highlighting typical issues and methods in examining weighted networks. Various alternative exploratory perspectives are proposed for the dense network of 35 countries with a wide range of visit weights. Transformation techniques are employed to address the vast weight range. Skeleton reduction methods reveal Spain as a key node in the network, along with dominant roles of Germany, France, and Italy. Matrix representations unveil block patterns showcasing clustering of countries into developed and less developed clusters. Balassa normalization matrices indicate deviations from expected visit patterns, with certain clusters exceeding or falling below expectations. Overall, the study offers insights into network structure, highlighting key players and patterns of mobility flow within the Erasmus network.<br /><br />Summary: <div>
arXiv:2505.00889v1 Announce Type: new 
Abstract: Analyzing the Erasmus mobility network, we illustrate typical problems and approaches in analyzing weighted networks. We propose alternative exploratory views on the network "Erasmus+ learning mobility flows since 2014". The network has 35 nodes (countries), is very dense, and the range of link weights (number of visits) is huge (from 1 to 217003). An increasing transformation is used to reduce the range. The traditional graph-based visualization is unreadable. To gain insight into the structure of a dense network, it can be reduced to a skeleton by removing less essential links and/or nodes. We have determined the 1-neighbors and 2-neighbors subnetworks. The 1-neighbors skeleton highlights Spain as the main attractor in the network. The 2-neighbors skeleton shows the dominant role of Spain, Germany, France, and Italy. The hubs and authorities, Pathfinder and Ps cores methods confirm these observations.
  Using the "right" order of the nodes in a matrix representation can reveal the network structure as block patterns in the displayed matrix. The clustering of network nodes based on corrected Salton dissimilarity again shows the dominant role of Spain, Germany, France, and Italy, but also two main clusters of the division into developed/less developed countries. The Balassa normalization (log(measured/expected) visits) matrix shows that most visits within the two main clusters are above expected, while most visits between them are below expected; within the clusters of Balkan countries, Baltic countries, {SK, CZ, HU}, {IS, DK, NO} visits are much above expected, etc.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a format for describing networks / 1. Networks and knowledge graphs</title>
<link>https://arxiv.org/abs/2505.00912</link>
<guid>https://arxiv.org/abs/2505.00912</guid>
<content:encoded><![CDATA[
<div> Keywords: network, knowledge graph, RDF, Semantic Web, network analysis<br />
Summary:<br />
The article explores the relationship between networks and knowledge graphs, identifying knowledge graphs as a specialized form of network. It discusses how a knowledge graph can be transformed into various networks and subject to network analysis procedures. RDF is highlighted as a formalization of the knowledge graph idea within the context of the Semantic Web, with potential applicability to general network descriptions. The discussion underscores the interchangeability of concepts between knowledge graphs and networks, emphasizing the utility of knowledge graphs in generating diverse network structures. The article suggests that analysis techniques developed for knowledge graphs can be extended to network analysis, indicating a cross-pollination of methodologies in these domains. <div>
arXiv:2505.00912v1 Announce Type: new 
Abstract: The relationship between the concepts of network and knowledge graph is explored. A knowledge graph can be considered a special type of network. When using a knowledge graph, various networks can be obtained from it, and network analysis procedures can be applied to them. RDF is a formalization of the knowledge graph concept for the Semantic Web, but some of its solutions are also extensible to a format for describing general networks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a format for describing networks / 2. Format elements</title>
<link>https://arxiv.org/abs/2505.00921</link>
<guid>https://arxiv.org/abs/2505.00921</guid>
<content:encoded><![CDATA[
<div> networks, common format, key elements, describing, discussed <br />
Summary: 
This article delves into the essential components that a standardized format for describing networks should encompass. The discussion highlights the significance of establishing a common structure to accurately convey network information. Key elements identified for inclusion in such a format involve comprehensive descriptions of network configurations and characteristics. The necessity of incorporating specific details regarding network components, connections, and functionalities is emphasized to enhance the clarity and utility of network descriptions. The article underscores the importance of standardizing the language and terminology used to define network attributes to facilitate accurate communication and understanding among stakeholders. In conclusion, the article advocates for a systematic approach towards developing a universal format that can effectively capture the complexities and nuances of diverse network systems. <div>
arXiv:2505.00921v1 Announce Type: new 
Abstract: The key elements that a common format for describing networks should include are discussed.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Persistent Homology Distinguishes Simple and Complex Contagions with High Accuracy</title>
<link>https://arxiv.org/abs/2505.00958</link>
<guid>https://arxiv.org/abs/2505.00958</guid>
<content:encoded><![CDATA[
<div> classification, regression, extended persistent homology, simple contagion, complex contagion

Summary: The study explores distinguishing simple and complex contagions using extended persistent homology (EPH) in the context of network dynamics. Traditional methods struggle due to confounding factors and individual heterogeneity. EPH, applied to simulated contagion dynamics on real-world networks, effectively differentiates between simple and complex contagion processes and predicts their parameters. The models exhibit high predictive performance across various contagion parameters, even with noise and partial observability. EPH captures the influence of cycles of different lengths on contagion dynamics, providing a valuable metric for model classification and parameter prediction. The findings suggest that topological data analysis tools can aid in solving network optimization problems like seeding and vaccination strategies, as well as network inference and reconstruction challenges. <div>
arXiv:2505.00958v1 Announce Type: new 
Abstract: The social contagion literature makes a distinction between simple (independent cascade or bond percolation processes that pass infections through edges) and complex contagions (bootstrap percolation or threshold processes that require local reinforcement to spread). However, distinguishing simple and complex contagions using observational data poses a significant challenge in practice. Estimating population-level activation functions from observed contagion dynamics is hindered by confounding factors that influence adoptions (other than neighborhood interactions), as well as heterogeneity in individual behaviors and modeling variations that make it difficult to design appropriate null models for inferring contagion types. Here, we show that a new tool from topological data analysis (TDA), called extended persistent homology (EPH), when applied to contagion processes over networks, can effectively detect simple and complex contagion processes, as well as predict their parameters. We train classification and regression models using EPH-based topological summaries computed on simulated simple and complex contagion dynamics on three real-world network datasets and obtain high predictive performance over a wide range of contagion parameters and under a variety of informational constraints, including uncertainty in model parameters, noise, and partial observability of contagion dynamics. EPH captures the role of cycles of varying lengths in the observed contagion dynamics and offers a useful metric to classify contagion models and predict their parameters. Analyzing geometrical features of network contagion using TDA tools such as EPH can find applications in other network problems such as seeding, vaccination, and quarantine optimization, as well as network inference and reconstruction problems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-moderation in the decentralized era: decoding blocking behavior on Bluesky</title>
<link>https://arxiv.org/abs/2505.01174</link>
<guid>https://arxiv.org/abs/2505.01174</guid>
<content:encoded><![CDATA[
<div> Keywords: moderation, blocking behavior, decentralized networks, online communities, user blocking

Summary:
This study focuses on self-moderation through blocking behavior on the decentralized social networking platform Bluesky. By analyzing user activity over three months, the research aims to understand the connection between online behavior and the likelihood of being blocked. The study defines user profiles based on various features related to user activity, content characteristics, and network interactions. The research addresses two primary questions: whether users' blocking likelihood can be predicted from their behavior, and which behavioral features are linked to a higher chance of being blocked. The findings provide valuable insights into moderation on decentralized social networks and offer a robust analytical framework for future research in this area.<br /><br />Summary: Keywords: moderation, blocking behavior, decentralized networks, online communities, user blocking <div>
arXiv:2505.01174v1 Announce Type: new 
Abstract: Moderation and blocking behavior, both closely related to the mitigation of abuse and misinformation on social platforms, are fundamental mechanisms for maintaining healthy online communities. However, while centralized platforms typically employ top-down moderation, decentralized networks rely on users to self-regulate through mechanisms like blocking actions to safeguard their online experience. Given the novelty of the decentralized paradigm, addressing self-moderation is critical for understanding how community safety and user autonomy can be effectively balanced. This study examines user blocking on Bluesky, a decentralized social networking platform, providing a comprehensive analysis of over three months of user activity through the lens of blocking behaviour. We define profiles based on 86 features that describe user activity, content characteristics, and network interactions, addressing two primary questions: (1) Is the likelihood of a user being blocked inferable from their online behavior? and (2) What behavioral features are associated with an increased likelihood of being blocked? Our findings offer valuable insights and contribute with a robust analytical framework to advance research in moderation on decentralized social networks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell me who its founders are and I'll tell you what your online community looks like: Online community founders' personality and community attributes</title>
<link>https://arxiv.org/abs/2505.01219</link>
<guid>https://arxiv.org/abs/2505.01219</guid>
<content:encoded><![CDATA[
<div> personality traits, online communities, founders, sustainability, engagement

Summary:<br />
This study focuses on the personality traits of online community founders and their impact on community sustainability and attributes. By analyzing the Big Five personality traits of 35,164 founders in 8,625 Reddit communities, the study finds that founder traits play a significant role in determining community engagement, social network structure, and founder activity within the community. The research highlights the importance of considering behavioral and psychological aspects of community members and leaders in understanding and predicting community outcomes. The findings suggest that founder traits can serve as predictors of community success and offer valuable insights into the factors that contribute to the growth and longevity of online communities. <div>
arXiv:2505.01219v1 Announce Type: new 
Abstract: Online communities are an increasingly important stakeholder for firms, and despite the growing body of research on them, much remains to be learned about them and about the factors that determine their attributes and sustainability. Whereas most of the literature focuses on predictors such as community activity, network structure, and platform interface, there is little research about behavioral and psychological aspects of community members and leaders. In the present study we focus on the personality traits of community founders as predictors of community attributes and sustainability. We develop a tool to estimate community members' Big Five personality traits from their social media text and use it to estimate the traits of 35,164 founders in 8,625 Reddit communities. We find support for most of our predictions about the relationships between founder traits and community sustainability and attributes, including the level of engagement within the community, aspects of its social network structure, and whether the founders themselves remain active in it.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMSAT: A Multimodal Acoustic Dataset and Deep Contrastive Learning Framework for Affective and Physiological Modeling of Spiritual Meditation</title>
<link>https://arxiv.org/abs/2505.00839</link>
<guid>https://arxiv.org/abs/2505.00839</guid>
<content:encoded><![CDATA[
<div> Keywords: auditory stimuli, affective computing, biometric signals, deep learning, stress monitoring <br />
Summary: 
This study examines the emotional and physiological effects of spiritual meditation, music, and natural silence on individuals using a new dataset called SMSAT. The researchers developed a deep learning model to extract features from the acoustic time series data, achieving high accuracy in classifying affective states. They also introduced the Calmness Analysis Model (CAM), a deep learning framework that combines handcrafted and learned features to classify affective states with 99.99% accuracy. The study found significant differences in cardiac response characteristics among the different auditory conditions, with spiritual meditation inducing the most pronounced physiological fluctuations. The proposed models outperformed existing methods in affective state classification tasks, indicating potential applications in stress monitoring, mental well-being, and therapeutic audio-based interventions. <br /><br />Summary: <div>
arXiv:2505.00839v1 Announce Type: cross 
Abstract: Understanding how auditory stimuli influence emotional and physiological states is fundamental to advancing affective computing and mental health technologies. In this paper, we present a multimodal evaluation of the affective and physiological impacts of three auditory conditions, that is, spiritual meditation (SM), music (M), and natural silence (NS), using a comprehensive suite of biometric signal measures. To facilitate this analysis, we introduce the Spiritual, Music, Silence Acoustic Time Series (SMSAT) dataset, a novel benchmark comprising acoustic time series (ATS) signals recorded under controlled exposure protocols, with careful attention to demographic diversity and experimental consistency. To model the auditory induced states, we develop a contrastive learning based SMSAT audio encoder that extracts highly discriminative embeddings from ATS data, achieving 99.99% classification accuracy in interclass and intraclass evaluations. Furthermore, we propose the Calmness Analysis Model (CAM), a deep learning framework integrating 25 handcrafted and learned features for affective state classification across auditory conditions, attaining robust 99.99% classification accuracy. In contrast, pairwise t tests reveal significant deviations in cardiac response characteristics (CRC) between SM analysis via ANOVA inducing more significant physiological fluctuations. Compared to existing state of the art methods reporting accuracies up to 90%, the proposed model demonstrates substantial performance gains (up to 99%). This work contributes a validated multimodal dataset and a scalable deep learning framework for affective computing applications in stress monitoring, mental well-being, and therapeutic audio-based interventions.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Data-centric Directed Graph Learning: An Entropy-driven Approach</title>
<link>https://arxiv.org/abs/2505.00983</link>
<guid>https://arxiv.org/abs/2505.00983</guid>
<content:encoded><![CDATA[
<div> Keywords: DiGraph Neural Networks, knowledge distillation, hierarchical encoding theory, topology, graph datasets

Summary: 
The paper introduces EDEN, a novel approach for data-centric learning in directed graphs. EDEN leverages hierarchical knowledge trees constructed from directed structural measurements to refine knowledge flow and enhance data-centric knowledge distillation during model training. By quantifying mutual information between node profiles, EDEN significantly improves the predictive performance of (Di)Graph Neural Networks across various graph datasets and downstream tasks. The proposed framework not only achieves state-of-the-art results but also demonstrates strong enhancements for existing (Di)GNN models. This approach paves the way for a deeper exploration of correlations between directed edges and node profiles in complex topology systems, highlighting the importance of data-centric perspectives for enhancing model-centric neural networks.

<br /><br />Summary: <div>
arXiv:2505.00983v1 Announce Type: cross 
Abstract: The directed graph (digraph), as a generalization of undirected graphs, exhibits superior representation capability in modeling complex topology systems and has garnered considerable attention in recent years. Despite the notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage directed edges, they still fail to comprehensively delve into the abundant data knowledge concealed in the digraphs. This data-level limitation results in model-level sub-optimal predictive performance and underscores the necessity of further exploring the potential correlations between the directed edges (topology) and node profiles (feature and labels) from a data-centric perspective, thereby empowering model-centric neural networks with stronger encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a data-centric digraph learning paradigm or a model-agnostic hot-and-plug data-centric Knowledge Distillation (KD) module. The core idea is to achieve data-centric ML, guided by our proposed hierarchical encoding theory for structured data. Specifically, EDEN first utilizes directed structural measurements from a topology perspective to construct a coarse-grained Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual information of node profiles to refine knowledge flow in the HKT, enabling data-centric KD supervision within model training. As a general framework, EDEN can also naturally extend to undirected scenarios and demonstrate satisfactory performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph datasets (homophily and heterophily) and across 4 downstream tasks. The results demonstrate that EDEN attains SOTA performance and exhibits strong improvement for prevalent (Di)GNNs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering complementary information sharing in spider monkey collective foraging using higher-order spatial networks</title>
<link>https://arxiv.org/abs/2505.01167</link>
<guid>https://arxiv.org/abs/2505.01167</guid>
<content:encoded><![CDATA[
<div> Collectives, distributed processing, fission-fusion dynamics, foraging information, simplicial complexes <br />
Summary:<br />
The study focuses on how collectives can process information in a distributed manner through fission-fusion dynamics. By analyzing the overlaps between individual core ranges that represent seasonal knowledge, the research identifies sets of individuals with balanced overlap between redundantly and uniquely known areas. Using simplicial complexes, higher-order interactions are represented, revealing complementarity in shared foraging information. The complex spatial networks from fission-fusion dynamics enable adaptive collective processing of foraging information in dynamic environments. <div>
arXiv:2505.01167v1 Announce Type: cross 
Abstract: Collectives are often able to process information in a distributed fashion, surpassing each individual member's processing capacity. In fission-fusion dynamics, where group members come together and split from others often, sharing complementary information about uniquely known foraging areas could allow a group to track a heterogenous foraging environment better than any group member on its own. We analyse the partial overlaps between individual core ranges, which we assume represent the knowledge of an individual during a given season. We identify sets of individuals whose overlap shows a balance between redundantly and uniquely known portions and we use simplicial complexes to represent these higher-order interactions. The structure of the simplicial complexes shows holes in various dimensions, revealing complementarity in the foraging information that is being shared. We propose that the complex spatial networks arising from fission-fusion dynamics allow for adaptive, collective processing of foraging information in dynamic environments.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversation-Based Multimodal Abuse Detection Through Text and Graph Embeddings</title>
<link>https://arxiv.org/abs/2503.12994</link>
<guid>https://arxiv.org/abs/2503.12994</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, abuse detection, representation learning, textual content, conversational graphs

Summary:
The article addresses the common issue of abusive behavior on online social networks and proposes a novel approach using representation learning methods to generate embeddings of both textual content and conversational graphs. Two methods are proposed to learn whole-graph representations using edge directions, weights, signs, and vertex attributes. The study experiments with various textual and graph embedding methods on a dataset annotated for abuse detection, achieving high F-measure scores of 81.02 using text alone and 80.61 using graphs alone. Combining both modalities through fusion strategies significantly improves abuse detection performance, increasing the F-measure to 87.06. The study also identifies specific engineered features captured by the embedding methods, shedding light on the discriminative information considered by the representation learning methods. <div>
arXiv:2503.12994v2 Announce Type: replace 
Abstract: Abusive behavior is common on online social networks, and forces the hosts of such platforms to find new solutions to address this problem. Various methods have been proposed to automate this task in the past decade. Most of them rely on the exchanged content, but ignore the structure and dynamics of the conversation, which could provide some relevant information. In this article, we propose to use representation learning methods to automatically produce embeddings of this textual content and of the conversational graphs depicting message exchanges. While the latter could be enhanced by including additional information on top of the raw conversational structure, no method currently exists to learn wholegraph representations using simultaneously edge directions, weights, signs, and vertex attributes. We propose two such methods to fill this gap in the literature. We experiment with 5 textual and 13 graph embedding methods, and apply them to a dataset of online messages annotated for abuse detection. Our best results achieve an F -measure of 81.02 using text alone and 80.61 using graphs alone. We also combine both modalities of information (text and graphs) through three fusion strategies, and show that this strongly improves abuse detection performance, increasing the F -measure to 87.06. Finally, we identify which specific engineered features are captured by the embedding methods under consideration. These features have clear interpretations and help explain what information the representation learning methods deem discriminative.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief System Dynamics as Network of Single Layered Neural Network</title>
<link>https://arxiv.org/abs/2505.00005</link>
<guid>https://arxiv.org/abs/2505.00005</guid>
<content:encoded><![CDATA[
<div> belief propagation, social network, polarization, misinformation, neural network

Summary:
In this study, a modified model of the Friedkin-Johnsen model was proposed to investigate belief propagation on social networks. The model treated individuals as single-layer neural networks, with confidence levels on evidence as inputs and belief as the output. The research reaffirmed Madison's remedy for factionalism and found that a network with a giant component reduced belief distribution variance more than a network with two communities, despite creating more social pressure. Additionally, a community structure decreased sensitivity of belief distribution variance to individual confidence levels. The model's insights have implications for political polarization, misinformation, economic conflicts, as well as applications in personality theory and behavioral psychology. <div>
arXiv:2505.00005v1 Announce Type: new 
Abstract: As problems in political polarization and the spread of misinformation become serious, belief propagation on a social network becomes an important question to explore. Previous breakthroughs have been made in algorithmic approaches to understanding how group consensus or polarization can occur in a population. This paper proposed a modified model of the Friedkin-Johnsen model that tries to explain the underlying stubbornness of individual as well as possible back fire effect by treating each individual as a single layer neural network on a set of evidence for a particular statement with input being confidence level on each evidence, and belief of the statement is the output of this neural network.
  In this papar, we reafirmed the importance of Madison's cure for the mischief of faction, and found that when structure of understanding is polarized, a network with a giant component can decrease the variance in the belief distribution more than a network with two communities, but creates more social pressure by doing so. We also found that when community structure is formed, variance in the belief distribution become less sensitive to confidence level of individuals. The model can have various applications to political and historical problems caused by misinfomation and conflicting economic interest as well as applications to personality theory and behavior psychology.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams</title>
<link>https://arxiv.org/abs/2505.00242</link>
<guid>https://arxiv.org/abs/2505.00242</guid>
<content:encoded><![CDATA[
<div> Interpretable, Partial Differential Equation, Tensor Decomposition, Time-Varying, Forecasting <br />
<br />
Summary: 
The paper introduces D-Tracker, a method for capturing time-varying temporal patterns in social activity tensor data streams and forecasting future activities. D-Tracker utilizes a tensor decomposition framework incorporating partial differential equations to interpret trends, seasonality, and interest diffusion between locations. It automatically models tensor data streams without the need for hyperparameters and is computationally scalable. Experiments using web search volume and COVID-19 infection data demonstrate D-Tracker's superior forecasting accuracy and efficiency compared to existing methods, highlighting its ability to extract location-based interest diffusion information. The D-Tracker source code and datasets are freely available for access, enabling further research and applications in analyzing and predicting social activity patterns.  <br /><br />Summary: <div>
arXiv:2505.00242v1 Announce Type: new 
Abstract: Large quantities of social activity data, such as weekly web search volumes and the number of new infections with infectious diseases, reflect peoples' interests and activities. It is important to discover temporal patterns from such data and to forecast future activities accurately. However, modeling and forecasting social activity data streams is difficult because they are high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and interest diffusion. In this paper, we propose D-Tracker, a method for continuously capturing time-varying temporal patterns within social activity tensor data streams and forecasting future activities. Our proposed method has the following properties: (a) Interpretable: it incorporates the partial differential equation into a tensor decomposition framework and captures time-varying temporal patterns such as trends, seasonality, and interest diffusion between locations in an interpretable manner; (b) Automatic: it has no hyperparameters and continuously models tensor data streams fully automatically; (c) Scalable: the computation time of D-Tracker is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data Repository show that our method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the interest diffusion between locations. Our source code and datasets are available at {https://github.com/Higashiguchi-Shingo/D-Tracker.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avatar Communication Provides More Efficient Online Social Support Than Text Communication</title>
<link>https://arxiv.org/abs/2505.00287</link>
<guid>https://arxiv.org/abs/2505.00287</guid>
<content:encoded><![CDATA[
<div> avatar communication, online social support, social relationships, offline social resources, metaverse societies
<br />
Summary: 
The study investigates the differences in online social support between avatar communication service users and text communication service users. Avatar communication users received more online social support, had more stable relationships, and had fewer offline social resources compared to text communication users. However, the positive association between online and offline social support was stronger for avatar communication users. The study emphasizes the importance of realistic online communication experiences through avatars, including nonverbal and real-time interactions. It also highlights the challenges faced by avatar communication users in the physical world, such as the lack of offline social resources. Enhancing online social support through avatars could help address these issues and potentially improve social resource problems in both online and offline settings in future metaverse societies. 
<br /> <div>
arXiv:2505.00287v1 Announce Type: new 
Abstract: Online communication via avatars provides a richer online social experience than text communication. This reinforces the importance of online social support. Online social support is effective for people who lack social resources because of the anonymity of online communities. We aimed to understand online social support via avatars and their social relationships to provide better social support to avatar users. Therefore, we administered a questionnaire to three avatar communication service users (Second Life, ZEPETO, and Pigg Party) and three text communication service users (Facebook, X, and Instagram) (N=8,947). There was no duplication of users for each service. By comparing avatar and text communication users, we examined the amount of online social support, stability of online relationships, and the relationships between online social support and offline social resources (e.g., offline social support). We observed that avatar communication service users received more online social support, had more stable relationships, and had fewer offline social resources than text communication service users. However, the positive association between online and offline social support for avatar communication users was more substantial than for text communication users. These findings highlight the significance of realistic online communication experiences through avatars, including nonverbal and real-time interactions with co-presence. The findings also highlighted avatar communication service users' problems in the physical world, such as the lack of offline social resources. This study suggests that enhancing online social support through avatars can address these issues. This could help resolve social resource problems, both online and offline in future metaverse societies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Sexual Predation and Victimization Through Warnings and Awareness among High-Risk Users</title>
<link>https://arxiv.org/abs/2505.00293</link>
<guid>https://arxiv.org/abs/2505.00293</guid>
<content:encoded><![CDATA[
<div> Keywords: online sexual predators, prevention strategy, high-risk individuals, machine learning, randomized controlled trial

Summary:
This study focused on preventing online sexual predation by targeting high-risk individuals through warnings and awareness-building messages based on criminal psychology theories. Using a machine learning model, high-risk users on an avatar-based communication platform were identified and divided into intervention and control groups. The intervention successfully reduced violations and victimization among women for a significant period, highlighting the effectiveness of targeted interventions in preventing online sexual abuse. However, the impact on men was not as pronounced, indicating a need for gender-specific prevention strategies. These findings contribute to the ongoing efforts to combat online sexual predators and enhance understanding of criminal psychology in the digital age. 

<br /><br />Summary: <div>
arXiv:2505.00293v1 Announce Type: new 
Abstract: Online sexual predators target children by building trust, creating dependency, and arranging meetings for sexual purposes. This poses a significant challenge for online communication platforms that strive to monitor and remove such content and terminate predators' accounts. However, these platforms can only take such actions if sexual predators explicitly violate the terms of service, not during the initial stages of relationship-building. This study designed and evaluated a strategy to prevent sexual predation and victimization by delivering warnings and raising awareness among high-risk individuals based on the routine activity theory in criminal psychology. We identified high-risk users as those with a high probability of committing or being subjected to violations, using a machine learning model that analyzed social networks and monitoring data from the platform. We conducted a randomized controlled trial on a Japanese avatar-based communication application, Pigg Party. High-risk players in the intervention group received warnings and awareness-building messages, while those in the control group did not receive the messages, regardless of their risk level. The trial involved 12,842 high-risk players in the intervention group and 12,844 in the control group for 138 days. The intervention successfully reduced violations and being violated among women for 12 weeks, although the impact on men was limited. These findings contribute to efforts to combat online sexual abuse and advance understanding of criminal psychology.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a digital twin of U.S. Congress</title>
<link>https://arxiv.org/abs/2505.00006</link>
<guid>https://arxiv.org/abs/2505.00006</guid>
<content:encoded><![CDATA[
<div> virtual model, U.S. congresspersons, language models, digital twin, Tweets 

Summary: 
This paper presents a virtual model of U.S. congresspersons using language models, which qualifies as a digital twin. A dataset containing Tweets from congresspersons is analyzed, and language models simulate their Tweets accurately. These generated Tweets can predict voting behavior and the likelihood of bipartisanship, aiding in resource allocation and legislative dynamics. The study discusses the analysis's limitations and potential extensions. <div>
arXiv:2505.00006v1 Announce Type: cross 
Abstract: In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S3AND: Efficient Subgraph Similarity Search Under Aggregated Neighbor Difference Semantics (Technical Report)</title>
<link>https://arxiv.org/abs/2505.00393</link>
<guid>https://arxiv.org/abs/2505.00393</guid>
<content:encoded><![CDATA[
<div> semantic, subgraph similarity search, keyword set, aggregated neighbor difference, indexing mechanism  
Summary:  
- This paper introduces the Subgraph Similarity Search under Aggregated Neighbor Difference Semantics (S$^3$AND) problem, which aims to find subgraphs in a data graph that are similar to a query graph by considering keywords and graph structures. 
- The authors propose two pruning methods, namely keyword set and aggregated neighbor difference lower bound pruning, to reduce the search space by eliminating false alarms of candidate vertices/subgraphs. 
- An effective indexing mechanism is designed to support the efficient S$^3$AND query answering algorithm. 
- Extensive experiments show the effectiveness and efficiency of the S$^3$AND approach on both real and synthetic graphs across various parameter settings.  
Summary: <div>
arXiv:2505.00393v1 Announce Type: cross 
Abstract: For the past decades, the \textit{subgraph similarity search} over a large-scale data graph has become increasingly important and crucial in many real-world applications, such as social network analysis, bioinformatics network analytics, knowledge graph discovery, and many others. While previous works on subgraph similarity search used various graph similarity metrics such as the graph isomorphism, graph edit distance, and so on, in this paper, we propose a novel problem, namely \textit{subgraph similarity search under aggregated neighbor difference semantics} (S$^3$AND), which identifies subgraphs $g$ in a data graph $G$ that are similar to a given query graph $q$ by considering both keywords and graph structures (under new keyword/structural matching semantics). To efficiently tackle the S$^3$AND problem, we design two effective pruning methods, \textit{keyword set} and \textit{aggregated neighbor difference lower bound pruning}, which rule out false alarms of candidate vertices/subgraphs to reduce the S$^3$AND search space. Furthermore, we construct an effective indexing mechanism to facilitate our proposed efficient S$^3$AND query answering algorithm. Through extensive experiments, we demonstrate the effectiveness and efficiency of our S$^3$AND approach over both real and synthetic graphs under various parameter settings.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Cultural and Digital Divides: A Low-Latency JackTrip Framework for Equitable Music Education in the Global South</title>
<link>https://arxiv.org/abs/2505.00550</link>
<guid>https://arxiv.org/abs/2505.00550</guid>
<content:encoded><![CDATA[
<div> Keywords: digital technologies, music education, JackTrip framework, Global South, cultural preservation

Summary: 
This paper introduces a low-latency JackTrip framework that addresses infrastructural and cultural challenges in music education in the Global South. The framework utilizes an open-source UDP-based audio streaming protocol to overcome technical constraints like limited bandwidth and high latency prevalent in rural and underserved regions. A comparison with conventional platforms like Zoom shows that JackTrip achieves sub-30 ms latency under simulated low-resource conditions while maintaining intricate audio details crucial for non-Western musical traditions. Spectral analysis confirms JackTrip's ability to handle microtonal scales, complex rhythms, and harmonic textures, providing an authentic medium for real-time ensemble performance and music education. These results highlight the potential of decentralized, edge-computing solutions in promoting technological equity and cultural preservation among educators and musicians in the Global South.<br /><br />Summary: <div>
arXiv:2505.00550v1 Announce Type: cross 
Abstract: The rapid expansion of digital technologies has transformed educational landscapes worldwide, yet significant infrastructural and cultural challenges persist in the Global South. This paper introduces a low-latency JackTrip framework designed to bridge both the cultural and digital divides in music education. By leveraging an open-source, UDP-based audio streaming protocol originally developed at Stanford's CCRMA, the framework is tailored to address technical constraints such as intermittent connectivity, limited bandwidth, and high latency that characterize many rural and underserved regions. The study systematically compares the performance of JackTrip with conventional platforms like Zoom, demonstrating that JackTrip achieves sub-30~ms latency under simulated low-resource conditions while preserving the intricate audio details essential for non-Western musical traditions. Spectral analysis confirms that JackTrip's superior handling of microtonal scales, complex rhythms, and harmonic textures provides a culturally authentic medium for real-time ensemble performance and music education. These findings underscore the transformative potential of decentralized, edge-computing solutions in empowering educators and musicians across the Global South, promoting both technological equity and cultural preservation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new sociology of humans and machines</title>
<link>https://arxiv.org/abs/2402.14410</link>
<guid>https://arxiv.org/abs/2402.14410</guid>
<content:encoded><![CDATA[
<div> Keywords: fake social media accounts, generative artificial intelligence, complex social systems, human-machine interactions, collective decision-making

Summary:<br /><br />
The article discusses the proliferation of robots, bots, and algorithms in various aspects of society and the need to study the interactions between humans and intelligent machines. It reviews research on competition, coordination, cooperation, contagion, and collective decision-making in complex social systems. Examples are provided from high-frequency trading markets, social media platforms, open collaboration communities, and discussion forums. The importance of a new sociology of humans and machines is emphasized, highlighting the need for researchers to use complex system methods, engineers to design AI for human-machine and machine-machine interactions, and regulators to govern the development of human-machine communities. By understanding and addressing the dynamics and patterns in human-machine interactions, we can ensure the resilience and robustness of these communities in the face of increasing technological integration. <div>
arXiv:2402.14410v3 Announce Type: replace 
Abstract: From fake social media accounts and generative artificial intelligence chatbots to trading algorithms and self-driving vehicles, robots, bots and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions and transportation arteries. Networks of multiple interdependent and interacting humans and intelligent machines constitute complex social systems for which the collective outcomes cannot be deduced from either human or machine behaviour alone. Under this paradigm, we review recent research and identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion and collective decision-making, with context-rich examples from high-frequency trading markets, a social media platform, an open collaboration community and a discussion forum. To ensure more robust and resilient human-machine communities, we require a new sociology of humans and machines. Researchers should study these communities using complex system methods; engineers should explicitly design artificial intelligence for human-machine and machine-machine interactions; and regulators should govern the ecological diversity and social co-development of humans and machines.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The illusion of households as entities in social networks</title>
<link>https://arxiv.org/abs/2502.14764</link>
<guid>https://arxiv.org/abs/2502.14764</guid>
<content:encoded><![CDATA[
<div> household, individual, social network, entitativity, data analysis  
Summary:  
This article discusses the differences between household and individual social networks and the importance of choosing the correct network for study design and data analysis. The study explores how the results of social network analysis can vary depending on whether the household or individual network is studied, impacting findings on assortativity, influence-maximizing nodes, and information spread within households. The authors propose systematic recommendations for determining the relevant network representation to study, considering entitativity criteria and cultural or experimental contexts. They highlight the illusion of entitativity as a factor where grouping individuals into households may not adequately capture social dynamics. Understanding which network to study is crucial for researchers and practitioners analyzing social network data, and this work aims to provide guidance for making informed decisions in data collection and analysis.  
<br /><br />Summary: <div>
arXiv:2502.14764v2 Announce Type: replace 
Abstract: Data recording connections between people in communities and villages are collected and analyzed in various ways, most often as either networks of individuals or as networks of households. These two networks can differ in substantial ways. The methodological choice of which network to study, therefore, is an important aspect in both study design and data analysis. In this work, we consider various key differences between household and individual social network structure, and ways in which the networks cannot be used interchangeably. In addition to formalizing the choices for representing each network, we explore the consequences of how the results of social network analysis change depending on the choice between studying the individual and household network -- from determining whether networks are assortative or disassortative to the ranking of influence-maximizing nodes. As our main contribution, we draw upon related work to propose a set of systematic recommendations for determining the relevant network representation to study. Our recommendations include assessing a series of entitativity criteria and relating these criteria to theories and observations about patterns and norms in social dynamics at the household level: notably, how information spreads within households and how power structures and gender roles affect this spread. We draw upon the definition of an illusion of entitativity to identify cases wherein grouping people into households does not satisfy these criteria or adequately represent given cultural or experimental contexts. Given the widespread use of social network data for studying communities, there is broad impact in understanding which network to study and the consequences of that decision. We hope that this work gives guidance to practitioners and researchers collecting and studying social network data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omitted Labels Induce Nontransitive Paradoxes in Causality</title>
<link>https://arxiv.org/abs/2311.06840</link>
<guid>https://arxiv.org/abs/2311.06840</guid>
<content:encoded><![CDATA[
<div> omitted label contexts, training data, specialized human experts, Simpson's paradox, nontransitive structures <br />
Summary: 
The article discusses the concept of omitted label contexts in training data, common in specialized fields or focused studies. It explores how adjustments in such contexts may require non-exchangeable treatment and control groups. Through studying Simpson's paradox, the article identifies the existence of nontransitivity in networks of conclusions drawn from different contexts. It demonstrates that the space of possible nontransitive structures in these networks corresponds to structures formed from aggregating ranked-choice votes. Overall, the study sheds light on the complexities of analyzing datasets with limited label contexts and the implications of nontransitivity in drawing conclusions from diverse sets of data. <div>
arXiv:2311.06840v4 Announce Type: replace-cross 
Abstract: We explore "omitted label contexts," in which training data is limited to a subset of the possible labels. This setting is standard among specialized human experts or specific, focused studies. By studying Simpson's paradox, we observe that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. A generalization of Simpson's paradox leads us to study networks of conclusions drawn from different contexts, within which a paradox of nontransitivity arises. We prove that the space of possible nontransitive structures in these networks exactly corresponds to structures that form from aggregating ranked-choice votes.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Updating Katz centrality by counting walks</title>
<link>https://arxiv.org/abs/2411.19560</link>
<guid>https://arxiv.org/abs/2411.19560</guid>
<content:encoded><![CDATA[
<div> Efficient, effective, update, Katz centralities, node removal, edge removal, loss of walks, network, algorithms, F-avoiding first-passage walks, total network communicability, numerical experiments, synthetic networks, real-world networks.

Summary:
This article introduces strategies for updating Katz centralities in simple graphs following node and edge removal. Formulas for measuring the "loss of walks" in a network due to these removals are provided, based on the concept of F-avoiding first-passage walks. The article also presents algorithms informed by these formulas and derives bounds on changes in total network communicability. Extensive numerical experiments on both synthetic and real-world networks validate the theoretical findings. The study emphasizes efficient and effective approaches for maintaining centrality measures in networks undergoing structural changes. <div>
arXiv:2411.19560v2 Announce Type: replace-cross 
Abstract: We develop efficient and effective strategies for the update of Katz centralities after node and edge removal in simple graphs. We provide explicit formulas for the ``loss of walks" a network suffers when nodes/edges are removed, and use these to inform our algorithms. The theory builds on the newly introduced concept of $\cF$-avoiding first-passage walks. Further, bounds on the change of total network communicability are also derived. Extensive numerical experiments on synthetic and real-world networks complement our theoretical results.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Small Communities in the Planted Partition Model</title>
<link>https://arxiv.org/abs/2504.01663</link>
<guid>https://arxiv.org/abs/2504.01663</guid>
<content:encoded><![CDATA[
<div> community recovery, planted partition model, correlation coefficient, Diamond Percolation, power-law distribution<br />
Summary:<br />
In this paper, the authors analyze community recovery in the planted partition model (PPM) with a focus on scenarios with a large number of communities. They redefine recovery regimes using the correlation coefficient to accommodate varying community sizes. The Diamond Percolation algorithm is introduced as an effective method for recovering communities with minimal constraints on community numbers and sizes. The algorithm shows promising results under mild assumptions on edge probabilities. Additionally, the study considers unbalanced partitions, particularly when community sizes follow a power-law distribution, which is common in real-world networks. These findings present valuable insights into community recovery techniques in complex network structures. <div>
arXiv:2504.01663v2 Announce Type: replace-cross 
Abstract: We analyze community recovery in the planted partition model (PPM) in regimes where the number of communities is arbitrarily large. We examine the three standard recovery regimes: exact recovery, almost exact recovery, and weak recovery. When communities vary in size, traditional accuracy- or alignment-based metrics become unsuitable for assessing the correctness of a predicted partition. To address this, we redefine these recovery regimes using the correlation coefficient, a more versatile metric for comparing partitions. We then demonstrate that $\textit{Diamond Percolation}$, an algorithm based on common-neighbors, successfully recovers communities under mild assumptions on edge probabilities, with minimal restrictions on the number and sizes of communities. As a key application, we consider the case where community sizes follow a power-law distribution, a characteristic frequently found in real-world networks. To the best of our knowledge, we provide the first recovery results for such unbalanced partitions.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection</title>
<link>https://arxiv.org/abs/2504.21357</link>
<guid>https://arxiv.org/abs/2504.21357</guid>
<content:encoded><![CDATA[
<div> Keywords: information cocoon, social media debates, double-layer network, graph auto-encoder, community detection algorithms

Summary: 
This paper addresses the issue of information cocoons in social media debates resulting from homogeneous viewpoints and preferences clustering users into sub-networks. The authors propose a double-layer network model considering relational ties and feature-based user similarity. They develop graph auto-encoder based community detection algorithms to identify and break information cocoons. Testing on real and synthetic datasets shows the proposed algorithms outperform existing methods in partitioning user communities. An intervention strategy based on influence is introduced, showing how the algorithms can effectively reduce polarization and information cocoon formation with minimal intervention. The Markov states transition model is used to simulate intervention effects, demonstrating the effectiveness of the proposed algorithms in mitigating information cocoons. <br /><br />Summary: <div>
arXiv:2504.21357v1 Announce Type: new 
Abstract: With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Machine Learning for characterizing social networks Agent-based models</title>
<link>https://arxiv.org/abs/2504.21609</link>
<guid>https://arxiv.org/abs/2504.21609</guid>
<content:encoded><![CDATA[
<div> Keywords: social media networks, agent-based modeling, High Performance Computing, Machine Learning, user behaviors

Summary: 
Agent-based modeling (ABM) is a valuable tool for studying social media networks, allowing for the simulation of individual behaviors and system-level evolution. However, the complexity of modeling social networks requires superior data processing and storage capabilities, which can be provided by High Performance Computing (HPC). By leveraging Machine Learning (ML) methods, researchers can efficiently analyze vast amounts of data from social media users to better understand behaviors, preferences, and trends. This proposal aims to use ML to characterize user attributes and develop a general user model for ABM simulations of social networks on HPC systems. By combining ABM, HPC, and ML, researchers can gain valuable insights into social network dynamics and user interactions. <div>
arXiv:2504.21609v1 Announce Type: new 
Abstract: Nowadays, social media networks are increasingly significant to our lives, the imperative to study social media networks becomes more and more essential. With billions of users across platforms and constant updates, the complexity of modeling social networks is immense. Agent-based modeling (ABM) is widely employed to study social networks community, allowing us to define individual behaviors and simulate system-level evolution. It can be a powerful tool to test how the algorithms affect users behavior. To fully leverage agent-based models,superior data processing and storage capabilities are essential. High Performance Computing (HPC) presents an optimal solution, adept at managing complex computations and analysis, particularly for voluminous or iteration-intensive tasks. We utilize Machine Learning (ML) methods to analyze social media users due to their ability to efficiently process vast amounts of data and derive insights that aid in understanding user behaviors, preferences, and trends. Therefore, our proposal involves ML to characterize user attributes and to develop a general user model for ABM simulation of in social networks on HPC systems.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models</title>
<link>https://arxiv.org/abs/2504.21026</link>
<guid>https://arxiv.org/abs/2504.21026</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual users, abusive language detection, code-mixed text, low-resource languages, NLP<br />
Summary:<br />
- Detecting abusive language in code-mixed text poses challenges due to linguistic blending and context dependency.
- A manually annotated dataset of Telugu-English and Nepali-English code-mixed comments was introduced for abusive language detection.
- Different machine learning and deep learning models were experimented with, including Logistic Regression, Neural Networks, and Large Language Models.
- Performance was optimized through hyperparameter tuning and evaluated using 10-fold cross-validation.
- The study provides insights into the difficulties of detecting abusive language in code-mixed settings and establishes benchmarks for abusive language detection in low-resource languages like Telugu and Nepali. This can aid in the development of more robust moderation strategies for multilingual social media environments.<br /> <div>
arXiv:2504.21026v1 Announce Type: cross 
Abstract: With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construct to Commitment: The Effect of Narratives on Economic Growth</title>
<link>https://arxiv.org/abs/2504.21060</link>
<guid>https://arxiv.org/abs/2504.21060</guid>
<content:encoded><![CDATA[
<div> Keywords: government-led narratives, mass media, Innovation-Driven Development Strategy, total factor productivity, economic growth

Summary:
The article presents the "Narratives-Construct-Commitment (NCC)" framework, which explores how government-led narratives evolve from framing expectations to becoming sustainable pillars for growth. By analyzing the Innovation-Driven Development Strategy of 2016 as a case study, the study identifies the impact of narrative shocks and their influence on investment incentives, R&amp;D resources, and total factor productivity (TFP). The findings highlight the role of credible narratives in shaping expectations, driving economic growth, and institutionalizing vision for sustained improvements. The research provides insights into the transformation of visions into tangible economic outcomes through the strategic use of narratives in policy-making and development initiatives. <div>
arXiv:2504.21060v1 Announce Type: cross 
Abstract: We study how government-led narratives through mass media evolve from construct, a mechanism for framing expectations, into commitment, a sustainable pillar for growth. We propose the ``Narratives-Construct-Commitment (NCC)" framework outlining the mechanism and institutionalization of narratives, and formalize it as a dynamic Bayesian game. Using the Innovation-Driven Development Strategy (2016) as a case study, we identify the narrative shock from high-frequency financial data and trace its impact using local projection method. By shaping expectations, credible narratives institutionalize investment incentives, channel resources into R\&amp;D, and facilitate sustained improvements in total factor productivity (TFP). Our findings strive to provide insights into the New Quality Productive Forces initiative, highlighting the role of narratives in transforming vision into tangible economic growth.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Manipulated Contents Using Knowledge-Grounded Inference</title>
<link>https://arxiv.org/abs/2504.21165</link>
<guid>https://arxiv.org/abs/2504.21165</guid>
<content:encoded><![CDATA[
<div> Fake news, manipulated content, detection, zero-day, mainstream search engines <br />
<br />
Summary: 
The article introduces Manicod, a tool designed to detect zero-day manipulated content by utilizing contextual information from mainstream search engines. By sourcing real-time context and using a large language model (LLM) with retrieval-augmented generation (RAG), Manicod can determine if a piece of content is truthful or manipulated, providing an explanation for its decision. The tool is validated using a dataset of 4270 manipulated fake news articles and achieves an overall F1 score of 0.856, outperforming existing methods in fact-checking and claim verification. Manicod addresses the challenge of zero-day manipulated content, offering a promising solution for identifying fake news in real-time scenarios. <div>
arXiv:2504.21165v1 Announce Type: cross 
Abstract: The detection of manipulated content, a prevalent form of fake news, has been widely studied in recent years. While existing solutions have been proven effective in fact-checking and analyzing fake news based on historical events, the reliance on either intrinsic knowledge obtained during training or manually curated context hinders them from tackling zero-day manipulated content, which can only be recognized with real-time contextual information. In this work, we propose Manicod, a tool designed for detecting zero-day manipulated content. Manicod first sources contextual information about the input claim from mainstream search engines, and subsequently vectorizes the context for the large language model (LLM) through retrieval-augmented generation (RAG). The LLM-based inference can produce a "truthful" or "manipulated" decision and offer a textual explanation for the decision. To validate the effectiveness of Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake news derived from 2500 recent real-world news headlines. Manicod achieves an overall F1 score of 0.856 on this dataset and outperforms existing methods by up to 1.9x in F1 score on their benchmarks on fact-checking and claim verification.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Metric Dimension of Sparse Random Graphs</title>
<link>https://arxiv.org/abs/2504.21244</link>
<guid>https://arxiv.org/abs/2504.21244</guid>
<content:encoded><![CDATA[
<div> bounds, metric dimension, random graphs, Erds-Rnyi, connectivity transition <br />
Summary: 
The article presents upper and lower bounds on the likely metric dimension of Erds-Rnyi random graphs. Previous research had provided bounds for random graphs with expected degrees greater than or equal to log^5 n, leaving a gap for sparser graphs with lower expected degrees. The new bounds cover the range just above the connectivity transition, where the expected degree is a constant multiple of the logarithm of n, up to log^5 n. The lower bound is based on an entropic argument, offering a more general approach compared to previous methodologies, while the upper bound is similar to existing results. These findings contribute to understanding the metric properties of random graphs across a wide range of densities. <br /><br />Summary: <div>
arXiv:2504.21244v1 Announce Type: cross 
Abstract: In 2013, Bollob\'as, Mitsche, and Pralat at gave upper and lower bounds for the likely metric dimension of random Erd\H{o}s-R\'enyi graphs $G(n,p)$ for a large range of expected degrees $d=pn$. However, their results only apply when $d \ge \log^5 n$, leaving open sparser random graphs with $d < \log^5 n$. Here we provide upper and lower bounds on the likely metric dimension of $G(n,p)$ from just above the connectivity transition, i.e., where $d=pn=c \log n$ for some $c > 1$, up to $d=\log^5 n$. Our lower bound technique is based on an entropic argument which is more general than the use of Suen's inequality by Bollob\'as, Mitsche, and Pralat, whereas our upper bound is similar to theirs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defense Against Shortest Path Attacks</title>
<link>https://arxiv.org/abs/2305.19083</link>
<guid>https://arxiv.org/abs/2305.19083</guid>
<content:encoded><![CDATA[
<div> defense, shortest paths, graph manipulation, Stackelberg game, NP-hard

Summary:
This paper addresses the issue of defending against malicious manipulation of graphs to control traffic flow between nodes. The proposed defense strategy involves modifying edge weights in order to recommend shortest paths to users while maintaining the integrity of the original graph. The defender aims to minimize the probability of attacks from malicious actors while minimizing negative impacts on benign users. The defense is formulated as a Stackelberg game, with the defender taking the leading role. The problem is proven to be NP-hard, and heuristic solutions are proposed for both zero-sum and non-zero-sum scenarios. By formulating a linear program for local optimization, the defense strategy achieves results close to the lower bound of the defender's cost. Experimental results with synthetic and real networks demonstrate the effectiveness of the proposed methods in defending against graph manipulation attacks. 

<br /><br />Summary: <div>
arXiv:2305.19083v2 Announce Type: replace 
Abstract: Identifying shortest paths between nodes in a network is an important task in many applications. Recent work has shown that a malicious actor can manipulate a graph to make traffic between two nodes of interest follow their target path. In this paper, we develop a defense against such attacks by modifying the edge weights that users observe. The defender must balance inhibiting the attacker against any negative effects on benign users. Specifically, the defender's goals are: (a) recommend the shortest paths to users, (b) make the lengths of the shortest paths in the published graph close to those of the same paths in the true graph, and (c) minimize the probability of an attack. We formulate the defense as a Stackelberg game in which the defender is the leader and the attacker is the follower. We also consider a zero-sum version of the game in which the defender's goal is to minimize cost while achieving the minimum possible attack probability. We show that the defense problem is NP-hard and propose heuristic solutions for both the zero-sum and non-zero-sum settings. By relaxing some constraints of the original problem, we formulate a linear program for local optimization around a feasible point. We present defense results with both synthetic and real networks and show that our methods often reach the lower bound of the defender's cost.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOPIM: Bayesian Optimization for influence maximization on temporal networks</title>
<link>https://arxiv.org/abs/2308.04700</link>
<guid>https://arxiv.org/abs/2308.04700</guid>
<content:encoded><![CDATA[
<div> Bayesian Optimization, Influence Maximization, Temporal Networks, Gaussian Process Regression, Expected Improvement <br />
Summary:<br />
The study introduces BOPIM, a Bayesian Optimization approach for Influence Maximization on temporal networks. The challenges addressed include constructing kernel functions based on Hamming distance and Jaccard coefficient, and optimizing the acquisition function using Expected Improvement with noise adjustment. Numerical experiments on real-world networks show that BOPIM outperforms other methods and achieves comparable influence spreads to a gold-standard greedy algorithm, with a significantly faster runtime. Surprisingly, the Hamming kernel performs better than the Jaccard kernel. The study also explores ways to quantify uncertainty in optimal seed sets, a novel approach in Influence Maximization research. <div>
arXiv:2308.04700v4 Announce Type: replace 
Abstract: The goal of influence maximization (IM) is to select a small set of seed nodes which maximizes the spread of influence on a network. In this work, we propose BOPIM, a Bayesian Optimization (BO) algorithm for IM on temporal networks. The IM task is well-suited for a BO solution due to its expensive and complicated objective function. There are at least two key challenges, however, that must be overcome, primarily due to the inputs coming from a cardinality-constrained, non-Euclidean, combinatorial space. The first is constructing the kernel function for the Gaussian Process regression. We propose two kernels, one based on the Hamming distance between seed sets and the other leveraging the Jaccard coefficient between node's neighbors. The second challenge is the acquisition function. For this, we use the Expected Improvement function, suitably adjusting for noise in the observations, and optimize it using a greedy algorithm to account for the cardinality constraint. In numerical experiments on real-world networks, we prove that BOPIM outperforms competing methods and yields comparable influence spreads to a gold-standard greedy algorithm while being as much as ten times faster. In addition, we find that the Hamming kernel performs favorably compared to the Jaccard kernel in nearly all settings, a somewhat surprising result as the former does not explicitly account for the graph structure. Finally, we demonstrate two ways that the proposed method can quantify uncertainty in optimal seed sets. To our knowledge, this is the first attempt to look at uncertainty in the seed sets for IM.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes</title>
<link>https://arxiv.org/abs/2408.05794</link>
<guid>https://arxiv.org/abs/2408.05794</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Multimodal Models, HateSieve, Hateful Memes, Contrastive Meme Generator, Memes 

Summary:<br /><br />Amidst the increasing use of Large Multimodal Models (LMMs) in creating and interpreting complex content, the threat of spreading biased and harmful memes persists. Current safety protocols often struggle to uncover hate speech subtly embedded in "Confounder Memes." To tackle this issue, the researchers introduce HateSieve, a new framework focused on improving the detection and segmentation of hateful elements in memes. HateSieve employs a unique Contrastive Meme Generator to create semantically paired memes, a tailored triplet dataset for contrastive learning, and an Image-Text Alignment module to generate context-aware embeddings for precise meme segmentation. Experimental results using the Hateful Meme Dataset demonstrate that HateSieve outperforms existing LMMs in accuracy with fewer parameters while providing a reliable method for pinpointing and isolating hate speech within memes. Viewer discretion is advised due to the academic discussions of hate speech. <div>
arXiv:2408.05794v2 Announce Type: replace-cross 
Abstract: Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within ``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new framework designed to enhance the detection and segmentation of hateful elements in memes. \textsc{HateSieve} features a novel Contrastive Meme Generator that creates semantically paired memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. \textcolor{red}{Caution: Contains academic discussions of hate speech; viewer discretion advised.}
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triadic Closure-Heterogeneity-Harmony GCN for Link Prediction</title>
<link>https://arxiv.org/abs/2504.20492</link>
<guid>https://arxiv.org/abs/2504.20492</guid>
<content:encoded><![CDATA[
<div> Link prediction, TriHetGCN, Graph Convolutional Networks, topological indicators, connection probability <br />
<br />
Summary: TriHetGCN is proposed to enhance link prediction in complex networks. It integrates topological indicators like triadic closure and degree heterogeneity into the Graph Convolutional Networks (GCNs) framework. The model consists of three modules: topology feature construction, graph structural representation, and connection probability prediction. By incorporating node features, TriHetGCN improves global structure perception and effectively captures intrinsic structural relationships between node pairs. Evaluated on various real-world datasets, TriHetGCN outperforms existing methods and demonstrates strong generalizability across different network types. This work bridges statistical physics and graph deep learning, offering a promising framework for diverse applications in link prediction. <div>
arXiv:2504.20492v1 Announce Type: new 
Abstract: Link prediction aims to estimate the likelihood of connections between pairs of nodes in complex networks, which is beneficial to many applications from friend recommendation to metabolic network reconstruction. Traditional heuristic-based methodologies in the field of complex networks typically depend on predefined assumptions about node connectivity, limiting their generalizability across diverse networks. While recent graph neural network (GNN) approaches capture global structural features effectively, they often neglect node attributes and intrinsic structural relationships between node pairs. To address this, we propose TriHetGCN, an extension of traditional Graph Convolutional Networks (GCNs) that incorporates explicit topological indicators -- triadic closure and degree heterogeneity. TriHetGCN consists of three modules: topology feature construction, graph structural representation, and connection probability prediction. The topology feature module constructs node features using shortest path distances to anchor nodes, enhancing global structure perception. The graph structural module integrates topological indicators into the GCN framework to model triadic closure and heterogeneity. The connection probability module uses deep learning to predict links. Evaluated on nine real-world datasets, from traditional networks without node attributes to large-scale networks with rich features, TriHetGCN achieves state-of-the-art performance, outperforming mainstream methods. This highlights its strong generalization across diverse network types, offering a promising framework that bridges statistical physics and graph deep learning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts</title>
<link>https://arxiv.org/abs/2504.20065</link>
<guid>https://arxiv.org/abs/2504.20065</guid>
<content:encoded><![CDATA[
<div> Plato, Aristotle, references, network analysis, historical works
Summary: The study used computational methods to analyze references in 2,245 philosophical texts from 550 BCE to 1940 AD. It mapped over 294,970 references between authors to measure how philosophical ideas spread over time. Plato and Aristotle accounted for nearly 10% of all references, indicating their significant influence. The analysis supported the view of St. Thomas Aquinas as a synthesizer between Aristotelian and Christian philosophy. The results were presented through an interactive visualization tool, allowing users to explore the networks dynamically. The methodology demonstrated the value of applying network analysis to study the intellectual lineages of philosophical scholarship through textual references. <div>
arXiv:2504.20065v1 Announce Type: cross 
Abstract: We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication in Agile Software Development - A Mapping Study</title>
<link>https://arxiv.org/abs/2504.20186</link>
<guid>https://arxiv.org/abs/2504.20186</guid>
<content:encoded><![CDATA[
<div> Keywords: Software industry, Agile software development, Communication, Review, Research gaps

Summary: 
In the fast-paced software industry, Agile software development (ASD) is crucial to ensure fast and efficient development processes. However, despite ASD being prevalent for over two decades, there are still many unknowns related to it. This study focuses on the critical factor of communication within ASD. Through a review of 14 studies, the areas of interest and research gaps in ASD communication were identified. The community's interest in communication within ASD was highlighted, shedding light on the importance of effective communication in agile development processes. Addressing these research gaps can lead to a better understanding of how communication impacts the success of ASD initiatives. Overall, this study emphasizes the significance of clear and efficient communication in Agile software development practices. 

Summary: <div>
arXiv:2504.20186v1 Announce Type: cross 
Abstract: Software industry is a fast-moving industry and to keep up with this pace the development process also needs to be fast and efficient and Agile software development (ASD) is the answer to this problem. Even though ASD has been in there for over two decades there are still multiple unknown questions tied to ASD that need to be addressed. In this study we are going to address one of the most critical factors of ASD i.e. Communication. We conducted a review of 14 studies and found the areas under ASD communication that the community is interested in as well as research gaps.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community detection in multi-layer networks by regularized debiased spectral clustering</title>
<link>https://arxiv.org/abs/2409.07956</link>
<guid>https://arxiv.org/abs/2409.07956</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, multi-layer networks, regularized Laplacian matrix, stochastic block model, modularity

Summary:<br /><br />
This study introduces the regularized debiased sum of squared adjacency matrices (RDSoS) method for community detection in multi-layer networks. RDSoS extends the classical regularized Laplacian matrix to handle multi-layer networks, showing potential in various applications such as gene function prediction and fraud detection. The method is consistent under the multi-layer stochastic block model and its degree-corrected version. A new metric, sum of squared adjacency matrices modularity (SoS-modularity), is introduced to assess community quality and estimate the number of communities. Experimental results demonstrate the method's superiority over state-of-the-art techniques, insensitivity to regularizer selection, and ability to reveal the assortative property of real networks. SoS-modularity provides a more accurate evaluation of community quality compared to traditional metrics. This work opens up new possibilities for community detection in complex multi-layer networks. <div>
arXiv:2409.07956v2 Announce Type: replace-cross 
Abstract: Community detection is a crucial problem in the analysis of multi-layer networks. While regularized spectral clustering methods using the classical regularized Laplacian matrix have shown great potential in handling sparse single-layer networks, to our knowledge, their potential in multi-layer network community detection remains unexplored. To address this gap, in this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical one to multi layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. Additionally, we introduce a sum of squared adjacency matrices modularity (SoS-modularity) to measure the quality of community partitions in multi-layer networks and estimate the number of communities by maximizing this metric. Our methods offer promising applications for predicting gene functions, improving recommender systems, detecting medical insurance fraud, and facilitating link prediction. Experimental results demonstrate that our methods exhibit insensitivity to the selection of the regularizer, generally outperform state-of-the-art techniques, uncover the assortative property of real networks, and that our SoS-modularity provides a more accurate assessment of community quality compared to the average of the Newman-Girvan modularity across layers.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events</title>
<link>https://arxiv.org/abs/2504.18837</link>
<guid>https://arxiv.org/abs/2504.18837</guid>
<content:encoded><![CDATA[
<div> Keywords: extreme weather events, sentiment analysis, social media, climate change, wildfires 

Summary: 
Sentiment analysis plays a crucial role in understanding public perception of extreme weather events driven by climate change, such as wildfires and floods, on social media platforms. The survey explores various methods for sentiment analysis, including lexicon-based, machine learning models, and large language models. It also discusses challenges and ethical considerations related to analyzing sentiment during real-time, high-impact situations like the 2025 Los Angeles forest fires. Data collection and annotation techniques, such as weak supervision and real-time event tracking, are important for accurate sentiment analysis. Open problems include misinformation detection, multimodal sentiment extraction, and ensuring alignment with human values. The goal of the survey is to provide guidance for researchers and practitioners in effectively understanding sentiment during the climate crisis era. 

Summary: <div>
arXiv:2504.18837v1 Announce Type: new 
Abstract: Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Substructure Discovery Algorithm For Homogeneous Multilayer Networks</title>
<link>https://arxiv.org/abs/2504.19328</link>
<guid>https://arxiv.org/abs/2504.19328</guid>
<content:encoded><![CDATA[
<div> Keywords: substructure discovery, graph mining, multilayer networks, decoupling approach, distributed processing<br />
Summary:<br />
Graph mining focuses on finding core substructures in real-world graphs. Substructure discovery involves identifying meaningful patterns in large datasets. Multilayer networks (MLNs) are effective for modeling complex datasets with multiple entity types and relationships. This paper proposes a novel decoupling-based approach for substructure discovery in homogeneous MLNs. The approach processes each layer independently and then composes results from multiple layers to identify substructures in the entire network. The algorithm is implemented using the Map/Reduce paradigm for scalability. Experimental analysis on synthetic and real-world datasets demonstrates the correctness, speedup, and response time of the algorithm. <div>
arXiv:2504.19328v1 Announce Type: new 
Abstract: Graph mining analyzes real-world graphs to find core substructures (connected subgraphs) in applications modeled as graphs. Substructure discovery is a process that involves identifying meaningful patterns, structures, or components within a large data set. These substructures can be of various types, such as frequent patterns, motifs, or other relevant features within the data.
  To model complex data sets -- with multiple types of entities and relationships -- multilayer networks (or MLNs) have been shown to be more effective as compared to simple and attributed graphs. Analysis algorithms on MLNs using the decoupling approach have been shown to be both efficient and accurate. Hence, this paper focuses on substructure discovery in homogeneous multilayer networks (one type of MLN) using a novel decoupling-based approach. In this approach, each layer is processed independently, and then the results from two or more layers are composed to identify substructures in the entire MLN. The algorithm is designed and implemented, including the composition part, using one of the distributed processing frameworks (the Map/Reduce paradigm) to provide scalability.
  After establishing the correctness, we analyze the speedup and response time of the proposed algorithm and approach through extensive experimental analysis on large synthetic and real-world data sets with diverse graph characteristics.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleScope: A Longitudinal Dataset for Investigating Online Discourse and Information Interaction on Telegram</title>
<link>https://arxiv.org/abs/2504.19536</link>
<guid>https://arxiv.org/abs/2504.19536</guid>
<content:encoded><![CDATA[
<div> Keyword: Telegram, dataset, social media, analysis, research <br />
Summary:<br />
This paper introduces TeleScope, a comprehensive dataset suite for analyzing Telegram channels. The dataset includes metadata for 500K channels and message data for 71K public channels, totaling 120M messages. It also provides channel connections and user interaction data for studying information spread and message forwarding patterns. Enrichments like language detection and message posting periods enhance the dataset for in-depth discourse analysis. The dataset enables diverse applications and reproducible social media studies, similar to those on platforms like Twitter. 

<br /><br />Summary: <div>
arXiv:2504.19536v1 Announce Type: new 
Abstract: Telegram is a globally popular instant messaging platform known for its strong emphasis on security, privacy, and unique social networking features. It has recently emerged as the host for various cross-domain analysis and research works, such as social media influence, propaganda studies, and extremism. This paper introduces TeleScope, an extensive dataset suite that, to our knowledge, is the largest of its kind. It comprises metadata for about 500K Telegram channels and downloaded message metadata for about 71K public channels, accounting for around 120M crawled messages. We also release channel connections and user interaction data built using Telegram's message-forwarding feature to study multiple use cases, such as information spread and message forwarding patterns. In addition, we provide data enrichments, such as language detection, active message posting periods for each channel, and Telegram entities extracted from messages, that enable online discourse analysis beyond what is possible with the original data alone. The dataset is designed for diverse applications, independent of specific research objectives, and sufficiently versatile to facilitate the replication of social media studies comparable to those conducted on platforms like X (formerly Twitter)
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Italian Telegram Ecosystem</title>
<link>https://arxiv.org/abs/2504.19594</link>
<guid>https://arxiv.org/abs/2504.19594</guid>
<content:encoded><![CDATA[
<div> Keywords: Telegram, Italian, network analysis, toxicity, extremism<br />
Summary: <br />
This study analyzes the Italian Telegram sphere, examining the spread of misinformation, extremism, and toxicity within the platform's unmoderated environment. Using a dataset of 186 million messages from 13,151 chats collected in 2023, the research employs network analysis and Large Language Models to explore thematic communities and ideological alignment. Results reveal strong thematic and ideological homophily, with far-left and far-right rhetoric coexisting in mixed ideological communities on certain issues. Toxicity is found to be normalized within highly toxic communities, with Italians primarily targeting Black people, Jews, and gay individuals. Additionally, intra-national hostility is observed, reflecting regional and intra-regional cultural conflicts rooted in historical divisions. This comprehensive analysis provides valuable insights into the dynamics of the Italian Telegram ecosystem and sheds light on online toxicity in various cultural and linguistic contexts. <br /> <div>
arXiv:2504.19594v1 Announce Type: new 
Abstract: Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observational Learning with a Budget</title>
<link>https://arxiv.org/abs/2504.19396</link>
<guid>https://arxiv.org/abs/2504.19396</guid>
<content:encoded><![CDATA[
<div> Bayesian learning, observational model, signal, central planner, budget allocation <br />
Summary: <br />
The article discusses a Bayesian observational learning model in which agents receive private signals about a binary state and make decisions based on their signals and previous observations. A central planner aims to enhance signal quality across agents by allocating a limited budget. The budget allocation problem is formulated and analyzed, and two optimal strategies are proposed. One of these strategies maximizes the likelihood of achieving a correct information cascade. <div>
arXiv:2504.19396v1 Announce Type: cross 
Abstract: We consider a model of Bayesian observational learning in which a sequence of agents receives a private signal about an underlying binary state of the world. Each agent makes a decision based on its own signal and its observations of previous agents. A central planner seeks to improve the accuracy of these signals by allocating a limited budget to enhance signal quality across agents. We formulate and analyze the budget allocation problem and propose two optimal allocation strategies. At least one of these strategies is shown to maximize the probability of achieving a correct information cascade.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Cohesive Are Community Search Results on Online Social Networks?: An Experimental Evaluation</title>
<link>https://arxiv.org/abs/2504.19489</link>
<guid>https://arxiv.org/abs/2504.19489</guid>
<content:encoded><![CDATA[
<div> community search algorithms, cohesiveness measures, online social networks, group cohesion, CHASE framework

Summary: This paper evaluates the effectiveness of community search algorithms in online social networks based on cohesiveness measures. While current methods primarily use structural or attribute-based approaches to measure cohesiveness, this study introduces five psychology-informed measures based on group cohesion theory from social psychology. The novel CHASE framework is proposed to evaluate eight representative algorithms on these measures. The analysis reveals a lack of correlation between structural and psychological cohesiveness, highlighting the challenge in identifying psychologically cohesive communities in online social networks. This study provides valuable insights for the development of future community search methods. <br /><br /> <div>
arXiv:2504.19489v1 Announce Type: cross 
Abstract: Recently, numerous community search methods for large graphs have been proposed, at the core of which is defining and measuring cohesion. This paper experimentally evaluates the effectiveness of these community search algorithms w.r.t. cohesiveness in the context of online social networks. Social communities are formed and developed under the influence of group cohesion theory, which has been extensively studied in social psychology. However, current generic methods typically measure cohesiveness using structural or attribute-based approaches and overlook domain-specific concepts such as group cohesion. We introduce five novel psychology-informed cohesiveness measures, based on the concept of group cohesion from social psychology, and propose a novel framework called CHASE for evaluating eight representative CS algorithms w.r.t.these measures on online social networks. Our analysis reveals that there is no clear correlation between structural and psychological cohesiveness, and no algorithm effectively identifies psychologically cohesive communities in online social networks. This study provides new insights that could guide the development of future community search methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding</title>
<link>https://arxiv.org/abs/2504.19734</link>
<guid>https://arxiv.org/abs/2504.19734</guid>
<content:encoded><![CDATA[
<div> Keywords: Dialogue data, Large Language Models, Automated coding, Communicative acts, Contextual complexity

Summary:
- The study introduces a novel LLM-assisted automated coding approach for dialogue data.
- Code prediction for utterances is based on dialogue-specific characteristics using separate prompts.
- Multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek are engaged in collaborative code prediction.
- Contextual consistency checking using GPT-4o substantially improved accuracy.
- Accuracy of act predictions was consistently higher than that of event predictions.

<br /><br />Summary: This study presents a new methodological framework for improving the precision of automated coding of dialogue data by leveraging Large Language Models. It introduces a novel approach where code prediction for utterances is based on specific dialogue characteristics through separate prompts. Multiple LLMs are used for collaborative code prediction, and a contextual consistency checking method significantly enhances accuracy. The study highlights the importance of understanding communicative acts and events in dialogue analysis, with act predictions consistently outperforming event predictions. This innovative framework provides a scalable solution for addressing contextual challenges in dialogue analysis. <div>
arXiv:2504.19734v1 Announce Type: cross 
Abstract: Dialogue data has been a key source for understanding learning processes, offering critical insights into how students engage in collaborative discussions and how these interactions shape their knowledge construction. The advent of Large Language Models (LLMs) has introduced promising opportunities for advancing qualitative research, particularly in the automated coding of dialogue data. However, the inherent contextual complexity of dialogue presents unique challenges for these models, especially in understanding and interpreting complex contextual information. This study addresses these challenges by developing a novel LLM-assisted automated coding approach for dialogue data. The novelty of our proposed framework is threefold: 1) We predict the code for an utterance based on dialogue-specific characteristics -- communicative acts and communicative events -- using separate prompts following the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We leveraged the interrelation between events and acts to implement consistency checking using GPT-4o. In particular, our contextual consistency checking provided a substantial accuracy improvement. We also found the accuracy of act predictions was consistently higher than that of event predictions. This study contributes a new methodological framework for enhancing the precision of automated coding of dialogue data as well as offers a scalable solution for addressing the contextual challenges inherent in dialogue analysis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets</title>
<link>https://arxiv.org/abs/2410.13105</link>
<guid>https://arxiv.org/abs/2410.13105</guid>
<content:encoded><![CDATA[
<div> DeFi, lending, liquidity pools, interest rate curves, decentralized

Summary:
The article presents a dynamic model for the lending market in Decentralized Finance (DeFi) that addresses inefficiencies and risks in current platforms like Aave and Compound. The proposed model incorporates evolving demand and supply curves along with an adaptive interest rate controller that reacts in real-time to market changes. By using a Recursive Least Squares algorithm, the controller ensures stable utilization and manages default and liquidation risks. The algorithm offers theoretical guarantees on interest rate convergence and utilization stability while reducing vulnerability to adversarial manipulation compared to static curves. Two approaches are suggested to counter adversarial manipulation, including a detection method for extreme fluctuations and a market-based strategy to enhance elasticity. The model's performance is validated through Aave data, demonstrating low best-fit error, and improved utilization and liquidation management compared to static curve protocols. 
<br /><br />Summary: <div>
arXiv:2410.13105v4 Announce Type: replace 
Abstract: Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller tracks the external market and achieves stable utilization, while also controlling default and liquidation risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. We propose two complementary approaches to mitigating adversarial manipulation: an algorithmic method that detects extreme demand and supply fluctuations and a market-based strategy that enhances elasticity, potentially via interest rate derivative markets. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Guide to Misinformation Detection Data and Evaluation</title>
<link>https://arxiv.org/abs/2411.05060</link>
<guid>https://arxiv.org/abs/2411.05060</guid>
<content:encoded><![CDATA[
<div> Keywords: Misinformation, Datasets, Evaluation Quality Assurance, Detection model, Research

Summary:
This study addresses the challenging issue of misinformation by curating a comprehensive collection of datasets, totaling 75, for empirical research. The evaluation of these datasets reveals flaws in many, such as spurious correlations and ambiguous examples, impacting their reliability. State-of-the-art baselines are provided, highlighting the limitations of categorical labels in assessing detection model performance accurately. The authors propose Evaluation Quality Assurance (EQA) as a tool to guide the field towards systemic solutions and improve research quality in misinformation detection. The ultimate goal of this guide is to promote higher quality data and grounded evaluations to enhance the field's understanding and combat misinformation effectively.
<br /><br />Summary: <div>
arXiv:2411.05060v3 Announce Type: replace 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at misinfo-datasets.complexdatalab.com.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Participation in Collective Action from Social Media</title>
<link>https://arxiv.org/abs/2501.07368</link>
<guid>https://arxiv.org/abs/2501.07368</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, collective action, text classifiers, Reddit, computational social science

Summary:
Social media is crucial for mobilizing collective action and understanding individual engagement in global challenges. However, research in this area lacks granular data on participation levels. To address this gap, this study presents text classifiers that can identify participation expressions across different levels, from recognizing issues to active involvement. By training BERT and Llama3 models on Reddit data, the study demonstrates the effectiveness of smaller language models in detecting participation nuances. Applying this methodology to Reddit enables a more robust characterization of online communities compared to existing methods. This framework provides reliable annotations for Computational Social Science research to analyze the dynamics of collective action in online spaces. <div>
arXiv:2501.07368v2 Announce Type: replace 
Abstract: Social media play a key role in mobilizing collective action, holding the potential for studying the pathways that lead individuals to actively engage in addressing global challenges. However, quantitative research in this area has been limited by the absence of granular and large-scale ground truth about the level of participation in collective action among individual social media users. To address this limitation, we present a novel suite of text classifiers designed to identify expressions of participation in collective action from social media posts, in a topic-agnostic fashion. Grounded in the theoretical framework of social movement mobilization, our classification captures participation and categorizes it into four levels: recognizing collective issues, engaging in calls-to-action, expressing intention of action, and reporting active involvement. We constructed a labeled training dataset of Reddit comments through crowdsourcing, which we used to train BERT classifiers and fine-tune Llama3 models. Our findings show that smaller language models can reliably detect expressions of participation (weighted F1=0.71), and rival larger models in capturing nuanced levels of participation. By applying our methodology to Reddit, we illustrate its effectiveness as a robust tool for characterizing online communities in innovative ways compared to topic modeling, stance detection, and keyword-based methods. Our framework contributes to Computational Social Science research by providing a new source of reliable annotations useful for investigating the social dynamics of collective action.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Unknown Social Networks for Discovering Hidden Nodes</title>
<link>https://arxiv.org/abs/2501.12571</link>
<guid>https://arxiv.org/abs/2501.12571</guid>
<content:encoded><![CDATA[
<div> Hidden-node discovery, social networks, graph exploration, machine learning, node embeddings
<br />
Summary:
In this paper, the authors address the challenge of discovering hidden nodes in unknown social networks by formulating three types of hidden-node discovery problems: Sybil-node discovery, peripheral-node discovery, and influencer discovery. They employ a graph exploration framework grounded in machine learning to tackle these problems, constructing prediction models based on the subgraph structure obtained during exploration. Empirical investigations on real social graphs demonstrate the efficiency of graph exploration strategies in uncovering hidden nodes, with query cost multipliers of 1.2 for discovering 10% and 1.4 for discovering 90% of hidden nodes compared to the known topology case. The use of node embeddings for hidden-node discovery is found to be effective in certain scenarios but can degrade efficiency in others. The authors propose a bandit algorithm to combine prediction models using node embeddings with those that do not, showing that this approach achieves efficient node discovery across various settings.
<br /><br />Summary: <div>
arXiv:2501.12571v2 Announce Type: replace 
Abstract: In this paper, we address the challenge of discovering hidden nodes in unknown social networks, formulating three types of hidden-node discovery problems, namely, Sybil-node discovery, peripheral-node discovery, and influencer discovery. We tackle these problems by employing a graph exploration framework grounded in machine learning. Leveraging the structure of the subgraph gradually obtained from graph exploration, we construct prediction models to identify target hidden nodes in unknown social graphs. Through empirical investigations of real social graphs, we investigate the efficiency of graph exploration strategies in uncovering hidden nodes. Our results show that our graph exploration strategies discover hidden nodes with an efficiency comparable to that when the graph structure is known. Specifically, the query cost of discovering 10% of the hidden nodes is at most only 1.2 times that when the topology is known, and the query-cost multiplier for discovering 90% of the hidden nodes is at most only 1.4. Furthermore, our results suggest that using node embeddings, which are low-dimensional vector representations of nodes, for hidden-node discovery is a double-edged sword: it is effective in certain scenarios but sometimes degrades the efficiency of node discovery. Guided by this observation, we examine the effectiveness of using a bandit algorithm to combine the prediction models that use node embeddings with those that do not, and our analysis shows that the bandit-based graph exploration strategy achieves efficient node discovery across a wide array of settings.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Wisdom of Intellectually Humble Networks</title>
<link>https://arxiv.org/abs/2502.02015</link>
<guid>https://arxiv.org/abs/2502.02015</guid>
<content:encoded><![CDATA[
<div> Keywords: collective wisdom, intellectual humility, social networks, agent-based modeling, polarization

Summary: 
This paper examines the impact of intellectual humility on collective wisdom within social networks. Using agent-based modeling and data-calibrated simulations, the researchers demonstrate that intellectual humility can lead to more accurate estimations and reduce polarization in social networks. By fostering a mindset of openness to perspectives and willingness to revise beliefs, intellectual humility helps individuals avoid cognitive and social biases that can hinder collective wisdom. The study shows that interventions promoting intellectual humility can enhance the overall accuracy of group estimations while maintaining cohesion within social networks. The findings are robust across different task settings and network structures, suggesting the potential for practical applications in improving decision-making processes and policy outcomes in democratic societies. By understanding how intellectual humility influences group dynamics, policymakers and stakeholders can leverage this trait to enhance the quality of collective decision-making. 

<br /><br />Summary: <div>
arXiv:2502.02015v2 Announce Type: replace 
Abstract: People's collectively shared beliefs can have significant social implications, including on democratic processes and policies. Unfortunately, as people interact with peers to form and update their beliefs, various cognitive and social biases can hinder their collective wisdom. In this paper, we probe whether and how the psychological construct of intellectual humility can modulate collective wisdom in a networked interaction setting. Through agent-based modeling and data-calibrated simulations, we provide a proof of concept demonstrating that intellectual humility can foster more accurate estimations while mitigating polarization in social networks. We investigate the mechanisms behind the performance improvements and confirm robustness across task settings and network structures. Our work can guide intervention designs to capitalize on the promises of intellectual humility in boosting collective wisdom in social networks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The dynamics of leadership and success in software development teams</title>
<link>https://arxiv.org/abs/2404.18833</link>
<guid>https://arxiv.org/abs/2404.18833</guid>
<content:encoded><![CDATA[
<div> Keywords: teamwork, software development, collaborative processes, leadership change, success growth

Summary: 
Teams in software development projects were analyzed using fine-grained temporal data from Rust, JavaScript, and Python ecosystems. The study found that workload distribution within teams is uneven, with greater heterogeneity leading to higher success rates. A lead developer often emerges early on, shouldering the majority of work. A significant number of projects undergo a change in lead developer, with this transition more likely in projects led by inexperienced users. Leadership changes are associated with faster growth in project success. This research provides insights into the dynamics of online collaborative projects, highlighting the importance of understanding team evolution and its impact on success in collaborative processes.<br /><br />Summary: <div>
arXiv:2404.18833v3 Announce Type: replace-cross 
Abstract: From science to industry, teamwork plays a crucial role in knowledge production and innovation. Most studies consider teams as static groups of individuals, thereby failing to capture how the micro-dynamics of collaborative processes and organizational changes determine team success. Here, we leverage fine-grained temporal data on software development teams from three software ecosystems -- Rust, JavaScript, and Python -- to gain insights into the dynamics of online collaborative projects. Our analysis reveals an uneven workload distribution in teams, with stronger heterogeneity correlated with higher success, and the early emergence of a lead developer carrying out the majority of work. Moreover, we find that a sizeable fraction of projects experience a change of lead developer, with such a transition being more likely in projects led by inexperienced users. Finally, we show that leadership change is associated with faster success growth. Our work contributes to a deeper understanding of the link between team evolution and success in collaborative processes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models for newspaper sentiment analysis during COVID-19: The Guardian</title>
<link>https://arxiv.org/abs/2405.13056</link>
<guid>https://arxiv.org/abs/2405.13056</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, sentiment analysis, newspapers, The Guardian, public response

Summary:<br /><br />During the COVID-19 pandemic, The Guardian newspaper was analyzed for sentiment trends using large language models. The study focused on various stages of the pandemic, including initial transmission, lockdowns, and vaccination. The analysis revealed a shift in public sentiment from urgent crisis response to concerns about health and the economy. The study found a predominance of negative sentiments, such as sadness, annoyance, anxiety, and denial, in The Guardian's coverage, both before and during the pandemic. This contrasts with social media sentiment analyses, which showed a more diverse emotional reflection. Overall, The Guardian portrayed a grim narrative with negative sentiments prevailing across news sections for countries like Australia, the UK, and the world. Sentiment analysis of newspaper sources during COVID-19 can offer insights into how the media covered the pandemic and captured the evolving public response. <br /><br />Summary: <div>
arXiv:2405.13056v2 Announce Type: replace-cross 
Abstract: During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Compounded Burr Probability Distribution for Fitting Heavy-Tailed Data with Applications to Biological Networks</title>
<link>https://arxiv.org/abs/2407.04465</link>
<guid>https://arxiv.org/abs/2407.04465</guid>
<content:encoded><![CDATA[
<div> Keywords: biological networks, scale-free structures, Compounded Burr distribution, heavy-tailed degree distributions, maximum likelihood estimation <br />
Summary: <br />
Complex biological networks often exhibit scale-free structures, but empirical studies show deviations from ideal power law behavior. The Compounded Burr (CBurr) distribution, a novel four parameter family, is proposed to accurately model network degree distributions, with a focus on biological networks. Statistical properties of the CBurr distribution are rigorously derived, and an efficient maximum likelihood estimation framework is developed. The CBurr model showcases broad applicability in various domains, outperforming classical models like power-law and log-normal on biological network datasets. By providing a statistically grounded framework, the CBurr model enhances our understanding of the structural heterogeneity of biological networks. <div>
arXiv:2407.04465v3 Announce Type: replace-cross 
Abstract: Complex biological networks, encompassing metabolic pathways, gene regulatory systems, and protein-protein interaction networks, often exhibit scale-free structures characterized by heavy-tailed degree distributions. However, empirical studies reveal significant deviations from ideal power law behavior, underscoring the need for more flexible and accurate probabilistic models. In this work, we propose the Compounded Burr (CBurr) distribution, a novel four parameter family derived by compounding the Burr distribution with a discrete mixing process. This model is specifically designed to capture both the body and tail behavior of real-world network degree distributions with applications to biological networks. We rigorously derive its statistical properties, including moments, hazard and risk functions, and tail behavior, and develop an efficient maximum likelihood estimation framework. The CBurr model demonstrates broad applicability to networks with complex connectivity patterns, particularly in biological, social, and technological domains. Extensive experiments on large-scale biological network datasets show that CBurr consistently outperforms classical power-law, log-normal, and other heavy-tailed models across the full degree spectrum. By providing a statistically grounded and interpretable framework, the CBurr model enhances our ability to characterize the structural heterogeneity of biological networks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Collective Accuracy in Socially Connected Networks</title>
<link>https://arxiv.org/abs/2411.08625</link>
<guid>https://arxiv.org/abs/2411.08625</guid>
<content:encoded><![CDATA[
<div> network-mediated influence, collective decision-making, social networks, binary choices, group performance 

Summary:
In this study, the accuracy of collective decision-making in socially connected populations was analyzed. Agents in the network update binary choices based on private signals that are slightly biased towards the correct alternative. Through local interactions on the network, social influence plays a crucial role in aggregating these signals. The research found that in large-population scenarios, the probability of a correct majority converges to a specific mathematical expression involving the regularized incomplete beta function. Surprisingly, the collective accuracy exceeds that of individual agents when private signals are better than random, indicating that network-mediated influence can improve group performance. These results have implications for designing resilient decision-making systems in various networks, including social, biological, and engineered systems, where accuracy relies on the interactions of interdependent and noisy agents.<br /><br />Summary: <div>
arXiv:2411.08625v2 Announce Type: replace-cross 
Abstract: We analyze the accuracy of collective decision-making in socially connected populations, where agents update binary choices through local interactions on a network. Each agent receives a private signal that is biased -- even marginally -- toward the correct alternative, and social influence mediates the aggregation of these signals. We show analytically that, in the large-population limit, the probability of a correct majority converges to a nontrivial expression involving the regularized incomplete beta function. Remarkably, this collective accuracy surpasses that of any individual agent whenever private signals are better than random, revealing that network-mediated influence can enhance, rather than impair, group performance. Our findings may inform the design of resilient decision-making systems in social, biological, and engineered networks, where accuracy must emerge from interdependent and noisy agents.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Egocentric Temporal Neighborhoods to probe for spatial correlations in temporal networks and infer their topology</title>
<link>https://arxiv.org/abs/2501.16070</link>
<guid>https://arxiv.org/abs/2501.16070</guid>
<content:encoded><![CDATA[
<div> Keywords: motifs, temporal networks, triangles, edge-centered motifs, graph tiling theory

Summary:
Motifs are fundamental components of social face-to-face interaction temporal networks, with traditional motifs lacking in inclusivity or efficiency. This study introduces edge-centered motifs that encompass triangles and can be efficiently mined in any temporal network. Analytical comparisons with Egocentric Temporal Neighborhoods motifs show that edge-centered motifs provide relevant information. Empirical data analysis supports the significance of edge-centered motifs in probing spatial correlations in network dynamics. The distribution of edge-centered motifs in social face-to-face interaction networks is approximated. Exploration of using edge-centered motif statistics to infer complete network topology leads to the development of graph tiling theory, a new mathematical framework. The study highlights the importance of considering triangles and spatial correlations in understanding social systems through temporal networks. 

<br /><br />Summary: <div>
arXiv:2501.16070v2 Announce Type: replace-cross 
Abstract: Motifs are thought to be some fundamental components of social face-to-face interaction temporal networks. However, the motifs previously considered are either limited to a handful of nodes and edges, or do not include triangles, which are thought to be of critical relevance to understand the dynamics of social systems. Thus, we introduce a new class of motifs, that include these triangles, are not limited in their number of nodes or edges, and yet can be mined efficiently in any temporal network. Referring to these motifs as the edge-centered motifs, we show analytically how they subsume the Egocentric Temporal Neighborhoods motifs of the literature. We also confirm in empirical data that the edge-centered motifs bring relevant information with respect to the Egocentric motifs by using a principle of maximum entropy. Then, we show how mining for the edge-centered motifs in a network can be used to probe for spatial correlations in the underlying dynamics that have produced that network. We deduce an approximate formula for the distribution of the edge-centered motifs in empirical networks of social face-to-face interactions. In the last section of this paper, we explore how the statistics of the edge-centered motifs can be used to infer the complete topology of the network they were sampled from. This leads to the needs of mathematical development, that we inaugurate here under the name of graph tiling theory.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arab Spring's Impact on Science through the Lens of Scholarly Attention, Funding, and Migration</title>
<link>https://arxiv.org/abs/2503.13238</link>
<guid>https://arxiv.org/abs/2503.13238</guid>
<content:encoded><![CDATA[
<div> Keywords: Arab Spring, scholarly attention, academic discourse, funding, migration networks 

Summary: 
The study examines the impact of the Arab Spring on scholarly attention in 10 countries in the Middle East and North Africa region. Using a difference-in-difference statistical framework on over 25 million articles published from 2002 to 2019, the researchers found that most target countries experienced a significant increase in scholarly attention post-Arab Spring compared to other regions, with Egypt garnering the most attention. The study also delves into the role of funding and migration networks in shaping scholarly attention, highlighting Saudi Arabia as a key player in attracting researchers and funding projects in the region. This research sheds light on the evolving academic landscape in the aftermath of the Arab Spring and underscores the importance of analyzing the influence of socio-political movements on scholarly discourse in the region. 

<br /><br />Summary: <div>
arXiv:2503.13238v2 Announce Type: replace-cross 
Abstract: The Arab Spring is a major socio-political movement that reshaped democratic aspirations in the Middle East and North Africa, attracting global attention through news, social media, and academic discourse. However, its consequences on the academic landscape in the region are still unclear. Here, we conduct the first study of scholarly attention toward 10 target countries affected by the Arab Spring by analyzing more than 25 million articles published from 2002 to 2019. Using a difference-in-difference statistical framework, we find that most target countries have experienced a significant increase in scholarly attention post-Arab Spring compared to the rest of the world, with Egypt attracting the most attention. We investigate how funding and migration networks relate to scholarly attention and reveal that Saudi Arabia has emerged as a key player among Western nations by attracting researchers and funding projects that shape research on the region.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflection on Code Contributor Demographics and Collaboration Patterns in the Rust Community</title>
<link>https://arxiv.org/abs/2503.22066</link>
<guid>https://arxiv.org/abs/2503.22066</guid>
<content:encoded><![CDATA[
<div> GitHub, pull request, Rust, diversity, social network analysis
Summary:
The study examines the demographic composition and interaction patterns of contributors in the Rust programming language ecosystem, focusing on key projects like Rust, Rust Analyzer, and Cargo. Using GitHub pull request data and social network analysis, disparities in gender and geographic representation among pivotal contributors are revealed, highlighting the need for more inclusive practices. The results suggest that while the Rust community is globally active, the contributor base lacks diversity, indicating a disconnect with the wider user community. To encourage broader participation and ensure alignment with the diverse global community, inclusive measures are necessary within the Rust ecosystem. <div>
arXiv:2503.22066v3 Announce Type: replace-cross 
Abstract: Open-source software communities thrive on global collaboration and contributions from diverse participants. This study explores the Rust programming language ecosystem to understand its contributors' demographic composition and interaction patterns. Our objective is to investigate the phenomenon of participation inequality in key Rust projects and the presence of diversity among them. We studied GitHub pull request data from the year leading up to the release of the latest completed Rust community annual survey in 2023. Specifically, we extracted information from three leading repositories: Rust, Rust Analyzer, and Cargo, and used social network graphs to visualize the interactions and identify central contributors and sub-communities. Social network analysis has shown concerning disparities in gender and geographic representation among contributors who play pivotal roles in collaboration networks and the presence of varying diversity levels in the sub-communities formed. These results suggest that while the Rust community is globally active, the contributor base does not fully reflect the diversity of the wider user community. We conclude that there is a need for more inclusive practices to encourage broader participation and ensure that the contributor base aligns more closely with the diverse global community that utilizes Rust.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learning on Large Graphs using a Densifying Regularity Lemma</title>
<link>https://arxiv.org/abs/2504.18273</link>
<guid>https://arxiv.org/abs/2504.18273</guid>
<content:encoded><![CDATA[
<div> Keywords: large graphs, Intersecting Block Graph (IBG), weak regularity lemma, graph neural network, node classification<br />
Summary: 
Large graphs pose challenges for traditional Message Passing Neural Networks due to computational and memory costs scaling linearly with the number of edges. The Intersecting Block Graph (IBG) is introduced as a low-rank factorization of large directed graphs, based on intersecting bipartite components. By weighting non-edges less, any graph can be efficiently approximated by a dense IBG. A constructive version of the weak regularity lemma is proven, showing that any graph can be approximated by a dense IBG with rank depending only on the chosen accuracy. A graph neural network architecture operating on the IBG representation shows competitive performance on node classification, spatio-temporal graph analysis, and knowledge graph completion with linear memory and computational complexity in the number of nodes. This approach offers a more efficient solution for learning on large graphs compared to traditional methods. <br /><br /> <div>
arXiv:2504.18273v1 Announce Type: new 
Abstract: Learning on large graphs presents significant challenges, with traditional Message Passing Neural Networks suffering from computational and memory costs scaling linearly with the number of edges. We introduce the Intersecting Block Graph (IBG), a low-rank factorization of large directed graphs based on combinations of intersecting bipartite components, each consisting of a pair of communities, for source and target nodes. By giving less weight to non-edges, we show how to efficiently approximate any graph, sparse or dense, by a dense IBG. Specifically, we prove a constructive version of the weak regularity lemma, showing that for any chosen accuracy, every graph, regardless of its size or sparsity, can be approximated by a dense IBG whose rank depends only on the accuracy. This dependence of the rank solely on the accuracy, and not on the sparsity level, is in contrast to previous forms of the weak regularity lemma. We present a graph neural network architecture operating on the IBG representation of the graph and demonstrating competitive performance on node classification, spatio-temporal graph analysis, and knowledge graph completion, while having memory and computational complexity linear in the number of nodes rather than edges.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Algorithmic Audits of TikTok: Poor Reproducibility and Short-term Validity of Findings</title>
<link>https://arxiv.org/abs/2504.18140</link>
<guid>https://arxiv.org/abs/2504.18140</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, algorithmic audits, reproducibility, generalizability, TikTok

Summary:
In this study, the researchers examine the reproducibility of previous sockpuppeting audits on TikTok's recommender systems and the generalizability of their findings. They encounter challenges due to changes in the platform and content evolution, as well as limitations in the previous research methods. The experiments reveal that one-shot audit findings may only hold in the short term, emphasizing the importance of reproducible audits to track changes over time. The audit reproducibility relies heavily on methodological choices and the state of algorithms and content on the platform. This highlights the need for systematic social media algorithmic audits to ensure users are not confined to filter bubbles and not exposed to problematic content. <div>
arXiv:2504.18140v1 Announce Type: cross 
Abstract: Social media platforms are constantly shifting towards algorithmically curated content based on implicit or explicit user feedback. Regulators, as well as researchers, are calling for systematic social media algorithmic audits as this shift leads to enclosing users in filter bubbles and leading them to more problematic content. An important aspect of such audits is the reproducibility and generalisability of their findings, as it allows to draw verifiable conclusions and audit potential changes in algorithms over time. In this work, we study the reproducibility of the existing sockpuppeting audits of TikTok recommender systems, and the generalizability of their findings. In our efforts to reproduce the previous works, we find multiple challenges stemming from social media platform changes and content evolution, but also the research works themselves. These drawbacks limit the audit reproducibility and require an extensive effort altogether with inevitable adjustments to the auditing methodology. Our experiments also reveal that these one-shot audit findings often hold only in the short term, implying that the reproducibility and generalizability of the audits heavily depend on the methodological choices and the state of algorithms and content on the platform. This highlights the importance of reproducible audits that allow us to determine how the situation changes in time.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Freshness in Dynamic Gossip Networks</title>
<link>https://arxiv.org/abs/2504.18504</link>
<guid>https://arxiv.org/abs/2504.18504</guid>
<content:encoded><![CDATA[
<div> version age of information, time-varying connections, network topology, continuous time Markov chain, information freshness

Summary:
- The article examines a source sharing updates with a network of gossiping nodes where the network's topology switches between two arbitrary topologies based on a continuous time Markov chain (CTMC).
- It assesses the impact of time-varying connections on information freshness using the version age of information metric.
- If the two networks have differing static long-term average version ages, the version age of the varying-topologies network is influenced by the transition rates in the CTMC.
- When the CTMC transition rates exceed the faster of the two static network's average version ages, the average version age of the varying-topologies network aligns with the faster average version age.
- The behavior of a small fraction of nodes can significantly affect the network's long-term average version age negatively, leading to the definition of a typical set of nodes.
- The study also evaluates the impact of fast and slow CTMC transition rates on this typical set of nodes.<br /><br />Summary: <div>
arXiv:2504.18504v1 Announce Type: cross 
Abstract: We consider a source that shares updates with a network of $n$ gossiping nodes. The network's topology switches between two arbitrary topologies, with switching governed by a two-state continuous time Markov chain (CTMC) process. Information freshness is well-understood for static networks. This work evaluates the impact of time-varying connections on information freshness. In order to quantify the freshness of information, we use the version age of information metric. If the two networks have static long-term average version ages of $f_1(n)$ and $f_2(n)$ with $f_1(n) \ll f_2(n)$, then the version age of the varying-topologies network is related to $f_1(n)$, $f_2(n)$, and the transition rates in the CTMC. If the transition rates in the CTMC are faster than $f_1(n)$, the average version age of the varying-topologies network is $f_1(n)$. Further, we observe that the behavior of a vanishingly small fraction of nodes can severely impact the long-term average version age of a network in a negative way. This motivates the definition of a typical set of nodes in the network. We evaluate the impact of fast and slow CTMC transition rates on the typical set of nodes.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing The Secret Power: How Algorithms Can Influence Content Visibility on Social Media</title>
<link>https://arxiv.org/abs/2410.17390</link>
<guid>https://arxiv.org/abs/2410.17390</guid>
<content:encoded><![CDATA[
<div> transparency, social networks, recommendation algorithms, shadow banning, public discourse <br />
<br />
Summary: This paper explores the impact of visibility alterations on Twitter discussions regarding the Ukraine-Russia conflict and the 2024 US Presidential Elections. The study, based on two large datasets, reveals that tweets containing external links are systematically penalized in terms of visibility, regardless of their ideological stance or reliability. The algorithm also appears to favor or penalize content based on the specific accounts producing it, as evident in the comparison between tweets from different sources or political figures. The findings underscore the critical need for transparency in content moderation and recommendation systems to safeguard public discourse integrity and provide fair access to online platforms. <div>
arXiv:2410.17390v2 Announce Type: replace 
Abstract: In recent years, the opaque design and the limited public understanding of social networks' recommendation algorithms have raised concerns about potential manipulation of information exposure. While reducing content visibility, aka shadow banning, may help limit harmful content, it can also be used to suppress dissenting voices. This prompts the need for greater transparency and a better understanding of this practice.
  In this paper, we investigate the presence of visibility alterations through a large-scale quantitative analysis of two Twitter/X datasets comprising over 40 million tweets from more than 9 million users, focused on discussions surrounding the Ukraine-Russia conflict and the 2024 US Presidential Elections. We use view counts to detect patterns of reduced or inflated visibility and examine how these correlate with user opinions, social roles, and narrative framings. Our analysis shows that the algorithm systematically penalizes tweets containing links to external resources, reducing their visibility by up to a factor of eight, regardless of the ideological stance or source reliability. Rather, content visibility may be penalized or favored depending on the specific accounts producing it, as observed when comparing tweets from the Kyiv Independent and RT.com or tweets by Donald Trump and Kamala Harris. Overall, our work highlights the importance of transparency in content moderation and recommendation systems in protecting the integrity of public discourse and ensuring equitable access to online platforms.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forensics and security issues in the Internet of Things</title>
<link>https://arxiv.org/abs/2309.02707</link>
<guid>https://arxiv.org/abs/2309.02707</guid>
<content:encoded><![CDATA[
<div> Keywords: IoT, forensics, security, smart home system, blockchain-based authentication mechanism

Summary: 
IoT devices face security challenges due to the lack of standardized measures, making them vulnerable to cyberattacks. Unauthorized access can lead to data compromises and control over critical infrastructure. To address these issues, a FLIP-based smart home system can be developed with security-conscious design. Implementing a blockchain-based authentication mechanism with a multi-chain structure can enhance security between trust domains. Deep learning can aid in creating a network forensics framework for effective detection and tracking of cyberattacks. Limiting data usage in big data applications for IoT-based systems is crucial. Researchers are encouraged to explore solutions to these challenges to advance the IoT field.<br /><br />Summary: <div>
arXiv:2309.02707v2 Announce Type: replace-cross 
Abstract: Given the exponential expansion of the internet, the possibilities of security attacks and cybercrimes have increased accordingly. However, poorly implemented security mechanisms in the Internet of Things (IoT) devices make them susceptible to cyberattacks, which can directly affect users. IoT forensics is thus needed to investigate and mitigate such attacks. While many works have examined IoT applications and challenges, only a few have focused on both the forensic and security issues in IoT. Therefore, this paper reviews forensic and security issues associated with IoT in different fields. Prospects and challenges in IoT research and development are also highlighted. As the literature demonstrates, most IoT devices are vulnerable to attacks due to a lack of standardized security measures. Unauthorized users could get access, compromise data, and even benefit from control of critical infrastructure. To fulfill the security-conscious needs of consumers, IoT can be used to develop a smart home system by designing the security-conscious needs of consumers; IoT can be used to create a smart home system by designing an IoT can be used to develop a smart home system by designing a FLIP-based system that is highly scalable and adaptable. A blockchain-based authentication mechanism with a multi-chain structure can provide additional security protection between different trust domains. Deep learning can be utilized to develop a network forensics framework with a high-performing system for detecting and tracking cyberattack incidents. Moreover, researchers should consider limiting the amount of data created and delivered when using big data to develop IoT-based smart systems. The findings of this review will stimulate academics to seek potential solutions for the identified issues, thereby advancing the IoT field.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-driven embedding of networks in hyperbolic space</title>
<link>https://arxiv.org/abs/2406.10711</link>
<guid>https://arxiv.org/abs/2406.10711</guid>
<content:encoded><![CDATA[
<div> Bayesian hyperbolic random graph model, MCMC algorithm, uncertainty quantification, embeddings, network properties <br />
<br />
Summary: BIGUE is a novel MCMC algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model, allowing for uncertainty quantification in inferred network coordinates. This algorithm can provide credible intervals for coordinates and network properties, offering a more comprehensive analysis than current algorithms. It highlights that some networks may have multiple plausible embeddings, a factor that traditional optimization methods might overlook. BIGUE's exploration of the posterior distribution aligns with existing algorithms while enhancing the understanding of hyperbolic network models. <div>
arXiv:2406.10711v2 Announce Type: replace-cross 
Abstract: Hyperbolic models are known to produce networks with properties observed empirically in most network datasets, including heavy-tailed degree distribution, high clustering, and hierarchical structures. As a result, several embeddings algorithms have been proposed to invert these models and assign hyperbolic coordinates to network data. Current algorithms for finding these coordinates, however, do not quantify uncertainty in the inferred coordinates. We present BIGUE, a Markov chain Monte Carlo (MCMC) algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model. We show that the samples are consistent with current algorithms while providing added credible intervals for the coordinates and all network properties. We also show that some networks admit two or more plausible embeddings, a feature that an optimization algorithm can easily overlook.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2Vec: Self-Supervised Geospatial Embeddings</title>
<link>https://arxiv.org/abs/2504.16942</link>
<guid>https://arxiv.org/abs/2504.16942</guid>
<content:encoded><![CDATA[
<div> S2Vec, geospatial embeddings, S2 Geometry library, self-supervised framework, rasterization, masked autoencoding

Summary: 
S2Vec is introduced as a self-supervised framework for creating geospatial embeddings using the S2 Geometry library to partition areas into cells, rasterizing feature vectors within cells as images, and applying masked autoencoding for encoding. These embeddings capture local feature characteristics and spatial relationships for various geospatial tasks. The framework is evaluated on socioeconomic prediction tasks, demonstrating competitive performance against image-based embeddings. Combining S2Vec with image-based embeddings through multimodal fusion shows improved performance. The results showcase S2Vec's ability to learn effective general-purpose geospatial representations and complement other data modalities in geospatial artificial intelligence.<br /><br />Summary: <div>
arXiv:2504.16942v1 Announce Type: new 
Abstract: Scalable general-purpose representations of the built environment are crucial for geospatial artificial intelligence applications. This paper introduces S2Vec, a novel self-supervised framework for learning such geospatial embeddings. S2Vec uses the S2 Geometry library to partition large areas into discrete S2 cells, rasterizes built environment feature vectors within cells as images, and applies masked autoencoding on these rasterized images to encode the feature vectors. This approach yields task-agnostic embeddings that capture local feature characteristics and broader spatial relationships. We evaluate S2Vec on three large-scale socioeconomic prediction tasks, showing its competitive performance against state-of-the-art image-based embeddings. We also explore the benefits of combining S2Vec embeddings with image-based embeddings downstream, showing that such multimodal fusion can often improve performance. Our results highlight how S2Vec can learn effective general-purpose geospatial representations and how it can complement other data modalities in geospatial artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Burning some myths on privacy properties of social networks against active attacks</title>
<link>https://arxiv.org/abs/2504.16944</link>
<guid>https://arxiv.org/abs/2504.16944</guid>
<content:encoded><![CDATA[
<div> Keywords: social networks, privacy, active attacks, (k,l)-anonymity, graph theory <br />
Summary: <br />
The research focuses on challenging the common belief that social networks have no privacy. It discusses active attacks on social network privacy and the (k,l)-anonymity measure to quantify privacy against such attacks. Utilizing the concept of k-metric antidimensional graphs, the study highlights that social networks may not be as insecure as believed. Computational experiments on random and real networks reveal that only a small number of graphs are vulnerable to privacy breaches. The research also explores mathematical properties of k-metric antidimensional graphs and proposes operations to obscure privacy vulnerabilities in graphs. This work contributes to reevaluating the privacy risks in social networks and offers insights into enhancing privacy protection against active attacks. <br /> <div>
arXiv:2504.16944v1 Announce Type: new 
Abstract: This work focuses on showing some arguments addressed to dismantle the extended idea about that social networks completely lacks of privacy properties. We consider the so-called active attacks to the privacy of social networks and the counterpart $(k,\ell)$-anonymity measure, which is used to quantify the privacy satisfied by a social network against active attacks. To this end, we make use of the graph theoretical concept of $k$-metric antidimensional graphs for which the case $k=1$ represents those graphs achieving the worst scenario in privacy whilst considering the $(k,\ell)$-anonymity measure.
  As a product of our investigation, we present a large number of computational results stating that social networks might not be as insecure as one often thinks. In particular, we develop a large number of experiments on random graphs which show that the number of $1$-metric antidimensional graphs is indeed ridiculously small with respect to the total number of graphs that can be considered. Moreover, we search on several real networks in order to check if they are $1$-metric antidimensional, and obtain that none of them are such. Along the way, we show some theoretical studies on the mathematical properties of the $k$-metric antidimensional graphs for any suitable $k\ge 1$. In addition, we also describe some operations on graphs that are $1$-metric antidimensional so that they get embedded into another larger graphs that are not such, in order to obscure their privacy properties against active attacks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation</title>
<link>https://arxiv.org/abs/2504.16946</link>
<guid>https://arxiv.org/abs/2504.16946</guid>
<content:encoded><![CDATA[
<div> simulation framework, urban mobility, behavioral choices, transportation modes, population groups
<br />
The article introduces a new simulation framework for generative agents to simulate realistic urban behaviors. It addresses the limitations of existing methods by incorporating multiple functional buildings and transportation modes in a virtual city. Extensive surveys were conducted to model behavioral choices and mobility preferences among population groups, resulting in a scalable framework capable of simulating over 4,000 agents. The framework captures the complexity of urban mobility while enabling in-depth analyses of crowd density prediction and vehicle preferences across agent demographics. The realism of the generated behaviors was assessed through micro and macro-level analyses, showcasing the framework's ability to simulate large-scale urban scenarios with accuracy and efficiency.
<br /><br />Summary: <div>
arXiv:2504.16946v1 Announce Type: new 
Abstract: Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices in modern cities, and require prohibitive computational resources for large-scale population simulation. To address these limitations, we first present a virtual city that features multiple functional buildings and transportation modes. Then, we conduct extensive surveys to model behavioral choices and mobility preferences among population groups. Building on these insights, we introduce a simulation framework that captures the complexity of urban mobility while remaining scalable, enabling the simulation of over 4,000 agents. To assess the realism of the generated behaviors, we perform a series of micro and macro-level analyses. Beyond mere performance comparison, we explore insightful experiments, such as predicting crowd density from movement patterns and identifying trends in vehicle preferences across agent demographics.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments</title>
<link>https://arxiv.org/abs/2504.16947</link>
<guid>https://arxiv.org/abs/2504.16947</guid>
<content:encoded><![CDATA[
<div> Framework, prediction, social computing, community responses, sentiment<br />
<br />
Summary: <br />
SCRAG is a prediction framework inspired by social computing, designed to forecast community responses to social media posts. It integrates large language models with a Retrieval-Augmented Generation technique to overcome challenges like reliance on static training data and hallucinations. By retrieving historical responses and external knowledge, SCRAG can accurately predict how a community will respond to new narratives. Extensive experiments on the X platform show over 10% improvements in key metrics. The framework is effective in capturing diverse ideologies and nuances, providing concrete insights into community responses for applications like public sentiment prediction and crisis management. <div>
arXiv:2504.16947v1 Announce Type: new 
Abstract: This paper introduces SCRAG, a prediction framework inspired by social computing, designed to forecast community responses to real or hypothetical social media posts. SCRAG can be used by public relations specialists (e.g., to craft messaging in ways that avoid unintended misinterpretations) or public figures and influencers (e.g., to anticipate social responses), among other applications related to public sentiment prediction, crisis management, and social what-if analysis. While large language models (LLMs) have achieved remarkable success in generating coherent and contextually rich text, their reliance on static training data and susceptibility to hallucinations limit their effectiveness at response forecasting in dynamic social media environments. SCRAG overcomes these challenges by integrating LLMs with a Retrieval-Augmented Generation (RAG) technique rooted in social computing. Specifically, our framework retrieves (i) historical responses from the target community to capture their ideological, semantic, and emotional makeup, and (ii) external knowledge from sources such as news articles to inject time-sensitive context. This information is then jointly used to forecast the responses of the target community to new posts or narratives. Extensive experiments across six scenarios on the X platform (formerly Twitter), tested with various embedding models and LLMs, demonstrate over 10% improvements on average in key evaluation metrics. A concrete example further shows its effectiveness in capturing diverse ideologies and nuances. Our work provides a social computing tool for applications where accurate and concrete insights into community responses are crucial.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Sampling: An Overview and Comparative Analysis</title>
<link>https://arxiv.org/abs/2504.17701</link>
<guid>https://arxiv.org/abs/2504.17701</guid>
<content:encoded><![CDATA[
<div> node-based, edge-based, exploration-based, network sampling, temporal networks

Summary:
- Network sampling is essential for analyzing large or partially observable networks.
- Different sampling methods vary in effectiveness depending on the context.
- Empirical comparison of node-based, edge-based, and exploration-based sampling methods.
- Advanced methods show better accuracy on static networks, while simpler techniques are more effective for temporal networks.
- The choice of sampling strategy should consider both network structure and analytical objectives. 

<br /><br />Summary: <div>
arXiv:2504.17701v1 Announce Type: new 
Abstract: Network sampling is a crucial technique for analyzing large or partially observable networks. However, the effectiveness of different sampling methods can vary significantly depending on the context. In this study, we empirically compare representative methods from three main categories: node-based, edge-based, and exploration-based sampling. We used two real-world datasets for our analysis: a scientific collaboration network and a temporal message-sending network. Our results indicate that no single sampling method consistently outperforms the others in both datasets. Although advanced methods tend to provide better accuracy on static networks, they often perform poorly on temporal networks, where simpler techniques can be more effective. These findings suggest that the best sampling strategy depends not only on the structural characteristics of the network but also on the specific metrics that need to be preserved or analyzed. Our work offers practical insights for researchers in choosing sampling approaches that are tailored to different types of networks and analytical objectives.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Percolation as Decision Threshold for Risk Management in Cross-Country Thermal Soaring</title>
<link>https://arxiv.org/abs/2504.16945</link>
<guid>https://arxiv.org/abs/2504.16945</guid>
<content:encoded><![CDATA[
<div> Keywords: soaring, updrafts, graph percolation, risk management, flight logs

Summary:
Graph percolation theory is used to analyze the risk of failure to find updrafts for long-range flight by fixed-wing aircraft without propulsion systems. By evaluating flight logs from human soaring pilots, it is found that pilots rarely fly in conditions that do not satisfy graph percolation, indicating a desired minimum node degree. Pilots accept reduced climb rates to maintain percolation, showcasing the importance of this risk measure in optimizing speed and managing risk in soaring flight. The uncertainty of updraft locations underscores the necessity of determining when to exploit updrafts for continued flight, crucial for successful long-distance soaring. The study highlights the practical utility of graph percolation as a tool for evaluating and improving in-flight decision-making in soaring aircraft, aiding pilots in maximizing their efficiency and safety during flight.<br /><br />Summary: <div>
arXiv:2504.16945v1 Announce Type: cross 
Abstract: Long range flight by fixed-wing aircraft without propulsion systems can be accomplished by "soaring" -- exploiting randomly located updrafts to gain altitude which is expended in gliding flight. As the location of updrafts is uncertain and cannot be determined except through in situ observation, aircraft exploiting this energy source are at risk of failing to find a subsequent updraft. Determining when an updraft must be exploited to continue flight is essential to managing risk and optimizing speed. Graph percolation offers a theoretical explanation for this risk, and a framework for evaluating it using information available to the operator of a soaring aircraft in flight. The utility of graph percolation as a risk measure is examined by analyzing flight logs from human soaring pilots. This analysis indicates that in sport soaring pilots rarely operate in a condition which does not satisfy graph percolation, identifies an apparent desired minimum node degree, and shows that pilots accept reduced climb rates in order to maintain percolation.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social sustainability through engagement in a training context with tools such as the Native Podcast and Facebook social network</title>
<link>https://arxiv.org/abs/2504.16964</link>
<guid>https://arxiv.org/abs/2504.16964</guid>
<content:encoded><![CDATA[
<div> Keywords: sustainability, social dimension, EUTIC 2023 symposium, engagement process, digital tools

Summary:
The article addresses the previously neglected social dimension of sustainability within literature, with a particular focus on the upcoming EUTIC 2023 symposium. It introduces an engagement process designed to promote sustainable development using digital tools that are inspired by everyday life and cater to lifelong learning in training contexts. Rooted in information and communication sciences, the work advocates for a multi-disciplinary approach that can be applied across various disciplines. The authors aim to challenge current perspectives and generate insights on the intersection of digital tools, sustainability, and lifelong learning. This innovative approach holds promise for addressing sustainability concerns and promoting social well-being through interactive and accessible digital solutions. <div>
arXiv:2504.16964v1 Announce Type: cross 
Abstract: The social dimension of sustainability seems to have been a notion rarely addressed in the literature (Dubois et al., 2001) until the early 2000s. The EUTIC 2023 symposium provides an opportunity to take up this topical issue. To this end, we are presenting an engagement process that is part of a sustainable development dynamic, based on digital tools inspired by everyday life, for applications in the context of training, with a view to lifelong learning. Our work, which stems from the information and communication sciences, is rooted in a multi-disciplinary approach that we believe can be echoed in a variety of disciplines, but which it is interesting to challenge, hence the purpose of this contribution.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2504.17099</link>
<guid>https://arxiv.org/abs/2504.17099</guid>
<content:encoded><![CDATA[
<div> geometric information, entity representations, knowledge graphs, RDF2Vec, location-aware embeddings
Summary:
- The paper introduces a new approach that incorporates geometric information to learn location-aware entity embeddings.
- Existing methods for learning entity representations do not consider geometries stored in knowledge graphs.
- The approach expands nodes by flooding the graph from geographic nodes to ensure all reachable nodes are considered.
- A modified version of RDF2Vec, biased with spatial weights from the flooded graph, is then applied to learn location-aware embeddings.
- Evaluations on benchmark datasets show that the proposed approach outperforms non-location-aware RDF2Vec and GeoTransE.<br /><br />Summary: <div>
arXiv:2504.17099v1 Announce Type: cross 
Abstract: Many knowledge graphs contain a substantial number of spatial entities, such as cities, buildings, and natural landmarks. For many of these entities, exact geometries are stored within the knowledge graphs. However, most existing approaches for learning entity representations do not take these geometries into account. In this paper, we introduce a variant of RDF2Vec that incorporates geometric information to learn location-aware embeddings of entities. Our approach expands different nodes by flooding the graph from geographic nodes, ensuring that each reachable node is considered. Based on the resulting flooded graph, we apply a modified version of RDF2Vec that biases graph walks using spatial weights. Through evaluations on multiple benchmark datasets, we demonstrate that our approach outperforms both non-location-aware RDF2Vec and GeoTransE.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Dynamic Time Warping for Pandemic Surveillance: Understanding the Relationship between Google Trends Network Metrics and COVID-19 Incidences</title>
<link>https://arxiv.org/abs/2504.17146</link>
<guid>https://arxiv.org/abs/2504.17146</guid>
<content:encoded><![CDATA[
<div> Keywords: network statistics, Google Trends data, COVID-19 disease progression, dynamic time warping, infodemiology

Summary: 
The study utilized network statistics derived from Google Trends data to predict COVID-19 progression in Metro Manila, Philippines. By applying dynamic time warping (DTW), the temporal alignment between network metrics and COVID-19 case trajectories was measured. The study explored various parameters and found that network density and data preprocessing methods significantly influenced alignment quality. The optimal configuration, using network density with Rescaling Daily Data transformation, achieved substantial temporal alignment with COVID-19 confirmed cases data. The findings suggest that online search behavior can be a useful indicator for epidemic surveillance in urban areas like Metro Manila, leveraging the Philippines' high online usage during the pandemic. This approach offers a valuable tool for early disease spread detection and complements public health monitoring in resource-limited settings. 

<br /><br />Summary: <div>
arXiv:2504.17146v1 Announce Type: cross 
Abstract: The premise of network statistics derived from Google Trends data to foresee COVID-19 disease progression is gaining momentum in infodemiology. This approach was applied in Metro Manila, National Capital Region, Philippines. Through dynamic time warping (DTW), the temporal alignment was quantified between network metrics and COVID-19 case trajectories, and systematically explored 320 parameter configurations including two network metrics (network density and clustering coefficient), two data preprocessing methods (Rescaling Daily Data and MSV), multiple thresholds, two correlation window sizes, and Sakoe-Chiba band constraints. Results from the Kruskal-Wallis tests revealed that five of the six parameters significantly influenced alignment quality, with the disease comparison type (active cases vs. confirmed cases) demonstrating the strongest effect. The optimal configuration, which is using the network density statistic with a Rescaling Daily Data transformation, a threshold of 0.8, a 15-day window, and a 50-day radius constraint, achieved a DTW score of 36.30. This indicated substantial temporal alignment with the COVID-19 confirmed cases data. The discoveries demonstrate that network metrics rooted from online search behavior can serve as complementary indicators for epidemic surveillance in urban locations like Metro Manila. This strategy leverages the Philippines' extensive online usage during the pandemic to provide potentially valuable early signals of disease spread, and offers a supplementary tool for public health monitoring in resource-limited situations.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online posting effects: Unveiling the non-linear journeys of users in depression communities on Reddit</title>
<link>https://arxiv.org/abs/2311.17684</link>
<guid>https://arxiv.org/abs/2311.17684</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, depression, online self-disclosure, mental health discourse, Reddit

Summary:
Through an analysis of user interactions on Reddit boards focused on depression, this study investigates the consequences of online self-disclosure on mental health discourse. Utilizing a data-informed framework, the research identifies 4 distinct clusters representing different psychological states among users. The findings reveal that online exposure to peers' emotional and semantic content can lead to transitions in users' psychological states. These transitions do not follow a linear progression but rather a spiral journey through positive and negative phases. The study suggests that the type and layout of online social interactions play a significant role in users' experiences and outcomes when discussing depression. The insights gained from this research can contribute to understanding the impact of online social platforms as self-help forums for individuals seeking support for mental health issues. 

<br /><br />Summary: Through an analysis of user interactions on Reddit boards focused on depression, this study identified 4 distinct clusters representing different psychological states. Online exposure to peers' content can lead to transitions in users' psychological states in a spiral journey. The study suggests that the type and layout of online social interactions play a significant role in users' experiences and outcomes when discussing depression. <div>
arXiv:2311.17684v3 Announce Type: replace 
Abstract: Social media platforms have become pivotal as self-help forums, enabling individuals to share personal experiences and seek support. However, on topics as sensitive as depression, what are the consequences of online self-disclosure? Here, we delve into the dynamics of mental health discourse on various Reddit boards focused on depression. To this aim, we introduce a data-informed framework reconstructing online dynamics from 303k users interacting over two years. Through user-generated content, we identify 4 distinct clusters representing different psychological states. Our analysis unveils online posting effects: a user can transition to another psychological state after online exposure to peers' emotional/semantic content. As described by conditional Markov chains and different levels of social exposure, users' transitions reveal navigation through both positive and negative phases in a spiral rather than a linear progression. Interpreted in light of psychological literature, related particularly to the Patient Health Engagement (PHE) model, our findings can provide evidence that the type and layout of online social interactions have an impact on users' "journeys" when posting about depression.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exposing Cross-Platform Coordinated Inauthentic Activity in the Run-Up to the 2024 U.S. Election</title>
<link>https://arxiv.org/abs/2410.22716</link>
<guid>https://arxiv.org/abs/2410.22716</guid>
<content:encoded><![CDATA[
<div> Russian interference, coordinated information operations, social media, cross-platform, influence campaigns <br />
<br />
Summary: <br />
This study examines coordinated information operations on social media platforms, focusing on interactions across Twitter (now $\mathbb{X$), Facebook, and Telegram related to the 2024 U.S. Election. By analyzing similarity networks, the research identifies coordinated communities engaging in suspicious sharing behaviors within and between platforms. The study uncovers potential foreign interference, with Russian-affiliated media systematically promoted on Telegram and $\mathbb{X$. In addition, the analysis reveals significant intra- and cross-platform coordinated inauthentic activity driving the dissemination of highly partisan, low-credibility, and conspiratorial content. The findings emphasize the need for regulatory measures that go beyond individual platforms to effectively combat the challenges posed by cross-platform coordinated influence campaigns. <div>
arXiv:2410.22716v4 Announce Type: replace 
Abstract: Coordinated information operations remain a persistent challenge on social media, despite platform efforts to curb them. While previous research has primarily focused on identifying these operations within individual platforms, this study shows that coordination frequently transcends platform boundaries. Leveraging newly collected data of online conversations related to the 2024 U.S. Election across $\mathbb{X}$ (formerly, Twitter), Facebook, and Telegram, we construct similarity networks to detect coordinated communities exhibiting suspicious sharing behaviors within and across platforms. Proposing an advanced coordination detection model, we reveal evidence of potential foreign interference, with Russian-affiliated media being systematically promoted across Telegram and $\mathbb{X}$. Our analysis also uncovers substantial intra- and cross-platform coordinated inauthentic activity, driving the spread of highly partisan, low-credibility, and conspiratorial content. These findings highlight the urgent need for regulatory measures that extend beyond individual platforms to effectively address the growing challenge of cross-platform coordinated influence campaigns.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</title>
<link>https://arxiv.org/abs/2410.05401</link>
<guid>https://arxiv.org/abs/2410.05401</guid>
<content:encoded><![CDATA[
<div> demographic targeting, fairness, large language models, climate change communication, social media

Summary:
Large language models (LLMs) were used to analyze microtargeting practices in climate change communication on social media, focusing on demographic targeting and fairness. The study achieved an 88.55% accuracy in predicting demographic targets and revealed distinct messaging strategies for different audiences. Young adults were targeted with activism and environmental themes, while women were engaged through caregiving and advocacy topics. LLMs provided transparent explanations for their classifications, highlighting the thematic elements used. A fairness analysis uncovered biases in predicting senior citizens and male audiences, underscoring the need for accountability and inclusivity in social media-driven climate campaigns. This study's framework demonstrates the effectiveness of LLMs in dissecting targeted communication strategies and emphasizes the importance of addressing biases for a more transparent and inclusive climate communication approach.

<br /><br />Summary: <div>
arXiv:2410.05401v2 Announce Type: replace-cross 
Abstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating DAO Sustainability and Longevity Through On-Chain Governance Metrics</title>
<link>https://arxiv.org/abs/2504.11341</link>
<guid>https://arxiv.org/abs/2504.11341</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralised Autonomous Organisations, governance efficiency, financial robustness, decentralisation, community engagement

Summary: 
Decentralised Autonomous Organisations (DAOs) automate governance and resource allocation through smart contracts, aiming to shift decision-making to distributed token holders. This paper addresses sustainability challenges faced by DAOs, such as limited user participation and concentrated voting power. By introducing a framework of Key Performance Indicators (KPIs) focusing on governance efficiency, financial robustness, decentralisation, and community engagement, the study identifies recurring governance patterns in real-world DAOs. Analysis of a custom dataset using non-parametric methods highlights issues like low participation rates and high proposer concentration. The proposed KPIs provide a data-driven method for assessing and improving DAO governance structures, supporting a comprehensive evaluation of decentralised systems. This research offers practical tools for enhancing the resilience and effectiveness of DAO-based governance models.

Summary: <div>
arXiv:2504.11341v2 Announce Type: replace-cross 
Abstract: Decentralised Autonomous Organisations (DAOs) automate governance and resource allocation through smart contracts, aiming to shift decision-making to distributed token holders. However, many DAOs face sustainability challenges linked to limited user participation, concentrated voting power, and technical design constraints. This paper addresses these issues by identifying research gaps in DAO evaluation and introducing a framework of Key Performance Indicators (KPIs) that capture governance efficiency, financial robustness, decentralisation, and community engagement. We apply the framework to a custom-built dataset of real-world DAOs constructed from on-chain data and analysed using non-parametric methods. The results reveal recurring governance patterns, including low participation rates and high proposer concentration, which may undermine long-term viability. The proposed KPIs offer a replicable, data-driven method for assessing DAO governance structures and identifying potential areas for improvement. These findings support a multidimensional approach to evaluating decentralised systems and provide practical tools for researchers and practitioners working to improve the resilience and effectiveness of DAO-based governance models.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schelling segregation dynamics in densely-connected social network graphs</title>
<link>https://arxiv.org/abs/2504.16307</link>
<guid>https://arxiv.org/abs/2504.16307</guid>
<content:encoded><![CDATA[
<div> network model, segregation, polarization, agent-based, social<br />
Summary: <br />
- Schelling segregation model used to study segregation dynamics in dense social networks.
- Dense networks show less segregation compared to sparse networks.
- Agents do not end up more segregated than they desire to be.
- Polarization is difficult in networks with one smaller group and unstable with an extremely small group.
- Mixed evidence for the "paradox of weak minority preferences" in a densely connected social network. <br /> <div>
arXiv:2504.16307v1 Announce Type: new 
Abstract: Schelling segregation is a well-established model used to investigate the dynamics of segregation in agent-based models. Since we consider segregation to be key for the development of political polarisation, we are interested in what insights it could give for this problem. We tested basic questions of segregation on an agent-based social network model where agents' connections were not restricted by their spatial position, and made the network graph much denser than previous tests of Schelling segregation in social networks.
  We found that a dense social network does not become as strongly segregated as a sparse network, and that agents' numbers of same-group neighbours do not greatly exceed their desired numbers (i.e. they do not end up more segregated than they desire to be). Furthermore, we found that the network was very difficult to polarise when one group was somewhat smaller than the other, and that it became unstable when one group was extremely small, both of which provide insights into real-world polarisation dynamics. Finally, we tested the question of whether an increase in the minority group's desire for same-group neighbours created more segregation than a similar increase for the majority group -- the "paradox of weak minority preferences" -- and found mixed evidence for this effect in a densely connected social network.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Social Computing Tools for Undergraduate Research Community Building</title>
<link>https://arxiv.org/abs/2504.16366</link>
<guid>https://arxiv.org/abs/2504.16366</guid>
<content:encoded><![CDATA[
<div> impostor syndrome, undergraduate students, social computing tools, research community, sense of belonging <br />
Summary: <br />
The article proposes the use of spontaneous online social networks (SOSNs) to overcome barriers faced by new members, particularly undergraduate students, in research communities. By integrating a photo sharing bot inspired by SOSNs like BeReal into a computing research lab, the study aimed to enhance sense of belonging, peripheral awareness, and feelings of togetherness. Through surveys and interviews with 17 lab members over 2 weeks, an increase in sense of togetherness was observed. The approach facilitated greater awareness of peers' personal lives, fostering a stronger sense of community and reducing feelings of disconnectedness. Overall, the integration of social computing tools in small research communities shows promise in promoting a more inclusive and supportive environment for new members. <br /> <div>
arXiv:2504.16366v1 Announce Type: new 
Abstract: Many barriers exist when new members join a research community, including impostor syndrome. These barriers can be especially challenging for undergraduate students who are new to research. In our work, we explore how the use of social computing tools in the form of spontaneous online social networks (SOSNs) can be used in small research communities to improve sense of belonging, peripheral awareness, and feelings of togetherness within an existing CS research community. Inspired by SOSNs such as BeReal, we integrated a Wizard-of-Oz photo sharing bot into a computing research lab to foster community building among members. Through a small sample of lab members (N = 17) over the course of 2 weeks, we observed an increase in participants' sense of togetherness based on pre- and post-study surveys. Our surveys and semi-structured interviews revealed that this approach has the potential to increase awareness of peers' personal lives, increase feelings of community, and reduce feelings of disconnectedness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.16323</link>
<guid>https://arxiv.org/abs/2504.16323</guid>
<content:encoded><![CDATA[
<div> Keywords: digital media, Media Content Atlas, multimodal large language models, content analysis, screen data

Summary: 
The study introduces the Media Content Atlas (MCA), a pipeline for investigating large-scale screen data using multimodal large language models (MLLMs). MCA allows for moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Tested on 1.12 million smartphone screenshots from 112 adults over a month, MCA supports open-ended exploration and hypothesis generation, as well as hypothesis-driven investigations. Expert evaluators found MCA to be usable and promising for research and intervention design, with high relevance and accuracy ratings for clustering results and descriptions. By combining methodological possibilities with specific research needs, MCA enables both inductive and deductive inquiry, offering new opportunities for media and HCI research.<br /><br />Summary: <div>
arXiv:2504.16323v1 Announce Type: cross 
Abstract: As digital media use continues to evolve and influence various aspects of life, developing flexible and scalable tools to study complex media experiences is essential. This study introduces the Media Content Atlas (MCA), a novel pipeline designed to help researchers investigate large-scale screen data beyond traditional screen-use metrics. Leveraging multimodal large language models (MLLMs), MCA enables moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Evaluated on 1.12 million smartphone screenshots continuously captured during screen use from 112 adults over an entire month, MCA facilitates open-ended exploration and hypothesis generation as well as hypothesis-driven investigations at an unprecedented scale. Expert evaluators underscored its usability and potential for research and intervention design, with clustering results rated 96% relevant and descriptions 83% accurate. By bridging methodological possibilities with domain-specific needs, MCA accelerates both inductive and deductive inquiry, presenting new opportunities for media and HCI research.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</title>
<link>https://arxiv.org/abs/2504.16604</link>
<guid>https://arxiv.org/abs/2504.16604</guid>
<content:encoded><![CDATA[
<div> Keywords: Counterspeech, Large Language Models, Conspiracy theories, GPT-4o, Llama 3

Summary:
Large Language Models (LLMs) like GPT-4o, Llama 3, and Mistral are being explored for countering conspiracy theories online through counterspeech. However, the study found that these models often generate generic or superficial responses when provided with expert-crafted counterspeech prompts. They also tend to over-acknowledge fear and frequently fabricate facts or sources, posing challenges for practical application. Unlike hate speech, there is a lack of datasets pairing conspiracy theory comments with expert counterspeech, highlighting the need for further research in this area. The study underscores the limitations of using prompt-based approaches with LLMs for countering harmful online content, indicating the importance of developing more effective strategies for addressing conspiracy theories. 

<br /><br />Summary: 
- Large Language Models like GPT-4o, Llama 3, and Mistral are explored for countering conspiracy theories through counterspeech. 
- These models often generate generic or superficial responses to expert-crafted counterspeech prompts. 
- They tend to over-acknowledge fear and frequently fabricate facts or sources, challenging their practical application. 
- Lack of datasets pairing conspiracy theory comments with expert counterspeech highlights the need for further research in this area. 
- Limitations of using prompt-based approaches with LLMs for countering harmful online content are underscored, emphasizing the need for more effective strategies. <div>
arXiv:2504.16604v1 Announce Type: cross 
Abstract: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Science (SecSci), Basic Concepts and Mathematical Foundations</title>
<link>https://arxiv.org/abs/2504.16617</link>
<guid>https://arxiv.org/abs/2504.16617</guid>
<content:encoded><![CDATA[
<div> Keywords: textbook, security courses, lecture notes, research problems, advanced courses

Summary: 
This textbook is a compilation of lecture notes from security courses taught at various universities, including Oxford, Royal Holloway, and Hawaii. The early chapters are designed for a first course in security, while the middle chapters have been used in advanced courses. Towards the end of the textbook, there are also research problems included. The material covered in the textbook spans different levels of expertise, making it suitable for a range of students from beginners to more advanced learners. The inclusion of research problems at the end of the textbook provides an opportunity for students to engage with the material in a more in-depth and practical manner. Overall, this textbook offers a comprehensive overview of security concepts and is a valuable resource for students studying the subject. 

<br /><br />Summary: <div>
arXiv:2504.16617v1 Announce Type: cross 
Abstract: This textbook compiles the lecture notes from security courses taught at Oxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii. The early chapters are suitable for a first course in security. The middle chapters have been used in advanced courses. Towards the end there are also some research problems.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Traps and Algorithmic Interventions</title>
<link>https://arxiv.org/abs/2406.04462</link>
<guid>https://arxiv.org/abs/2406.04462</guid>
<content:encoded><![CDATA[
<div> philosophical literature, pessimism traps, information cascades, economics, mathematics <br />
Summary: <br />
This paper explores the relationship between philosophical concepts of pessimism traps and formal models of information cascades in economics and mathematics. Pessimism traps describe how individuals in a community mimic sub-optimal actions of others in uncertain situations, similar to how information cascades involve agents making decisions based on private signals and public histories. The study shows that information cascades inevitably occur in many scenarios, and populations can easily fall into incorrect cascades based on the strength of signals. Once formed, cascades are difficult to break without external intervention. The paper proposes an intervention strategy to redirect populations from incorrect to correct cascades, demonstrating both theoretical and empirical analyses of its efficacy. <div>
arXiv:2406.04462v3 Announce Type: replace 
Abstract: In this paper, we relate the philosophical literature on pessimism traps to information cascades, a formal model derived from the economics and mathematics literature. A pessimism trap is a social pattern in which individuals in a community, in situations of uncertainty, begin to copy the sub-optimal actions of others, despite their individual beliefs. This maps nicely onto the concept of an information cascade, which involves a sequence of agents making a decision between two alternatives, with a private signal of the superior alternative and a public history of others' actions. Key results from the economics literature show that information cascades occur with probability one in many contexts, and depending on the strength of the signal, populations can fall into the incorrect cascade very easily and quickly. Once formed, in the absence of external perturbation, a cascade cannot be broken -- therefore, we derive an intervention that can be used to nudge a population from an incorrect to a correct cascade and, importantly, maintain the cascade once the subsidy is discontinued. We study this both theoretically and empirically.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness-Aware Dense Subgraph Discovery</title>
<link>https://arxiv.org/abs/2412.02604</link>
<guid>https://arxiv.org/abs/2412.02604</guid>
<content:encoded><![CDATA[
<div> Keywords: Dense subgraph discovery, Fairness, Graph mining, Subgraph density, Tractable formulations <br />
Summary: 
This study introduces two tractable formulations for fair Dense subgraph discovery (DSD), focusing on promoting fairness in detecting dense subgraphs that represent diverse subgroups within the vertex set. Existing methods for fair DSD have limitations in NP-hard formulations and lack flexibility in defining fairness levels. The proposed methods offer structured approaches to incorporate fairness with varying levels and introduce a measure of fairness-induced relative loss in subgraph density. Results from extensive experiments on real-world datasets demonstrate that the new methods outperform existing solutions, with significantly lower subgraph density loss in some cases, particularly excelling in scenarios with extreme subgroup imbalances. By addressing these limitations, the study contributes to a better understanding of the trade-off between density and fairness in DSD. <br /> <div>
arXiv:2412.02604v2 Announce Type: replace 
Abstract: Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition. In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density. Existing methods for promoting fairness in DSD have important limitations - the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness. In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness. Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels. We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off. We are the first to study such a notion in the context of detecting fair dense subgraphs. Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels. Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerable Connectivity Caused by Local Communities in Spatial Networks</title>
<link>https://arxiv.org/abs/2412.14513</link>
<guid>https://arxiv.org/abs/2412.14513</guid>
<content:encoded><![CDATA[
<div> Keywords: local communities, spatial networks, robustness, infrastructure, long-distance links

Summary:
This study examines the impact of local communities on the robustness of connectivity in spatial networks with a focus on planar infrastructure. The research reveals that networks with strong local communities have weaker robustness against malicious attacks. The presence of concentrated nodes connected with short links within local communities leads to vulnerabilities in the network's connectivity. However, the study also suggests that introducing long-distance links can help mitigate the negative effects of local communities on network robustness. By incorporating long-distance connections, the network's ability to withstand attacks and maintain connectivity can be improved. Overall, the findings emphasize the importance of considering the structure of local communities in spatial networks when assessing network robustness and the potential benefits of incorporating longer links to enhance resilience. 

<br /><br />Summary: <div>
arXiv:2412.14513v4 Announce Type: replace 
Abstract: Local communities by concentration of nodes connected with short links are widely observed in spatial networks. However, how such structure affects robustness of connectivity against malicious attacks remains unclear. This study investigates the impact of local communities on the robustness by modeling planar infrastructure reveals that the robustness is weakened by strong local communities in spatial networks. These results highlight the potential of long-distance links in mitigating the negative effects of local community on the robustness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNN-based Anchor Embedding for Exact Subgraph Matching</title>
<link>https://arxiv.org/abs/2502.00031</link>
<guid>https://arxiv.org/abs/2502.00031</guid>
<content:encoded><![CDATA[
<div> Keyword: subgraph matching, graph neural network, anchor embedding, exact matching, query plan
Summary:
The paper introduces a novel graph neural network (GNN)-based anchor embedding framework (GNN-AE) to address the subgraph matching problem with exact results. Unlike other approaches that provide only approximate solutions, the GNN-AE approach leverages anchor concepts and embeddings to ensure no false dismissals in subgraph matching. By transforming the problem into a search task in the embedding space, the proposed method guarantees exact matching. An efficient matching growth algorithm is developed to retrieve all exact matches in parallel, supported by a cost-model-based DFS query plan for enhanced performance. Experimental results on various datasets validate the effectiveness and efficiency of the GNN-AE approach for exact subgraph matching.<br /><br />Summary: <div>
arXiv:2502.00031v3 Announce Type: replace 
Abstract: Subgraph matching query is a classic problem in graph data management and has a variety of real-world applications, such as discovering structures in biological or chemical networks, finding communities in social network analysis, explaining neural networks, and so on. To further solve the subgraph matching problem, several recent advanced works attempt to utilize deep-learning-based techniques to handle the subgraph matching query. However, most of these works only obtain approximate results for subgraph matching without theoretical guarantees of accuracy. In this paper, we propose a novel and effective graph neural network (GNN)-based anchor embedding framework (GNN-AE), which allows exact subgraph matching. Unlike GNN-based approximate subgraph matching approaches that only produce inexact results, in this paper, we pioneer a series of concepts related to anchor (including anchor, anchor graph/path, etc.) in subgraph matching and carefully devise the anchor (graph) embedding technique based on GNN models. We transform the subgraph matching problem into a search problem in the embedding space via the anchor (graph & path) embedding techniques. With the proposed anchor matching mechanism, GNN-AE can guarantee subgraph matching has no false dismissals. We design an efficient matching growth algorithm, which can retrieve the locations of all exact matches in parallel. We also propose a cost-model-based DFS query plan to enhance the parallel matching growth algorithm. Through extensive experiments on 6 real-world and 3 synthetic datasets, we confirm the effectiveness and efficiency of our GNN-AE approach for exact subgraph matching.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic hashtag recommendation in social media with trend shift detection and adaptation</title>
<link>https://arxiv.org/abs/2504.00044</link>
<guid>https://arxiv.org/abs/2504.00044</guid>
<content:encoded><![CDATA[
<div> recommendation systems, hashtags, social media, trend shifts, Apache Storm

Summary:
Hashtag recommendation systems play a crucial role in enhancing content categorization and search on social media platforms. However, existing static models face challenges in adapting to the dynamic nature of social media conversations where new hashtags constantly emerge and existing ones evolve semantically. To address this issue, the H-ADAPTS methodology is introduced, which incorporates a trend-aware mechanism to detect shifts in hashtag usage, reflecting evolving trends in social media discussions. By leveraging the Apache Storm framework for scalable and fault-tolerant analysis of high-velocity social data, H-ADAPTS can efficiently adapt its recommendation model based on recent posts. Experimental results from real-world case studies, such as the COVID-19 pandemic and the 2020 US presidential election, demonstrate that H-ADAPTS outperforms existing solutions by providing timely and relevant hashtag recommendations that align with emerging trends in social media conversations. <br /><br />Summary: <div>
arXiv:2504.00044v2 Announce Type: replace 
Abstract: Hashtag recommendation systems have emerged as a key tool for automatically suggesting relevant hashtags and enhancing content categorization and search. However, existing static models struggle to adapt to the highly dynamic nature of social media conversations, where new hashtags constantly emerge and existing ones undergo semantic shifts. To address these challenges, this paper introduces H-ADAPTS (Hashtag recommendAtion by Detecting and adAPting to Trend Shifts), a dynamic hashtag recommendation methodology that employs a trend-aware mechanism to detect shifts in hashtag usage-reflecting evolving trends and topics within social media conversations-and triggers efficient model adaptation based on a (small) set of recent posts. Additionally, the Apache Storm framework is leveraged to support scalable and fault-tolerant analysis of high-velocity social data, enabling the timely detection of trend shifts. Experimental results from two real-world case studies, including the COVID-19 pandemic and the 2020 US presidential election, demonstrate the effectiveness of H-ADAPTS in providing timely and relevant hashtag recommendations by adapting to emerging trends, significantly outperforming existing solutions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Noise Reduction in Dense Mixed-Membership Stochastic Block Models under Diverging Spiked Eigenvalues Condition</title>
<link>https://arxiv.org/abs/2307.14530</link>
<guid>https://arxiv.org/abs/2307.14530</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, overlapping community, stochastic block model, estimation error, lower bound

Summary: 
Community detection in networks is a crucial issue with applications in various fields. This study focuses on the Mixed-Membership Stochastic Block Model (MMSB) for overlapping community detection. The goal is to reconstruct relationships between communities based on network observations. Different approaches are compared, and a new estimator is proposed that matches the minimax lower bound on estimation error. Theoretical results are established under general model conditions, and experiments are conducted to demonstrate the theory. This work contributes to advancing the understanding and techniques for identifying overlapping communities in networks.<br /><br />Summary: <div>
arXiv:2307.14530v2 Announce Type: replace-cross 
Abstract: Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics</title>
<link>https://arxiv.org/abs/2504.15927</link>
<guid>https://arxiv.org/abs/2504.15927</guid>
<content:encoded><![CDATA[
<div> community detection, semi-supervised, crystallization kinetics, CLANN, Transitive Annealer

Summary:
The paper introduces CLANN, a novel semi-supervised community detection method inspired by crystallization kinetics. Traditional methods face challenges with scalability and unreasonable starting points. CLANN integrates annealing principles to optimize the identification of community cores, mimicking a crystal subgrain expanding into a complete grain. Through a Transitive Annealer, neighboring cliques are merged, and the community core is repositioned for spontaneous growth, improving scalability. Extensive experiments across 43 network settings show CLANN outperforms existing methods on multiple real-world datasets, demonstrating its effectiveness and efficiency in community detection. <br /><br />Summary: <div>
arXiv:2504.15927v1 Announce Type: new 
Abstract: Semi-supervised community detection methods are widely used for identifying specific communities due to the label scarcity. Existing semi-supervised community detection methods typically involve two learning stages learning in both initial identification and subsequent adjustment, which often starts from an unreasonable community core candidate. Moreover, these methods encounter scalability issues because they depend on reinforcement learning and generative adversarial networks, leading to higher computational costs and restricting the selection of candidates. To address these limitations, we draw a parallel between crystallization kinetics and community detection to integrate the spontaneity of the annealing process into community detection. Specifically, we liken community detection to identifying a crystal subgrain (core) that expands into a complete grain (community) through a process similar to annealing. Based on this finding, we propose CLique ANNealing (CLANN), which applies kinetics concepts to community detection by integrating these principles into the optimization process to strengthen the consistency of the community core. Subsequently, a learning-free Transitive Annealer was employed to refine the first-stage candidates by merging neighboring cliques and repositioning the community core, enabling a spontaneous growth process that enhances scalability. Extensive experiments on \textbf{43} different network settings demonstrate that CLANN outperforms state-of-the-art methods across multiple real-world datasets, showcasing its exceptional efficacy and efficiency in community detection.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Widely Known Findings Easier to Retract?</title>
<link>https://arxiv.org/abs/2504.15504</link>
<guid>https://arxiv.org/abs/2504.15504</guid>
<content:encoded><![CDATA[
<div> Keywords: retraction, failures, science, citation, Altmetric

Summary: 
Retraction failures are common in science, and this study aims to understand why they occur and what factors make findings harder or easier to retract. Utilizing data from Microsoft Academic Graph, Retraction Watch, and Altmetric, the study tests the hypothesis that the social spread of scientific information contributes to retraction failures. Surprisingly, the study finds that widely known or well-established results are actually easier to retract, as their retractions are more relevant to a larger number of scientists. Highly cited papers experience more significant reductions in citations post-retraction and attract more attention to their retractions. These findings suggest that the popularity and impact of a study can influence the ease of retraction, highlighting the importance of social spread and scientific dissemination in the retractions process. 

Summary: <div>
arXiv:2504.15504v1 Announce Type: cross 
Abstract: Failures of retraction are common in science. Why do these failures occur? And, relatedly, what makes findings harder or easier to retract? We use data from Microsoft Academic Graph, Retraction Watch, and Altmetric -- including retracted papers, citation records, and Altmetric scores and mentions -- to test recently proposed answers to these questions. A recent previous study by LaCroix et al. employ simple network models to argue that the social spread of scientific information helps explain failures of retraction. One prediction of their models is that widely known or well established results, surprisingly, should be easier to retract, since their retraction is more relevant to more scientists. Our results support this conclusion. We find that highly cited papers show more significant reductions in citation after retraction and garner more attention to their retractions as they occur.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing well-balanced spanning trees of unweighted networks</title>
<link>https://arxiv.org/abs/2205.06628</link>
<guid>https://arxiv.org/abs/2205.06628</guid>
<content:encoded><![CDATA[
<div> algorithm, spanning tree, network, breadth-first search, graph

Summary:<br /><br />
Spanning trees are crucial for simplifying and sampling networks. Prim's and Kruskal's algorithms are commonly used for weighted networks, but for unweighted networks, breadth-first search may be a better choice as it preserves network structure better and produces more compact and well-balanced spanning trees. The study conducted experiments on synthetic and real networks to compare the performance of different algorithms. The results suggest that breadth-first search algorithm outperforms priority-first search algorithms in terms of preserving network structure and producing efficient spanning trees. This research provides valuable insights into the impact of different algorithms on spanning tree construction and highlights the importance of choosing the right algorithm based on the network's characteristics. <div>
arXiv:2205.06628v2 Announce Type: replace 
Abstract: A spanning tree of a network or graph is a subgraph that connects all nodes with the least number or weight of edges. The spanning tree is one of the most straightforward techniques for network simplification and sampling, and for discovering its backbone or skeleton. Prim's algorithm and Kruskal's algorithm are well-known algorithms for computing a spanning tree of a weighted network, and are therefore also the default procedure for unweighted networks in the most popular network libraries. In this paper, we empirically study the performance of these algorithms on unweighted networks and compare them with different priority-first search algorithms. We show that the structure of a network, such as the distances between the nodes, is better preserved by a simpler algorithm based on breadth-first search. The spanning trees are also most compact and well-balanced as measured by classical graph indices. We support our findings with experiments on synthetic graphs and more than a thousand real networks, and demonstrate practical applications of the computed spanning trees. We conclude that if a spanning tree is to maintain the structure of an unweighted network, the breadth-first search algorithm should be the preferred choice.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-Based Model for Vehicle-Centric Data Sharing Ecosystem</title>
<link>https://arxiv.org/abs/2410.22897</link>
<guid>https://arxiv.org/abs/2410.22897</guid>
<content:encoded><![CDATA[
<div> Connected services, autonomous driving, data collection, privacy concerns, ontology<br />
Summary:<br />
The automotive industry is undergoing a transformation towards connected services and autonomous driving, leading to increased data collection and sharing in vehicles. This shift raises privacy concerns, prompting the need for understanding how modern vehicles handle data exchange among different parties. A high-level conceptual graph-based model was developed using the ontology 101 methodology, incorporating information from various sources including privacy policy analysis and literature review. The model provides insights into data sharing practices and can be expanded for diverse contexts. Realistic examples demonstrate the model's effectiveness in uncovering privacy issues related to vehicle-related data sharing. Future research directions include exploring advanced ontology languages for reasoning, conducting topological analysis to identify data privacy risks, and developing tools for comparative analysis. These efforts aim to enhance understanding of the vehicle-centric data sharing ecosystem. <div>
arXiv:2410.22897v3 Announce Type: replace 
Abstract: The development of technologies has prompted a paradigm shift in the automotive industry, with an increasing focus on connected services and autonomous driving capabilities. This transformation allows vehicles to collect and share vast amounts of vehicle-specific and personal data. While these technological advancements offer enhanced user experiences, they also raise privacy concerns. To understand the ecosystem of data collection and sharing in modern vehicles, we adopted the ontology 101 methodology to incorporate information extracted from different sources, including analysis of privacy policies using GPT-4, a small-scale systematic literature review, and an existing ontology, to develop a high-level conceptual graph-based model, aiming to get insights into how modern vehicles handle data exchange among different parties. This serves as a foundational model with the flexibility and scalability to further expand for modelling and analysing data sharing practices across diverse contexts. Two realistic examples were developed to demonstrate the usefulness and effectiveness of discovering insights into privacy regarding vehicle-related data sharing. We also recommend several future research directions, such as exploring advanced ontology languages for reasoning tasks, supporting topological analysis for discovering data privacy risks/concerns, and developing useful tools for comparative analysis, to strengthen the understanding of the vehicle-centric data sharing ecosystem.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the accurate computation of expected modularity in probabilistic networks</title>
<link>https://arxiv.org/abs/2408.07161</link>
<guid>https://arxiv.org/abs/2408.07161</guid>
<content:encoded><![CDATA[
<div> modularity, probabilistic networks, expected value, FPWP, computation <br />
Summary: <br />
Modularity is a key measure for assessing community structures in networks. In probabilistic networks with uncertain edge existence, computing the expected modularity is challenging. The proposed FPWP technique efficiently computes the probability distribution and expected value of modularity. Comparison with other methods reveals that removing low-probability edges or treating probabilities as weights leads to inaccuracies. Monte Carlo sampling has variable convergence based on network parameters. Brute-force computation, while accurate, is slow. In contrast, FPWP is fast and guarantees precise results. Comprehensive experiments on real-world and synthetic networks showcase the accuracy and time efficiency of the FPWP technique. <div>
arXiv:2408.07161v3 Announce Type: replace-cross 
Abstract: Modularity is one of the most widely used measures for evaluating communities in networks. In probabilistic networks, where the existence of edges is uncertain and uncertainty is represented by probabilities, the expected value of modularity can be used instead. However, efficiently computing expected modularity is challenging. To address this challenge, we propose a novel and efficient technique (FPWP) for computing the probability distribution of modularity and its expected value. In this paper, we implement and compare our method and various general approaches for expected modularity computation in probabilistic networks. These include: (1) translating probabilistic networks into deterministic ones by removing low-probability edges or treating probabilities as weights, (2) using Monte Carlo sampling to approximate expected modularity, and (3) brute-force computation. We evaluate the accuracy and time efficiency of FPWP through comprehensive experiments on both real-world and synthetic networks with diverse characteristics. Our results demonstrate that removing low-probability edges or treating probabilities as weights produces inaccurate results, while the convergence of the sampling method varies with the parameters of the network. Brute-force computation, though accurate, is prohibitively slow. In contrast, our method is much faster than brute-force computation, but guarantees an accurate result.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Model of Silence, or the Probability of "Un Ange Passe"</title>
<link>https://arxiv.org/abs/2504.13931</link>
<guid>https://arxiv.org/abs/2504.13931</guid>
<content:encoded><![CDATA[
<div> Keywords: silence, conversation dynamics, Markov chain model, phase-transition-like phenomenon, intercultural communication 

Summary: 
The study introduces the concept of "Un ange passe" or "An angel passes," a universal phenomenon of sudden silence in a co-present group. The significance and interpretation of silence vary across cultures, impacting organizational productivity, creativity, and medical settings. Mathematical modeling of silence dynamics is relatively unexplored, prompting the development of a Markov chain model to analyze silence behavior. Results indicate a phase-transition-like occurrence where silence abruptly ends once individuals lose awareness of the conversation, underscoring the necessity of mutual awareness for silence to arise. This model not only enhances understanding of conversational dynamics but also holds potential for enhancing intercultural communication, organizational efficiency, and medical practices. 

<br /><br />Summary: <div>
arXiv:2504.13931v1 Announce Type: new 
Abstract: In French, the phrase "Un ange passe" ("An angel passes") refers to the sudden silence that falls over a co-present group -- that is, a group of people sharing the same physical space. As evidenced by the presence of similar expressions across languages and cultures, this phenomenon represents a universal feature of human conversation. At the same time, the meaning attributed to silence can differ greatly across national, cultural, and interpersonal contexts. Consequently, a wide range of studies have focused on the impact of silence on organizational productivity, its relationship to ideas and creativity, and its potential effectiveness in medical settings. Despite the important role that silence plays, very few studies have attempted to characterize its features using mathematical modeling. In this study, we propose a Markov chain model to describe the dynamics of silence in a co-present group and attempt to analyze its behavior. Our results reveal a phase-transition-like phenomenon, where the probability of silence abruptly drops to zero once individuals' awareness of the surrounding conversation falls below a critical threshold. In other words, such silence can emerge only when individuals retain a minimal degree of mutual awareness of those around them. The model proposed in this study not only offers a deeper understanding of conversational dynamics, but also holds potential for contributing to intercultural communication, organizational productivity, and medical practice.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Writing Patterns Reveal a Hidden Division of Labor in Scientific Teams</title>
<link>https://arxiv.org/abs/2504.14093</link>
<guid>https://arxiv.org/abs/2504.14093</guid>
<content:encoded><![CDATA[
<div> Keywords: individual contributions, coauthored papers, author order, writing signatures, labor specialization

Summary:
This study proposes a behavior-based approach to identifying individual contributions in scientific papers by analyzing author-specific LaTeX macros used as writing signatures in over 730,000 arXiv papers. The research covers the period from 1991 to 2023 and includes over half a million scientists. The method is validated against self-reports, author order, disciplinary norms, and Overleaf records to reliably infer author-level writing activity. The findings reveal a hidden division of labor in collaborative research, with first authors focusing on technical sections and last authors contributing to conceptual sections. This empirical evidence of labor specialization at scale provides new tools to improve credit allocation in scientific collaborations. <div>
arXiv:2504.14093v1 Announce Type: new 
Abstract: The recognition of individual contributions is central to the scientific reward system, yet coauthored papers often obscure who did what. Traditional proxies like author order assume a simplistic decline in contribution, while emerging practices such as self-reported roles are biased and limited in scope. We introduce a large-scale, behavior-based approach to identifying individual contributions in scientific papers. Using author-specific LaTeX macros as writing signatures, we analyze over 730,000 arXiv papers (1991-2023), covering over half a million scientists. Validated against self-reports, author order, disciplinary norms, and Overleaf records, our method reliably infers author-level writing activity. Section-level traces reveal a hidden division of labor: first authors focus on technical sections (e.g., Methods, Results), while last authors primarily contribute to conceptual sections (e.g., Introduction, Discussion). Our findings offer empirical evidence of labor specialization at scale and new tools to improve credit allocation in collaborative research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking mob Dynamics in online social networks Using epidemiology model based on Mobility Equations</title>
<link>https://arxiv.org/abs/2504.14172</link>
<guid>https://arxiv.org/abs/2504.14172</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, epidemiological models, COVID-19 spread, Twitter activity, mobility dynamics

Summary:
<br />
This research focuses on analyzing the social behavior related to the spread of COVID-19 using Twitter activity data from April to June 2020. A mathematical model is introduced to integrate mobility dynamics, derived from real data, to adjust outbreak rates based on social interactions. The model considers mobility as a time-varying parameter, accounting for fluctuations in contact rates influenced by factors like personal behavior and external factors such as lockdowns and quarantines. The study identifies a threshold number, examines the existence of bifurcation, and establishes the stability of steady states. Numerical simulations and sensitivity analysis of relevant parameters are conducted to track public sentiment and engagement trends during the pandemic. <div>
arXiv:2504.14172v1 Announce Type: new 
Abstract: Nowadays, social media is the main tool in our new lives. The outbreak news and all related obtained from social media, and mob events affect the of spread these news fast. Recently, epidemiological models to study disease spread and analyze the behavior of mob groups by dealing with "contagions" that propagate through user networks. In this research, we introduced a mathematical model to analyze social behavior related to COVID-19 spread by examining Twitter activity from April 2020 to June 2020. The main feature of this model is the integration of mobility dynamics that be derived from the above real data, to adjust the rate of outbreak based on the response of social interactions. Consider mobility as a parameter of time-varying, and fluctuations in the rate of contact that is driven by factors like personal behavior or external affecting such as "lockdown" and "quarantine" etc., to track public sentiment and engagement trends during the pandemic. The threshold number is derived, and the existence of bifurcation and the stability of the steady states are established. Numerical simulations and sensitivity analysis of relevant parameters are also carried out.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Diffusion and Preferential Attachment in a Network of Large Language Models</title>
<link>https://arxiv.org/abs/2504.14438</link>
<guid>https://arxiv.org/abs/2504.14438</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, information diffusion, two-time-scale dynamical model, reputation-based preferential attachment mechanism, numerical experiments

Summary:
This paper presents a model for information diffusion in a network of Large Language Models (LLMs) that can provide answers to queries from distributed datasets, even potentially hallucinating the answer. The model incorporates a two-time-scale dynamical system where opinions evolve rapidly while network structure changes slowly. By using a mean-field approximation, the paper establishes conditions for a stable equilibrium where all LLMs remain truthful. Additionally, approximation guarantees for the mean-field and singularly perturbed approximations are provided. To address hallucination and enhance the influence of truthful nodes, a reputation-based preferential attachment mechanism is proposed to reconfigure the network based on evaluations by LLMs of their neighbors. Numerical experiments conducted on an open-source LLM validate the effectiveness of the preferential attachment mechanism and demonstrate optimization of a cost function for the two-time-scale system. <br /><br />Summary: <div>
arXiv:2504.14438v1 Announce Type: new 
Abstract: This paper models information diffusion in a network of Large Language Models (LLMs) that is designed to answer queries from distributed datasets, where the LLMs can hallucinate the answer. We introduce a two-time-scale dynamical model for the centrally administered network, where opinions evolve faster while the network's degree distribution changes more slowly. Using a mean-field approximation, we establish conditions for a locally asymptotically stable equilibrium where all LLMs remain truthful. We provide approximation guarantees for the mean-field approximation and a singularly perturbed approximation of the two-time-scale system. To mitigate hallucination and improve the influence of truthful nodes, we propose a reputation-based preferential attachment mechanism that reconfigures the network based on LLMs' evaluations of their neighbors. Numerical experiments on an open-source LLM (LLaMA-3.1-8B) validate the efficacy of our preferential attachment mechanism and demonstrate the optimization of a cost function for the two-time-scale system.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Local Separators Shape Community Structure in Large Networks</title>
<link>https://arxiv.org/abs/2504.14501</link>
<guid>https://arxiv.org/abs/2504.14501</guid>
<content:encoded><![CDATA[
<div> community detection, local separators, network structure, modularity optimization, hierarchical structures

Summary:
This study explores the use of local separators for community detection in large networks. Local 1-separators are found to effectively identify densely connected communities, outperforming traditional modularity-based methods. On the other hand, local 2-separators reveal hierarchical structures within networks, although they may lead to over-fragmentation of small clusters. These findings are particularly relevant for road networks, suggesting potential applications in transportation and infrastructure analysis. Overall, local separators offer a scalable and interpretable alternative to traditional community detection algorithms, providing a more nuanced understanding of network structure based on structural bottlenecks rather than global connectivity.<br /><br />Summary: <div>
arXiv:2504.14501v1 Announce Type: new 
Abstract: Community detection is a key tool for analyzing the structure of large networks. Standard methods, such as modularity optimization, focus on identifying densely connected groups but often overlook natural local separations in the graph. In this paper, we investigate local separator methods, which decompose networks based on structural bottlenecks rather than global connectivity. We systematically compare them with well-established community detection algorithms on large real-world networks. Our results show that local 1-separators consistently identify the densest communities, outperforming modularity-based methods in this regard, while local 2-separators reveal hierarchical structures but may over-fragment small clusters. These findings are particularly strong for road networks, suggesting practical applications in transportation and infrastructure analysis. Our study highlights local separators as a scalable and interpretable alternative for network decomposition.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform</title>
<link>https://arxiv.org/abs/2504.14904</link>
<guid>https://arxiv.org/abs/2504.14904</guid>
<content:encoded><![CDATA[
<div> benchmark, content moderation, short video platforms, user feedback, KuaiMod <br />
Summary: <br />
Exponentially growing short video platforms (SVPs) struggle with content moderation, especially concerning minors' mental health. Existing methods have limitations such as human bias, lack of understanding nuanced content, and slow update cycles. This paper introduces a new SVP content moderation benchmark with authentic user feedback. The proposed KuaiMod framework addresses these challenges through training data construction, offline adaptation, and online deployment & refinement using large vision language models and Chain-of-Thought reasoning. KuaiMod outperforms other methods in moderation performance on the benchmark, reducing user reporting rates and increasing user engagement on Kuaishou. The open-sourced benchmark aims to advance research in SVP content moderation. <br /> <div>
arXiv:2504.14904v1 Announce Type: new 
Abstract: Exponentially growing short video platforms (SVPs) face significant challenges in moderating content detrimental to users' mental health, particularly for minors. The dissemination of such content on SVPs can lead to catastrophic societal consequences. Although substantial efforts have been dedicated to moderating such content, existing methods suffer from critical limitations: (1) Manual review is prone to human bias and incurs high operational costs. (2) Automated methods, though efficient, lack nuanced content understanding, resulting in lower accuracy. (3) Industrial moderation regulations struggle to adapt to rapidly evolving trends due to long update cycles. In this paper, we annotate the first SVP content moderation benchmark with authentic user/reviewer feedback to fill the absence of benchmark in this field. Then we evaluate various methods on the benchmark to verify the existence of the aforementioned limitations. We further propose our common-law content moderation framework named KuaiMod to address these challenges. KuaiMod consists of three components: training data construction, offline adaptation, and online deployment & refinement. Leveraging large vision language model (VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video toxicity based on sparse user feedback and fosters dynamic moderation policy with rapid update speed and high accuracy. Offline experiments and large-scale online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the best moderation performance on our benchmark. The deployment of KuaiMod reduces the user reporting rate by 20% and its application in video recommendation increases both Daily Active User (DAU) and APP Usage Time (AUT) on several Kuaishou scenarios. We have open-sourced our benchmark at https://kuaimod.github.io.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis</title>
<link>https://arxiv.org/abs/2504.15072</link>
<guid>https://arxiv.org/abs/2504.15072</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion propagation dynamics, multi-dimensional Hawkes processes, Graph Neural Network, VISTA dataset 

Summary:
The article introduces an innovative approach that combines multi-dimensional Hawkes processes with Graph Neural Network to model opinion propagation dynamics in social media. The extended multi-dimensional Hawkes process captures hierarchical structures, multi-dimensional interactions, and mutual influences across various topics, forming a complex propagation network. Additionally, the authors present the VISTA dataset, comprising 159 trending topics with detailed sentiment annotations across 11 categories. This dataset allows for a comprehensive understanding of public opinion dynamics across different domains like politics, entertainment, sports, health, and medicine. The approach offers strong interpretability by linking sentiment propagation to comment hierarchy and temporal evolution, serving as a robust baseline for future research. 

<br /><br />Summary: <div>
arXiv:2504.15072v1 Announce Type: new 
Abstract: The rapid development of social media has significantly reshaped the dynamics of public opinion, resulting in complex interactions that traditional models fail to effectively capture. To address this challenge, we propose an innovative approach that integrates multi-dimensional Hawkes processes with Graph Neural Network, modeling opinion propagation dynamics among nodes in a social network while considering the intricate hierarchical relationships between comments. The extended multi-dimensional Hawkes process captures the hierarchical structure, multi-dimensional interactions, and mutual influences across different topics, forming a complex propagation network. Moreover, recognizing the lack of high-quality datasets capable of comprehensively capturing the evolution of public opinion dynamics, we introduce a new dataset, VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015 second-level comments, and 29,578 third-level comments, covering diverse domains such as politics, entertainment, sports, health, and medicine. The dataset is annotated with detailed sentiment labels across 11 categories and clearly defined hierarchical relationships. When combined with our method, it offers strong interpretability by linking sentiment propagation to the comment hierarchy and temporal evolution. Our approach provides a robust baseline for future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Binary Opinions: A Deep Reinforcement Learning-Based Approach to Uncertainty-Aware Competitive Influence Maximization</title>
<link>https://arxiv.org/abs/2504.15131</link>
<guid>https://arxiv.org/abs/2504.15131</guid>
<content:encoded><![CDATA[
<div> Keywords: Competitive Influence Maximization, Deep Reinforcement Learning, Subjective Logic, Uncertainty, Online Social Networks

Summary: <br /><br />The article introduces DRIM, a novel framework for the Competitive Influence Maximization (CIM) problem in online social networks. DRIM leverages Deep Reinforcement Learning (DRL) and Subjective Logic to model uncertainty in user opinions and preferences, enhancing decision-making in seed selection for information propagation. The framework incorporates an Uncertainty-based Opinion Model (UOM) to capture realistic user uncertainty and optimize the spread of true information while countering false information. Results demonstrate that DRIM-based CIM schemes outperform existing methods in influence spread and efficiency. Sensitivity analysis reveals that network observability and information propagation positively impact performance, while high network activity can mitigate the effects of initial user biases. Overall, DRIM offers a comprehensive approach to CIM that embraces uncertainty and improves influence maximization strategies in online social networks. <div>
arXiv:2504.15131v1 Announce Type: new 
Abstract: The Competitive Influence Maximization (CIM) problem involves multiple entities competing for influence in online social networks (OSNs). While Deep Reinforcement Learning (DRL) has shown promise, existing methods often assume users' opinions are binary and ignore their behavior and prior knowledge. We propose DRIM, a multi-dimensional uncertainty-aware DRL-based CIM framework that leverages Subjective Logic (SL) to model uncertainty in user opinions, preferences, and DRL decision-making. DRIM introduces an Uncertainty-based Opinion Model (UOM) for a more realistic representation of user uncertainty and optimizes seed selection for propagating true information while countering false information. In addition, it quantifies uncertainty in balancing exploration and exploitation. Results show that UOM significantly enhances true information spread and maintains influence against advanced false information strategies. DRIM-based CIM schemes outperform state-of-the-art methods by up to 57% and 88% in influence while being up to 48% and 77% faster. Sensitivity analysis indicates that higher network observability and greater information propagation boost performance, while high network activity mitigates the effect of users' initial biases.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An experimental study of the influence of anonymous information on social media users</title>
<link>https://arxiv.org/abs/2504.15215</link>
<guid>https://arxiv.org/abs/2504.15215</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, anonymous information, influence, opinions, agent-based modeling

Summary: 
The study aimed to investigate the impact of anonymous information on individuals' opinions. Through an online experiment with participants from the U.S., the results showed that anonymous comments can influence up to half of the participants' opinion selections, potentially altering popularity rankings. Agent-based modeling was used to understand how this influence occurs, revealing a straightforward mechanism at play. The study also found that the strength of influence lessens as participants' confidence in their selections increases. Additionally, participants with higher confidence in their initial opinions were less likely to change them based on anonymous information. This study emphasizes the significant role that anonymous information from social media can play in shaping individuals' opinions, indicating the need for caution and critical thinking when consuming such content.<br /><br />Summary: <div>
arXiv:2504.15215v1 Announce Type: new 
Abstract: Increasingly, people use social media for their day-to-day interactions and as a source of information, even though much of this information is practically anonymous. This raises the question: does anonymous information influence its recipients? We conducted an online, two-phase, preregistered experiment using a nationally representative sample of participants from the U.S. to find the answer. To avoid biases of opinions among participants, in the first phase, each participant examines ten Rorschach inkblots and chooses one of four opinions assigned to each inkblot. In the second phase, the participants are randomly assigned to one of four distinct information conditions and are asked to revisit their opinions for the same ten inkblots. Conditions ranged from repeating phase one to receiving anonymous comments about certain opinions. Results were consistent with the preregistration. Importantly, anonymous comments shown in phase two influence up to half of the participants' opinion selections. To better understand the role of anonymous comments in influencing the selections of opinions, we implemented agent-based modeling (ABM). ABM results suggest that a straightforward mechanism can explain the impact of such information. Overall, our results indicate that even anonymous information can have a significant impact on its recipients, potentially altering their popularity rankings. However, the strength of such influence weakens when recipients' confidence in their selections increases. Additionally, we found that participants' confidence in the first phase is inversely related to the number of change opinions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Theoretic Approach for Exploring the Relationship between EV Adoption and Charging Infrastructure Growth</title>
<link>https://arxiv.org/abs/2504.13902</link>
<guid>https://arxiv.org/abs/2504.13902</guid>
<content:encoded><![CDATA[
<div> Electric Vehicles, Charging Infrastructure, Graph Model, Time Granularity, Causality <br />
<br />
Summary: This study explores the relationship between Electric Vehicle (EV) adoption and Charging Infrastructure (CI) growth in 137 counties across six states in the U.S. Using a graph model and different time granularities, the analysis reveals that lower levels of time granularity lead to more homogeneous clusters, showing differences in EV adoption and CI growth. Causal relationships between EV adoption and CI growth are identified, with causality more prevalent in Early Adoption scenarios compared to Late Adoption ones. However, causal effects in Early Adoption are slower than in Late Adoption. The findings highlight the importance of understanding the complex dynamics between EV adoption and CI growth to facilitate the widespread adoption of electrified vehicles and address challenges in reducing CO2 emissions and natural resource depletion. <br /> <div>
arXiv:2504.13902v1 Announce Type: cross 
Abstract: The increasing global demand for conventional energy has led to significant challenges, particularly due to rising CO2 emissions and the depletion of natural resources. In the U.S., light-duty vehicles contribute significantly to transportation sector emissions, prompting a global shift toward electrified vehicles (EVs). Among the challenges that thwart the widespread adoption of EVs is the insufficient charging infrastructure (CI). This study focuses on exploring the complex relationship between EV adoption and CI growth. Employing a graph theoretic approach, we propose a graph model to analyze correlations between EV adoption and CI growth across 137 counties in six states. We examine how different time granularities impact these correlations in two distinct scenarios: Early Adoption and Late Adoption. Further, we conduct causality tests to assess the directional relationship between EV adoption and CI growth in both scenarios. Our main findings reveal that analysis using lower levels of time granularity result in more homogeneous clusters, with notable differences between clusters in EV adoption and those in CI growth. Additionally, we identify causal relationships between EV adoption and CI growth in 137 counties, and show that causality is observed more frequently in Early Adoption scenarios than in Late Adoption ones. However, the causal effects in Early Adoption are slower than those in Late Adoption.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Conspiratorial Narratives within Arabic Online Content</title>
<link>https://arxiv.org/abs/2504.14037</link>
<guid>https://arxiv.org/abs/2504.14037</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiracy theories, Arabic digital spaces, Named Entity Recognition, Topic Modeling, socio-political contexts

Summary: 
This study examines the spread of conspiracy theories in Arabic online platforms by using computational analysis techniques. The researchers apply Named Entity Recognition and Topic Modeling, particularly the Top2Vec algorithm, to analyze data from Arabic blogs and Facebook. They identify and classify various conspiratorial narratives, including gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering theories. The research demonstrates how these narratives are deeply ingrained in Arabic social media discussions, influenced by regional historical, cultural, and socio-political factors. By leveraging advanced Natural Language Processing methods with Arabic content, this study fills a gap in conspiracy theory research that has traditionally focused on English-language or offline data. The findings offer fresh insights into how conspiracy theories manifest and evolve in Arabic digital spaces, contributing to a better understanding of their impact on public discourse in the Arab world.

<br /><br />Summary: <div>
arXiv:2504.14037v1 Announce Type: cross 
Abstract: This study investigates the spread of conspiracy theories in Arabic digital spaces through computational analysis of online content. By combining Named Entity Recognition and Topic Modeling techniques, specifically the Top2Vec algorithm, we analyze data from Arabic blogs and Facebook to identify and classify conspiratorial narratives. Our analysis uncovers six distinct categories: gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering. The research highlights how these narratives are deeply embedded in Arabic social media discourse, shaped by regional historical, cultural, and sociopolitical contexts. By applying advanced Natural Language Processing methods to Arabic content, this study addresses a gap in conspiracy theory research, which has traditionally focused on English-language content or offline data. The findings provide new insights into the manifestation and evolution of conspiracy theories in Arabic digital spaces, enhancing our understanding of their role in shaping public discourse in the Arab world.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matrix Factorization with Dynamic Multi-view Clustering for Recommender System</title>
<link>https://arxiv.org/abs/2504.14565</link>
<guid>https://arxiv.org/abs/2504.14565</guid>
<content:encoded><![CDATA[
<div> dynamic multi-view clustering, matrix factorization, recommender systems, representation learning, interpretability

Summary:<br /><br />Matrix Factorization with Dynamic Multi-view Clustering (MFDMC) is proposed as a unified framework for efficient and comprehensive representation learning in recommender systems. MFDMC addresses the challenges of large-scale applications by performing end-to-end training and leveraging dynamic multi-view clustering. This approach improves efficiency and interpretability by adaptively pruning poorly formed clusters and capturing diverse roles of entities across views. The proposed framework demonstrates superior performance in recommender systems and other representation learning domains, such as computer vision. MFDMC's scalability and versatility make it a promising solution for web-scale applications. <div>
arXiv:2504.14565v1 Announce Type: cross 
Abstract: Matrix factorization (MF), a cornerstone of recommender systems, decomposes user-item interaction matrices into latent representations. Traditional MF approaches, however, employ a two-stage, non-end-to-end paradigm, sequentially performing recommendation and clustering, resulting in prohibitive computational costs for large-scale applications like e-commerce and IoT, where billions of users interact with trillions of items. To address this, we propose Matrix Factorization with Dynamic Multi-view Clustering (MFDMC), a unified framework that balances efficient end-to-end training with comprehensive utilization of web-scale data and enhances interpretability. MFDMC leverages dynamic multi-view clustering to learn user and item representations, adaptively pruning poorly formed clusters. Each entity's representation is modeled as a weighted projection of robust clusters, capturing its diverse roles across views. This design maximizes representation space utilization, improves interpretability, and ensures resilience for downstream tasks. Extensive experiments demonstrate MFDMC's superior performance in recommender systems and other representation learning domains, such as computer vision, highlighting its scalability and versatility.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Telegram as a Battlefield: Kremlin-related Communications during the Russia-Ukraine Conflict</title>
<link>https://arxiv.org/abs/2501.01884</link>
<guid>https://arxiv.org/abs/2501.01884</guid>
<content:encoded><![CDATA[
<div> Dataset, Telegram, Russia-Ukraine conflict, content moderation, misinformation 

Summary: 
The paper introduces a dataset of posts from both pro-Kremlin and anti-Kremlin Telegram channels, collected before and after the Russian invasion of Ukraine. The dataset includes over 4 million posts from 404 pro-Kremlin channels and over 1 million posts from 114 anti-Kremlin channels. It outlines the data collection process, processing methods, and characteristics of the dataset. The study highlights the dissemination of Pro-Kremlin narratives and potential misinformation on Telegram due to its minimal content moderation policies. It also notes the spread of anti-Kremlin narratives, including war footage, troop movements, and air raid warnings. The paper concludes by discussing the research opportunities this dataset presents for scholars in various disciplines. <br /><br />Summary: <div>
arXiv:2501.01884v3 Announce Type: replace 
Abstract: Telegram emerged as a crucial platform for both parties during the conflict between Russia and Ukraine. Per its minimal policies for content moderation, Pro-Kremlin narratives and potential misinformation were spread on Telegram, while anti-Kremlin narratives with related content were also propagated, such as war footage, troop movements, maps of bomb shelters, and air raid warnings. This paper presents a dataset of posts from both pro-Kremlin and anti-Kremlin Telegram channels, collected over a period spanning a year before and a year after the Russian invasion. The dataset comprises 404 pro-Kremlin channels with 4,109,645 posts and 114 anti-Kremlin channels with 1,117,768 posts. We provide details on the data collection process, processing methods, and dataset characterization. Lastly, we discuss the potential research opportunities this dataset may enable researchers across various disciplines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences</title>
<link>https://arxiv.org/abs/2504.09428</link>
<guid>https://arxiv.org/abs/2504.09428</guid>
<content:encoded><![CDATA[
<div> Keywords: online games, friend recommendation, user features, structural information, FROG<br />
<br />
Summary: 
The article discusses the importance of friend recommendation in online games due to the popularity of mobile devices. Existing approaches have limitations in incorporating multi-modal user features and structural information from friendship graphs effectively. These limitations include ignoring high-order structural proximity between users, failing to learn pairwise relevance between users at a modality-specific level, and not capturing both local and global user preferences on different modalities. To address these issues, the paper introduces an end-to-end model called FROG that better models user preferences for potential friends. The model has been evaluated through comprehensive experiments, including offline evaluation and online deployment at Tencent, demonstrating its superiority over existing approaches. <div>
arXiv:2504.09428v2 Announce Type: replace 
Abstract: Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (e.g., images and texts) with the structural information in the friendship graph, due to the following limitations: (1) some of them ignore the high-order structural proximity between users, (2) some fail to learn the pairwise relevance between users at modality-specific level, and (3) some cannot capture both the local and global user preferences on different modalities. By addressing these issues, in this paper, we propose an end-to-end model FROG that better models the user preferences on potential friends. Comprehensive experiments on both offline evaluation and online deployment at Tencent have demonstrated the superiority of FROG over existing approaches.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrustMap: Mapping Truthfulness Stance of Social Media Posts on Factual Claims for Geographical Analysis</title>
<link>https://arxiv.org/abs/2504.10511</link>
<guid>https://arxiv.org/abs/2504.10511</guid>
<content:encoded><![CDATA[
<div> TrustMap, social media, factual claims, misinformation, stance detection  
Summary:  
TrustMap is a tool that categorizes social media posts into positive, negative, or neutral stances based on the truthfulness of factual claims, using advanced language models for automatic classification. The tool analyzes regional variations in stance patterns across the U.S., allowing users to explore how public opinions differ geographically. By connecting stance detection with geographical analysis, TrustMap provides valuable insights into how people engage with factual claims on social media. This innovative approach helps to combat the spread of misinformation and understand how individuals form opinions and make decisions based on the information they encounter online. <div>
arXiv:2504.10511v2 Announce Type: replace 
Abstract: Factual claims and misinformation circulate widely on social media and affect how people form opinions and make decisions. This paper presents a truthfulness stance map (TrustMap), an application that identifies and maps public stances toward factual claims across U.S. regions. Each social media post is classified as positive, negative, or neutral/no stance, based on whether it believes a factual claim is true or false, expresses uncertainty about the truthfulness, or does not explicitly take a position on the claim's truthfulness. The tool uses a retrieval-augmented model with fine-tuned language models for automatic stance classification. The stance classification results and social media posts are grouped by location to show how stance patterns vary geographically. TrustMap allows users to explore these patterns by claim and region and connects stance detection with geographical analysis to better understand public engagement with factual claims.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Graph Rewiring and Feature Denoising via Spectral Resonance</title>
<link>https://arxiv.org/abs/2408.07191</link>
<guid>https://arxiv.org/abs/2408.07191</guid>
<content:encoded><![CDATA[
<div> Algorithm, Denoise, Rewire, Graph Data, Node Classification

Summary: 
The proposed algorithm, Jointly Denoise and Rewire (JDR), simultaneously denoises node features and rewires the graph to enhance the performance of graph neural networks for node classification. By aligning the leading spectral spaces of the graph and feature matrices, JDR effectively handles graphs with multiple classes and varying levels of homophily or heterophily. The non-convex optimization problem associated with JDR is approximately solved, leading to improved results compared to existing rewiring methods across various synthetic and real-world node classification tasks. The theoretical justification for JDR in a simplified scenario further supports its effectiveness in enhancing the accuracy of downstream GNNs. <div>
arXiv:2408.07191v4 Announce Type: replace-cross 
Abstract: When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to jointly denoise the features and rewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Another Hour on TikTok: Reverse-engineering unique identifiers to obtain a complete slice of TikTok</title>
<link>https://arxiv.org/abs/2504.13279</link>
<guid>https://arxiv.org/abs/2504.13279</guid>
<content:encoded><![CDATA[
<div> Keywords: TikTok, platform, dataset, metadata, posts

Summary:
Through a newly developed method, researchers extracted a representative sample from TikTok to collect a vast amount of post data. The dataset obtained includes post metadata, video media data, and comments, providing a close to complete snapshot of TikTok activity. Critical statistics of the platform were reported, estimating that 117 million posts were produced on the day observed. This method allowed for a deeper understanding of TikTok's impact on global events and addressed issues in determining fundamental characteristics of the platform. The research sheds light on the massive scale of TikTok and its significance in today's digital landscape. <div>
arXiv:2504.13279v1 Announce Type: new 
Abstract: TikTok is now a massive platform, and has a deep impact on global events. But for all the preliminary studies done on it, there are still issues with determining fundamental characteristics of the platform. We develop a method to extract a representative sample from a specific time range on TikTok, and use it to collect >99\% of posts from a full hour on the platform, alongside a dataset of >99\% of posts from a single minute from each hour of a day. Through this, we obtain post metadata, video media data, and comments from a close to complete slice of TikTok. Using this dataset, we report the critical statistics of the platform, notably estimating a total of 117 million posts produced on the day we looked at on TikTok.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Informed by Micro and Mesoscopic Statistical Physics Methods for Community Detection</title>
<link>https://arxiv.org/abs/2504.13538</link>
<guid>https://arxiv.org/abs/2504.13538</guid>
<content:encoded><![CDATA[
<div> machine learning, community detection, complex networks, ensemble learning, structural patterns

Summary: 
The article introduces a novel framework that combines machine learning with statistical physics to enhance community detection in complex networks. Previous methods have focused on mesoscopic structures but struggle with incorporating fine-grained node similarities. The proposed framework integrates micro-level node-pair similarities into mesoscopic community structures using ensemble learning models. Experimental evaluations on artificial and real-world networks show that the framework outperforms conventional methods by achieving higher modularity and improved accuracy in NMI and ARI metrics. When ground-truth labels are available, the framework yields the most accurate results, effectively recovering real-world community structures with minimal misclassifications. The analysis of the correlation between node-pair similarity and evaluation metrics indicates a strong relationship, emphasizing the importance of node-pair similarity in enhancing detection accuracy. Overall, the study highlights the synergistic relationship between machine learning and statistical physics in uncovering complex structural patterns in networks. 

<br /><br />Summary: <div>
arXiv:2504.13538v1 Announce Type: new 
Abstract: Community detection plays a crucial role in understanding the structural organization of complex networks. Previous methods, particularly those from statistical physics, primarily focus on the analysis of mesoscopic network structures and often struggle to integrate fine-grained node similarities. To address this limitation, we propose a low-complexity framework that integrates machine learning to embed micro-level node-pair similarities into mesoscopic community structures. By leveraging ensemble learning models, our approach enhances both structural coherence and detection accuracy. Experimental evaluations on artificial and real-world networks demonstrate that our framework consistently outperforms conventional methods, achieving higher modularity and improved accuracy in NMI and ARI. Notably, when ground-truth labels are available, our approach yields the most accurate detection results, effectively recovering real-world community structures while minimizing misclassifications. To further explain our framework's performance, we analyze the correlation between node-pair similarity and evaluation metrics. The results reveal a strong and statistically significant correlation, underscoring the critical role of node-pair similarity in enhancing detection accuracy. Overall, our findings highlight the synergy between machine learning and statistical physics, demonstrating how machine learning techniques can enhance network analysis and uncover complex structural patterns.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reinforcement Learning Method to Factual and Counterfactual Explanations for Session-based Recommendation</title>
<link>https://arxiv.org/abs/2504.13632</link>
<guid>https://arxiv.org/abs/2504.13632</guid>
<content:encoded><![CDATA[
<div> Session-based Recommendation, Explanation, FCESR, Factual, Counterfactual<br />
Summary:<br />
Session-based Recommendation (SR) systems often lack transparency in their recommendations. The FCESR framework aims to provide explanations for SR models by highlighting both factual and counterfactual factors influencing recommendations. This novel approach uses combinatorial optimization and reinforcement learning to identify the critical sequence of items impacting recommendations. By incorporating factual and counterfactual insights into a contrastive learning paradigm, the framework enhances SR accuracy significantly. Through extensive evaluations on various datasets and SR architectures, FCESR not only improves recommendation accuracy but also enhances the quality and interpretability of explanations. This advancement in transparency paves the way for more trustworthy recommendation systems. <br />Summary: <div>
arXiv:2504.13632v1 Announce Type: new 
Abstract: Session-based Recommendation (SR) systems have recently achieved considerable success, yet their complex, "black box" nature often obscures why certain recommendations are made. Existing explanation methods struggle to pinpoint truly influential factors, as they frequently depend on static user profiles or fail to grasp the intricate dynamics within user sessions. In response, we introduce FCESR (Factual and Counterfactual Explanations for Session-based Recommendation), a novel framework designed to illuminate SR model predictions by emphasizing both the sufficiency (factual) and necessity (counterfactual) of recommended items. By recasting explanation generation as a combinatorial optimization challenge and leveraging reinforcement learning, our method uncovers the minimal yet critical sequence of items influencing recommendations. Moreover, recognizing the intrinsic value of robust explanations, we innovatively utilize these factual and counterfactual insights within a contrastive learning paradigm, employing them as high-quality positive and negative samples to fine-tune and significantly enhance SR accuracy. Extensive qualitative and quantitative evaluations across diverse datasets and multiple SR architectures confirm that our framework not only boosts recommendation accuracy but also markedly elevates the quality and interpretability of explanations, thereby paving the way for more transparent and trustworthy recommendation systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propagational Proxy Voting</title>
<link>https://arxiv.org/abs/2504.13641</link>
<guid>https://arxiv.org/abs/2504.13641</guid>
<content:encoded><![CDATA[
<div> Fractional Votes, Expected Utility, Voting Matrix, Absorbing Markov Chains, Budget Allocation <br />
Summary: 
This paper introduces a novel voting process where voters can allocate fractional votes based on their expected utility across various domains. By using a voting matrix to reflect preferences, the approach allows for a more nuanced expression of preferences by calculating results and relevance within each node. The authors leverage absorbing Markov chains to determine consensus and assess influence within participating nodes. An experiment involving 69 students on budget allocation demonstrates the practical application of this method. The study showcases the effectiveness of this approach in capturing diverse preferences and providing a more detailed understanding of individual and collective decision-making processes. <div>
arXiv:2504.13641v1 Announce Type: new 
Abstract: This paper proposes a voting process in which voters allocate fractional votes to their expected utility in different domains: over proposals, other participants, and sets containing proposals and participants. This approach allows for a more nuanced expression of preferences by calculating the result and relevance within each node. We modeled this by creating a voting matrix that reflects their preference. We use absorbing Markov chains to gain the consensus, and also calculate the influence within the participating nodes. We illustrate this method in action through an experiment with 69 students using a budget allocation topic.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Stereotypes: Exploring How Minority College Students Experience Stigma on Reddit</title>
<link>https://arxiv.org/abs/2504.13674</link>
<guid>https://arxiv.org/abs/2504.13674</guid>
<content:encoded><![CDATA[
<div> stigma, minority students, college, stereotype, discrimination
<br />
Summary: 
The research focuses on the unique challenges faced by minority college students, influenced by their gender/sexual orientation, race, religion, and academic environment. The study examines stigma processes experienced by minority groups, particularly in online spaces like the r/college subreddit. By utilizing a Stereotype-BERT model, the study identifies stages of stigma processes such as labeling, stereotyping, separation, status loss, and discrimination. Professional identity posts are mainly associated with stereotyping, while racial identity posts are more prevalent in status loss and discrimination. Intersectional posts, reflecting compounded vulnerabilities due to intersecting identities, show a higher frequency of status loss and discrimination. The study emphasizes the importance of intersectional approaches in identifying stigma processes to promote equity for minority groups, especially racial minorities. 
<br /><br /> <div>
arXiv:2504.13674v1 Announce Type: new 
Abstract: Minority college students face unique challenges shaped by their identities based on their gender/sexual orientation, race, religion, and academic institutions, which influence their academic and social experiences. Although research has highlighted the challenges faced by individual minority groups, the stigma process-labeling, stereotyping, separation, status loss, and discrimination-that underpin these experiences remains underexamined, particularly in the online spaces where college students are highly active. We address these gaps by examining posts on subreddit, r/college, as indicators for stigma processes, our approach applies a Stereotype-BERT model, including stance toward each stereotype. We extend the stereotype model to encompass status loss and discrimination by using semantic distance with their reference sentences. Our analyses show that professional indicated posts are primarily labeled under the stereotyping stage, whereas posts indicating racial are highly represented in status loss and discrimination. Intersectional identified posts are more frequently associated with status loss and discrimination. The findings of this study highlight the need for multifaceted intersectional approaches to identifying stigma, which subsequently serve as indicators to promote equity for minority groups, especially racial minorities and those experiencing compounded vulnerabilities due to intersecting identities.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models</title>
<link>https://arxiv.org/abs/2504.13261</link>
<guid>https://arxiv.org/abs/2504.13261</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, pedagogical grammar, foreign language education, evaluation 
Summary: 

The paper introduces CPG-EVAL, a benchmark designed to assess large language models' (LLMs) knowledge of pedagogical grammar in foreign language instruction. The benchmark includes tasks to test grammar recognition, fine-grained distinctions, categorical discrimination, and resistance to linguistic interference. Smaller LLMs perform well in single language instance tasks but struggle with interference and multiple instances. Larger models show better resistance to interference but still need accuracy improvement. The study emphasizes the importance of aligning LLMs with educational goals and creating more rigorous benchmarks. CPG-EVAL aims to guide the deployment of LLMs in Chinese language teaching contexts. The evaluation provides insights for educators, policymakers, and developers to understand LLM capabilities in education and informs decisions on their integration in foreign language instruction. Further research is needed to enhance model alignment and educational suitability. 

<br /><br />Summary: <div>
arXiv:2504.13261v1 Announce Type: cross 
Abstract: Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT has significantly impacted foreign language education, yet their pedagogical grammar competence remains under-assessed. This paper introduces CPG-EVAL, the first dedicated benchmark specifically designed to evaluate LLMs' knowledge of pedagogical grammar within the context of foreign language instruction. Methodology: The benchmark comprises five tasks designed to assess grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Findings: Smaller-scale models can succeed in single language instance tasks, but struggle with multiple instance tasks and interference from confusing instances. Larger-scale models show better resistance to interference but still have significant room for accuracy improvement. The evaluation indicates the need for better instructional alignment and more rigorous benchmarks, to effectively guide the deployment of LLMs in educational contexts. Value: This study offers the first specialized, theory-driven, multi-tiered benchmark framework for systematically evaluating LLMs' pedagogical grammar competence in Chinese language teaching contexts. CPG-EVAL not only provides empirical insights for educators, policymakers, and model developers to better gauge AI's current abilities in educational settings, but also lays the groundwork for future research on improving model alignment, enhancing educational suitability, and ensuring informed decision-making concerning LLM integration in foreign language instruction.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces</title>
<link>https://arxiv.org/abs/2504.13277</link>
<guid>https://arxiv.org/abs/2504.13277</guid>
<content:encoded><![CDATA[
<div> Interpersonal Theory of Suicide, Suicidal Ideation, Machine Learning, Natural Language Analysis, AI Chatbots
<br />
Summary: 
The study utilized the Interpersonal Theory of Suicide to analyze posts from Reddit's r/SuicideWatch, identifying dimensions and risk factors of suicidal ideation. High-risk posts often included mentions of planning, attempts, and pain. Supportive responses were analyzed, showing varied reactions at different stages of suicidal ideation posts. AI chatbots were explored for providing support, showing improved structural coherence but lacking in dynamic and personalized responses. Overall, the study highlights the importance of understanding factors affecting high-risk suicidal intent and the need for careful consideration in developing AI-driven interventions for mental health support. <div>
arXiv:2504.13277v1 Announce Type: cross 
Abstract: Suicide is a critical global public health issue, with millions experiencing suicidal ideation (SI) each year. Online spaces enable individuals to express SI and seek peer support. While prior research has revealed the potential of detecting SI using machine learning and natural language analysis, a key limitation is the lack of a theoretical framework to understand the underlying factors affecting high-risk suicidal intent. To bridge this gap, we adopted the Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607 posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions (Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired Capability of Suicide). We found that high-risk SI posts express planning and attempts, methods and tools, and weaknesses and pain. In addition, we also examined the language of supportive responses through psycholinguistic and content analyses to find that individuals respond differently to different stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI chatbots in providing effective supportive responses to suicidal ideation posts. We found that although AI improved structural coherence, expert evaluations highlight persistent shortcomings in providing dynamic, personalized, and deeply empathetic support. These findings underscore the need for careful reflection and deeper understanding in both the development and consideration of AI-driven interventions for effective mental health support.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal</title>
<link>https://arxiv.org/abs/2504.13284</link>
<guid>https://arxiv.org/abs/2504.13284</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet penetration, Africa, mobile Internet, Senegal, social networks

Summary:
Young people in Senegal are increasingly using the Internet, especially social networks, as Internet penetration rates rise in Africa. The availability of smartphones has further boosted mobile Internet usage among the youth. However, the limited number of operators in the market restricts the choices available, impacting the value for money for users. In this study, Twitter and Facebook comments were analyzed to understand the sentiment of young people towards the price of mobile Internet in Senegal and its perceived quality. The sentiment analysis model revealed the general feelings of the users towards the service. Overall, the study sheds light on the importance of affordable and quality mobile Internet services in Senegal to meet the demands of the younger population who heavily rely on social networks for communication and self-expression.<br /><br />Summary: <div>
arXiv:2504.13284v1 Announce Type: cross 
Abstract: Internet penetration rates in Africa are rising steadily, and mobile Internet is getting an even bigger boost with the availability of smartphones. Young people are increasingly using the Internet, especially social networks, and Senegal is no exception to this revolution. Social networks have become the main means of expression for young people. Despite this evolution in Internet access, there are few operators on the market, which limits the alternatives available in terms of value for money. In this paper, we will look at how young people feel about the price of mobile Internet in Senegal, in relation to the perceived quality of the service, through their comments on social networks. We scanned a set of Twitter and Facebook comments related to the subject and applied a sentiment analysis model to gather their general feelings.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Can't believe I'm crying over an anime girl": Public Parasocial Grieving and Coping Towards VTuber Graduation and Termination</title>
<link>https://arxiv.org/abs/2504.13421</link>
<guid>https://arxiv.org/abs/2504.13421</guid>
<content:encoded><![CDATA[
<div> Keywords: Virtual YouTubers, parasocial relationships, VTuber retirements, coping methods, community dynamics

Summary:
This study explores the dynamics of viewer-VTuber relationships, focusing on how English-speaking viewers cope with the retirement of VTubers. The research categorizes different types of VTuber retirements and analyzes Reddit posts to understand viewer reactions. Findings show that viewers experience emotions like sadness, shock, and loyalty, with coping methods resembling those used when losing loved ones. As time passes, emotions like sadness and love decrease while regret and loyalty show opposite trends. Viewers' reactions also highlight the VTuber identity's place within a larger community of content creators and viewers. The study discusses design implications and their impact on the VTuber ecosystem, providing insights for future research in this emerging field.<br /><br />Summary: This study delves into how English-speaking viewers navigate the retirement of Virtual YouTubers, categorizing different types of retirements and analyzing viewer reactions on Reddit. The findings reveal a range of emotions experienced by viewers, with coping methods resembling those used in times of loss. Over time, emotions like sadness and love decrease, while regret and loyalty show contrasting trends. Viewer reactions also shed light on the VTuber identity's role within a broader community of creators and viewers. The study concludes by discussing design implications and their potential impact on the VTuber ecosystem, suggesting directions for further research in this evolving field. <div>
arXiv:2504.13421v1 Announce Type: cross 
Abstract: Despite the significant increase in popularity of Virtual YouTubers (VTubers), research on the unique dynamics of viewer-VTuber parasocial relationships is nascent. This work investigates how English-speaking viewers grieved VTubers whose identities are no longer used, an interesting context as the nakanohito (i.e., the person behind the VTuber identity) is usually alive post-retirement and might "reincarnate" as another VTuber. We propose a typology for VTuber retirements and analyzed 13,655 Reddit posts and comments spanning nearly three years using mixed-methods. Findings include how viewers coped using methods similar to when losing loved ones, alongside novel coping methods reflecting different attachment styles. Although emotions like sadness, shock, concern, disapproval, confusion, and love decreased with time, regret and loyalty showed opposite trends. Furthermore, viewers' reactions situated a VTuber identity within a community of content creators and viewers. We also discuss design implications alongside implications on the VTuber ecosystem and future research directions.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dataset of the Representatives Elected in France During the Fifth Republic</title>
<link>https://arxiv.org/abs/2504.02869</link>
<guid>https://arxiv.org/abs/2504.02869</guid>
<content:encoded><![CDATA[
<div> Keywords: electoral system, political representation, France, database, political change<br />
Summary:<br /> 
This article discusses the importance of the electoral system in shaping democracy, particularly in France. It highlights the challenges in accessing data on elected representatives and introduces a new relational database that compiles information on representatives in France since the Fifth Republic. This database provides a valuable resource for analyzing trends in political representation, party dynamics, gender equality, and the professionalization of politics. By offering a longitudinal view of elected representatives, the database enables researchers to study the institutional stability of the Fifth Republic and identify factors driving political change. Overall, this database enhances understanding of political processes in France and facilitates in-depth analyses of the country's political landscape over time. <br /><br />Summary: <div>
arXiv:2504.02869v2 Announce Type: replace 
Abstract: The electoral system is a cornerstone of democracy, shaping the structure of political competition, representation, and accountability. In the case of France, it is difficult to access data describing elected representatives, though, as they are scattered across a number of sources, including public institutions, but also academic and individual efforts. This article presents a unified relational database that aims at tackling this issue by gathering information regarding representatives elected in France over the whole Fifth Republic (1958-present). This database constitutes an unprecedented resource for analyzing the evolution of political representation in France, exploring trends in party system dynamics, gender equality, and the professionalization of politics. By providing a longitudinal view of French elected representatives, the database facilitates research on the institutional stability of the Fifth Republic, offering insights into the factors of political change.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning when to rank: Estimation of partial rankings from sparse, noisy comparisons</title>
<link>https://arxiv.org/abs/2501.02505</link>
<guid>https://arxiv.org/abs/2501.02505</guid>
<content:encoded><![CDATA[
<div> Bayesian methodology, partial rankings, pairwise comparisons, Bradley-Terry model, inference-based methods <br />
Summary: <br />
- The article introduces a Bayesian methodology for learning partial rankings from pairwise comparison data, addressing the challenge of distinguishing between items with limited or noisy comparisons.
- Current inference-based ranking methods often assign unique ranks to each item, even when there is insufficient evidence to differentiate their performance.
- The proposed framework allows for the incorporation of ties in rankings, only distinguishing between items when there is enough evidence from the data.
- An agglomerative algorithm is developed for Maximum A Posteriori (MAP) inference of partial rankings.
- The method is evaluated on real and synthetic network datasets, demonstrating its ability to provide a more parsimonious summary of the data compared to traditional ranking methods, especially in cases of sparse observations. 

<br />
Summary: <div>
arXiv:2501.02505v2 Announce Type: replace-cross 
Abstract: A common task arising in various domains is that of ranking items based on the outcomes of pairwise comparisons, from ranking players and teams in sports to ranking products or brands in marketing studies and recommendation systems. Statistical inference-based methods such as the Bradley-Terry model, which extract rankings based on an underlying generative model of the comparison outcomes, have emerged as flexible and powerful tools to tackle the task of ranking in empirical data. In situations with limited and/or noisy comparisons, it is often challenging to confidently distinguish the performance of different items based on the evidence available in the data. However, existing inference-based ranking methods overwhelmingly choose to assign each item to a unique rank or score, suggesting a meaningful distinction when there is none. Here, we address this problem by developing a principled Bayesian methodology for learning partial rankings -- rankings with ties -- that distinguishes among the ranks of different items only when there is sufficient evidence available in the data. Our framework is adaptable to any statistical ranking method in which the outcomes of pairwise observations depend on the ranks or scores of the items being compared. We develop a fast agglomerative algorithm to perform Maximum A Posteriori (MAP) inference of partial rankings under our framework and examine the performance of our method on a variety of real and synthetic network datasets, finding that it frequently gives a more parsimonious summary of the data than traditional ranking, particularly when observations are sparse.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of Bluesky</title>
<link>https://arxiv.org/abs/2504.12902</link>
<guid>https://arxiv.org/abs/2504.12902</guid>
<content:encoded><![CDATA[
<div> Keywords: Bluesky, rapid growth, evolving network structure, user migrations, viral information diffusion 

Summary: 
Bluesky, a social media platform, experienced rapid growth and network evolution from August 2023 to February 2025. Multiple waves of user migrations contributed to the platform's establishment of a stable, actively engaged user base. The growth process led to the formation of a dense follower network characterized by clustering and hub features, facilitating the viral diffusion of information. These developments underscore the similarities in engagement and network structure between Bluesky and established social media platforms. This study sheds light on the dynamics of user engagement and network evolution in the context of Bluesky's growth trajectory. <div>
arXiv:2504.12902v1 Announce Type: new 
Abstract: This study investigates the rapid growth and evolving network structure of Bluesky from August 2023 to February 2025. Through multiple waves of user migrations, the platform has reached a stable, persistently active user base. The growth process has given rise to a dense follower network with clustering and hub features that favor viral information diffusion. These developments highlight engagement and structural similarities between Bluesky and established platforms.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media</title>
<link>https://arxiv.org/abs/2504.12325</link>
<guid>https://arxiv.org/abs/2504.12325</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, taxonomy, large language models, factual claims, automated construction

Summary:
LLMTaxo is a new framework that utilizes large language models to automatically generate a taxonomy of factual claims from social media. By creating topics at multiple granular levels, LLMTaxo helps stakeholders navigate the complex landscape of online discourse. The framework was tested on three diverse datasets using various models, and was evaluated using both human assessments and GPT-4. Results indicated that LLMTaxo effectively categorizes factual claims and highlights the superior performance of certain models on specific datasets. This innovative approach offers valuable insights for analyzing and comprehending online content, providing a more efficient way to classify and understand information shared on social media platforms. <div>
arXiv:2504.12325v1 Announce Type: cross 
Abstract: With the vast expansion of content on social media platforms, analyzing and comprehending online discourse has become increasingly complex. This paper introduces LLMTaxo, a novel framework leveraging large language models for the automated construction of taxonomy of factual claims from social media by generating topics from multi-level granularities. This approach aids stakeholders in more effectively navigating the social media landscapes. We implement this framework with different models across three distinct datasets and introduce specially designed taxonomy evaluation metrics for a comprehensive assessment. With the evaluations from both human evaluators and GPT-4, the results indicate that LLMTaxo effectively categorizes factual claims from social media, and reveals that certain models perform better on specific datasets.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Embeddings Track Social Group Changes Across 70 Years in China</title>
<link>https://arxiv.org/abs/2504.12327</link>
<guid>https://arxiv.org/abs/2504.12327</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese state-controlled media, word embeddings, social group representation, societal beliefs, linguistic analysis 

Summary: 
The article presents a computational analysis of Chinese state-controlled media from 1950 to 2019, focusing on how societal beliefs about social groups are reflected in language. The study utilizes diachronic word embeddings to analyze the evolution of representations of social groups in Chinese media. The findings suggest significant differences in Chinese representations compared to Western contexts, particularly in terms of ethnicity, economic status, and gender. The study reveals that stereotypes related to ethnicity, age, and body type remain stable over time, while representations of gender and economic classes undergo dramatic shifts in alignment with historical transformations. This research contributes to our understanding of how language encodes societal beliefs and social structures, emphasizing the importance of non-Western perspectives in computational social science. 

<br /><br />Summary: <div>
arXiv:2504.12327v1 Announce Type: cross 
Abstract: Language encodes societal beliefs about social groups through word patterns. While computational methods like word embeddings enable quantitative analysis of these patterns, studies have primarily examined gradual shifts in Western contexts. We present the first large-scale computational analysis of Chinese state-controlled media (1950-2019) to examine how revolutionary social transformations are reflected in official linguistic representations of social groups. Using diachronic word embeddings at multiple temporal resolutions, we find that Chinese representations differ significantly from Western counterparts, particularly regarding economic status, ethnicity, and gender. These representations show distinct evolutionary dynamics: while stereotypes of ethnicity, age, and body type remain remarkably stable across political upheavals, representations of gender and economic classes undergo dramatic shifts tracking historical transformations. This work advances our understanding of how officially sanctioned discourse encodes social structure through language while highlighting the importance of non-Western perspectives in computational social science.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"It Listens Better Than My Therapist": Exploring Social Media Discourse on LLMs as Mental Health Tool</title>
<link>https://arxiv.org/abs/2504.12337</link>
<guid>https://arxiv.org/abs/2504.12337</guid>
<content:encoded><![CDATA[
<div> mental health support, AI chatbots, user engagement, TikTok comments, ethical scrutiny 
Summary: 
This study examines user engagement with large language models (LLMs) like ChatGPT as mental health tools through the analysis of over 10,000 TikTok comments. Approximately 20% of comments reflect personal use, with users reporting positive attitudes towards LLMs for mental health support. Accessibility, emotional support, and perceived therapeutic value are commonly cited benefits. However, concerns around privacy, generic responses, and the lack of professional oversight are prominent. The study highlights the need for clinical and ethical scrutiny in the use of AI for mental health support, as user feedback does not indicate alignment with any therapeutic framework. The findings emphasize the growing relevance of AI in everyday practices while underscoring the importance of addressing privacy, ethical, and clinical considerations when utilizing AI chatbots for mental health support. 
<br /><br />Summary: <div>
arXiv:2504.12337v1 Announce Type: cross 
Abstract: The emergence of generative AI chatbots such as ChatGPT has prompted growing public and academic interest in their role as informal mental health support tools. While early rule-based systems have been around for several years, large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability. This study explores how users engage with LLMs as mental health tools by analyzing over 10,000 TikTok comments from videos referencing LLMs as mental health tools. Using a self-developed tiered coding schema and supervised classification models, we identify user experiences, attitudes, and recurring themes. Results show that nearly 20% of comments reflect personal use, with these users expressing overwhelmingly positive attitudes. Commonly cited benefits include accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and the lack of professional oversight remain prominent. It is important to note that the user feedback does not indicate which therapeutic framework, if any, the LLM-generated output aligns with. While the findings underscore the growing relevance of AI in everyday practices, they also highlight the urgent need for clinical and ethical scrutiny in the use of AI for mental health support.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis</title>
<link>https://arxiv.org/abs/2408.09613</link>
<guid>https://arxiv.org/abs/2408.09613</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots, misinformation, Sina Weibo, dataset, information spread

Summary: 
Social bots play a significant role in spreading misinformation on the Sina Weibo platform, as shown in a dataset containing annotations of both misinformation and social bots. Through extensive experiments, it is evident that the dataset is comprehensive, with distinguishable misinformation and real information, and high-quality social bot annotations. The analysis reveals that social bots are actively involved in disseminating information, contributing to echo chambers by amplifying similar content on the same topics, and generating content to manipulate public opinions. This study sheds light on the interplay between social bots and misinformation on social media platforms, highlighting the need for further research and measures to combat the spread of false information. 

<br /><br />Summary: <div>
arXiv:2408.09613v2 Announce Type: replace 
Abstract: The social media platform is an ideal medium to spread misinformation, where social bots might accelerate the spread. This paper is the first to explore the interplay between social bots and misinformation on the Sina Weibo platform. We construct a large-scale dataset that contains annotations of misinformation and social bots. From the misinformation perspective, this dataset is multimodal, containing 11,393 pieces of misinformation and 16,416 pieces of real information. From the social bot perspective, this dataset contains 65,749 social bots and 345,886 genuine accounts, where we propose a weak-supervised annotator to annotate automatically. Extensive experiments prove that the dataset is the most comprehensive, misinformation and real information are distinguishable, and social bots have high annotation quality. Further analysis illustrates that: (i) social bots are deeply involved in information spread; (ii) misinformation with the same topics has similar content, providing the basis of echo chambers, and social bots amplify this phenomenon; and (iii) social bots generate similar content aiming to manipulate public opinions.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governance as a complex, networked, democratic, satisfiability problem</title>
<link>https://arxiv.org/abs/2412.03421</link>
<guid>https://arxiv.org/abs/2412.03421</guid>
<content:encoded><![CDATA[
<div> modeling, governance structures, decision-making, social network, democratic governments

Summary:
The article examines different governance structures within democratic governments through a social network framework. By modeling decision-making as a satisfiability problem and information flow as a social hypergraph, the study explores various governance strategies from dictatorships to direct democracy. The research suggests that effective governance can be achieved through small overlapping decision groups that make specific decisions and share information. This approach allows even polarized populations to make coherent decisions with low coordination costs. The conceptual framework can simulate different governance strategies and their effectiveness in addressing complex societal challenges. This new perspective on governance structures offers insights into improving decision-making processes in democratic societies. 

<br /><br />Summary: <div>
arXiv:2412.03421v2 Announce Type: replace-cross 
Abstract: Democratic governments comprise a subset of a population whose goal is to produce coherent decisions, solving societal challenges while respecting the will of the people. New governance frameworks represent this as a social network rather than as a hierarchical pyramid with centralized authority. But how should this network be structured? We model the decisions a population must make as a satisfiability problem and the structure of information flow involved in decision-making as a social hypergraph. This framework allows to consider different governance structures, from dictatorships to direct democracy. Between these extremes, we find a regime of effective governance where small overlapping decision groups make specific decisions and share information. Effective governance allows even incoherent or polarized populations to make coherent decisions at low coordination costs. Beyond simulations, our conceptual framework can explore a wide range of governance strategies and their ability to tackle decision problems that challenge standard governments.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models</title>
<link>https://arxiv.org/abs/2504.00046</link>
<guid>https://arxiv.org/abs/2504.00046</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, disaster response, LLMs, classification techniques, generative AI 

Summary: 
This paper discusses the use of social media in disaster response and management, highlighting the need for automated, aggregated, and customized data analysis to provide actionable insights for various stakeholders. The methodology presented utilizes Language Model-based approaches to analyze social media posts during disasters, focusing on user-reported issues and challenges. By employing full-spectrum Language Models like BERT for precise classification and generative models like ChatGPT for tailored report generation, the system bridges the gap between raw feedback and stakeholder-specific reports. The comparison between standard approaches and the advanced methodology shows superior performance in both quantitative metrics and qualitative assessments. The methodology enhances coordination of relief efforts, resource distribution, and media communication, delivering precise insights for diverse stakeholders involved in disaster response. <div>
arXiv:2504.00046v2 Announce Type: replace-cross 
Abstract: In recent years, social media has emerged as a primary channel for users to promptly share feedback and issues during disasters and emergencies, playing a key role in crisis management. While significant progress has been made in collecting and analyzing social media content, there remains a pressing need to enhance the automation, aggregation, and customization of this data to deliver actionable insights tailored to diverse stakeholders, including the press, police, EMS, and firefighters. This effort is essential for improving the coordination of activities such as relief efforts, resource distribution, and media communication. This paper presents a methodology that leverages the capabilities of LLMs to enhance disaster response and management. Our approach combines classification techniques with generative AI to bridge the gap between raw user feedback and stakeholder-specific reports. Social media posts shared during catastrophic events are analyzed with a focus on user-reported issues, service interruptions, and encountered challenges. We employ full-spectrum LLMs, using analytical models like BERT for precise, multi-dimensional classification of content type, sentiment, emotion, geolocation, and topic. Generative models such as ChatGPT are then used to produce human-readable, informative reports tailored to distinct audiences, synthesizing insights derived from detailed classifications. We compare standard approaches, which analyze posts directly using prompts in ChatGPT, to our advanced method, which incorporates multi-dimensional classification, sub-event selection, and tailored report generation. Our methodology demonstrates superior performance in both quantitative metrics, such as text coherence scores and latent representations, and qualitative assessments by automated tools and field experts, delivering precise insights for diverse disaster response stakeholders.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Alignment: Discovering the Gap Between Social Media and Real-World Sentiments in Persian Tweets and Images</title>
<link>https://arxiv.org/abs/2504.10662</link>
<guid>https://arxiv.org/abs/2504.10662</guid>
<content:encoded><![CDATA[
<div> social media, emotions, Persian community, sentiment analysis, real-world

Summary: 
- An analysis of the emotional expressions in the Persian community on social media platform X compared to real-world experiences was conducted.
- A novel pipeline using Transformers-based text and image sentiment analysis modules was designed to measure emotional similarity between online and offline interactions.
- Results showed a 28.67% similarity between images on social media and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings.
- The study included 105 participants, 393 friends providing insights, over 8,300 collected tweets, and 2,000 media images.
- Statistical analysis confirmed the disparities in sentiment proportions between social media posts and real-world emotions. 

<br /><br />Summary: <div>
arXiv:2504.10662v2 Announce Type: replace-cross 
Abstract: In contemporary society, widespread social media usage is evident in people's daily lives. Nevertheless, disparities in emotional expressions between the real world and online platforms can manifest. We comprehensively analyzed Persian community on X to explore this phenomenon. An innovative pipeline was designed to measure the similarity between emotions in the real world compared to social media. Accordingly, recent tweets and images of participants were gathered and analyzed using Transformers-based text and image sentiment analysis modules. Each participant's friends also provided insights into the their real-world emotions. A distance criterion was used to compare real-world feelings with virtual experiences. Our study encompassed N=105 participants, 393 friends who contributed their perspectives, over 8,300 collected tweets, and 2,000 media images. Results indicated a 28.67% similarity between images and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings. Additionally, the statistical significance confirmed that the observed disparities in sentiment proportions.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Markov stability for community detection at a scale learned based on the structure</title>
<link>https://arxiv.org/abs/2504.11621</link>
<guid>https://arxiv.org/abs/2504.11621</guid>
<content:encoded><![CDATA[
arXiv:2504.11621v1 Announce Type: new 
Abstract: Community detection, the unsupervised task of clustering nodes of a graph, finds applications across various fields. The common approaches for community detection involve optimizing an objective function to partition the nodes into communities at a single scale of granularity. However, the single-scale approaches often fall short of producing partitions that are robust and at a suitable scale. The existing algorithm, PyGenStability, returns multiple robust partitions for a network by optimizing the multi-scale Markov stability function. However, in cases where the suitable scale is not known or assumed by the user, there is no principled method to select a single robust partition at a suitable scale from the multiple partitions that PyGenStability produces. Our proposed method combines the Markov stability framework with a pre-trained machine learning model for scale selection to obtain one robust partition at a scale that is learned based on the graph structure. This automatic scale selection involves using a gradient boosting model pre-trained on hand-crafted and embedding-based network features from a labeled dataset of 10k benchmark networks. This model was trained to predicts the scale value that maximizes the similarity of the output partition to the planted partition of the benchmark network. Combining our scale selection algorithm with the PyGenStability algorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale community detection algorithm that returns one robust partition at a suitable scale without the need for any assumptions, input, or tweaking from the user. We compare the performance of PO against 29 algorithms and show that it outperforms 25 other algorithms by statistically meaningful margins. Our results facilitate choosing between community detection algorithms, among which PO stands out as the accurate, robust, and hyperparameter-free method.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technological Complexity Based on Japanese Patent Data</title>
<link>https://arxiv.org/abs/2504.11932</link>
<guid>https://arxiv.org/abs/2504.11932</guid>
<content:encoded><![CDATA[
arXiv:2504.11932v1 Announce Type: new 
Abstract: As international competition intensifies in technologies, nations need to identify key technologies to foster innovation. However, the identification is difficult because a technology is independent, therefore has complex nature. Here, this study aims to assess patent technological fields by applying Technological Complexity Index from a corporate perspective, addressing its underutilization in Japan despite its potential. By utilizing carefully processed patent data from fiscal years 1981 to 2010, we analyze the bipartite network which consists of 1,938 corporations and 35 or 124 technological fields. Our findings provide quantitative characteristics of ubiquity and sophistication for patent fields, the detailed technological trends that reflect the social context, and methodological stability for policymakers and researchers, contributing to targeted innovation strategies in Japan.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2504.11699</link>
<guid>https://arxiv.org/abs/2504.11699</guid>
<content:encoded><![CDATA[
arXiv:2504.11699v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in representation learning, a challenge further amplified in self-supervised settings. We propose H$^3$GNNs, an end-to-end self-supervised learning framework that harmonizes both structural properties through two key innovations: (i) Joint Structural Node Encoding. We embed nodes into a unified space combining linear and non-linear feature projections with K-hop structural representations via a Weighted Graph Convolution Network(WGCN). A cross-attention mechanism enhances awareness and adaptability to heterophily and homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive Architectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a teacher-student model, the student sees the masked input graph and predicts node features inferred by the teacher that sees the full input graph in the joint encoding space. To enhance learning difficulty, we introduce two novel node-predictive-difficulty-based masking strategies. Experiments on seven benchmarks (four heterophily datasets and three homophily datasets) confirm the effectiveness and efficiency of H$^3$GNNs across diverse graph types. Our H$^3$GNNs achieves overall state-of-the-art performance on the four heterophily datasets, while retaining on-par performance to previous state-of-the-art methods on the three homophily datasets.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Analysis of Mixer Activities in the Bitcoin Network</title>
<link>https://arxiv.org/abs/2504.11924</link>
<guid>https://arxiv.org/abs/2504.11924</guid>
<content:encoded><![CDATA[
arXiv:2504.11924v1 Announce Type: cross 
Abstract: Cryptocurrency users increasingly rely on obfuscation techniques such as mixers, swappers, and decentralised or no-KYC exchanges to protect their anonymity. However, at the same time, these services are exploited by criminals to conceal and launder illicit funds. Among obfuscation services, mixers remain one of the most challenging entities to tackle. This is because their owners are often unwilling to cooperate with Law Enforcement Agencies, and technically, they operate as 'black boxes'. To better understand their functionalities, this paper proposes an approach to analyse the operations of mixers by examining their address-transaction graphs and identifying topological similarities to uncover common patterns that can define the mixer's modus operandi. The approach utilises community detection algorithms to extract dense topological structures and clustering algorithms to group similar communities. The analysis is further enriched by incorporating data from external sources related to known Exchanges, in order to understand their role in mixer operations. The approach is applied to dissect the Blender.io mixer activities within the Bitcoin blockchain, revealing: i) consistent structural patterns across address-transaction graphs; ii) that Exchanges play a key role, following a well-established pattern, which raises several concerns about their AML/KYC policies. This paper represents an initial step toward dissecting and understanding the complex nature of mixer operations in cryptocurrency networks and extracting their modus operandi.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Patterns of Viral Content on WhatsApp</title>
<link>https://arxiv.org/abs/2407.08172</link>
<guid>https://arxiv.org/abs/2407.08172</guid>
<content:encoded><![CDATA[
arXiv:2407.08172v2 Announce Type: replace 
Abstract: This paper explores the nature and spread of viral WhatsApp content among everyday users in three diverse countries: India, Indonesia, and Colombia. By analyzing hundreds of viral messages collected with participants' consent from private WhatsApp groups, we provide one of the first cross-cultural categorizations of viral content on WhatsApp. Despite the differences in cultural and geographic settings, our findings reveal striking similarities in the types of groups users engage with and the viral content they receive, particularly in the prevalence of misinformation. Our comparative analysis shows that viral content often includes political and religious narratives, with misinformation frequently recirculated despite prior debunking by fact-checking organizations. These parallels suggest that closed messaging platforms like WhatsApp facilitate similar patterns of information dissemination across different cultural contexts. This work contributes to the broader understanding of global digital communication ecosystems and provides a foundation for future research on information flow and moderation strategies in private messaging platforms.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socio-cognitive Networks between Researchers: Investigating Scientific Dualities with the Group-Oriented Relational Hyperevent Model</title>
<link>https://arxiv.org/abs/2407.21067</link>
<guid>https://arxiv.org/abs/2407.21067</guid>
<content:encoded><![CDATA[
arXiv:2407.21067v2 Announce Type: replace 
Abstract: Understanding why researchers cite certain works remains a key question in the study of scientific networks. Prior research has identified factors such as relevance, group cohesion, and source crediting. However, the interplay between cognitive and social dimensions in citation behavior - often conceptualized as a socio-cognitive network - is frequently overlooked, particularly regarding the intermediary steps that lead to a citation. Since a citation first requires a work to be published by a set of authors, we examine how the structure of coauthorship networks influences citation patterns. To investigate this relationship, we analyze the citation and collaboration behavior of Chilean astronomers from 2013 to 2015 using the Group-Oriented Relational Hyperevent Model, which allows us to study coauthorship and citation networks in a joint framework. Our findings suggest that when selecting which works to cite, authors favor recent research and maintain cognitive continuity across cited works. At the same time, we observe that coherent groups - closely connected coauthors - tend to be co-cited more frequently in subsequent publications, reinforcing the interdependence of collaboration and citation networks.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining higher-order triadic interactions</title>
<link>https://arxiv.org/abs/2404.14997</link>
<guid>https://arxiv.org/abs/2404.14997</guid>
<content:encoded><![CDATA[
arXiv:2404.14997v2 Announce Type: replace-cross 
Abstract: Complex systems often involve higher-order interactions which require us to go beyond their description in terms of pairwise networks. Triadic interactions are a fundamental type of higher-order interaction that occurs when one node regulates the interaction between two other nodes. Triadic interactions are found in a large variety of biological systems, from neuron-glia interactions to gene-regulation and ecosystems. However, triadic interactions have so far been mostly neglected. In this article, we propose a theoretical model that demonstrates that triadic interactions can modulate the mutual information between the dynamical state of two linked nodes. Leveraging this result, we propose the Triadic Interaction Mining (TRIM) algorithm to mine triadic interactions from node metadata, and we apply this framework to gene expression data, finding new candidates for triadic interactions relevant for Acute Myeloid Leukemia. Our work reveals important aspects of higher-order triadic interactions that are often ignored, yet can transform our understanding of complex systems and be applied to a large variety of systems ranging from biology to the climate.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiReddit: Tracing Information and Attention Flows Between Online Platforms</title>
<link>https://arxiv.org/abs/2502.04942</link>
<guid>https://arxiv.org/abs/2502.04942</guid>
<content:encoded><![CDATA[
arXiv:2502.04942v2 Announce Type: replace-cross 
Abstract: The World Wide Web is a complex interconnected digital ecosystem, where information and attention flow between platforms and communities throughout the globe. These interactions co-construct how we understand the world, reflecting and shaping public discourse. Unfortunately, researchers often struggle to understand how information circulates and evolves across the web because platform-specific data is often siloed and restricted by linguistic barriers. To address this gap, we present a comprehensive, multilingual dataset capturing all Wikipedia mentions and links shared in posts and comments on Reddit 2020-2023, excluding those from private and NSFW subreddits. Each linked Wikipedia article is enriched with revision history, page view data, article ID, redirects, and Wikidata identifiers. Through a research agreement with Reddit, our dataset ensures user privacy while providing a query and ID mechanism that integrates with the Reddit and Wikipedia APIs. This enables extended analyses for researchers studying how information flows across platforms. For example, Reddit discussions use Wikipedia for deliberation and fact-checking which subsequently influences Wikipedia content, by driving traffic to articles or inspiring edits. By analyzing the relationship between information shared and discussed on these platforms, our dataset provides a foundation for examining the interplay between social media discourse and collaborative knowledge consumption and production.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grassroots Platforms with Atomic Transactions: Social Networks, Cryptocurrencies, and Democratic Federations</title>
<link>https://arxiv.org/abs/2502.11299</link>
<guid>https://arxiv.org/abs/2502.11299</guid>
<content:encoded><![CDATA[
arXiv:2502.11299v3 Announce Type: replace-cross 
Abstract: Grassroots platforms aim to offer an egalitarian alternative to global platforms. Whereas global platforms can have only a single instance, grassroots platforms can have multiple instances that emerge and operate independently of each other and of any global resource except the network, and can interoperate and coalesce into ever-larger instances once interconnected. Key grassroots platforms include grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations. Previously, grassroots platforms were defined formally and proven grassroots using unary distributed transition systems, in which each transition is carried out by a single agent. However, grassroots platforms cater for a more abstract specification using transactions carried out atomically by multiple agents, something that cannot be expressed by unary transition systems. As a result, their original specifications and proofs were unnecessarily cumbersome and opaque.
  We enhance the notion of a distributed transition system to include atomic transactions and revisit the notion of grassroots platforms within this new foundation; present crisp specifications of key grassroots platforms using atomic transactions: befriending and defriending for grassroots social networks, coin swaps for grassroots cryptocurrencies, and communities forming, joining, and leaving a federation for grassroots democratic federations; prove a general theorem that a platform specified by atomic transactions that are so-called interactive is grassroots; show that the atomic transactions used to specify all three platforms are interactive; and conclude that the platforms thus specified are indeed grassroots. We thus provide a crisp mathematical foundation for grassroots platforms and a solid and clear starting point from which their implementation can commence.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>